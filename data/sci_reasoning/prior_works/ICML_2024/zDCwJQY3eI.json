{
  "prior_works": [
    {
      "title": "Polar Factorization and Monotone Rearrangement of Vector-Valued Functions",
      "authors": "Yann Brenier",
      "year": 1991,
      "role": "Foundational theorem",
      "relationship_sentence": "This paper proves the polar factorization theorem F = \u2207u \u2218 M that the ICML 2024 work operationalizes in a neural framework, defining both the convex potential u and the measure-preserving map M the method seeks to learn."
    },
    {
      "title": "Convex Analysis",
      "authors": "R. Tyrrell Rockafellar",
      "year": 1970,
      "role": "Convex conjugacy and optimization backbone",
      "relationship_sentence": "Properties of convex conjugates and the identity \u2207u* = (\u2207u)^{-1} (under suitable conditions) ground the paper\u2019s use of M = \u2207u* \u2218 F, enabling a principled route to compute the measure-preserving factor via the learned potential\u2019s conjugate."
    },
    {
      "title": "Input Convex Neural Networks",
      "authors": "Brandon Amos, Lei Xu, J. Zico Kolter",
      "year": 2017,
      "role": "Neural parameterization of convex potentials",
      "relationship_sentence": "ICNNs provide the architectural template used to parameterize the convex potential u, ensuring convexity by design so that its gradient can serve as the monotone map in Brenier\u2019s decomposition."
    },
    {
      "title": "Optimal Transport Mapping via Input Convex Neural Networks",
      "authors": "Y. Makkuva, A. Taghvaei, S. Oh, P. Viswanath",
      "year": 2020,
      "role": "Neural OT methodology for learning Monge maps",
      "relationship_sentence": "This work showed how to train ICNN potentials whose gradients approximate OT maps, directly informing the paper\u2019s training objectives and practice of learning \u2207u as the monotone factor in the polar factorization."
    },
    {
      "title": "Large-Scale Optimal Transport and Mapping Estimation",
      "authors": "Vincent Seguy, Bharath Damodaran, R\u00e9mi Flamary, Nicolas Courty, Alexandre Rolet, Mathieu Blondel",
      "year": 2018,
      "role": "Scalable neural OT and dual potentials",
      "relationship_sentence": "Introduces scalable neural parameterizations and optimization strategies for OT and mapping recovery, which the paper leverages conceptually for learning transport-related components at scale."
    },
    {
      "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport",
      "authors": "Marco Cuturi",
      "year": 2013,
      "role": "Computational enabler for modern OT",
      "relationship_sentence": "Entropic regularization and Sinkhorn iterations underlie many practical neural OT pipelines; this computational perspective enables efficient training signals the paper draws on when fitting convex potentials and related maps."
    },
    {
      "title": "Computational Optimal Transport",
      "authors": "Gabriel Peyr\u00e9, Marco Cuturi",
      "year": 2019,
      "role": "Algorithmic and theoretical toolkit",
      "relationship_sentence": "Provides algorithms and theory for Monge maps, Brenier potentials, and related numerical schemes, guiding the paper\u2019s practical choices for learning \u2207u and reasoning about measure-preserving rearrangements."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014realizing Brenier\u2019s polar factorization F = \u2207u \u2218 M with neural networks\u2014rests squarely on Brenier\u2019s 1991 theorem, which guarantees a decomposition of any suitable vector field into a monotone gradient map and a measure-preserving rearrangement. To make this constructive, the authors adopt input convex neural networks (ICNNs) to parameterize the convex potential u, following Amos, Xu, and Kolter\u2019s architecture that enforces convexity in the input. Building on neural optimal transport advances, particularly Makkuva et al.\u2019s demonstration that ICNNs can learn Monge maps as gradients of convex potentials, the paper trains \u2207u as the monotone factor of the decomposition.\nConvex analysis is essential to the implementation: Rockafellar\u2019s conjugacy theory justifies retrieving the measure-preserving component via M = \u2207u* \u2218 F, linking the learned u to its convex conjugate u*. Practical neural OT techniques also influence the pipeline. Seguy et al. introduced scalable neural parameterizations and training for OT maps and dual potentials, while Cuturi\u2019s Sinkhorn regularization enabled computationally tractable OT objectives that often serve as training signals or regularizers in such settings. The broader algorithmic and theoretical framing from Peyr\u00e9 and Cuturi\u2019s monograph informs choices around potentials, gradients, and measure-preserving properties.\nTogether, these works directly scaffold the paper\u2019s contribution: a practical neural mechanism to factor a vector field into a gradient of a convex potential and a measure-preserving map, with the conjugate-based route to M and ICNN-based parameterization of u providing the key operational pieces.",
  "analysis_timestamp": "2026-01-07T00:02:04.902154"
}
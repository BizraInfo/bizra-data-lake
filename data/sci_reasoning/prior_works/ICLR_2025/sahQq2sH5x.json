{
  "prior_works": [
    {
      "title": "Predictive coding in the visual cortex: a functional interpretation",
      "authors": "Rao and Ballard",
      "year": 1999,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work established the hierarchical predictive coding framework with prediction and error units that underlies the PCN formulations standardized and evaluated across tasks in the benchmark."
    },
    {
      "title": "An Approximation of the Error Back-Propagation Algorithm in a Predictive Coding Network",
      "authors": "Whittington and Bogacz",
      "year": 2017,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Its supervised PCN learning rule that approximates backprop serves as the primary algorithmic baseline the library implements, optimizes, and scales to larger architectures and datasets."
    },
    {
      "title": "Predictive Coding Approximates Backprop Along Arbitrary Computation Graphs",
      "authors": "Millidge et al.",
      "year": 2020,
      "arxiv_id": "2006.04182",
      "role": "Extension",
      "relationship_sentence": "The benchmark adopts this general formulation of PCN updates for arbitrary network graphs, implementing these variants to compare their efficiency and scalability under a unified setup."
    },
    {
      "title": "Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation",
      "authors": "Scellier and Bengio",
      "year": 2017,
      "arxiv_id": "1602.05179",
      "role": "Related Problem",
      "relationship_sentence": "Its two-phase nudging and energy-minimization procedure directly inspired algorithmic variants adapted into the PCN framework and included as standardized baselines in the benchmark."
    },
    {
      "title": "Random synaptic feedback weights support error backpropagation for deep learning",
      "authors": "Lillicrap et al.",
      "year": 2016,
      "arxiv_id": "1609.01596",
      "role": "Related Problem",
      "relationship_sentence": "The idea of fixed random feedback pathways (Direct Feedback Alignment) is integrated as a PCN-compatible training variant to assess bio-plausible alternatives within the shared benchmarking suite."
    },
    {
      "title": "Difference Target Propagation",
      "authors": "Lee et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Local target-setting from DTP motivates PCN variants with layerwise targets that the library implements to systematically compare against standard PCN learning on common datasets."
    }
  ],
  "synthesis_narrative": "Rao and Ballard formalized hierarchical predictive coding with distinct prediction and error units, defining the computational scaffold used by modern predictive coding networks. Whittington and Bogacz then provided a concrete supervised learning rule for these networks, showing iterative inference and local updates can approximate backpropagation, which quickly became the de facto PCN training baseline. Millidge and colleagues generalized this equivalence beyond simple chains, deriving update equations for arbitrary computation graphs and clarifying how PCN dynamics map onto standard deep models. In parallel, energy-based approaches like Equilibrium Propagation introduced two-phase nudged inference to compute gradients without explicit backprop, while Direct Feedback Alignment demonstrated that fixed random feedback pathways can guide deep learning without symmetric weight transport. Difference Target Propagation contributed a complementary local-learning perspective by constructing layerwise targets, enabling learning with only local signals rather than global error backpropagation.\nTogether these works established the PCN computational template, multiple closely related bio-plausible training mechanisms, and a set of algorithmic variants that had largely been evaluated on small-scale tasks with disparate implementations. This created a clear need for a unified, efficient platform to instantiate the canonical PCN rule and its generalizations, and to adapt EP-, DFA-, and DTP-inspired procedures within the same predictive-coding formalism. The present work naturally follows by standardizing these methods into a simple, high-performance library and comprehensive benchmarks, enabling fair, scalable comparisons across algorithms and architectures, and revealing how design choices and inference schemes impact PCN performance at modern scales.",
  "target_paper": {
    "title": "Benchmarking Predictive Coding Networks -- Made Simple",
    "authors": "Luca Pinchetti, Chang Qi, Oleh Lokshyn, Cornelius Emde, Amine M'Charrak, Mufeng Tang, Simon Frieder, Bayar Menzat, Gaspard Oliviers, Rafal Bogacz, Thomas Lukasiewicz, Tommaso Salvatori",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "cognitive science, predictive coding, computational neuroscience",
    "abstract": "In this work, we tackle the problems of efficiency and scalability for predictive coding networks (PCNs) in machine learning. To do so, we  propose a library that focuses on performance and simplicity, and use it to implement a large set of standard benchmarks for the community to use for their experiments. As most works in the field propose their own tasks and architectures, do not compare one against each other, and focus on small-scale tasks, a simple and fast open-source library, and a comprehensive set of benchmarks, would address all of these concerns. Then, we perform extensive tests on such benchmarks using both existing algorithms for PCNs, as well as adaptations of other methods popular in the bio-plausible deep learning community. All of this has allowed us to (i) test architectures much larger than commonly used in the literature, on more complex datasets; (ii) reach new state-of-the-art results in all of the tasks and dataset provided; (iii) clearly highlight what the curr",
    "openreview_id": "sahQq2sH5x",
    "forum_id": "sahQq2sH5x"
  },
  "analysis_timestamp": "2026-01-06T11:15:37.620598"
}
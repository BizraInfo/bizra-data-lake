{
  "prior_works": [
    {
      "title": "Human-level control through deep reinforcement learning",
      "authors": "Volodymyr Mnih et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "Introduced the target network mechanism that this paper formalizes and analyzes; Che et al. prove that target networks, when combined with over-parameterized linear function approximation, can guarantee stable off-policy bootstrapping."
    },
    {
      "title": "Tree-Based Batch Mode Reinforcement Learning (Fitted Q-Iteration)",
      "authors": "Damien Ernst et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "Established the fixed-target iterative update paradigm (frozen targets per iteration) that underlies the target-network view analyzed here as a stabilizing ingredient for bootstrapped value estimation."
    },
    {
      "title": "Residual Algorithms: Reinforcement Learning with Function Approximation",
      "authors": "Leemon C. Baird",
      "year": 1995,
      "role": "Gap Identification",
      "relationship_sentence": "Provided the canonical counterexample showing divergence of off-policy TD with linear function approximation, a limitation the present paper directly addresses and empirically resolves."
    },
    {
      "title": "An Analysis of Temporal-Difference Learning with Function Approximation",
      "authors": "John N. Tsitsiklis et al.",
      "year": 1997,
      "role": "Foundation",
      "relationship_sentence": "Gave the core theoretical framework for TD with linear function approximation (contraction and stability conditions), which Che et al. relax by leveraging target networks plus over-parameterization."
    },
    {
      "title": "Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation",
      "authors": "Richard S. Sutton et al.",
      "year": 2009,
      "role": "Baseline",
      "relationship_sentence": "Introduced GTD/TDC, a prior provably convergent approach to stabilize off-policy TD with linear function approximation; the new paper provides an alternative stabilization route via target networks and over-parameterization."
    },
    {
      "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
      "authors": "Richard S. Sutton et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "Proposed Emphatic TD to ensure off-policy convergence with linear function approximation; Che et al. show stability can instead be obtained under weaker conditions by combining a target network with over-parameterization."
    },
    {
      "title": "A Finite Time Analysis of Temporal Difference Learning with Linear Function Approximation",
      "authors": "Jalaj Bhandari et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Provides finite-sample/high-probability error analyses for linear TD that the present work extends to the target-network + over-parameterized setting to derive value estimation error bounds."
    }
  ],
  "synthesis_narrative": "The core innovation of Che et al. is a theoretical justification that the widespread target-network heuristic, when paired with over-parameterized linear function approximation, stabilizes off-policy bootstrapped value estimation under weaker conditions than previously known. This builds directly on two foundational strands. First, the fixed-target paradigm\u2014made explicit in Fitted Q-Iteration\u2014 and later popularized in deep Q-learning via target networks, provided the architectural device of a lagged (or frozen) target that empirically curbs instability in bootstrapped updates. Second, classic theory established both the power and brittleness of TD with function approximation: Tsitsiklis and Van Roy characterized when linear TD is stable, while Baird\u2019s counterexample crystallized the off-policy divergence risk at the heart of the deadly triad.\n\nHistorically, provable off-policy stabilization with function approximation came from algorithmic corrections such as GTD/TDC and Emphatic TD, which modify update rules or weighting to restore convergence. Che et al. chart a different path: they show that the combination of a target network and an over-parameterized linear function class suffices to yield a weaker but natural convergence condition\u2014even when either ingredient alone is insufficient. Technically, their finite-sample, high-probability guarantees leverage and extend finite-time analyses for linear TD to this new regime, yielding error bounds and constructive conditions applicable to expected updates, batches of complete trajectories, and truncated-trajectory variants. Empirically, resolving Baird\u2019s counterexample and validating on control tasks underscores that this theoretical mechanism offers a principled, practically simple alternative to prior correction-based stabilizers.",
  "analysis_timestamp": "2026-01-06T23:09:26.402049"
}
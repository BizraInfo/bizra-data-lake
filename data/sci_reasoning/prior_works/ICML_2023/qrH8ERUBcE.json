{
  "prior_works": [
    {
      "title": "Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning",
      "authors": "Rodrigo Toro Icarte et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "Hierarchies of Reward Machines directly extends the RM formalism by adding callable submachines; the paper\u2019s empirical baseline and the notion of a flat, automaton-structured reward come from this work."
    },
    {
      "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning",
      "authors": "Richard S. Sutton et al.",
      "year": 1999,
      "role": "Foundation",
      "relationship_sentence": "HRMs are operationalized by treating each RM call as an option, relying explicitly on the options framework\u2019s initiation/termination and temporally extended actions to learn each subtask independently."
    },
    {
      "title": "Reinforcement Learning with Hierarchies of Machines",
      "authors": "Ronald Parr et al.",
      "year": 1998,
      "role": "Inspiration",
      "relationship_sentence": "The core idea of empowering an automaton to invoke subroutines mirrors HAMs; HRMs adapt this call/return semantics from hierarchical controllers to the reward-function automaton setting."
    },
    {
      "title": "Statecharts: A Visual Formalism for Complex Systems",
      "authors": "David Harel",
      "year": 1987,
      "role": "Inspiration",
      "relationship_sentence": "HRMs import the hierarchical finite-state-machine notion\u2014states that encapsulate nested machines and callable substructures\u2014directly from the Statecharts paradigm."
    },
    {
      "title": "Compositional Reinforcement Learning from Logical Specifications",
      "authors": "Nimrod Jothimurugan et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "This work showed how flat automata from temporal logic can guide options but lacked hierarchical callable structure; HRMs address this gap by enabling nested RM calls for deeper abstraction."
    },
    {
      "title": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition",
      "authors": "Thomas G. Dietterich",
      "year": 2000,
      "role": "Related Problem",
      "relationship_sentence": "MAXQ established the efficiency of hierarchical decomposition; HRMs achieve analogous benefits by decomposing the reward specification (via RM calls) rather than the value function directly."
    },
    {
      "title": "Decision-Theoretic Planning with Non-Markovian Rewards",
      "authors": "Sylvie Thi\u00e9baux et al.",
      "year": 2006,
      "role": "Foundation",
      "relationship_sentence": "RMs build on the NMRDP tradition formalizing rewards as history-dependent automata; HRMs inherit and extend this foundation to hierarchical, callable reward machines."
    }
  ],
  "synthesis_narrative": "The core innovation in Hierarchies of Reward Machines (HRMs) is to endow the reward-machine formalism with callable submachines, enabling hierarchical abstraction over the reward structure itself. This directly builds on Reward Machines by Toro Icarte et al., which introduced flat automaton-structured rewards and Q-learning schemes over them; HRMs generalize that baseline by permitting nested calls, creating reusable subtask specifications. To exploit these call-based subtasks, HRMs lean on the options framework (Sutton et al.), treating each RM call as an option with its own initiation and termination\u2014making the hierarchical reward decomposition learnable with off-the-shelf option-based RL. The callable-automaton idea is inspired by hierarchical controllers, notably Hierarchical Abstract Machines (Parr & Russell), and by Statecharts (Harel), which pioneered hierarchical finite-state machines with subroutine-like semantics; HRMs adapt these notions specifically to reward-specification automata. Prior compositional RL from logical specifications (Jothimurugan et al.) demonstrated that flat automata can guide option construction but did not provide hierarchical callable structure, a gap HRMs explicitly fill to improve scalability and reuse. Finally, the non-Markovian reward literature (Thi\u00e9baux et al.) provides the theoretical foundation for representing history-dependent rewards via automata, a lineage from which RMs\u2014and thus HRMs\u2014directly descend. Together, these works establish the formal, hierarchical, and learning machinery that HRMs integrate and extend.",
  "analysis_timestamp": "2026-01-06T23:09:26.540705"
}
{
  "prior_works": [
    {
      "title": "Invariant Risk Minimization",
      "authors": "Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, David Lopez-Paz",
      "year": 2019,
      "role": "Foundational formulation of learning invariances across environments to avoid spurious correlations.",
      "relationship_sentence": "The paper\u2019s goal of severing reliance on spurious features builds on IRM\u2019s central insight that robustness arises from focusing on features invariant across environments, while seeking a more practical, data-centric route via pruning rather than explicit invariance constraints."
    },
    {
      "title": "Distributionally Robust Neural Networks for Group Shifts",
      "authors": "Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, Percy Liang",
      "year": 2020,
      "role": "Group-DRO objective for worst-group robustness under spurious correlations.",
      "relationship_sentence": "By highlighting the importance\u2014and difficulty\u2014of worst-group robustness (often requiring group labels), this work motivates the present paper\u2019s label-free alternative that removes a small set of culprit samples to reduce spurious reliance."
    },
    {
      "title": "Environment Inference for Invariant Learning (EIIL)",
      "authors": "Elliot Creager, J\u00f6rn-Henrik Jacobsen, Richard Zemel",
      "year": 2021,
      "role": "Infers pseudo-environments without group labels to enable invariant learning.",
      "relationship_sentence": "EIIL\u2019s idea of extracting environment structure from model behavior underpins the new paper\u2019s strategy of leveraging model signals to identify and target samples that induce spurious correlations, but it replaces reweighting with pruning."
    },
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh, Percy Liang",
      "year": 2017,
      "role": "Per-example influence estimation linking training points to test-time behavior.",
      "relationship_sentence": "The notion that individual training samples can disproportionately drive harmful behavior directly informs the paper\u2019s core mechanism of ranking and pruning samples most responsible for spurious feature reliance."
    },
    {
      "title": "An Empirical Study of Example Forgetting in Deep Neural Networks",
      "authors": "Ivan Toneva, Alessandro Sordoni, R\u00e9mi Tachet des Combes, Adam Trischler, Yoshua Bengio, Geoffrey J. Gordon",
      "year": 2019,
      "role": "Shows that a small subset of examples disproportionately shapes learning and generalization, enabling data-pruning perspectives.",
      "relationship_sentence": "This empirical finding supports the paper\u2019s discovery that spurious correlations can be driven by a handful of samples, motivating targeted removal rather than wholesale reweighting or architectural changes."
    },
    {
      "title": "Data Shapley: Towards Data Valuation using Shapley Values",
      "authors": "Amirata Ghorbani, James Zou",
      "year": 2019,
      "role": "Framework for quantifying each training point\u2019s contribution to a model\u2019s performance.",
      "relationship_sentence": "The data valuation perspective provides a principled basis for identifying \u2018harmful\u2019 examples whose removal can improve robustness, aligning with the paper\u2019s pruning-based intervention to sever spurious correlations."
    },
    {
      "title": "Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations",
      "authors": "Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson",
      "year": 2023,
      "role": "Shows that features often contain both core and spurious signals and that adjusting decision boundaries can restore robustness.",
      "relationship_sentence": "This insight supports the paper\u2019s premise that a small set of training signals can tilt the decision boundary toward spurious cues\u2014hence pruning those signals can realign the boundary toward core features."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014severing spurious correlations by pruning a small set of harmful training examples\u2014emerges at the intersection of robustness-under-shift and per-example attribution. IRM formalized the goal of learning invariances to avoid shortcut features, while Group DRO operationalized worst-group robustness, revealing practical gaps when group labels are unavailable. EIIL advanced this by inferring environments from model behavior, demonstrating that one can recover structure related to spuriousness without annotations. Parallel to these robustness threads, influence functions and data valuation (Data Shapley) established that individual training points can be quantified for their impact on model predictions, offering mechanisms to identify samples that disproportionately induce undesirable behavior. Complementing these, example forgetting studies showed that a small subset of examples critically shapes generalization, suggesting that strategic removal can alter what models learn. Finally, results on last-layer retraining indicate that learned representations often contain both core and spurious signals and that decision boundaries can be corrected with minimal interventions\u2014implying that pruning targeted examples could similarly reorient the boundary toward invariant features. Building on these insights, the present work addresses a less-explored regime where spurious signals are weaker and harder to detect per-example, showing that even then, a handful of samples can trigger catastrophic reliance on spurious cues and that carefully identifying and pruning them provides a simple, annotation-free route to robust generalization.",
  "analysis_timestamp": "2026-01-06T23:42:48.088371"
}
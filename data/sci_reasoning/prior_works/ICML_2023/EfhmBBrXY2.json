{
  "prior_works": [
    {
      "title": "Arithmetic Coding for Data Compression",
      "authors": "Ian H. Witten et al.",
      "year": 1987,
      "role": "Foundation",
      "relationship_sentence": "Introduces the arithmetic-coding interval mapping that Arithmetic Sampling directly adopts to define an implicit codebook over model-generated sequences, enabling disjoint sub-interval sampling that preserves the true model distribution."
    },
    {
      "title": "Stochastic Beams and the Gumbel-Top-k Trick",
      "authors": "Wouter Kool et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Provides sampling-without-replacement for sequence models (guaranteed diversity) but relies on search/beam-style expansion that is not embarrassingly parallel; Arithmetic Sampling replaces this with arithmetic-code stratification to achieve parallel, distribution-faithful diverse decoding."
    },
    {
      "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models",
      "authors": "Ashwin K. Vijayakumar et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Serves as a primary diversity-seeking decoding baseline that enforces difference across beams via heuristic penalties but is sequential and distribution-altering; Arithmetic Sampling achieves provable beam diversity without search-time coupling and without biasing the model distribution."
    },
    {
      "title": "The Curious Case of Neural Text Degeneration",
      "authors": "Ari Holtzman et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Introduces nucleus (top\u2011p) sampling\u2014embarrassingly parallel yet prone to duplicate samples and truncation bias\u2014highlighting the precise gap Arithmetic Sampling fills by guaranteeing non-duplicate outputs while preserving unbiased expectations."
    },
    {
      "title": "Hierarchical Neural Story Generation",
      "authors": "Angela Fan et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Popularizes top\u2011k sampling, which is parallel but offers no non-duplication guarantees and alters the target distribution; Arithmetic Sampling delivers the same parallelism with provable diversity and unbiasedness."
    },
    {
      "title": "A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code",
      "authors": "M. D. McKay et al.",
      "year": 1979,
      "role": "Inspiration",
      "relationship_sentence": "Latin Hypercube/stratified sampling motivates Arithmetic Sampling\u2019s use of equal-mass stratification of the [0,1) code space to reduce variance of expectation estimates while maintaining correctness."
    }
  ],
  "synthesis_narrative": "Arithmetic Sampling\u2019s core insight is to view a language model as implicitly defining an arithmetic codebook over all token sequences and to sample multiple outputs by stratifying this code space. This idea rests directly on arithmetic coding (Witten et al., 1987), which maps sequences to disjoint subintervals of [0,1); by drawing one uniform per equal-mass stratum, the method yields parallel samples that are almost surely distinct while exactly preserving the model distribution. The variance reduction effect of sampling once per stratum is inspired by classical stratified/Latin hypercube sampling (McKay et al., 1979), explaining the paper\u2019s empirical reduction in standard deviation when estimating expected BLEU.\n\nThe work is positioned between two established families of decoders. On one side, diversity-seeking search methods like Diverse Beam Search (Vijayakumar et al., 2018) and Gumbel-Top\u2011k-based stochastic beams (Kool et al., 2019) guarantee different outputs but are inherently sequential or tightly coupled, hindering embarrassingly parallel generation and often biasing expectations. On the other, widely used parallel samplers such as top\u2011k (Fan et al., 2018) and nucleus/top\u2011p (Holtzman et al., 2020) preserve parallelism but provide no guarantees against duplicate samples and can distort the target distribution. Arithmetic Sampling unifies the benefits: it retains embarrassingly parallel generation like top\u2011k/top\u2011p, inherits diversity guarantees reminiscent of Gumbel/beam methods, and\u2014crucially\u2014produces unbiased, consistent expectations under the original model via arithmetic-code stratification.",
  "analysis_timestamp": "2026-01-06T23:09:26.551006"
}
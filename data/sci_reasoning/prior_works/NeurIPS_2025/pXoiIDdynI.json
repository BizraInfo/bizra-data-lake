{
  "prior_works": [
    {
      "title": "Multicalibration: Calibration for the (Computationally-Identifiable) Masses",
      "authors": [
        "Guy Hebert-Johnson",
        "Michael P. Kim",
        "Omer Reingold",
        "Aaron Rothblum"
      ],
      "year": 2018,
      "role": "Foundational definition and algorithmic framework",
      "relationship_sentence": "Introduced multicalibration and the audit\u2013correct boosting template that underlies later reductions from omniprediction/fairness constraints to online learning, providing the conceptual starting point the present paper strengthens in the swap and online settings."
    },
    {
      "title": "Multiaccuracy: Black-box Post-processing for Fairness in Classification",
      "authors": [
        "Michael P. Kim",
        "Atoosa Kasirzadeh Ghorbani",
        "James Zou"
      ],
      "year": 2019,
      "role": "Auditing/correction technique and multigroup guarantees",
      "relationship_sentence": "Developed the auditor-based post-processing paradigm and multi-group error control that the new algorithm generalizes to swap-style constraints and leverages when converting L2 swap-multicalibration guarantees into L1 and omniprediction rates."
    },
    {
      "title": "Omnipredictors",
      "authors": [
        "Parikshit Gopalan",
        "Adam Tauman Kalai",
        "Omer Reingold",
        "Vatsal Sharan",
        "Udi Wieder"
      ],
      "year": 2021,
      "role": "Formal link between multicalibration and omniprediction",
      "relationship_sentence": "Established that suitable multicalibration implies omniprediction for classes of losses; the present work exploits this pipeline, propagating improved L2 swap-multicalibration bounds to sharper L1 swap-multicalibration and swap-omniprediction rates for convex Lipschitz losses."
    },
    {
      "title": "Online Multivalid Learning",
      "authors": [
        "Varun Gupta",
        "Christopher Jung",
        "Akshay Krishnamurthy",
        "Gurcharan S. Noarov",
        "Aaron Roth",
        "Cathy Wu"
      ],
      "year": 2022,
      "role": "Online framework for multicalibration-style guarantees",
      "relationship_sentence": "Provided an online learning perspective and regret-style guarantees for sequential calibration across many subpopulations, a template the new paper refines to achieve faster \u00d5(T^{1/3}) swap-multicalibration error in the adversarial sequence setting."
    },
    {
      "title": "From External to Internal Regret",
      "authors": [
        "Avrim Blum",
        "Yishay Mansour"
      ],
      "year": 2007,
      "role": "Swap/internal-regret algorithms",
      "relationship_sentence": "Introduced efficient algorithms controlling swap/internal regret, the game-theoretic analogue of swap constraints; the present work translates this swap-regret viewpoint to calibration, guiding the design of no-swap-style updates that enforce swap multicalibration."
    },
    {
      "title": "Online Learning with Predictable Sequences",
      "authors": [
        "Alexander Rakhlin",
        "Karthik Sridharan"
      ],
      "year": 2013,
      "role": "Optimistic online learning for faster-than-\u221aT rates",
      "relationship_sentence": "Showed how optimism/predictability can yield sub-\u221aT rates via optimistic mirror descent; the improved \u00d5(T^{1/3}) bound here leverages similar optimistic and saddle-point ideas in the predictor\u2013auditor game underlying swap multicalibration."
    },
    {
      "title": "Swap Multicalibration and Swap Omniprediction (and an Open Problem on \u221aT L2-multicalibration)",
      "authors": [
        "S. Garg",
        "et al."
      ],
      "year": 2024,
      "role": "Immediate predecessor and open problem",
      "relationship_sentence": "Formulated the swap variants and explicitly posed whether efficient \u00d5(\u221aT) L2-multicalibration is achievable against bounded linear functions; the present paper resolves this more strongly by attaining an \u00d5(T^{1/3}) L2 swap-multicalibration rate and propagating it to L1 and omniprediction."
    }
  ],
  "synthesis_narrative": "The paper advances a line of work that connects multigroup calibration to omniprediction by tightening online rates under swap-style constraints. Hebert-Johnson et al. (2018) introduced multicalibration and the audit\u2013correct paradigm, giving a reduction from fairness-style guarantees to iterative learning against an auditor. Kim et al. (2019) operationalized this through black-box multiaccuracy auditing, establishing the correction mechanics that later works adapt when moving between L2 and L1 metrics. Gopalan et al. (2021) formalized omnipredictors and proved that sufficiently strong multicalibration implies simultaneous risk minimization for broad loss classes, creating the conduit the present paper uses to transfer improved calibration error into improved omniprediction error for convex Lipschitz losses.\n\nOn the online front, Gupta et al. (2022) cast multicalibration-like guarantees as regret bounds in sequential settings, providing the predictor\u2013auditor game template. The \u201cswap\u201d strengthening draws directly on swap/internal-regret theory (Blum and Mansour, 2007), replacing external-style constraints with permutation-based constraints that better capture post-hoc recoding of predictions. Achieving rates beyond \u221aT in such adversarial games leverages optimistic online learning (Rakhlin and Sridharan, 2013), which provides faster convergence under structure/predictability; the present work deploys analogous optimistic saddle-point updates to reach \u00d5(T^{1/3}) L2 swap-multicalibration. Finally, Garg et al. (2024) articulated the swap framework and posed the \u221aT open problem against bounded linear functions; this paper resolves it in a strongly affirmative fashion by surpassing \u221aT with \u00d5(T^{1/3}) and then propagating the gain to \u00d5(T^{2/3}) bounds for L1 swap multicalibration and swap omniprediction.",
  "analysis_timestamp": "2026-01-06T23:42:48.155592"
}
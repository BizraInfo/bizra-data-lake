{
  "prior_works": [
    {
      "title": "Socially Aware Motion Planning with Deep Reinforcement Learning",
      "authors": "Yu Fan Chen et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "SDA adopts the RL-based crowd-navigation/POMDP formulation introduced here, but replaces explicit human-state inputs with a learned latent 'social dynamics' code and later a history-based inference mechanism."
    },
    {
      "title": "Crowd-Robot Interaction: Navigation in Dense Crowds with Attention-based Deep Reinforcement Learning",
      "authors": "Changan Chen et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "This attention-based DRL method requires full observability of each pedestrian\u2019s state; SDA explicitly addresses this limitation by training with privileged human trajectories yet deploying with no trajectory inputs, inferring dynamics from the robot\u2019s own state\u2013action history."
    },
    {
      "title": "Social LSTM: Human Trajectory Prediction in Crowded Spaces",
      "authors": "Alexandre Alahi et al.",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "The notion that multi-agent 'social dynamics' can be captured by encoding human trajectories directly motivates SDA\u2019s first-stage trajectory encoder that provides privileged supervision to the control policy."
    },
    {
      "title": "Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data",
      "authors": "Boris Ivanovic et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Trajectron++ demonstrates compact latent representations for multi-agent social interactions; SDA repurposes this idea to produce a latent social-dynamics code that conditions policy learning."
    },
    {
      "title": "A new learning paradigm: Learning Using Privileged Information",
      "authors": "Vladimir Vapnik et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "SDA\u2019s two-stage design\u2014training with access to trajectories as privileged signals and testing without them\u2014is a direct instantiation of the LUPI paradigm."
    },
    {
      "title": "Learning Dexterous In-Hand Manipulation",
      "authors": "Marcin Andrychowicz et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "This work popularized asymmetric training in deep RL where learning is aided by privileged state while deployment uses partial observations, a strategy SDA adapts to social navigation with human-trajectory privilege."
    },
    {
      "title": "Deep Recurrent Q-Learning for Partially Observable MDPs",
      "authors": "Matthew Hausknecht et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "SDA\u2019s reliance on the robot\u2019s state\u2013action history to infer unobserved social dynamics builds on DRQN\u2019s use of recurrence to approximate belief in POMDPs."
    }
  ],
  "synthesis_narrative": "Following the Human Thread in Social Navigation fuses three intellectual lines into a single contribution: (i) RL-based crowd navigation, (ii) trajectory-encoded social dynamics, and (iii) privileged-information training for partial observability. The RL/POMDP framing pioneered in Socially Aware Motion Planning with Deep Reinforcement Learning anchors SDA\u2019s problem setup and evaluation, while Crowd-Robot Interaction highlights a critical gap\u2014policies that assume full observability of pedestrian states. SDA explicitly targets this gap by designing a controller that learns from trajectories when available but must operate without them. The choice to encode human trajectories as a compact social-dynamics latent is directly inspired by trajectory forecasting advances: Social LSTM established that social interactions are captured by sequence encoders, and Trajectron++ showed the efficacy of latent, multi-agent interaction representations\u2014ideas SDA retools for control rather than prediction. The two-stage training strategy is grounded in Learning Using Privileged Information, with a pragmatic instantiation drawn from asymmetric training in deep RL, as exemplified by Learning Dexterous In-Hand Manipulation, where privileged state accelerates policy learning but is not used at deployment. Finally, the move to infer unobserved dynamics from the robot\u2019s own state\u2013action history rests on Deep Recurrent Q-Learning\u2019s insight that recurrence can approximate belief in POMDPs. Together, these works directly enable SDA\u2019s core innovation: a privileged-to-deployed pipeline that encodes human social dynamics and then reconstructs them from egocentric history for robust social navigation.",
  "analysis_timestamp": "2026-01-06T23:09:26.597238"
}
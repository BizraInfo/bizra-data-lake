{
  "prior_works": [
    {
      "title": "Breaking the Curse of Dimensionality with Convex Neural Networks",
      "authors": "Francis Bach",
      "year": 2017,
      "arxiv_id": "1412.8690",
      "role": "Foundation",
      "relationship_sentence": "This work introduced the convex (variation-norm/atomic-norm) formulation of two-layer networks that underlies casting regularized network training as a convex program whose dual can be analyzed to characterize solutions."
    },
    {
      "title": "Global Optimality in Tensor Factorization, Deep Learning, and Beyond",
      "authors": "Benjamin Haeffele et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By exploiting positive homogeneity to link nonconvex factorizations with convex programs and global-optimality certificates, this paper provided the key homogeneity/atomic-norm insight leveraged to map ReLU networks with regularization to a convex dual framework."
    },
    {
      "title": "Convex Formulations for Two-Layer Neural Networks",
      "authors": "Ergen et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "The present work directly builds on this convex-duality characterization of L2-regularized two-layer ReLU training, extending it to a full loss-landscape analysis that classifies stationary points and proves connectivity of optimal sets (and to vector outputs and parallel three-layer architectures)."
    },
    {
      "title": "Spurious Valleys in Two-Layer Neural Networks",
      "authors": "Luca Venturi et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Their identification of spurious valleys and disconnected low-loss regions in shallow networks motivates resolving when and why such pathologies occur, which is addressed here via dual certificates that delineate topology and phase transitions of global optima with width."
    },
    {
      "title": "Mode Connectivity in Loss Landscapes of Neural Networks",
      "authors": "Timur Garipov et al.",
      "year": 2018,
      "arxiv_id": "1802.10026",
      "role": "Inspiration",
      "relationship_sentence": "The empirical observation of non-increasing-loss paths connecting minima directly inspired the construction of certified non-increasing-loss paths to arbitrary global optima using the convex program\u2019s dual structure."
    },
    {
      "title": "Gradient Descent Finds Global Minima of Over-parameterized Deep Neural Networks",
      "authors": "Simon S. Du et al.",
      "year": 2019,
      "arxiv_id": "1810.02054",
      "role": "Related Problem",
      "relationship_sentence": "Results showing global convergence under sufficient width motivated the dual-based analysis that pinpoints a phase transition in the topology of global optima as width varies and explains when connectivity emerges."
    }
  ],
  "synthesis_narrative": "Bach established that two-layer networks admit a convex representation via variation/atomic norms, enabling training perspectives grounded in convex geometry and duality. Haeffele and Vidal leveraged positive homogeneity to relate nonconvex factorizations to convex programs with global-optimality guarantees, clarifying how appropriate regularization induces convex atomic norms over network outputs. Building on these convex viewpoints, Ergen and Pilanci formulated L2-regularized two-layer ReLU training as an equivalent convex problem via duality, providing a precise bridge from parameter-space nonconvexity to function-space convexity. In contrast, Venturi, Bandeira, and Bruna highlighted that shallow ReLU networks can exhibit spurious valleys and disconnected low-loss regions, pinpointing structural pitfalls in the landscape. Garipov and colleagues empirically demonstrated mode connectivity, exhibiting non-increasing-loss paths between minima that suggested hidden geometric regularity. Du and coauthors showed that sufficient overparameterization yields global convergence of gradient descent, suggesting that width governs qualitative changes in optimization topology.\nTogether, these works revealed an opportunity: convex function-space formulations and dual certificates could resolve open questions about nonconvex landscapes\u2014stationary-point structure, connectivity, and multiplicity\u2014beyond empirical observations and width-asymptotic results. The present paper synthesizes the convex duality program (from Bach and Ergen\u2013Pilanci) with the landscape phenomena (from Venturi and Garipov) and the width-driven guarantees (from Du), to rigorously characterize all stationary points, certify non-increasing-loss paths to arbitrary global optima, expose nonuniqueness of optimal solutions, and reveal a phase transition in the topology of global optima as width increases, extending the analysis to vector outputs and parallel three-layer networks.",
  "target_paper": {
    "title": "Exploring The Loss Landscape Of Regularized Neural Networks Via Convex Duality",
    "authors": "Sungyoon Kim, Aaron Mishkin, Mert Pilanci",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Convex duality, Machine Learning Theory, Loss Landscape, Optimal Sets",
    "abstract": "We discuss several aspects of the loss landscape of regularized neural networks: the structure of stationary points, connectivity of optimal solutions, path with non-increasing loss to arbitrary global optimum, and the nonuniqueness of optimal solutions, by casting the problem into an equivalent convex problem and considering its dual. Starting from two-layer neural networks with scalar output, we first characterize the solution set of the convex problem using its dual and further characterize all stationary points. With the characterization, we show that the topology of the global optima goes through a phase transition as the width of the network changes, and construct counterexamples where the problem may have a continuum of optimal solutions. Finally, we show that the solution set characterization and connectivity results can be extended to different architectures, including two layer vector-valued neural networks and parallel three-layer neural networks.",
    "openreview_id": "4xWQS2z77v",
    "forum_id": "4xWQS2z77v"
  },
  "analysis_timestamp": "2026-01-06T12:18:08.942221"
}
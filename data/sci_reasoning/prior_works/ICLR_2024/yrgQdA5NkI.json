{
  "prior_works": [
    {
      "title": "MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields",
      "authors": "Ilyes Batatia et al.",
      "year": 2022,
      "arxiv_id": "2206.07697",
      "role": "Baseline",
      "relationship_sentence": "MFNs take the E(3)-equivariant, size-extensive local message passing core of MACE and augment it with learned matrix-function propagation to capture non-local interactions that MACE\u2019s strictly local design cannot represent."
    },
    {
      "title": "Invariant and Equivariant Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work\u2019s characterization of permutation-equivariant linear maps as polynomials in the adjacency matrix directly motivates MFN\u2019s design of layers as analytic matrix functions, ensuring permutation equivariance while enabling global, non-local mixing."
    },
    {
      "title": "CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters",
      "authors": "Ron Levie et al.",
      "year": 2018,
      "arxiv_id": "1705.07664",
      "role": "Inspiration",
      "relationship_sentence": "By introducing rational spectral filters that approximate Green\u2019s-function-like responses, CayleyNets provides the template MFNs generalize via analytic matrix functions and resolvent expansions to achieve efficient long-range propagation."
    },
    {
      "title": "Graph Neural Networks with Convolutional ARMA Filters",
      "authors": "Filippo Maria Bianchi et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "ARMA filters implement resolvent-like rational graph filters through recursive layers, which MFNs extend to a principled class of analytic matrix functions (including resolvents) integrated with geometric equivariance."
    },
    {
      "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank (APPNP)",
      "authors": "Johannes Klicpera et al.",
      "year": 2019,
      "arxiv_id": "1810.05997",
      "role": "Inspiration",
      "relationship_sentence": "APPNP\u2019s use of a Neumann-series approximation to the personalized PageRank resolvent (I \u2212 \u03b1P)^{-1} directly inspires MFN\u2019s resolvent expansion to parameterize non-local interactions with linear-time implementations."
    },
    {
      "title": "Diffusion Improves Graph Learning (GDC)",
      "authors": "Johannes Klicpera et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "GDC shows that applying diffusion-based matrix functions (e.g., PPR/heat kernels) to graphs improves long-range information flow, which MFNs internalize as learnable analytic matrix functions rather than as a preprocessing step."
    },
    {
      "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
      "authors": "Uri Alon and Eran Yahav",
      "year": 2021,
      "arxiv_id": "2006.05205",
      "role": "Gap Identification",
      "relationship_sentence": "This work\u2019s formalization of oversquashing in MPNNs motivates MFN\u2019s non-local matrix-function propagation as a principled remedy for long-range information bottlenecks."
    }
  ],
  "synthesis_narrative": "Permutation equivariance on graphs was rigorously characterized by showing that linear equivariant layers can be written as polynomials of the adjacency (and related) matrices, establishing matrix functions as a principled vehicle for global, structure-respecting operations. Building on spectral methods, rational graph filters such as CayleyNets and ARMA introduced resolvent-like responses that approximate Green\u2019s functions, enabling stable long-range propagation beyond finite-hop polynomials while remaining computationally tractable. Complementarily, diffusion-based techniques like APPNP operationalized personalized PageRank via a Neumann-series expansion of a resolvent, and GDC demonstrated that applying diffusion kernels (e.g., PPR, heat) as matrix functions enhances global information mixing without heavy message passing. In atomistic machine learning, MACE provided a highly accurate, E(3)-equivariant, size-extensive local message-passing architecture, yet by design emphasized strictly local interactions, leaving non-local dependencies underrepresented. Concurrently, the oversquashing literature made explicit the limitations of finite-hop MPNNs in transmitting long-range signals.\nTaken together, these works reveal a clear opportunity: marry the theoretical guarantees of matrix-function-based equivariant operators and the efficiency of resolvent/diffusion filters with the geometric fidelity and extensivity of modern E(3)-equivariant atomistic models. The present approach synthesizes these strands by learning analytic matrix functions\u2014implemented via resolvent expansions\u2014to parameterize non-local interactions in a way that preserves permutation and rotational equivariance and scales linearly, directly addressing oversquashing while complementing strong local equivariant representations.",
  "target_paper": {
    "title": "Equivariant Matrix Function Neural Networks",
    "authors": "Ilyes Batatia, Lars Leon Schaaf, Gabor Csanyi, Christoph Ortner, Felix Andreas Faber",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "equivariance, graph neural networks, long range",
    "abstract": "Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials.\nAlthough Spectral GNNs and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack extensivity, adaptability, generalizability, computational efficiency, or fail to capture detailed structural relationships or symmetries in the data. To address these concerns, we introduce Matrix Function Neural Networks (MFNs), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size.\nThe MFN architecture achieves state-of-the-art performance in standa",
    "openreview_id": "yrgQdA5NkI",
    "forum_id": "yrgQdA5NkI"
  },
  "analysis_timestamp": "2026-01-06T18:02:49.176326"
}
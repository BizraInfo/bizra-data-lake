{
  "prior_works": [
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy",
      "year": 2021,
      "role": "Foundational analysis of intrinsic knowledge storage",
      "relationship_sentence": "UniKE\u2019s view of intrinsic model knowledge as vectorized key\u2013value memories directly builds on Geva et al.\u2019s finding that transformer MLPs implement key\u2013value memory, enabling UniKE to cast weight-resident facts in a memory framework aligned with its unified paradigm."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME)",
      "authors": "Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",
      "year": 2022,
      "role": "Targeted intrinsic knowledge editing",
      "relationship_sentence": "ROME\u2019s mechanism for pinpointing and altering subject\u2013relation associations in specific MLP layers informs UniKE\u2019s intrinsic editing pathway and its separation of semantic carriers (keys) from factual assertions (values)."
    },
    {
      "title": "Mass-Editing Memory in a Transformer (MEMIT)",
      "authors": "Kevin Meng, Adam Dziedzic, David Bau, Jacob Andreas",
      "year": 2023,
      "role": "Scalable intrinsic memory injection",
      "relationship_sentence": "MEMIT\u2019s framing of edits as memory injections across multiple sites motivates UniKE\u2019s representation of intrinsic edits as coherent key\u2013value updates and its aim for reliable, multi-fact editing behavior."
    },
    {
      "title": "MEND: Fast Model Editing at Scale",
      "authors": "Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, Chelsea Finn",
      "year": 2022,
      "role": "Learning to edit with locality and reliability",
      "relationship_sentence": "MEND\u2019s emphasis on balancing locality, reliability, and generality in intrinsic edits shapes UniKE\u2019s training objectives for stable edits while maintaining unaffected knowledge, particularly in the multimodal setting."
    },
    {
      "title": "SERAC: Memory-based Model Editing at Scale",
      "authors": "Eric Mitchell et al.",
      "year": 2022,
      "role": "External memory resorting for edits",
      "relationship_sentence": "SERAC\u2019s gating to an external datastore for edited facts directly inspires UniKE\u2019s external knowledge resorting track and its treatment of external edits as key\u2013value memory lookups coordinated with intrinsic knowledge."
    },
    {
      "title": "Generalization through Memorization: Nearest Neighbor Language Models (kNN-LM)",
      "authors": "Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer",
      "year": 2020,
      "role": "Vector-store external memory overlay",
      "relationship_sentence": "kNN-LM\u2019s vectorized datastore of (key, value) pairs underpins UniKE\u2019s formulation of external knowledge as a vector memory operating at the same semantic level as internal representations."
    },
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP (RAG)",
      "authors": "Patrick Lewis et al.",
      "year": 2020,
      "role": "Retrieval-augmented external knowledge integration",
      "relationship_sentence": "RAG\u2019s paradigm of augmenting generation with retrieved evidence informs UniKE\u2019s external resorting mechanism and its coordination with intrinsic knowledge to improve factuality without global model drift."
    }
  ],
  "synthesis_narrative": "UniKE\u2019s core contribution is to unify intrinsic knowledge editing and external knowledge resorting for multimodal LLMs by casting both as operations over vectorized key\u2013value memories, and by disentangling semantic carriers from truthfulness signals to promote collaborative knowledge use. This perspective is grounded in Geva et al., who showed that transformer feed-forward layers function as key\u2013value memories\u2014an insight that lets UniKE model in-weight facts as structured memories. Building on this, ROME operationalizes precise intrinsic edits by manipulating subject\u2013relation associations at targeted MLP sites, providing a template for UniKE\u2019s intrinsic pathway and for separating semantic keys (entity/subject representations) from factual values (relations/truths). MEMIT extends this view to scalable, multi-fact memory injection, directly informing UniKE\u2019s goal of reliable, batchable intrinsic edits.\nAt the same time, UniKE\u2019s external resorting path draws on retrieval-based overlays. kNN-LM introduced a vector datastore that plugs into generation as a key\u2013value lookup, while RAG formalized end-to-end retrieval-augmented generation\u2014both shaping UniKE\u2019s representation of external knowledge as vector memories aligned with internal semantics. SERAC specifically frames edited knowledge as an external memory with gating to ensure locality and reliability, which UniKE adapts in a multimodal context to decide when to rely on external versus intrinsic stores. Finally, MEND informs UniKE\u2019s training and evaluation priorities\u2014preserving locality and generality\u2014while UniKE extends these principles across modalities and introduces an explicit semantic-versus-truthfulness disentanglement to coordinate intrinsic assimilation and external accommodation at the same semantic levels.",
  "analysis_timestamp": "2026-01-07T00:02:04.767992"
}
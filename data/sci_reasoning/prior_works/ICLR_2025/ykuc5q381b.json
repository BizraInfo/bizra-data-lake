{
  "prior_works": [
    {
      "title": "BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models",
      "authors": "Nandan Thakur et al.",
      "year": 2021,
      "arxiv_id": "2104.08663",
      "role": "Gap Identification",
      "relationship_sentence": "BEIR established the dominant zero-shot IR evaluation suite largely populated by information-seeking queries, whose lack of reasoning-heavy retrieval is the explicit gap BRIGHT targets."
    },
    {
      "title": "MS MARCO: A Human Generated Machine Reading Comprehension Dataset",
      "authors": "Tri Nguyen et al.",
      "year": 2016,
      "arxiv_id": "1611.09268",
      "role": "Gap Identification",
      "relationship_sentence": "MS MARCO\u2019s search-log\u2013derived queries epitomize cases where lexical or semantic matching is sufficient, motivating BRIGHT\u2019s focus on queries where identifying relevant documents requires logical and multi-step reasoning."
    },
    {
      "title": "MTEB: Massive Text Embedding Benchmark",
      "authors": "Niklas Muennighoff et al.",
      "year": 2023,
      "arxiv_id": "2210.07316",
      "role": "Foundation",
      "relationship_sentence": "MTEB provides the embedding-model leaderboard and evaluation protocols that BRIGHT stress-tests, and BRIGHT explicitly measures leading MTEB models on reasoning-centric retrieval queries."
    },
    {
      "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
      "authors": "Zhilin Yang et al.",
      "year": 2018,
      "arxiv_id": "1809.09600",
      "role": "Foundation",
      "relationship_sentence": "HotpotQA introduced multi-hop questions requiring chaining evidence across documents, providing the key insight that finding relevant support can itself demand reasoning beyond single-hop similarity matching."
    },
    {
      "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval",
      "authors": "Wenhan Xiong et al.",
      "year": 2020,
      "arxiv_id": "2009.12756",
      "role": "Related Problem",
      "relationship_sentence": "This work operationalized reasoning at the retrieval stage via multi-hop dense retrieval paths, directly highlighting the need for a benchmark that isolates and evaluates reasoning in retrieval itself."
    },
    {
      "title": "KILT: A Benchmark for Knowledge Intensive Language Tasks",
      "authors": "Fabio Petroni et al.",
      "year": 2021,
      "arxiv_id": "2009.02252",
      "role": "Foundation",
      "relationship_sentence": "KILT unified knowledge-intensive tasks under shared retrieval corpora and metrics, a framework BRIGHT adopts conceptually while refocusing strictly on retrieval difficulty driven by reasoning."
    }
  ],
  "synthesis_narrative": "Existing IR evaluations have been shaped by datasets like MS MARCO, whose search-log\u2013based queries generally reward lexical or straightforward semantic matching, and by BEIR\u2019s heterogeneous zero-shot suite that aggregates mostly information-seeking tasks. MTEB extended this landscape with standardized evaluation of text embeddings across many retrieval tasks, establishing widely used baselines and leaderboards. In contrast, multi-hop QA efforts such as HotpotQA showed that answering complex questions requires chaining evidence across documents, implying that the act of identifying relevant evidence can demand reasoning. Methodologically, Xiong et al. formalized this idea by modeling multi-hop dense retrieval paths, making retrieval itself an iterative reasoning process. Complementing these trends, KILT unified knowledge-intensive tasks and metrics around a shared retrieval corpus, encouraging careful, comparable evaluation of retrieval components across tasks. Together, these works established strong baselines and evaluation practices while implicitly assuming that most retrieval benchmarks center on information-seeking queries where surface-form or single-hop semantic similarity suffices. Yet the multi-hop QA literature revealed that retrieval can be the reasoning bottleneck, and MTEB\u2019s embedding models\u2014optimized for broad coverage\u2014were never stress-tested on retrieval tasks that truly require logical or procedural inference. This confluence created a clear opportunity: construct a retrieval-only benchmark composed of real-world, cross-domain queries where relevance hinges on multi-step, logic-aware understanding, evaluated with standardized IR protocols but designed to expose the limits of state-of-the-art embedding retrievers.",
  "target_paper": {
    "title": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval",
    "authors": "Hongjin SU, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Liu Haisu, Quan Shi, Zachary S Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O Arik, Danqi Chen, Tao Yu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Retrieval benchmark, Reasoning",
    "abstract": "Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. Our dataset consists of 1,398 real-world queries spanning diverse domains such as economics, psychology, mathematics, coding, and more. These queries are drawn from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard (M",
    "openreview_id": "ykuc5q381b",
    "forum_id": "ykuc5q381b"
  },
  "analysis_timestamp": "2026-01-06T08:57:01.063795"
}
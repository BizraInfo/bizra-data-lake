{
  "prior_works": [
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Koh et al.",
      "year": 2017,
      "arxiv_id": "1703.04730",
      "role": "Foundation",
      "relationship_sentence": "This work provides the core mechanism\u2014per-example influence estimation via (approximate) inverse-Hessian\u2013vector products for (locally) convex objectives\u2014that the paper adapts to quantify how individual training points affect a chosen evaluation function and to interpret impacts in feature space."
    },
    {
      "title": "GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning",
      "authors": "Killamsetty et al.",
      "year": 2021,
      "arxiv_id": "unknown",
      "role": "Extension",
      "relationship_sentence": "GLISTER\u2019s bilevel, influence-based subset selection to optimize validation performance is directly generalized here to select data using influence signals targeted at functions beyond accuracy (fairness and robustness) and augmented with feature-space interpretability."
    },
    {
      "title": "Data Shapley: Towards Equitable Valuation of Data for Machine Learning",
      "authors": "Ghorbani et al.",
      "year": 2019,
      "arxiv_id": "1904.02868",
      "role": "Foundation",
      "relationship_sentence": "By framing data valuation as measuring each point\u2019s marginal contribution to model utility, this work motivates the paper\u2019s objective of scoring training examples for their benefit, while the paper addresses Data Shapley\u2019s computational cost by replacing Shapley values with tractable influence estimates."
    },
    {
      "title": "Estimating Training Data Influence by Tracing Gradient Descent",
      "authors": "Pruthi et al.",
      "year": 2020,
      "arxiv_id": "2002.08484",
      "role": "Inspiration",
      "relationship_sentence": "TracIn\u2019s scalable gradient-path influence estimator informs the paper\u2019s use of practical influence estimation to score data and serves as a direct comparator/alternative within the proposed influence-based selection framework."
    },
    {
      "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Group Loss",
      "authors": "Sagawa et al.",
      "year": 2020,
      "arxiv_id": "1911.08731",
      "role": "Baseline",
      "relationship_sentence": "GroupDRO is a primary robustness/fairness baseline whose limitation\u2014needing group labels and modifying the training objective\u2014is addressed by the paper\u2019s influence-driven data selection that targets worst-group metrics without changing the loss."
    },
    {
      "title": "FairBatch: Batch Selection for Model Fairness",
      "authors": "Roh et al.",
      "year": 2021,
      "arxiv_id": "unknown",
      "role": "Related Problem",
      "relationship_sentence": "FairBatch\u2019s idea of fairness-aware data selection for training motivates the paper\u2019s broader, influence-based selection that directly optimizes fairness metrics and provides feature-space explanations rather than heuristic batch composition."
    }
  ],
  "synthesis_narrative": "Influence functions were adapted to modern machine learning by Koh and Liang, who showed how to estimate the effect of a single training point on a model\u2019s parameters and predictions via inverse-Hessian\u2013vector products under convexity assumptions; this provided a principled route from data to measured performance impact. GLISTER operationalized this influence signal for data subset selection, formulating a bilevel objective that greedily chooses training examples to maximize validation performance using first-order approximations. Data Shapley defined the goal of valuing individual datapoints by their marginal contribution to a target utility, but incurred prohibitive computational cost, highlighting the need for tractable proxies. TracIn introduced a scalable alternative by tracing gradients along the training trajectory to estimate point-wise influence, demonstrating that influence can guide practical data curation. For distributional robustness and fairness, GroupDRO reweighted objectives to protect worst-group performance but relied on group labels and specialized optimization, while FairBatch steered batch composition toward fairness, indicating that data selection itself can move fairness metrics without architectural changes. Collectively, these works exposed a clear opportunity: unify the valuation perspective with efficient influence estimation to select training data that optimizes arbitrary evaluation functions. The paper synthesizes this by adapting influence estimation for convex (or convex-surrogate) learners to score examples against utility, fairness, and robustness metrics, and by interpreting influence in feature space to explain and drive selection, thus providing a general, interpretable, and efficient data-centric route to improved performance.",
  "target_paper": {
    "title": "\"What Data Benefits My Classifier?\" Enhancing Model Performance and Interpretability through Influence-Based Data Selection",
    "authors": "Anshuman Chhabra, Peizhao Li, Prasant Mohapatra, Hongfu Liu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Data Selection, Interpretability, Fairness, Robustness",
    "abstract": "Classification models are ubiquitously deployed in society and necessitate high utility, fairness, and robustness performance. Current research efforts mainly focus on improving model architectures and learning algorithms on fixed datasets to achieve this goal. In contrast, in this paper, we address an orthogonal yet crucial problem: given a fixed convex learning model (or a convex surrogate for a non-convex model) and a function of interest, we assess what data benefits the model by interpreting the feature space, and then aim to improve performance as measured by this function. To this end, we propose the use of influence estimation models for interpreting the classifier's performance from the perspective of the data feature space. Additionally, we propose data selection approaches based on influence that enhance model utility, fairness, and robustness. Through extensive experiments on synthetic and real-world datasets, we validate and demonstrate the effectiveness of our approaches ",
    "openreview_id": "HE9eUQlAvo",
    "forum_id": "HE9eUQlAvo"
  },
  "analysis_timestamp": "2026-01-06T10:39:01.210800"
}
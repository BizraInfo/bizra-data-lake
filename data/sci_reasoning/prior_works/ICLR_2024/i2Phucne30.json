{
  "prior_works": [
    {
      "title": "Neural Networks and the Bias/Variance Dilemma",
      "authors": "Stuart Geman et al.",
      "year": 1992,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized the classical bias\u2013variance trade-off that the present work directly re-examines by shifting from aggregate trade-off to sample-level behavior and revealing alignment instead of opposition."
    },
    {
      "title": "A Unified Bias-Variance Decomposition for Zero-One and Squared Loss",
      "authors": "Pedro Domingos",
      "year": 2000,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "It provides the classification-oriented bias\u2013variance decomposition and sample-wise definitions that underlie how this work measures and interprets per-sample bias and variance in deep classifiers."
    },
    {
      "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "authors": "Balaji Lakshminarayanan et al.",
      "year": 2017,
      "arxiv_id": "1612.01474",
      "role": "Foundation",
      "relationship_sentence": "Deep ensembles supply the concrete training-and-aggregation protocol used here to obtain independent model predictions so that per-sample predictive variance can be reliably estimated to test the alignment phenomenon."
    },
    {
      "title": "On Calibration of Modern Neural Networks",
      "authors": "Chuan Guo et al.",
      "year": 2017,
      "arxiv_id": "1706.04599",
      "role": "Inspiration",
      "relationship_sentence": "Its calibration framework and temperature scaling motivate the paper\u2019s first theoretical explanation, where under well-calibrated probabilities the squared bias is shown to align with the variance at the sample level."
    },
    {
      "title": "Prevalence of Neural Collapse in the Interpolating Regime of Deep Learning",
      "authors": "Vardan Papyan et al.",
      "year": 2020,
      "arxiv_id": "2004.13665",
      "role": "Inspiration",
      "relationship_sentence": "The neural collapse geometry of class means and classifier weights is the structural assumption leveraged to derive the paper\u2019s second theoretical account linking bias and variance through the collapsed feature/logit configuration."
    },
    {
      "title": "Deep Double Descent: Where Bigger Models and More Data Hurt",
      "authors": "Preetum Nakkiran et al.",
      "year": 2020,
      "arxiv_id": "1912.02292",
      "role": "Gap Identification",
      "relationship_sentence": "By showing that the traditional bias\u2013variance trade-off breaks at the aggregate level in modern deep learning, this work motivates the paper\u2019s shift to a sample-level analysis that uncovers bias\u2013variance alignment."
    },
    {
      "title": "Neural Network Ensembles, Cross Validation, and Active Learning",
      "authors": "Anders Krogh et al.",
      "year": 1995,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Its ambiguity (diversity) decomposition for ensembles directly informs using inter-model disagreement as a measurable variance proxy, which this paper exploits to empirically probe sample-wise alignment."
    }
  ],
  "synthesis_narrative": "Classical work on the bias\u2013variance dilemma established that model generalization error decomposes into opposing bias and variance components, setting the conceptual foundation for measuring and interpreting these quantities in supervised learning. Subsequent advances extended the decomposition to classification and clarified sample-wise definitions under zero-one and squared losses, making per-example bias and variance operationally accessible. Ensemble theory connected generalization to diversity via the ambiguity decomposition, establishing that disagreement across independently trained models is an informative variance proxy. In parallel, deep ensembles provided a practical, scalable method to train independent predictors and quantify predictive variability per input, enabling robust empirical estimation of sample-level variance in modern deep networks. On the probabilistic side, calibration studies formalized when predicted confidences match empirical correctness and introduced simple fixes like temperature scaling, offering conditions under which probabilistic errors admit clean structure. Finally, neural collapse revealed a striking late-training geometry of features and classifier weights, providing a concrete structural model of deep classifier logits and within-class variability.\nTogether, the breakdown of aggregate bias\u2013variance trade-offs in modern practice highlighted a gap: prior analyses did not explain sample-level behavior in deep ensembles. The confluence of reliable variance estimation from deep ensembles, calibration principles that tie predicted probabilities to outcomes, and the neural collapse geometry suggested a natural next step\u2014probe per-sample bias and variance and seek structural relations. Building directly on these ingredients, the paper uncovers and theoretically rationalizes a bias\u2013variance alignment at the sample level in deep classification models.",
  "target_paper": {
    "title": "On Bias-Variance Alignment in Deep Models",
    "authors": "Lin Chen, Michal Lukasik, Wittawat Jitkrittum, Chong You, Sanjiv Kumar",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "bias-variance decomposition, ensemble, deep learning",
    "abstract": "Classical wisdom in machine learning holds that the generalization error can be decomposed into bias and variance, and these two terms exhibit a \\emph{trade-off}. However, in this paper, we show that for an ensemble of deep learning based classification models, bias and variance are \\emph{aligned} at a sample level, where squared bias is approximately \\emph{equal} to variance for correctly classified sample points. We present empirical evidence confirming this phenomenon in a variety of deep learning models and datasets. Moreover, we study this phenomenon from two theoretical perspectives: calibration and neural collapse. We first show theoretically that under the assumption that the models are well calibrated, we can observe the bias-variance alignment. Second, starting from the picture provided by the neural collapse theory, we show an approximate correlation between bias and variance.",
    "openreview_id": "i2Phucne30",
    "forum_id": "i2Phucne30"
  },
  "analysis_timestamp": "2026-01-06T09:24:34.834543"
}
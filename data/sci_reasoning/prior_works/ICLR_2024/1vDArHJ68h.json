{
  "prior_works": [
    {
      "title": "Mastering Diverse Domains via World Models (DreamerV3)",
      "authors": "Danijar Hafner et al.",
      "year": 2023,
      "arxiv_id": "2301.04104",
      "role": "Baseline",
      "relationship_sentence": "R2I adopts the DreamerV3 world-model training pipeline and objectives as its main baseline and directly addresses DreamerV3\u2019s weakness on long-term dependencies by augmenting the RSSM dynamics with an SSM-based recall mechanism."
    },
    {
      "title": "Learning Latent Dynamics for Planning (PlaNet)",
      "authors": "Danijar Hafner et al.",
      "year": 2019,
      "arxiv_id": "1811.04551",
      "role": "Foundation",
      "relationship_sentence": "R2I builds on PlaNet\u2019s recurrent state-space model (RSSM) formulation for latent imagination, replacing the GRU-based latent dynamics with state space models to better preserve information across long horizons."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces (S4)",
      "authors": "Albert Gu et al.",
      "year": 2022,
      "arxiv_id": "2111.00396",
      "role": "Extension",
      "relationship_sentence": "R2I instantiates its memory-capable world-model dynamics using SSM layers derived from S4, leveraging their long-range recurrence and scan efficiency to extend the temporal context used for imagination and control."
    },
    {
      "title": "RUDDER: Return Decomposition for Delayed Rewards",
      "authors": "Jos\u00e9 A. Arjona-Medina et al.",
      "year": 2019,
      "arxiv_id": "1806.07857",
      "role": "Gap Identification",
      "relationship_sentence": "By highlighting the challenge of long-horizon credit assignment with delayed rewards, RUDDER motivates R2I\u2019s design to propagate value/gradient information to distant latent states via recall within a world model rather than return decomposition."
    },
    {
      "title": "Unsupervised Predictive Memory in a Goal-Directed Agent (MERLIN)",
      "authors": "Greg Wayne et al.",
      "year": 2018,
      "arxiv_id": "1803.10760",
      "role": "Inspiration",
      "relationship_sentence": "MERLIN\u2019s finding that predictive memory retrieval improves performance under partial observability directly inspires R2I\u2019s recall-to-imagine mechanism that brings distant observations into latent rollouts."
    },
    {
      "title": "Stabilizing Transformers for Reinforcement Learning (GTrXL)",
      "authors": "Emilio Parisotto et al.",
      "year": 2020,
      "arxiv_id": "1910.06764",
      "role": "Related Problem",
      "relationship_sentence": "GTrXL shows that strengthening sequence models improves long-range memory and credit assignment in RL, informing R2I\u2019s choice to replace GRU dynamics with a stronger long-context sequence model (SSM) inside a world model."
    }
  ],
  "synthesis_narrative": "PlaNet introduced a recurrent state-space model that learns latent dynamics for imagination-based control, using GRU-augmented stochastic latents to plan in a compact space. DreamerV3 refined this world-model paradigm into a scalable, general recipe that achieves strong performance across domains, but its GRU-based dynamics remain limited on long-term dependencies. In parallel, Structured State Space models (S4) showed that linear state-space recursions with learned kernels can retain information over very long sequences while remaining training- and hardware-efficient via parallel scan. MERLIN demonstrated that predictive memory and targeted retrieval of past observations can be crucial in partially observable settings, empirically validating the importance of recall mechanisms for decision making. RUDDER squarely framed the difficulty of long-horizon credit assignment with delayed rewards and proposed return decomposition to address it, underscoring the need to propagate learning signals to events far in the past. GTrXL established that upgrading the sequence model itself can stabilize and dramatically extend temporal credit in RL.\nBuilding on these ideas, the next step is to inject a long-memory sequence model directly into the latent dynamics of a Dreamer-style world model, and to marry it with an explicit recall mechanism so imagination can condition on distant, task-relevant observations. This synthesis addresses the observed limits of GRU-based RSSMs, leverages SSMs\u2019 long-range memory and efficiency, and routes value information back across long gaps\u2014thereby unifying memory and credit assignment within model-based RL.",
  "target_paper": {
    "title": "Mastering Memory Tasks with World Models",
    "authors": "Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, Sarath Chandar",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "model-based reinforcement learning, state space models, memory in reinforcement learning",
    "abstract": "Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggesting the generality of our method. We also show that",
    "openreview_id": "1vDArHJ68h",
    "forum_id": "1vDArHJ68h"
  },
  "analysis_timestamp": "2026-01-06T17:11:06.691975"
}
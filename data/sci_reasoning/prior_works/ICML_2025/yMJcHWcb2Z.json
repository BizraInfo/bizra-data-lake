{
  "prior_works": [
    {
      "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
      "authors": "Karen Simonyan and Andrew Zisserman",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "The core idea that motion is a distinct signal from appearance directly motivates VideoJAM\u2019s training objective to learn a single representation that must explain both pixels (appearance) and their motion (e.g., flow), generalizing two-stream principles to the generative setting."
    },
    {
      "title": "MoCoGAN: Decomposing Motion and Content for Video Generation",
      "authors": "Sergey Tulyakov et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "MoCoGAN\u2019s explicit separation of motion and content established that treating motion as a first-class factor improves video realism; VideoJAM adopts this insight but enforces a joint appearance\u2013motion latent that simultaneously predicts pixels and motion, addressing diffusion-era models\u2019 motion deficits."
    },
    {
      "title": "Video Diffusion Models",
      "authors": "Jonathan Ho et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "VDM popularized diffusion for video with a pixel reconstruction objective; VideoJAM modifies this baseline by augmenting the loss with motion prediction and adding motion-based inner guidance at sampling to fix VDM\u2019s motion-coherence weaknesses."
    },
    {
      "title": "Make-A-Video: Text-to-Video Generation without Text-Video Data",
      "authors": "Uriel Singer et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Make-A-Video leveraged strong image priors but exhibited typical shortcomings in temporal consistency and physics; VideoJAM is explicitly designed to remedy these gaps by injecting a motion prior via joint appearance\u2013motion learning and motion-steered sampling."
    },
    {
      "title": "Classifier-Free Diffusion Guidance",
      "authors": "Jonathan Ho and Tim Salimans",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "VideoJAM\u2019s Inner-Guidance extends the guidance paradigm by replacing external conditions with the model\u2019s own evolving motion prediction as a dynamic guidance signal during sampling."
    },
    {
      "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
      "authors": "Ethan Teed and Jia Deng",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "VideoJAM supervises its motion head using dense optical flow; high-fidelity RAFT flow provides the concrete motion target that enables learning the joint appearance\u2013motion representation."
    },
    {
      "title": "Stable Video Diffusion: Scaling Latent Video Diffusion Models",
      "authors": "Andreas Blattmann et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "As a strong latent video diffusion baseline trained with pixel-centric objectives, Stable Video Diffusion highlights the appearance-over-motion bias that VideoJAM counteracts via motion prediction during training and inner guidance at inference."
    }
  ],
  "synthesis_narrative": "VideoJAM\u2019s core innovation\u2014learning a joint appearance\u2013motion representation and using the model\u2019s own motion prediction as dynamic guidance\u2014emerges from two converging lineages. From video understanding, Two-Stream CNNs established motion as a distinct and essential signal, while MoCoGAN demonstrated in generative modeling that explicitly modeling motion markedly improves realism. VideoJAM synthesizes these insights but departs from explicit disentanglement: it trains a single latent to simultaneously predict pixels and their motion, compelling the representation to internalize dynamics rather than overfit to appearance.\n\nFrom diffusion-based video generation, Video Diffusion Models and Stable Video Diffusion offered powerful baselines yet exposed a central limitation: pixel reconstruction objectives bias models toward appearance fidelity at the expense of temporal coherence and physical plausibility. VideoJAM directly addresses this by adding a motion prediction head and loss, using accurate optical flow (e.g., RAFT) as supervision to instill a motion prior. At inference, VideoJAM reframes guidance\u2014traditionally external in Classifier-Free Guidance\u2014into an internal mechanism: Inner-Guidance steers sampling using the model\u2019s own evolving motion estimates, aligning denoising updates with coherent dynamics. Finally, Make-A-Video exemplified the appearance-driven bias inherited from image priors; VideoJAM targets precisely this gap, delivering a drop-in training and sampling strategy that transfers across video diffusion architectures to produce motion that is consistent, physically plausible, and temporally stable.",
  "analysis_timestamp": "2026-01-06T23:07:19.634477"
}
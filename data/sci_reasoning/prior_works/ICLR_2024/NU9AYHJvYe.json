{
  "prior_works": [
    {
      "title": "Learning a Distance Metric from Relative Comparisons",
      "authors": "Matthew Schultz and Thorsten Joachims",
      "year": 2003,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work formalized triplet-based relative-comparison supervision\u2014labels of the form d(xi,xj) < d(xi,xk)\u2014which is exactly the tuple-labeling problem whose hypothesis class the paper bounds via VC/Natarajan dimension."
    },
    {
      "title": "Adaptively Learning the Crowd Kernel",
      "authors": "Ofer Tamuz et al.",
      "year": 2011,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "By showing that Euclidean embeddings can be learned from triplet comparisons in practice via adaptive querying, this paper crystallized the metric-learning-from-triplets setting and motivated precise sample-complexity analysis for such tuple labels."
    },
    {
      "title": "Low-Dimensional Embedding Using Adaptively Selected Ordinal Data",
      "authors": "Kevin G. Jamieson and Robert D. Nowak",
      "year": 2011,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Their O(nd log n) active-query guarantees for ordinal embedding into Rd highlighted the nd dependence that the current work proves (up to logs) to be necessary and sufficient in the passive, distribution-free setting and extends to general \u2113p and tree metrics."
    },
    {
      "title": "Bounding the Vapnik\u2013Chervonenkis Dimension of Concept Classes Parameterized by Real Numbers",
      "authors": "Paul W. Goldberg and Mark R. Jerrum",
      "year": 1995,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "The general O(W log W) VC-dimension bound for classes defined by polynomial inequalities is instantiated here with W = n\u00b7d and inequalities of the form \u2225xi\u2212xj\u2225p \u2264 \u2225xi\u2212xk\u2225p to derive tight upper bounds on sample complexity."
    },
    {
      "title": "Multiclass Learnability and the ERM Principle",
      "authors": "Amit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz",
      "year": 2011,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work connects multiclass sample complexity to the Natarajan dimension, which the paper leverages by casting tuple-based distance comparisons as multiclass decisions and then bounding the corresponding Natarajan dimension."
    },
    {
      "title": "A Theory of Unsupervised Learning via Contrastive Learning",
      "authors": "Arun Saunshi, Orestis Plevrakis, Sanjeev Arora, et al.",
      "year": 2019,
      "arxiv_id": "1902.09229",
      "role": "Gap Identification",
      "relationship_sentence": "While providing theory for contrastive objectives, this line focused on view-based unsupervised settings and did not address sample complexity for recovering metric relations from labeled tuples, motivating the present distribution-free, optimal analysis."
    }
  ],
  "synthesis_narrative": "Relative-comparison supervision for metric learning was crystallized by work showing that one can learn distances from triplet labels of the form \u201cxi is closer to xj than xk.\u201d Early formulations made this precise and positioned triplet judgments as the core unit of information for representation learning. Practical systems demonstrated that Euclidean embeddings can be recovered from such triplets, especially when queries are adaptively selected, establishing that tuple-labeled metric learning is both feasible and useful. On the theoretical side, active ordinal-embedding results proved that O(nd log n) triplet queries suffice to embed n points in Rd up to similarity, strongly suggesting an intrinsic nd dependence. Independently, classical learning-theory results bounded the VC dimension of real-parameterized concept classes defined by polynomial inequalities by O(W log W), exactly the regime encountered when distance comparisons can be written as polynomial relations in the embedding parameters. Complementing this, multiclass learnability theory tied sample complexity to the Natarajan dimension, providing the vehicle to convert dimension bounds for tuple-labeling problems into distribution-free sample-complexity guarantees. Meanwhile, theory for contrastive learning primarily analyzed view-based unsupervised objectives, leaving the sample complexity of metric recovery from labeled tuples largely open. Together, these strands pointed to a gap: obtain tight, distribution-free passive sample-complexity bounds for triplet-supervised metric learning across \u2113p and structured metrics. The synthesis is to cast triplet comparisons as multiclass classification, leverage parametric VC/Natarajan dimension machinery to get sharp O(nd log n) upper bounds, and pair them with matching \u03a9(nd) lower bounds, thereby pinning down optimal rates.",
  "target_paper": {
    "title": "Optimal Sample Complexity of Contrastive Learning",
    "authors": "Noga Alon, Dmitrii Avdiukhin, Dor Elboim, Orr Fischer, Grigory Yaroslavtsev",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "learning theory, sample complexity, vc dimension, contrastive learning, metric learning",
    "abstract": "Contrastive learning is a highly successful technique for learning representations of data from labeled tuples, specifying the distance relations within the tuple. We study the sample complexity of contrastive learning, i.e. the minimum number of labeled tuples sufficient for getting high generalization accuracy. We give tight bounds on the sample complexity in a variety of settings, focusing on arbitrary distance functions,  $\\ell_p$-distances, and tree metrics. Our main result is an (almost) optimal bound on the sample complexity of learning $\\ell_p$-distances for integer $p$. For any $p \\ge 1$, we show that $\\tilde \\Theta(nd)$ labeled tuples are necessary and sufficient for learning $d$-dimensional representations of $n$-point datasets. Our results hold for an arbitrary distribution of the input samples and are based on giving the corresponding bounds on the Vapnik-Chervonenkis/Natarajan dimension of the associated problems. We further show that the theoretical bounds on sample comp",
    "openreview_id": "NU9AYHJvYe",
    "forum_id": "NU9AYHJvYe"
  },
  "analysis_timestamp": "2026-01-06T10:44:43.489822"
}
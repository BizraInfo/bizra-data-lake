{
  "prior_works": [
    {
      "title": "Learning to Prompt for Continual Learning",
      "authors": "Wang et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "PGP builds directly on L2P\u2019s instance-wise prompt retrieval/tuning to avoid task identifiers and inserts a projection step that regulates those prompt updates to prevent interference with past knowledge."
    },
    {
      "title": "DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning",
      "authors": "Zhang et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "PGP targets DualPrompt\u2019s residual prompt interference by enforcing theoretically grounded orthogonality on prompt-gradient updates to provide anti-forgetting guarantees."
    },
    {
      "title": "Visual Prompt Tuning",
      "authors": "Jia et al.",
      "year": 2022,
      "arxiv_id": "2203.12119",
      "role": "Foundation",
      "relationship_sentence": "PGP relies on VPT\u2019s mechanism that prompts are injected into ViT self-attention, enabling the derivation that making prompt-gradient updates orthogonal mitigates attention-mediated interference."
    },
    {
      "title": "Gradient Episodic Memory for Continual Learning",
      "authors": "Lopez-Paz et al.",
      "year": 2017,
      "arxiv_id": "1706.08840",
      "role": "Foundation",
      "relationship_sentence": "PGP adopts GEM\u2019s core idea of projecting current-task gradients so they do not harm past tasks, but applies the projection specifically to prompt gradients and removes task identifiers via instance-wise prompt querying."
    },
    {
      "title": "Efficient Lifelong Learning with A-GEM",
      "authors": "Chaudhry et al.",
      "year": 2019,
      "arxiv_id": "1812.01474",
      "role": "Gap Identification",
      "relationship_sentence": "PGP addresses A-GEM\u2019s reliance on episodic memory and task delineations by showing that prompt-tuning enables task-agnostic gradient projection with theoretical guarantees at the prompt level."
    },
    {
      "title": "Orthogonal Gradient Descent for Continual Learning",
      "authors": "Farajtabar et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "PGP extends OGD\u2019s orthogonality principle by deriving an explicit orthogonality condition for prompt gradients within ViT self-attention and enforcing it via SVD-based projection."
    },
    {
      "title": "OWM: Orthogonal Weight Modification to Protect Previous Knowledge in Neural Networks",
      "authors": "Zeng et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "PGP leverages OWM\u2019s insight that preserving the subspace spanned by past inputs prevents forgetting, adapting it to construct a projector in the joint input\u2013prompt space for prompt updates."
    }
  ],
  "synthesis_narrative": "Instance-wise prompting for continual learning was established by Learning to Prompt for Continual Learning, which retrieves and tunes a small prompt set per input to avoid explicit task identifiers; this highlighted that restricting updates to relevant prompts reduces interference. DualPrompt refined this idea with complementary prompt pools, yet still exhibited prompt interference without formal guarantees against forgetting. Visual Prompt Tuning formalized how learnable prompt tokens are injected into Vision Transformer self-attention, clarifying where and how prompt parameters influence representations and gradients. On the anti-forgetting side, Gradient Episodic Memory introduced projecting the current gradient to avoid increasing loss on past tasks, formulating gradient projection as a principled constraint. A-GEM made this projection efficient via averaged memory gradients, though it depended on episodic memories and task delineations. Orthogonal Gradient Descent reframed protection as enforcing orthogonality of new gradients to a subspace capturing prior tasks, while OWM showed that building projectors from input-feature subspaces can preserve previous knowledge. Bringing these threads together, the opportunity emerged to couple instance-wise prompt updates with projection-based guarantees: prompts obviate task identifiers, and projection/orthogonality offers formal anti-forgetting control. PGP synthesizes this by deriving an orthogonality condition specifically for prompt gradients in ViT self-attention and realizing it via SVD in a joint input\u2013prompt space, yielding task-agnostic, theoretically grounded prompt updates that curb forgetting while retaining the flexibility of instance-wise prompt selection.",
  "target_paper": {
    "title": "Prompt Gradient Projection for Continual Learning",
    "authors": "Jingyang Qiao, zhizhong zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yong Peng, Yuan Xie",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Continual Learning, Prompt Tuning, Gradient Projection, Anti-forgetting",
    "abstract": "Prompt-tuning has demonstrated impressive performance in continual learning by querying relevant prompts for each input instance, which can avoid the introduction of task identifier. Its forgetting is therefore reduced as this instance-wise query mechanism enables us to select and update only relevant prompts. In this paper, we further integrate prompt-tuning with gradient projection approach. Our observation is: prompt-tuning releases the necessity of task identifier for gradient projection method; and gradient projection provides theoretical guarantees against forgetting for prompt-tuning. This inspires a new prompt gradient projection approach (PGP) for continual learning. In PGP, we deduce that reaching the orthogonal condition for prompt gradient can effectively prevent forgetting via the self-attention mechanism in vision-transformer. The condition equations are then realized by conducting Singular Value Decomposition (SVD) on an element-wise sum space between input space and pro",
    "openreview_id": "EH2O3h7sBI",
    "forum_id": "EH2O3h7sBI"
  },
  "analysis_timestamp": "2026-01-06T23:47:40.594720"
}
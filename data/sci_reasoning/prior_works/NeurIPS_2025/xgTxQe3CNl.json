{
  "prior_works": [
    {
      "title": "Combining Labeled and Unlabeled Data with Co-Training",
      "authors": "Avrim Blum, Tom Mitchell",
      "year": 1998,
      "role": "Foundational multi-view learning paradigm",
      "relationship_sentence": "Introduced joint training across views and the sufficiency/independence assumptions, framing how multi-view models can inadvertently suppress view-specific information\u2014an insight the paper leverages to diagnose fitness evaluation bias arising from joint training that overemphasizes dominant views."
    },
    {
      "title": "A Co-regularization Approach to Semi-supervised Learning with Multiple Views",
      "authors": "Vikas Sindhwani, Partha Niyogi, Mikhail Belkin",
      "year": 2005,
      "role": "Core joint-training principle in multi-view classification",
      "relationship_sentence": "Co-regularization enforces agreement between views during learning; the new paper builds on this by showing such coupling can skew individual rankings in evolutionary populations and motivates decoupling/adjusting fitness to respect per-view information content."
    },
    {
      "title": "SimpleMKL",
      "authors": "Alain Rakotomamonjy, Francis R. Bach, St\u00e9phane Canu, Yves Grandvalet",
      "year": 2008,
      "role": "View-weighted fusion baseline for multi-view classification",
      "relationship_sentence": "Demonstrates principled weighting across feature spaces (views), directly inspiring the paper\u2019s perspective that fitness should account for heterogeneous, unevenly informative views rather than implicitly biasing toward the most dominant one."
    },
    {
      "title": "Deep Canonical Correlation Analysis",
      "authors": "Galen Andrew, Raman Arora, Jeff A. Bilmes, Karen Livescu",
      "year": 2013,
      "role": "Joint representation learning across views",
      "relationship_sentence": "Shows how maximizing inter-view correlation can compress view-specific signals; the present work connects this to febrile rankings in EMVC and proposes fitness designs that avoid penalizing individuals capturing complementary, less-correlated view information."
    },
    {
      "title": "Bias in error estimation when using cross-validation for model selection",
      "authors": "Saharon Rosset Varma, Raviv Simon",
      "year": 2006,
      "role": "Foundational analysis of selection/evaluation bias",
      "relationship_sentence": "Establishes that reusing validation data for selection induces optimistic bias; this directly underpins the paper\u2019s redesign of FE to separate training and evaluation signals (e.g., per-view or nested evaluation) to obtain unbiased individual rankings."
    },
    {
      "title": "On over-fitting in model selection and subsequent selection bias in performance evaluation",
      "authors": "Gavin C. Cawley, Nicola L. C. Talbot",
      "year": 2010,
      "role": "Methodological guidance for unbiased performance estimation",
      "relationship_sentence": "Highlights mechanisms by which evaluation pipelines distort comparative performance; the paper adapts these lessons to evolutionary settings, stabilizing selection pressure by correcting fitness estimates that would otherwise mislead evolution."
    },
    {
      "title": "Evolutionary Optimization in Uncertain Environments\u2014A Survey",
      "authors": "Yaochu Jin, Dirk Branke",
      "year": 2005,
      "role": "Noise-handling strategies for evolutionary algorithms",
      "relationship_sentence": "Synthesizes techniques like resampling, averaging, and uncertainty-aware selection for noisy fitness; the paper extends these ideas to the multi-view setting to mitigate fitness evaluation bias and improve reliability of evolutionary ranking."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014identifying and eliminating fitness evaluation bias (FEB) in evolutionary multi-view classification\u2014rests on two intertwined lines of prior work: multi-view joint training and unbiased/robust performance estimation in evolutionary pipelines. Classical multi-view methods such as co-training (Blum & Mitchell, 1998) and co-regularization (Sindhwani et al., 2005) established the goal of aligning or agreeing across views, while MKL (Rakotomamonjy et al., 2008) formalized adaptive view weighting. More recent representation approaches like Deep CCA (Andrew et al., 2013) maximize inter-view correlation. Collectively, these paradigms reveal a pitfall: aggressive joint training or correlation pursuit can suppress view-specific, complementary information. The present work transforms this representational observation into an evolutionary selection problem\u2014showing that joint-training-induced distortions manifest as biased fitness estimates that misrank individuals and misdirect search.\nA second pillar comes from the rigor of model evaluation. Varma & Simon (2006) and Cawley & Talbot (2010) demonstrated how selection and evaluation entanglement creates optimistic bias, motivating FE protocols that decouple training from assessment. Translating these insights to population-based search, the paper designs fitness procedures that avoid leakage and over-optimism, especially across heterogeneous views. Finally, evolutionary optimization under uncertainty (Jin & Branke, 2005) provides concrete mechanisms\u2014resampling, averaging, and uncertainty-aware selection\u2014to stabilize rankings with noisy or partial evidence. By synthesizing these strands, the paper proposes a fitness evaluation scheme that respects per-view information content and reduces selection noise, thereby correcting FEB and improving the evolutionary trajectory in multi-view classification.",
  "analysis_timestamp": "2026-01-07T00:21:32.343457"
}
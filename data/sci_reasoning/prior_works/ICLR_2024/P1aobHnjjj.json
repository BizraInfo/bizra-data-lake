{
  "prior_works": [
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe et al.",
      "year": 2014,
      "arxiv_id": "1312.6120",
      "role": "Foundation",
      "relationship_sentence": "Their singular-value\u2013mode decomposition of deep linear learning dynamics under squared loss gives the analytic scaffold for reasoning about how ranks emerge or vanish, which underlies the paper\u2019s characterization of rank-stratified minima and mode-wise transitions."
    },
    {
      "title": "Deep Learning Without Poor Local Minima",
      "authors": "Kenji Kawaguchi",
      "year": 2016,
      "arxiv_id": "1605.07110",
      "role": "Gap Identification",
      "relationship_sentence": "By proving that unregularized deep linear networks have no suboptimal local minima, this work isolates explicit regularization as the source of nontrivial rank-separated minima, directly motivating the paper\u2019s focus on the L2-regularized setting."
    },
    {
      "title": "Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization",
      "authors": "Benjamin Recht et al.",
      "year": 2010,
      "arxiv_id": "0706.4138",
      "role": "Foundation",
      "relationship_sentence": "Establishing nuclear norm as a convex surrogate for rank connects Frobenius-regularized factorizations to low-rank solutions, grounding the interpretation of different local minima as competing rank choices in the regularized deep linear objective."
    },
    {
      "title": "Global Optimality in Tensor Factorization, Deep Learning, and Beyond",
      "authors": "Benjamin D. Haeffele et al.",
      "year": 2015,
      "arxiv_id": "1506.07540",
      "role": "Foundation",
      "relationship_sentence": "They show that separable factor regularization in deep factorizations induces an (atomic) low-rank\u2013promoting penalty on the product map, providing the formal link between L2 penalties and rank strata that the absorbing sets B_r build on."
    },
    {
      "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks",
      "authors": "Suriya Gunasekar et al.",
      "year": 2018,
      "arxiv_id": "1806.00468",
      "role": "Inspiration",
      "relationship_sentence": "This work established that gradient dynamics in linear networks select solutions according to the parameterization-induced norm, highlighting the need to understand, in the L2-regularized case, how stochasticity (SGD) selects among multiple rank-differentiated minima."
    },
    {
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "authors": "Stephan Mandt et al.",
      "year": 2017,
      "arxiv_id": "1704.04289",
      "role": "Related Problem",
      "relationship_sentence": "Their SDE approximation of small\u2013step-size SGD provides the stochastic dynamical framework for analyzing transition probabilities between attraction basins, enabling the one-way (irreversible) rank-jump argument in the regularized linear setting."
    },
    {
      "title": "High-Dimensional Dynamics of Generalization Error in Neural Networks",
      "authors": "Madhur Advani et al.",
      "year": 2017,
      "arxiv_id": "1710.03667",
      "role": "Related Problem",
      "relationship_sentence": "By showing that deep linear learning proceeds via SVD-mode dynamics and that weight decay shrinks lower-variance modes first, this work supplies mode-wise intuition for progressive low-rank emergence that is formalized here as absorbing rank sets under SGD."
    }
  ],
  "synthesis_narrative": "The study of deep linear networks (DLNs) has precise mode-wise dynamics: Saxe, McClelland, and Ganguli derived exact singular-value decomposed learning trajectories under squared loss, revealing how individual modes grow or shrink with depth and initialization. Kawaguchi established that, absent explicit regularization, DLNs have no suboptimal local minima, implying that any nontrivial local structure must be induced by penalties such as weight decay. Parallel developments in low-rank modeling connected factor regularization to rank: Recht, Fazel, and Parrilo formalized the nuclear norm as a convex proxy for rank, while Haeffele and Vidal showed that separable factor regularization in deep factorizations induces atomic, low-rank\u2013promoting penalties on the product map. Implicit-bias work by Gunasekar and collaborators demonstrated that gradient dynamics in linear networks select solutions according to parameterization-defined norms, making clear that the geometry of the regularizer and parameterization dictates which solutions are preferred. On the stochastic side, Mandt, Hoffman, and Blei modeled small\u2013step-size SGD as an SDE, enabling analysis of basin-hopping probabilities, and Advani and colleagues described how weight decay and dynamics shrink or suppress certain SVD modes, providing intuition for progressive low-rank structure. Together, these results expose an opportunity: L2-regularized DLNs possess rank-stratified minima grounded in atomic/nuclear-norm geometry, GD\u2019s implicit bias alone cannot explain selection among them, and SGD\u2019s stochastic dynamics near these structured basins remain under-theorized. The present work synthesizes mode-wise DLN dynamics with the induced low-rank penalty and an SDE perspective on SGD to show a directional, probabilistic selection mechanism: stochastic updates can trigger rank-decreasing transitions with positive probability, while rank-increasing moves are absorbing-barred, yielding one-way jumps to lower-rank minima.",
  "target_paper": {
    "title": "Implicit bias of SGD in $L_2$-regularized linear DNNs: One-way jumps from high to low rank",
    "authors": "Zihan Wang, Arthur Jacot",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "implicit bias, SGD, low-rank, linear networks",
    "abstract": "The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with\nmore than one hidden layers has multiple local minima, corresponding\nto matrices with different ranks. In tasks such as matrix completion,\nthe goal is to converge to the local minimum with the smallest rank\nthat still fits the training data. While rank-underestimating minima\ncan be avoided since they do not fit the data, GD might get\nstuck at rank-overestimating minima. We show that with SGD, there is always a probability to jump\nfrom a higher rank minimum to a lower rank one, but the probability\nof jumping back is zero. More precisely, we define a sequence of sets\n$B_{1}\\subset B_{2}\\subset\\cdots\\subset B_{R}$ so that $B_{r}$\ncontains all minima of rank $r$ or less (and not more) that are absorbing\nfor small enough ridge parameters $\\lambda$ and learning rates $\\eta$:\nSGD has prob. 0 of leaving $B_{r}$, and from any starting point there\nis a non-zero prob. for SGD to go in $B_{r}$.",
    "openreview_id": "P1aobHnjjj",
    "forum_id": "P1aobHnjjj"
  },
  "analysis_timestamp": "2026-01-06T14:33:15.228095"
}
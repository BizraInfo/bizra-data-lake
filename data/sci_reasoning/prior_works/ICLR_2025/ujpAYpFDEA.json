{
  "prior_works": [
    {
      "title": "A Watermark for Large Language Models",
      "authors": "Kirchenbauer et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This keyed green/red token-binning watermark introduces position-wise, key-dependent token biases that Water-Probe explicitly exploits by crafting prompts to surface the consistent bias patterns induced by a fixed key."
    },
    {
      "title": "SynthID-Text: Watermarking for Large Language Models",
      "authors": "DeepMind/Google et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "SynthID-Text\u2019s binning-and-bias framework with a secret key is a primary baseline whose assumed imperceptibility Water-Probe challenges by showing users can identify the watermark via behavioral probes."
    },
    {
      "title": "SemStamp: A Semantic Watermark for LLM-Generated Text",
      "authors": "Zhang et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "SemStamp\u2019s key-conditioned semantic constraints produce stable, cross-prompt generation biases that Water-Probe targets to test whether such semantic watermarks can be detected by end users through crafted prompts."
    },
    {
      "title": "Robust Watermarking for AI-Generated Text",
      "authors": "Kuditipudi et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "By strengthening watermark-induced biases to improve robustness under edits, this line of work implicitly increases the consistency signal across prompts that Water-Probe leverages to identify the presence of a watermark."
    },
    {
      "title": "Adversarial Attacks on Text Watermarking",
      "authors": "Carlini et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "While focusing on robustness attacks and evasion, this work highlights that prior research overlooked imperceptibility under black-box user interaction\u2014a gap Water-Probe directly addresses by testing if users can detect watermarking via prompts."
    },
    {
      "title": "Fingerprinting Large Language Models via Black-Box Queries",
      "authors": "Krishna et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates that consistent, model-specific output biases can be surfaced with carefully designed prompts, inspiring Water-Probe\u2019s use of crafted queries to expose key-induced behavioral regularities of watermarked LLMs."
    }
  ],
  "synthesis_narrative": "Keyed watermarking for text generation established that a secret seed can partition the vocabulary into preferred and disfavored bins at each position, gently biasing sampling without noticeably harming fluency; this idea, introduced in A Watermark for Large Language Models, grounded the prevailing assumption that such perturbations are practically imperceptible. SynthID-Text generalized the same binning-and-bias principle into a production-ready scheme, further reinforcing the view that secret-key watermarks are stealthy in user-facing settings. Semantic variants like SemStamp shifted from purely lexical to meaning-level constraints, yet still relied on key-conditioned, repeatable tendencies in token selection or content planning. Robustness-focused work (e.g., Kuditipudi et al.) amplified the watermark signal to survive paraphrasing and edits, implicitly increasing the stability of the watermark\u2019s bias across prompts. In parallel, attacks on text watermarks by Carlini et al. concentrated on evasion and resilience, leaving the question of human- or user-side detectability largely unexamined. Separately, black-box fingerprinting research (Krishna et al.) showed that stable, model-specific biases are extractable through carefully crafted probes. Taken together, these strands suggested an untested vulnerability: if watermark schemes create consistent, key-dependent biases, then a user might reveal them without knowing the key. This paper synthesizes those insights by designing Water-Probe\u2014prompt pairs that amplify cross-key differences and same-key similarities\u2014thereby testing and ultimately challenging the imperceptibility assumption underlying modern lexical and semantic watermarks.",
  "target_paper": {
    "title": "Can Watermarked LLMs be Identified by Users via Crafted Prompts?",
    "authors": "Aiwei Liu, Sheng Guan, Yiming Liu, Leyi Pan, Yifei Zhang, Liancheng Fang, Lijie Wen, Philip S. Yu, Xuming Hu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Large Language Models, Watermark, Identification",
    "abstract": "Text watermarking for Large Language Models (LLMs) has made significant progress in detecting LLM outputs and preventing misuse. Current watermarking techniques offer high detectability, minimal impact on text quality, and robustness to text editing. \n    However, current researches lack investigation into the imperceptibility of watermarking techniques in LLM services.\n    This is crucial as LLM providers may not want to disclose the presence of watermarks in real-world scenarios, as it could reduce user willingness to use the service and make watermarks more vulnerable to attacks. This work is the first to investigate the imperceptibility of watermarked LLMs. We design an identification algorithm called Water-Probe that detects watermarks through well-designed prompts to the LLM. Our key motivation is that current watermarked LLMs expose consistent biases under the same watermark key, resulting in similar differences across prompts under different watermark keys. Experiments show tha",
    "openreview_id": "ujpAYpFDEA",
    "forum_id": "ujpAYpFDEA"
  },
  "analysis_timestamp": "2026-01-06T11:32:55.260763"
}
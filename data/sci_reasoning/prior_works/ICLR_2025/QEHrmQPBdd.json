{
  "prior_works": [
    {
      "title": "Learning to summarize with human feedback",
      "authors": "Nisan Stiennon et al.",
      "year": 2020,
      "arxiv_id": "2009.01325",
      "role": "Foundation",
      "relationship_sentence": "RM-Bench adopts the pairwise-preference reward modeling setup and the goal of aligning reward scores with downstream policy quality first operationalized in this work\u2019s RM+best-of-N/RLHF pipeline."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "arxiv_id": "2203.02155",
      "role": "Foundation",
      "relationship_sentence": "RM-Bench directly evaluates the kind of instruction-following reward models introduced here, testing whether they prefer content-correct responses over stylistically appealing but inferior ones, as required for effective RLHF and reranking."
    },
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "arxiv_id": "2204.05862",
      "role": "Foundation",
      "relationship_sentence": "This work\u2019s large-scale preference modeling and observations of reward-driven behavioral artifacts motivate RM-Bench\u2019s explicit tests for style biases that can mislead RMs despite minimal content changes."
    },
    {
      "title": "MT-Bench: Benchmarking LLMs with Multi-Turn Questions",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "arxiv_id": "2306.05685",
      "role": "Related Problem",
      "relationship_sentence": "Findings from LLM-as-a-judge evaluation (e.g., verbosity and surface-form biases) inspire RM-Bench\u2019s style-controlled A/B comparisons to ensure reward models are not swayed by superficial writing style."
    },
    {
      "title": "CheckList: A Behavioral Testing Framework for NLP",
      "authors": "Marco Tulio Ribeiro et al.",
      "year": 2020,
      "arxiv_id": "2005.04118",
      "role": "Inspiration",
      "relationship_sentence": "RM-Bench borrows CheckList\u2019s minimal-pair/contrastive-edit methodology to construct subtle content perturbations that probe whether reward models truly attend to meaning rather than artifacts."
    },
    {
      "title": "RewardBench: A Benchmark for Evaluating Reward Models",
      "authors": "First author et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "RM-Bench is proposed explicitly to address RewardBench\u2019s limitation of mainly testing RM discrimination across model-strength responses, by instead targeting sensitivity to fine-grained content and robustness to style bias with stronger correlation to policy performance."
    }
  ],
  "synthesis_narrative": "Preference-based reward models emerged as a practical way to steer language models, with early work showing how to train reward functions on human comparisons and use them for best-of-N selection or RL to improve outputs. Instruction-following RLHF further cemented the standard formulation\u2014pairwise preferences over candidate completions\u2014that contemporary reward models adopt, while exposing how these learned judges must reliably prefer substantively better answers over superficially polished ones. Large-scale helpful/harmless alignment revealed reward-driven artifacts and failure modes that can arise when preference models pick up on superficial cues, underscoring the risk of over-weighting style. In parallel, evaluation research with LLM-as-judge demonstrated pronounced biases (e.g., verbosity, position) in pairwise judgments, suggesting that any reliable evaluator must control for surface form. Methodologically, behavioral testing frameworks based on contrastive minimal pairs established how targeted edits can reveal whether models track meaning versus artifacts. Finally, early RM-specific benchmarks largely assessed discrimination across responses from models of differing strengths, providing an incomplete picture of sensitivity to subtle content changes and style confounds. Together, these strands revealed a gap: reward models\u2014the core RLHF component\u2014lacked an evaluation that directly targets fine-grained content sensitivity while resisting style bias and that meaningfully predicts downstream policy quality. RM-Bench synthesizes contrastive minimal pairs with style-controlled comparisons, evaluates reward models in the instruction-following RM setting they\u2019re trained for, and prioritizes correlation with policy performance, thus addressing the precise shortcomings exposed by prior RM and evaluator benchmarks.",
  "target_paper": {
    "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style",
    "authors": "Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Reward Models, Language Models, Evaluation, Alignment",
    "abstract": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. \nDespite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. \nHowever, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance.\nTo this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. \nExtensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively.\nWe evaluate nearly 40 reward models on RM-Bench. \nOur results reveal that even st",
    "openreview_id": "QEHrmQPBdd",
    "forum_id": "QEHrmQPBdd"
  },
  "analysis_timestamp": "2026-01-06T05:51:17.113437"
}
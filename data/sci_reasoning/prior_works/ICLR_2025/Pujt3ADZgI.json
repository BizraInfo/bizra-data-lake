{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano et al.",
      "year": 2017,
      "arxiv_id": "1706.03741",
      "role": "Foundation",
      "relationship_sentence": "Established the RLHF paradigm by fitting a Bradley\u2013Terry-style reward model from pairwise comparisons and optimizing it with RL, providing the scalar-reward, BT-based setup that this work generalizes beyond."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "arxiv_id": "2203.02155",
      "role": "Baseline",
      "relationship_sentence": "Provided the dominant BT-based RLHF pipeline for LLMs (KL-regularized PPO on a learned reward), which this paper replaces with a two-player no-regret formulation to avoid reward/win-rate estimation."
    },
    {
      "title": "Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons",
      "authors": "R. A. Bradley et al.",
      "year": 1952,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Introduced the Bradley\u2013Terry paired-comparison model that underlies most RLHF objectives; the present work explicitly departs from its transitive-utility assumption to handle general, possibly intransitive preferences."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Alexander M. Rafailov et al.",
      "year": 2023,
      "arxiv_id": "2305.18290",
      "role": "Baseline",
      "relationship_sentence": "Proposed a direct preference optimization loss derived under the BT model and a reference policy, serving as the primary baseline that this work generalizes with a new objective grounded in a Nash no-regret game."
    },
    {
      "title": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning",
      "authors": "Marc Lanctot et al.",
      "year": 2017,
      "arxiv_id": "1711.00832",
      "role": "Inspiration",
      "relationship_sentence": "Showed that self-play with no-regret/meta-solvers over a growing policy set approximates Nash equilibria, directly inspiring the idea of letting the policy play against itself to reach a Nash policy in preference games."
    },
    {
      "title": "The Multiplicative Weights Update Method: a Meta-Algorithm and Applications",
      "authors": "Sanjeev Arora et al.",
      "year": 2012,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Provided the core guarantee that no-regret dynamics in two-player zero-sum games converge to minimax/Nash solutions, which underpins the iterative no-regret updates used to approximate a Nash policy here."
    }
  ],
  "synthesis_narrative": "Preference-based learning from comparisons was crystallized by Christiano et al., who modeled pairwise choices with a Bradley\u2013Terry (BT) logistic link and trained a scalar reward that RL then optimized; this established the template of converting preferences into an individual-response utility. Ouyang et al. extended this recipe to large language models with KL-regularized PPO on a learned reward, making BT-based reward modeling the standard RLHF pipeline at scale. The statistical backbone of both is the Bradley\u2013Terry model itself, whose implicit assumption of a global scalar utility yields transitive preferences and struggles to represent cyclic or context-dependent judgments common in open-ended generation. Rafailov et al. simplified training via Direct Preference Optimization, deriving a closed-form, preference-only loss under the BT and reference-policy assumptions, but still remained within the BT/transitivity regime. In parallel, Lanctot et al. demonstrated that self-play and policy-space response oracles with no-regret meta-solvers can approximate Nash equilibria in games, suggesting a way to reason about nontransitive interactions via mixed strategies. The multiplicative weights literature formalized that no-regret dynamics in zero-sum games converge to minimax equilibria, offering algorithmic and theoretical footing for iterative play.\nTogether, these works reveal a gap: BT-based scalar rewards and their DPO-style surrogates cannot faithfully capture general, potentially intransitive preferences, while game-theoretic self-play with no-regret offers a natural machinery for such settings. The present paper synthesizes these threads by recasting RLHF as a two-player preference game and applying no-regret self-play to approximate a Nash policy, yielding a new loss directly minimized on preference data and sidestepping expensive per-response win-rate estimation.",
  "target_paper": {
    "title": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning",
    "authors": "Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, Dong Yu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "RLHF Theory, LLM Alignment",
    "abstract": "Reinforcement Learning with Human Feedback (RLHF) has achieved great success\nin aligning large language models (LLMs) with human preferences. Prevalent\nRLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human preferences. In\nthis paper, we explore RLHF under a general preference framework and approach\nit from a game-theoretic perspective. Specifically, we formulate the problem as\na two-player game and propose a novel online algorithm, iterative Nash policy\noptimization (INPO). The key idea is to let the policy play against itself via no-\nregret learning, thereby approximating the Nash policy. Unlike previous methods,\nINPO bypasses the need for estimating the expected win rate for individual responses, which typically incurs high computational or annotation costs. Instead,\nwe introduce a new loss objective that is directly minimized over a preference\ndataset. We provide theoretical analysis for our approach ",
    "openreview_id": "Pujt3ADZgI",
    "forum_id": "Pujt3ADZgI"
  },
  "analysis_timestamp": "2026-01-06T08:41:15.267397"
}
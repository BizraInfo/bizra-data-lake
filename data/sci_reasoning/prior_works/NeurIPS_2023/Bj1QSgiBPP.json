{
  "prior_works": [
    {
      "title": "Equality of Opportunity in Supervised Learning",
      "authors": "Moritz Hardt, Eric Price, Nati Srebro",
      "year": 2016,
      "role": "Foundational fairness post-processing using protected attributes at prediction time",
      "relationship_sentence": "This work established the efficacy of using protected group attributes at decision time via group-specific thresholding, directly motivating the paper\u2019s core idea of optionally leveraging group information at prediction time to improve performance while addressing consent."
    },
    {
      "title": "On Fairness and Calibration",
      "authors": "Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, Kilian Q. Weinberger",
      "year": 2017,
      "role": "Fairness\u2013calibration trade-offs and group-dependent thresholds",
      "relationship_sentence": "By showing that achieving calibration and certain fairness criteria requires group-dependent decisions, this paper provides the technical rationale for personalization with group attributes that participatory systems make opt-in and transparent."
    },
    {
      "title": "Predict Responsibly: Increasing Fairness by Learning to Defer",
      "authors": "H. Madras, E. Creager, T. Pitassi, R. Zemel",
      "year": 2018,
      "role": "Gating/deferral mechanism learning based on error differences",
      "relationship_sentence": "The learn-to-defer framework\u2014training a gate to route instances to different decision rules by estimating error gains\u2014inspires the participatory system\u2019s model-agnostic gating that decides whether using group attributes (personalization) benefits an individual."
    },
    {
      "title": "SelectiveNet: A Deep Neural Network with an Integrated Reject Option",
      "authors": "Yair Geifman, Ran El-Yaniv",
      "year": 2019,
      "role": "Selective prediction and abstention to manage accuracy\u2013coverage trade-offs",
      "relationship_sentence": "Selective prediction provides the operative paradigm for allowing a model to abstain or switch decision modes, analogous to the paper\u2019s mechanism that selectively engages personalization only when it demonstrably helps an individual."
    },
    {
      "title": "Adaptive Classification for Prediction under a Budget",
      "authors": "Feng Nan, Joseph Wang, Venkatesh Saligrama",
      "year": 2017,
      "role": "Test-time feature acquisition under costs",
      "relationship_sentence": "This work formalizes deciding at prediction time whether acquiring an extra feature is worth its cost, directly informing the paper\u2019s treatment of group membership as a costly/sensitive feature that users may opt to reveal."
    },
    {
      "title": "Metalearners for Estimating Heterogeneous Treatment Effects using Machine Learning",
      "authors": "S\u00f6ren R. K\u00fcnzel, Jasjeet S. Sekhon, Peter J. Bickel, Bin Yu",
      "year": 2019,
      "role": "Uplift/individual treatment effect estimation",
      "relationship_sentence": "Estimating individual-level gains from an intervention underpins the paper\u2019s model-agnostic estimation of the personal benefit from personalization (disclosing group attributes) to inform consent at prediction time."
    },
    {
      "title": "Assessing Algorithmic Fairness with Unobserved Protected Class",
      "authors": "Nathan Kallus, Xiaojie Mao, Angela Zhou",
      "year": 2020,
      "role": "Fairness when sensitive attributes are unavailable or partially observed",
      "relationship_sentence": "By analyzing decisions when protected attributes are unobserved and discussing proxy/imputation strategies, this work frames the alternative baselines that participatory systems compare against when users withhold group information."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central innovation is a model-agnostic, prediction-time mechanism that lets individuals opt into revealing a sensitive group attribute only when it yields demonstrable benefit\u2014thereby operationalizing consent while improving accuracy and privacy. Two streams of prior work directly converge to enable this idea. First, fairness research established that using protected attributes at decision time can be necessary to achieve desirable properties. Equality of Opportunity (Hardt et al., 2016) and On Fairness and Calibration (Pleiss et al., 2017) formalized group-dependent thresholds and trade-offs, providing the normative and technical basis for personalization with group attributes. Second, selective and deferred decision-making provided the algorithmic blueprint for when to engage personalization. Learning to Defer (Madras et al., 2018) learns a gate based on predicted error differences between experts, which maps cleanly onto deciding between a non-personalized model and a group-personalized one; SelectiveNet (Geifman & El-Yaniv, 2019) reinforces the accuracy\u2013coverage framing that underlies optional engagement. The test-time feature acquisition literature (Nan et al., 2017) contributes the notion of treating group membership as a costly/sensitive feature whose acquisition is a decision variable, aligning with the paper\u2019s opt-in mechanism. Finally, uplift/heterogeneous treatment effect estimation (K\u00fcnzel et al., 2019) supplies model-agnostic techniques to estimate the individual benefit from personalization, which the system exposes to facilitate informed consent, while work on fairness without observed demographics (Kallus et al., 2020) grounds the comparisons to imputation/proxy-based alternatives when users decline disclosure.",
  "analysis_timestamp": "2026-01-06T23:42:49.067929"
}
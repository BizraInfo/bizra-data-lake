{
  "prior_works": [
    {
      "title": "Improving generalization in reinforcement learning: The successor representation",
      "authors": "Dayan",
      "year": 1993,
      "role": "Foundational concept of the successor representation/measure used for goal-conditioned and reward-decomposed RL",
      "relationship_sentence": "The paper reinterprets Dayan\u2019s successor representation through a spectral lens, showing that low-rank structure emerges not in the raw SR but in a shifted (k-step) SR that spectrally filters early transients."
    },
    {
      "title": "Successor Features for Transfer in Reinforcement Learning",
      "authors": "Barreto et al.",
      "year": 2017,
      "role": "Establishes successor features as a linear/low-rank vehicle for transfer and goal-conditioned RL across reward functions",
      "relationship_sentence": "The authors\u2019 low-rank SF premise motivates this work\u2019s central claim: the low-rank structure underpinning SF-based transfer reliably materializes after shifting the successor measure, not before."
    },
    {
      "title": "Eigenoption Discovery through the Successor Representation",
      "authors": "Machado et al.",
      "year": 2017,
      "role": "Connects SR to spectral decompositions and controllable eigenmodes of the Markov operator",
      "relationship_sentence": "This spectral viewpoint directly informs the paper\u2019s insight that applying k-step shifts acts as a spectral filter, attenuating high-frequency components and revealing a low-rank subspace."
    },
    {
      "title": "Contextual Decision Processes with Low Bellman Rank are PAC-Learnable",
      "authors": "Jiang et al.",
      "year": 2017,
      "role": "Introduces low-rank/Bellman-rank structure as a condition enabling sample-efficient RL",
      "relationship_sentence": "The new notion of spectral recoverability provides a mechanistic justification for when low-rank assumptions (like low Bellman rank) hold\u2014specifically after shifting the dynamics."
    },
    {
      "title": "Reward-Free Exploration for Reinforcement Learning",
      "authors": "Jin et al.",
      "year": 2020,
      "role": "Reward-free frameworks that implicitly rely on estimating successor/occupancy structures for downstream tasks",
      "relationship_sentence": "The paper\u2019s finite-sample guarantees for estimating a low-rank, shifted successor measure from sampled entries supply the missing statistical backbone for reward-free pipelines that presuppose such structure."
    },
    {
      "title": "Exact Matrix Completion via Convex Optimization",
      "authors": "Cand\u00e8s and Recht",
      "year": 2009,
      "role": "Foundational theory for recovering low-rank matrices from sampled entries under coherence-type conditions",
      "relationship_sentence": "Their coherence-based recovery theory inspires the paper\u2019s spectral recoverability condition and entrywise estimation analysis for the shifted successor matrix."
    },
    {
      "title": "Log-Sobolev Inequalities for Finite Markov Chains",
      "authors": "Diaconis and Saloff-Coste",
      "year": 1996,
      "role": "Functional inequalities relating Markov chain spectral properties to mixing and concentration",
      "relationship_sentence": "Building on this tradition, the paper derives new functional inequalities to bound spectral recoverability, linking chain geometry to the recoverability of low-rank shifted SR."
    }
  ],
  "synthesis_narrative": "This work reframes the widespread low-rank assumption in successor-based RL by pinpointing where the low-rank structure truly arises: not in the raw successor measure, but in a shifted version that bypasses the first few transitions. Dayan\u2019s original successor representation and Barreto et al.\u2019s successor features established SR as a vehicle for generalization and transfer, often implicitly presuming low-rankness across goals or rewards. Machado et al.\u2019s spectral view of SR provided the key intuition that repeated application of the transition operator acts as a spectral filter; this paper operationalizes that insight via a k-step shift, which damps high-frequency modes and exposes a low-dimensional subspace.\nIn parallel, the low-rank paradigm in RL theory\u2014from low Bellman rank to linear MDPs and reward-free exploration\u2014has banked on such structure for sample efficiency. The present work supplies a principled justification: it introduces spectral recoverability to quantify when shifted SR is well-approximated by a low-rank matrix and delivers finite-sample, entrywise estimation guarantees from sampled trajectories. The statistical machinery draws on matrix completion\u2014particularly coherence-style recoverability from Cand\u00e8s and Recht\u2014while replacing generic incoherence with a chain-dependent spectral recoverability tailored to SR. Finally, by deriving new functional inequalities for Markov chains in the spirit of Diaconis and Saloff-Coste, the paper connects mixing geometry to recoverability, yielding practical bounds that explain when the low-rank premise holds after an initial shift. Collectively, these strands culminate in the shift-before-you-learn principle for representation learning in RL.",
  "analysis_timestamp": "2026-01-07T00:21:32.256304"
}
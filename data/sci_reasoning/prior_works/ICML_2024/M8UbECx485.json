{
  "prior_works": [
    {
      "title": "The Benefit of Multitask Representation Learning",
      "authors": "Andreas Maurer et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized the shared-representation multi-task setting\u2014learn a common representation and fit task-specific linear heads\u2014that the present work adopts, but only for linear feature maps."
    },
    {
      "title": "Provable Meta-Learning of Linear Representations",
      "authors": "Kshipra Tripuraneni et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "The current paper directly extends the provable guarantees for learning linear representations in multitask/meta-learning to the nonlinear two-layer ReLU setting, removing the core linearity restriction in Tripuraneni et al."
    },
    {
      "title": "A Theoretical Analysis of Contrastive Unsupervised Learning",
      "authors": "Pratiksha Saunshi et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "The key insight that multi-task pretraining induces a pseudo-contrastive objective aligns with Saunshi et al.\u2019s theoretical lens on contrastive learning, and the present paper leverages this perspective to formalize alignment of same-label pairs across tasks."
    },
    {
      "title": "Supervised Contrastive Learning",
      "authors": "Prannay Khosla et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "By interpreting across-task label agreement as implicitly defining positive pairs, this work draws directly on the supervised-contrastive idea of using labels to drive alignment in representation space."
    },
    {
      "title": "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere",
      "authors": "Ting Chen Wang and Phillip Isola",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "The paper\u2019s pseudo-contrastive interpretation explicitly exploits the alignment principle from Wang and Isola, arguing that multi-task pretraining favors aligned representations for inputs with high label-coincidence across tasks."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "M. Chizat et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Chizat et al. identified the lazy/NTK regime where features do not move; the present work addresses this gap by proving genuine feature learning in a nonlinear two-layer network trained on multiple tasks."
    },
    {
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "authors": "Song Mei et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "Mean-field analyses established that two-layer networks can learn by moving features beyond NTK, informing the current paper\u2019s nonlinear training-dynamics reasoning that underpins its multitask feature-learning result."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014proving that multi-task pretraining with a nonlinear two-layer ReLU network provably learns features\u2014sits at the intersection of multitask representation learning and contrastive learning theory. Maurer et al. laid the foundational formulation of shared-representation multitask learning, where a common feature map is learned and task-specific linear heads are fit; Tripuraneni et al. advanced this line with provable guarantees but restricted to linear representations. The present work directly extends this framework to nonlinear networks, closing the linearity gap and aligning with practice where a frozen representation is adapted by retraining the final linear layer.\nA key conceptual move is to reinterpret multi-task pretraining as optimizing a pseudo-contrastive objective: inputs that share labels across tasks are encouraged to align in the learned embedding. This builds on contrastive learning theory (Saunshi et al.) and the supervised-contrastive principle of label-defined positives (Khosla et al.), and it leverages the alignment perspective articulated by Wang and Isola to formalize why representation alignment emerges under multi-task averaging. Methodologically, the work departs from the lazy/NTK regime highlighted by Chizat et al., demonstrating genuine feature movement\u2014a central open gap in prior analyses of multitask pretraining. Complementing this, mean-field insights from Mei et al. provide conceptual grounding for tracking feature evolution in two-layer ReLU networks, supporting the paper\u2019s nonlinear training-dynamics argument. Together, these strands directly enable the paper\u2019s proof that multi-task pretraining induces contrastive-like alignment and thus provable feature learning in nonlinear networks.",
  "analysis_timestamp": "2026-01-06T23:09:26.488971"
}
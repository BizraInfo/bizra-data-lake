{
  "prior_works": [
    {
      "title": "Batched Bandit Problems",
      "authors": "Valentin Perchet, Philippe Rigollet, Sylvain Chassang, Erik Snowberg",
      "year": 2016,
      "role": "Foundational model of sampling with limited rounds (batches) over k sources",
      "relationship_sentence": "Established the paradigm and core techniques for analyzing performance degradation under a bounded number of sampling rounds, providing the round-elimination and polynomial-in-k slowdowns that directly inform the k^{\u0398(1/r)}-type tradeoffs proved for on-demand sampling in MDL."
    },
    {
      "title": "The Adaptive Complexity of Maximizing a Submodular Function",
      "authors": "Mariana Balkanski, Yaron Singer",
      "year": 2018,
      "role": "General methodology for adaptivity lower bounds via round-elimination",
      "relationship_sentence": "Introduced powerful round-elimination frameworks showing polynomial losses with few adaptive rounds, a technique mirrored in this paper\u2019s nearly tight lower bounds on round complexity in the OODS abstraction."
    },
    {
      "title": "The Reusable Holdout: Preserving Validity in Adaptive Data Analysis",
      "authors": "Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, Aaron Roth",
      "year": 2015,
      "role": "Conceptual foundation for sample\u2013adaptivity tradeoffs",
      "relationship_sentence": "Formalized how adaptivity induces additional sample requirements, motivating the explicit accounting of rounds versus samples and informing the paper\u2019s framing of sample complexity that grows with adaptivity constraints."
    },
    {
      "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization",
      "authors": "Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, Percy Liang",
      "year": 2020,
      "role": "Canonical MDL/Group-DRO objective and algorithmic template",
      "relationship_sentence": "Provides a prototypical multi-distribution (group) learning objective that relies on reweighting and iterative estimation; these procedures are captured by the OODS framework and motivate the need to understand round requirements for near-optimal sample usage."
    },
    {
      "title": "A Reductions Approach to Fair Classification",
      "authors": "Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\u00edk, John Langford, Hanna Wallach",
      "year": 2018,
      "role": "Oracle-based optimization via on-demand sampling across constraints/groups",
      "relationship_sentence": "Supplies the oracle-reduction and multiplicative-weights style blueprint\u2014sampling from targeted subpopulations/constraints to drive optimization\u2014that OODS abstracts and that underlies the paper\u2019s upper bounds and round-efficient algorithms."
    },
    {
      "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness",
      "authors": "Michael Kearns, Seth Neel, Aaron Roth, Zhiwei Steven Wu",
      "year": 2018,
      "role": "Auditing-as-optimization with adaptive sampling over many subgroups",
      "relationship_sentence": "Demonstrates iterative, on-demand sampling across many distributions (subgroups) to optimize worst-case criteria, aligning closely with MDL objectives and informing the design space OODS seeks to unify and analyze for round complexity."
    },
    {
      "title": "Learnability and the Vapnik\u2013Chervonenkis Dimension",
      "authors": "Avrim L. Blumer, Andrzej Ehrenfeucht, David Haussler, Manfred K. Warmuth",
      "year": 1989,
      "role": "Baseline sample complexity in realizable PAC learning",
      "relationship_sentence": "Provides the d/\u03b5 sample scaling in the realizable setting that the paper refines under r-round on-demand sampling, isolating how limited adaptivity introduces the additional k^{\u0398(1/r)} factor."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014tight sample\u2013round tradeoffs for on-demand sampling in multi-distribution learning (MDL) and the unifying OODS framework\u2014rests on two pillars: (i) adaptivity-limited sampling theory and (ii) oracle-based optimization procedures for MDL-style objectives. On the adaptivity side, Perchet et al. introduced batched bandits, crystallizing how restricting the number of adaptive rounds induces polynomial slowdowns in k, and providing proof techniques (e.g., batch schedules, information flow arguments) that anticipate the k^{\u0398(1/r)} dependence established here. Balkanski and Singer\u2019s round-elimination methodology generalized such insights, offering a versatile lower-bound toolkit to translate limited adaptivity into provable polynomial losses\u2014an approach the present work leverages to derive nearly tight round lower bounds within OODS. Complementing these, Dwork et al.\u2019s reusable holdout framed the general phenomenon that adaptivity consumes statistical power, shaping the paper\u2019s formalization of the sample-versus-round axis. On the MDL/optimization side, Group DRO (Sagawa et al.) and reductions-based frameworks for fairness (Agarwal et al.; Kearns et al.) supply the canonical algorithmic patterns\u2014iterative reweighting, constraint/auditor oracles, and targeted on-demand sampling across subpopulations\u2014that OODS abstracts and analyzes, yielding the \u221ak-round upper bounds for agnostic MDL. Finally, classical VC theory (Blumer et al.) anchors the realizable d/\u03b5 baseline, clarifying how limited rounds introduce the additional k^{\u0398(1/r)} factor, thereby completing the characterization of optimal sample\u2013adaptivity tradeoffs.",
  "analysis_timestamp": "2026-01-07T00:21:32.261478"
}
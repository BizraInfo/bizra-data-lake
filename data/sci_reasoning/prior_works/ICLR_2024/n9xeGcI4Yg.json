{
  "prior_works": [
    {
      "title": "Predicting pragmatic reasoning in language games",
      "authors": "Michael C. Frank and Noah D. Goodman",
      "year": 2012,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work introduced the speaker\u2013listener (signaling) framework underlying pragmatic communication, which the consensus game instantiates with an LM generator and LM-based discriminator to model decoding as cooperative signaling."
    },
    {
      "title": "Reasoning about Pragmatics with Neural Listeners and Speakers",
      "authors": "Jacob Andreas and Dan Klein",
      "year": 2016,
      "arxiv_id": "1604.00562",
      "role": "Extension",
      "relationship_sentence": "By coupling neural speakers and listeners for pragmatic generation, this paper provides the concrete neural instantiation that is directly extended to LM-based generator/discriminator roles and equilibrium-driven decoding."
    },
    {
      "title": "Quantal Response Equilibria for Normal Form Games",
      "authors": "Richard D. McKelvey and Thomas R. Palfrey",
      "year": 1995,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The use of entropy-regularized (noisy) best responses and equilibrium concepts in this work motivates the regularized equilibrium notion and smoothed dynamics used to compute the consensus game\u2019s decoding fixed point."
    },
    {
      "title": "Regret Minimization in Games with Incomplete Information",
      "authors": "Martin Zinkevich et al.",
      "year": 2007,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Counterfactual regret minimization for extensive-form games supplies the computational blueprint the paper adapts to find approximate equilibria in its sequential signaling game for decoding."
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems",
      "authors": "Karl Cobbe et al.",
      "year": 2021,
      "arxiv_id": "2110.14168",
      "role": "Inspiration",
      "relationship_sentence": "This work established the generate-then-verify paradigm where a (learned) verifier ranks candidate solutions, directly inspiring the paper\u2019s LM-as-discriminator setup while highlighting the need for a training-free, principled coupling with generation."
    },
    {
      "title": "Language Models (Mostly) Know What They Know",
      "authors": "Saurav Kadavath et al.",
      "year": 2022,
      "arxiv_id": "2207.05221",
      "role": "Foundation",
      "relationship_sentence": "Its finding that LMs can estimate their own correctness provides the discriminative scoring signal the paper formalizes as the listener\u2019s utility and reconciles with generative decoding via equilibrium search."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2023,
      "arxiv_id": "2203.11171",
      "role": "Baseline",
      "relationship_sentence": "As a primary decoding baseline that aggregates multiple generations by voting without a principled coupling to verification, it motivates and is directly improved upon by the equilibrium-ranking procedure that jointly links generation and evaluation."
    }
  ],
  "synthesis_narrative": "Work on pragmatic communication formalized language use as cooperative signaling between a speaker and listener, where utterances are chosen to shape a listener\u2019s beliefs (Frank and Goodman), and subsequent neural implementations showed how to couple trainable speakers and listeners to improve generation via pragmatic inference (Andreas and Klein). In parallel, game theory introduced smooth, entropy-regularized equilibria via quantal response (McKelvey and Palfrey) and practical equilibrium-finding in sequential, imperfect-information settings via regret minimization in extensive-form games (Zinkevich et al.), providing algorithms for computing stable profiles of interacting agents. Within language modeling, generate-then-verify pipelines demonstrated that verifiers can reliably score candidate rationales or answers (Cobbe et al.), while separate findings showed LMs can estimate their own correctness, enabling a training-free, discriminative scoring signal (Kadavath et al.). Decoding methods like self-consistency highlighted the gains from aggregating diverse generations, but operated by unweighted voting rather than principled coupling between generation and evaluation (Wang et al.). Together, these threads revealed an opportunity: cast decoding as a cooperative signaling game where a generator\u2019s utterances and a discriminator\u2019s correctness assessments influence one another. By adopting pragmatic speaker\u2013listener structure, using LM-based correctness estimates as utilities, and importing regularized equilibrium concepts and equilibrium-finding dynamics from game theory, the paper synthesizes a training-free equilibrium-ranking procedure that reconciles generative sampling and discriminative scoring into coherent, fixed-point LM predictions.",
  "target_paper": {
    "title": "The Consensus Game: Language Model Generation via Equilibrium Search",
    "authors": "Athul Paul Jacob, Yikang Shen, Gabriele Farina, Jacob Andreas",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "language models, decoding, planning, game theory",
    "abstract": "When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate answers). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game\u2014which we term the concensus game\u2014in which a generator seeks to communicate an abstract correctness parameter using natural language sentences to a discriminator. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call equilibrium-ranking. Applied to a large number of tasks (including reading comprehension, commonsense r",
    "openreview_id": "n9xeGcI4Yg",
    "forum_id": "n9xeGcI4Yg"
  },
  "analysis_timestamp": "2026-01-06T06:40:19.638946"
}
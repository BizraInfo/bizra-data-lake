{
  "prior_works": [
    {
      "title": "Random Features for Large-Scale Kernel Machines",
      "authors": "Ali Rahimi, Benjamin Recht",
      "year": 2007,
      "role": "Foundational model of random feature maps and random-feature ridge regression (RFRR)",
      "relationship_sentence": "The paper\u2019s object of study\u2014ridge regression with random features\u2014traces directly to Rahimi\u2013Recht\u2019s construction, providing the algorithmic framework whose test error the present work characterizes with a deterministic equivalent."
    },
    {
      "title": "Generalization error of random features and random kitchen sinks",
      "authors": "Song Mei, Andrea Montanari",
      "year": 2019,
      "role": "Asymptotic high-dimensional characterization of RFRR test error",
      "relationship_sentence": "This work provides precise asymptotic predictions for RFRR generalization, which the present paper extends by giving a non-asymptotic, multiplicative, dimension-free deterministic equivalent that holds beyond proportional asymptotics."
    },
    {
      "title": "High-dimensional asymptotics of prediction: Ridge regression and classification",
      "authors": "Edgar Dobriban, Stefan Wager",
      "year": 2018,
      "role": "Deterministic equivalents for prediction risk via spectral analysis in random matrix theory",
      "relationship_sentence": "The methodology and perspective of expressing ridge risk through spectral deterministic equivalents directly inspire the paper\u2019s eigenvalue-only formula for RFRR, generalized here to random features and infinite-dimensional settings with non-asymptotic guarantees."
    },
    {
      "title": "Generalization Properties of Random Features",
      "authors": "Alessandro Rudi, Lorenzo Rosasco",
      "year": 2017,
      "role": "Statistical rates and feature-budget requirements for RF approximations to kernels",
      "relationship_sentence": "Their bounds on the number of random features needed to match kernel rates under spectral assumptions are sharpened here: using the new deterministic equivalent, the paper derives tight excess-risk rates and the smallest feature count achieving minimax performance."
    },
    {
      "title": "Optimal rates for regularized least-squares algorithms",
      "authors": "A. Caponnetto, E. De Vito",
      "year": 2007,
      "role": "Minimax benchmarks for kernel ridge regression under eigen-decay and target smoothness",
      "relationship_sentence": "These minimax rates under power-law spectral and target decay provide the benchmark that the present paper attains with RFRR, enabling its claim of optimal and sharp excess-risk scaling and minimal feature requirements."
    },
    {
      "title": "Spectrum-Dependent Learning Curves in Kernel Ridge Regression",
      "authors": "Brenden Bordelon, Bilge Canatar, Cengiz Pehlevan",
      "year": 2020,
      "role": "Eigenvalue-based learning-curve formulas for kernel methods",
      "relationship_sentence": "By showing kernel generalization depends only on spectral properties and target coefficients, this work motivates the paper\u2019s eigenvalue-only deterministic equivalent for RFRR that is valid non-asymptotically and dimension-free."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a non-asymptotic, multiplicative, dimension-free deterministic equivalent for the test error of random feature ridge regression (RFRR) that depends only on the feature-map eigenvalues\u2014sits at the intersection of three influential threads. First, Rahimi and Recht inaugurated the random-features paradigm, defining the precise RFRR model whose generalization behavior is analyzed here. Second, asymptotic theories for prediction error, notably Mei and Montanari\u2019s high-dimensional analysis specific to RFRR and Dobriban and Wager\u2019s deterministic-equivalent approach for ridge with random design, established that risk can often be expressed through spectral quantities; the present work elevates this perspective by delivering a closed-form, spectrum-only approximation that holds non-asymptotically and even for infinite-dimensional features. Third, the statistical learning-theory line linking kernel spectra to optimal rates\u2014anchored by Caponnetto and De Vito\u2019s minimax benchmarks and extended to random features by Rudi and Rosasco\u2014connects eigen-decay and target smoothness to achievable excess risks and to the feature budget needed to emulate kernel performance. Building on these, the paper derives sharp scaling laws under power-law assumptions and pins down the minimal number of features required to reach the minimax rate. Complementary kernel learning-curve work (Bordelon, Canatar, Pehlevan) reinforced the eigenvalue-centric view that the paper now proves for RFRR with dimension-free, non-asymptotic guarantees.",
  "analysis_timestamp": "2026-01-06T23:42:49.032837"
}
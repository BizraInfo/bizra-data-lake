{
  "prior_works": [
    {
      "title": "Regression Quantiles",
      "authors": "Roger Koenker et al.",
      "year": 1978,
      "role": "Foundation",
      "relationship_sentence": "Introduced the quantile regression framework and the check (pinball) loss, which defines the robust risk minimized by KQR and is the statistical objective analyzed in this paper."
    },
    {
      "title": "Support Vector Quantile Regression",
      "authors": "Ichiro Takeuchi et al.",
      "year": 2006,
      "role": "Foundation",
      "relationship_sentence": "Established the RKHS-based formulation of kernel quantile regression with the pinball loss, the exact kernel learning problem that KQR-RF approximates and analyzes at scale."
    },
    {
      "title": "Estimating Conditional Quantiles with the Pinball Loss",
      "authors": "Ingo Steinwart et al.",
      "year": 2011,
      "role": "Gap Identification",
      "relationship_sentence": "Provided self-calibration inequalities and learning-theoretic tools for quantile regression in RKHS but without random features; their calibration results are exploited to connect KQR-RF to KRR-RF, addressing the open gap of scalable, optimal-rate KQR."
    },
    {
      "title": "Random Features for Large-Scale Kernel Machines",
      "authors": "Ali Rahimi et al.",
      "year": 2007,
      "role": "Foundation",
      "relationship_sentence": "Introduced the random feature approximation central to constructing KQR-RF, enabling scalable optimization of the quantile objective in high dimensions."
    },
    {
      "title": "Generalization Properties of Learning with Random Features",
      "authors": "Alessandro Rudi et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "Gave sharp excess-risk rates and RF-number conditions for kernel ridge regression with random features (KRR-RF); the present work builds a novel bridge from KQR-RF to these KRR-RF results to derive capacity-dependent optimal rates for the non-smooth pinball loss."
    },
    {
      "title": "On the Equivalence between Quadrature Rules and Random Features",
      "authors": "Francis Bach",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "Developed data-dependent (ridge-leverage) sampling strategies for random features that yield optimal learning behavior; this paper adapts such sampling to the quantile setting to attain minimax-optimal rates, including in the agnostic case."
    },
    {
      "title": "Optimal rates for the regularized least-squares algorithm",
      "authors": "Andrea Caponnetto et al.",
      "year": 2007,
      "role": "Inspiration",
      "relationship_sentence": "Established capacity/source-condition frameworks and minimax-optimal rates for kernel ridge regression that this paper targets and matches (up to logs) in the quantile setting with random features."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014deriving minimax-optimal, capacity-dependent learning rates for kernel quantile regression with random features (KQR-RF) via a refined error decomposition and a novel connection to KRR-RF\u2014rests on a tight intellectual lineage. The statistical objective originates in Koenker and Bassett\u2019s regression quantiles, whose pinball loss motivates robustness to heavy-tailed noise. Takeuchi et al. brought this objective into RKHS, defining kernel quantile regression (KQR) as the nonparametric problem the present work studies. Steinwart and Christmann then provided self-calibration inequalities and learning-theoretic tools for the pinball loss, furnishing the key mechanism to relate excess quantile risk to L2-type errors; this paper leverages these inequalities to transfer generalization guarantees from squared-loss regression to the quantile setting. On the scalability side, Rahimi and Recht introduced random features, the approximation apparatus enabling KQR at large scale. Rudi and Rosasco developed sharp excess-risk bounds and RF-count requirements for kernel ridge regression with random features (KRR-RF); the current paper explicitly builds a bridge from KQR-RF to KRR-RF to inherit and adapt these rates despite the non-smooth loss. To optimize feature sampling, Bach\u2019s leverage-score-based, data-dependent random feature distributions are adopted, allowing minimax rates with fewer features and extending naturally to the agnostic (misspecified) case. Finally, Caponnetto and De Vito\u2019s capacity/source-condition framework and minimax benchmarks set the targets that this work achieves\u2014up to logarithmic factors\u2014thereby closing a key gap between robust quantile learning and scalable random-feature theory.",
  "analysis_timestamp": "2026-01-06T23:09:26.439918"
}
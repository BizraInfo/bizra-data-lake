{
  "prior_works": [
    {
      "title": "Prioritized Experience Replay",
      "authors": "Tom Schaul et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "The paper directly replaces sampling-based PER with a parametric, generative alternative, addressing PER\u2019s tendency to overfit rare but high-priority transitions while preserving the core idea of prioritizing useful experience."
    },
    {
      "title": "Continual Learning with Deep Generative Replay",
      "authors": "Hanul Shin et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduces the notion of a parametric memory via a generative model to replay past experiences; the proposed prioritized generative replay adapts this core idea to online RL and augments it with relevance-guided sampling."
    },
    {
      "title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming (Dyna)",
      "authors": "Richard S. Sutton",
      "year": 1990,
      "role": "Foundation",
      "relationship_sentence": "Provides the foundational principle of improving sample efficiency by learning from model-generated experience; the new method instantiates this principle with a learned generative replay model instead of an explicit dynamics model."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Supplies the generative modeling machinery the paper uses to parametrize the replay distribution; the proposed approach instantiates replay as a conditional diffusion model over agent experience."
    },
    {
      "title": "Classifier-Free Diffusion Guidance",
      "authors": "Jonathan Ho et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "Inspires the paper\u2019s relevance-guided sampling: the method adapts diffusion guidance to steer generations toward high-relevance (e.g., value- or curiosity-scored) regions of the learned replay distribution."
    },
    {
      "title": "Exploration by Random Network Distillation",
      "authors": "Yuri Burda et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Provides a simple, scalable curiosity signal that the paper uses as a concrete relevance function to guide the generative replay toward informative, underexplored experiences."
    },
    {
      "title": "Dream to Control: Learning Behaviors by Latent Imagination",
      "authors": "Danijar Hafner et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrates training from imagined data in model-based RL; the proposed work differs by learning a parametric replay distribution and prioritizing generations, but draws on the same core insight that synthetic data can densify learning signals."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014replacing sampling-based replay with a prioritized, parametric generative memory\u2014emerges from a tight synthesis of ideas in replay, generative memory, model-based data generation, and diffusion guidance. Prioritized Experience Replay established that focusing updates on informative transitions can accelerate learning, but also revealed the pitfall of overfitting to rare samples. Deep Generative Replay contributed the key concept of parametric memory: using a learned generator to replay experiences, which the present work adapts from continual learning to online RL. The Dyna framework provided the foundational principle that synthetic data from a learned model can improve sample efficiency, a principle the paper reinterprets by modeling the replay distribution itself rather than environment dynamics. Modern diffusion models (DDPM) make this parametric memory practical and high-fidelity, while classifier-free guidance offers a direct mechanism to steer sampling toward desired regions\u2014here instantiated as relevance-guided generation using value or curiosity scores. Random Network Distillation supplies a plug-and-play curiosity metric that concretely operationalizes relevance in the proposed guidance family. Finally, Dreamer exemplifies the efficacy of training on imagined data, reinforcing the premise that synthetic experience can densify learning signals. Together, these works directly motivate and enable prioritized generative replay: a guided diffusion-based replay model that densifies useful experience while mitigating PER\u2019s overfitting by sampling around, rather than repeatedly reusing, rare informative transitions.",
  "analysis_timestamp": "2026-01-06T23:09:26.623116"
}
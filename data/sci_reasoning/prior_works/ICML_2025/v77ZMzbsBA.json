{
  "prior_works": [
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized training-point influence via inverse-Hessian\u2013weighted gradients; Outlier Gradient Analysis (OGA) explicitly reinterprets this influence-function objective and replaces the costly H^{-1} term with a gradient-based, Hessian-free outlier criterion."
    },
    {
      "title": "Estimating Training Data Influence by Tracing Gradient Descent (TracIn)",
      "authors": "Avinava Pruthi et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "TracIn demonstrated that influence can be approximated using sums of gradient inner products along the training trajectory; OGA builds on this gradient-only perspective and formalizes a single-run, Hessian-free outlier test in gradient space to identify detrimental samples."
    },
    {
      "title": "Representer Point Selection for Explaining Deep Neural Networks",
      "authors": "Chih-Kuan Yeh et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Representer Point Selection connects predictions to training samples through gradient-like representer values without explicit Hessians; OGA generalizes this Hessian-free, gradient-centered view from last-layer linearization to full-network per-sample gradients for detrimental-sample detection."
    },
    {
      "title": "On the Fragility of Influence Functions",
      "authors": "Basu et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "This work exposed instability and heavy computational demands of influence functions in deep networks, directly motivating OGA\u2019s reformulation that avoids inverse-Hessian computation and yields a more stable, scalable gradient-based detector."
    },
    {
      "title": "Beyond Neural Scaling Laws: Beating Power Law Scaling via Data Pruning",
      "authors": "Nadav S. Sorscher et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "By showing that early-training gradient norms (GraNd) and related metrics reveal mislabeled or unhelpful data, this paper inspired OGA\u2019s core insight that detrimental samples are outliers in gradient space, which OGA then theoretically links back to influence functions."
    },
    {
      "title": "Confident Learning: Estimating Uncertainty in Dataset Labels",
      "authors": "Curtis G. Northcutt et al.",
      "year": 2021,
      "role": "Related Problem",
      "relationship_sentence": "Confident Learning established a leading framework for mislabeled-sample detection; OGA targets the same problem but grounds detection in a principled gradient-based surrogate to influence, serving as a complementary, more efficient baseline family."
    },
    {
      "title": "An Empirical Study of Example Forgetting in Neural Networks",
      "authors": "Sofia Toneva et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "This work showed that harmful or mislabeled points exhibit distinctive training dynamics (frequent forgetting), informing OGA\u2019s premise that detrimental samples can be identified by atypical gradient behavior, which OGA formalizes and ties to influence."
    }
  ],
  "synthesis_narrative": "OGA\u2019s core innovation is to recast detrimental training-sample identification\u2014traditionally framed by influence functions\u2014into a Hessian-free outlier test in gradient space. The lineage starts with Koh and Liang (2017), who introduced influence functions to quantify how individual training points affect test predictions via an inverse-Hessian\u2013weighted gradient, but at the cost of expensive and unstable H^{-1} computations. Subsequent work sought gradient-only surrogates: TracIn (Pruthi et al., 2020) approximated influence through sums of gradient inner products along the optimization trajectory, while Representer Point Selection (Yeh et al., 2018) linked predictions to training samples through representer values derivable from gradients without explicit Hessians. Basu et al. (2021) crystallized the limitations of exact influence functions in deep networks\u2014computational fragility and instability\u2014making a strong case for a robust, scalable reformulation. In parallel, data-centric studies such as Sorscher et al. (2022) empirically showed that gradient norms early in training (GraNd) effectively flag mislabeled or unhelpful examples, suggesting that detrimental points are gradient outliers. OGA unifies these threads: it mathematically bridges the influence-function view with gradient-only criteria, yielding a simple, principled outlier-gradient analysis that retains the interpretability of influence while eliminating Hessian burdens. Finally, methods like Confident Learning (Northcutt et al., 2021) and example forgetting (Toneva et al., 2019) define practical mislabeled-sample baselines and dynamics that OGA targets, but OGA\u2019s contribution is the theoretical and algorithmic consolidation of influence and gradient outlier detection into one efficient framework.",
  "analysis_timestamp": "2026-01-06T23:07:19.571605"
}
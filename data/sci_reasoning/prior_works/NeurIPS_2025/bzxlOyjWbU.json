{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei",
      "year": 2017,
      "role": "Foundation for preference-based training",
      "relationship_sentence": "DVB directly interrogates whether models trained on human preference data (as in RLHF-style setups originating from this work) internalize underlying human values or merely fit superficial correlates present in preference datasets."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, et al.",
      "year": 2022,
      "role": "Mainstream RLHF for LLMs",
      "relationship_sentence": "Because RLHF is the prevailing method for aligning LLMs with user preferences, DVB is designed to evaluate whether such instruction-tuned models generalize principled values or overfit to shallow stylistic cues embedded in feedback data."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Ben Mann, Nova DasSarma, et al.",
      "year": 2022,
      "role": "Principle- (value-) guided alignment",
      "relationship_sentence": "DVB\u2019s focus on distinguishing deep moral principles from surface preferences aligns with CAI\u2019s shift from raw preference imitation to explicit normative principles, providing a benchmark to test whether principle-based training actually yields value-level generalization."
    },
    {
      "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference (HANS)",
      "authors": "Tom McCoy, Ellie Pavlick, Tal Linzen",
      "year": 2019,
      "role": "Benchmark design breaking heuristic correlations",
      "relationship_sentence": "DVB adapts HANS\u2019s core idea\u2014train on data where shallow heuristics correlate with correct labels, then test where correlations are broken\u2014to reveal whether models relied on shallow cues or learned the intended underlying concept (here, deep values)."
    },
    {
      "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization (Waterbirds)",
      "authors": "Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, Percy Liang",
      "year": 2020,
      "role": "Spurious correlation evaluation under group shifts",
      "relationship_sentence": "The Waterbirds setup\u2014constructing training data with correlated spurious features and measuring worst-group generalization\u2014informs DVB\u2019s methodology for confounding deep values with superficial attributes during training and decoupling them at test."
    },
    {
      "title": "Hypothesis Only Baselines in Natural Language Inference",
      "authors": "Adam Poliak, Jason Naradowsky, Tal Linzen, Benjamin Van Durme",
      "year": 2018,
      "role": "Revealing dataset artifacts and shallow shortcuts",
      "relationship_sentence": "By showing that models can perform well via superficial artifacts, this work motivates DVB\u2019s controlled confounding to detect when LLMs exploit stylistic or surface cues rather than reasoning over value content."
    },
    {
      "title": "ETHICS: A Benchmark for Ethical Reasoning",
      "authors": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt",
      "year": 2021,
      "role": "Evaluating moral/ethical reasoning in LMs",
      "relationship_sentence": "ETHICS provides task structure and value domains for assessing normative judgments; DVB extends this by explicitly disentangling moral principles from shallow features to test genuine value generalization."
    }
  ],
  "synthesis_narrative": "The Deep Value Benchmark (DVB) sits at the intersection of preference-based alignment and robustness to spurious correlations. Preference learning (Christiano et al., 2017) and its large-scale instantiation in LLMs via RLHF (Ouyang et al., 2022) provide the training paradigm DVB targets: learning from human feedback. Yet, Constitutional AI (Bai et al., 2022) highlights an aspiration to go beyond mere preference imitation toward explicit principles\u2014raising the question DVB seeks to answer: do models truly internalize deep values or just surface proxies? Methodologically, DVB borrows the rigorous diagnostic strategy from HANS (McCoy et al., 2019) and Waterbirds/GroupDRO (Sagawa et al., 2020): engineer training distributions where shallow features correlate with correct choices, then evaluate out-of-correlation to test reliance on shortcuts. This paradigm directly addresses the dataset artifact concerns surfaced by Poliak et al. (2018), ensuring that apparent success is not driven by stylistic or lexical cues. While prior ethical benchmarks like ETHICS (Hendrycks et al., 2021) assess moral competence, they do not disentangle deep moral principles from confounded surface attributes. DVB\u2019s core innovation is to fuse these strands: it evaluates RLHF/values-trained LLMs under controlled confounding, operationalizing the principle-versus-preference distinction. In doing so, it offers a principled test of value generalization that can validate (or falsify) claims that instruction- or constitution-trained models capture human values robustly rather than overfitting to superficial patterns.",
  "analysis_timestamp": "2026-01-07T00:02:04.931502"
}
{
  "prior_works": [
    {
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": "Aaron van den Oord et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Introduced the InfoNCE contrastive objective that this paper analyzes to determine which features CL actually learns, making CPC the foundational loss underlying the theory."
    },
    {
      "title": "Supervised Contrastive Learning",
      "authors": "Prannay Khosla et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Defines the supervised CL objective that pulls same-class examples together; this paper explains, via SGD\u2019s simplicity bias, why SupCon collapses meaningful intra-class (subclass) features and proposes principled remedies."
    },
    {
      "title": "A Theoretical Analysis of Contrastive Unsupervised Representation Learning",
      "authors": "Nikunj Saunshi et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Provides a formal latent-class framework and guarantees for when CL recovers useful features; the present work extends this line by modeling subclasses and incorporating optimization (simplicity) bias to predict test-time feature selection and collapse."
    },
    {
      "title": "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere",
      "authors": "Tongzhou Wang et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Decomposes CL into alignment and uniformity; this paper leverages that view to show how uniformity pressure, combined with SGD\u2019s simplicity bias, suppresses harder class-relevant features and how larger embedding dimensionality mitigates it."
    },
    {
      "title": "On the Spectral Bias of Neural Networks",
      "authors": "Nasim Rahaman et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates that neural networks learn simpler (low-frequency) components first; this simplicity bias directly motivates the paper\u2019s core thesis that SGD causes CL to favor easy features, leading to subclass collapse and feature suppression."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Formalizes gradient descent\u2019s implicit bias (e.g., max-margin solutions), which the paper adapts to contrastive settings to model SGD\u2019s preference for simpler solutions and derive which features are selected at test time."
    },
    {
      "title": "What Makes for Good Views for Contrastive Learning?",
      "authors": "Yonglong Tian et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Empirically establishes that augmentation quality governs the invariances CL learns; the current work provides a theoretical explanation of this dependence and identifies improved augmentations as a remedy for feature suppression."
    }
  ],
  "synthesis_narrative": "The paper builds a unified theory of which features contrastive learning (CL) actually acquires by situating modern CL objectives and practices within the implicit-bias lens of optimization. Contrastive Predictive Coding introduced the InfoNCE objective that underlies most CL methods studied here, while Supervised Contrastive Learning provided the supervised objective whose tendency to merge subclass variations motivates the analysis of class collapse. Earlier theoretical work by Saunshi et al. formalized latent-class settings in which CL yields useful representations; the present paper extends this line by modeling subclasses and, crucially, by incorporating optimization bias to predict test-time feature selection, bridging supervised collapse and unsupervised feature suppression. The alignment\u2013uniformity perspective of Wang and Isola offers a decomposition the authors use to explain how uniformity, coupled with SGD\u2019s tendencies, prioritizes easy features and how increasing embedding dimensionality alleviates suppression. The core mechanism is supplied by the simplicity/implicit bias literature: Rahaman et al. showed neural networks learn simple (low-frequency) components first, and Soudry et al. established gradient descent\u2019s implicit bias toward particular solutions. This paper translates those insights to contrastive objectives, arguing that SGD\u2019s simplicity bias is the driver of subclass collapse and suppression of harder class-relevant features. Finally, empirical findings on the central role of augmentations by Tian et al. are placed on firm theoretical footing, yielding prescriptions\u2014better augmentations and higher-dimensional embeddings\u2014that follow directly from the proposed framework.",
  "analysis_timestamp": "2026-01-06T23:09:26.516401"
}
{
  "prior_works": [
    {
      "title": "Are Sixteen Heads Really Better than One?",
      "authors": "Paul Michel et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "This work introduced per-head importance scoring and head ablation/pruning as a way to quantify individual attention heads\u2019 contributions to model behavior, which the present paper adapts by redefining the objective to a safety metric (Ships) rather than task performance."
    },
    {
      "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
      "authors": "Elena Voita et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "By demonstrating that specific attention heads implement specialized functions and can be selectively masked or pruned, this paper provides the mechanistic premise that enables head-level safety attribution in Ships."
    },
    {
      "title": "Attention is not Explanation",
      "authors": "Sarthak Jain et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "This paper\u2019s critique of raw attention weights as explanations motivates the need for causal, intervention-based measures of head importance, directly addressed by the safety-specific head ablation/attribution in Ships."
    },
    {
      "title": "In-context Learning and Induction Heads",
      "authors": "Catherine Olsson et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "By identifying concrete transformer circuits implemented by specific attention heads and validating them via ablation/patching, this work inspires the current paper\u2019s focus on attributing safety behavior to particular heads."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "Showing that targeted interventions on localized components can causally alter high-level behaviors, this work informs the paper\u2019s strategy of systematically intervening on attention heads to quantify their causal impact on safety."
    },
    {
      "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
      "authors": "Shiran Gehman et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "This paper establishes standard safety evaluation via toxicity/harmfulness prompts, providing the problem formulation and dataset-level metrics that Ships generalizes its head-level safety attribution to."
    },
    {
      "title": "Quantifying Attention Flow in Transformers",
      "authors": "Samira Abnar et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "By proposing attribution methods that trace how attention contributes to outputs, this work conceptually underpins the idea of attributing model behavior to attention components, which Ships adapts to the head level for safety."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014quantifying the causal contribution of individual attention heads to safety via a safety-specific head-importance metric (Ships) and a dataset-level attribution algorithm\u2014builds directly on the mechanistic view that heads implement distinct functions. Voita et al. established that individual heads can be specialized and pruned, and Michel et al. operationalized per-head importance via ablation and pruning; these two works form the direct methodological baseline that the present paper extends by optimizing importance for safety rather than task accuracy. Olsson et al. further cemented the head-centric perspective by revealing concrete circuits (e.g., induction heads) validated through ablation/patching, which directly inspires attributing safety behavior to particular heads. At the same time, Jain and Wallace highlighted that raw attention weights are unreliable explanations, motivating the paper\u2019s causal, intervention-based Ships metric rather than attention-weight heuristics. To ground the safety problem, Gehman et al. introduced RealToxicityPrompts and standardized harmfulness evaluation, enabling Ships to be generalized and validated at the dataset level. Finally, Meng et al. demonstrated that targeted component interventions can causally change high-level behaviors, reinforcing the paper\u2019s design of systematic head ablations to measure safety effects, while Abnar and Zuidema\u2019s attention-flow attribution conceptually supports attributing behavior to attention mechanisms. Together, these works directly motivate, enable, and frame the paper\u2019s head-level safety attribution approach.",
  "analysis_timestamp": "2026-01-06T23:09:26.610972"
}
{
  "prior_works": [
    {
      "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models",
      "authors": "First author et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "LeanDojo operationalized retrieval-augmented modeling over Lean\u2019s library by fetching prerequisite definitions/lemmas, directly inspiring this paper\u2019s Dependency Retrieval to ground autoformalization and curb hallucinated symbols."
    },
    {
      "title": "DeepMath: Deep Sequence Models for Premise Selection",
      "authors": "Alexander A. Alemi et al.",
      "year": 2016,
      "arxiv_id": "1606.04442",
      "role": "Foundation",
      "relationship_sentence": "DeepMath established learned premise selection as a crucial step for reusing formal libraries, and this paper extends that idea to statement autoformalization by retrieving dependent objects to condition generation on the right context."
    },
    {
      "title": "Hammering Towards QED",
      "authors": "Jasmin C. Blanchette et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Hammering Towards QED systematized premise selection and dependency tracking in large formal libraries, providing the concrete notion of dependency graphs that this work leverages for retrieval during autoformalization."
    },
    {
      "title": "Generative Language Modeling for Automated Theorem Proving",
      "authors": "Karlis Polu et al.",
      "year": 2020,
      "arxiv_id": "2009.03393",
      "role": "Gap Identification",
      "relationship_sentence": "Polu et al. evaluated generative models mainly via proof success or surface-level matches, highlighting the lack of a faithful automated equivalence metric for generated statements that this paper addresses with BEq."
    },
    {
      "title": "The Lean 4 Theorem Prover and Programming Language",
      "authors": "Leonardo de Moura et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Lean 4 formalizes definitional equality and reduction rules; BEq directly builds on and extends these definitional-equivalence mechanisms to check bidirectional, definition-unfolding equivalence between formal statements."
    },
    {
      "title": "MiniF2F: A Cross-System Benchmark for Formal Reasoning",
      "authors": "First author et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "MiniF2F highlighted the mismatch between human-perceived equivalence and surface-form comparisons across systems, motivating a faithful, system-grounded equivalence metric like BEq for autoformalized statements."
    }
  ],
  "synthesis_narrative": "Learned premise selection demonstrated that retrieving the right library facts is pivotal for effective reuse of formal knowledge: DeepMath showed deep models can select relevant premises from large corpora, while Hammering Towards QED integrated dependency tracking and premise selection into mainstream hammering pipelines, making dependency graphs a practical interface to large formal libraries. LeanDojo then operationalized retrieval augmentation specifically in Lean, showing that fetching precise formal dependencies (definitions and lemmas) stabilizes language-model behavior in proof tasks. In parallel, generative ATP systems such as Polu et al. primarily assessed models via proof success or surface-form agreement, exposing how fragile and indirect these proxies are for judging statement correctness. Lean 4 clarified the formal bedrock\u2014definitional equality, reductions, and unfolding\u2014on which equivalence inside the proof assistant truly rests, and cross-system efforts like MiniF2F underscored that syntactic similarity often misaligns with mathematical equivalence.\nCollectively, these lines reveal two complementary opportunities: first, evaluation should be grounded in the proof assistant\u2019s own equivalence notions rather than surface forms; second, generation should be conditioned on the exact formal context to avoid inventing symbols or misusing definitions. This paper synthesizes those insights by proposing a neuro-symbolic equivalence checker, BEq, that extends definitional equivalence with bidirectional unfolding and alignment, and by adapting retrieval\u2014rooted in premise/dependency selection\u2014to autoformalization via targeted Dependency Retrieval that injects the precise objects needed to faithfully formalize a statement.",
  "target_paper": {
    "title": "Rethinking and Improving Autoformalization: Towards a Faithful Metric and a Dependency Retrieval-based Approach",
    "authors": "Qi Liu, Xinhao Zheng, Xudong Lu, Qinxiang Cao, Junchi Yan",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Large Language Model, Formal Verification, Autoformalization",
    "abstract": "As a central component in formal verification, statement autoformalization has been widely studied including the recent efforts from machine learning community, but still remains a widely-recognized difficult and open problem. In this paper, we delve into two critical yet under-explored gaps: 1) absence of faithful and universal automated evaluation for autoformalization results; 2) agnosia of contextual information, inducing severe hallucination of formal definitions and theorems. To address the first issue, we propose **BEq** (_**B**idirectional **E**xtended Definitional E**q**uivalence_), an automated neuro-symbolic method to determine the equivalence between two formal statements, which is formal-grounded and well-aligned with human intuition. For the second, we propose **RAutoformalizer** (_**R**etrieval-augmented **Autoformalizer**_), augmenting statement autoformalization by _Dependency Retrieval_, retrieving potentially dependent objects from formal libraries. We parse the depe",
    "openreview_id": "hUb2At2DsQ",
    "forum_id": "hUb2At2DsQ"
  },
  "analysis_timestamp": "2026-01-06T17:43:51.467392"
}
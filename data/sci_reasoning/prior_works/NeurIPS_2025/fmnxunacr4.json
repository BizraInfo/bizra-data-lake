{
  "prior_works": [
    {
      "title": "The Abstraction and Reasoning Corpus (ARC)",
      "authors": "Fran\u00e7ois Chollet",
      "year": 2019,
      "role": "Benchmark/Dataset",
      "relationship_sentence": "ARC crystallized the goal of puzzle-like, domain-knowledge-free reasoning and exposed LLM weaknesses in such tasks, directly motivating Enigmata\u2019s puzzle-centered task design and its use of ARC-style evaluation for generalization."
    },
    {
      "title": "The Procgen Benchmark for Procedural Generalization in RL",
      "authors": "Karl Cobbe et al.",
      "year": 2019,
      "role": "Benchmark/Framework",
      "relationship_sentence": "Procgen\u2019s procedurally generated tasks with automatic, verifiable rewards informed Enigmata\u2019s generator\u2013verifier paradigm and its emphasis on scalable training with out-of-distribution generalization."
    },
    {
      "title": "TextWorld: A Learning Environment for Text-based Games",
      "authors": "Marc-Alexandre C\u00f4t\u00e9 et al.",
      "year": 2018,
      "role": "Environment/Framework",
      "relationship_sentence": "TextWorld\u2019s programmatic task generators and rule-based success checkers provided a concrete blueprint for building text-native, verifiable puzzle environments, which Enigmata generalizes across many puzzle types."
    },
    {
      "title": "DeepSeek-R1: Incentivizing Reasoning in LLMs via Reinforcement Learning",
      "authors": "DeepSeek-AI Team",
      "year": 2024,
      "role": "Method (RL with verifiable rewards)",
      "relationship_sentence": "R1 operationalized reinforcement learning with verifiable rewards (e.g., unit tests and checkers) for math/coding; Enigmata extends this RLVR recipe to diverse puzzles by supplying scalable generators and precise rule-based verifiers."
    },
    {
      "title": "PAL: Program-Aided Language Models",
      "authors": "Luyu Gao et al.",
      "year": 2023,
      "role": "Method/Framework",
      "relationship_sentence": "PAL demonstrated the value of delegating reasoning to executable programs and using program outputs for solution checking, directly supporting Enigmata\u2019s design choice to rely on programmatic, rule-based verification."
    },
    {
      "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
      "authors": "Oyvind Tafjord et al.",
      "year": 2021,
      "role": "Dataset/Method",
      "relationship_sentence": "ProofWriter\u2019s synthetic logic tasks paired with a symbolic prover exemplify generator\u2013verifier pipelines for logical reasoning, inspiring Enigmata\u2019s logic puzzle category with controllable difficulty and automatic checking."
    },
    {
      "title": "APPS: A Benchmark for Code Generation",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "role": "Benchmark/Dataset",
      "relationship_sentence": "APPS popularized automated unit-test-based evaluation in language modeling, reinforcing the viability of programmatic verifiers as reward signals\u2014a core ingredient in Enigmata\u2019s RLVR training across puzzle tasks."
    }
  ],
  "synthesis_narrative": "Enigmata\u2019s core contribution\u2014scalable puzzle reasoning via multi-task RL with verifiable rewards\u2014sits at the intersection of three converging lines of work. First, ARC framed the ambition: human-solvable, domain-agnostic puzzles that stress abstraction and generalization. Enigmata adopts this target and evaluates on ARC-style suites to demonstrate genuine reasoning gains. Second, the generator\u2013verifier paradigm is grounded in procedural RL environments like Procgen and TextWorld, which pair unlimited instance generation with rule-based success criteria. Enigmata generalizes this blueprint to text-native puzzles across multiple categories, adding controllable difficulty and uniform interfaces for RL training. Third, recent progress in RL for reasoning with programmatic feedback\u2014exemplified by DeepSeek-R1\u2014proved that verifiable rewards (tests, checkers) can effectively train LLMs on math and code. Complementary advances such as PAL and ProofWriter showed that delegating to executable programs and symbolic provers yields reliable verification signals for reasoning tasks. APPS further normalized unit-test-driven evaluation as a scalable supervisory signal in language tasks. Integrating these ideas, Enigmata builds a broad suite of synthetic puzzle environments with deterministic verifiers, enabling multi-task RLVR training and fine-grained analysis. This yields measurable improvements on puzzle reasoning benchmarks and transfer to ARC-AGI variants, highlighting that verifiable, procedurally generated supervision can extend RL-driven reasoning beyond math and code into general logical puzzles.",
  "analysis_timestamp": "2026-01-07T00:02:04.951470"
}
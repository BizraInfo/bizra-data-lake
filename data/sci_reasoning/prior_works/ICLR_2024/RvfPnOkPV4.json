{
  "prior_works": [
    {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "authors": "Colin Raffel et al.",
      "year": 2020,
      "arxiv_id": "1910.10683",
      "role": "Foundation",
      "relationship_sentence": "By introducing the C4 corpus and detailing large-scale Common Crawl cleaning/dedup heuristics, this work established the concrete pretraining data formulation that WIMBD audits and systematically analyzes for duplication, PII, toxicity, and contamination."
    },
    {
      "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
      "authors": "Leo Gao et al.",
      "year": 2021,
      "arxiv_id": "2101.00027",
      "role": "Foundation",
      "relationship_sentence": "This paper defined a widely used, composite pretraining corpus and documented source-level filtering/dedup, directly providing one of the principal datasets WIMBD interrogates and compares with uniform count-and-search analyses."
    },
    {
      "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
      "authors": "Guillaume Wenzek et al.",
      "year": 2020,
      "arxiv_id": "1911.00359",
      "role": "Foundation",
      "relationship_sentence": "CCNet\u2019s perplexity-based filtering, language-ID, and dedup pipeline underpin many web corpora (including C4), and WIMBD revisits these outputs with scalable search/count to reveal residual low-quality and duplicate content."
    },
    {
      "title": "Dolma: an Open Corpus of Training Data for Large Language Models",
      "authors": "Luca Soldaini et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Dolma proposed an openly documented, filter-heavy large corpus (including PII and safety filtering), and WIMBD generalizes and standardizes such checks across multiple corpora to assess their prevalence and effectiveness at scale."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "arxiv_id": "2012.07805",
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating memorization and leakage of sensitive strings such as PII from pretrained LMs, this work motivated WIMBD\u2019s explicit large-scale searches for PII and sensitive content directly in pretraining corpora."
    },
    {
      "title": "Deduplicating Training Data Makes Language Models Better",
      "authors": "Katherine Lee et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This paper showed that document and near-duplicate repetition harms LM training, directly motivating WIMBD\u2019s corpus-wide quantification of duplicates and synthetic repetition via scalable count-and-search."
    },
    {
      "title": "Datasheets for Datasets",
      "authors": "Timnit Gebru et al.",
      "year": 2021,
      "arxiv_id": "1803.09010",
      "role": "Inspiration",
      "relationship_sentence": "By proposing structured, standardized documentation of dataset contents and provenance, this work inspired WIMBD\u2019s operationalization of automated, scalable \u2018what\u2019s-in-the-corpus\u2019 reporting via count-and-search primitives."
    }
  ],
  "synthesis_narrative": "C4 established a concrete, large-scale web-crawl-derived pretraining corpus and publicized practical cleaning and deduplication heuristics that set the template for modern LM data. The Pile expanded this paradigm with a composite, openly documented dataset and source-level dedup/filters, highlighting the diversity and heterogeneity of ingredients used in practice. CCNet contributed the widely adopted perplexity-based filtering and language-ID pipeline for Common Crawl, defining de facto quality control steps and dedup strategies for web text. Dolma advanced open data transparency with explicit PII and safety filtering and thorough provenance reporting, offering a modern, large-scale corpus with stronger stated safeguards. In parallel, Carlini et al. demonstrated that pretrained LMs memorize and can leak sensitive strings, foregrounding the importance of quantifying PII and sensitive content in the training data itself. Lee et al. showed that duplication and near-duplication materially degrade LM performance, emphasizing the need to measure duplicate prevalence rather than assume it is solved. Finally, Datasheets for Datasets articulated the blueprint for systematic, standardized documentation of dataset contents.\nTogether, these works revealed a gap: despite influential corpora and filtering pipelines, the field lacked a unified, scalable way to directly inspect and compare what is actually inside massive text datasets\u2014duplicates, PII, toxicity, and benchmark contamination\u2014across heterogeneous sources. The natural next step was to operationalize datasheet-like transparency at web scale by building simple, robust primitives\u2014count and search\u2014that can run over tens of terabytes, enabling standardized, cross-corpus audits and stress-testing the efficacy of prevailing filtering and dedup assumptions.",
  "target_paper": {
    "title": "What's In My Big Data?",
    "authors": "Yanai Elazar, Akshita Bhagia, Ian Helgi Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hannaneh Hajishirzi, Noah A. Smith, Jesse Dodge",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "nlp, dataset, analaysis, data-statistics, data-quality, PII",
    "abstract": "Large text corpora are the backbone of language models.\nHowever, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination).\nIn this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities---count and search---*at scale*, which allows us to analyze more than 35 terabytes on a standard compute node. \nWe apply WIMBD to ten different corpora used to train popular language models, including *C4*, *The Pile*, and *RedPajama*.\nOur analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. \nFor instance, we find that about 50% of the documents in *",
    "openreview_id": "RvfPnOkPV4",
    "forum_id": "RvfPnOkPV4"
  },
  "analysis_timestamp": "2026-01-06T07:37:29.656984"
}
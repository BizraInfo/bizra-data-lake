{
  "prior_works": [
    {
      "title": "pix2code: Generating Code from a Graphical User Interface Screenshot",
      "authors": "Tony Beltramelli",
      "year": 2017,
      "arxiv_id": "1705.07962",
      "role": "Inspiration",
      "relationship_sentence": "Established the image-to-program formulation that VLMaterial adopts, replacing GUI DSL code with executable procedural material programs predicted directly from input images."
    },
    {
      "title": "Learning to Infer Graphics Programs from Hand-Drawn Images",
      "authors": "Kevin Ellis et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Provided the core paradigm of inferring executable graphics programs from images within a domain-specific language, which VLMaterial extends to procedural material graphs using a VLM rather than symbolic search."
    },
    {
      "title": "ShapeAssembly: Learning to Generate Programs for 3D Shape Structure",
      "authors": "Daniel Ritchie et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated that representing structured visual content as interpretable, editable programs is practical and desirable, directly motivating VLMaterial\u2019s choice of programmatic procedural materials for editability."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Foundation",
      "relationship_sentence": "Supplied the pre-trained vision-language modeling and instruction-tuning recipe that VLMaterial fine-tunes to map material images to Python programs implementing procedural graphs."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2023,
      "arxiv_id": "2212.10560",
      "role": "Extension",
      "relationship_sentence": "Directly informs VLMaterial\u2019s program-level augmentation strategy by prompting another LLM to synthesize diverse training programs and prompts to enable effective fine-tuning."
    },
    {
      "title": "DiffVG: Differentiable Vector Graphics Rasterization",
      "authors": "Wenzheng Li et al.",
      "year": 2020,
      "arxiv_id": "2006.06404",
      "role": "Related Problem",
      "relationship_sentence": "Showed how programmatic graphics representations can be rendered and compared to images, underpinning VLMaterial\u2019s render-and-validate paradigm for program outputs even without explicit differentiability."
    },
    {
      "title": "Single-Image SVBRDF Capture with a Rendering-Aware CNN",
      "authors": "Victor Deschaintre et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Highlighted that single-image material reconstruction typically yields non-programmatic SVBRDFs with limited editability, motivating VLMaterial\u2019s shift to procedural programs that are inherently editable."
    }
  ],
  "synthesis_narrative": "Early image-to-program research established that visual inputs can be translated into executable code: pix2code framed the task by mapping GUI screenshots to DSL code, and Learning to Infer Graphics Programs showed that images could specify graphics programs within a domain-specific language. In parallel, ShapeAssembly demonstrated the advantages of representing complex visual structure as interpretable, editable programs, cementing editability as a core benefit of programmatic representations. On the modeling side, LLaVA introduced visual instruction tuning, revealing that pre-trained VLMs can be adapted to follow image-conditioned instructions and output structured text like code. Self-Instruct then showed that instruction-following models can be effectively improved via synthetic, LLM-generated training data, a recipe well-suited to expanding program datasets. Complementing these, DiffVG connected programmatic graphics to raster supervision by rendering and comparing program outputs to images, while single-image SVBRDF methods like Deschaintre et al. illustrated that non-programmatic reconstructions limit downstream editing.\n\nTogether these works exposed an opportunity: leverage a pre-trained VLM\u2019s image-conditioned code generation, boost it with LLM-synthesized program data, and target a representation that preserves editability. VLMaterial naturally synthesizes these ideas by converting procedural material graphs into executable Python programs, fine-tuning a VLM to generate them from images, and validating via rendering\u2014addressing the editability gap in prior material reconstruction methods.",
  "target_paper": {
    "title": "VLMaterial: Procedural Material Generation with Large Vision-Language Models",
    "authors": "Beichen Li, Rundi Wu, Armando Solar-Lezama, Changxi Zheng, Liang Shi, Bernd Bickel, Wojciech Matusik",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "generative model, procedural material, appearance modeling",
    "abstract": "Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, creating a procedural material given an input image requires professional knowledge and significant effort. In this work, we leverage the ability to convert procedural materials into standard Python programs and fine-tune a large pre-trained vision-language model (VLM) to generate such programs from input images. To enable effective fine-tuning, we also contribute an open-source procedural material dataset and propose to perform program-level augmentation by prompting another pre-trained large language model (LLM). Through extensive evaluation, we show that our method outperforms previous methods on both synthetic and real-world examples.",
    "openreview_id": "wHebuIb6IH",
    "forum_id": "wHebuIb6IH"
  },
  "analysis_timestamp": "2026-01-06T12:48:06.305884"
}
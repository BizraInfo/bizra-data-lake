{
  "prior_works": [
    {
      "title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills",
      "authors": "Xue Bin Peng et al.",
      "year": 2018,
      "arxiv_id": "1804.02717",
      "role": "Foundation",
      "relationship_sentence": "DeepMimic established the physics-based motion imitation formulation and reward design from mocap clips, which the present work uses to train the broad-coverage motion imitator prior to skill distillation."
    },
    {
      "title": "Adversarial Motion Priors for Stylized Physics-Based Character Control",
      "authors": "Xue Bin Peng et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "AMP demonstrated how to leverage a discriminator over large, unstructured human motion datasets to train robust, multi-clip imitators, directly inspiring the paper\u2019s choice to first learn an \u2018imitate-all\u2019 controller before distilling a representation."
    },
    {
      "title": "ASE: Adversarial Skill Embeddings for Reinforcement Learning",
      "authors": "Xue Bin Peng et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "ASE introduced an adversarially trained latent skill space for locomotion, serving as the primary baseline whose narrow skill coverage this work overcomes by distilling a compact, universal motion representation from an all-motion imitator."
    },
    {
      "title": "Neural Probabilistic Motor Primitives for Humanoid Control",
      "authors": "Josh Merel et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "NPMP\u2019s variational information bottleneck with a state (proprioception)-conditioned prior over latent motor commands is directly extended here to distill skills and jointly learn a proprioceptive prior for universal control."
    },
    {
      "title": "AMASS: Archive of Motion Capture as Surface Shapes",
      "authors": "Naureen Mahmood et al.",
      "year": 2019,
      "arxiv_id": "1904.03278",
      "role": "Foundation",
      "relationship_sentence": "AMASS provides the large, diverse, and unstructured human motion corpus that enables training an imitator capable of covering \u2018all\u2019 human motions, which is the prerequisite for the paper\u2019s distillation step."
    },
    {
      "title": "MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control",
      "authors": "Zhengyi Luo et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "MoCapAct formalized the multi-task physics-based humanoid control setting from mocap data and highlighted the need for reusable motion representations, directly motivating a universal skill space."
    }
  ],
  "synthesis_narrative": "DeepMimic introduced the core physics-based imitation objective and reward shaping that allowed simulated characters to track mocap clips with high fidelity, proving RL could realize rich skills from examples. Building on this, Adversarial Motion Priors (AMP) showed that a discriminator trained on diverse, unstructured motion can guide a single policy to imitate many clips, loosening the dependence on per-clip objectives and enabling broad coverage. ASE then packaged diverse locomotion into a compact latent skill space via adversarially trained embeddings, demonstrating the utility of low-dimensional control variables but within a narrow movement domain. In parallel, Neural Probabilistic Motor Primitives (NPMP) established an encoder\u2013decoder with a variational information bottleneck and a state-conditioned (proprioceptive) prior to yield closed-loop latent motor commands that are both expressive and controllable. AMASS unified heterogeneous mocap sources into a large-scale, diverse corpus suitable for training general-purpose imitators, while MoCapAct framed multi-task humanoid control from mocap as a benchmark, underscoring the value of reusable motion abstractions. Together, these works revealed a gap: adversarial imitation can cover broad data, and variational motor primitives offer controllable latents, but prior skill embeddings remained domain-limited. The natural next step is to first train an imitate-all policy on comprehensive motion data (AMP-on-AMASS) and then distill its behaviors into an NPMP-style latent with a proprioception-conditioned prior, yielding a universal, compact motion representation that supports physics-based control across diverse skills.",
  "target_paper": {
    "title": "Universal Humanoid Motion Representations for Physics-Based Control",
    "authors": "Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris M. Kitani, Weipeng Xu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "humanoid control, motion generation, physics simulation",
    "abstract": "We present a universal motion representation that encompasses a comprehensive range of motor skills for physics-based humanoid control. Due to the high dimensionality of humanoids and the inherent difficulties in reinforcement learning, prior methods have focused on learning skill embeddings for a narrow range of movement styles (e.g. locomotion, game characters) from specialized motion datasets. This limited scope hampers their applicability in complex tasks. We close this gap by significantly increasing the coverage of our motion representation space. To achieve this, we first learn a motion imitator that can imitate all of human motion from a large, unstructured motion dataset. We then create our motion representation by distilling skills directly from the imitator. This is achieved by using an encoder-decoder structure with a variational information bottleneck. Additionally, we jointly learn a prior conditioned on proprioception (humanoid's own pose and velocities) to improve model",
    "openreview_id": "OrOd8PxOO2",
    "forum_id": "OrOd8PxOO2"
  },
  "analysis_timestamp": "2026-01-07T00:10:50.776979"
}
{
  "prior_works": [
    {
      "title": "Look, Listen and Learn",
      "authors": "Relja Arandjelovi\u0107, Andrew Zisserman",
      "year": 2017,
      "role": "Foundational audio-visual correspondence learning",
      "relationship_sentence": "Established that synchronized audio and visual cues can be leveraged to localize and track sounding objects, directly motivating SAVVY\u2019s use of audio cues to stabilize egocentric object tracks in dynamic scenes."
    },
    {
      "title": "Sound event localization and detection of overlapping sources using convolutional recurrent neural networks (SELDnet)",
      "authors": "Sharath Adavanne, Archontis Politis, Tuomas Virtanen",
      "year": 2018,
      "role": "Spatial audio DoA estimation and event localization",
      "relationship_sentence": "Provided practical methods for estimating direction-of-arrival from spatial audio (e.g., FOA/ambisonics), a core ingredient for SAVVY\u2019s Egocentric Spatial Tracks Estimation using spatial audio cues."
    },
    {
      "title": "SoundSpaces: Audio for 3D Environments",
      "authors": "Changan Chen, Unnat Jain, Carl Schissler, Shubham Tulsiani, Kristen Grauman, Abhinav Gupta",
      "year": 2020,
      "role": "3D spatialized audio platform and tasks",
      "relationship_sentence": "Demonstrated the value of physically-grounded spatial audio for embodied 3D reasoning and navigation, directly inspiring SAVVY-Bench\u2019s emphasis on synchronized spatial audio for 3D spatial questions."
    },
    {
      "title": "ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras",
      "authors": "Ra\u00fal Mur-Artal, Juan D. Tard\u00f3s",
      "year": 2017,
      "role": "Egocentric mapping and global frame construction",
      "relationship_sentence": "Pioneered robust, training-free global mapping and consistent world-frame estimation, informing SAVVY\u2019s Dynamic Global Map Construction stage for fusing multi-object tracks over time."
    },
    {
      "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
      "authors": "Kristen Grauman et al.",
      "year": 2022,
      "role": "Egocentric, temporally grounded video benchmark",
      "relationship_sentence": "Highlighted the challenges of egocentric, dynamic, and temporally grounded reasoning, shaping SAVVY-Bench\u2019s design for fine-grained temporal grounding and egocentric 3D localization."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",
      "year": 2023,
      "role": "Instruction-following VLM foundation",
      "relationship_sentence": "Introduced the instruction-tuning paradigm for vision-language reasoning, which SAVVY leverages by using AV-LLMs as a controllable reasoning component to parse queries and guide object/track selection."
    },
    {
      "title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
      "authors": "Rongjie Huang et al.",
      "year": 2023,
      "role": "LLM-based audio understanding",
      "relationship_sentence": "Showed that LLMs can be extended to process and reason about audio, directly supporting SAVVY\u2019s use of AV-LLMs to integrate spatial audio with vision during trajectory estimation and reasoning."
    }
  ],
  "synthesis_narrative": "SAVVY\u2019s core contribution\u2014a training-free, two-stage pipeline that unifies spatial audio and visual cues for egocentric 3D spatial reasoning, coupled with a benchmark for dynamic scenes\u2014emerges at the intersection of three lines of prior work. First, audio-visual correspondence and spatial audio localization laid the sensing foundations. Look, Listen and Learn demonstrated that synchronized audio and vision can localize sounding objects, while SELDnet established reliable direction-of-arrival estimation from ambisonic inputs, the precise primitive SAVVY exploits to stabilize egocentric tracks with directional cues. SoundSpaces further validated that physically grounded, spatialized audio in 3D environments enables stronger embodied spatial reasoning, motivating SAVVY-Bench\u2019s synchronized spatial audio. Second, egocentric mapping methods, epitomized by ORB-SLAM2, showed how to build consistent global frames without task-specific training; SAVVY\u2019s Dynamic Global Map Construction echoes these ideas to fuse multi-object trajectories over time into a coherent global map. Third, instruction-following multimodal LLMs catalyzed controllable reasoning over rich inputs: LLaVA established visual instruction tuning, and AudioGPT extended LLMs to audio, together informing SAVVY\u2019s use of AV-LLMs to parse queries and guide track selection and temporal grounding. Finally, Ego4D highlighted the demands of egocentric, dynamic, temporally grounded queries, shaping SAVVY-Bench\u2019s focus on moving objects and fine-grained temporal reasoning. Combined, these works directly scaffold SAVVY\u2019s sensing primitives, mapping strategy, and instruction-driven, training-free reasoning design.",
  "analysis_timestamp": "2026-01-07T00:21:32.324325"
}
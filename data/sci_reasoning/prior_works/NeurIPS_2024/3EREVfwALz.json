{
  "prior_works": [
    {
      "title": "Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm",
      "authors": "Nick Littlestone",
      "year": 1988,
      "role": "Foundational theory (mistake-bound model and Littlestone dimension)",
      "relationship_sentence": "The paper\u2019s Level-constrained Littlestone dimension is a structured refinement of Littlestone\u2019s tree-based combinatorial framework, extending the classic mistake-bound characterization from binary online learning to multiclass transductive settings with unbounded labels."
    },
    {
      "title": "Characterizations of Learnability for Multiclass Concept Classes (via the One-Inclusion Graph/Graph Dimension)",
      "authors": "Shai Ben-David, Nicol\u00f2 Cesa-Bianchi, David Haussler, Philip M. Long",
      "year": 1997,
      "role": "Foundational multiclass/transductive machinery (graph/one-inclusion framework)",
      "relationship_sentence": "The one-inclusion graph perspective and associated multiclass dimensions underpin transductive analysis; the present work adapts this combinatorial viewpoint to define a new dimension that handles unbounded label spaces while preserving optimal mistake-rate characterizations."
    },
    {
      "title": "Multiclass Learnability and the ERM Principle",
      "authors": "Amit Daniely, Sivan Sabato, Shai Ben-David, Shai Shalev-Shwartz",
      "year": 2011,
      "role": "Core multiclass dimension theory (Natarajan/Graph dimensions)",
      "relationship_sentence": "By clarifying how multiclass learnability is governed by combinatorial dimensions (e.g., Natarajan/Graph), this work motivates the search for the \u2018right\u2019 multiclass online/transductive dimension\u2014here realized as the Level-constrained Littlestone dimension for unbounded labels."
    },
    {
      "title": "Bandit Multiclass Learning and the Bandit Littlestone Dimension",
      "authors": "Amit Daniely, Shai Shalev-Shwartz",
      "year": 2014,
      "role": "Dimension-based characterization in multiclass online variants",
      "relationship_sentence": "This paper\u2019s extension of Littlestone-style parameters to multiclass/bandit feedback demonstrates the power of tailoring tree dimensions to feedback/constraint regimes, directly inspiring the present paper\u2019s constraint-aware (level-constrained) Littlestone dimension."
    },
    {
      "title": "Transductive Inference and Statistical Learning Theory",
      "authors": "Vladimir N. Vapnik",
      "year": 1998,
      "role": "Transductive learning paradigm",
      "relationship_sentence": "The paper works in a transductive online setting; Vapnik\u2019s transductive viewpoint provides the foundational paradigm and objective (predicting labels on a fixed unlabeled pool), which the new dimension precisely characterizes under multiclass/unbounded labels."
    },
    {
      "title": "Multiclass Transductive Online Learning with Finite Label Spaces: A Trichotomy of Minimax Rates",
      "authors": "Steve Hanneke, Vinod Raman, Amirreza Shaeiri, Unique Subedi",
      "year": 2024,
      "role": "Direct predecessor; posed the open problem",
      "relationship_sentence": "That work established the \u0398(T)/\u0398(log T)/\u0398(1) trichotomy for finite label spaces and explicitly asked for a characterization with unbounded labels; the current paper resolves this by introducing the Level-constrained Littlestone dimension and proving the same trichotomy persists."
    }
  ],
  "synthesis_narrative": "The core advance of Multiclass Transductive Online Learning is a new combinatorial parameter\u2014the Level-constrained Littlestone dimension\u2014that characterizes learnability and minimax mistake rates when the label space is unbounded. This contribution sits squarely in the Littlestone tradition: Littlestone (1988) established the tree-based mistake-bound framework and the Littlestone dimension for binary online learning. Building on this, the one-inclusion graph/graph-dimension line of work by Ben-David and co-authors (1997) furnished the multiclass and transductive toolkit that links combinatorial structure to optimal prediction strategies on a fixed unlabeled pool, a perspective central to transductive online analysis.\nMulticlass learnability theory (Daniely, Sabato, Ben-David, Shalev-Shwartz, 2011) clarified the roles of Natarajan and graph dimensions, reinforcing that the right dimension yields tight characterizations. Subsequent progress in online multiclass variants (e.g., bandit) by Daniely and Shalev-Shwartz (2014) showed how Littlestone-style parameters can be adapted to constraint-specific regimes\u2014an approach mirrored here by introducing a level-constrained version suited to unbounded labels. The transductive paradigm itself stems from Vapnik\u2019s formulation, which frames the precise objective the present work addresses. Most directly, Hanneke et al. (2024) established a \u0398(T)/\u0398(log T)/\u0398(1) trichotomy for finite label spaces and posed the unbounded-label question; the present paper answers it, proving the same trichotomy persists and pinning it to the new Level-constrained Littlestone dimension, thus completing the dimensional characterization of multiclass transductive online learning across finite and unbounded label spaces.",
  "analysis_timestamp": "2026-01-06T23:33:35.551395"
}
{
  "prior_works": [
    {
      "title": "End-to-end optimized image compression",
      "authors": "Johannes Ball\u00e9 et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Established the end-to-end learned transform coding framework and R\u2013D objective on which this work operates; the proposed linear AuxT is inserted into this very framework to offload energy compaction from the nonlinear transforms and speed training."
    },
    {
      "title": "Variational image compression with a scale hyperprior",
      "authors": "Johannes Ball\u00e9 et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Provides a primary LIC baseline whose nonlinear analysis/synthesis transforms must learn energy compaction; the proposed AuxT is designed to reduce this burden and accelerate convergence within such hyperprior-based models."
    },
    {
      "title": "Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules",
      "authors": "Zhengxue Cheng et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Achieves strong R\u2013D with heavy attention-based nonlinear transforms that are slow to train; the present work targets this gap by disentangling decorrelation and energy modulation via a linear AuxT to enable faster training without degrading R\u2013D."
    },
    {
      "title": "Density modeling of images using a generalized normalization transformation",
      "authors": "Johannes Ball\u00e9 et al.",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "Introduced GDN to Gaussianize and decorrelate features, implicitly coupling decorrelation and gain control; the key idea here explicitly disentangles these components and performs coarse energy compaction linearly before nonlinear distribution fitting."
    },
    {
      "title": "A theory for multiresolution signal decomposition: the wavelet representation",
      "authors": "St\u00e9phane Mallat",
      "year": 1989,
      "role": "Foundation",
      "relationship_sentence": "Provides the multiresolution wavelet framework whose subband decomposition decorrelates signals and compacts energy; the proposed wavelet-based linear shortcuts in AuxT directly leverage this property."
    },
    {
      "title": "Image coding using wavelet transform",
      "authors": "Martine Antonini et al.",
      "year": 1992,
      "role": "Extension",
      "relationship_sentence": "Demonstrated biorthogonal wavelets for efficient linear energy compaction in image coding; this work embeds wavelet-based downsampling as linear shortcuts within learned transform training to obtain similar benefits."
    },
    {
      "title": "Analysis of a complex of statistical variables into principal components",
      "authors": "Harold Hotelling",
      "year": 1933,
      "role": "Foundation",
      "relationship_sentence": "Introduced PCA/KLT for linear decorrelation and variance concentration; AuxT\u2019s orthogonal linear projection explicitly adopts this principle to decorrelate features prior to nonlinear modeling."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014using a linear auxiliary transform (AuxT) to disentangle energy compaction into feature decorrelation and energy modulation for faster training\u2014sits at the intersection of classic transform coding and modern learned image compression (LIC). Classical theory established that linear transforms can decorrelate and compact energy: PCA/KLT (Hotelling) formalized orthogonal projection for decorrelation, while wavelet multiresolution analysis (Mallat) and its practical biorthogonal implementations (Antonini et al.) showed subband decompositions that concentrate image energy and enable efficient coding. Modern LIC (Ball\u00e9 et al., 2017) recast compression as end-to-end optimization of nonlinear analysis/synthesis transforms under an R\u2013D objective, often employing GDN (Ball\u00e9 et al., 2016) to Gaussianize features\u2014implicitly coupling decorrelation with gain control. The hyperprior model (Ball\u00e9 et al., 2018) and subsequent high-performing variants with attention and richer likelihoods (Cheng et al., 2020) boost R\u2013D performance, but their increasingly powerful nonlinear transforms become slow to train because they must learn energy compaction from scratch. This paper identifies that entanglement as a root cause of training inefficiency and reinstates a classic coding insight: perform coarse, efficient energy compaction linearly. Concretely, it introduces AuxT with wavelet-based linear shortcuts (drawing from Mallat/Antonini) and an orthogonal projection (in the spirit of PCA) to handle decorrelation and subband energy shaping, leaving the nonlinear transforms to focus on fine-grained distribution fitting. In doing so, it directly extends the LIC framework while addressing the training-time gap of strong hyperprior/attention baselines.",
  "analysis_timestamp": "2026-01-06T23:09:26.609675"
}
{
  "prior_works": [
    {
      "title": "Ego-Exo4D: Understanding Skilled Human Activities from First and Third Person Perspectives",
      "authors": "Ego-Exo4D Consortium (Meta AI)",
      "year": 2024,
      "role": "Dataset and problem framing for synchronized ego\u2013exo perception",
      "relationship_sentence": "Demonstrated the complementary value of paired first- and third-person views with time-synchronized captures, directly motivating the paper\u2019s use of exocentric cameras to mitigate egocentric blind spots and inspiring the design of E3VQA around aligned ego\u2013exo pairs."
    },
    {
      "title": "Charades-Ego: A Large-Scale Dataset of Paired Third and First-Person Videos",
      "authors": "Gunnar A. Sigurdsson, G\u00fcl Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, Abhinav Gupta",
      "year": 2018,
      "role": "Early evidence for cross-view transfer between observer (exo) and actor (ego) perspectives",
      "relationship_sentence": "Provided foundational evidence that modeling actor\u2013observer correspondence across synchronized ego\u2013exo videos improves recognition, a direct precedent for integrating perspectives in the proposed LVLM pipeline."
    },
    {
      "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
      "authors": "Kristen Grauman et al.",
      "year": 2022,
      "role": "Large-scale egocentric benchmark and tasks highlighting limits of ego-only context",
      "relationship_sentence": "Showcased egocentric tasks (e.g., QA, episodic memory) and exposed failures from narrow FoV and occlusions, motivating the paper\u2019s augmentation of ego inputs with exo views to recover global scene context."
    },
    {
      "title": "Scaling Egocentric Vision: The EPIC-KITCHENS Dataset",
      "authors": "Dima Damen et al.",
      "year": 2018,
      "role": "Egocentric action/object benchmarks emphasizing hand\u2013object interactions",
      "relationship_sentence": "Established the value of fine-grained, hands-on egocentric cues while implicitly revealing the lack of global layout\u2014supporting the paper\u2019s claim that exo views complement ego signals in LVLM reasoning."
    },
    {
      "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "authors": "Ranjay Krishna et al.",
      "year": 2017,
      "role": "Scene graph representation and annotations linking objects, attributes, and relations",
      "relationship_sentence": "Popularized scene graphs as a structured representation; the paper\u2019s M3CoT builds on this idea by constructing and merging scene graphs from multiple viewpoints into a unified scene representation."
    },
    {
      "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
      "authors": "Drew A. Hudson, Christopher D. Manning",
      "year": 2019,
      "role": "Scene-graph-grounded visual reasoning for QA",
      "relationship_sentence": "Demonstrated that explicit, compositional scene-graph reasoning benefits VQA, directly informing M3CoT\u2019s design to use graph-structured, multi-perspective context for training-free LVLM prompting."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Quoc V. Le, et al.",
      "year": 2022,
      "role": "Prompting methodology for step-by-step reasoning without additional training",
      "relationship_sentence": "Provided the core prompting paradigm that M3CoT extends to the multimodal, multi-view setting by guiding LVLMs to reason over integrated ego\u2013exo scene graphs in a training-free manner."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014comprehensive scene understanding for LVLMs by fusing first- and third-person views and prompting them with a unified, graph-structured scene representation\u2014stands on three converging lines of prior work. First, egocentric foundations (Ego4D; EPIC-KITCHENS) established the strengths of ego views for attention and hand\u2013object interactions while exposing blind spots due to narrow field-of-view and occlusions. Second, paired ego\u2013exo datasets (Charades-Ego and, at scale, Ego-Exo4D) showed that synchronized third-person observations supply complementary global layout and visibility cues and can be aligned with egocentric evidence, directly motivating E3VQA\u2019s design around synchronized ego\u2013exo pairs and the integration strategy in the LVLM pipeline. Third, structured visual reasoning via scene graphs (Visual Genome; GQA) demonstrated that object\u2013relation abstractions enable compositional QA, suggesting a representation amenable to merging multi-view evidence. Building on these, the paper\u2019s M3CoT adopts Chain-of-Thought prompting to orchestrate training-free, stepwise reasoning over multi-perspective scene graphs, unifying ego and exo signals into a coherent scene model. Together, these prior works provided: the problem pressure (ego-only limitations), the data and alignment paradigm (paired ego\u2013exo captures), and the reasoning substrate (scene-graph-grounded CoT) that directly shaped E3VQA and the M3CoT prompting framework.",
  "analysis_timestamp": "2026-01-07T00:21:32.326649"
}
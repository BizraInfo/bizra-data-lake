{
  "prior_works": [
    {
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "authors": "Tuomas Haarnoja et al.",
      "year": 2018,
      "arxiv_id": "1801.01290",
      "role": "Extension",
      "relationship_sentence": "HASAC directly generalizes SAC\u2019s maximum-entropy objective and soft policy iteration to coordinated updates over multiple heterogeneous agents, forming the algorithmic backbone for stochastic multi-agent policies with monotonic improvement."
    },
    {
      "title": "Reinforcement Learning and Control as Probabilistic Inference: A Tutorial",
      "authors": "Sergey Levine",
      "year": 2018,
      "arxiv_id": "1805.00909",
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s embedding of cooperative MARL into a probabilistic graphical model and its derivation of a MaxEnt objective follow the control-as-inference formulation laid out by this work."
    },
    {
      "title": "Quantal Response Equilibria for Normal Form Games",
      "authors": "Richard D. McKelvey et al.",
      "year": 1995,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The convergence analysis targets Quantal Response Equilibrium (QRE), adopting this equilibrium concept as induced by entropy-regularized (logit) best responses in the proposed MaxEnt MARL framework."
    },
    {
      "title": "Conservative Policy Iteration",
      "authors": "Sham Kakade et al.",
      "year": 2002,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The monotonic improvement guarantee for HASAC/MEHAML builds on CPI\u2019s performance difference lemma and KL-regularized update template, adapted to joint multi-agent policy updates."
    },
    {
      "title": "Trust Region Policy Optimization",
      "authors": "John Schulman et al.",
      "year": 2015,
      "arxiv_id": "1502.05477",
      "role": "Extension",
      "relationship_sentence": "The heterogeneous-agent mirror step and its monotonicity guarantee extend TRPO\u2019s KL-constrained optimization idea to coordinated multi-agent, maximum-entropy policy updates."
    },
    {
      "title": "The Surprising Effectiveness of MAPPO in Cooperative Multi-Agent Reinforcement Learning",
      "authors": "Yu et al.",
      "year": 2021,
      "arxiv_id": "2103.01955",
      "role": "Baseline",
      "relationship_sentence": "MAPPO is the primary on-policy cooperative MARL baseline whose sample-efficiency and stability limitations motivate a MaxEnt, theoretically grounded alternative, and which MEHAML can subsume as a special case of mirror-style updates."
    },
    {
      "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "authors": "Tabish Rashid et al.",
      "year": 2018,
      "arxiv_id": "1803.11485",
      "role": "Gap Identification",
      "relationship_sentence": "Observed training instability and convergence to suboptimal equilibria in value factorization methods like QMIX are explicitly addressed by moving to a MaxEnt joint-policy framework with QRE convergence guarantees."
    }
  ],
  "synthesis_narrative": "Maximum entropy reinforcement learning established a stochastic control objective and soft policy iteration (soft Bellman backups with an entropy term), with Soft Actor-Critic providing a practical off-policy algorithmic realization of this idea. The control-as-inference view formalized how to derive such objectives from probabilistic graphical models, tying optimal control to variational inference and making the entropy term principled rather than heuristic. In game theory, Quantal Response Equilibrium introduced the equilibrium notion arising from entropy-regularized (logit) best responses, thereby linking stochastic policies and bounded-rational outcomes. On the policy optimization side, Conservative Policy Iteration and Trust Region Policy Optimization supplied the performance difference bounds and KL-constrained update machinery that yield monotonic improvement guarantees. In cooperative MARL practice, MAPPO demonstrated a strong on-policy baseline yet revealed stability and sample-efficiency limitations, while value factorization methods such as QMIX exhibited instability and susceptibility to suboptimal equilibria.\n\nTaken together, these works suggested a natural synthesis: embed cooperative MARL into a probabilistic graphical model to derive a principled maximum-entropy joint-policy objective; optimize it with mirror/trust-region style updates that inherit monotonic improvement; and analyze the induced stochastic fixed points as QRE. This directly motivates a heterogeneous-agent generalization of SAC (HASAC) with soft policy iteration over multiple agents, and a unifying mirror-learning template (MEHAML) that recovers strong baselines while endowing them with MaxEnt structure and convergence guarantees.",
  "target_paper": {
    "title": "Maximum Entropy Heterogeneous-Agent Reinforcement Learning",
    "authors": "Jiarong Liu, Yifan Zhong, Siyi Hu, Haobo Fu, QIANG FU, Xiaojun Chang, Yaodong Yang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "cooperative multi-agent reinforcement learning, heterogeneous-agent soft actor-critic, maximum entropy heterogeneous-agent mirror learning",
    "abstract": "*Multi-agent reinforcement learning* (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning \\emph{stochastic} policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose *Heterogeneous-Agent Soft Actor-Critic* (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to *quantal response equilibrium* (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named *Maximum Entropy Heterogeneous-Agent Mirror Learning* (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six",
    "openreview_id": "tmqOhBC4a5",
    "forum_id": "tmqOhBC4a5"
  },
  "analysis_timestamp": "2026-01-06T11:40:56.689931"
}
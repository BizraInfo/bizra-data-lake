{
  "prior_works": [
    {
      "title": "The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices",
      "authors": "Romain Benaych-Georges et al.",
      "year": 2011,
      "arxiv_id": "1103.2221",
      "role": "Foundation",
      "relationship_sentence": "Provides the precise spiked-matrix theory and eigenvector-overlap formulas used to formalize when and how low-rank outlier eigenspaces emerge, which the paper leverages to track Hessian/gradient outliers that SGD aligns with."
    },
    {
      "title": "Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices",
      "authors": "Jinho Baik et al.",
      "year": 2005,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Introduces the BBP phase transition for spiked covariance models, underpinning the notion that learnable low-rank signal produces outlier eigenvalues and aligned eigenvectors\u2014core to the paper\u2019s outlier-eigenspace alignment results."
    },
    {
      "title": "Hessian Eigenspectrum of Deep Networks: A Tale of Two Components",
      "authors": "Zhewei Yao et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Empirically showed a bulk-plus-outliers Hessian structure tied to class/feature directions during training but lacked theory, directly motivating the paper\u2019s rigorous proof of SGD\u2019s rapid alignment with these outlier eigenspaces."
    },
    {
      "title": "A Three-Level Hierarchical Model for the Hessian of Deep Neural Networks",
      "authors": "Yoram Carmon Papyan et al.",
      "year": 2019,
      "arxiv_id": "1909.11365",
      "role": "Inspiration",
      "relationship_sentence": "Identified evolving, class-aligned low-rank outlier blocks in the Hessian over training, which the paper formalizes by proving layerwise alignment of SGD with the corresponding emerging outlier eigenspaces."
    },
    {
      "title": "Prevalence of Neural Collapse during the terminal phase of deep learning training",
      "authors": "Yair Han et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Revealed last-layer low-rank class-simplex geometry at interpolation, informing the paper\u2019s result that the final layer\u2019s outlier eigenspace evolves and can become rank-deficient at suboptimal classifiers."
    },
    {
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "authors": "Song Mei et al.",
      "year": 2018,
      "arxiv_id": "1804.06561",
      "role": "Foundation",
      "relationship_sentence": "Provides the mean-field/gradient-flow framework for two-layer networks that the paper builds on to rigorously couple SGD dynamics with spectral evolution and establish layerwise alignment."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe et al.",
      "year": 2013,
      "arxiv_id": "1312.6120",
      "role": "Inspiration",
      "relationship_sentence": "Shows gradient descent aligns weights with principal signal directions and learns modes sequentially, an alignment paradigm the paper extends to nonlinear networks via Hessian/gradient outlier eigenspaces."
    }
  ],
  "synthesis_narrative": "Low-rank perturbation theory established that spiked structures in large random matrices generate outlier eigenvalues with eigenvectors aligned to the underlying signal; Benaych-Georges and Nadakuditi provided general eigenvalue\u2013eigenvector overlap formulas for finite-rank perturbations, while the BBP transition characterized when such outliers detach from the bulk, signaling detectable structure. Empirical studies then revealed that deep-network Hessians display precisely this bulk-plus-outliers phenomenon: Yao and colleagues documented two components with outliers linked to class/feature directions that evolve over training, and Papyan introduced a three-level hierarchical model where the low-rank outlier blocks encode class structure and change during optimization. Complementarily, neural collapse uncovered a striking low-rank geometry in the last layer\u2014class means forming a simplex\u2014indicating that certain layers naturally compress to low-dimensional subspaces near interpolation. On the dynamics side, mean-field analyses of two-layer networks by Mei, Montanari, and Nguyen developed rigorous tools to track gradient flow/SGD in overparameterized regimes, while Saxe, McClelland, and Ganguli showed in deep linear models that gradient descent aligns weights with dominant signal modes in a sequential manner. Together these works pointed to a gap: empirical evidence and linear-theory intuition strongly suggested that training should rapidly align with emergent low-rank spectral structure, yet a rigorous, dynamical coupling between SGD and the evolving Hessian/gradient spectra\u2014especially layerwise and on structured high-dimensional mixtures\u2014was missing. By marrying spiked-matrix theory with mean-field SGD dynamics and building on the empirical Hessian observations and last-layer geometry, the paper takes the natural next step: it proves that SGD quickly aligns with the emerging outlier eigenspaces, tracks their layerwise evolution, and explains rank deficiencies at suboptimal solutions.",
  "target_paper": {
    "title": "High-dimensional SGD aligns with emerging outlier eigenspaces",
    "authors": "Gerard Ben Arous, Reza Gheissari, Jiaoyang Huang, Aukosh Jagannath",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "stochastic gradient descent, Hessian, multi-layer neural networks, high-dimensional classification, Gaussian mixture model, XOR problem",
    "abstract": "We rigorously study the joint evolution of training dynamics via stochastic gradient descent (SGD) and the spectra of empirical Hessian and gradient matrices. We prove that in two canonical classification tasks for multi-class high-dimensional mixtures and either 1 or 2-layer neural networks, the SGD trajectory rapidly aligns with emerging low-rank outlier eigenspaces of the Hessian and gradient matrices. Moreover, in multi-layer settings this alignment occurs per layer, with the final layer's outlier eigenspace evolving over the course of training, and exhibiting rank deficiency when the SGD converges to sub-optimal classifiers. This establishes  some of the rich predictions that have arisen from extensive numerical studies in the last decade about the spectra of Hessian and information matrices over the course of training in overparametrized networks.",
    "openreview_id": "MHjigVnI04",
    "forum_id": "MHjigVnI04"
  },
  "analysis_timestamp": "2026-01-06T18:53:15.286965"
}
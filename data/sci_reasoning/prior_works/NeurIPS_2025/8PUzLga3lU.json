{
  "prior_works": [
    {
      "title": "GPT-4o: OpenAI\u2019s most capable multimodal model",
      "authors": "OpenAI",
      "year": 2024,
      "role": "End-to-end omni multimodality and real-time speech interaction baseline",
      "relationship_sentence": "VITA-1.5 directly follows GPT-4o\u2019s end-to-end, low-latency paradigm for unified vision\u2013audio\u2013text interaction, aiming to match its real-time speech-to-speech dialogue without external ASR/TTS."
    },
    {
      "title": "SeamlessM4T: Massively Multilingual & Multimodal Machine Translation",
      "authors": "Meta AI (Seamless Communication Team)",
      "year": 2023,
      "role": "Direct speech-to-speech modeling with discrete units (no explicit ASR/TTS cascade)",
      "relationship_sentence": "The paper\u2019s claim of efficient speech-to-speech capability without separate ASR and TTS is grounded in SeamlessM4T\u2019s evidence that unit-based encoders/decoders can enable text-free S2S translation and generation."
    },
    {
      "title": "VALL-E: Neural Codec Language Models are Zero-Shot Text-to-Speech Synthesizers",
      "authors": "Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqi Wang, Long Zhou, et al.",
      "year": 2023,
      "role": "Neural codec language modeling for high-fidelity, low-latency speech generation and voice preservation",
      "relationship_sentence": "VITA-1.5\u2019s speech tokenization and speech-generation design draw on VALL-E\u2019s demonstration that modeling discrete codec tokens enables fast, high-quality, and expressive speech synthesis within a language modeling framework."
    },
    {
      "title": "High Fidelity Neural Audio Compression (EnCodec)",
      "authors": "Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi",
      "year": 2023,
      "role": "Neural audio codec providing discrete units for speech modeling",
      "relationship_sentence": "By leveraging EnCodec-style discrete acoustic tokens, VITA-1.5 can unify speech understanding and generation in a single LLM stack while minimizing latency and preserving audio quality."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee",
      "year": 2023,
      "role": "Multi-stage visual\u2013language alignment and instruction tuning recipe",
      "relationship_sentence": "VITA-1.5\u2019s multi-stage curriculum that preserves strong vision\u2013language ability is directly inspired by LLaVA\u2019s approach to aligning visual encoders with an LLM via instruction tuning."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li, Dongxu Li, Silvio Savarese, Steven C. H. Hoi",
      "year": 2023,
      "role": "Efficient vision-LLM bridging via learned connector (e.g., Q-Former) with largely frozen backbones",
      "relationship_sentence": "The architectural strategy of coupling a powerful LLM with a frozen vision encoder through a lightweight connector informs VITA-1.5\u2019s way of retaining strong visual competence while extending to speech."
    },
    {
      "title": "AudioLM: A Language Modeling Approach to Audio Generation",
      "authors": "Radu Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Marco Tagliasacchi, Neil Zeghidour",
      "year": 2022,
      "role": "Tokenized audio modeling for long-horizon coherent speech generation without text supervision",
      "relationship_sentence": "AudioLM\u2019s demonstration that language modeling over discrete audio tokens can produce coherent speech underpins VITA-1.5\u2019s unified speech modeling and its avoidance of separate ASR/TTS components."
    }
  ],
  "synthesis_narrative": "VITA-1.5\u2019s core contribution\u2014an end-to-end, real-time model that unifies vision and speech for fluent speech-to-speech dialogue while preserving strong visual-language ability\u2014stands on two converging lines of prior work. On the vision\u2013language side, LLaVA and BLIP-2 established practical blueprints for coupling a powerful LLM with a frozen visual encoder through a lightweight connector, then solidifying the interface via multi-stage instruction tuning. This recipe directly motivates VITA-1.5\u2019s staged training to retain high image/video understanding while extending the modality stack.\nOn the speech side, the shift from cascaded ASR\u2192LLM\u2192TTS to direct unit-based modeling was catalyzed by SeamlessM4T, AudioLM, and VALL-E. These works showed that modeling discrete acoustic tokens enables robust speech understanding and generation\u2014including long-horizon coherence, expressivity, and voice preservation\u2014without explicit ASR/TTS boundaries. EnCodec provides the practical tokenization backbone, offering high-fidelity, low-latency discrete representations that make unified speech modeling feasible in production settings. Finally, GPT-4o crystallized the end-to-end, low-latency \u201comni\u201d interaction target\u2014real-time vision and speech I/O within one model\u2014setting the performance and latency bar VITA-1.5 aims to meet.\nTogether, these influences shape VITA-1.5\u2019s multi-stage curriculum (to preserve V-L competence), its discrete speech token pathway (to enable text-free S2S), and its unified decoding for low-latency audio responses, yielding a single model capable of fast, high-quality image/video understanding and natural speech interaction.",
  "analysis_timestamp": "2026-01-07T00:29:42.064750"
}
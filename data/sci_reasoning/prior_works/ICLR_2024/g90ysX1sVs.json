{
  "prior_works": [
    {
      "title": "Rational Neural Networks",
      "authors": "Nicolas Boull\u00e9 and Alex Townsend",
      "year": 2020,
      "arxiv_id": "arXiv:2004.01902",
      "role": "Extension",
      "relationship_sentence": "The paper adopts learnable rational activation units introduced here and extends them by sharing coefficients across layers and deriving conditions for closure under residual connections to control plasticity in RL."
    },
    {
      "title": "Pad\u00e9 Activation Units: End-to-end Learning of Activation Functions for Deep Networks",
      "authors": "M. G. Apicella et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "This work established trainable rational-form activations (Pad\u00e9 units), directly inspiring the choice of rational parameterizations that the current paper regularizes via joint sharing and adapts for residual architectures."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "authors": "Kaiming He et al.",
      "year": 2016,
      "arxiv_id": "arXiv:1512.03385",
      "role": "Inspiration",
      "relationship_sentence": "Residual connections in ResNets motivated the authors to derive the specific closure condition ensuring that rational activation units remain well-formed under residual compositions."
    },
    {
      "title": "Learning Activation Functions to Improve Deep Neural Networks",
      "authors": "Forest Agostinelli et al.",
      "year": 2014,
      "arxiv_id": "arXiv:1412.6830",
      "role": "Related Problem",
      "relationship_sentence": "By showing that learnable activations can boost performance but risk overfitting when overly flexible, this paper motivated the current work\u2019s regularized, jointly-parameterized rational activations to balance flexibility and control."
    },
    {
      "title": "Activate or Not: Learning Customized Activation",
      "authors": "Ningning Ma et al.",
      "year": 2021,
      "arxiv_id": "arXiv:2009.04759",
      "role": "Inspiration",
      "relationship_sentence": "The idea of globally or channel-wise shared, learnable activation parameters here informed the decision to tie rational activation coefficients across layers as a natural regularizer."
    },
    {
      "title": "Differentiable Plasticity: Training Plastic Neural Networks with Backpropagation",
      "authors": "Thomas Miconi et al.",
      "year": 2018,
      "arxiv_id": "arXiv:1804.02464",
      "role": "Gap Identification",
      "relationship_sentence": "This work demonstrated the importance of neural plasticity for RL but implemented it via synaptic rules, highlighting a gap that the current paper fills by realizing plasticity through adaptive activation functions instead."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
      "authors": "Kaiming He et al.",
      "year": 2015,
      "arxiv_id": "arXiv:1502.01852",
      "role": "Baseline",
      "relationship_sentence": "Parametric ReLU from this paper serves as a principal learnable-activation baseline that the proposed joint-rational activations aim to surpass in flexibility and stability for RL."
    }
  ],
  "synthesis_narrative": "Rational activation research established that activations parameterized as ratios of polynomials can flexibly approximate diverse nonlinearities, with Boull\u00e9 and Townsend showing learnable rational units achieve strong function approximation properties and stability, while Pad\u00e9 Activation Units demonstrated practical end-to-end training of such rational forms. Earlier work on learnable activations, such as Adaptive Piecewise Linear units, validated that making activation shapes task-adaptive improves learning but also revealed the tendency to overfit when flexibility is unconstrained. Meta-ACON further explored customization and parameter sharing of activation behavior across channels or layers, indicating that tying activation parameters can regularize expressivity while preserving adaptability. In parallel, the importance of neural plasticity for nonstationary reinforcement learning was highlighted by Differentiable Plasticity, which introduced trainable, Hebbian-like synaptic updates to enable rapid adaptation. Finally, residual networks formalized the utility of residual connections, motivating analysis of how activation classes behave under residual compositions. Together, these works revealed a clear opportunity: rational activations offer the right expressive family for adaptive neurons, but they require principled regularization and compatibility with residual architectures to avoid instability and overfitting. The current paper synthesizes these insights by introducing jointly-parameterized rational activations that share coefficients across layers for natural regularization, and by deriving specific closure conditions under residual connections, thereby operationalizing neuron-level plasticity in deep RL without the complexity of explicit synaptic plasticity rules.",
  "target_paper": {
    "title": "Adaptive Rational Activations to Boost Deep Reinforcement Learning",
    "authors": "Quentin Delfosse, Patrick Schramowski, Martin Mundt, Alejandro Molina, Kristian Kersting",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Deep Reinforcement Learning, Neural Plasticity, Activation Functions, Rational Functions",
    "abstract": "Latest insights from biology show that intelligence not only emerges from the connections between neurons, but that individual neurons shoulder more computational responsibility than previously anticipated. Specifically, neural plasticity should be critical in the context of constantly changing reinforcement learning (RL) environments, yet current approaches still primarily employ static activation functions. In this work, we motivate the use of adaptable activation functions in RL and show that rational activation functions are particularly suitable for augmenting plasticity. Inspired by residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version. The proposed joint-rational activation allows for desirable degrees of flexibility, yet regularises plasticity to an extent that avoids overfitting by leveraging a mutual set of activation function parameters across layers. We demonstrate that equippi",
    "openreview_id": "g90ysX1sVs",
    "forum_id": "g90ysX1sVs"
  },
  "analysis_timestamp": "2026-01-07T00:12:47.812590"
}
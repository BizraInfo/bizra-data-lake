{
  "prior_works": [
    {
      "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity",
      "authors": "Namhoon Lee et al.",
      "year": 2019,
      "arxiv_id": "1810.02340",
      "role": "Extension",
      "relationship_sentence": "SalUn repurposes SNIP\u2019s gradient-based connection sensitivity into a per-weight saliency score computed on the forget set, and then limits unlearning updates to these high-saliency parameters."
    },
    {
      "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
      "authors": "Karen Simonyan et al.",
      "year": 2013,
      "arxiv_id": "1312.6034",
      "role": "Inspiration",
      "relationship_sentence": "The use of gradients as saliency signals for attribution directly inspires SalUn\u2019s core idea of gradient-based weight saliency to target parameters most responsible for the forget data."
    },
    {
      "title": "Machine Unlearning",
      "authors": "Adrien Bourtoule et al.",
      "year": 2021,
      "arxiv_id": "1912.03817",
      "role": "Foundation",
      "relationship_sentence": "SISA formalizes the unlearning problem and establishes retraining-after-removal as the gold standard that SalUn explicitly aims to approximate efficiently and uses for evaluation."
    },
    {
      "title": "Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks",
      "authors": "Agastya Golatkar et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This Fisher/noise-based weight-space unlearning shows selective forgetting is possible but suffers accuracy and stability issues that SalUn addresses by concentrating updates on salient weights rather than perturbing the full parameter space."
    },
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh et al.",
      "year": 2017,
      "arxiv_id": "1703.04730",
      "role": "Gap Identification",
      "relationship_sentence": "Influence-function\u2013based unlearning quantifies training-point effects but is computationally heavy and unstable at scale, motivating SalUn\u2019s tractable gradient-based weight saliency as a practical alternative for localizing forgetting."
    },
    {
      "title": "Ablating Concepts in Text-to-Image Diffusion Models",
      "authors": "Kumari et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This concept-erasure baseline for diffusion models fine-tunes against target concepts but often over-forgets and harms unrelated content, a failure SalUn mitigates by updating only salient parameters tied to the forget set."
    },
    {
      "title": "Erasing Concepts from Diffusion Models",
      "authors": "Gandikota et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "As a primary generative unlearning baseline, this method erases concepts via targeted fine-tuning yet lacks stability and generality, directly motivating SalUn\u2019s saliency-guided, principled procedure that scales across classification and generation."
    }
  ],
  "synthesis_narrative": "Gradient signals have long been used as attribution measures: Simonyan et al. showed that taking gradients of class scores with respect to inputs yields saliency maps that pinpoint responsible features, establishing gradients as a practical saliency proxy. In parallel, SNIP demonstrated that gradients can quantify connection sensitivity, enabling single-shot identification of critical weights via a gradient-based score, a parameter-centric saliency concept closely tied to training loss. The unlearning problem was formalized by Bourtoule et al. with SISA, which defined efficient unlearning protocols and positioned retraining-after-removal as the gold standard target for approximate unlearning methods. Golatkar et al. then pursued selective forgetting directly in weight space using Fisher information and noise injection, showing feasibility but exposing accuracy and stability weaknesses when perturbing the full parameter set. On the data-influence side, Koh and Liang\u2019s influence functions quantified how specific training examples affect parameters and predictions, but incurred computational and stability burdens in deep networks. In generative models, Kumari et al. and Gandikota et al. erased concepts from diffusion models via fine-tuning, but these methods frequently over-forgot and degraded unrelated content.\nTaken together, these works suggested a gap: practical, stable unlearning needed a way to localize updates to just the parameters most tied to the forget data, while matching the retrain-from-scratch ideal across both classifiers and diffusion generators. SalUn synthesizes gradient-based saliency from attribution and pruning with the unlearning objective, computing weight saliency on the forget set and restricting updates to salient parameters, thereby improving accuracy, stability, and cross-domain applicability toward the SISA ideal.",
  "target_paper": {
    "title": "SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation",
    "authors": "Chongyu Fan, Jiancheng Liu, Yihua Zhang, Eric Wong, Dennis Wei, Sijia Liu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Machine unlearning, generative model, diffusion model, weight saliency",
    "abstract": "With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often suffer limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' for MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting data points). To the best of our knowledge, SalUn is the first principled MU approach that can effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation tasks. As highlighted below,",
    "openreview_id": "gn0mIhQGNM",
    "forum_id": "gn0mIhQGNM"
  },
  "analysis_timestamp": "2026-01-06T19:39:49.015565"
}
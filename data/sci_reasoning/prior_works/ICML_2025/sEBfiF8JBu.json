{
  "prior_works": [
    {
      "title": "Lost in the Middle: How Language Models Use Long Context",
      "authors": "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang",
      "year": 2023,
      "role": "Long-context vulnerability analysis",
      "relationship_sentence": "Established systematic position and recency biases in long contexts, directly motivating PANDAS\u2019s strategy of leveraging extended fabricated dialogues and its attention analysis to exploit long-context weaknesses."
    },
    {
      "title": "Rethinking the Role of Demonstrations: Are Labels Necessary for In-context Learning?",
      "authors": "Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi",
      "year": 2022,
      "role": "Mechanisms of demonstration-driven behavior",
      "relationship_sentence": "Showed that models mimic patterns from demonstrations, informing PANDAS\u2019s Positive Affirmations and Negative Demonstrations as principled in-context steering mechanisms for jailbreak success."
    },
    {
      "title": "Learning to Retrieve Prompts for In-Context Learning",
      "authors": "Ohad Rubin, Jonathan Herzig, Jonathan Berant",
      "year": 2022,
      "role": "Example selection for ICL",
      "relationship_sentence": "Provided the foundation for query-aware exemplar selection, directly inspiring PANDAS\u2019s Adaptive Sampling that conditions fabricated dialogues on the target prompt\u2019s topic."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou, Zifan Wang, J Zico Kolter, Matt Fredrikson",
      "year": 2023,
      "role": "Baseline adversarial jailbreak methodology",
      "relationship_sentence": "Introduced robust adversarial prompt attacks (e.g., GCG), serving as a key baseline and demonstrating the need for more powerful, generalizable jailbreak methods that PANDAS advances in long-context settings."
    },
    {
      "title": "Red Teaming Language Models with Language Models",
      "authors": "Eric Perez, Sam Ringer, Kamil Staniszewski, et al.",
      "year": 2022,
      "role": "Harmful prompt generation and evaluation pipeline",
      "relationship_sentence": "Pioneered automated red-teaming to curate harmful interactions, informing PANDAS\u2019s construction of ManyHarm and its systematic evaluation of safety circumvention."
    },
    {
      "title": "HarmBench: A Standardized Evaluation of Harmful Behaviors in Language Models",
      "authors": "Alexander Mazeika, Tom Brown, Eric Wallace, et al.",
      "year": 2024,
      "role": "Safety benchmarking and harmful QA corpora",
      "relationship_sentence": "Provided standardized harmful queries and evaluation practices, guiding PANDAS\u2019s dataset design (ManyHarm) and experimental protocol for assessing jailbreak efficacy."
    },
    {
      "title": "Role-Playing to Jailbreak Large Language Models",
      "authors": "Wei Zeng, Tianjin Huang, et al.",
      "year": 2024,
      "role": "Persona/affirmation-based jailbreak prompting",
      "relationship_sentence": "Demonstrated that affirmative persona framing increases unsafe compliance, directly motivating PANDAS\u2019s Positive Affirmations component to systematize and amplify this effect within many-shot attacks."
    }
  ],
  "synthesis_narrative": "PANDAS builds on two converging lines of work: the mechanics of in-context learning and the evolving toolkit of jailbreak attacks, with a special focus on long-context vulnerabilities. From the ICL side, Min et al. showed that models strongly imitate patterns in demonstrations, a key insight PANDAS operationalizes via Positive Affirmations and Negative Demonstrations to steer model behavior toward compliance despite safety training. Complementing this, Rubin et al.\u2019s retrieval-based exemplar selection motivates PANDAS\u2019s Adaptive Sampling, which conditions the many-shot fabrications on the target topic to maximize in-context influence.\nLong-context works shape PANDAS\u2019s core setting and analysis. Liu et al.\u2019s Lost in the Middle exposes recency and positional biases that PANDAS exploits by crafting extended fabricated dialogues and validates via attention analysis, explaining why strategically placed affirmations and demonstrations override safety alignment. On the attack/benchmark front, Zou et al.\u2019s universal adversarial prompting (GCG) establishes strong baselines and the need for robust, generalizable jailbreak techniques; PANDAS addresses this by leveraging length and curated demonstrations rather than model gradients. Red-teaming frameworks (Perez et al.) and standardized safety benchmarks (HarmBench) directly inform ManyHarm\u2019s construction and PANDAS\u2019s evaluation methodology. Finally, role-playing jailbreak studies reveal that affirmative persona framing increases unsafe compliance, an effect PANDAS formalizes and amplifies through its Positive Affirmations module. Together, these works directly enable PANDAS\u2019s hybrid design\u2014demonstration-driven steering, topic-aware sampling, and long-context exploitation\u2014to substantially advance many-shot jailbreaking performance.",
  "analysis_timestamp": "2026-01-07T00:05:12.563343"
}
{
  "prior_works": [
    {
      "title": "Tensor Decompositions and Applications",
      "authors": "Tamara G. Kolda; Brett W. Bader",
      "year": 2009,
      "role": "Foundational CP/Tucker tensor factorization theory and algorithms",
      "relationship_sentence": "OFTD builds on the CP model formalized in Kolda & Bader, using CP factors as the structural backbone that the proposed implicit neural representation parameterizes continuously over space-time."
    },
    {
      "title": "Subspace Learning and Imputation for Streaming Big Data Matrices and Tensors",
      "authors": "Morteza Mardani; Gonzalo Mateos; Georgios B. Giannakis",
      "year": 2015,
      "role": "Online/streaming CP tensor completion framework",
      "relationship_sentence": "This work provides the online CP decomposition and streaming tensor completion paradigm that OFTD reinterprets in functional form, replacing explicit factor updates with continual INR weight updates."
    },
    {
      "title": "Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
      "authors": "Matthew Tancik; Pratul P. Srinivasan; Ben Mildenhall; et al.",
      "year": 2020,
      "role": "Core INR technique for coordinate-based signal representation",
      "relationship_sentence": "The use of coordinate-based networks with frequency mappings underpins OFTD\u2019s functional view of tensors, enabling continuous spatiotemporal modeling and easy extension to newly arriving coordinates."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions (SIREN)",
      "authors": "Vincent Sitzmann; Julien N. P. Martel; Alexander W. Bergman; David B. Lindell; Gordon Wetzstein",
      "year": 2020,
      "role": "INR architecture enabling smooth, high-fidelity continuous signals",
      "relationship_sentence": "SIREN\u2019s demonstration that MLPs can accurately parameterize continuous signals informs OFTD\u2019s choice to encode CP factors as INRs and motivates exploiting their local continuity in the replay strategy."
    },
    {
      "title": "TensoRF: Tensorial Radiance Fields",
      "authors": "Anpei Chen; Zexiang Xu; Andreas Geiger; Jingyi Yu; Hao Su",
      "year": 2022,
      "role": "Bridges tensor factorizations with neural fields",
      "relationship_sentence": "By fusing tensor factorization with neural field representations, TensoRF directly inspires OFTD\u2019s coupling of CP structure and coordinate-based networks for efficient continuous field modeling."
    },
    {
      "title": "Gradient Episodic Memory for Continual Learning",
      "authors": "David Lopez-Paz; Marc\u2019Aurelio Ranzato",
      "year": 2017,
      "role": "Replay-based continual learning to mitigate forgetting",
      "relationship_sentence": "GEM\u2019s memory-based constraints motivate OFTD\u2019s continual learning view of online tensor updates and inform the design of a replay mechanism to protect prior tensor knowledge."
    },
    {
      "title": "Experience Replay for Continual Learning",
      "authors": "David Rolnick; Arun Ahuja; Jonathan Schwarz; Timothy Lillicrap; Greg Wayne",
      "year": 2019,
      "role": "General rehearsal framework and sample selection for CL",
      "relationship_sentence": "OFTD\u2019s long-tail memory replay adapts the rehearsal principle from this work to the INR setting, selecting replay samples that respect local continuity to reduce interference while updating on streams."
    }
  ],
  "synthesis_narrative": "The core innovation of OFTD is to recast online CP tensor decomposition as a continual learning problem over a continuous spatiotemporal function parameterized by implicit neural representations (INRs), paired with a tailored long-tail replay strategy. Foundationally, Kolda and Bader\u2019s formalization of CP provides the structural scaffold that OFTD retains while shifting from discrete factor updates to functional parameterization. Prior online/streaming tensor completion, exemplified by Mardani\u2013Mateos\u2013Giannakis, establishes the algorithmic objective of updating factors as new slices arrive; OFTD inherits this streaming goal but implements it via weight updates to a coordinate network that embodies the CP factors as continuous functions.\nAdvances in INRs make this functionalization viable. Tancik et al.\u2019s Fourier features and Sitzmann et al.\u2019s SIREN show that coordinate-based MLPs can represent complex, high-frequency continuous signals, enabling OFTD to model spatiotemporal fields continuously and to naturally accommodate expanding streams by simply evaluating new coordinates. TensoRF further bridges tensor factorization and neural fields, demonstrating that low-rank tensor structures synergize with neural representations; OFTD extends this synergy to an online/continual setting by functionally encoding CP factors.\nFinally, continual learning via replay\u2014pioneered by GEM and generalized by Experience Replay\u2014directly motivates OFTD\u2019s shift from classical online factor updates to rehearsal-based weight updates that prevent forgetting. OFTD\u2019s long-tail memory replay adapts these ideas to the INR\u2019s local continuity, selecting and weighting replay samples to balance stability and plasticity during streaming tensor completion.",
  "analysis_timestamp": "2026-01-07T00:21:32.280308"
}
{
  "prior_works": [
    {
      "title": "Orthogonal negation in vector spaces for modeling word-meaning and document retrieval",
      "authors": "Dominic Widdows",
      "year": 2003,
      "role": "Foundation",
      "relationship_sentence": "Widdows mapped Boolean logic (NOT/OR) to concrete vector-space operations (orthogonal complement and additive superposition), directly motivating this paper\u2019s treatment of classes as vectors with set-theoretic operators realized by linear algebra and the existence of a special \u2018zero\u2019 element."
    },
    {
      "title": "Holographic Reduced Representations",
      "authors": "Tony A. Plate",
      "year": 1995,
      "role": "Inspiration",
      "relationship_sentence": "Plate\u2019s vector-symbolic architecture showed how sets and composites can be represented by single vectors using superposition (vector addition), inspiring the paper\u2019s core idea that union-like set operations on classes correspond to vector arithmetic and that there is an additive identity (the Metta/zero class)."
    },
    {
      "title": "Prototypical Networks for Few-shot Learning",
      "authors": "Jake Snell et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Prototypical Networks established the practice of representing a class by a single vector (a prototype) in an embedding space; the present work generalizes this to a full vector space of classes with well-defined algebra (addition/subtraction, complement) and a zero-vector class."
    },
    {
      "title": "iCaRL: Incremental Classifier and Representation Learning",
      "authors": "Sylvestre-Alvise Rebuffi et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "iCaRL uses class means and exemplars for incremental learning but suffers from exemplar dependence and forgetting; this paper\u2019s class-as-vector algebra (with a zero class) is proposed to enable continual learning via composable class vectors without heavy replay."
    },
    {
      "title": "Estimating the Support of a High-Dimensional Distribution",
      "authors": "Bernhard Sch\u00f6lkopf et al.",
      "year": 2001,
      "role": "Baseline",
      "relationship_sentence": "One-Class SVM formalized unary class learning as boundary (support) estimation; the new \u2018unary class learning\u2019 reframes this by learning the class manifold as a vector-class object and addressing the OCSVM limitation of focusing on decision boundaries rather than the intrinsic class structure."
    },
    {
      "title": "Diffusion Models Beat GANs on Image Synthesis",
      "authors": "Prafulla Dhariwal et al.",
      "year": 2021,
      "role": "Related Problem",
      "relationship_sentence": "Classifier guidance composes class gradients additively (enabling AND/NOT-like behavior), directly inspiring the paper\u2019s claim that set/Boolean operations on classes can be realized as algebra on class vectors and supporting the \u2018classifier as generator\u2019 application."
    },
    {
      "title": "Visualizing Higher-Layer Features of a Deep Network",
      "authors": "Dumitru Erhan et al.",
      "year": 2009,
      "role": "Inspiration",
      "relationship_sentence": "Activation maximization demonstrated that classifiers can act generatively by producing representative samples, underpinning the paper\u2019s \u2018classifier as generator\u2019 view that emerges naturally when classes are explicit vectors defining a data manifold and a zero-class reference."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core move\u2014treating classes themselves as elements of a vector space with algebra mirroring set/Boolean operations and identifying a zero-vector \u2018Metta-Class\u2019\u2014sits at the intersection of logical vector semantics and practical class representations in ML. Widdows\u2019 vector-space logic provided the conceptual foundation that logical operators (NOT/OR) can be implemented as linear-algebraic transforms, suggesting that set operations on classes need not be external to learning. Plate\u2019s holographic reduced representations further demonstrated that collections and compositions can be encoded by single vectors via superposition, furnishing a concrete algebra where addition corresponds to combining symbolic content\u2014precisely the intuition this paper extends to class union and complement. On the ML side, Prototypical Networks established that a class can be faithfully represented by a vector in an embedding space; this work elevates that idea into a full algebra over classes, identifying an additive identity (zero class) and enabling subtraction/inversion. The unary classification lineage from One-Class SVM exposes the boundary-centric limitation that the new \u2018clear learning\u2019 explicitly targets: learning a class manifold rather than merely its support. Continual learning pressures, highlighted by iCaRL\u2019s reliance on exemplars and vulnerability to forgetting, motivate a compositional calculus over class vectors for incremental addition/removal. Finally, classifier guidance in diffusion and activation maximization show classifiers can act as generators and that class signals compose additively\u2014empirical evidence that Boolean-like operations on classes align with vector arithmetic, supporting the proposed class-vector framework.",
  "analysis_timestamp": "2026-01-06T23:07:19.603760"
}
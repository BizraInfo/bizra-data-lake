{
  "prior_works": [
    {
      "title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping",
      "authors": "Scott Reed et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "ULAREF generalizes bootstrapping\u2019s core idea of replacing unreliable hard labels with refined soft targets to a unified refinement mechanism that works across multiple inaccurate supervision regimes and is guided by explicit reliability detection."
    },
    {
      "title": "DivideMix: Learning with Noisy Labels as Semi-supervised Learning",
      "authors": "Junnan Li et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "ULAREF extends DivideMix\u2019s global clean/noisy separation via per-sample loss modeling into a general reliability detector usable beyond noisy-label settings, and replaces semi-supervised pseudo-labeling with a principled local label enhancement step."
    },
    {
      "title": "Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels",
      "authors": "Bo Han et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "ULAREF addresses Co-teaching\u2019s limitation of relying solely on small-loss selection under the noisy-label setting by unifying reliability detection with label refinement, rather than discarding or sidelining suspected noisy samples."
    },
    {
      "title": "Learning from Complementary Labels",
      "authors": "Takuya Ishida et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "ULAREF builds on the complementary-label learning formulation by interpreting complementary annotations as partial evidence and refining them into probabilistic labels within its unified framework."
    },
    {
      "title": "Learning from Partial Labels",
      "authors": "Timoth\u00e9e Cour et al.",
      "year": 2011,
      "role": "Foundation",
      "relationship_sentence": "ULAREF adopts the partial-label learning problem definition and turns candidate label sets into refined label distributions via its local enhancement module, enabling a single treatment across supervision types."
    },
    {
      "title": "Facial Age Estimation by Learning from Label Distributions",
      "authors": "Xin Geng et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "ULAREF\u2019s refined-label training is grounded in the label distribution learning principle introduced by Geng et al., which treats supervision as distributions over labels rather than single hard labels."
    },
    {
      "title": "Learning with Local and Global Consistency",
      "authors": "Dengyong Zhou et al.",
      "year": 2004,
      "role": "Inspiration",
      "relationship_sentence": "ULAREF\u2019s local label enhancement leverages neighborhood consistency to propagate and smooth label confidence, echoing the local/global consistency framework for label propagation."
    }
  ],
  "synthesis_narrative": "ULAREF\u2019s core innovation\u2014training with refined labels via global reliability detection and local enhancement\u2014emerges from two converging lines of work. First, in noisy-label learning, bootstrapping demonstrated that replacing unreliable hard labels with soft, model-informed targets can markedly stabilize training; ULAREF generalizes this refinement notion beyond noisy labels to disparate inaccurate annotations. Co-teaching and DivideMix then operationalized a crucial prerequisite: identifying reliable examples with small-loss criteria and loss modeling. ULAREF abstracts this into a general global reliability detector not tied to any single supervision type, and substitutes DivideMix\u2019s semi-supervised pseudo-labeling with a dedicated local enhancement step that directly improves label quality.\nSecond, the representation of supervision as graded label distributions rather than single labels\u2014pioneered by label distribution learning\u2014provides the conceptual basis for ULAREF\u2019s refined labels. To realize local enhancement, ULAREF draws on the neighborhood-consistency principle of label propagation (local and global consistency), using sample locality to denoise and enrich label signals.\nFinally, the unified scope is anchored in problem formulations for specific inaccurate supervisions: partial-label learning (Cour et al.) and complementary-label learning (Ishida et al.). ULAREF interprets these annotations as partial evidence about a latent label distribution and refines them within a single framework. Together, these works directly shape ULAREF\u2019s design: detect reliability globally, refine labels locally into distributions, and thereby unify learning across heterogeneous inaccurate supervision.",
  "analysis_timestamp": "2026-01-06T23:09:26.399719"
}
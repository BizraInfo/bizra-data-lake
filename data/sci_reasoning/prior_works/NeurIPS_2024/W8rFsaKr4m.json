{
  "prior_works": [
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "authors": "Albert Gu, Karan Goel, Christopher R\u00e9",
      "year": 2022,
      "role": "Foundational SSM architecture for long-sequence modeling",
      "relationship_sentence": "Established the core state space formulation and linear-time recurrence that MambaTree retains while replacing the 1D sequential propagation geometry with a tree to alleviate sequence-geometry bottlenecks."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "authors": "Albert Gu, Tri Dao",
      "year": 2024,
      "role": "Direct predecessor introducing selective SSM and hardware-friendly scan",
      "relationship_sentence": "MambaTree builds directly on Mamba\u2019s selective SSM dynamics and efficient scan, but generalizes the propagation topology from a fixed sequence to a dynamically induced tree to improve long-range interactions."
    },
    {
      "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
      "authors": "Kai Sheng Tai, Richard Socher, Christopher D. Manning",
      "year": 2015,
      "role": "Tree-structured recurrence demonstrating benefits over chains",
      "relationship_sentence": "Showed that propagating information along tree topologies captures hierarchical, long-range dependencies better than linear chains; MambaTree imports this insight to SSMs by performing state propagation on learned trees."
    },
    {
      "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon (PRPN)",
      "authors": "Yikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio",
      "year": 2018,
      "role": "Latent tree induction from input features",
      "relationship_sentence": "Pioneered learning input-dependent tree structures to guide computation; MambaTree similarly derives a data-driven tree (from spatial relationships and features) to route SSM propagation beyond rigid sequences."
    },
    {
      "title": "Dynamic Graph CNN for Learning on Point Clouds",
      "authors": "Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, Justin M. Solomon",
      "year": 2019,
      "role": "Dynamic topology construction from feature space",
      "relationship_sentence": "Introduced feature-driven, layer-wise graph construction (k-NN) to support message passing; MambaTree echoes this by dynamically generating a topology\u2014specialized to trees\u2014using spatial and feature cues for efficient propagation."
    },
    {
      "title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "authors": "Judea Pearl",
      "year": 1988,
      "role": "Linear-time message passing on trees via belief propagation",
      "relationship_sentence": "Provides the classical template for two-pass, linear-time inference on trees; MambaTree\u2019s linear-complexity dynamic programming for enhancing long-range interactions is conceptually aligned with tree message passing."
    }
  ],
  "synthesis_narrative": "MambaTree\u2019s core contribution\u2014replacing the rigid 1D scan of state space models with a dynamically induced tree and augmenting it with linear-time dynamic programming\u2014sits at the intersection of advances in SSMs, tree-structured computation, and dynamic topology learning. The S4 framework established the modern, efficient SSM formulation for long sequences, while Mamba contributed selective, input-dependent SSM dynamics and hardware-friendly scan routines; MambaTree preserves these SSM benefits but removes the sequential geometry bottleneck by changing the propagation substrate.\nTreeLSTM demonstrated that routing information along trees better captures hierarchical and long-range dependencies than chain models, and PRPN showed that such trees can be induced from input features rather than provided externally. MambaTree carries these ideas into the SSM regime by learning a data-driven tree from spatial relations and features, then performing state propagation on this structure.\nOn the topology side, DGCNN\u2019s dynamic, feature-based graph construction informs MambaTree\u2019s strategy of recomputing connectivity conditioned on evolving representations\u2014here specialized to yield a sparse, efficient tree. Finally, the paper\u2019s linear-complexity dynamic programming over the induced tree echoes classical belief propagation on trees: a two-pass message-passing scheme that delivers global, long-range interactions without quadratic cost. Together, these works directly inform MambaTree\u2019s design: selective SSM dynamics executed over learned tree topologies with linear-time tree DP, yielding stronger long-range modeling for vision and language.",
  "analysis_timestamp": "2026-01-06T23:33:35.570344"
}
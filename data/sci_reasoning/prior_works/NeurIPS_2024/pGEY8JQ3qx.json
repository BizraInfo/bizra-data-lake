{
  "prior_works": [
    {
      "title": "REGAL: A Regularization Based Algorithm for Reinforcement Learning in Weakly Communicating MDPs",
      "authors": "Peter L. Bartlett, Ambuj Tewari",
      "year": 2009,
      "role": "Span-based analysis for average-reward RL",
      "relationship_sentence": "Introduced the bias-span sp(h*) as the key structural parameter for weakly communicating average-reward MDPs and tied learning difficulty to span rather than diameter, directly motivating this paper\u2019s span-H parameterization and analysis."
    },
    {
      "title": "Near-Optimal Regret Bounds for Reinforcement Learning",
      "authors": "Thomas Jaksch, Ronald Ortner, Peter Auer",
      "year": 2010,
      "role": "Diameter-based benchmark for undiscounted MDPs",
      "relationship_sentence": "Established UCRL2 and regret bounds in terms of the MDP diameter D, providing the classical baseline that this work improves upon by achieving span-based (H) dependence and optimal SA\u00b7H/\u03b5^2 sample complexity under a generative model."
    },
    {
      "title": "Bias-Span-Constrained Exploration in Average-Reward Reinforcement Learning (SCAL/SCAL+)",
      "authors": "Michel Fruit, Matteo Pirotta, Alessandro Lazaric",
      "year": 2018,
      "role": "Operationalized span in average-reward regret analyses",
      "relationship_sentence": "Showed algorithms and regret guarantees controlled by the optimal bias span in average-reward MDPs, reinforcing span as the right complexity measure that this paper leverages to obtain minimax PAC bounds with a simulator."
    },
    {
      "title": "On the Sample Complexity of Reinforcement Learning with a Generative Model",
      "authors": "Mohammad Gheshlaghi Azar, R\u00e9mi Munos, Hilbert J. Kappen",
      "year": 2013,
      "role": "Minimax PAC bounds with a simulator (discounted case)",
      "relationship_sentence": "Established near-optimal SA/((1\u2212\u03b3)^3 \u03b5^2) sample complexity under a generative model and introduced analysis tools (plug-in estimators, variance-aware concentration) that inform this paper\u2019s PAC methodology in the average-reward setting."
    },
    {
      "title": "Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model",
      "authors": "Aaron Sidford, Mengdi Wang, Xian Wu, Yinyu Ye",
      "year": 2018,
      "role": "Advanced generative-model techniques and analysis",
      "relationship_sentence": "Developed efficient generative-model algorithms and refined concentration/variance reduction techniques that underpin modern PAC analyses, methods that this paper adapts to derive tight SA\u00b7H/\u03b5^2 and SA\u00b7(B+H)/\u03b5^2 bounds."
    },
    {
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "authors": "Martin L. Puterman",
      "year": 1994,
      "role": "Foundations of average-reward MDPs",
      "relationship_sentence": "Provided the formal framework for average-reward MDPs, the bias (differential value) function and its span, and the weakly communicating/multichain classifications that are central to the definitions of H and the new transient parameter B in this work."
    },
    {
      "title": "Sample Complexity of Reinforcement Learning in Stochastic Shortest Path Problems",
      "authors": "Jean Tarbouriech, Matteo Pirotta, Alessandro Lazaric, Marek Petrik",
      "year": 2020,
      "role": "Transient-time parameterization in SSP",
      "relationship_sentence": "Identified episode/transient-length parameters controlling generative-model sample complexity in SSPs, inspiring this paper\u2019s introduction of a necessary transient-time parameter B for general multichain average-reward MDPs and the resulting SA\u00b7(B+H)/\u03b5^2 bounds."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014minimax-optimal sample complexity SA\u00b7H/\u03b5^2 for weakly communicating average-reward MDPs and SA\u00b7(B+H)/\u03b5^2 for general multichain MDPs under a generative model\u2014rests on two intertwined lines of prior work. First, average-reward structure and the centrality of the bias function and its span trace back to Puterman\u2019s foundational treatment, while REGAL (Bartlett & Tewari) crystallized the span sp(h*) as the right problem parameter in weakly communicating MDPs. Subsequent algorithms such as SCAL/SCAL+ (Fruit, Pirotta, Lazaric) operationalized span-based guarantees, demonstrating that span\u2014not diameter\u2014captures learning difficulty in average-reward settings. In parallel, the generative-model (simulator) PAC tradition from Azar\u2013Munos\u2013Kappen and Sidford\u2013Wang\u2013Wu\u2013Ye established near-optimal SA/\u03b5^2-type rates for discounted MDPs, along with plug-in estimation, variance-aware concentration, and computational techniques that this paper adapts to the average-reward regime. UCRL2 (Jaksch, Ortner, Auer) serves as the historical benchmark with diameter-based regret; the present work advances beyond diameter dependencies by proving span-optimal PAC bounds. Finally, insights from stochastic shortest path (Tarbouriech et al.)\u2014where transient-time parameters determine sample complexity\u2014motivate and justify the introduction of a new transient parameter B for multichain average-reward MDPs, leading to sharp SA\u00b7(B+H)/\u03b5^2 bounds and matching lower bounds. Together, these works directly inform the paper\u2019s span-based parameterization, simulator-based PAC analysis, and the necessity of a transient-time term in the general multichain case.",
  "analysis_timestamp": "2026-01-06T23:33:36.261479"
}
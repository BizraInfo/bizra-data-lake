{
  "prior_works": [
    {
      "title": "Retinex-Net: A Deep Retinex Decomposition for Low-Light Enhancement",
      "authors": "Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu",
      "year": 2018,
      "role": "Physical-model-driven restoration and data formulation",
      "relationship_sentence": "ControlFusion\u2019s degraded imaging model leverages Retinex-style illumination\u2013reflectance decomposition to simulate and counter low-light components within composite degradations, directly inheriting the Retinex-Net idea of learning on physically meaningful factors."
    },
    {
      "title": "Single Image Haze Removal Using Dark Channel Prior",
      "authors": "Kaiming He, Jian Sun, Xiaoou Tang",
      "year": 2011,
      "role": "Atmospheric scattering model for haze/fog degradation",
      "relationship_sentence": "The atmospheric scattering principle underlying Dark Channel Prior provides the physics used by ControlFusion to model and parameterize haze-related components in its composite degradation simulator and prompt space."
    },
    {
      "title": "U2Fusion: A Unified Unsupervised Image Fusion Network",
      "authors": "Hui Li, Xiao-Jun Wu, Jiayi Ma",
      "year": 2020,
      "role": "Foundational deep image fusion paradigm",
      "relationship_sentence": "ControlFusion extends the unified fusion philosophy of U2Fusion by introducing degradation-aware, user-controllable guidance; it builds on unsupervised fusion training and general-purpose fusion backbones while adding prompt-modulated restoration/fusion."
    },
    {
      "title": "FiLM: Visual Reasoning with a General Conditioning Layer",
      "authors": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron Courville",
      "year": 2018,
      "role": "Feature-wise modulation conditioned on language/auxiliary signals",
      "relationship_sentence": "ControlFusion\u2019s prompt-modulated restoration and fusion blocks directly echo FiLM\u2019s feature-wise linear modulation, using prompt embeddings to dynamically scale/shift features according to degradation types and levels."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, et al.",
      "year": 2021,
      "role": "Language\u2013vision alignment and text encoding",
      "relationship_sentence": "By adopting a text encoder to embed user-defined degradation prompts, ControlFusion builds on CLIP\u2019s principle that natural language can parameterize visual behavior, enabling controllable restoration/fusion via textual descriptions."
    },
    {
      "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration",
      "authors": "Syed Waqas Zamir, Aditya Arora, Salman Khan, et al.",
      "year": 2022,
      "role": "Backbone design for handling diverse/composite degradations",
      "relationship_sentence": "ControlFusion\u2019s restoration pathway draws on transformer-based restoration design choices popularized by Restormer (hierarchical attention, long-range modeling), then augments them with prompt-driven gating to adapt to mixed degradations."
    },
    {
      "title": "FcaNet: Frequency Channel Attention Networks",
      "authors": "Qilong Wang, Bang Zhang, Yukang Chen, et al.",
      "year": 2021,
      "role": "Spatial\u2013frequency representation and attention",
      "relationship_sentence": "The spatial\u2013frequency collaborative visual adapter in ControlFusion is motivated by FcaNet\u2019s insight that explicitly modeling frequency responses improves representation, enabling the adapter to better perceive and localize degradations across bands."
    }
  ],
  "synthesis_narrative": "ControlFusion\u2019s core contribution\u2014controllable, degradation-aware image fusion guided by language\u2013vision prompts\u2014emerges from unifying physical imaging priors, cross-modal conditioning, and modern restoration/fusion architectures. On the physics side, Retinex-Net operationalizes illumination\u2013reflectance decomposition, while the Dark Channel Prior formalizes atmospheric scattering; together they furnish the principled degradation factors needed to simulate real composite artifacts and to structure prompt variables around interpretable parameters (illumination, transmission, airlight). Building on robust fusion foundations, U2Fusion\u2019s unified training and modality-agnostic backbone inform ControlFusion\u2019s base fusion design, which is then extended to be degradation-adaptive. The mechanism for control derives from conditioning paradigms: FiLM demonstrates that feature-wise linear modulation can inject external semantics into intermediate features, a template ControlFusion uses to translate degradation prompts into dynamic adjustments of restoration and fusion pathways. CLIP contributes the language\u2013vision alignment and reliable text embeddings that let user-specified degradation types and severities be expressed as vectors that the network can act upon. On the architectural side, Restormer\u2019s transformer-based restoration shows how to robustly handle diverse and composite degradations at high resolution; ControlFusion integrates similar hierarchical attention while making it prompt-aware. Finally, FcaNet\u2019s frequency channel attention motivates the spatial\u2013frequency collaborative adapter, enabling the system to autonomously sense degradation patterns across spatial and frequency domains, closing the loop between prompt intention and data-driven degradation perception.",
  "analysis_timestamp": "2026-01-06T23:42:48.150506"
}
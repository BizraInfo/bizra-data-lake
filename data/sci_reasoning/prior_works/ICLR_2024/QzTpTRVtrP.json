{
  "prior_works": [
    {
      "title": "BENDR: BErt-inspired Neural Data Representations for EEG",
      "authors": "Dimitrios K. I. Kostas et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "BENDR established transformer-based self-supervised pretraining for EEG but assumed fixed channel layouts and task settings, which this paper explicitly generalizes by introducing a unified, montage-agnostic pretraining framework over heterogeneous datasets."
    },
    {
      "title": "BEETL: A Benchmark for EEG Transfer Learning",
      "authors": "Dimitrios K. I. Kostas et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "BEETL formalized cross-subject and cross-dataset EEG transfer and highlighted heterogeneity in channels, lengths, and sampling rates, directly motivating the need for a unified foundation model that this work proposes."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "authors": "Kaiming He et al.",
      "year": 2022,
      "arxiv_id": "2111.06377",
      "role": "Extension",
      "relationship_sentence": "The core pretraining strategy extends MAE-style masked reconstruction to multichannel EEG with electrode-aware tokenization and variable-length handling to learn generic representations at scale."
    },
    {
      "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning",
      "authors": "Anastasia Zerveas et al.",
      "year": 2021,
      "arxiv_id": "2010.02803",
      "role": "Inspiration",
      "relationship_sentence": "This work\u2019s transformer encoder design for multivariate time series informs the architecture adapted here, which augments it with EEG-specific spatial/positional encoding to accommodate diverse montages."
    },
    {
      "title": "TS2Vec: Towards Universal Representation Learning for Time Series",
      "authors": "Zhiyuan Yang Yue et al.",
      "year": 2022,
      "arxiv_id": "2106.10466",
      "role": "Related Problem",
      "relationship_sentence": "TS2Vec\u2019s demonstration that universal time-series embeddings transfer across tasks directly motivates scaling unsupervised pretraining to EEG for broad downstream BCI tasks."
    },
    {
      "title": "Self-supervised representation learning from electroencephalography signals",
      "authors": "Alexandre Banville et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By showing that contrastive/self-supervised objectives and EEG-specific augmentations boost downstream performance, this paper catalyzes the shift toward large-scale unsupervised EEG pretraining adopted here."
    },
    {
      "title": "EEGNet: A Compact Convolutional Neural Network for EEG-based Brain\u2013Computer Interfaces",
      "authors": "Vernon J. Lawhern et al.",
      "year": 2018,
      "arxiv_id": "1611.08024",
      "role": "Gap Identification",
      "relationship_sentence": "EEGNet exemplifies highly task-specific architectures that struggle to generalize across datasets and montages, a limitation this work addresses by learning dataset-agnostic EEG representations."
    }
  ],
  "synthesis_narrative": "Early attempts to generalize EEG representations at scale emerged with BENDR, which introduced transformer-based self-supervised pretraining on large EEG corpora, demonstrating cross-task transfer but presuming fixed channel layouts and relatively homogeneous data. BEETL codified the transfer-learning problem in EEG, surfacing persistent heterogeneity\u2014mismatched electrodes, sampling rates, and trial lengths\u2014that undermines broad generalization. Parallel advances in self-supervision provided the methodological backbone: Masked Autoencoders showed that reconstructive masked modeling yields scalable, data-efficient pretraining; the Transformer-based framework for multivariate time series established viable encoder designs and patching schemes for sensor data; and TS2Vec revealed that universal time-series embeddings can transfer across diverse tasks with minimal supervision. Within EEG specifically, Banville et al. demonstrated that contrastive/self-supervised objectives with domain-tailored augmentations improve downstream BCI performance, underscoring the promise of unsupervised pretraining. Meanwhile, EEGNet remained a compact, task-specific baseline, exemplifying strong within-dataset accuracy yet limited cross-dataset robustness. Together, these works expose an opportunity: combine the scalability and masked-modeling efficiency of MAE with time-series Transformers, while directly addressing EEG\u2019s dataset heterogeneity identified by BEETL and the fixed-montage constraints evident in BENDR. The natural next step is a unified, montage-agnostic pretraining pipeline over massive, diverse EEG corpora that learns generic representations transferable to many BCI tasks, surpassing task-specific CNNs like EEGNet and building on the self-supervised insights from Banville, MAE, TST, and TS2Vec.",
  "target_paper": {
    "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
    "authors": "Weibang Jiang, Liming Zhao, Bao-liang Lu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "EEG, brain-computer interface, representation learning",
    "abstract": "The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation ",
    "openreview_id": "QzTpTRVtrP",
    "forum_id": "QzTpTRVtrP"
  },
  "analysis_timestamp": "2026-01-06T11:51:27.593076"
}
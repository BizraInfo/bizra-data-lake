{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Radford et al.",
      "year": 2021,
      "arxiv_id": "2103.00020",
      "role": "Foundation",
      "relationship_sentence": "CLIP provides the image\u2013text embedding space and ViT backbone whose strong image-level recognition CLIPSelf explicitly adapts to local regions without extra region\u2013text supervision."
    },
    {
      "title": "GLIP: Grounded Language-Image Pre-training",
      "authors": "Li et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "GLIP achieves region\u2013language alignment by large-scale phrase\u2013region grounding pretraining, and CLIPSelf is motivated to obtain comparable region alignment without any region\u2013text pairs by distilling within a CLIP ViT."
    },
    {
      "title": "Detic: Detecting Twenty-thousand Classes using Image-level Supervision",
      "authors": "Zhou et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Detic shows that CLIP text embeddings can scale detector vocabularies using only image-level labels but still requires detector training, whereas CLIPSelf directly improves CLIP ViT\u2019s region\u2013language alignment in a label-free manner."
    },
    {
      "title": "OWL-ViT: Open-Vocabulary Object Detection Using Vision Transformers",
      "authors": "Minderer et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "OWL-ViT is a primary open-vocabulary detection baseline that adapts ViTs for region localization with training, and CLIPSelf targets the same capability by aligning CLIP ViT\u2019s regional features to language without any region-level supervision."
    },
    {
      "title": "OpenSeg: Open-Vocabulary Semantic Segmentation",
      "authors": "Ghiasi et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "OpenSeg builds pixel\u2013text alignment for segmentation via masks and CLIP text embeddings, while CLIPSelf tackles the same pixel/region alignment problem by transferring CLIP\u2019s image-level semantics to local regions without mask or region\u2013text labels."
    },
    {
      "title": "Emerging Properties in Self-Supervised Vision Transformers",
      "authors": "Caron et al.",
      "year": 2021,
      "arxiv_id": "2104.14294",
      "role": "Inspiration",
      "relationship_sentence": "CLIPSelf borrows DINO\u2019s teacher\u2013student consistency across global/local views and recasts it in CLIP\u2019s language-aligned space so that a CLIP ViT can teach its regional views to align with image-level semantics."
    }
  ],
  "synthesis_narrative": "Contrastive language\u2013image pretraining established that a ViT can learn a powerful image\u2013text embedding space, and CLIP demonstrated particularly strong zero-shot image-level recognition grounded in textual concepts. GLIP showed that robust region\u2013language alignment could be obtained through large-scale phrase\u2013region grounding pretraining, directly supervising regions with text spans. Detic revealed a complementary route for open-vocabulary detection by using CLIP text embeddings as classifier weights with only image-level supervision, scaling vocabulary without region annotations but still relying on detector training. OWL\u2011ViT further adapted ViTs for open-vocabulary detection, coupling localization heads with image\u2013text pretraining to produce region-aware representations through task-specific training. For dense prediction, OpenSeg leveraged CLIP text embeddings and mask-level supervision to learn pixel\u2013text alignment for open-vocabulary segmentation. Orthogonally, DINO introduced a teacher\u2013student self-distillation mechanism that enforces consistency between global and local views, revealing that ViTs can transfer semantic signals across crops without labels. Taken together, these works exposed a clear opportunity: CLIP\u2019s strong image-level semantics were not directly transferred to local regions without resorting to region\u2013text or mask supervision, yet self-distillation offered a label-free path to propagate global knowledge to local views. CLIPSelf synthesizes these insights by instantiating a DINO-style global/local consistency inside a CLIP ViT and aligning regional features to CLIP\u2019s language space, thereby bridging the image-to-region domain shift without any region\u2013text pairs.",
  "target_paper": {
    "title": "CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction",
    "authors": "Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, Chen Change Loy",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "open-vocabulary object detection, open-vocabulary image segmentation",
    "abstract": "Open-vocabulary dense prediction tasks including object detection and image segmentation have been advanced by the success of Contrastive Language-Image Pre-training (CLIP). CLIP models, particularly those incorporating vision transformers (ViTs), have exhibited remarkable generalization ability in zero-shot image classification. However, when transferring the vision-language alignment of CLIP from global image representation to local region representation for the open-vocabulary dense prediction tasks, CLIP ViTs suffer from the domain shift from full images to local image regions. In this paper, we embark on an in-depth analysis of the region-language alignment in CLIP models, which is essential for downstream open-vocabulary dense prediction tasks. Subsequently, we propose an approach named CLIPSelf, which adapts the image-level recognition ability of CLIP ViT to local image regions without needing any region-text pairs. CLIPSelf empowers ViTs to distill itself by aligning a region r",
    "openreview_id": "DjzvJCRsVf",
    "forum_id": "DjzvJCRsVf"
  },
  "analysis_timestamp": "2026-01-06T11:49:56.970696"
}
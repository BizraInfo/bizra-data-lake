{
  "prior_works": [
    {
      "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
      "authors": "Jascha Sohl-Dickstein et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "Introduced the forward\u2013reverse Markov noising framework for generative modeling that Generator Matching retains, but GM reframes it via the infinitesimal generator to cover arbitrary Markov processes rather than a specific diffusion."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Provides the primary diffusion baseline that GM recovers as a special case when the chosen generator is a Gaussian diffusion, while GM extends beyond by allowing arbitrary (including non-Gaussian and discrete/jump) generators and their superpositions."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Yang Song et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "Established the marginal-dynamics viewpoint (e.g., probability flow ODE/Fokker\u2013Planck) for diffusions; GM generalizes this from SDEs to the infinitesimal generator of arbitrary Markov processes, addressing the limitation to continuous diffusions."
    },
    {
      "title": "Flow Matching for Generative Modeling",
      "authors": "Yaron Lipman et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "GM directly extends flow matching\u2019s recipe\u2014learning a marginal field by fitting conditional single-sample dynamics\u2014by replacing velocity fields with Markov process generators and matching the marginal generator instead of a marginal vector field."
    },
    {
      "title": "Stochastic Interpolants: Bridging Normalizing Flows and Diffusion Models",
      "authors": "Michael S. Albergo et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Inspired GM\u2019s conditional construction: GM adopts the idea of specifying single-sample conditional dynamics to induce the desired marginal evolution, but elevates it to the operator (generator) level to handle jump and discrete processes."
    },
    {
      "title": "D3PM: Discrete Denoising Diffusion Probabilistic Models",
      "authors": "Jacob Austin et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "Showed diffusion-style training for discrete data via Markov chains; GM subsumes this by treating discrete dynamics as jump-process generators and closes the gap between discrete and continuous models within one unified generator-matching objective."
    }
  ],
  "synthesis_narrative": "Generator Matching (GM) emerges by unifying the marginal-dynamics lens of diffusion/flow methods with the operator-theoretic notion of a Markov process generator. The lineage begins with Sohl-Dickstein et al. (2015), who framed generative modeling as inverting a forward noising Markov process. Ho et al. (2020) made this practical, establishing diffusion models as a dominant baseline. Song et al. (2021) recast these models through SDEs and the probability flow ODE, emphasizing marginal evolution via Fokker\u2013Planck dynamics but remaining restricted to continuous diffusions. In parallel, Lipman et al. (2023) and Albergo et al. (2023) showed that one can learn generative dynamics by fitting conditional single-sample paths whose aggregate behavior matches desired marginals\u2014flow matching and stochastic interpolants\u2014yet these focus on vector fields (ODE/SDE) rather than general Markov dynamics. Discrete diffusion work such as Austin et al. (2021) demonstrated a separate toolbox for categorical data via Markov chains, highlighting the lack of a unified treatment across state spaces and process types. GM synthesizes these threads by replacing the marginal velocity/score with the infinitesimal generator itself and learning it via conditional generators. This generalization recovers diffusion, flow matching, and discrete diffusion as special cases, and crucially opens new design space\u2014e.g., jump processes and superpositions of generators\u2014enabling principled multimodal compositions that were awkward or impossible under prior, process-specific formulations.",
  "analysis_timestamp": "2026-01-06T23:09:26.625036"
}
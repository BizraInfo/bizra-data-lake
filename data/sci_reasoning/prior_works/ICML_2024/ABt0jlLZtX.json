{
  "prior_works": [
    {
      "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation",
      "authors": "Richard S. Sutton et al.",
      "year": 2000,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s analysis is built on the stochastic policy gradient theorem introduced by Sutton et al., using this framework to study convergence when training stochastic policies but deploying their deterministic (mean) counterpart."
    },
    {
      "title": "Parameter-Exploring Policy Gradients",
      "authors": "Frank Sehnke et al.",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "Sehnke et al. formalized parameter-based exploration (hyperpolicies), which the present work directly adopts and places side-by-side with action-based exploration to analyze their impact on learning a deterministic deployed policy."
    },
    {
      "title": "On the Global Convergence of Policy Gradient Methods in Tabular Markov Decision Processes",
      "authors": "Alekh Agarwal et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "This work introduced the gradient domination/PL-style framework for proving global convergence of policy gradient methods, which the current paper adapts to the new objective of converging to the best deterministic policy learned via stochastic gradients."
    },
    {
      "title": "Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator",
      "authors": "Maryam Fazel et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "Fazel et al. established global convergence of policy gradient under structural conditions in LQR, inspiring the present paper\u2019s use of gradient-domination-type assumptions to obtain global guarantees in the deterministic-deployment setting."
    },
    {
      "title": "Deterministic Policy Gradient Algorithms",
      "authors": "David Silver et al.",
      "year": 2014,
      "role": "Related Problem",
      "relationship_sentence": "Silver et al. introduced learning deterministic policies directly; the current paper targets the same end\u2014optimal deterministic control\u2014but shows how stochastic policy gradients can provably reach the best deterministic policy and clarifies when this practice is preferable."
    },
    {
      "title": "Parameter Space Noise for Exploration",
      "authors": "Matthias Plappert et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Plappert et al. empirically contrasted action noise with parameter-space noise, highlighting a lack of theory; the present work fills this gap by quantitatively comparing action-based versus parameter-based exploration when the deployed policy is deterministic and by prescribing variance tuning."
    }
  ],
  "synthesis_narrative": "The core idea of learning with stochastic policy gradients while ultimately deploying a deterministic controller stands on two pillars: the policy gradient framework and modern global convergence theory for policy optimization. Sutton et al. provided the stochastic policy gradient theorem that underlies the updates studied here, enabling analysis when training is stochastic but deployment is deterministic. In parallel, Sehnke et al. introduced parameter-based exploration (hyperpolicies), furnishing the second exploration modality that this paper formalizes and compares against action-based exploration within a unified framework. Recent theoretical advances established that policy gradients can enjoy global convergence under gradient-domination (PL-style) conditions. Agarwal et al. formalized this perspective in tabular MDPs, while Fazel et al. demonstrated analogous guarantees in LQR, together motivating the present paper\u2019s adoption of gradient domination to prove global convergence specifically to the best deterministic policy when learning is stochastic. Against this theoretical backdrop, Silver et al.\u2019s deterministic policy gradient algorithms define the classic route to deterministic control, providing a natural point of comparison for the paper\u2019s thesis that stochastic training with deterministic deployment is not only common practice but also theoretically justified. Finally, the empirical focus on exploration mechanisms by Plappert et al. exposed a gap: when and why parameter-space versus action-space exploration is preferable. This paper directly addresses that gap by quantifying the sample-complexity and performance trade-offs and by prescribing how to tune exploration variance for optimal deterministic deployment.",
  "analysis_timestamp": "2026-01-06T23:09:26.400674"
}
{
  "prior_works": [
    {
      "title": "Sparse Online Gaussian Processes",
      "authors": "Lehel Csat\u00f3, Manfred Opper",
      "year": 2002,
      "role": "Early online/budgeted GP framework",
      "relationship_sentence": "Introduced budgeted, sequential GP updates with selective basis-vector growth/pruning, directly inspiring the paper\u2019s goal of adaptively controlling GP model size as data arrive in a continual setting."
    },
    {
      "title": "Sparse Gaussian Processes using Pseudo-inputs",
      "authors": "Edward Snelson, Zoubin Ghahramani",
      "year": 2006,
      "role": "Inducing-point formulation of model capacity",
      "relationship_sentence": "Established inducing points (pseudo-inputs) as the core capacity knob for GPs, a notion the paper leverages when deciding how and when to increase model size."
    },
    {
      "title": "Variational Learning of Inducing Variables in Sparse Gaussian Processes",
      "authors": "Michalis Titsias",
      "year": 2009,
      "role": "Principled variational objective for inducing-point GPs",
      "relationship_sentence": "Provided the ELBO underpinning that adding inducing points monotonically tightens the bound, which the paper uses as a principled criterion for growing capacity only until additional points cease to yield gains."
    },
    {
      "title": "Gaussian Processes for Big Data",
      "authors": "James Hensman, Nicolo Fusi, Neil D. Lawrence",
      "year": 2013,
      "role": "Stochastic variational inference for scalable/streaming GPs",
      "relationship_sentence": "Enabled minibatch and streaming-friendly GP training via SVGP, forming the computational backbone that allows the paper to adjust model size online without full-batch retraining."
    },
    {
      "title": "Understanding Probabilistic Sparse Gaussian Process Approximations",
      "authors": "Matthias Bauer, Mark van der Wilk, Carl E. Rasmussen",
      "year": 2016,
      "role": "Theoretical unification and guidance for inducing-point approximations",
      "relationship_sentence": "Clarified the variational nature and limitations of sparse GP approximations, informing the paper\u2019s near-optimality claims and its safe growth strategy for inducing sets."
    },
    {
      "title": "Streaming Sparse Gaussian Process Approximations",
      "authors": "Thang D. Bui, Cuong Nguyen, Richard E. Turner",
      "year": 2017,
      "role": "Continual/streaming variational updates with evolving inducing sets",
      "relationship_sentence": "Developed sequential variational updates that can add inducing points over time, a direct precursor to the paper\u2019s automated, on-the-fly model-size adjustment under continual data arrival."
    },
    {
      "title": "Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies",
      "authors": "Andreas Krause, Ajit Singh, Carlos Guestrin",
      "year": 2008,
      "role": "Information-theoretic perspective on when added points stop helping",
      "relationship_sentence": "Showed diminishing returns via submodular information gain in GPs, motivating the paper\u2019s criterion of adding capacity only until dataset information is effectively captured."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014automatically adjusting Gaussian process (GP) model size in a continual learning setting\u2014builds on three intertwined threads: inducing-point capacity control, scalable variational inference, and online/budgeted updating. Snelson and Ghahramani (2006) crystallized inducing points as the explicit capacity parameter for GPs, while Titsias (2009) supplied a principled variational objective showing that the ELBO tightens monotonically with more inducing points. This gave a rigorous criterion for when additional capacity is beneficial. Bauer, van der Wilk, and Rasmussen (2016) unified sparse GP approximations, clarifying their variational nature and guiding safe capacity growth without degrading posterior quality.\n\nTo make capacity decisions feasible under streaming data, Hensman et al. (2013) introduced stochastic variational GPs, enabling minibatch training and efficient updates. Bui et al. (2017) extended this to streaming settings with sequential variational updates that can add inducing points over time, a direct precursor to dynamically resizing models as data accrue. Csat\u00f3 and Opper (2002) provided the early blueprint for budgeted, online GP learning\u2014adding, pruning, and maintaining a compact representation\u2014which resonates with the paper\u2019s constraint of avoiding unnecessary computational growth. Finally, Krause et al. (2008) offered an information-theoretic lens on diminishing returns in GPs, motivating a stopping principle: increase capacity only until the incremental information gain is negligible. Together, these works enable the paper\u2019s near-optimal, hyperparameter-light approach to growing GP capacity in continual learning: combine variationally justified inducing-point additions with streaming-compatible updates and an information-guided stopping rule to answer \u201chow big is big enough?\u201d across diverse datasets.",
  "analysis_timestamp": "2026-01-07T00:21:33.192497"
}
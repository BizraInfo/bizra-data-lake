{
  "prior_works": [
    {
      "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games",
      "authors": "Junhyuk Oh et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "Established the formulation of an action-conditioned generative dynamics model as a surrogate interactive environment; Genie generalizes this by learning the action channel itself as latent variables from videos without action labels."
    },
    {
      "title": "World Models",
      "authors": "David Ha et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Introduced the idea of a learned generative world model (compressed visual tokens + autoregressive dynamics) for control; Genie adopts this world-model framing and scales it to Internet video while removing the need for observed actions."
    },
    {
      "title": "Dreamer: Reinforcement Learning by Latent Imagination",
      "authors": "Danijar Hafner et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Showed that latent dynamics models enable effective control but rely on ground-truth action labels and environment interaction; Genie directly addresses this gap by learning a latent action space from unlabelled videos."
    },
    {
      "title": "Behavioral Cloning from Observation",
      "authors": "Faraz Torabi et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "Proposed inferring actions from state-only demonstrations via inverse dynamics to enable imitation without action labels; Genie echoes this core idea by learning action-like latent codes that explain video transitions, enabling control and imitation from raw videos."
    },
    {
      "title": "Video PreTraining (VPT): Learning to Act from Large-Scale Internet Videos",
      "authors": "Bowen Baker et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated learning from Internet videos for control but required recovering actions using a supervised inverse dynamics model; Genie removes this supervision by discovering a controllable latent action space directly from unlabelled videos."
    },
    {
      "title": "Neural Discrete Representation Learning (VQ-VAE)",
      "authors": "Aaron van den Oord et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "Introduced vector-quantized discrete codes enabling autoregressive modeling over tokens; Genie extends this idea with a spatiotemporal video tokenizer to discretize videos for large-scale autoregressive dynamics and latent action learning."
    },
    {
      "title": "A Generalist Agent",
      "authors": "Scott Reed et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "Framed sequence modeling over tokenized multimodal trajectories for generalist control but required explicit actions; Genie carries this sequence-modeling paradigm to world models and contributes the key step of inferring action-controllable latent codes from raw video."
    }
  ],
  "synthesis_narrative": "Genie\u2019s core innovation\u2014an action-controllable generative world model learned entirely from unlabelled Internet videos\u2014emerges at the junction of world modeling, action-free imitation, and tokenized video generation. The action-conditioned predictive modeling of Oh et al. defined the interactive-video-as-environment formulation that Genie adopts, while Ha and Schmidhuber\u2019s World Models established the architectural blueprint of compressed visual tokens plus a learned dynamics prior. Dreamer pushed this line into powerful latent dynamics for control, but its reliance on explicit action labels and interaction exposed a critical bottleneck; Genie closes this gap by learning a latent action space that explains video transitions without ever seeing ground-truth actions. This idea is directly foreshadowed by Behavioral Cloning from Observation, which showed that actions can be inferred from state-only sequences to enable imitation; Genie scales and internalizes that principle within a single generative model. On the generative side, VQ-VAE\u2019s discrete tokenization underpins Genie\u2019s spatiotemporal video tokenizer, enabling efficient autoregressive dynamics at scale. Finally, Gato\u2019s demonstration that sequence models can serve as generalist control policies informs Genie\u2019s framing of a foundation world model, but Genie advances the paradigm by discovering the action interface from raw videos. Together, these works directly enable Genie\u2019s key contribution: turning Internet video into controllable, interactive environments through learned latent actions.",
  "analysis_timestamp": "2026-01-06T23:09:26.494042"
}
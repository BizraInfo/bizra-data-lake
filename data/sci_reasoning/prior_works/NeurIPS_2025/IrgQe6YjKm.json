{
  "prior_works": [
    {
      "title": "Multi-Task Learning as Multi-Objective Optimization",
      "authors": "Ozan Sener, Vladlen Koltun",
      "year": 2018,
      "role": "Formulation of MOL/Pareto trade-offs",
      "relationship_sentence": "Established the Pareto-based view of jointly optimizing competing objectives, providing the formal MOL setup whose trade-off structure and model-class dependence the paper\u2019s sample-complexity analysis targets."
    },
    {
      "title": "Does Unlabeled Data Improve Generalization in Classification?",
      "authors": "Shai Ben-David, Tyler Lu, D\u00e1vid P\u00e1l, Noam Srebro",
      "year": 2008,
      "role": "Foundational negative results in SSL",
      "relationship_sentence": "Provided core lower-bound insights showing unlabeled data does not help without additional structure, directly motivating the paper\u2019s impossibility results for certain losses and its identification of structural conditions (Bregman losses) where unlabeled data can help."
    },
    {
      "title": "The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition",
      "authors": "Vittorio Castelli, Thomas M. Cover",
      "year": 1996,
      "role": "Value-of-information tradeoffs for labeled vs. unlabeled data",
      "relationship_sentence": "Quantified when unlabeled data can reduce labeled sample requirements, informing the paper\u2019s core contribution that, under Bregman objectives, unlabeled data can substantially alleviate labeled sample complexity."
    },
    {
      "title": "Clustering with Bregman Divergences",
      "authors": "Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, Joydeep Ghosh",
      "year": 2005,
      "role": "Bregman divergence theory and optimality of means",
      "relationship_sentence": "Established key properties of Bregman losses (e.g., conditional expectation as the risk minimizer) that the paper exploits to design and analyze its pseudo-labeling scheme and to separate labeled vs. unlabeled complexity contributions."
    },
    {
      "title": "Strictly Proper Scoring Rules, Prediction, and Estimation",
      "authors": "Tilmann Gneiting, Adrian E. Raftery",
      "year": 2007,
      "role": "Link between proper scoring rules and Bregman-type losses",
      "relationship_sentence": "Provided the proper-scoring framework underpinning why targets derived from distributional quantities are Bayes-optimal for Bregman/proper losses, enabling the paper\u2019s semi-supervised reductions using unlabeled data."
    },
    {
      "title": "Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks",
      "authors": "Dong-Hyun Lee",
      "year": 2013,
      "role": "Algorithmic template for using unlabeled data",
      "relationship_sentence": "Inspired the paper\u2019s simple pseudo-labeling mechanism; the new work gives theory showing when such pseudo-labeling provably reduces labeled sample complexity for MOL with Bregman losses."
    },
    {
      "title": "A Vector-Contraction Inequality for Rademacher Complexities",
      "authors": "Andreas Maurer",
      "year": 2016,
      "role": "Generalization tools for vector-valued/ multi-loss settings",
      "relationship_sentence": "Supplied technical machinery for controlling uniform deviations of multi-objective losses, informing the paper\u2019s sample complexity bounds that depend on the hypothesis class complexity."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014characterizing when and how unlabeled data reduces labeled sample complexity in semi-supervised multi-objective learning (MOL), with sharp separations for Bregman losses\u2014builds on three intellectual pillars. First, the MOL formulation and Pareto viewpoint popularized by Sener and Koltun frame the fundamental tension of jointly optimizing competing tasks with a single model class, clarifying why class capacity enters generalization bounds. Second, classical semi-supervised learning theory sets the backdrop for what is and is not possible: Ben-David et al. deliver negative results showing unlabeled data is unhelpful absent structure, while Castelli and Cover quantify conditions under which unlabeled data can trade off against labeled data. The present work extends this line by revealing a nuanced answer for MOL: for some losses the statistical cost tied to the model class is unavoidable, yet for Bregman objectives unlabeled data can carry most of the burden.\nThird, the structural lens is provided by Bregman/proper-loss theory. Banerjee et al. and Gneiting\u2013Raftery establish that conditional expectations are Bayes-optimal under Bregman/proper scoring rules, enabling a principled pseudo-labeling route: estimate distributional quantities from unlabeled data and convert them into targets for labeled learning. This algorithmic idea is grounded in practice by Lee\u2019s pseudo-labeling template, while Maurer\u2019s vector-contraction inequality offers the technical vehicle to translate these insights into uniform convergence and sample complexity bounds for multi-loss settings. Together, these works directly inform the paper\u2019s central theorems separating loss classes and proving when unlabeled data measurably reduces labeled sample requirements in MOL.",
  "analysis_timestamp": "2026-01-06T23:42:48.110725"
}
{
  "prior_works": [
    {
      "title": "Universal Value Function Approximators",
      "authors": "Tom Schaul et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "UVFA introduced the formalism of goal-conditioned value/policy functions that PTGM adopts for its low-level goal-conditioned policy and high-level goal selection."
    },
    {
      "title": "Hindsight Experience Replay",
      "authors": "Marcin Andrychowicz et al.",
      "year": 2017,
      "arxiv_id": "1707.01495",
      "role": "Foundation",
      "relationship_sentence": "HER\u2019s goal relabeling enables learning goal-conditioned behaviors from task-agnostic datasets, a mechanism PTGM leverages to pre-train the low-level goal-conditioned policy."
    },
    {
      "title": "Data-Efficient Hierarchical Reinforcement Learning (HIRO)",
      "authors": "Ofir Nachum et al.",
      "year": 2018,
      "arxiv_id": "1805.08296",
      "role": "Baseline",
      "relationship_sentence": "HIRO established the subgoal-setting hierarchical structure that PTGM builds upon while directly addressing HIRO\u2019s instability in continuous, high-dimensional goal spaces by discretizing goals via clustering and adding a goal prior."
    },
    {
      "title": "Learning to Reach Goals via Iterated Supervised Learning (GCSL)",
      "authors": "Dibya Ghosh et al.",
      "year": 2021,
      "arxiv_id": "1912.06088",
      "role": "Extension",
      "relationship_sentence": "GCSL\u2019s supervised pretraining of goal-conditioned policies from offline, relabeled data is directly extended in PTGM to pre-train the low-level goal-based policy on large task-agnostic datasets."
    },
    {
      "title": "Hindsight Goal Generation for Reinforcement Learning",
      "authors": "Zhenghao Ren et al.",
      "year": 2019,
      "arxiv_id": "1909.13888",
      "role": "Inspiration",
      "relationship_sentence": "HGG\u2019s idea of learning a goal generator to propose effective goals informs PTGM\u2019s high-level goal policy, which is further stabilized by a learned goal prior and discretized action space."
    },
    {
      "title": "Behavior Regularized Offline Reinforcement Learning (BRAC)",
      "authors": "Yifan Wu et al.",
      "year": 2019,
      "arxiv_id": "1911.11361",
      "role": "Inspiration",
      "relationship_sentence": "BRAC\u2019s KL regularization to a learned behavior prior directly motivates PTGM\u2019s pre-trained goal prior used to regularize the high-level policy, improving sample efficiency and stability."
    }
  ],
  "synthesis_narrative": "Universal Value Function Approximators established that policies and value functions can be conditioned on explicit goals, making it natural to train components that map to and act over goal representations. Hindsight Experience Replay showed that relabeling outcomes as goals unlocks learning from sparse rewards and task-agnostic interaction data, providing a practical mechanism to train goal-conditioned policies from broad datasets. Data-Efficient Hierarchical RL (HIRO) demonstrated a powerful structure in which a high-level controller sets subgoals for a low-level goal-conditioned policy, while exposing difficulties in handling continuous, high-dimensional goal spaces and non-stationarity. Goal-Conditioned Supervised Learning (GCSL) further showed that goal-reaching policies can be pre-trained via supervised learning on relabeled offline data, enabling efficient bootstrapping of goal-conditioned skills. Hindsight Goal Generation proposed learning goal generators that propose useful goals to accelerate learning, highlighting the promise of learned goal proposal mechanisms. Behavior Regularized Offline RL (BRAC) introduced KL regularization to a learned behavior prior, evidencing that priors derived from data can stabilize and improve policy learning.\n\nTogether, these works suggested a path: pre-train robust goal-conditioned skills from broad data (HER, GCSL, UVFA), structure control hierarchically with a goal-setting high-level (HIRO, HGG), and stabilize decision-making by constraining policies toward data-driven priors (BRAC). The core opportunity was to overcome the instability of continuous subgoal spaces and the brittleness of unconstrained high-level goal selection by discretizing goals and regularizing them with a learned prior. The resulting synthesis naturally leads to pre-training goal-based models, clustering achieved goals to form a discrete, effective high-level action space, and applying a goal prior to regularize high-level decisions for more sample-efficient and stable reinforcement learning.",
  "target_paper": {
    "title": "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning",
    "authors": "Haoqi Yuan, Zhancun Mu, Feiyang Xie, Zongqing Lu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "reinforcement learning, pre-training, goal-conditioned RL, open-world environments",
    "abstract": "Pre-training on task-agnostic large datasets is a promising approach for enhancing the sample efficiency of reinforcement learning (RL) in solving complex tasks. We present PTGM, a novel method that pre-trains goal-based models to augment RL by providing temporal abstractions and behavior regularization. PTGM involves pre-training a low-level, goal-conditioned policy and training a high-level policy to generate goals for subsequent RL tasks. To address the challenges posed by the high-dimensional goal space, while simultaneously maintaining the agent's capability to accomplish various skills, we propose clustering goals in the dataset to form a discrete high-level action space. Additionally, we introduce a pre-trained goal prior model to regularize the behavior of the high-level policy in RL, enhancing sample efficiency and learning stability. Experimental results in a robotic simulation environment and the challenging open-world environment of Minecraft demonstrate PTGM\u2019s superiority ",
    "openreview_id": "o2IEmeLL9r",
    "forum_id": "o2IEmeLL9r"
  },
  "analysis_timestamp": "2026-01-06T07:23:08.679283"
}
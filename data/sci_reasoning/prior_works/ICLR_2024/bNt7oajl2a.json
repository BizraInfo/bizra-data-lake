{
  "prior_works": [
    {
      "title": "FOIL: Learning Logical Definitions from Relations",
      "authors": "J. R. Quinlan",
      "year": 1990,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "FOIL established the classic ILP workflow of proposing, selecting, and refining candidate rules from examples, which this work directly re-instantiates with LMs generating textual rules and a symbolic interpreter enforcing selection and refinement."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "arxiv_id": "2305.10601",
      "role": "Extension",
      "relationship_sentence": "Tree of Thoughts introduced multi-step exploration and evaluation over intermediate hypotheses (\u201cthoughts\u201d), which this work extends by casting nodes as explicit textual rule hypotheses and replacing heuristic evaluators with a task-specific symbolic interpreter plus a refinement loop."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2023,
      "arxiv_id": "2203.11171",
      "role": "Baseline",
      "relationship_sentence": "Self-Consistency\u2019s idea of sampling diverse candidate reasoning paths is adopted as the hypothesis proposing stage here, but its majority-vote selection is replaced by interpreter-based filtering to address inductive generalization failures."
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "authors": "Aman Madaan et al.",
      "year": 2023,
      "arxiv_id": "2303.17651",
      "role": "Inspiration",
      "relationship_sentence": "Self-Refine\u2019s iterative critique-and-rewrite paradigm directly motivates the refine step, which in this paper is driven by concrete symbolic feedback (e.g., failing cases) rather than purely LLM-generated critiques."
    },
    {
      "title": "PAL: Program-Aided Language Models",
      "authors": "Xiaojun Gao et al.",
      "year": 2023,
      "arxiv_id": "2211.10435",
      "role": "Inspiration",
      "relationship_sentence": "PAL demonstrated that delegating verification/execution to an external interpreter can reliably validate LM proposals, a principle this work adopts by using a domain-specific symbolic interpreter to filter and score candidate rule hypotheses."
    },
    {
      "title": "DreamCoder: Growing Generalizable, Interpretable Knowledge with Wake-Sleep Program Induction",
      "authors": "Kevin Ellis et al.",
      "year": 2021,
      "arxiv_id": "2006.08381",
      "role": "Related Problem",
      "relationship_sentence": "DreamCoder\u2019s propose\u2013execute\u2013refactor loop for program induction informs this work\u2019s hypothesis refinement cycle, translating program refactoring into natural-language rule revision guided by execution feedback."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "arxiv_id": "2210.03629",
      "role": "Related Problem",
      "relationship_sentence": "ReAct\u2019s use of environment/tool feedback to guide subsequent reasoning steps inspires the use of a symbolic interpreter as an external feedback signal to iteratively improve textual rule hypotheses."
    }
  ],
  "synthesis_narrative": "Inductive Logic Programming\u2019s FOIL codified the core cycle of proposing candidate rules from examples, selecting those that fit the data, and refining them when they fail, providing a procedural blueprint for rule induction. Tree of Thoughts generalized single-pass prompting into a search over intermediate hypotheses with explicit evaluation, showing that structured exploration can unlock stronger problem solving. Self-Consistency revealed that sampling diverse reasoning paths improves robustness, but its majority-vote selection lacks a principled notion of correctness beyond surface agreement. Self-Refine introduced iterative critique-and-rewrite, demonstrating that iterative revision can correct LM outputs when guided by targeted feedback. PAL established that external symbolic execution can act as a reliable verifier for LM-generated artifacts, separating generation from correctness checking via an interpreter. DreamCoder showed that propose\u2013execute\u2013refactor loops can accumulate generalizable structure in program induction by learning from execution feedback. ReAct further illustrated how tool/environment feedback can steer reasoning, integrating external signals into the generation process. Together these works expose a gap: LMs are strong at proposing hypotheses, but selection and improvement require verifiable feedback and structured iteration. The present approach synthesizes FOIL\u2019s inductive rule-learning template with Tree-of-Thoughts-style exploration, PAL\u2019s interpreter-based verification, and Self-Refine\u2019s iterative revision to create a propose\u2013select\u2013refine loop where hypotheses are textual rules vetted by a symbolic interpreter. This integration naturally targets inductive generalization by aligning generation with executable, failure-driven refinement.",
  "target_paper": {
    "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
    "authors": "Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "language model, natural language processing, inductive reasoning",
    "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations---known as inductive reasoning---is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through $\\textit{iterative hypothesis refinement}$, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal $\\textit{hypothesis proposers}$ (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approa",
    "openreview_id": "bNt7oajl2a",
    "forum_id": "bNt7oajl2a"
  },
  "analysis_timestamp": "2026-01-06T09:14:31.863159"
}
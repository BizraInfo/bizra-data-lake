{
  "prior_works": [
    {
      "title": "Flow Matching for Generative Modeling",
      "authors": "Yaron Lipman et al.",
      "year": 2023,
      "role": "Foundational training principle for learning continuous-time vector fields by regressing the velocity along prescribed interpolation paths.",
      "relationship_sentence": "The paper\u2019s core analysis targets the straight-path (flow-matching) objective, showing that, under deterministic training, optimization can converge to memorizing vector fields\u2014directly interrogating the very mechanism introduced by flow matching."
    },
    {
      "title": "Rectified Flow: Training Straight Flows for Fast Inference",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Introduces rectified flows that enforce straight trajectories between source and target for near one-step sampling.",
      "relationship_sentence": "The failure mode uncovered\u2014memorization of arbitrary pairings when interpolant lines intersect\u2014directly critiques and explains when/why rectified-flow training leads to ill-defined vector fields and deterministic pairing at inference."
    },
    {
      "title": "Stochastic Interpolants: Bridging the Gap Between Diffusion and Flow-based Generative Models",
      "authors": "Mark B. Albergo, Eric Vanden-Eijnden",
      "year": 2022,
      "role": "Unifies stochastic and deterministic training via interpolants and velocity regression, highlighting how path stochasticity affects learning.",
      "relationship_sentence": "The paper\u2019s gradient-variance lens across stochastic vs. deterministic regimes builds on the stochastic-interpolant framework to explain which vector fields optimization prefers under each regime."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole",
      "year": 2021,
      "role": "Establishes SDE-based training and the associated probability flow ODE, connecting stochastic training with deterministic ODE inference.",
      "relationship_sentence": "The contrast between stochastic training and deterministic ODE inference informs this paper\u2019s analysis of how deterministic objectives (with low gradient variance) can bias learning toward memorization."
    },
    {
      "title": "Denoising Diffusion Implicit Models (DDIM)",
      "authors": "Jiaming Song, Chenlin Meng, Stefano Ermon",
      "year": 2020,
      "role": "Introduces deterministic non-stochastic sampling trajectories for diffusion models.",
      "relationship_sentence": "DDIM\u2019s deterministic sampling motivates the paper\u2019s focus on deterministic integration and how straight or low-variance training signals can lock in specific pairings at inference."
    },
    {
      "title": "Neural Ordinary Differential Equations",
      "authors": "Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud",
      "year": 2018,
      "role": "Framework for learning ODE vector fields parameterized by neural networks.",
      "relationship_sentence": "The paper\u2019s core object of study is an ODE vector field learned from data; Neural ODEs provide the modeling substrate and motivate the analysis of convergence to ill-defined fields under certain objectives."
    },
    {
      "title": "Optimal Transport: Old and New",
      "authors": "C\u00e9dric Villani",
      "year": 2009,
      "role": "Comprehensive theory of optimal transport, displacement interpolations, and Gaussian-to-Gaussian maps.",
      "relationship_sentence": "Their Gaussian-to-Gaussian analysis and the role of straight-line displacement interpolants rely on OT theory, clarifying when intersecting line interpolants lead to ambiguous couplings and memorization."
    }
  ],
  "synthesis_narrative": "The paper interrogates the straight-path training paradigm that underpins modern flow-based generative modeling. Flow Matching for Generative Modeling established learning vector fields by regressing velocities along chosen interpolants, a principle that Rectified Flow specialized to straight trajectories to enable near one-step sampling. Building directly on these, the present work shows that in deterministic training regimes the straight-path objective exhibits low gradient variance that can drive convergence to memorizing, ill-defined vector fields\u2014replicating arbitrary training pairings even when interpolant lines intersect.\nStochastic Interpolants formalized how stochastic vs deterministic paths shape the regression targets and variance properties. The authors leverage this lens to compare gradient variance across regimes, explaining which vector fields optimization favors and why stochasticity mitigates the memorization bias. Score-Based Generative Modeling through SDEs and DDIM provided the blueprint for relating stochastic training to deterministic ODE inference, framing how deterministic integration at test time can entrench learned pairings.\nMethodologically, the analysis hinges on ODE vector fields as in Neural ODEs and on optimal transport structure. Villani\u2019s OT theory (including displacement interpolation and Gaussian-to-Gaussian maps) supplies closed-form structure for the Gaussian setting, letting the authors prove existence and attraction of memorizing vector fields despite intersecting interpolants. Together, these works directly scaffold the paper\u2019s central contribution: revealing and formalizing a fundamental failure mode of rectified/flow-matching objectives via a gradient-variance perspective.",
  "analysis_timestamp": "2026-01-07T00:21:32.291816"
}
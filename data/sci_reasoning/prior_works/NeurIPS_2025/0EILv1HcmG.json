{
  "prior_works": [
    {
      "title": "Attention Is All You Need",
      "authors": "Ashish Vaswani et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Defines the self-attention mechanism with softmax-normalized (right-stochastic) attention that QDSFormer explicitly replaces with a doubly stochastic, quantum-parameterized alternative."
    },
    {
      "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
      "authors": "Kelvin Xu et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "Introduced the idea of doubly stochastic attention via a column-sum regularizer, motivating QDSFormer's move from heuristic regularization to an explicit DSM attention parameterization."
    },
    {
      "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport",
      "authors": "Marco Cuturi",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "Popularized entropic Sinkhorn scaling in ML, the core iterative tool used to obtain doubly stochastic matrices that QDSFormer aims to replace with a parametric quantum circuit."
    },
    {
      "title": "Learning Latent Permutations with Gumbel\u2013Sinkhorn",
      "authors": "Gaspard Mena et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Provided a differentiable, Sinkhorn-based relaxation to permutation/DSM matrices widely adopted in deep models; QDSFormer addresses its iterative, approximative, non-parametric nature with a learnable quantum DSM layer."
    },
    {
      "title": "Fast Differentiable Sorting and Ranking using Optimal Transport",
      "authors": "Marco Blondel et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Established efficient Sinkhorn-based differentiable layers producing DSMs, forming the practical baseline class that QDSFormer seeks to supersede with a parametric quantum construction."
    },
    {
      "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification",
      "authors": "Andr\u00e9 F. T. Martins et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated that alternative normalizations (e.g., sparsemax) can fix softmax shortcomings yet still yield only right-stochastic vectors, highlighting the gap QDSFormer fills by enforcing double stochasticity."
    },
    {
      "title": "Quantum Circuit Learning",
      "authors": "Kosuke Mitarai et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Introduced variational quantum circuits as differentiable parametric models, the enabling paradigm that QDSFormer leverages to realize a learnable quantum layer producing DSM attention via the Born rule."
    }
  ],
  "synthesis_narrative": "The core innovation of Quantum Doubly Stochastic Transformers (QDSFormer) is to replace the softmax in self-attention with a parametric quantum circuit that outputs a doubly stochastic matrix (DSM). This builds squarely on the Transformer formulation of softmax-normalized attention introduced by Vaswani et al., which defines the right-stochastic baseline being replaced. Early evidence from Xu et al. that doubly stochastic attention (via a column-sum regularizer) improves coverage and stability directly inspired the shift from right-stochastic vectors to DSMs in attention. Practically, the ML community has enforced DSMs with entropic optimal transport and Sinkhorn scaling following Cuturi, with Mena et al. and Blondel et al. providing differentiable, widely used DSM layers (e.g., Gumbel\u2013Sinkhorn and OT-based sorting/ranking). These methods, however, are iterative, approximative, and non-parametric\u2014precisely the limitations QDSFormer targets by introducing a learnable DSM mechanism. In parallel, the quantum ML literature established variational quantum circuits as differentiable parametric models (Mitarai et al.), and Born-rule outputs as normalized probability distributions\u2014an inductive bias that naturally fits attention normalization. QDSFormer unifies these threads: it takes the motivation and empirical gains of DSM attention from Sinkhorn-based approaches, but swaps the iterative scaling with a parametric quantum circuit that directly generates DSMs, thereby addressing flexibility and expressivity while preserving the benefits of double stochasticity in Transformer attention.",
  "analysis_timestamp": "2026-01-06T23:08:23.945971"
}
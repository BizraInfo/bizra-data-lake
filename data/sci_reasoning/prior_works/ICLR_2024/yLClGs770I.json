{
  "prior_works": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work established CoT rationales as effective supervision signals for mathematical reasoning, which MathInstruct explicitly adopts as one half of its hybrid (CoT) annotation strategy."
    },
    {
      "title": "Program of Thoughts Prompting: Disentangle Reasoning from Language Models via Program Execution",
      "authors": "Chen et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "It introduced PoT supervision\u2014having models produce executable code for reasoning\u2014directly motivating MAmmoTH\u2019s inclusion of PoT traces as the complementary half of its hybrid rationale design."
    },
    {
      "title": "PAL: Program-Aided Language Models",
      "authors": "Luyu Gao et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "PAL showed that delegating arithmetic/logic to a Python interpreter via model-generated code boosts math performance, informing MAmmoTH\u2019s decision to curate PoT rationales that enable tool execution during inference."
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems (GSM8K)",
      "authors": "Karl Cobbe et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "GSM8K provided high-quality, step-by-step solution rationales for grade-school math that serve as a core source format and content for MathInstruct\u2019s CoT component."
    },
    {
      "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "MATH defined a competition-level, topic-diverse benchmark and solution style that shaped MathInstruct\u2019s coverage goals and the evaluation target for the MAmmoTH series."
    },
    {
      "title": "WizardMath: Empowering Large Language Models to Solve Math via Evol-Instruct",
      "authors": "Luo et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "As the main math-instruction baseline built from synthetic Evol-Instruct data (largely CoT-only), WizardMath\u2019s limitations in tool-use/programmatic reasoning directly motivated MAmmoTH\u2019s hybrid CoT+PoT instruction tuning and is the primary system MAmmoTH surpasses."
    },
    {
      "title": "MetaMath: Bootstrap LLMs for Math with Self-Improvement",
      "authors": "Yu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "MetaMath\u2019s self-improvement pipeline yields scale but primarily CoT-centric supervision with limited curated executable traces, highlighting the need for MathInstruct\u2019s PoT-integrated, broad-coverage instruction data."
    }
  ],
  "synthesis_narrative": "Chain-of-thought prompting established that supervising models with step-by-step natural language solutions substantially improves mathematical reasoning, leading to widespread use of CoT-style training data. In parallel, program-based reasoning demonstrated that generating executable code can offload arithmetic and algorithmic steps to a Python interpreter; both Program-of-Thoughts prompting and PAL provided concrete recipes showing how code-as-rationale and execution materially improve math performance. Foundational datasets reinforced these supervision formats: GSM8K offered carefully curated word problems with explicit rationales that fit the CoT paradigm, while the MATH dataset defined a competition-level, topic-diverse target distribution and solution style that emphasized breadth and rigor. On the modeling side, WizardMath introduced math-specific instruction tuning via Evol-Instruct, but largely centered on CoT-only synthetic data; MetaMath scaled math ability via self-improvement loops yet similarly relied on CoT-centric supervision with limited executable traces or systematic domain coverage. Together, these works revealed that CoT supervision is powerful but incomplete for tasks benefitting from tool use, that program execution can close this gap, and that existing math instruction corpora are either narrow or code-sparse. The natural next step was to unify these strands by constructing an instruction-tuning corpus that deliberately mixes CoT and executable PoT rationales across diverse mathematical subfields, enabling models to flexibly choose natural language reasoning or program generation and thereby become generalist math solvers that outperform CoT-only instruction-tuned baselines.",
  "target_paper": {
    "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning",
    "authors": "Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Math Reasoning, Instruction Tuning, Large Language Model",
    "abstract": "We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 2",
    "openreview_id": "yLClGs770I",
    "forum_id": "yLClGs770I"
  },
  "analysis_timestamp": "2026-01-06T23:09:06.583274"
}
{
  "prior_works": [
    {
      "title": "Non-local Neural Networks",
      "authors": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He",
      "year": 2018,
      "role": "Foundational pairwise self-attention in vision",
      "relationship_sentence": "NSDA explicitly departs from the quadratic, pairwise-similarity formulation of Non-local Networks by replacing global similarity aggregation with local, parameter-free self-dissimilarity, directly addressing the accuracy\u2013complexity burden introduced by non-local/self-attention."
    },
    {
      "title": "A Non-Local Algorithm for Image Denoising (Non-Local Means)",
      "authors": "Antoni Buades, Bartomeu Coll, Jean-Michel Morel",
      "year": 2005,
      "role": "Origin of patch-based (dis)similarity weighting without learning",
      "relationship_sentence": "The core idea of computing neighborhood-based (dis)similarity to weight contributions\u2014without learnable parameters\u2014directly inspires NSDA\u2019s parameter-free, neighborhood-centric attention scores."
    },
    {
      "title": "Matching Local Self-Similarities Across Images and Videos",
      "authors": "Eli Shechtman, Michal Irani",
      "year": 2007,
      "role": "Self-similarity descriptor underpinning structure-from-internal comparisons",
      "relationship_sentence": "NSDA\u2019s focus on internal neighborhood contrast (self-dissimilarity) echoes the self-similarity principle that local internal comparisons capture salient structure, guiding how NSDA highlights diagnostically distinctive regions."
    },
    {
      "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "authors": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo",
      "year": 2021,
      "role": "Local windowed attention for efficiency",
      "relationship_sentence": "Swin\u2019s windowed attention establishes the efficacy of restricting attention to neighborhoods for tractability; NSDA adopts a neighborhood focus but eliminates learned attention parameters and flips to dissimilarity to further cut cost without sacrificing accuracy."
    },
    {
      "title": "Neighborhood Attention Transformer",
      "authors": "Ali Hassani, Steven Walton, Jiachen Li, Ehsan Adeli, Humphrey Shi",
      "year": 2023,
      "role": "Neighborhood-based efficient attention design",
      "relationship_sentence": "NAT shows that attention confined to local neighborhoods preserves accuracy with lower complexity; NSDA builds on this locality but computes parameter-free, size-adaptive dissimilarity weights instead of learned similarity scores."
    },
    {
      "title": "CCNet: Criss-Cross Attention for Semantic Segmentation",
      "authors": "Zilong Huang, Xiaoyong Wei, Xihui Liu, Hanchao Li, Gang Yu, Yichen Wei",
      "year": 2019,
      "role": "Sparse/structured attention to reduce quadratic cost in dense prediction",
      "relationship_sentence": "CCNet\u2019s structured sparsification motivates alternatives to full pairwise attention; NSDA provides a different route\u2014local, parameter-free dissimilarity\u2014that achieves efficiency without handcrafted sparse patterns."
    },
    {
      "title": "SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks",
      "authors": "Yang et al.",
      "year": 2021,
      "role": "Parameter-free attention feasibility",
      "relationship_sentence": "SimAM demonstrates that attention maps can be computed without learnable parameters; NSDA extends this ethos to spatial neighborhood attention using self-dissimilarity tailored for medical image segmentation."
    }
  ],
  "synthesis_narrative": "NSDA\u2019s core innovation\u2014parameter-free, neighborhood self-dissimilarity attention that breaks the accuracy\u2013complexity trade-off\u2014emerges at the intersection of three lines of work. First, Non-local Neural Networks established self-attention in vision via global pairwise similarity, but their quadratic cost and parameterization hinder deployment in resource-limited settings. A sequence of efficiency-centric attention designs (Swin\u2019s shifted-window attention, CCNet\u2019s criss-cross sparsification, and the Neighborhood Attention Transformer) showed that constraining attention to local neighborhoods retains performance while curbing complexity. NSDA adopts the neighborhood principle but goes further by discarding learned similarity projections in favor of deterministic, size-adaptive scoring.\nSecond, classical image processing introduced parameter-free, data-driven weighting based on internal comparisons. Non-Local Means grounded the idea that patch-level (dis)similarity can guide robust aggregation without training, while Shechtman and Irani\u2019s local self-similarity descriptor demonstrated that internal relational structure highlights salient patterns. NSDA reframes these insights for modern deep segmentation by computing neighborhood self-dissimilarity to emphasize diagnostically distinctive regions\u2014mirroring radiologists\u2019 focus on contrasts\u2014while avoiding pairwise global computation.\nThird, contemporary work on parameter-free attention (e.g., SimAM) validated that useful attention maps can be derived without additional parameters. NSDA synthesizes these strands: it retains the locality and scalability of windowed/neighborhood attention, inherits the nonparametric robustness of classical self-(dis)similarity, and leverages parameter-free computation to achieve low overhead. The result is an attention mechanism tailored for medical segmentation that highlights high-contrast regions with minimal compute, directly addressing real-world deployment constraints.",
  "analysis_timestamp": "2026-01-07T00:21:32.328248"
}
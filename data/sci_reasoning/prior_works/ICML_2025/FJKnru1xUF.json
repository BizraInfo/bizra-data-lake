{
  "prior_works": [
    {
      "title": "Explaining and Harnessing Adversarial Examples",
      "authors": "Ian J. Goodfellow et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "Established the core adversarial example problem and threat model that AutoAdvExBench operationalizes by asking whether LLM agents can execute the kinds of attacks adversarial ML researchers perform in practice."
    },
    {
      "title": "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods",
      "authors": "Nicholas Carlini et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Framed the concrete task of exploiting adversarial example defenses (especially detectors), which AutoAdvExBench encodes directly as agent tasks requiring code inspection and adaptive attack design."
    },
    {
      "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples",
      "authors": "Anish Athalye et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Diagnosed why many defenses fail (e.g., gradient masking) and motivated evaluating adaptive exploitation; AutoAdvExBench explicitly tests whether LLM agents can recognize and exploit these failure modes across CTF-like and real-world defenses."
    },
    {
      "title": "The NIPS 2017 Adversarial Attacks and Defenses Competition",
      "authors": "Alexey Kurakin et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Established the attack-vs-defense benchmark paradigm that AutoAdvExBench extends from model-level evaluation to agentic, code-level exploitation of defenses, highlighting the difference between curated \u2018CTF-like\u2019 setups and real systems."
    },
    {
      "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse attacks (AutoAttack)",
      "authors": "Francesco Croce et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated that ensembles of complementary attacks yield stronger, more reliable evaluations; AutoAdvExBench adapts this insight by using ensembles of LLM agents to robustly probe defenses."
    },
    {
      "title": "Measuring the Cybersecurity Capabilities of AI Models",
      "authors": "OpenAI Preparedness Team",
      "year": 2024,
      "role": "Gap Identification",
      "relationship_sentence": "Used proxy cybersecurity tasks and highlighted limitations in assessing real offensive capability; AutoAdvExBench addresses this gap by directly measuring autonomous exploitation on tasks ML security experts actually perform."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Introduced the reasoning-and-acting agent paradigm with tool use that enables autonomous multi-step problem solving; AutoAdvExBench leverages this agentic setup to let LLMs inspect code, run tools, and iteratively craft exploits."
    }
  ],
  "synthesis_narrative": "AutoAdvExBench\u2019s core contribution\u2014benchmarking whether LLMs can autonomously exploit adversarial-example defenses\u2014stands on the adversarial ML foundation laid by Goodfellow et al., who formalized adversarial examples, and by Carlini & Wagner, who concretized the task of defeating defenses by adaptively bypassing detectors. Athalye et al. exposed the widespread pitfall of gradient obfuscation, creating a clear need to test whether attackers can recognize and exploit such failure modes\u2014precisely the kind of capability AutoAdvExBench measures in both CTF-like and real-world settings. The NeurIPS 2017 competition (Kurakin et al.) established the benchmark framing of attacks versus defenses; AutoAdvExBench extends that framing from model-level stress tests to realistic, code-level exploitation, revealing a stark gap between curated \u201chomework\u201d defenses and production code.\n\nMethodologically, Croce & Hein\u2019s AutoAttack showed that ensembles of complementary attacks yield far more reliable robustness evaluations; AutoAdvExBench transfers this principle to the agent era by employing ensembles of LLM agents to more thoroughly probe defenses. On the LLM side, ReAct introduced the reasoning-and-acting paradigm that enables autonomous tool use, a necessary scaffold for agents to read defense code, iterate, and execute exploits. Finally, OpenAI\u2019s 2024 Preparedness report highlighted that existing cyber evaluations often rely on proxies rather than real offensive tasks; AutoAdvExBench directly addresses this gap by evaluating exactly the workflows adversarial ML practitioners use, enabling immediate, practical utility if an LLM agent succeeds.",
  "analysis_timestamp": "2026-01-06T23:07:19.575035"
}
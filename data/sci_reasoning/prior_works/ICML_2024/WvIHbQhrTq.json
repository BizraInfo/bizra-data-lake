{
  "prior_works": [
    {
      "title": "Finite-Time Analysis of the Multiarmed Bandit Problem",
      "authors": "Peter Auer et al.",
      "year": 2002,
      "role": "Baseline",
      "relationship_sentence": "UCB is the canonical baseline whose regret is the benchmark in this paper\u2019s impossibility result and the starting point from which MIN-UCB is constructed and to which it reduces when offline data are uninformative."
    },
    {
      "title": "Asymptotically efficient adaptive allocation rules",
      "authors": "Tze Leung Lai et al.",
      "year": 1985,
      "role": "Foundation",
      "relationship_sentence": "Lai\u2013Robbins introduced the stochastic MAB formulation and instance-dependent lower bounds that MIN-UCB is proven to match, grounding the paper\u2019s claims of tight instance-dependent regret."
    },
    {
      "title": "The KL-UCB Algorithm for Bounded Stochastic Bandits",
      "authors": "Aur\u00e9lien Garivier et al.",
      "year": 2011,
      "role": "Extension",
      "relationship_sentence": "This work established the modern UCB-index framework for instance-dependent optimality via confidence bounds; MIN-UCB directly extends the UCB-index paradigm by incorporating a bounded bias term to safely exploit offline samples."
    },
    {
      "title": "Stochastic Multi-Armed Bandits in the Presence of Adversarial Corruptions",
      "authors": "Thodoris Lykouris et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "They showed that without a bound on corruption one cannot improve over standard stochastic-bandit guarantees, motivating this paper\u2019s core insight that a non-trivial upper bound on offline\u2013online discrepancy is necessary to beat UCB."
    },
    {
      "title": "Better Algorithms for Stochastic Bandits with Adversarial Corruptions",
      "authors": "Anupam Gupta et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Their corruption-robust UCB variants formalized how to adjust confidence indices using a known/controlled corruption budget; MIN-UCB adapts this bias-aware indexing idea to the offline\u2013online mismatch setting with provably tight regret."
    },
    {
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "authors": "Aviral Kumar et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "CQL\u2019s pessimism principle\u2014use offline data only when it can be trusted under distribution shift\u2014directly informs MIN-UCB\u2019s design that adaptively leverages offline samples only when a known discrepancy bound certifies informativeness."
    }
  ],
  "synthesis_narrative": "The paper builds squarely on the stochastic bandit canon and the robustness insights from corruption-robust and offline-learning lines of work. Lai and Robbins (1985) provide the foundational problem formulation and instance-dependent lower bounds that underpin the paper\u2019s tight-regret claims. Auer et al. (2002) introduced UCB, the baseline both theoretically and algorithmically: the paper\u2019s impossibility theorem explicitly states that, without a non-trivial bound on distribution shift between offline and online data, no non-anticipatory policy can outperform UCB\u2019s guarantees. Garivier and Capp\u00e9 (2011) further established the UCB-index methodology for attaining instance-dependent optimality via concentration inequalities; MIN-UCB is a direct extension of this index-based approach, augmenting the confidence bounds with a bounded-bias term derived from the offline\u2013online discrepancy. The necessity of a discrepancy bound is foreshadowed by Lykouris and Sridharan (2018), who showed in corrupted-feedback bandits that improvement over clean-bandit rates is impossible without bounding corruption\u2014precisely the gap this paper formalizes for offline data. Gupta, Koren, and Talwar (2019) refined how to incorporate a corruption budget into UCB-style indices, a technique MIN-UCB adapts to the bias between offline and online reward distributions. Finally, the conservative/pessimistic principle from offline RL (Kumar et al., 2020) directly inspires MIN-UCB\u2019s adaptive behavior: exploit offline data when certified informative by the bound, otherwise ignore it\u2014yielding provably tight instance-independent and dependent regret.",
  "analysis_timestamp": "2026-01-06T23:09:26.430625"
}
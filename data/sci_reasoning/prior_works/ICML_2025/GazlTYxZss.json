{
  "prior_works": [
    {
      "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "The paper adopts the LLM-as-a-judge paradigm as a primary baseline for attributing responsibility from agent interaction logs, and shows that judge-style prompting is insufficient for reliable agent/step attribution in multi-agent settings."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "authors": "Noah Shinn et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "Reflexion\u2019s self-critique over past trajectories directly inspires one of the proposed attribution methods that analyzes conversation histories to identify which prior action caused failure, extending critique from single-agent self-debugging to multi-agent blame assignment."
    },
    {
      "title": "Let's Verify Step by Step",
      "authors": "Tal Schuster Lightman et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "This work formalizes step-level verification of reasoning processes, which the paper generalizes from single-model chains-of-thought to multi-agent dialogues by defining and evaluating step-level failure attribution across agents."
    },
    {
      "title": "Counterfactual Multi-Agent Policy Gradients (COMA)",
      "authors": "Jakob N. Foerster et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "COMA introduces multi-agent credit assignment via counterfactual baselines; the paper\u2019s core formulation\u2014assigning responsibility to specific agents and steps\u2014transposes this credit-assignment perspective to LLM multi-agent logs."
    },
    {
      "title": "A Unified Approach to Interpreting Model Predictions",
      "authors": "Scott M. Lundberg et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "The Shapley-value view of attribution motivates the paper\u2019s counterfactual/ablation-style failure attribution methods that estimate each step\u2019s contribution to eventual failure in a dialogue trajectory."
    },
    {
      "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace",
      "authors": "Yongliang Shen et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "By orchestrating multiple models/tools via an LLM controller, HuggingGPT highlights complex multi-agent pipelines but provides no mechanism to localize which component caused failures\u2014an explicit gap the paper addresses with automated failure attribution."
    },
    {
      "title": "Improving Factuality of Large Language Models via Multi-Agent Debate",
      "authors": "Yilun Du et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "Debate frameworks rely on judge models to arbitrate multi-agent interactions; the paper leverages and stress-tests such judge-style reasoning for attribution and shows its limitations for pinpointing failure-causing agents and steps."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central idea\u2014automated attribution of failures to specific agents and steps in LLM multi-agent systems\u2014sits at the intersection of process evaluation, critique-based diagnosis, and multi-agent credit assignment. Step-level verification from \u201cLet\u2019s Verify Step by Step\u201d provides the methodological foundation for reasoning about correctness at intermediate steps, which this work generalizes from single-model chains-of-thought to multi-agent conversational trajectories. From the multi-agent learning literature, COMA\u2019s counterfactual credit assignment supplies a principled lens for assigning responsibility across agents and time, and Shapley-value-based attribution (SHAP) motivates counterfactual/ablation estimators that quantify each step\u2019s contribution to failure.\n\nOn the systems and evaluation side, the community has leaned heavily on LLM-as-a-Judge to score outputs and even processes; this paper adopts it as a primary baseline and demonstrates that judge-style prompting is unreliable for failure attribution in multi-agent logs. Reflexion\u2019s self-critique over past trajectories directly informs one of the proposed methods, extending critique from single-agent self-debugging to multi-agent blame assignment over interaction histories. Finally, recent multi-agent orchestration systems such as HuggingGPT expose the real-world complexity of agent/tool pipelines while lacking mechanisms to localize faults; this conspicuous gap motivates the Who&When dataset and the formalization of automated failure attribution. Together, these works directly enable the formulation, baselines, and methodological choices that define the paper\u2019s core contribution.",
  "analysis_timestamp": "2026-01-06T23:07:19.641270"
}
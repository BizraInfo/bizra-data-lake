{
  "prior_works": [
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "GSDM adopts the DDPM forward\u2013reverse diffusion framework and training objective, but restructures the denoiser to mirror a problem-specific graphical model rather than using a generic network."
    },
    {
      "title": "Structured Denoising Diffusion Models in Discrete State Spaces",
      "authors": "Jacob Austin et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "GSDM builds directly on discrete diffusion (D3PM) by using categorical corruption/denoising kernels for combinatorial variables and extends it with factor- and variable-wise parameterization aligned to a given factor graph."
    },
    {
      "title": "DiGress: Discrete Denoising Diffusion for Graph Generation",
      "authors": "Thibaut Vignac et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "DiGress demonstrated permutation-equivariant discrete diffusion on graphs but was limited to graph generation; GSDM addresses this gap by generalizing diffusion to arbitrary problem-specified graphical models with explicit subcomputations (e.g., Sudoku constraints, sorting comparators)."
    },
    {
      "title": "Neural Message Passing for Quantum Chemistry",
      "authors": "Justin Gilmer et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "GSDM\u2019s denoiser is instantiated as message passing on the provided factor/variable graph, directly leveraging the MPNN paradigm to compute factor-to-variable and variable-to-factor updates during denoising."
    },
    {
      "title": "Deep Sets",
      "authors": "Manzil Zaheer et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "GSDM exploits permutation invariances by tying parameters and using permutation-invariant/equivariant aggregations across isomorphic subcomputations, following the Deep Sets framework."
    },
    {
      "title": "Recurrent Relational Networks",
      "authors": "Rasmus Berg Palm et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "RRN formulated Sudoku as iterative message passing over a constraint graph; GSDM retains this structured problem formulation but replaces recurrent relational updates with a diffusion-based generative denoiser and achieves better scaling."
    },
    {
      "title": "Conditional Random Fields as Recurrent Neural Networks",
      "authors": "Shuai Zheng et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "CRF-as-RNN established how to turn graphical-model inference into a neural architecture; GSDM similarly maps a factor graph into a learnable network, but for diffusion denoising rather than mean-field CRF inference."
    }
  ],
  "synthesis_narrative": "Graphically Structured Diffusion Models sit at the intersection of diffusion-based generative modeling and structured, graph-aligned neural computation. The DDPM framework provides the core probabilistic machinery\u2014forward noising, reverse denoising, and training objectives\u2014that GSDM directly adopts. Austin et al.\u2019s discrete diffusion (D3PM) extends these ideas to categorical variables; GSDM builds on this to operate in combinatorial domains, but crucially restructures the denoiser so that its computations and parameter tying align with a user-specified factor graph and explicit subcomputations.\n\nThis graph-aligned design draws on two foundational strands. First, the message passing paradigm of MPNNs provides the operational template for GSDM\u2019s factor-to-variable and variable-to-factor updates during denoising. Second, Deep Sets underwrites the exploitation of permutation invariances, enabling weight sharing and invariant/equivariant aggregation across interchangeable variables or repeated substructures, which is key to improved scaling.\n\nRecent discrete diffusion on graphs (DiGress) showed the benefits of symmetry-respecting denoisers but was confined to graph generation; GSDM closes this gap by generalizing to arbitrary, problem-defined graphical structures (e.g., Sudoku constraints, sorting comparators, matrix-factorization relations) and by allowing explicit subcomputation modules. Historically, CRF-as-RNN demonstrated how to cast probabilistic graphical inference as neural layers; GSDM echoes this architectural mapping but situates it within diffusion-based generative modeling. Finally, recurrent relational networks provided a structured baseline for Sudoku via iterative message passing; GSDM retains the graph-based formulation yet replaces the update rule with a diffusion denoiser, yielding superior accuracy and scaling with problem dimension.",
  "analysis_timestamp": "2026-01-06T23:09:26.520903"
}
{
  "prior_works": [
    {
      "title": "Synaesthesia\u2014A Window into Perception, Thought and Language",
      "authors": "V. S. Ramachandran, E. M. Hubbard",
      "year": 2001,
      "role": "Psycholinguistic foundation of the bouba/kiki effect and cross-modal sound\u2013shape correspondences",
      "relationship_sentence": "This work popularized and formalized the bouba/kiki effect that the paper operationalizes as a testbed for probing sound symbolism in modern vision\u2013language models."
    },
    {
      "title": "Gestalt Psychology (introducing the takete\u2013maluma phenomenon)",
      "authors": "Wolfgang K\u00f6hler",
      "year": 1929,
      "role": "Original demonstration of systematic mappings between nonsense word sounds and visual shapes",
      "relationship_sentence": "K\u00f6hler\u2019s early takete\u2013maluma findings provide the seminal behavioral paradigm that the paper translates into zero-shot computational probes for CLIP and Stable Diffusion."
    },
    {
      "title": "Arbitrariness, iconicity, and systematicity in language",
      "authors": "Mark Dingemanse, Dami\u00e1n E. Blasi, Gary Lupyan, Morten H. Christiansen, Padraic Monaghan",
      "year": 2015,
      "role": "Theoretical synthesis establishing sound symbolism (iconicity) as a robust, cross-linguistic phenomenon",
      "relationship_sentence": "By framing sound symbolism as a pervasive linguistic principle, this paper motivates testing whether such iconic mappings are internalized by multimodal neural models."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, et al.",
      "year": 2021,
      "role": "Core vision\u2013language architecture enabling zero-shot image\u2013text alignment",
      "relationship_sentence": "CLIP is the primary model probed in the paper, whose zero-shot retrieval/classification capabilities make it possible to test for emergent bouba/kiki-like associations without finetuning."
    },
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models (Stable Diffusion)",
      "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
      "year": 2022,
      "role": "Text-to-image generative framework for probing linguistic-to-visual mappings",
      "relationship_sentence": "Stable Diffusion provides the generative counterpart the paper probes, allowing the authors to test whether text prompts with kiki/bouba-like phonology elicit diagnostic visual shapes."
    },
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller",
      "year": 2019,
      "role": "Methodological basis for zero-shot knowledge probing in pretrained models",
      "relationship_sentence": "The paper adapts the zero-shot probing paradigm exemplified by LAMA to the multimodal setting, treating sound\u2013shape correspondences as \u2018facts\u2019 to elicit from VLMs without supervision."
    },
    {
      "title": "Winoground: Probing Vision and Language Models for Grounded Language Understanding",
      "authors": "Catherine Wong (Thrush), Max Bain, Gabriel Ilharco, Jack Hessel, Yonatan Bisk, et al.",
      "year": 2022,
      "role": "Evaluation framework emphasizing fine-grained multimodal alignment via carefully controlled prompts",
      "relationship_sentence": "This work\u2019s controlled probing of VLM alignment informs the paper\u2019s experimental design, where carefully crafted nonce words and shape prompts test subtle cross-modal correspondences."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central insight\u2014probing whether vision\u2013language models internalize sound symbolism\u2014rests on a bridge between classic psycholinguistics and modern multimodal AI. The bouba/kiki phenomenon originates with K\u00f6hler\u2019s takete\u2013maluma demonstrations and was popularized by Ramachandran and Hubbard, establishing systematic mappings between phonetic form and visual shape. Dingemanse et al. synthesized such findings into a broader account of iconicity in language, providing a theoretical rationale to expect stable cross-modal associations that might surface in learned representations.\n\nTechnically, the work is enabled by two pillars of multimodal modeling. First, CLIP (Radford et al.) offers a powerful text\u2013image alignment mechanism that naturally supports zero-shot retrieval and classification, allowing the authors to test for kiki/bouba-like biases without task-specific training. Second, Stable Diffusion (Rombach et al.) supplies a generative pathway to elicit visual realizations of textual prompts, making it possible to see whether phonological cues alone steer image synthesis toward spiky or rounded shapes.\n\nMethodologically, the paper adapts the zero-shot knowledge probing paradigm of Petroni et al., recasting sound\u2013shape correspondences as latent \u2018facts\u2019 that can be elicited from pretrained models. Finally, inspiration from controlled multimodal evaluations like Winoground informs the careful prompt design needed to isolate subtle cross-modal effects. Together, these strands yield a novel computational test of sound symbolism in VLMs, showing that emergent representations echo well-established human perceptual\u2013linguistic mappings.",
  "analysis_timestamp": "2026-01-07T00:02:04.857487"
}
{
  "prior_works": [
    {
      "title": "The Dark (Patterns) Side of UX Design",
      "authors": "Colin M. Gray et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work provided the conceptual definition and typology of deceptive interface strategies that DarkBench operationalizes for LLM dialogue (e.g., anthropomorphic cues and retention hooks)."
    },
    {
      "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
      "authors": "Arunesh Mathur et al.",
      "year": 2019,
      "arxiv_id": "1907.07032",
      "role": "Foundation",
      "relationship_sentence": "Mathur et al.\u2019s empirically grounded taxonomy (including labels like \u201csneaking\u201d) directly informs DarkBench\u2019s category structure and prompt design for detecting manipulative behaviors."
    },
    {
      "title": "Shining a Light on Dark Patterns",
      "authors": "Jamie Luguri and Lior Jacob Strahilevitz",
      "year": 2021,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By showing causal effects of dark patterns on user choices and highlighting regulatory blind spots, this paper motivates DarkBench\u2019s focus on measuring manipulative outcomes in conversational AI interfaces."
    },
    {
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "authors": "Stephanie Lin et al.",
      "year": 2021,
      "arxiv_id": "2109.07958",
      "role": "Extension",
      "relationship_sentence": "TruthfulQA\u2019s paradigm of eliciting imitative falsehoods to quantify untruthfulness is adapted in DarkBench to dialog settings as part of its untruthful/manipulative communication dimension."
    },
    {
      "title": "Discovering Language Model Behaviors with Model-Written Evaluations",
      "authors": "Ethan Perez et al.",
      "year": 2022,
      "arxiv_id": "2212.09251",
      "role": "Extension",
      "relationship_sentence": "This work introduced standardized sycophancy evaluations for LLMs, which DarkBench generalizes into a benchmark category spanning multiple prompt templates and model providers."
    },
    {
      "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
      "authors": "Samuel Gehman et al.",
      "year": 2020,
      "arxiv_id": "2009.11462",
      "role": "Related Problem",
      "relationship_sentence": "RealToxicityPrompts established toxic-seed prompting and toxicity measurement for harmful generation, informing DarkBench\u2019s harmful-content probes within a dark-pattern framing."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou et al.",
      "year": 2023,
      "arxiv_id": "2307.15043",
      "role": "Related Problem",
      "relationship_sentence": "By showing that aligned models can be reliably jailbroken via universal prompts, this paper motivates DarkBench\u2019s adversarial-style probes to expose manipulative behaviors that persist despite safety training."
    }
  ],
  "synthesis_narrative": "Research on deceptive interface design established the vocabulary and mechanisms relevant to manipulative behavior. Gray et al. codified \u201cdark patterns\u201d as deliberate interface strategies that nudge or mislead, including anthropomorphic cues and retention tactics that exploit social heuristics. Mathur et al. then grounded this concept empirically at scale, introducing a usable taxonomy\u2014such as sneaking\u2014that linked specific patterns to observable behaviors. Luguri and Strahilevitz demonstrated that these patterns causally alter consumer choices and identified regulatory gaps, clarifying the stakes and the need for measurement with behavioral endpoints. In parallel, benchmarks for language models began isolating manipulative or socially harmful dimensions: TruthfulQA formalized eliciting imitative falsehoods to quantify untruthfulness; Perez et al. introduced standardized sycophancy tests, showing models\u2019 tendency to agree with users irrespective of truth; RealToxicityPrompts provided a recipe\u2014seeded prompts and toxicity measurement\u2014to test harmful generation; and Zou et al. showed universal jailbreak prompts can bypass safety, revealing how aligned models still exhibit problematic behaviors under pressure. Collectively, these works revealed precise behaviors (sycophancy, untruthfulness, harmful outputs) and a taxonomy of manipulative design (sneaking, retention, anthropomorphism), but lacked a unified, domain-specific benchmark for conversational LLMs. The natural next step was to synthesize the HCI dark-pattern taxonomy with LLM behavior evaluations into a comprehensive, category-driven suite that probes manipulative behaviors\u2014including branding favoritism and user-retention nudges\u2014across providers, standardizing prompts and metrics to surface systematic, ethically relevant failures.",
  "target_paper": {
    "title": "DarkBench: Benchmarking Dark Patterns in Large Language Models",
    "authors": "Esben Kran, Hieu Minh Nguyen, Akash Kundu, Sami Jawhar, Jinsuk Park, Mateusz Maria Jurewicz",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Dark Patterns, AI Deception, Large Language Models",
    "abstract": "We introduce DarkBench, a comprehensive benchmark for detecting dark design patterns\u2014manipulative techniques that influence user behavior\u2014in interactions with large language models (LLMs). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies (OpenAI, Anthropic, Meta, Mistral, Google) and find that some LLMs are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing LLMs should recognize and mitigate the impact of dark design patterns to promote more ethical Al.",
    "openreview_id": "odjMSBSWRt",
    "forum_id": "odjMSBSWRt"
  },
  "analysis_timestamp": "2026-01-06T05:49:03.842595"
}
{
  "prior_works": [
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "arxiv_id": "2210.03629",
      "role": "Foundation",
      "relationship_sentence": "By formalizing an interleaved reasoning\u2013acting loop with natural language traces and tool/API calls, ReAct directly motivates training a single model to fluently switch between language reasoning and executable actions (code/tool use) that Lemur aims to unify."
    },
    {
      "title": "PAL: Program-Aided Language Models",
      "authors": "Luyu Gao et al.",
      "year": 2023,
      "arxiv_id": "2211.10435",
      "role": "Inspiration",
      "relationship_sentence": "PAL shows that generating and executing short programs markedly improves reasoning, providing the key insight that strong code-generation competence is a direct path to better agentic problem solving that Lemur explicitly targets."
    },
    {
      "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "authors": "Timo Schick et al.",
      "year": 2023,
      "arxiv_id": "2302.04761",
      "role": "Related Problem",
      "relationship_sentence": "Toolformer\u2019s self-supervised learning of API calls from raw text informs Lemur\u2019s mixed text+code supervision to ground language responses in executable, tool-using behaviors."
    },
    {
      "title": "Code Llama: Open Foundation Models for Code",
      "authors": "Baptiste Rozi\u00e8re et al.",
      "year": 2023,
      "arxiv_id": "2308.12950",
      "role": "Baseline",
      "relationship_sentence": "As a state-of-the-art open code-pretrained baseline that excels at coding but lags in general language, Code Llama concretely defines the code\u2013chat trade-off Lemur addresses via code-intensive pretraining without sacrificing natural language ability."
    },
    {
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "authors": "Hugo Touvron et al.",
      "year": 2023,
      "arxiv_id": "2307.09288",
      "role": "Baseline",
      "relationship_sentence": "Llama-2-Chat serves as the strong NL-aligned baseline that is comparatively weak on code/tool use, providing the opposite pole that Lemur bridges through joint text+code instruction tuning."
    },
    {
      "title": "WizardCoder: Empowering Code LLMs with Evol-Instruct",
      "authors": "Luo et al.",
      "year": 2023,
      "arxiv_id": "2306.08568",
      "role": "Gap Identification",
      "relationship_sentence": "WizardCoder\u2019s evol-instruct code tuning dramatically boosts coding while degrading general NL skills, a concrete trade-off that Lemur\u2019s harmonized instruction mixture is designed to fix."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2023,
      "arxiv_id": "2212.10560",
      "role": "Extension",
      "relationship_sentence": "The self-instruct pipeline for creating diverse instruction-following data is directly extended by Lemur to co-curate balanced text and code instructions for alignment without over-specializing."
    }
  ],
  "synthesis_narrative": "ReAct established a language-agent paradigm where models interleave natural-language reasoning with concrete actions via tool or API calls, defining the need for systems that fluently move between explanation and execution. PAL demonstrated that delegating intermediate steps to generated Python code and executing them yields large gains on reasoning tasks, isolating code generation as a key mechanism for stronger problem solving. Toolformer showed language models can learn API-calling behaviors from text-only corpora via self-supervision, indicating that mixed supervision on language and tool-use traces can ground responses in executable actions. Code Llama revealed that large-scale code-centric pretraining produces excellent coding ability but tends to erode general language competence. Conversely, Llama-2-Chat highlighted that instruction alignment for dialogue yields strong natural-language skills but comparatively weak coding and tool-use. WizardCoder pushed code instruction tuning with evol-instruct, boosting coding benchmarks while noticeably hurting general NL performance. Self-Instruct provided a practical recipe to synthesize diverse instruction-following data at scale, enabling targeted alignment beyond purely human-written prompts.\n\nTogether, these works expose a consistent opportunity: code-focused training improves tool use and reasoning but sacrifices dialogue and generalization, while chat alignment does the opposite. The natural next step is to harmonize these strengths by combining code-intensive pretraining with a carefully balanced instruction mixture spanning both text and code, leveraging self-instruct style generation and tool-use traces to preserve alignment while retaining executable competence\u2014precisely the synthesis that enables a single backbone model to reason, plan, and act as a versatile language agent.",
  "target_paper": {
    "title": "Lemur: Harmonizing Natural Language and Code for Language Agents",
    "authors": "Yiheng Xu, Hongjin SU, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "large language model, agent, code generation, reasoning, decision making",
    "abstract": "We introduce Lemur and Lemur-Chat, openly accessible language models optimized\nfor both natural language and coding capabilities to serve as the backbone\nof versatile language agents. The evolution from language chat models to\nfunctional language agents demands that models not only master human interaction,\nreasoning, and planning but also ensure grounding in the relevant environments.\nThis calls for a harmonious blend of language and coding capabilities\nin the models. Lemur and Lemur-Chat are proposed to address this necessity,\ndemonstrating balanced proficiencies in both domains, unlike existing\nopen-source models that tend to specialize in either. Through meticulous pretraining\nusing a code-intensive corpus and instruction fine-tuning on text and code\ndata, our models achieve state-of-the-art averaged performance across diverse\ntext and coding benchmarks. Comprehensive experiments demonstrate Lemur\u2019s\nsuperiority over existing open-source models and its proficiency across various\nage",
    "openreview_id": "hNhwSmtXRh",
    "forum_id": "hNhwSmtXRh"
  },
  "analysis_timestamp": "2026-01-06T15:06:47.824720"
}
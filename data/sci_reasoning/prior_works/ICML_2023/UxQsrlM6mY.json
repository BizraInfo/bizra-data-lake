{
  "prior_works": [
    {
      "title": "Dual Parameterization of Sparse Variational Gaussian Processes",
      "authors": "S. T. John et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "The proposed method directly builds on the dual SVGP reparameterization\u2014using its natural-parameter (information-form) pseudo-observation view to accumulate and combine evidence\u2014extending it to a sequential, memory-based setting with active memory updates for generic likelihoods."
    },
    {
      "title": "Variational Learning of Inducing Variables in Sparse Gaussian Processes",
      "authors": "Michalis K. Titsias et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "Introduced the variational inducing-point framework that underlies both SVGP and the dual-SVGP formulations; the paper\u2019s memory is precisely a variational pseudo-point summary of past data in this framework."
    },
    {
      "title": "Gaussian Processes for Big Data",
      "authors": "James Hensman et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "Established the stochastic variational GP (SVGP) approach that dual-SVGP reparameterizes; the present work inherits this variational objective while changing the parameterization to enable stable sequential accumulation via a memory."
    },
    {
      "title": "Scalable Variational Gaussian Process Classification",
      "authors": "James Hensman et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "Provided the SVGP treatment for generic (non-Gaussian) likelihoods, which the present paper leverages within the dual parameterization to support accurate sequential inference beyond Gaussian regression."
    },
    {
      "title": "Sparse Online Gaussian Processes",
      "authors": "Lehel Csat\u00f3 et al.",
      "year": 2002,
      "role": "Inspiration",
      "relationship_sentence": "Introduced the idea of maintaining a compact, online \u2018memory\u2019 via sparse pseudo-points; the current method adopts this memory concept but replaces EP-style updates with dual-SVGP variational information-form updates to reduce error accumulation."
    },
    {
      "title": "Streaming Sparse Gaussian Process Approximations",
      "authors": "Thang D. Bui et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "A primary sequential GP baseline that summarizes past data with variational pseudo-points; the paper explicitly addresses its drift/error accumulation by using dual-parameter updates and active memory management."
    },
    {
      "title": "Rates of Convergence for Sparse Variational Gaussian Process Regression",
      "authors": "David R. Burt et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Showed that approximation accuracy hinges on the number and placement of inducing points, motivating the paper\u2019s active memory-building and inducing-point updates to keep errors in check over time."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014accurate sequential learning with Gaussian processes via an actively maintained memory\u2014rests on a precise lineage in sparse variational GP methodology. Titsias (2009) introduced the variational inducing-point formulation that defines pseudo-points as variational summaries of data, while Hensman et al. (2013, 2015) made this scalable and applicable to generic likelihoods through SVGP. The recently proposed dual parameterization of SVGP (John et al., 2022) is the key enabling step: by expressing the variational posterior in information-form natural parameters (pseudo-observations), evidence from new data can be additively composed, which is ideal for sequential updates. Earlier streaming GP methods\u2014Csat\u00f3 and Opper (2002) and Bui et al. (2017)\u2014established the notion of online memory via sparse pseudo-points, but suffered from drift and accumulation of approximation errors, especially under non-Gaussian likelihoods and changing hyperparameters. Burt et al. (2019) theoretically highlighted that accuracy depends critically on the number and placement of inducing points, directly motivating the paper\u2019s active memory construction and update strategy. Combining the dual-SVGP\u2019s stable natural-parameter updates with principled, active management of inducing points yields a memory mechanism that curbs posterior, hyperparameter, and inducing-point errors, enabling accurate sequential inference for generic likelihoods across continual learning, active learning, and Bayesian optimization.",
  "analysis_timestamp": "2026-01-06T23:09:26.579245"
}
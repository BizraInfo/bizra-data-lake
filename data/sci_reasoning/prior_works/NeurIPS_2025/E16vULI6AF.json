{
  "prior_works": [
    {
      "title": "Supervised Contrastive Learning",
      "authors": "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan",
      "year": 2020,
      "role": "Foundational method for supervised contrastive representation learning",
      "relationship_sentence": "The proposed Balanced Supervised Contrastive Learning (B-SCL) directly builds on SupCon\u2019s anchor\u2013positive/negative formulation, modifying how pairs are weighted to better emphasize tail classes."
    },
    {
      "title": "Debiased Contrastive Learning",
      "authors": "Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, Stefanie Jegelka",
      "year": 2020,
      "role": "Contrastive learning refinement addressing sampling/false-negative bias",
      "relationship_sentence": "B-SCL\u2019s dynamic re-weighting of pair contributions is conceptually aligned with debiasing ideas in contrastive objectives to correct sampling imbalances that especially harm rare classes."
    },
    {
      "title": "Class-Balanced Loss Based on Effective Number of Samples",
      "authors": "Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, Serge Belongie",
      "year": 2019,
      "role": "Loss re-weighting for imbalanced data",
      "relationship_sentence": "The class-balanced weighting principle motivates B-SCL\u2019s frequency-aware adjustments, transferring effective-number reweighting ideas from cross-entropy to a supervised contrastive objective."
    },
    {
      "title": "Learning Imbalanced Datasets with Label-Distribution-Aware Margin (LDAM) Loss and Deferred Re-Weighting",
      "authors": "Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, Tengyu Ma",
      "year": 2019,
      "role": "Long-tailed recognition with margin adaptation and training schedule (DRW)",
      "relationship_sentence": "LDAM-DRW\u2019s insight that margins/weights should depend on class frequencies informs B-SCL\u2019s tail-aware margin/weighting in the contrastive space and its staged emphasis on tail classes."
    },
    {
      "title": "Decoupling Representation and Classifier for Long-Tailed Recognition",
      "authors": "Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Jiashi Feng, Yannis Kalantidis",
      "year": 2020,
      "role": "Decoupled training paradigm for robust representations under imbalance",
      "relationship_sentence": "The paper\u2019s end-to-end design prioritizing balanced representation learning echoes the decoupling principle, but integrates the balancing directly into representation learning via B-SCL rather than a post-hoc classifier stage."
    },
    {
      "title": "Long-Tailed Classification via Logit Adjustment",
      "authors": "Aditya K. Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, Sanjiv Kumar",
      "year": 2021,
      "role": "Prior-aware calibration for imbalanced classification",
      "relationship_sentence": "B-SCL\u2019s distribution-aware weighting parallels logit adjustment\u2019s use of class priors, extending the idea from classifier calibration to pairwise contrastive objectives for tail-feature strengthening."
    },
    {
      "title": "Focal Loss for Dense Object Detection",
      "authors": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r",
      "year": 2017,
      "role": "Imbalance-robust loss emphasizing hard/rare examples",
      "relationship_sentence": "The emphasis-on-rare/hard examples principle in Focal Loss underpins B-SCL\u2019s mechanism of amplifying tail-class contributions within the supervised contrastive loss."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014an end-to-end framework for pathological long-tailed recognition (LTR) in scientific datasets driven by a Balanced Supervised Contrastive Learning (B-SCL) mechanism\u2014sits at the intersection of contrastive representation learning and class-imbalance remedies. Supervised Contrastive Learning established a powerful supervised pairwise objective that B-SCL adopts as its backbone, while Debiased Contrastive Learning highlighted how sampling and false-negative biases undermine contrastive training\u2014an issue that becomes acute for rare classes. From the LTR literature, Class-Balanced Loss and LDAM-DRW provide principled ways to tie training weights/margins to class frequencies, directly inspiring B-SCL\u2019s dynamic, frequency-aware reweighting of contrastive pairs and potential staged emphasis on tails. The decoupled training paradigm demonstrated that robust, class-balanced representations are pivotal; the proposed method internalizes this lesson by embedding balancing into the representation objective itself, rather than relying on a later classifier recalibration stage. Finally, Logit Adjustment\u2019s prior-aware calibration and Focal Loss\u2019s focus on rare/hard examples motivate B-SCL\u2019s tail-centric weighting, but now in the pairwise, representation-learning domain. Together, these works shape a method that strengthens tail-class features under extreme imbalance and limited sample regimes typical of scientific discovery, yielding representations that remain discriminative without overfitting scarce tails or being dominated by head classes.",
  "analysis_timestamp": "2026-01-06T23:42:48.106565"
}
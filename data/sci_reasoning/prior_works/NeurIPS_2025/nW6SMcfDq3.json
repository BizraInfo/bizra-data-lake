{
  "prior_works": [
    {
      "title": "A method of solving a convex programming problem with convergence rate O(1/k^2)",
      "authors": "Yurii Nesterov",
      "year": 1983,
      "role": "Foundational accelerated optimization benchmark",
      "relationship_sentence": "The paper targets and matches Nesterov\u2019s accelerated rates in discrete time; RHGD is analyzed specifically to achieve the same accelerated complexity guarantees as AGD."
    },
    {
      "title": "A differential equation for modeling Nesterov\u2019s accelerated gradient method",
      "authors": "Weijie Su, Stephen Boyd, Emmanuel J. Cand\u00e8s",
      "year": 2016,
      "role": "Continuous-time acceleration framework",
      "relationship_sentence": "RHF\u2019s accelerated continuous-time rates are positioned as the optimization analogue to the accelerated gradient flow analyzed by Su\u2013Boyd\u2013Cand\u00e8s, providing the ODE benchmark that RHF matches via randomized integration time."
    },
    {
      "title": "A Variational Perspective on Accelerated Methods in Optimization",
      "authors": "Andre Wibisono, Ashia C. Wilson, Michael I. Jordan",
      "year": 2016,
      "role": "Lagrangian/Hamiltonian analytical framework for acceleration",
      "relationship_sentence": "The variational (Bregman Lagrangian) viewpoint connects accelerated methods to mechanics; the present work leverages a Hamiltonian-flow perspective and Lyapunov/energy arguments in this spirit to analyze HF-opt and its randomized variant."
    },
    {
      "title": "MCMC using Hamiltonian dynamics",
      "authors": "Radford M. Neal",
      "year": 2011,
      "role": "Algorithmic template from sampling (HMC)",
      "relationship_sentence": "HF-opt is explicitly the optimization analogue of HMC, borrowing the simulate-then-refresh (velocity reset) structure from HMC to ensure descent in optimization rather than stationarity in sampling."
    },
    {
      "title": "Randomized Hamiltonian Monte Carlo",
      "authors": "Nawaf Bou-Rabee, Jes\u00fas Mar\u00eda Sanz-Serna",
      "year": 2018,
      "role": "Randomization mechanism in Hamiltonian integration",
      "relationship_sentence": "The idea to randomize the integration time originates from randomized HMC; this paper adapts and repurposes that mechanism to optimization, proving that randomizing integration time yields acceleration in RHF/RHGD."
    },
    {
      "title": "Adaptive Restart for Accelerated Gradient Methods",
      "authors": "Brendan O\u2019Donoghue, Emmanuel J. Cand\u00e8s",
      "year": 2015,
      "role": "Restart principle for momentum methods",
      "relationship_sentence": "HF-opt\u2019s periodic velocity reset echoes restart ideas; the paper\u2019s randomized resets connect conceptually to restart schemes that enforce descent and improve practical convergence in accelerated methods."
    },
    {
      "title": "Understanding the acceleration phenomenon via high-resolution differential equations",
      "authors": "Bin Shi, Simon S. Du, Michael I. Jordan, Weijie Su",
      "year": 2018,
      "role": "Discrete\u2013continuous bridge for accelerated rates",
      "relationship_sentence": "High-resolution ODE analyses inform how to discretize accelerated flows without losing rates; this perspective underpins the design and analysis of RHGD so that its discrete-time behavior matches AGD."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014showing that randomizing the integration time of a Hamiltonian-flow-based optimization scheme yields accelerated rates and a discrete algorithm (RHGD) matching AGD\u2014sits at the intersection of Hamiltonian Monte Carlo, continuous-time acceleration theory, and restart mechanisms in first-order methods. On the sampling side, Neal\u2019s exposition of HMC provides the simulate-then-refresh template: integrate Hamiltonian dynamics and resample momentum to avoid metastability. The authors transplant this structure into optimization by integrating Hamiltonian dynamics and resetting velocity to enforce descent. Crucially, insights from randomized HMC (Bou-Rabee and Sanz-Serna) motivate randomizing the integration time; in optimization, this breaks detrimental phase locking and produces accelerated continuous-time rates.\n\nOn the optimization side, Su\u2013Boyd\u2013Cand\u00e8s and Wibisono\u2013Wilson\u2013Jordan established that acceleration is naturally expressed via second-order (Lagrangian/Hamiltonian) flows and Lyapunov energies. Their frameworks supply the continuous-time targets and analytical tools that the randomized Hamiltonian flow matches. O\u2019Donoghue\u2013Cand\u00e8s\u2019 adaptive restart highlights how momentum resets can restore monotonic descent and enhance practical performance; the present work leverages a principled, randomized reset schedule. Finally, high-resolution ODE analysis (Shi\u2013Du\u2013Jordan\u2013Su) guides the discretization so that RHGD inherits the same accelerated rates as Nesterov\u2019s AGD for smooth strongly and weakly convex objectives. Together, these works directly inform the paper\u2019s mechanics-based algorithm design, the key randomization device, and the rate-preserving discretization.",
  "analysis_timestamp": "2026-01-07T00:21:32.275262"
}
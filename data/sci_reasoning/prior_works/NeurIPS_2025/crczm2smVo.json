{
  "prior_works": [
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco S. Cohen, Max Welling",
      "year": 2016,
      "role": "Methodological predecessor in equivariant deep learning",
      "relationship_sentence": "Introduced group convolutions as a general recipe for equivariant networks, whose limitations for complex, non-linear actions like projective transformations motivate replacing convolutional lifting with invariantization via differential invariants."
    },
    {
      "title": "General E(2)-Equivariant Steerable CNNs",
      "authors": "Maurice Weiler, Gabriele Cesa",
      "year": 2019,
      "role": "Methodological predecessor (steerable representations and equivariant differential operators)",
      "relationship_sentence": "Formalized steerable feature fields and equivariant differential operators on the plane, providing the immediate contrast to this paper\u2019s approach, which attains projective equivariance not by steerable filters but by constructing fundamental differential invariants."
    },
    {
      "title": "Generalizing Convolutional Neural Networks for Equivariance to Lie Groups",
      "authors": "Marc Finzi, Samuel Stanton, Pavel Izmailov, Andrew Gordon Wilson",
      "year": 2020,
      "role": "Methodological attempt at broad Lie-group equivariance",
      "relationship_sentence": "Extended equivariant convolutions to continuous Lie groups with coordinate-based kernels, highlighting practical and theoretical hurdles for highly non-linear actions (e.g., projective), thereby motivating an invariant-based alternative developed in the present work."
    },
    {
      "title": "Moving Coframes I/II: A Practical Algorithm; Regularization, Invariants, and Applications",
      "authors": "Mark Fels, Peter J. Olver",
      "year": 1998,
      "role": "Foundational method (moving frames) used to derive differential invariants",
      "relationship_sentence": "Provides the algorithmic moving frame framework\u2014cross-sections, invariantization, invariant differentiation\u2014directly used here to construct a complete and concise set of second-order fundamental projective differential invariants for multi-dimensional functions."
    },
    {
      "title": "Equivalence, Invariants, and Symmetry",
      "authors": "Peter J. Olver",
      "year": 1995,
      "role": "Foundational theory of differential invariants and invariant differential operators",
      "relationship_sentence": "Supplies the theoretical underpinnings (differential invariant algebras, invariant derivations, replacement theorem) that justify completeness, simplification, and unification of the projective invariant set used to build equivariant networks."
    },
    {
      "title": "Differential and Numerically Invariant Signature Curves Applied to Object Recognition",
      "authors": "Eugenio Calabi, Peter J. Olver, Chehrzad Shakiban, Allen Tannenbaum, Steven Haker",
      "year": 1998,
      "role": "Vision application of (projective/affine) differential invariants",
      "relationship_sentence": "Demonstrated how differential invariants and invariant signatures can be stably constructed for recognition, informing this paper\u2019s translation of projective differential invariants into practical, numerically amenable network features."
    },
    {
      "title": "Projective Differential Geometry Old and New: From the Schwarzian Derivative to the Cohomology of Diffeomorphism Groups",
      "authors": "Valentin Ovsienko, Serge Tabachnikov",
      "year": 2005,
      "role": "Geometric foundation for projective differential invariants",
      "relationship_sentence": "Catalogues fundamental projective invariants and invariant operators (e.g., Schwarzian-type quantities) and their interrelations, guiding the identification, transformation rules, and simplification of the second-order projective invariants used in the network design."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014constructing projective equivariant networks via second-order fundamental differential invariants\u2014sits at the intersection of equivariant deep learning and the classical moving frame theory of differential invariants. On the deep learning side, group equivariant and steerable CNNs established the modern recipe for embedding symmetry into architectures (Cohen & Welling, 2016; Weiler & Cesa, 2019). Yet these approaches assume relatively linear, homogeneous group actions and tractable representation theory; the projective group\u2019s highly non-linear action on images stretches these assumptions. Finzi et al. (2020) advanced equivariance to general Lie groups with coordinate-based kernels, but practical obstacles for projective transformations remain, motivating an alternative path.\n\nThat alternative is grounded in the moving frame program. Olver\u2019s monograph (1995) and the Fels\u2013Olver algorithm (1998/1999) provide the constructive machinery\u2014cross-sections, invariantization, invariant derivations, and completeness proofs\u2014to derive a generating set of differential invariants for a prescribed group action. Building on these, the present paper tailors a cross-section for multi-dimensional functions and rigorously analyzes the resulting projective invariants and their interrelations to obtain a simplified fundamental second-order set. Ovsienko & Tabachnikov (2005) contributes the projective differential geometry background (e.g., Schwarzian-type invariants and invariant operators) that informs the identification and reduction of invariants. Finally, Calabi et al. (1998) bridge theory to practice by showing how differential invariant signatures can be made numerically stable for vision tasks. Together, these works directly enable the paper\u2019s strategy: replacing steerable or group-convolutional mechanisms with a principled invariantization pipeline that yields projective-equivariant architectures built from fundamental differential invariants.",
  "analysis_timestamp": "2026-01-07T00:02:04.918449"
}
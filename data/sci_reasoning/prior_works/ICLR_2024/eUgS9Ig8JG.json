{
  "prior_works": [
    {
      "title": "Simplicial Neural Networks",
      "authors": "Giulia Ebli et al.",
      "year": 2020,
      "arxiv_id": "2010.03633",
      "role": "Gap Identification",
      "relationship_sentence": "This work formalized aggregation on simplicial complexes via boundary/coboundary and upper/lower adjacencies but requires on-the-fly multi-adjacency message passing, whose heavy training-time memory and compute costs are exactly the scalability pain-points SaNN removes with pre-aggregated simplicial-aware features."
    },
    {
      "title": "Weisfeiler and Leman Go Topological: Message Passing Simplicial Networks",
      "authors": "Cristian Bodnar et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "MPSNs introduced WL-style message passing on simplices and established expressivity results, and SaNN is designed to match this expressivity while eliminating training-time message passing by precomputing the same simplicial interactions."
    },
    {
      "title": "Simplifying Graph Convolutional Networks",
      "authors": "Felix Wu et al.",
      "year": 2019,
      "arxiv_id": "1902.07153",
      "role": "Inspiration",
      "relationship_sentence": "SGC\u2019s key idea of decoupling propagation from learning by precomputing multi-hop aggregations directly inspires SaNN\u2019s pre-aggregation of simplicial interactions before a simple neural predictor."
    },
    {
      "title": "SIGN: Scalable Inception Graph Neural Networks",
      "authors": "Fabrizio Frasca et al.",
      "year": 2020,
      "arxiv_id": "2004.11198",
      "role": "Inspiration",
      "relationship_sentence": "SIGN demonstrated that multiple fixed graph diffusions can be precomputed and fed to a lightweight learner for scalability, which SaNN generalizes from graphs to simplicial complexes by precomputing boundary/coboundary and upper/lower adjacency-based features."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu et al.",
      "year": 2019,
      "arxiv_id": "1810.00826",
      "role": "Foundation",
      "relationship_sentence": "This paper established the expressivity lens connecting message-passing GNNs to the 1-WL test, a framework SaNN adopts to prove it surpasses WL while remaining as expressive as message passing on simplices under stated conditions."
    },
    {
      "title": "Weisfeiler and Leman Go Cellular: CW Networks",
      "authors": "Cristian Bodnar et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "By extending WL-style expressivity analysis to cell complexes, this work provided proof techniques and topological operators that inform SaNN\u2019s expressivity arguments and operator choices on simplicial complexes."
    }
  ],
  "synthesis_narrative": "Simplicial Neural Networks defined how to propagate information over simplicial complexes using boundary and coboundary maps together with upper and lower adjacencies, thereby operationalizing higher-order message passing but at the cost of large training-time memory and compute due to per-epoch aggregation. Weisfeiler and Leman Go Topological: Message Passing Simplicial Networks built a WL-style message passing framework on simplices, connecting topological aggregation to color-refinement expressivity and setting a high-expressivity baseline. Simplifying Graph Convolutional Networks showed that one can precompute multi-hop aggregations and train a simple classifier, decoupling propagation from learning to slash training costs. SIGN further generalized this idea by precomputing multiple diffusions and concatenating them as features to a small learner, demonstrating scalable training with constant-time epochs regardless of graph density. How Powerful are Graph Neural Networks? established the expressivity yardstick by relating MPNNs to the 1-WL test, a perspective later reused in topological settings. Weisfeiler and Leman Go Cellular: CW Networks extended WL-style reasoning from graphs to higher-order structures, offering proof strategies that translate to simplicial contexts. Together, these works expose a gap: simplicial message passing offers strong expressivity but suffers from prohibitive training-time aggregation, while precomputation on graphs yields scalability but lacks higher-order structure. SaNN synthesizes these strands by precomputing simplicial-aware features built from boundary/coboundary and upper/lower adjacencies, feeding them to a simple learner to achieve constant training-time/memory, and by leveraging WL-style analysis to show it exceeds 1-WL and matches MPSN-level power under stated conditions.",
  "target_paper": {
    "title": "SaNN: Simple Yet Powerful Simplicial-aware Neural Networks",
    "authors": "Sravanthi Gurugubelli, Sundeep Prabhakar Chepuri",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Graph Neural Networks, Higher-order Representation Learning, Simplicial Complexes, Simplicial Neural Networks, Weisfeiler-Lehman Isomorphism Test",
    "abstract": "Simplicial neural networks (SNNs) are deep models for higher-order graph representation learning. SNNs learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in SNNs is carried out during training. Since the number of simplices of various orders in a simplicial complex is significantly large, the memory and training-time requirement in SNNs is enormous. In this work, we propose a scalable simplicial-aware neural network (SaNN) model with a constant run-time and memory requirements independent of the size of the simplicial complex and the density of interactions in it. SaNN is based on pre-aggregated simplicial-aware features as inputs to a neural network, so it has a strong simplicial-structural inductive bias. We provide theoretical conditions under which SaNN is provably more powerful than the Weisfeiler-Lehman (WL) graph isomorphism test and as powerf",
    "openreview_id": "eUgS9Ig8JG",
    "forum_id": "eUgS9Ig8JG"
  },
  "analysis_timestamp": "2026-01-06T05:48:09.654225"
}
{
  "prior_works": [
    {
      "title": "Variational Inference for Diffusion Processes",
      "authors": "Archambeau et al.",
      "year": 2007,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Their variational framework derives an ELBO for Brownian-driven SDEs, which this paper generalizes to the Markov-augmented representation of fractional Brownian motion."
    },
    {
      "title": "Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit",
      "authors": "Tzen and Raginsky",
      "year": 2019,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "They cast latent SDEs as VAEs with pathwise reparameterization under Brownian noise, providing the core training setup that this work extends to fractional noise."
    },
    {
      "title": "Latent SDEs for Irregularly-Sampled Time Series",
      "authors": "Li et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "This practical VI framework for neural SDEs under Brownian motion motivates the need for a fractional-noise extension to capture long-range dependencies, which this paper provides."
    },
    {
      "title": "Turbocharging Monte Carlo pricing for the rough Bergomi model",
      "authors": "McCrickerd and Pakkanen",
      "year": 2018,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "They introduce multi-factor (sum-of-exponentials) approximations of fractional kernels that yield finite-dimensional Markov embeddings, the exact Markov approximation strategy adopted to enable VI with fBM."
    },
    {
      "title": "Markovian Structure of the Volterra Heston Model",
      "authors": "Abi Jaber and El Euch",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "They prove that Volterra processes with fractional kernels admit Markovian lifts via Laplace-transform measures, directly underpinning the state augmentation used to render fBM approximately Markov."
    },
    {
      "title": "The Characteristic Function of Rough Heston Models",
      "authors": "El Euch and Rosenbaum",
      "year": 2019,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "They demonstrate accurate finite-factor Markov approximations and constructive quadrature schemes for fractional kernels, informing the discretization/parameterization choices for the Markov-approximate fBM used here."
    }
  ],
  "synthesis_narrative": "Archambeau and colleagues developed a variational framework for diffusion processes that yields an evidence lower bound for Brownian-driven SDEs, establishing how to perform tractable inference over continuous-time latent dynamics. Tzen and Raginsky cast neural SDEs as VAE-style latent generative models with pathwise reparameterization under Brownian motion, while Li and co-authors operationalized this setup for irregularly sampled data and scalable training, consolidating Brownian-based neural SDE VI as a practical baseline. In parallel, McCrickerd and Pakkanen proposed multi-factor sum-of-exponentials approximations of fractional kernels, producing finite-dimensional Markov embeddings that mimic long-memory behavior with auxiliary Ornstein\u2013Uhlenbeck factors. Abi Jaber and El Euch formalized the Markovian lift of Volterra processes via Laplace-transform measures, providing theoretical justification for state augmentation that turns fractional dynamics into Markov ones. El Euch and Rosenbaum further demonstrated the practical accuracy of finite-factor approximations and offered constructive quadrature schemes for fractional kernels, validating these Markov embeddings for efficient computation.\nTaken together, these works suggest a clear opportunity: marry the proven VI machinery for Brownian SDEs with the Markovian lifts of fractional processes to capture long-range dependence without sacrificing tractability. By adopting multi-factor Markov approximations of fBM grounded in Volterra\u2013Laplace theory, and then plugging this augmented Markov system into the Brownian SDE ELBO and pathwise training pipelines, the present paper naturally extends neural SDE variational inference to fractional noise, overcoming the Brownian baseline\u2019s inability to model long-memory while retaining computational efficiency.",
  "target_paper": {
    "title": "Variational Inference for SDEs Driven by Fractional Noise",
    "authors": "Rembert Daems, Manfred Opper, Guillaume Crevecoeur, Tolga Birdal",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "variational inference, neural sdes, stochastic differential equations, brownian motion, fractional noise, fractional brownian motion, markov approximation, markov representation",
    "abstract": "We present a novel variational framework for performing inference in (neural) stochastic differential equations (SDEs) driven by Markov-approximate fractional Brownian motion (fBM). SDEs offer a versatile tool for modeling real-world continuous-time dynamic systems with inherent noise and randomness. Combining SDEs with the powerful inference capabilities of variational methods, enables the learning of representative distributions through stochastic gradient descent. However, conventional SDEs typically assume  the underlying noise to follow a Brownian motion (BM), which hinders their ability to capture long-term dependencies. In contrast, fractional Brownian motion (fBM) extends BM to encompass non-Markovian dynamics, but existing methods for inferring fBM parameters are either computationally demanding or statistically inefficient. \n\nIn this paper, building upon the Markov approximation of fBM, we derive the evidence lower bound essential for efficient variational inference of poster",
    "openreview_id": "rtx8B94JMS",
    "forum_id": "rtx8B94JMS"
  },
  "analysis_timestamp": "2026-01-06T23:20:17.961307"
}
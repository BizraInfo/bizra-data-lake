{
  "prior_works": [
    {
      "title": "An Analysis of Active Learning Strategies for Sequence Labeling",
      "authors": "Burr Settles, Mark Craven",
      "year": 2008,
      "role": "Foundational EMCM/EGL principle",
      "relationship_sentence": "Introduced expected model change via expected gradient length (EGL), the core idea the paper extends to graphs by deriving a closed-form EMCM acquisition over GNN-derived embeddings."
    },
    {
      "title": "Toward Optimal Active Learning through Monte Carlo Estimation of Error Reduction",
      "authors": "Nicholas Roy, Andrew McCallum",
      "year": 2001,
      "role": "Theoretical link to expected error minimization",
      "relationship_sentence": "Established the expected error reduction framework that this paper connects to by showing their EMCM criterion is directly tied to expected prediction error minimization with guarantees."
    },
    {
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "authors": "Thomas N. Kipf, Max Welling",
      "year": 2017,
      "role": "GNN backbone for semi-supervised node embeddings",
      "relationship_sentence": "Provides the standard GNN architecture and semi-supervised training setup whose node embeddings the paper endows with a Bayesian interpretation to compute model-change scores."
    },
    {
      "title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples",
      "authors": "Mikhail Belkin, Partha Niyogi, Vikas Sindhwani",
      "year": 2006,
      "role": "Graph/Laplacian prior for semi-supervised learning",
      "relationship_sentence": "Supplies the Laplacian-based regularization perspective that underpins a Gaussian/Bayesian view on functions over graphs, which the paper leverages to model uncertainty of GNN embeddings."
    },
    {
      "title": "Simplifying Graph Convolutional Networks",
      "authors": "Felix Wu, Amauri Holanda de Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Q. Weinberger",
      "year": 2019,
      "role": "Linearization of GNNs enabling closed-form analysis",
      "relationship_sentence": "Shows GNNs can be viewed as fixed graph propagation plus a linear classifier, a perspective that facilitates the paper\u2019s closed-form EMCM derivation without retraining."
    },
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh, Percy Liang",
      "year": 2017,
      "role": "Estimating parameter change without retraining",
      "relationship_sentence": "Provides the general technique of approximating parameter updates due to new labels, conceptually aligning with the paper\u2019s efficient computation of expected model change."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014an efficient, closed-form Expected Model Change Maximization (EMCM) criterion tailored to GNNs\u2014builds on three threads: active learning by model change/error, semi-supervised graph learning, and tractable approximations of parameter updates. Settles and Craven\u2019s expected gradient length formalizes EMCM, directly inspiring the paper\u2019s objective to quantify how much labeling a node would alter the model. Roy and McCallum\u2019s expected error reduction supplies the theoretical anchor; the authors explicitly connect their EMCM score to minimizing expected prediction error, yielding guarantees. On the graph learning side, Kipf and Welling\u2019s GCN defines the semi-supervised node-classification setting and produces the embeddings whose uncertainty the method must quantify. Manifold regularization provides the Laplacian-based prior that justifies a Bayesian perspective on graph-based representations, enabling the authors to derive a probabilistic characterization of GNN embeddings under semi-supervision. Wu et al.\u2019s SGC perspective (graph propagation followed by a linear classifier) makes the GNN amenable to closed-form analysis\u2014crucial for computing EMCM without retraining. Finally, Koh and Liang\u2019s influence functions motivate efficient estimation of parameter change, conceptually supporting the paper\u2019s training-free expected-update computation. Together, these works enable a principled, theoretically grounded acquisition function that marries EMCM with the Bayesian, semi-supervised structure of GNNs, achieving both accuracy and efficiency.",
  "analysis_timestamp": "2026-01-07T00:02:04.789546"
}
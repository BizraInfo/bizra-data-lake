{
  "prior_works": [
    {
      "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
      "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
      "year": 2015,
      "role": "Foundational framework for diffusion probabilistic models and ELBO training",
      "relationship_sentence": "This work introduced diffusion probabilistic models and their variational (ELBO) training view, providing the formal groundwork that the new paper revisits by learning the forward noising process as an approximate posterior."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
      "year": 2020,
      "role": "Core baseline and ELBO decomposition with fixed noise schedules",
      "relationship_sentence": "DDPM popularized the modern training objective and fixed forward noise schedules, whose assumed ELBO invariance the new paper directly challenges by making the forward process learned and input-conditional."
    },
    {
      "title": "Improved Denoising Diffusion Probabilistic Models",
      "authors": "Alex Nichol, Prafulla Dhariwal",
      "year": 2021,
      "role": "Demonstrated impact of noise schedules and reverse-variance parameterization",
      "relationship_sentence": "By showing that schedule choice and variance parameterization strongly affect likelihood and sample quality, this paper motivates learning the forward schedule itself; the new work extends this to multivariate, input-adaptive noise."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole",
      "year": 2021,
      "role": "Theoretical framework enabling flexible, continuous-time diffusion coefficients",
      "relationship_sentence": "The SDE formulation legitimizes modifying diffusion coefficients and connects diffusion to likelihood, paving the way for the new paper\u2019s learned, input-conditional (multivariate) forward diffusion as a principled design."
    },
    {
      "title": "Elucidating the Design Space of Diffusion-Based Generative Models",
      "authors": "Tero Karras, Miika Aittala, Samuli Laine, Timo Aila",
      "year": 2022,
      "role": "Systematic study of noise parameterization and preconditioning",
      "relationship_sentence": "EDM\u2019s analysis that design choices in noise parameterization and sigma distributions materially affect outcomes directly supports the new paper\u2019s premise to learn the diffusion process from data rather than fix it."
    },
    {
      "title": "Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design",
      "authors": "Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, Pieter Abbeel",
      "year": 2019,
      "role": "Conceptual precursor: learned noise as a variational posterior to tighten likelihood bounds",
      "relationship_sentence": "Flow++\u2019s learned dequantization treats injected noise as a variational posterior to tighten the ELBO, directly inspiring the new paper\u2019s view of the forward diffusion as a learnable approximate posterior for better likelihoods."
    },
    {
      "title": "Auxiliary Deep Generative Models",
      "authors": "Lars Maal\u00f8e, Casper Kaae S\u00f8nderby, S\u00f8ren Kaae S\u00f8nderby, Ole Winther",
      "year": 2016,
      "role": "Variational inference technique using auxiliary variables to enrich posteriors",
      "relationship_sentence": "The use of auxiliary variables to increase variational flexibility informs the new paper\u2019s auxiliary-variable component, which helps break ELBO invariance and enables learned, input-conditional forward noise."
    }
  ],
  "synthesis_narrative": "The core idea of Diffusion Models With Learned Adaptive Noise is to learn the forward diffusion process as an input-conditional, multivariate noise mechanism that functions as an approximate variational posterior, thereby tightening the ELBO and improving likelihood. This builds directly on the foundational variational view of diffusion probabilistic models introduced by Sohl-Dickstein et al., which cast diffusion as a Markov chain optimized via an ELBO. DDPM operationalized this framework with fixed forward schedules and a practical training objective, seeding a widely held belief that the ELBO is effectively invariant to the chosen forward noise process. Subsequent empirical work by Nichol and Dhariwal, and the broader design-space analysis of Karras et al. (EDM), demonstrated that schedule and parameterization choices materially influence both likelihood and synthesis, motivating a departure from fixed, isotropic noise.\n\nSong et al.\u2019s SDE formulation supplied a unifying continuous-time perspective, legitimizing flexible diffusion coefficients and clarifying links to likelihood, which the new paper leverages to justify input-conditional, spatially varying forward noise. Crucially, Flow++ provided the conceptual blueprint that learned noise can be treated as a variational posterior to tighten likelihood bounds; the present work transposes this idea from flow-based dequantization to diffusion\u2019s forward process. Finally, techniques from Auxiliary Deep Generative Models inform the introduction of auxiliary variables, enriching the variational family and breaking the ELBO invariance associated with fixed forward processes. Together, these strands culminate in MuLAN\u2019s learned multivariate schedules, adaptive per-input diffusion, and auxiliary variables, directly addressing ELBO tightness and performance.",
  "analysis_timestamp": "2026-01-07T00:02:04.742433"
}
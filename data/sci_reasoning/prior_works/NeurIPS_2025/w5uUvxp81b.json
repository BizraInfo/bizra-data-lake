{
  "prior_works": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "et al."
      ],
      "year": 2022,
      "role": "Foundational reasoning prompt method",
      "relationship_sentence": "Introduces explicit CoT prompting to improve multi-step reasoning; the present paper directly interrogates this technique and shows that applying CoT can degrade instruction-following accuracy on constraint-heavy tasks."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": [
        "Xuezhi Wang",
        "Jason Wei",
        "et al."
      ],
      "year": 2022,
      "role": "Reasoning enhancement baseline",
      "relationship_sentence": "Establishes a widely used enhancement to CoT; the paper evaluates reasoning-augmented setups and reveals that, despite such improvements on reasoning tasks, CoT-style deliberation can misdirect models away from following instructions."
    },
    {
      "title": "Training language models to follow instructions with human feedback (InstructGPT)",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "et al."
      ],
      "year": 2022,
      "role": "Instruction-following paradigm via RLHF",
      "relationship_sentence": "Defines the modern instruction-following objective and evaluation ethos; the current work measures how adding explicit reasoning affects compliance in instruction-tuned models."
    },
    {
      "title": "Finetuned Language Models are Zero-Shot Learners (FLAN)",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Vincent Y. Zhao",
        "et al."
      ],
      "year": 2022,
      "role": "Instruction tuning for broad generalization",
      "relationship_sentence": "Demonstrates that instruction tuning yields strong zero-shot instruction-following; the paper probes whether overlaying CoT on such models helps or harms adherence to explicit constraints."
    },
    {
      "title": "Lost in the Middle: How Language Models Use Long Context",
      "authors": [
        "Nelson F. Liu",
        "Kevin Lin",
        "et al."
      ],
      "year": 2023,
      "role": "Empirical analysis of attention over long inputs",
      "relationship_sentence": "Shows LLMs under-attend to some parts of long contexts; this insight informs the paper\u2019s hypothesis and analysis that lengthy CoT can divert attention away from instruction-relevant tokens, motivating the proposed 'constraint attention' metric."
    },
    {
      "title": "Attention is not Explanation",
      "authors": [
        "Sarthak Jain",
        "Byron C. Wallace"
      ],
      "year": 2019,
      "role": "Caution on interpreting attention",
      "relationship_sentence": "Questions the faithfulness of attention as explanation; the paper situates its attention-based 'constraint attention' metric within this discourse and uses it as a quantitative proxy to study focus during generation."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014showing that explicit chain-of-thought (CoT) reasoning can reduce instruction-following accuracy and introducing an attention-based metric to quantify this effect\u2014builds on two converging lines of work. First, seminal reasoning prompts (Wei et al., 2022) and their refinements (Wang et al., 2022) established CoT as a go-to method for hard reasoning tasks, shaping community practice to prepend or elicit rationales. Second, instruction-following research, notably InstructGPT (Ouyang et al., 2022) and FLAN (Wei et al., 2022), codified the goal of strict compliance with user directives and constraints, yielding models optimized for following instructions in natural language. These strands rarely interrogated each other: CoT focused on reasoning accuracy, while instruction tuning emphasized adherence and formatting discipline. By crossing them, this paper uncovers a trade-off\u2014CoT can distract models from simple but crucial constraints, harming compliance on benchmarks with rule- and composition-heavy instructions.\nTo diagnose mechanisms, the paper leverages insights from long-context behavior (Liu et al., 2023), hypothesizing that additional reasoning text may misallocate attention away from instruction-critical tokens. It formalizes this with a novel 'constraint attention' metric that tracks model focus during generation. Recognizing the debate on attention interpretability (Jain & Wallace, 2019), the authors present attention as a pragmatic proxy rather than a definitive explanation. Together, these prior works directly inform the paper\u2019s central question, experimental setup, and analytical tools, culminating in a nuanced understanding of when thinking helps\u2014and when it fails\u2014for instruction-following.",
  "analysis_timestamp": "2026-01-07T00:21:32.298996"
}
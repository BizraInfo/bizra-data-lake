{
  "prior_works": [
    {
      "title": "The Generative Exponent for Single-Index Models",
      "authors": "Alex Damian, Jason D. Lee, Joan Bruna",
      "year": 2024,
      "role": "Immediate precursor defining the core complexity measure",
      "relationship_sentence": "This paper introduced the generative exponent for Gaussian single-index models and used the low-degree framework to match algorithmic sample complexity, which the 2025 work generalizes to multi-index settings via the generative leap exponent and analogous lower/upper bounds."
    },
    {
      "title": "Sliced Inverse Regression for Dimension Reduction",
      "authors": "Ker-Chau Li",
      "year": 1991,
      "role": "Foundational method for estimating the central subspace",
      "relationship_sentence": "SIR established spectral estimation of the central subspace under Gaussian design via first-order inverse moments, providing the r-dimensional target and motivating the paper\u2019s Hermite-moment viewpoint when first-order information is present."
    },
    {
      "title": "SAVE: A Method for Dimension Reduction and Graphical Exploration in Regression",
      "authors": "R. Dennis Cook, Sanford Weisberg",
      "year": 1991,
      "role": "Classical higher-order counterpart to SIR",
      "relationship_sentence": "SAVE captures second-order inverse information when first-order signals vanish, directly mirroring the paper\u2019s sequential escalation across Hermite orders and motivating the need to adapt to the first nonzero Hermite rank k*."
    },
    {
      "title": "Notes on the Low-Degree Likelihood Ratio",
      "authors": "Dimitris S. Kalogerias Kunisky, Alexander S. Wein, Afonso S. Bandeira",
      "year": 2019,
      "role": "Formalization of the low-degree method for statistical lower bounds",
      "relationship_sentence": "This work codified the low-degree polynomial framework used to certify the paper\u2019s necessary sample complexity n = \u0398(d^{1 \u2228 k*/2}) for algorithms captured by low-degree tests."
    },
    {
      "title": "Score Function Features for Discriminative Learning: Matrix and Tensor Methods",
      "authors": "Majid Janzamin, Hamed Sedghi, Anima Anandkumar",
      "year": 2014,
      "role": "Moment/tensor methods for nonlinear regression under Gaussian designs",
      "relationship_sentence": "By linking Gaussian score/Hermite features to parameter recovery via spectral/tensor methods, this paper directly inspires constructing Hermite-tensor statistics and spectral estimators used in the 2025 algorithm."
    },
    {
      "title": "A Statistical Model for Tensor PCA",
      "authors": "Andrea Montanari, Eric Richard",
      "year": 2014,
      "role": "Spectral analysis and thresholds in spiked tensor estimation",
      "relationship_sentence": "The spectral recovery of low-dimensional structure from noisy higher-order tensors informs the paper\u2019s spectral U-statistic over Hermite tensors and the scaling laws tied to tensor order."
    },
    {
      "title": "Structure Adaptive Approach to Dimension Reduction in Regression",
      "authors": "Marian Hristache, Anatoli Juditsky, Vladimir Spokoiny",
      "year": 2001,
      "role": "Theory of multi-index models and central subspace estimation",
      "relationship_sentence": "This work formalized multi-index models and adaptive procedures for central subspace recovery, providing the statistical target and agnostic perspective that the 2025 paper resolves with tight Gaussian sample complexity."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014tight sample complexity for efficiently learning Gaussian multi-index models via the generative leap exponent\u2014builds directly on a blend of classical dimension reduction, modern low-degree lower bounds, and spectral/tensor moment methods. Damian, Lee, and Bruna (2024) introduced the generative exponent for single-index models and matched upper/lower bounds within the low-degree framework; the present work extends this conceptual and technical scaffold to r-dimensional multi-index settings, yielding the generative leap exponent and the n = \u0398(d^{1 \u2228 k*/2}) scaling. Classical sufficient-dimension-reduction methods, SIR (Li, 1991) and SAVE (Cook & Weisberg, 1991), supplied the subspace-identification paradigm and highlighted the role of moment order: when first-order signals vanish, one must ascend to second order\u2014precisely the Hermite-rank-driven escalation the new procedure systematizes. The low-degree framework, crystallized by Kunisky, Wein, and Bandeira (2019), provides the machinery to certify necessity within the class of polynomial-time algorithms modeled by low-degree tests. On the algorithmic side, Janzamin, Sedghi, and Anandkumar (2014) showed how Gaussian score/Hermite features and tensor moments enable parameter recovery in nonlinear regression, motivating the construction of Hermite tensors and spectral estimators. Finally, the spectral landscape and scaling intuition from tensor PCA (Montanari & Richard, 2014) inform both the spectral U-statistic design and the dependence on tensor order, while the multi-index modeling framework of Hristache, Juditsky, and Spokoiny (2001) anchors the central subspace target in an agnostic setting. Together, these works directly shape the paper\u2019s lower-bound methodology and its Hermite-tensor-based, sequential spectral estimator achieving tight sample complexity.",
  "analysis_timestamp": "2026-01-07T00:21:32.249044"
}
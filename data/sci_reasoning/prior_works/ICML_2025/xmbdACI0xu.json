{
  "prior_works": [
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "role": "Architectural foundation for multimodal pre-fusion and video understanding",
      "relationship_sentence": "AffectGPT\u2019s pre-fusion design for integrating temporally ordered visual (and likely audio) features with a frozen LLM mirrors Flamingo\u2019s gated cross-attention mechanism that interleaves vision features with language, adapted here to emphasize affective cues."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "role": "Bridging module for efficient vision-to-LLM coupling",
      "relationship_sentence": "AffectGPT\u2019s pre-fusion operations are conceptually aligned with BLIP-2\u2019s lightweight bridging (e.g., Q-Former style) between pretrained vision encoders and LLMs, enabling effective multimodal conditioning without end-to-end retraining."
    },
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Haotian Liu, Chunyuan Li, Qingyun Wang, Yong Jae Lee",
      "year": 2023,
      "role": "Model-based instruction data generation and visual instruction tuning",
      "relationship_sentence": "AffectGPT\u2019s model-based crowd-sourcing pipeline for producing descriptive emotion annotations directly builds on LLaVA\u2019s strategy of using strong LLMs to bootstrap high-quality instruction/annotation data for multimodal training."
    },
    {
      "title": "CLIP: Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Backbone vision-text alignment enabling MLLMs",
      "relationship_sentence": "By leveraging CLIP-style pretrained vision encoders for robust frame features and alignment with language, AffectGPT inherits the scalable multimodal representation that its pre-fusion module then specializes for affect understanding."
    },
    {
      "title": "MulT: Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": "Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov",
      "year": 2019,
      "role": "Pre-fusion cross-modal attention for audio-visual-text emotion/sentiment",
      "relationship_sentence": "AffectGPT\u2019s emphasis on early cross-modal interactions echoes MulT\u2019s pre-fusion cross-attention across audio, visual, and text streams, now transplanted into an LLM-centric architecture for richer emotion reasoning."
    },
    {
      "title": "CMU-MOSEI: A Multimodal Language Dataset for Human Emotional Analysis",
      "authors": "Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency",
      "year": 2018,
      "role": "Dataset precedent for large-scale multimodal emotion/sentiment",
      "relationship_sentence": "MER-Caption extends MOSEI\u2019s idea of large-scale multimodal affect annotations by moving from coarse categorical labels to fine-grained, descriptive emotion captions suitable for instruction-tuning MLLMs."
    },
    {
      "title": "Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition",
      "authors": "Dimitrios Kollias, Athanasios Tzirakis, Stefanos Zafeiriou",
      "year": 2021,
      "role": "Benchmarking standard for in-the-wild video-based affect",
      "relationship_sentence": "MER-UniBench\u2019s comprehensive, unified evaluation for multimodal emotion understanding draws on Aff-Wild2\u2019s in-the-wild, multi-protocol evaluation ethos, but adapts it to the instruction-following and generative capabilities of MLLMs."
    }
  ],
  "synthesis_narrative": "AffectGPT sits at the confluence of two lines of work: MLLM architectures and multimodal affect datasets/benchmarks. On the modeling side, Flamingo established that frozen LLMs augmented with gated cross-attention can fuse image/video features with language, while BLIP-2 showed how lightweight bridging modules (e.g., Q-Former) efficiently couple pretrained vision encoders to LLMs. AffectGPT\u2019s pre-fusion operations clearly inherit these ideas but redirect them toward affective signals, enabling richer cross-modal conditioning at the point where subtle emotion cues emerge. CLIP provides the practical backbone for robust vision-language alignment and scalable frame encoding that these pre-fusion layers build upon. In parallel, MulT demonstrated that early cross-modal interactions are crucial for emotion and sentiment tasks across audio, visual, and text streams; AffectGPT internalizes this pre-fusion principle in an LLM-centric framework to support nuanced emotion understanding and generation rather than mere classification.\nOn the data and evaluation side, CMU-MOSEI popularized large-scale multimodal affect datasets but with relatively coarse labels, and Aff-Wild2 set the standard for in-the-wild video affect evaluation. AffectGPT\u2019s MER-Caption advances this lineage by using an LLaVA-style, model-based crowd-sourcing pipeline to produce fine-grained, descriptive emotion annotations at scale, tailored for instruction tuning. Finally, MER-UniBench translates lessons from prior affect benchmarks into a unified, multimodal, instruction-following evaluation suite, aligning assessment with the broader capabilities of modern MLLMs.",
  "analysis_timestamp": "2026-01-07T00:21:32.388424"
}
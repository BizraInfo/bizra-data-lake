{
  "prior_works": [
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh and Percy Liang",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This paper provides the core influence-function formalism (upweight-one-point, implicit differentiation, inverse-Hessian\u2013vector products) that the present work generalizes from supervised losses to diffusion-model objectives and generation-probability proxies."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "The DDPM training objective and sampling framework define the concrete losses and generative mechanism that this paper differentiates through to formulate influence for \"probability of generating an example,\" supplying the core problem setting for their attribution extension."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Yang Song et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "The score-based/SDE formulation and its connections to likelihood via probability-flow ODE inform the proxy quantities for generation probability that this paper targets with influence functions in diffusion models."
    },
    {
      "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature",
      "authors": "James Martens and Roger Grosse",
      "year": 2015,
      "role": "Extension",
      "relationship_sentence": "The paper\u2019s scalable data attribution hinges on replacing the intractable Hessian inverse in influence functions with a K-FAC approximation; this work directly supplies the Kronecker-factored curvature machinery the authors adapt to diffusion-model GGN matrices."
    },
    {
      "title": "Deep learning via Hessian-free optimization",
      "authors": "James Martens",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "This work popularized using the generalized Gauss\u2013Newton (GGN) matrix as a stable, PSD curvature surrogate for deep networks\u2014an idea the paper leverages by basing its influence Hessian approximation on GGN to make computations well-behaved and scalable."
    },
    {
      "title": "Estimating Training Data Influence by Tracing Gradient Descent",
      "authors": "Garima Pruthi et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "TracIn is a leading scalable data attribution method; the authors position prior attribution techniques like TracIn as specific design choices within their generalized influence framework and improve on them with principled curvature (GGN/K-FAC) for diffusion models."
    }
  ],
  "synthesis_narrative": "The core innovation of this paper is to bring principled, scalable influence-function\u2013based data attribution to diffusion models by formulating appropriate generative-probability proxies and making the required second-order computations tractable. This directly builds on Koh and Liang\u2019s influence-function framework, reusing the upweight-one-point and implicit differentiation recipe but moving beyond supervised loss changes to changes in the probability of generating a particular example. The diffusion-model foundations of Ho et al.\u2019s DDPM and Song et al.\u2019s score-based SDE view provide the exact training objectives and generative formalisms whose quantities the new influence definitions differentiate through, as well as motivating proxy measures tied to likelihood or probability-flow formulations. A central technical barrier\u2014computing inverse-Hessian\u2013vector products at diffusion scale\u2014is overcome by adopting curvature approximations from second-order optimization: Martens\u2019 Hessian-free perspective establishes the generalized Gauss\u2013Newton matrix as a stable curvature surrogate, and Martens & Grosse\u2019s K-FAC supplies the Kronecker-factored structure that makes layerwise curvature inversion feasible. Finally, TracIn serves as the primary scalable baseline for data attribution; by showing how trajectory-based attributions fit as design choices within an influence framework, the paper both clarifies prior heuristics and supersedes them with a curvature-grounded, diffusion-specific formulation. Together, these works directly enable the paper\u2019s formulation and scalable computation of influence for data attribution in diffusion models.",
  "analysis_timestamp": "2026-01-06T23:09:26.613894"
}
{
  "prior_works": [
    {
      "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
      "authors": "Shrimai Dathathri et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "AcT adopts the core idea of controllable generation by directly intervening on hidden activations during inference, but replaces PPLM\u2019s gradient-based, compute-heavy updates with an optimal-transport formulation that yields lightweight, targeted shifts."
    },
    {
      "title": "Activation Addition: Steering Language Models Without Optimization",
      "authors": "Turner et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "AcT generalizes activation addition by interpreting simple direction-based shifts as a special case of an optimal-transport coupling that translates activations from a source to a target distribution."
    },
    {
      "title": "Representation Engineering: A Top-Down Approach to Steering Language Models",
      "authors": "Rimsky et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "AcT directly builds on representation engineering\u2019s practice of constructing contrastive activation interventions, replacing a single global direction with a data-adaptive OT transport that provides finer, layer- and token-wise control."
    },
    {
      "title": "Prompt-to-Prompt Image Editing with Cross Attention Control",
      "authors": "Amir Hertz et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "AcT extends the idea of diffusion-time control via internal feature manipulation by providing a unified activation-transport mechanism (rather than cross-attention map replacement) that applies broadly across U-Net layers and tasks."
    },
    {
      "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport",
      "authors": "Marco Cuturi",
      "year": 2013,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "AcT relies on entropically regularized optimal transport and Sinkhorn iterations to compute efficient activation couplings that minimally move representations while achieving the desired control signal."
    },
    {
      "title": "Null It Out: Guarding Protected Attributes in Neural Representations by Iterative Nullspace Projection (INLP)",
      "authors": "Shauli Ravfogel et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By showing that linear projection-based concept removal can over-suppress and hurt capabilities, INLP motivates AcT\u2019s minimal-transport criterion that steers away from undesired attributes while preserving model ability."
    }
  ],
  "synthesis_narrative": "Controllable generation by intervening on hidden states was first operationalized in Plug and Play Language Models, which demonstrated that attribute-guided inference-time activation updates can steer outputs but at significant computational cost and with sensitivity to optimization dynamics. Representation-level edits evolved with Representation Engineering, which distilled contrastive behaviors into activation directions applied at specific layers, offering practical, optimization-free control but largely relying on a single global shift. Activation Addition further simplified this paradigm by adding fixed direction vectors computed from contrasting prompts or datasets, making steering extremely lightweight but limited in granularity and adaptability. In diffusion, Prompt-to-Prompt revealed that modifying internal conditioning signals\u2014specifically cross-attention maps\u2014enables precise semantic edits without retraining, highlighting the power of activation-space interventions beyond language. Parallelly, INLP showed that linear subspace removal can suppress protected attributes but often over-prunes representational capacity, underlining the need for more conservative, behavior-preserving edits. Entropic optimal transport (Sinkhorn) introduced an efficient way to compute minimal-cost couplings between distributions, offering a principled tool for targeted, small shifts.\nSynthesizing these threads, a clear opportunity emerged: retain the efficiency of direction-based activation steering while moving beyond a single global vector to a minimal, data-adaptive transformation that preserves model ability across modalities. Activation Transport realizes this by casting steering as an optimal-transport problem over activations, generalizing direction addition and attention editing into a unified, fine-grained coupling with negligible overhead, directly addressing the limitations of optimization-heavy methods and over-aggressive linear projections.",
  "target_paper": {
    "title": "Controlling Language and Diffusion Models by Transporting Activations",
    "authors": "Pau Rodriguez, Arno Blaas, Michal Klein, Luca Zappella, Nicholas Apostoloff, marco cuturi, Xavier Suau",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "controllability, generative models, toxicity, diffusion",
    "abstract": "The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output.\nIn this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary ",
    "openreview_id": "l2zFn6TIQi",
    "forum_id": "l2zFn6TIQi"
  },
  "analysis_timestamp": "2026-01-06T15:26:08.365508"
}
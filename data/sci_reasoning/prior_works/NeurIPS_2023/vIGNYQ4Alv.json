{
  "prior_works": [
    {
      "title": "Introductory Lectures on Convex Optimization: A Basic Course",
      "authors": "Yurii Nesterov",
      "year": 2004,
      "role": "Foundational accelerated first-order method and benchmark rate",
      "relationship_sentence": "Provides the O(1/k^2) accelerated gradient rate and estimate-sequence perspective that the new method matches in low-iteration regimes and surpasses asymptotically, serving as the primary performance baseline."
    },
    {
      "title": "An Accelerated Hybrid Proximal Extragradient Method for Convex Optimization and its Implications to, Lower Complexity of Structured Optimization",
      "authors": "Renato D. C. Monteiro, Benar F. Svaiter",
      "year": 2013,
      "role": "Acceleration template via proximal extragradient",
      "relationship_sentence": "The proposed algorithm is built on a recent variant of the Monteiro\u2013Svaiter acceleration framework, instantiating it with a variable-metric (quasi-Newton) prox and extending the residual/approximate proximal conditions to obtain accelerated rates."
    },
    {
      "title": "A Universal Catalyst for First-Order Optimization",
      "authors": "Huan Li Lin, Julien Mairal, Zaid Harchaoui",
      "year": 2015,
      "role": "Generic acceleration via inexact proximal-point wrapper",
      "relationship_sentence": "Catalyst\u2019s inexact proximal-point viewpoint and complexity transfer principles inform how the paper wraps quasi-Newton preconditioning within an accelerated outer scheme and analyzes inexactness while preserving acceleration."
    },
    {
      "title": "Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators (Mirror-Prox)",
      "authors": "Arkadi Nemirovski",
      "year": 2004,
      "role": "Extragradient foundation and variable-metric prox ideas",
      "relationship_sentence": "The algorithm uses a proximal extragradient correction akin to Mirror-Prox, and the analysis leverages the stability benefits of extragradient-style two-step updates under changing (quasi-Newton) metrics."
    },
    {
      "title": "Updating Quasi-Newton Matrices with Limited Storage",
      "authors": "Jorge Nocedal",
      "year": 1980,
      "role": "Limited-memory quasi-Newton mechanism",
      "relationship_sentence": "Limited-memory BFGS provides the gradient-only curvature accumulation the paper uses to build the variable metric for its proximal/extragradient steps, and its spectral properties underpin the dimension-dependent factors in the rate."
    },
    {
      "title": "Quasi-Newton Methods (BFGS family)",
      "authors": "C. G. Broyden; R. Fletcher; D. Goldfarb; D. F. Shanno",
      "year": 1970,
      "role": "Gradient-only Hessian approximation backbone",
      "relationship_sentence": "The secant condition and positive-definite inverse-Hessian updates from BFGS are the core mechanism by which the method injects curvature using only gradients, enabling preconditioned proximal steps that yield faster convergence."
    },
    {
      "title": "Practical Inexact Proximal Quasi-Newton Methods for Regularized Optimization",
      "authors": "Katya Scheinberg, Xiaocheng Tang",
      "year": 2016,
      "role": "Variable-metric proximal framework with quasi-Newton updates",
      "relationship_sentence": "Demonstrates how quasi-Newton metrics can define proximal subproblems with only gradient information and inexact solves; the new paper adapts this variable-metric prox idea within an accelerated extragradient wrapper with provable rates."
    }
  ],
  "synthesis_narrative": "Jiang and Mokhtari\u2019s core contribution\u2014an accelerated quasi-Newton proximal extragradient method that matches NAG\u2019s O(1/k^2) and surpasses it to O(\u221a(d log k)/k^{2.5}) in high-iteration regimes\u2014rests on combining acceleration via proximal extragradient with gradient-only curvature learning. Nesterov\u2019s accelerated gradient (2004) supplies both the optimal first-order benchmark and the potential/estimate-sequence mindset the authors must match or exceed. The acceleration scaffold comes from the Monteiro\u2013Svaiter hybrid proximal extragradient framework (2013), whose residual-based, inexact proximal conditions the paper leverages in a recent variant to retain acceleration while altering the metric. Catalyst (2015) contributes the broader inexact proximal-point wrapper viewpoint and complexity transfer, guiding how a quasi-Newton preconditioned inner step can be embedded without losing accelerated rates.\nOn the curvature side, the quasi-Newton lineage is essential: BFGS (Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno, 1970) provides the gradient-only, positive-definite inverse-Hessian updates, and limited-memory BFGS (Nocedal, 1980) furnishes a scalable mechanism with spectral bounds that naturally introduce the dimension dependence appearing in the refined rate. Scheinberg and Tang (2016) demonstrate how quasi-Newton metrics define variable-metric proximal subproblems with only gradients and tolerable inexactness\u2014an interface the present work adapts to an accelerated extragradient context. Finally, Nemirovski\u2019s Mirror-Prox (2004) underpins the stability and error-robustness of the extragradient correction in variable metrics. Integrating these strands yields a gradient-only, variable-metric proximal extragradient whose analysis extends MS-style acceleration to quasi-Newton metrics, producing the first provable convex-case gain of a quasi-Newton-type method over NAG.",
  "analysis_timestamp": "2026-01-07T00:02:04.822946"
}
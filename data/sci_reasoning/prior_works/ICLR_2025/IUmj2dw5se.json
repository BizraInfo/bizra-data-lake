{
  "prior_works": [
    {
      "title": "Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP",
      "authors": "Su Lin Blodgett et al.",
      "year": 2020,
      "arxiv_id": "2005.14050",
      "role": "Gap Identification",
      "relationship_sentence": "Their critique that bias work uses inconsistent definitions, tasks, and metrics directly motivates CEB\u2019s move to formalize a compositional taxonomy and unified evaluation criteria across datasets."
    },
    {
      "title": "Holistic Evaluation of Language Models",
      "authors": "Percy Liang et al.",
      "year": 2022,
      "arxiv_id": "2211.09110",
      "role": "Inspiration",
      "relationship_sentence": "HELM\u2019s multi-dimensional, standardized evaluation and reporting framework inspires CEB\u2019s fairness-specific standardization across tasks, groups, and metrics."
    },
    {
      "title": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
      "authors": "Alicia Parrish et al.",
      "year": 2022,
      "arxiv_id": "2110.08193",
      "role": "Foundation",
      "relationship_sentence": "BBQ\u2019s ambiguous vs. disambiguated QA templates and bias-scoring scheme provide a core task and metric that CEB incorporates and normalizes within its broader compositional taxonomy."
    },
    {
      "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
      "authors": "Nikita Nangia et al.",
      "year": 2020,
      "arxiv_id": "2010.00133",
      "role": "Foundation",
      "relationship_sentence": "CrowS-Pairs\u2019 minimal-pair formulation for stereotypical preference directly informs one of the task types and bias-type dimensions that CEB aggregates and evaluates with consistent metrics."
    },
    {
      "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
      "authors": "Moin Nadeem et al.",
      "year": 2021,
      "arxiv_id": "2004.09456",
      "role": "Foundation",
      "relationship_sentence": "StereoSet\u2019s stereotype-preference metrics (e.g., SS/ICAT) exemplify the metric fragmentation that CEB resolves by mapping and harmonizing such scores under a unified evaluation framework."
    },
    {
      "title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation",
      "authors": "Sunipa Dhamala et al.",
      "year": 2021,
      "arxiv_id": "2101.11718",
      "role": "Foundation",
      "relationship_sentence": "BOLD establishes open-ended generation bias evaluation across topical and demographic categories, a genre that CEB integrates and aligns with its broader social-group taxonomy and standardized metrics."
    },
    {
      "title": "CheckList: A Behavioral Testing Framework for NLP",
      "authors": "Marco Tulio Ribeiro et al.",
      "year": 2020,
      "arxiv_id": "2005.04118",
      "role": "Related Problem",
      "relationship_sentence": "CheckList\u2019s capability-matrix and compositional behavioral testing concept informs CEB\u2019s design of a structured, compositional coverage across bias types, social groups, and tasks."
    }
  ],
  "synthesis_narrative": "Work on social bias in NLP revealed fundamental fragmentation: Blodgett et al. showed that definitions, tasks, and metrics are inconsistent, complicating comparison and progress. HELM demonstrated that holistic, multi-dimensional evaluation with standardized reporting can bring coherence across heterogeneous tasks and metrics. In bias-specific datasets, BBQ introduced a careful QA setup contrasting ambiguous and disambiguated contexts with a dedicated bias score, while CrowS-Pairs used minimal pairs to quantify stereotypical preferences in masked LMs. StereoSet advanced stereotype measurement with SS/ICAT, highlighting how different metrics target related but distinct notions of bias. For open-ended generation, BOLD broadened topical and demographic coverage and proposed metrics tailored to generative outputs, exemplifying another task family with its own scoring conventions. Complementing these, CheckList introduced a compositional behavioral testing mindset via capability matrices, encouraging systematic coverage rather than ad hoc probes. Taken together, these works established multiple task formulations (QA, minimal-pair discrimination, open-ended generation) and rich but incompatible metrics, while also hinting at the value of structured, comprehensive coverage. The natural next step was to synthesize these strands: adopt a holistic evaluation paradigm for fairness, define a compositional taxonomy spanning bias types, social groups, and tasks, and reconcile divergent scoring schemes into standardized metrics. By aggregating key datasets across these axes and aligning their measurements, a unified benchmark could enable fair, apples-to-apples comparisons of LLM bias and illuminate gaps in coverage.",
  "target_paper": {
    "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
    "authors": "Song Wang, Peng Wang, Tong Zhou, Yushun Dong, Zhen Tan, Jundong Li",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Fairness, Bias, Benchmark, Large Language Models",
    "abstract": "As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a variety of datasets. However, existing bias evaluation efforts often focus on only a particular type of bias and employ inconsistent evaluation metrics, leading to difficulties in comparison across different datasets and LLMs. To address these limitations, we collect a variety of datasets designed for the bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation Bechmark that covers different types of bias across different social groups and tasks. The curation of CEB is based on our newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. By combining the three dimensions, we develop a comprehensive ",
    "openreview_id": "IUmj2dw5se",
    "forum_id": "IUmj2dw5se"
  },
  "analysis_timestamp": "2026-01-06T09:08:35.184295"
}
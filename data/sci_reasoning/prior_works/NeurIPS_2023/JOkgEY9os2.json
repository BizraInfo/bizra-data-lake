{
  "prior_works": [
    {
      "title": "A Kernel Two-Sample Test",
      "authors": [
        "Arthur Gretton",
        "Karsten M. Borgwardt",
        "Malte J. Rasch",
        "Bernhard Sch\u00f6lkopf",
        "Alexander J. Smola"
      ],
      "year": 2012,
      "role": "Foundational MMD two-sample testing framework",
      "relationship_sentence": "MMD-Fuse builds directly on the unbiased MMD U-statistic, asymptotic theory, and kernel-based calibration principles introduced by Gretton et al., using these as the base statistic to be normalized and combined across kernels."
    },
    {
      "title": "Optimized Kernel Choice for Large-Scale Two-Sample Tests",
      "authors": [
        "Dougal J. Sutherland",
        "Heiko Strathmann",
        "Michael Arbel",
        "Arthur Gretton"
      ],
      "year": 2017,
      "role": "Kernel hyperparameter selection to maximize test power via data splitting",
      "relationship_sentence": "MMD-Fuse addresses the core limitation of Sutherland et al.\u2014the need for sample splitting\u2014by proposing a permutation-independent, data-dependent kernel selection and a soft-max fusion that retains calibration without holding out data."
    },
    {
      "title": "MMD Aggregated Two-Sample Test",
      "authors": [
        "Antonin Schrab",
        "Ilmun Kim",
        "Arthur Gretton"
      ],
      "year": 2022,
      "role": "Aggregation over kernel families without data splitting",
      "relationship_sentence": "MMD-Fuse advances the aggregation paradigm of MMD-AGG by replacing max/union-type aggregation with a weighted soft maximum of normalized MMDs, yielding higher power with exponential concentration bounds while remaining permutation-calibrated."
    },
    {
      "title": "Interpretable Distribution Features with Maximum Testing Power",
      "authors": [
        "Wittawat Jitkrittum",
        "Zolt\u00e1n Szab\u00f3",
        "Kacper P. Chwialkowski",
        "Arthur Gretton"
      ],
      "year": 2016,
      "role": "Data-dependent test statistic learning for two-sample testing (with sample splitting)",
      "relationship_sentence": "Echoing the idea of learning test parameters to maximize power, MMD-Fuse generalizes this adaptivity to kernel selection but crucially avoids data splitting by enforcing permutation-independence in the selection step."
    },
    {
      "title": "B-tests: Low Variance Kernel Two-Sample Tests",
      "authors": [
        "Wojciech Zaremba",
        "Arthur Gretton",
        "Matthew Blaschko"
      ],
      "year": 2013,
      "role": "Variance control and finite-sample behavior of MMD statistics",
      "relationship_sentence": "MMD-Fuse\u2019s normalization of per-kernel MMDs and its finite-sample concentration analysis build on insights about variance estimation and distributional behavior of MMD statistics developed in the B-test framework."
    },
    {
      "title": "Demystifying MMD GANs",
      "authors": [
        "Miko\u0142aj Bi\u0144kowski",
        "Dougal J. Sutherland",
        "Michael Arbel",
        "Arthur Gretton"
      ],
      "year": 2018,
      "role": "Demonstrated gains from deep feature (deep kernel) embeddings for MMD",
      "relationship_sentence": "MMD-Fuse leverages the lesson that learned deep features can drastically enhance MMD sensitivity, explicitly enabling deep kernels from unsupervised models (e.g., autoencoders) within a valid permutation-calibrated testing pipeline."
    },
    {
      "title": "Learning the Kernel Matrix with Semidefinite Programming",
      "authors": [
        "Gert R. G. Lanckriet",
        "N. Cristianini",
        "Peter Bartlett",
        "L\u00e9on Bottou",
        "Michael I. Jordan"
      ],
      "year": 2004,
      "role": "Multiple kernel learning (combining kernels) paradigm",
      "relationship_sentence": "MMD-Fuse adapts the multiple-kernel learning ethos to hypothesis testing by combining evidence across kernels\u2014here via a softmax over normalized MMDs\u2014rather than learning a single composite kernel, to directly maximize test power."
    }
  ],
  "synthesis_narrative": "MMD-Fuse\u2019s core idea\u2014maximizing two-sample test power by adaptively combining kernels without data splitting\u2014sits squarely on the MMD foundation of Gretton et al. (2012), which provides the U-statistic, asymptotics, and calibration mechanisms underpinning kernel tests. Prior attempts to enhance power focused on data-dependent tuning: Sutherland et al. (2017) optimized kernel hyperparameters using a held-out split, while Jitkrittum et al. (2016) learned test features/locations to maximize a power proxy, also requiring splitting to preserve validity. These works demonstrated that adaptivity helps power but at the cost of data efficiency and potential calibration challenges.\n\nAggregation-based testing then provided a route to adaptation without splitting. Schrab et al. (2022) showed that aggregating MMD tests across a kernel family can maintain type-I control via permutation calibration, yet such union-type aggregation can be conservative. MMD-Fuse advances this line by introducing a weighted soft maximum (log-sum-exp) of normalized MMDs, which smoothly pools evidence across kernels and yields exponential concentration under both null and alternative, improving sensitivity while preserving calibration through permutation independence.\n\nSupporting components come from variance/concentration analyses of MMD statistics (Zaremba et al., 2013), which motivate per-kernel normalization, and from deep-feature MMD practice (Bi\u0144kowski et al., 2018), which demonstrates the gains of neural representations\u2014now accommodated within MMD-Fuse\u2019s permutation-calibrated, no-split pipeline. Finally, the broader multiple-kernel learning literature (Lanckriet et al., 2004) motivates learning to combine kernels; MMD-Fuse tailors this principle to hypothesis testing by optimizing a statistic-level fusion rather than a single composite kernel.",
  "analysis_timestamp": "2026-01-07T00:02:04.782433"
}
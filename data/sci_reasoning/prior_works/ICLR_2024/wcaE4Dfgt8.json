{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Radford et al.",
      "year": 2021,
      "arxiv_id": "2103.00020",
      "role": "Foundation",
      "relationship_sentence": "Uni3D uses CLIP\u2019s image\u2013text embedding space as the supervisory target, aligning point\u2011cloud features to this space during pretraining."
    },
    {
      "title": "ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding",
      "authors": "Xue et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Uni3D directly extends ULIP\u2019s tri\u2011modal alignment idea by mapping point\u2011cloud embeddings into a frozen image\u2013text encoder\u2019s space, but scales the 3D backbone and data and streamlines the objective for end\u2011to\u2011end point\u2011based training."
    },
    {
      "title": "ULIP-2: Learning from Noisy Internet 3D Data with Language Supervision at Scale",
      "authors": "Xue et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "ULIP\u20112\u2019s scaled CLIP\u2011guided 3D pretraining serves as the main baseline Uni3D surpasses, with Uni3D addressing ULIP\u20112\u2019s limited model capacity and weaker 2D initialization by adopting billion\u2011parameter ViTs and stronger teachers."
    },
    {
      "title": "OpenShape: Scaling Open-Vocabulary 3D Shape Understanding in the Wild",
      "authors": "Liu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "OpenShape showed that CLIP\u2011guided multi\u2011view supervision can train 3D shape encoders for zero\u2011shot recognition, directly motivating Uni3D\u2019s choice to learn a native 3D encoder aligned to an image\u2013text space without rendering."
    },
    {
      "title": "ImageBind: One Embedding Space To Bind Them All",
      "authors": "Girdhar et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "ImageBind\u2019s paradigm of distilling CLIP into new modalities provided the key insight that a frozen image\u2013text model can supervise other encoders, which Uni3D adopts to bind 3D point clouds into the same space."
    },
    {
      "title": "EVA-CLIP: Improved Training Techniques for CLIP at Scale",
      "authors": "Fang et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "EVA\u2011CLIP supplies the high\u2011capacity image\u2013text teacher and ViT initializations that Uni3D leverages to scale 3D pretraining and strengthen zero\u2011shot transfer."
    },
    {
      "title": "Objaverse: A Universe of 3D Objects",
      "authors": "Deitke et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Objaverse provides the large, captioned 3D object corpus that enables large\u2011scale point\u2011cloud training with language/image supervision necessary for Uni3D\u2019s alignment objective."
    }
  ],
  "synthesis_narrative": "CLIP established an image\u2013text aligned semantic space where visual embeddings gain open\u2011world recognition ability from language supervision, making it a natural target for cross\u2011modal alignment. Building on this anchor, ULIP introduced a tri\u2011modal objective that aligns point\u2011cloud features to a frozen image\u2013text model, demonstrating that 3D encoders can inherit zero\u2011shot capabilities via such supervision. ULIP\u20112 scaled this recipe to noisy internet 3D data, reinforcing the feasibility of CLIP\u2011guided 3D pretraining while exposing capacity and initialization limits. In parallel, OpenShape showed that multi\u2011view image supervision can map 3D shapes into CLIP space at scale, highlighting the practical advantages of CLIP\u2011anchored training even when 3D is not natively encoded. ImageBind generalized the idea of binding new modalities to CLIP via distillation, crystallizing a teacher\u2013student paradigm for cross\u2011modal unification. EVA\u2011CLIP delivered stronger, scalable image\u2013text teachers and ViT initializations to make such distillation more effective. Finally, Objaverse furnished the large, captioned 3D corpus needed to train at scale under language/image supervision. Together, these works reveal a clear opportunity: directly learn a native 3D encoder that is initialized from strong 2D ViTs and distilled into a powerful image\u2013text space, avoiding rendering bottlenecks while unlocking open\u2011world transfer. Uni3D synthesizes these insights by aligning point\u2011cloud features to high\u2011capacity CLIP\u2011family teachers with simple, end\u2011to\u2011end objectives, and by scaling the 3D backbone and data to the billion\u2011parameter regime enabled by Objaverse and EVA\u2011style initializations.",
  "target_paper": {
    "title": "Uni3D: Exploring Unified 3D Representation at Scale",
    "authors": "Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, Xinlong Wang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "3D foundation model, universal 3D representation at scale, open-world 3D understanding",
    "abstract": "Scaling up representations for images or text has been extensively investigated in the past few years and has led to revolutions in learning vision and language. However, scalable representation for 3D objects and scenes is relatively unexplored. In this work, we present Uni3D, a 3D foundation model to explore the unified 3D representation at scale. Uni3D uses a 2D initialized ViT end-to-end pretrained to align the 3D point cloud features with the image-text aligned features. Via the simple architecture and pretext task, Uni3D can leverage abundant 2D pretrained models as initialization and image-text aligned models as the target, unlocking the great potential of 2D model zoos and scaling-up strategies to the 3D world. We efficiently scale up Uni3D to one billion parameters, and set new records on a broad range of 3D tasks, such as zero-shot classification, few-shot classification, open-world understanding and zero-shot part segmentation.  We show that the strong Uni3D representation a",
    "openreview_id": "wcaE4Dfgt8",
    "forum_id": "wcaE4Dfgt8"
  },
  "analysis_timestamp": "2026-01-06T22:36:14.422780"
}
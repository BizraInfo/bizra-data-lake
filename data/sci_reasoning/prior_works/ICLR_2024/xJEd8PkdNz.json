{
  "prior_works": [
    {
      "title": "Online actor\u2013critic algorithm to solve the continuous-time infinite-horizon optimal control problem",
      "authors": "K. G. Vamvoudakis et al.",
      "year": 2010,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work established the integral reinforcement learning policy-iteration framework for continuous-time HJB problems, where policy evaluation requires numerical integration of the utility along trajectories\u2014the exact computational stage whose accuracy the current paper analyzes."
    },
    {
      "title": "Optimal control of unknown continuous-time systems using off-policy reinforcement learning",
      "authors": "S. Modares et al.",
      "year": 2014,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "By deploying off-policy IntRL with trajectory sampling and quadrature-based policy evaluation but without quantifying integration error, this paper highlighted the unaddressed impact of quadrature accuracy on policy iteration that the current work explicitly analyzes and resolves."
    },
    {
      "title": "On an iterative technique for Riccati equation computations",
      "authors": "D. L. Kleinman",
      "year": 1968,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Kleinman\u2019s result that policy iteration for LQR is equivalent to a Newton step on the Riccati/HJB equation directly motivates framing IntRL policy iteration as Newton\u2019s method, enabling the current paper\u2019s analysis of how computation errors enter the Newton iteration."
    },
    {
      "title": "Inexact Newton methods",
      "authors": "R. S. Dembo et al.",
      "year": 1982,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The inexact Newton framework shows how additive evaluation errors affect Newton\u2019s convergence, which the current paper leverages by treating policy-evaluation quadrature error in IntRL as the inexactness term and deriving proportional error bounds."
    },
    {
      "title": "Bayes\u2013Hermite quadrature",
      "authors": "A. O\u2019Hagan",
      "year": 1991,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "This seminal work introduced Bayesian quadrature with principled posterior uncertainty on integrals, supplying the error-certification mechanism that the current paper uses to select and justify quadrature rules for IntRL policy evaluation."
    },
    {
      "title": "Probabilistic Integration: A Role in Statistical Computation",
      "authors": "F.-X. Briol et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "By formalizing error bounds and convergence rates for Bayesian quadrature via Gaussian processes, this paper provides the quantitative integration-error controls that the current work plugs into its Newton-iteration analysis of IntRL."
    }
  ],
  "synthesis_narrative": "Integral reinforcement learning for continuous-time control was concretely established by Vamvoudakis et al., who formulated policy iteration around the integral form of the HJB equation and thus required numerical integration during policy evaluation. Modares et al. extended IntRL to off-policy learning with sampled trajectories, operationalizing quadrature-based policy evaluation but without assessing how the chosen quadrature rule\u2019s error affects convergence or control. In parallel, Kleinman showed that policy iteration for LQR corresponds to a Newton step on the Riccati/HJB equation, providing a Newtonian lens on policy iteration. The numerical analysis literature on inexact Newton methods by Dembo et al. then characterized how evaluation inaccuracies enter Newton iterations as additive error terms with explicit convergence implications. From the probabilistic numerics side, O\u2019Hagan introduced Bayesian quadrature to estimate integrals with calibrated uncertainty, while Briol et al. supplied rigorous error bounds and convergence rates for GP-based quadrature.\nTogether these works reveal a critical junction: IntRL\u2019s policy evaluation hinges on numerical integration whose errors can, via the policy-iteration\u2013as\u2013Newton perspective, directly perturb convergence and final control performance. The synthesis is to model policy evaluation as an inexact Newton step and to control the inexactness by selecting quadrature rules with certified errors\u2014naturally provided by Bayesian quadrature\u2014thereby quantifying and mitigating computation-induced performance degradation in continuous-time IntRL.",
  "target_paper": {
    "title": "Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control",
    "authors": "Wenhan Cao, Wei Pan",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Integral Reinforcement Learning, Bayesian Quadrature, Newton's Method",
    "abstract": "Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method, with its upper bound proportional to the computat",
    "openreview_id": "xJEd8PkdNz",
    "forum_id": "xJEd8PkdNz"
  },
  "analysis_timestamp": "2026-01-06T11:31:43.631762"
}
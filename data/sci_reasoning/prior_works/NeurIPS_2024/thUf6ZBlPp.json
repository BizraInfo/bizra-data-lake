{
  "prior_works": [
    {
      "title": "Estimation of Non-Normalized Statistical Models by Score Matching",
      "authors": "Aapo Hyv\u00e4rinen",
      "year": 2005,
      "role": "Objective foundation (Fisher divergence / score matching)",
      "relationship_sentence": "EigenVI\u2019s core objective\u2014minimizing the Fisher divergence by matching the score of the target and the variational family\u2014directly builds on Hyv\u00e4rinen\u2019s score matching framework, leveraging its independence from normalization constants."
    },
    {
      "title": "Black Box Variational Inference",
      "authors": "Rajesh Ranganath, Sean Gerrish, David M. Blei",
      "year": 2014,
      "role": "Stochastic optimization framework for VI",
      "relationship_sentence": "EigenVI is a black-box VI method that uses stochastic estimators of its objective; the Monte Carlo gradient machinery and general BBVI paradigm stem from Ranganath et al.\u2019s framework."
    },
    {
      "title": "Operator Variational Inference",
      "authors": "Rajesh Ranganath, Dustin Tran, David M. Blei",
      "year": 2016,
      "role": "Operator-based VI objectives using score/Stein operators",
      "relationship_sentence": "By framing variational objectives via operators applied to test functions, OPVI legitimizes alternatives to KL\u2014such as score/Fisher-based criteria\u2014that EigenVI adopts and instantiates with an orthogonal expansion family."
    },
    {
      "title": "The Wiener\u2013Askey Polynomial Chaos for Stochastic Differential Equations",
      "authors": "Dongbin Xiu, George E. Karniadakis",
      "year": 2002,
      "role": "Orthogonal function families matched to variable supports",
      "relationship_sentence": "EigenVI\u2019s use of Hermite/Laguerre/Jacobi (and related) orthogonal bases to handle real, nonnegative, and bounded variables follows the Wiener\u2013Askey polynomial chaos principle of choosing support-adapted orthogonal expansions."
    },
    {
      "title": "The Variational Gaussian Approximation Revisited",
      "authors": "Manfred Opper, C\u00e9dric Archambeau",
      "year": 2009,
      "role": "Baseline variational family (Gaussian VI)",
      "relationship_sentence": "EigenVI\u2019s lowest-order term recovers a Gaussian variational approximation, explicitly generalizing the variational Gaussian approach of Opper and Archambeau with systematic higher-order non-Gaussian corrections."
    },
    {
      "title": "Generative Modeling by Estimating Gradients of the Data Distribution",
      "authors": "Yang Song, Stefano Ermon",
      "year": 2019,
      "role": "Score-based modeling of complex distributions",
      "relationship_sentence": "EigenVI\u2019s strategy of fitting distributions by matching score fields echoes score-based generative modeling, adapting the same core idea to variational inference with structured (orthogonal) function parameterizations."
    },
    {
      "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm",
      "authors": "Qiang Liu, Dilin Wang",
      "year": 2016,
      "role": "Operator-based inference using target scores",
      "relationship_sentence": "SVGD demonstrated the power of operator objectives that depend only on \u2207 log p(x); EigenVI similarly exploits access to target scores, but optimizes a Fisher divergence in an orthogonal basis leading to linear-algebraic solves."
    }
  ],
  "synthesis_narrative": "EigenVI\u2019s key contribution\u2014score-based variational inference with orthogonal function expansions\u2014sits at the intersection of three lines of work: operator/score-based objectives for inference, black-box stochastic optimization, and orthogonal spectral representations. Hyv\u00e4rinen\u2019s score matching established Fisher divergence as a principled objective that depends only on the score, not the intractable normalizer, which EigenVI adopts to align the variational and target score fields. Black Box Variational Inference provided the stochastic optimization machinery to estimate such objectives and their gradients from samples, enabling EigenVI to remain black-box despite using higher-order function families. Operator Variational Inference further legitimized replacing KL with operator-based criteria (including score/Fisher- and Stein-type objectives), conceptually grounding EigenVI\u2019s choice of Fisher divergence.\n\nOn the representation side, the Wiener\u2013Askey polynomial chaos literature supplies families of orthogonal functions tailored to the support (Hermite for R^D, Laguerre for R_+^D, Jacobi for bounded domains), directly informing EigenVI\u2019s design so that sampling and moments remain tractable. The lowest-order truncation reproduces the classical variational Gaussian approximation, while higher-order terms systematically capture skewness, kurtosis, and multimodality. Finally, recent score-based generative modeling showed that matching scores can model complex distributions; EigenVI brings this philosophy to VI with a compact, orthogonal basis rather than neural networks. In contrast to Stein variational methods that transport particles, EigenVI\u2019s Fisher objective and orthogonal expansion reduce optimization to structured linear-algebraic problems, yielding an expressive yet analytically manageable family for black-box inference across diverse variable domains.",
  "analysis_timestamp": "2026-01-06T23:33:36.273492"
}
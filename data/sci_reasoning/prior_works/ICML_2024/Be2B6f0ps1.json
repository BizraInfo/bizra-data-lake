{
  "prior_works": [
    {
      "title": "Artificial Intelligence Act (AI Act)",
      "authors": "European Commission",
      "year": 2021,
      "role": "Policy anchor (EU regulation)",
      "relationship_sentence": "The EU\u2019s risk-based AI Act specifies concrete obligations\u2014e.g., data governance, robustness testing, transparency, and post\u2011market monitoring\u2014whose technical feasibility the paper interrogates to expose capability gaps and motivate greater technical research and talent inside governance."
    },
    {
      "title": "Artificial Intelligence Risk Management Framework (AI RMF 1.0)",
      "authors": "National Institute of Standards and Technology (NIST)",
      "year": 2023,
      "role": "Framework (US risk management)",
      "relationship_sentence": "NIST\u2019s RMF operationalizes AI risk management and measurement, giving the authors a structured lens to identify where evaluation science, metrics, and tooling remain immature relative to policy expectations."
    },
    {
      "title": "Blueprint for an AI Bill of Rights",
      "authors": "White House Office of Science and Technology Policy (OSTP)",
      "year": 2022,
      "role": "Policy principles (US rights-based guidance)",
      "relationship_sentence": "By articulating requirements like safe and effective systems, algorithmic discrimination protections, and explainability, the Blueprint highlights sociotechnical demands that current ML methods only partially satisfy\u2014supporting the paper\u2019s call for deeper technical involvement in governance."
    },
    {
      "title": "Interim Measures for the Management of Generative AI Services",
      "authors": "Cyberspace Administration of China (CAC)",
      "year": 2023,
      "role": "Policy anchor (China regulation)",
      "relationship_sentence": "The Measures impose prescriptive duties (e.g., security assessments, watermarking, content moderation) on generative models, directly informing the paper\u2019s analysis of missing scalable technical solutions and specialized talent needed to implement such rules."
    },
    {
      "title": "Ethics Guidelines for Trustworthy AI (and ALTAI Assessment List)",
      "authors": "High-Level Expert Group on AI (European Commission)",
      "year": 2019,
      "role": "Guidelines and assessment tooling (EU)",
      "relationship_sentence": "These guidelines and their assessment list translate principles into operational checks, illustrating both the promise and current limits of principle-to-practice tooling that the paper argues requires tighter integration with technical research."
    },
    {
      "title": "Model Cards for Model Reporting",
      "authors": "Margaret Mitchell; Simone Wu; Andrew Zaldivar; Parker Barnes; Lucy Vasserman; Ben Hutchinson; Elena Spitzer; Inioluwa Deborah Raji; Timnit Gebru",
      "year": 2019,
      "role": "Technical governance artifact (documentation)",
      "relationship_sentence": "Model Cards exemplify how technical artifacts can make policy goals like transparency actionable, underpinning the paper\u2019s thesis that more such research and adoption are needed to meet emerging regulatory requirements."
    },
    {
      "title": "Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability",
      "authors": "Dillon Reisman; Jason Schultz; Kate Crawford; Meredith Whittaker",
      "year": 2018,
      "role": "Conceptual/practical framework (public-sector accountability)",
      "relationship_sentence": "AIA proposes a cross-disciplinary audit mechanism for agencies, which the paper extends by mapping the specific technical capabilities and talent governments now need to conduct AIA-like processes for modern AI systems."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014diagnosing the gap between AI governance aspirations and available technical tooling, and calling for deeper integration of technical research and talent into governance\u2014builds directly on a set of policy frameworks that specify what governments want and early technical artifacts that show how to deliver it. The EU AI Act, OSTP\u2019s Blueprint for an AI Bill of Rights, and China\u2019s Interim Measures for Generative AI collectively articulate concrete obligations: risk management, safety testing, transparency, watermarking, content moderation, and post-market surveillance. NIST\u2019s AI RMF provides a structured risk lens that makes these obligations auditable in principle, but also exposes where measurement science, benchmarks, and evaluation methods are underdeveloped.\n\nOn the technical side, Model Cards demonstrate how documentation can operationalize transparency mandates, while the EU HLEG\u2019s Ethics Guidelines and ALTAI reveal both the potential and current limitations of principle-to-practice assessment tools. Algorithmic Impact Assessments offer a public-sector mechanism for accountability, highlighting the institutional processes that must be matched with robust technical methods and skilled practitioners. Together, these works create the preconditions for the paper\u2019s argument: policy has moved faster than the technical infrastructure and talent pipelines required for implementation. By juxtaposing regulatory requirements with the state of technical art, the paper crystallizes a research and capacity-building agenda\u2014develop rigorous evals, assurance methods, and documentation practices\u2014and urges embedding ML expertise within governments to close the implementation gap.",
  "analysis_timestamp": "2026-01-07T00:02:04.888437"
}
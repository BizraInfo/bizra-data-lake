{
  "prior_works": [
    {
      "title": "Extracting and Composing Robust Features with Denoising Autoencoders",
      "authors": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol",
      "year": 2008,
      "role": "Foundational introduction of the denoising autoencoder objective and tied-weight architecture.",
      "relationship_sentence": "This paper defines the denoising criterion and tied-weight autoencoder the present work analyzes asymptotically, supplying the core learning objective and model class."
    },
    {
      "title": "What Regularized Auto-Encoders Learn from the Data-Generating Distribution",
      "authors": "Guillaume Alain, Yoshua Bengio",
      "year": 2014,
      "role": "Theoretical characterization of denoising/regularized autoencoders (score estimation and residual denoising).",
      "relationship_sentence": "By linking DAEs to score estimation and residual mappings, this work motivates the benefit of identity/skip pathways that the present paper quantitatively evaluates in high dimension."
    },
    {
      "title": "Neural Networks and Principal Component Analysis: Learning from Examples Without Local Minima",
      "authors": "Pierre Baldi, Kurt Hornik",
      "year": 1989,
      "role": "Classical result connecting linear autoencoders to PCA.",
      "relationship_sentence": "The paper\u2019s PCA-related baseline (autoencoder without skip connection) rests on this equivalence, against which the new theory measures the skip connection\u2019s advantage."
    },
    {
      "title": "Asymptotics of Sample Eigenstructure for Spiked Covariance Models",
      "authors": "Debashis Paul",
      "year": 2007,
      "role": "High-dimensional PCA theory under spiked models.",
      "relationship_sentence": "Provides the asymptotic framework to characterize PCA behavior on mixture/spiked models, enabling rigorous comparison with the autoencoder without skip connection."
    },
    {
      "title": "The Committee Machine: Computational to Statistical Gaps in Learning a Two-Layer Neural Network",
      "authors": "Benjamin Aubin, Florent Krzakala, Bruno Loureiro, Lenka Zdeborov\u00e1",
      "year": 2020,
      "role": "Statistical-physics/replica and state-evolution methodology for bounded-width two-layer networks.",
      "relationship_sentence": "Supplies analytical tools and teacher\u2013student inference templates that the present work adapts to derive closed-form high-dimensional test error for a two-layer denoising autoencoder."
    },
    {
      "title": "Semi-Supervised Learning with Ladder Networks",
      "authors": "Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, Tapani Raiko",
      "year": 2015,
      "role": "Introduces lateral/skip connections in denoising autoencoders, demonstrating empirical advantages.",
      "relationship_sentence": "Motivates the skip-connection architecture whose quantitative denoising gains the present paper explains and predicts in the high-dimensional limit."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "year": 2016,
      "role": "Popularized skip connections as identity mappings for improved optimization and representation.",
      "relationship_sentence": "Provides the residual/skip design principle whose effect in autoencoding denoising the present work isolates and analyzes theoretically."
    }
  ],
  "synthesis_narrative": "Cui and Zdeborov\u00e1\u2019s core contribution is a closed-form high-dimensional characterization of denoising error for a two-layer autoencoder with tied weights and a skip connection under Gaussian mixture data, and a quantitative comparison to the no-skip autoencoder that aligns with PCA. The denoising autoencoder objective and tied-weight design come directly from Vincent et al., while Alain and Bengio\u2019s theory connects denoising AEs to residual score estimation, motivating identity pathways that resemble the skip analyzed here. The comparison point\u2014autoencoder without skip\u2014rests on the classical linear AE\u2013PCA equivalence due to Baldi and Hornik, positioning PCA as the natural baseline. High-dimensional PCA under spiked/mixture models (Paul) supplies the asymptotic language and expected learning curves needed to benchmark and interpret the PCA-like autoencoder\u2019s performance. Methodologically, the paper\u2019s bounded-width, two-layer high-dimensional analysis follows the statistical-physics toolkit developed for teacher\u2013student two-layer networks (e.g., the committee machine of Aubin, Krzakala, Loureiro, and Zdeborov\u00e1), which enables precise generalization/denoising error predictions in closed form. Architecturally, the skip connection is rooted in residual learning (He et al.) and its denoising instantiations (Ladder Networks), whose empirical benefits this work elevates to a principled, quantitative theory. Together, these works directly inform the model choice, baseline, asymptotic regime, and analytical machinery that culminate in the paper\u2019s main result: precise high-dimensional denoising risk and a clear, provable advantage for the skip-connected autoencoder over its PCA-like counterpart.",
  "analysis_timestamp": "2026-01-07T00:02:04.840250"
}
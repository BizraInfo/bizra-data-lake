{
  "prior_works": [
    {
      "title": "Attention Is All You Need",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin",
      "year": 2017,
      "role": "foundational architecture",
      "relationship_sentence": "This paper introduced multi-head key\u2013value attention, defining the hypothesis class (single-layer, multi-head attention) whose linear variant the NeurIPS 2025 work recasts and proves learnable in polynomial time."
    },
    {
      "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
      "authors": "Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran\u00e7ois Fleuret",
      "year": 2020,
      "role": "methodological (linear attention formulation)",
      "relationship_sentence": "By expressing attention as a kernel-style feature mapping with linear complexity, this work provides the algebraic form that the new paper leverages to reinterpret multi-head linear attention as a kernel predictor in an RKHS."
    },
    {
      "title": "Rethinking Attention with Performers",
      "authors": "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller",
      "year": 2021,
      "role": "methodological (kernelization of attention)",
      "relationship_sentence": "Performer\u2019s FAVOR+ solidifies the kernel perspective on attention via random features, directly motivating the paper\u2019s treatment of multi-head linear attention as sums of kernel predictors amenable to convex, polynomial-time learning."
    },
    {
      "title": "A Generalized Representer Theorem",
      "authors": "Bernhard Sch\u00f6lkopf, Ralf Herbrich, Alex J. Smola",
      "year": 2001,
      "role": "theoretical framework (RKHS ERM structure)",
      "relationship_sentence": "The representer theorem underpins the paper\u2019s reduction from learning linear attention to kernel ERM in an RKHS and enables their certification procedure that checks when all empirical risk minimizers realize the same computation."
    },
    {
      "title": "Random Features for Large-Scale Kernel Machines",
      "authors": "Ali Rahimi, Benjamin Recht",
      "year": 2007,
      "role": "methodological (efficient kernel computation)",
      "relationship_sentence": "Random Fourier features provide the computational lens for scalable kernel methods that influenced linearized attention and support the paper\u2019s argument that learning linear attention can be performed in polynomial time via kernel machinery."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "theoretical framework (networks-as-kernels view)",
      "relationship_sentence": "The NTK line of work motivates analyzing neural architectures through kernel predictors, informing the paper\u2019s strategy to cast linear attention learning and generalization within an RKHS framework."
    },
    {
      "title": "Understanding Machine Learning: From Theory to Algorithms",
      "authors": "Shai Shalev-Shwartz, Shai Ben-David",
      "year": 2014,
      "role": "theoretical framework (agnostic PAC and kernel ERM)",
      "relationship_sentence": "This text provides the agnostic PAC framework and polynomial-time learnability results for kernel-based hypothesis classes that the paper instantiates for multi-head linear attention."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core step is to reframe learning a single-layer, multi-head linear attention network as a kernel learning problem in a suitable RKHS and to leverage this reduction for polynomial-time, strong agnostic PAC learnability and a certification of solution uniqueness. Three strands of prior work crystallize this path. First, architectural foundations from Vaswani et al. defined the multi-head key\u2013value attention template whose linear variant is analyzed. The linearization of attention (Katharopoulos et al.) and its kernelized reinterpretations (Choromanski et al.) supplied the exact algebra\u2014attention as feature-map inner products\u2014making an RKHS formulation natural and technically precise, especially when summing across heads corresponds to additive kernels.\nSecond, kernel theory provides the learnability backbone. The generalized representer theorem (Sch\u00f6lkopf, Herbrich, Smola) guarantees ERM solutions lie in the span of training evaluations, enabling convex optimization and allowing the paper\u2019s polynomial-time learner and its test for whether all ERM solutions induce the same function. Classical kernel efficiency and approximation tools (Rahimi & Recht) support scalable computation and cement the link between linearized attention and kernel predictors. The broader networks-as-kernels perspective (Jacot et al., NTK) further legitimizes recasting neural training as kernel regression for generalization analysis.\nThird, agnostic PAC learning theory (Shalev-Shwartz & Ben-David) provides the sample-complexity and algorithmic framework ensuring that the RKHS reduction yields strong, agnostic PAC guarantees. Together, these works directly enable the new results: a principled RKHS reduction for multi-head linear attention, polynomial-time learning, and a practical procedure to certify when all best-fit models compute an identical function with out-of-distribution implications.",
  "analysis_timestamp": "2026-01-06T23:42:48.127708"
}
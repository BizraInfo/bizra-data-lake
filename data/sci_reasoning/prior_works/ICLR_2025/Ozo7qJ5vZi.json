{
  "prior_works": [
    {
      "title": "On the representation of continuous functions of several variables by superposition of continuous functions of one variable and addition",
      "authors": "Andrey N. Kolmogorov",
      "year": 1957,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "KAN\u2019s edge-wise learnable univariate functions are a direct architectural instantiation of Kolmogorov\u2019s superposition theorem, which guarantees that any multivariate continuous function can be expressed via sums and compositions of univariate functions."
    },
    {
      "title": "On functions of three variables",
      "authors": "Vladimir I. Arnold",
      "year": 1957,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Arnold\u2019s strengthening of Kolmogorov\u2019s result into a concrete two-stage superposition (sums of univariate functions of affine combinations of univariate functions) informs KAN\u2019s repeating motif of summation plus univariate transformation across layers."
    },
    {
      "title": "Kolmogorov's mapping neural network existence theorem",
      "authors": "Robert Hecht-Nielsen",
      "year": 1987,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By explicitly mapping the Kolmogorov\u2013Arnold representation to a three-layer neural network with univariate nonlinearities, this work provided the blueprint that KAN realizes by making those univariate components trainable spline functions on edges rather than fixed node activations."
    },
    {
      "title": "Generalized Additive Models",
      "authors": "Trevor Hastie et al.",
      "year": 1986,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "GAMs introduced interpretable feature-wise univariate \u201cshape functions\u201d combined additively, an idea KAN generalizes from shallow additive models to deep compositions of per-edge shape functions to preserve interpretability while increasing expressivity."
    },
    {
      "title": "Neural Additive Models: Interpretable Machine Learning with Neural Nets",
      "authors": "Agarwal et al.",
      "year": 2021,
      "arxiv_id": "2004.13912",
      "role": "Inspiration",
      "relationship_sentence": "NAMs operationalized learnable univariate shape functions with neural networks; KAN borrows this learnable shape-function paradigm and extends it to every edge in a deep architecture to implement the Kolmogorov\u2013Arnold superposition with enhanced interpretability."
    },
    {
      "title": "Neural Spline Flows",
      "authors": "Conor Durkan et al.",
      "year": 2019,
      "arxiv_id": "1906.04032",
      "role": "Extension",
      "relationship_sentence": "This work demonstrated stable, backprop-trainable spline parameterizations for expressive univariate transformations, which KAN leverages by parameterizing each edge function as a spline to obtain smooth, flexible, and interpretable 1D mappings."
    },
    {
      "title": "AI Feynman: A physics-inspired method for symbolic regression",
      "authors": "Silviu-Marian Udrescu et al.",
      "year": 2020,
      "arxiv_id": "1905.11481",
      "role": "Gap Identification",
      "relationship_sentence": "AI Feynman showed the value of interpretable functional structure for scientific discovery but relies on combinatorial symbolic search, motivating KAN\u2019s gradient-based, human-interpretable univariate components that can aid equation rediscovery without explicit symbolic enumeration."
    }
  ],
  "synthesis_narrative": "Kolmogorov demonstrated that any continuous multivariate function can be represented via sums and compositions of univariate functions, and Arnold\u2019s companion result sharpened this into a concrete two-stage superposition involving sums of univariate functions of affine combinations of univariate functions. Hecht-Nielsen then mapped these representation-theoretic insights onto a neural architecture, showing that a small number of layers with univariate nonlinearities suffice in principle to realize Kolmogorov\u2013Arnold superpositions. In parallel, generalized additive models introduced interpretable modeling through feature-wise univariate \u201cshape functions\u201d summed additively, and Neural Additive Models updated this idea by learning flexible shape functions with neural networks while preserving interpretability. On the parametrization side, Neural Spline Flows provided a practical recipe to represent and train expressive univariate functions using splines via backpropagation, yielding smooth and stable transformations. Finally, AI Feynman highlighted the scientific value of explicit functional structure and interpretability for rediscovering equations, while exposing the brittleness and search costs of symbolic approaches.\nTogether, these works suggested a path: instantiate the Kolmogorov\u2013Arnold superposition within a trainable neural architecture by replacing linear weights with learnable univariate shape functions, parameterized with splines for stability, and arranged across layers to move beyond shallow additivity toward deep compositions. This synthesis preserves GAM/NAM-style interpretability, leverages spline-based trainability, and answers AI Feynman\u2019s search burden with continuous, gradient-based learning\u2014yielding a natural next step: a network whose edges are univariate functions embodying the Kolmogorov\u2013Arnold idea.",
  "target_paper": {
    "title": "KAN: Kolmogorov\u2013Arnold Networks",
    "authors": "Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljacic, Thomas Y. Hou, Max Tegmark",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Kolmogorov-Arnold networks, Kolmogorov-Arnold representation theorem, learnable activation functions, interpretability, AI + Science",
    "abstract": "Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (\"neurons''), KANs have learnable activation functions on edges (\"weights''). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability, on small-scale AI + Science tasks. For accuracy, smaller KANs can achieve comparable or better accuracy than larger MLPs in function fitting tasks. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful ``collaborators'' helping scientists (re)discover mathematical a",
    "openreview_id": "Ozo7qJ5vZi",
    "forum_id": "Ozo7qJ5vZi"
  },
  "analysis_timestamp": "2026-01-06T17:56:29.133271"
}
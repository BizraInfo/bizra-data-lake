{
  "prior_works": [
    {
      "title": "Stochastic Gradient Langevin Dynamics",
      "authors": [
        "Max Welling",
        "Yee Whye Teh"
      ],
      "year": 2011,
      "role": "Foundational method (noisy gradient descent via Langevin dynamics)",
      "relationship_sentence": "Langevin unlearning builds directly on SGLD\u2019s principle of injecting Gaussian noise into gradient descent, using Langevin dynamics both during training and during deletion updates to align model distributions with retraining-from-scratch."
    },
    {
      "title": "Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo",
      "authors": [
        "Yu-Xiang Wang",
        "Stephen E. Fienberg",
        "Alexander J. Smola"
      ],
      "year": 2015,
      "role": "Theoretical bridge between Langevin sampling and differential privacy",
      "relationship_sentence": "This work showed that SGMCMC (including SGLD) inherently provides differential privacy under suitable conditions, a key insight leveraged by Langevin unlearning to obtain DP-style approximate unlearning guarantees via noisy gradient dynamics."
    },
    {
      "title": "Deep Learning with Differential Privacy",
      "authors": [
        "Mart\u00edn Abadi",
        "Andy Chu",
        "Ian Goodfellow",
        "H. Brendan McMahan",
        "Ilya Mironov",
        "Kunal Talwar",
        "Li Zhang"
      ],
      "year": 2016,
      "role": "Core DP training algorithm and accounting (DP-SGD)",
      "relationship_sentence": "Langevin unlearning unifies DP-style training and certified unlearning; it inherits the indistinguishability perspective and privacy accounting spirit from DP-SGD while operationalizing it through Langevin/noisy GD to certify deletion without full retraining."
    },
    {
      "title": "Towards Making Systems Forget with Machine Unlearning",
      "authors": [
        "Yinzhi Cao",
        "Junfeng Yang"
      ],
      "year": 2015,
      "role": "Problem formulation and early systems approach to unlearning",
      "relationship_sentence": "As an origin point for machine unlearning, this paper\u2019s formulation of removing the influence of specific data directly motivates the new framework\u2019s goal of statistically matching retraining outcomes after deletions."
    },
    {
      "title": "Making AI Forget You: Data Deletion in Machine Learning",
      "authors": [
        "Alexei Ginart",
        "Melody Y. Guan",
        "Gregory Valiant",
        "James Zou"
      ],
      "year": 2019,
      "role": "Algorithmic unlearning for specific models with efficiency guarantees",
      "relationship_sentence": "By demonstrating efficient (approximate/exact) unlearning for certain models, this work highlights the retraining-vs-unlearning tradeoff that Langevin unlearning addresses generically via noisy GD with certified guarantees, including for non-convex models."
    },
    {
      "title": "Descent-to-Delete: Gradient-Based Methods for Machine Unlearning",
      "authors": [
        "Aditya Golatkar",
        "Alessandro Achille",
        "Stefano Soatto"
      ],
      "year": 2020,
      "role": "Gradient- and noise-based unlearning precursor",
      "relationship_sentence": "This paper\u2019s idea of using (noisy) gradient steps to mimic retraining distributions directly informs Langevin unlearning\u2019s use of Langevin noise to provide distributional closeness and certificates relative to retraining."
    },
    {
      "title": "Machine Unlearning (SISA Training)",
      "authors": [
        "Ulysse Bourtoule",
        "Varun Chandrasekaran",
        "Christopher A. Choquette-Choo",
        "Nicolas Papernot",
        "et al."
      ],
      "year": 2021,
      "role": "Practical framework for efficient repeated deletions",
      "relationship_sentence": "SISA established scalable unlearning for multiple requests; Langevin unlearning advances this theme by enabling sequential and batch unlearning while additionally providing DP-style approximate unlearning guarantees and applicability to non-convex models."
    }
  ],
  "synthesis_narrative": "Langevin Unlearning sits at the intersection of machine unlearning, differential privacy, and stochastic gradient sampling. The method\u2019s core mechanism\u2014injecting Gaussian noise into gradient updates to make model states statistically indistinguishable from retraining\u2014directly descends from Stochastic Gradient Langevin Dynamics (Welling & Teh), which casts optimization as noisy Langevin dynamics. The DP connection is crucial: Wang\u2013Fienberg\u2013Smola formalized that SGMCMC can confer privacy \u201cfor free,\u201d and Abadi et al.\u2019s DP-SGD crystallized the indistinguishability and accounting toolkit. Langevin Unlearning explicitly leverages this bridge, using Langevin noise to unify DP-style learning and certified unlearning within a single algorithmic perspective.\n\nOn the unlearning side, Cao & Yang\u2019s early formulation established the deletion objective, while Ginart et al. highlighted the efficiency vs. exactness trade-offs in model-specific settings. Golatkar\u2013Achille\u2013Soatto demonstrated that noise-injected, gradient-based updates can emulate retraining distributions\u2014an idea Langevin Unlearning generalizes and formalizes with DP-like guarantees and applicability to non-convex objectives. Finally, SISA (Bourtoule et al.) showed how to scale repeated deletions in practice; Langevin Unlearning extends this practicality by supporting sequential and batch unlearning while certifying approximate removal.\n\nTogether, these works motivate and enable Langevin Unlearning\u2019s key contribution: a noisy gradient descent framework that unifies DP training with privacy-certified unlearning, yielding approximate certificates for non-convex models and practical efficiency compared to retraining, even under multiple deletion requests.",
  "analysis_timestamp": "2026-01-06T23:33:35.561631"
}
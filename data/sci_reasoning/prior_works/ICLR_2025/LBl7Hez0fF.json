{
  "prior_works": [
    {
      "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
      "authors": "S. Dathathri et al.",
      "year": 2020,
      "arxiv_id": "1912.02164",
      "role": "Inspiration",
      "relationship_sentence": "VTI adopts PPLM\u2019s core idea of inference-time activation steering without additional training, but repurposes it to stabilize cross-modal representations in LVLMs to suppress hallucinations."
    },
    {
      "title": "Decoding by Contrasting Layers Improves Factuality",
      "authors": "Y. Chuang et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "By showing that decoding-time manipulation using internal layer signals can improve factuality, DoLa motivates VTI\u2019s use of model-internal representations (rather than external supervision) to reduce hallucinations at test time."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "J. Li et al.",
      "year": 2023,
      "arxiv_id": "2301.12597",
      "role": "Foundation",
      "relationship_sentence": "BLIP-2\u2019s modular LVLM design with separately pre-trained image encoders and text decoders highlights the fragile cross-modal interface that VTI explicitly stabilizes via latent-space interventions."
    },
    {
      "title": "Visual Instruction Tuning",
      "authors": "H. Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Foundation",
      "relationship_sentence": "LLaVA popularized LVLMs built from frozen vision encoders and LLM decoders, a configuration prone to image\u2013text misalignment and sensitivity that VTI targets with inference-time steering."
    },
    {
      "title": "Object Hallucination in Image Captioning",
      "authors": "A. Rohrbach et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work introduced the CHAIR metric and showed that language priors drive visual hallucinations, directly motivating VTI\u2019s emphasis on strengthening image-grounded signals during generation."
    },
    {
      "title": "Visual Contrastive Decoding for Reducing Hallucinations in LVLMs",
      "authors": "X. Yue et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "VCD serves as a primary training-free baseline that combats hallucinations by contrasting image-conditioned and language-only outputs, whereas VTI instead steers hidden representations to reinforce visual evidence."
    }
  ],
  "synthesis_narrative": "Plug and Play Language Models demonstrated that one can steer generation by directly modifying hidden activations at inference, achieving controllable text without any fine-tuning. Decoding by Contrasting Layers then showed that internal layer signals can be exploited at decoding time to improve factuality, leveraging model-internal representations rather than external supervision. In multimodal modeling, BLIP-2 established the now-standard LVLM architecture that couples frozen vision encoders with large language model decoders through a learned bridge, foregrounding a fragile cross-modal interface. Visual Instruction Tuning (LLaVA) further popularized this modular recipe at scale, revealing both the practical utility and the misalignment-induced sensitivity of text decoders to visual inputs. Earlier, Object Hallucination in Image Captioning introduced the CHAIR metric and pinpointed language priors as a key driver of visual hallucinations, crystallizing the importance of amplifying image-grounded signals. Most recently, Visual Contrastive Decoding provided a strong, training-free baseline that mitigates hallucinations by subtracting language-only priors at the logit level to emphasize visual evidence. Together, these works revealed a consistent opportunity: training-free, decoding-time interventions can improve factuality, and LVLM hallucinations stem from a brittle cross-modal interface where language priors dominate visual signals. The natural next step is to move from output-level contrasts to representation-level control within the multimodal hidden states. By steering the latent space at test time, one can stabilize vision features precisely where the encoder\u2013decoder interface is fragile, retaining the simplicity of no-training interventions while directly addressing the root cause of LVLM hallucinations.",
  "target_paper": {
    "title": "Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering",
    "authors": "Sheng Liu, Haotian Ye, James Zou",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Large Vision-Language Models, Multimodal large language model, Hallucination",
    "abstract": "Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs. This paper investigates the underlying mechanisms of hallucination, focusing on the unique structure of LVLMs that distinguishes them from LLMs. We identify that hallucinations often arise from the sensitivity of text decoders to vision inputs, a natural phenomenon when image encoders and text decoders are pre-trained separately. Inspired by this, we introduce Visual and Textual Intervention (VTI), a novel technique designed to reduce hallucinations by steering latent space representations during inference to enhance the stability of vision features. As a task-agnostic test-time intervention, VTI can be easily applied to any problem without additional training costs. Extensive experiments demonstrate that it can effectively reduce hallucinations",
    "openreview_id": "LBl7Hez0fF",
    "forum_id": "LBl7Hez0fF"
  },
  "analysis_timestamp": "2026-01-06T07:37:19.201126"
}
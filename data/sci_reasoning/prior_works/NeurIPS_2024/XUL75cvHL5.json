{
  "prior_works": [
    {
      "title": "Stochastic Approximation and Recursive Algorithms and Applications (2nd ed.)",
      "authors": "Harold J. Kushner, G. George Yin",
      "year": 2003,
      "role": "Foundational theory for SA with Markovian noise; weak convergence and Poisson-equation method",
      "relationship_sentence": "This book supplies the weak-convergence framework and Poisson-equation decomposition for SA with Markovian data that the paper refines to the constant-stepsize, nonlinear setting and to the joint process (x_k, \u03b8_k)."
    },
    {
      "title": "The ODE method for Markov chain models",
      "authors": "Vivek S. Borkar, Sean P. Meyn",
      "year": 2000,
      "role": "ODE/martingale approach to SA coupled with Markov chains",
      "relationship_sentence": "The ODE viewpoint for SA driven by Markov chains underlies the paper\u2019s treatment of iterate\u2013data coupling, which the authors sharpen to track fine correlations needed for joint-process weak convergence at constant stepsize."
    },
    {
      "title": "Markov Chains and Stochastic Stability (2nd ed.)",
      "authors": "Sean P. Meyn, Richard L. Tweedie",
      "year": 2009,
      "role": "Ergodicity/recurrence and Poisson-equation theory for Harris chains",
      "relationship_sentence": "The recurrence and Poisson-equation machinery from this monograph is used to control mixing and solve for bias terms when characterizing the stationary behavior of constant-stepsize SA with Markovian data."
    },
    {
      "title": "Adaptive Algorithms and Stochastic Approximations",
      "authors": "Albert Benveniste, Michel M\u00e9tivier, Pierre Priouret",
      "year": 1990,
      "role": "Early SA theory with dependent observations and smoothness-based expansions",
      "relationship_sentence": "Their smoothness and martingale\u2013Poisson decomposition techniques are extended in the paper to quantify constant-stepsize bias beyond linear updates and to separate contributions from nonlinearity versus data dependence."
    },
    {
      "title": "Finite-Time Error Bounds for Linear Stochastic Approximation and TD Learning",
      "authors": "R. Srikant, Lei Ying",
      "year": 2019,
      "role": "Constant-stepsize linear SA with Markovian data; bias and finite-time control",
      "relationship_sentence": "This work establishes O(\u03b1) stationary bias for linear SA/TD under Markovian noise; the paper generalizes beyond linearity and isolates an additional interaction term arising from the collusion of memory and nonlinearity."
    },
    {
      "title": "A Finite-Time Analysis of Temporal-Difference Learning with Linear Function Approximation",
      "authors": "Jalaj Bhandari, Daniel Russo, Raghav Singal",
      "year": 2018,
      "role": "Linear TD under Markovian sampling; Poisson-equation-based correlation control",
      "relationship_sentence": "The paper adapts and strengthens the Poisson-equation and mixing-based tools from this linear TD analysis to handle nonlinear SA and to prove weak convergence of the joint process."
    },
    {
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "authors": "Matthew D. Mandt, Matthew D. Hoffman, David M. Blei",
      "year": 2017,
      "role": "Constant-stepsize SGD diffusion/stationary viewpoint under i.i.d. noise",
      "relationship_sentence": "The paper extends the stationary-distribution and linearization intuition from this i.i.d. setting to Markovian data, revealing new bias terms, including a cross-term from the interaction of memory and nonlinearity."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core advance\u2014establishing weak convergence of the joint process (x_k, \u03b8_k) for constant-stepsize nonlinear stochastic approximation with Markovian data and precisely decomposing the asymptotic bias\u2014builds on two intertwined traditions. From classical SA with Markovian noise, Kushner\u2013Yin and Borkar\u2013Meyn provide the weak-convergence/ODE and Poisson-equation frameworks for converting dependent noise into tractable martingale terms. Meyn\u2013Tweedie anchors the required recurrence and ergodicity conditions and guarantees existence and properties of Poisson solutions, which the authors leverage to rigorously manage iterate\u2013data correlations. Benveniste\u2013M\u00e9tivier\u2013Priouret contributes smoothness-based expansions and martingale decompositions that the paper adapts to quantify higher-order bias in a nonlinear regime.\n\nOn the constant-stepsize front, linear SA and TD analyses under Markovian sampling (Bhandari\u2013Russo\u2013Singal; Srikant\u2013Ying) revealed an O(\u03b1) steady-state bias but were confined to linear updates; the present work both recovers those linear effects and shows an additional cross-term created by the interaction of Markovian memory with nonlinearity. Complementing these, Mandt\u2013Hoffman\u2013Blei\u2019s stationary OU approximation for i.i.d. SGD motivates a stationary-distribution viewpoint; the paper extends that intuition to Markovian data and develops a fine-grained control of the \u03b8_k\u2013x_k correlation. Together, these prior works directly enable the new joint-process weak convergence result and the first precise decomposition of constant-stepsize bias into memory, nonlinearity, and their interaction.",
  "analysis_timestamp": "2026-01-06T23:33:36.264521"
}
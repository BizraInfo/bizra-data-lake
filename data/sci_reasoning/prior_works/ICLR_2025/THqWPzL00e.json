{
  "prior_works": [
    {
      "title": "Topographic deep artificial neural networks reproduce the functional organization of visual cortex",
      "authors": "Lindsey et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "TopoLoss is designed explicitly to achieve cortex-like maps without the ImageNet performance drop reported when TDANNs use a spatial-correlation penalty over a cortical sheet."
    },
    {
      "title": "Self-Organizing Maps",
      "authors": "Teuvo Kohonen",
      "year": 1982,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "TopoLoss borrows the core SOM idea of a distance-weighted neighborhood function that encourages nearby units on a 2D lattice to learn similar features, but implements it as a modern differentiable regularizer integrated with supervised training."
    },
    {
      "title": "SOM-VAE: Interpretable Discrete Representation Learning with Self-Organizing Maps",
      "authors": "Fortuin et al.",
      "year": 2019,
      "arxiv_id": "1806.02199",
      "role": "Extension",
      "relationship_sentence": "This work showed how to couple a SOM-style topographic objective with deep networks; TopoLoss generalizes that principle beyond VAEs to high-performance CNNs and Transformers while maintaining task accuracy."
    },
    {
      "title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples",
      "authors": "Belkin et al.",
      "year": 2006,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "TopoLoss operationalizes a locality-based smoothness prior\u2014akin to Laplacian/manifold regularization\u2014over neuron positions, encouraging nearby units to have similar representations during supervised training."
    },
    {
      "title": "Natural speech reveals the semantic maps that tile human cerebral cortex",
      "authors": "Huth et al.",
      "year": 2016,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "These semantic maps provide the specific topographic signatures and brain benchmarks that TopoNets aim to replicate in language, motivating the formulation of a loss that yields localized, semantically organized units."
    },
    {
      "title": "Maps in the brain: What can we learn from them?",
      "authors": "Chklovskii and Koulakov",
      "year": 2004,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The wiring-cost principle\u2014nearby neurons should encode similar features to reduce wiring\u2014directly motivates TopoLoss\u2019s distance-weighted similarity penalty to induce biologically plausible topography."
    }
  ],
  "synthesis_narrative": "Classic self-organizing maps introduced a neighborhood function over a two-dimensional lattice, enforcing that nearby units develop similar tunings through a distance-weighted update; this anchored the idea that topography can be induced by explicit locality constraints. SOM-VAE demonstrated how to embed such a topographic objective into modern deep learning by coupling a SOM-style loss with a variational autoencoder, showing end-to-end differentiability and practical training with deep architectures. Manifold regularization formalized locality-based smoothness as a learnable penalty\u2014via graph/Laplacian terms\u2014that can be combined with supervised objectives, providing a general framework for blending inductive spatial smoothness with task learning. In neuroscience, wiring-economy theory argued that cortical maps arise because nearby neurons encoding similar features reduce wiring costs, offering a principled target for locality-promoting constraints. For language, the discovery of continuous semantic maps across cortex established concrete topographic signatures\u2014localized, smoothly varying feature representations\u2014that models could aim to emulate. Most directly, topographic deep artificial neural networks (TDANNs) instantiated a spatial-correlation penalty over a cortical sheet to reproduce visual cortical organization, but reported a trade-off: stronger topography typically came at the expense of task accuracy. Together, these works revealed both a mechanism (distance-weighted locality constraints) and a limitation (performance trade-offs). Building on SOM-style neighborhood kernels, manifold smoothness, and wiring-economy motivation, while addressing TDANNs\u2019 performance gap, the present work formulates a new, architecture-agnostic topographic loss that integrates seamlessly with supervised training to yield strong topography in both vision and language models without sacrificing accuracy.",
  "target_paper": {
    "title": "TopoNets: High performing vision and language models with brain-like topography",
    "authors": "Mayukh Deb, Mainak Deb, Apurva Ratan Murty",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "topography, neuro-inspired, convolutional neural networks, Transformers, visual cortex, neuroscience",
    "abstract": "Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we present *TopoLoss*, a new loss function that promotes spatially organized topographic representations in AI models without significantly sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision (ResNet-18, ResNet-50, ViT) and language models (GPT-Neo-125M, NanoGPT), collectively *TopoNets*. TopoNets are the highest performing supervised topographic models to date, exhibiting brain-like properties such as localized feature processing, lower dimensionality, and increased efficiency. TopoNets also predict responses in the brain and replicate the key topographic signatures observed in the brain\u2019s visual and language cort",
    "openreview_id": "THqWPzL00e",
    "forum_id": "THqWPzL00e"
  },
  "analysis_timestamp": "2026-01-06T07:43:41.466310"
}
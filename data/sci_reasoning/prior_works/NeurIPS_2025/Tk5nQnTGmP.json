{
  "prior_works": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "authors": "Alethea Power, Yuri Burda, Harri Edwards, Joshua Achiam, Vedant Misra",
      "year": 2022,
      "role": "Anchor phenomenon",
      "relationship_sentence": "Introduced and characterized the grokking phenomenon on small arithmetic tasks, providing the empirical setting and key behaviors (late generalization after memorization) that this paper reinterprets through a glass-relaxation lens."
    },
    {
      "title": "Progress Measures for Grokking via Mechanistic Interpretability",
      "authors": "Neel Nanda et al.",
      "year": 2023,
      "role": "Mechanistic analysis of grokking",
      "relationship_sentence": "Dissected transformers trained on modular arithmetic into memorization vs algorithmic circuits over time, directly informing the paper\u2019s phase-based interpretation and experimental choices on arithmetic tasks."
    },
    {
      "title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys",
      "authors": "Pratik Chaudhari et al.",
      "year": 2016,
      "role": "Local entropy/landscape methodology",
      "relationship_sentence": "Formalized and operationalized local entropy and basin volume in neural loss landscapes, enabling the present work\u2019s sampling and depiction of a Boltzmann-style entropy landscape over loss and test accuracy."
    },
    {
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "authors": "Martin Mandt, Matthew D. Hoffman, David M. Blei",
      "year": 2017,
      "role": "Thermodynamic view of SGD",
      "relationship_sentence": "Showed SGD behaves like a stochastic sampler with an effective temperature tied to noise, justifying the paper\u2019s mapping of training loss to energy and of learning-rate/batch-size schedules to quench/relaxation dynamics."
    },
    {
      "title": "The Loss Surfaces of Multilayer Networks",
      "authors": "Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00e9rard Ben Arous, Yann LeCun",
      "year": 2015,
      "role": "Spin-glass analogy for loss landscapes",
      "relationship_sentence": "Connected neural loss surfaces to spin-glass models, providing the statistical-physics foundation for interpreting memorization and late generalization as glassy non-equilibrium dynamics and relaxation."
    },
    {
      "title": "Don\u2019t Decay the Learning Rate, Increase the Batch Size",
      "authors": "Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, Quoc V. Le",
      "year": 2018,
      "role": "Noise/temperature control in training",
      "relationship_sentence": "Related SGD noise scale to learning rate and batch size, supporting the paper\u2019s framing of early training as a rapid cooling (noise reduction) and later training as slow relaxation in the landscape."
    },
    {
      "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
      "authors": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang",
      "year": 2017,
      "role": "Flatness\u2013generalization link",
      "relationship_sentence": "Connected basin geometry (sharp vs. flat) to generalization, motivating the paper\u2019s entropy-centric view and its conclusion about the absence of an entropy barrier during the memorization phase."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014recasting grokking as a computational glass relaxation and sampling a Boltzmann-style entropy landscape over loss and accuracy\u2014sits at the intersection of grokking studies and statistical-physics views of neural training. The empirical phenomenon originates in Power et al., who showed that small algorithmic datasets yield late generalization after prolonged overfitting; Nanda et al. then revealed a mechanistic phase transition from memorization to algorithmic circuits in transformers, shaping the present work\u2019s phase-based experimental design and interpretive lens.\nOn the physics side, Choromanska et al. drew a seminal connection between deep-network loss surfaces and spin-glass landscapes, legitimizing a glassy, non-equilibrium framing of optimization dynamics. Mandt et al. provided the thermodynamic bridge by interpreting SGD as a stochastic sampler with an effective temperature, allowing training loss to be treated as energy and learning schedules as annealing/quenches. Smith et al. made this operational by linking temperature to batch size and learning rate, underpinning the paper\u2019s interpretation of early memorization as a rapid quench and subsequent generalization as slow relaxation. Chaudhari et al.\u2019s Entropy-SGD introduced local-entropy tools to probe basin volume, directly inspiring the paper\u2019s entropy-landscape sampling methodology that relates configuration density to both training loss and test accuracy. Finally, Keskar et al. tied flatness (volume/entropy) to generalization, providing a geometry\u2013generalization rationale for assessing whether entropy barriers exist in the memorization regime. Together, these works enable a coherent computational-glass account of grokking and justify the paper\u2019s key finding of no entropy barrier during memorization.",
  "analysis_timestamp": "2026-01-07T00:21:33.127659"
}
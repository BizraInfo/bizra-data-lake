{
  "prior_works": [
    {
      "title": "Auto-Encoding Variational Bayes",
      "authors": "Diederik P. Kingma, Max Welling",
      "year": 2014,
      "role": "Foundational VAE objective and amortized inference",
      "relationship_sentence": "P\u2011VAE directly builds on the ELBO and amortized inference framework of AEVB, altering only the latent family (to Poisson spike counts) and inference dynamics while retaining the variational objective."
    },
    {
      "title": "\u03b2\u2011VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
      "authors": "Irina Higgins et al.",
      "year": 2017,
      "role": "Information bottleneck and cost interpretation of the KL term",
      "relationship_sentence": "P\u2011VAE\u2019s metabolically grounded regularizer echoes \u03b2\u2011VAE\u2019s KL-pressure idea, linking representational efficiency/sparsity to an explicit cost that shapes learned codes."
    },
    {
      "title": "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects",
      "authors": "Rajesh P. N. Rao, David H. Ballard",
      "year": 1999,
      "role": "Predictive coding computational principle",
      "relationship_sentence": "The P\u2011VAE\u2019s inference dynamics and error-correcting architecture are motivated by the Rao\u2013Ballard predictive coding paradigm that marries top\u2011down generative predictions with bottom\u2011up residuals."
    },
    {
      "title": "An approximation of the error backpropagation algorithm in a predictive coding network",
      "authors": "James C. R. Whittington, Rafal Bogacz",
      "year": 2017,
      "role": "Link between predictive coding and variational/free-energy training",
      "relationship_sentence": "P\u2011VAE leverages the result that predictive coding minimizes a variational free energy, providing a theoretical bridge that justifies implementing a VAE with predictive-coding style inference updates."
    },
    {
      "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
      "authors": "Bruno A. Olshausen, David J. Field",
      "year": 1996,
      "role": "Sparse coding normative objective and metabolic efficiency",
      "relationship_sentence": "P\u2011VAE\u2019s Poisson\u2011induced metabolic cost/penalty explicitly connects to the sparse coding objective of Olshausen & Field, and the paper empirically verifies this sparsity relationship."
    },
    {
      "title": "Deep Exponential Families",
      "authors": "Rajesh Ranganath, Linpeng Tang, Laurent Charlin, David M. Blei",
      "year": 2015,
      "role": "Deep generative modeling with count distributions via variational inference",
      "relationship_sentence": "DEFs established variational training for deep generative models with Poisson/count structure, a key antecedent for P\u2011VAE\u2019s choice of count-valued latent variables and VI-based learning."
    },
    {
      "title": "Neural Variational Inference and Learning in Belief Networks (NVIL)",
      "authors": "Andriy Mnih, Karol Gregor",
      "year": 2014,
      "role": "Gradient estimation for discrete latent variables",
      "relationship_sentence": "NVIL provided practical gradient estimators for models with discrete latents, informing how P\u2011VAE can train Poisson spike-count latents within an amortized variational framework."
    }
  ],
  "synthesis_narrative": "Poisson VAE (P\u2011VAE) integrates three strands of prior work: variational autoencoders, predictive coding, and sparse/efficient neural codes. Auto\u2011Encoding Variational Bayes supplied the ELBO and amortized inference template that P\u2011VAE preserves while changing the latent family to discrete Poisson spike counts. \u03b2\u2011VAE crystallized how the KL term acts as an information pressure shaping representations; P\u2011VAE reframes this pressure as a metabolically grounded spike\u2011rate cost that naturally arises from Poisson latents integrated with predictive coding, thereby tying representation quality to energetic efficiency. The predictive coding lineage\u2014initiated by Rao & Ballard and formalized algorithmically by Whittington & Bogacz\u2014provides the architectural and theoretical scaffolding for P\u2011VAE\u2019s error\u2011correcting dynamics and its equivalence to variational free\u2011energy minimization, justifying a predictive\u2011coding interpretation of VAE training.\n\nOn the latent\u2011variable side, Deep Exponential Families demonstrated that deep generative models can be trained with count distributions via variational inference, legitimizing P\u2011VAE\u2019s move from continuous Gaussians to count\u2011valued latents. Training discrete latents at scale draws on techniques like NVIL, which introduced practical low\u2011variance gradient estimators for non\u2011reparameterizable variables, informing how to optimize Poisson spike counts with amortized inference. Finally, Olshausen & Field\u2019s sparse coding objective connects directly to P\u2011VAE\u2019s emergent metabolic cost: the Poisson\u2011driven regularization mirrors classic sparsity penalties and explains the empirically observed sparse, high\u2011dimensional, linearly separable representations. Together, these works directly enable P\u2011VAE\u2019s core contribution: a biologically grounded, predictive\u2011coding VAE with Poisson spike\u2011count latents that couples metabolic efficiency with modern variational learning.",
  "analysis_timestamp": "2026-01-06T23:39:42.955366"
}
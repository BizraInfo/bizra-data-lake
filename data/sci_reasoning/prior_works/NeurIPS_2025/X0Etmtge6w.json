{
  "prior_works": [
    {
      "title": "On the Optimal Error and Reject Trade-Off",
      "authors": "C. K. Chow",
      "year": 1970,
      "role": "Foundational reject-option loss",
      "relationship_sentence": "Chow formalized the 0-1-c (abstain) loss and the corresponding Bayes decision rule that this paper targets; the proposed surrogates are designed to be Bayes-consistent with the Chow-optimal policy extended to a two-stage setting where stage two has extra information."
    },
    {
      "title": "Classification with Reject Option using Hinge Loss",
      "authors": "Radu Herbei, Marten H. Wegkamp",
      "year": 2006,
      "role": "Surrogate design and consistency for reject option",
      "relationship_sentence": "This work showed how margin-based convex surrogates can yield consistent learning under reject-option losses; the present paper adapts and generalizes these surrogate and calibration ideas to a joint two-classifier, defer-or-predict objective with an explicit consultation cost."
    },
    {
      "title": "On the Foundations of Selective Classification",
      "authors": "Ran El-Yaniv, Yair Wiener",
      "year": 2010,
      "role": "Theory of selective prediction/abstention",
      "relationship_sentence": "Their risk\u2013coverage framework underpins the decision-theoretic view of deferral, informing how the first-stage policy should trade accuracy against the cost of passing to a second stage."
    },
    {
      "title": "Boosting with Abstention",
      "authors": "Corinna Cortes, G. DeSalvo, Mehryar Mohri",
      "year": 2016,
      "role": "Convex surrogates for abstain losses",
      "relationship_sentence": "They provided concrete convex surrogates and algorithms for abstention losses; the current paper builds on such surrogate constructions to craft losses that are consistent for the 0-1+c objective in a two-stage, costed-defer pipeline."
    },
    {
      "title": "Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer",
      "authors": "David Madras, Toniann Pitassi, Richard Zemel",
      "year": 2018,
      "role": "Learning-to-defer framework",
      "relationship_sentence": "Introduced the modern defer-to-expert/model paradigm with explicit deferral costs; this paper extends that setting by jointly training both stages and by providing theoretically consistent surrogates when the second stage has additional features."
    },
    {
      "title": "Consistent Estimators for Learning to Defer to an Expert",
      "authors": "Hussein Mozannar, David Sontag",
      "year": 2020,
      "role": "Consistency in learning-to-defer",
      "relationship_sentence": "They proved that naive surrogates for deferral can be inconsistent and proposed consistent estimators; the present work advances this line by deriving consistency guarantees and surrogates tailored to the two-stage, extra-information setting with an explicit consultation cost."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014designing surrogate losses that are Bayes-consistent for a two-stage classifier that may defer to a costlier second model with access to additional features\u2014rests on three intertwined threads of prior work. First, the reject/abstain tradition, originating with Chow, defines the 0-1-c loss and the Bayes-optimal decision boundary for paying a fixed abstention cost. Subsequent developments by Herbei and Wegkamp, and Cortes, DeSalvo, and Mohri, established that carefully constructed convex surrogates can consistently optimize abstention-style risks, providing a toolbox of margin-based and boosting-style losses and calibration arguments. El-Yaniv and Wiener\u2019s selective classification framework further clarifies the risk\u2013coverage trade-off that any deferral mechanism must navigate.\nSecond, the modern learning-to-defer literature connects abstention to collaboration with another decision-maker. Madras et al. formulated an explicit defer-to-expert objective with costs, highlighting practical training schemes. Mozannar and Sontag then revealed that naive surrogates in this setting can be inconsistent and provided principled estimators with consistency guarantees.\nThis paper synthesizes these strands: it adopts the cost-sensitive defer objective of learning-to-defer, but departs in two crucial ways\u2014jointly training both stages and allowing the second stage access to strictly richer features. It leverages and extends the abstention-surrogate theory to construct losses that are calibrated to the 0-1+c objective in this asymmetric-information, two-stage pipeline, delivering Bayes-consistency and practical training procedures grounded in the earlier theoretical insights.",
  "analysis_timestamp": "2026-01-07T00:02:04.943477"
}
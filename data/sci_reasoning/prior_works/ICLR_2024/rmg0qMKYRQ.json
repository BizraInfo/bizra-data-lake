{
  "prior_works": [
    {
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
      "authors": "Robert Geirhos et al.",
      "year": 2019,
      "arxiv_id": "1811.12231",
      "role": "Foundation",
      "relationship_sentence": "This work introduced the cue-conflict shape-vs-texture paradigm and evaluation stimuli that we reuse to quantify shape bias, establishing the precise behavioral yardstick our generative classifiers are tested against."
    },
    {
      "title": "Shortcut Learning in Deep Neural Networks",
      "authors": "Robert Geirhos et al.",
      "year": 2020,
      "arxiv_id": "2004.07780",
      "role": "Gap Identification",
      "relationship_sentence": "By articulating that discriminative models latch onto shortcuts that diverge from human strategies, this paper directly motivates our central question of whether generative inference exhibits more human-like behavior."
    },
    {
      "title": "Diffusion Models Beat GANs on Image Synthesis",
      "authors": "Prafulla Dhariwal and Alex Nichol",
      "year": 2021,
      "arxiv_id": "2105.05233",
      "role": "Foundation",
      "relationship_sentence": "Their classifier guidance formalism links class gradients to diffusion model scores, providing the Bayes/score connection we leverage to derive decision rules for turning diffusion generators into classifiers."
    },
    {
      "title": "Classifier-Free Diffusion Guidance",
      "authors": "Jonathan Ho and Tim Salimans",
      "year": 2022,
      "arxiv_id": "2207.12598",
      "role": "Extension",
      "relationship_sentence": "We directly exploit the conditional\u2013unconditional score interpolation of classifier-free guidance to compute class-conditional likelihood surrogates from text prompts, enabling zero-shot generative classification."
    },
    {
      "title": "Imagen: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
      "authors": "Chitwan Saharia et al.",
      "year": 2022,
      "arxiv_id": "2205.11487",
      "role": "Foundation",
      "relationship_sentence": "Imagen is the specific text-to-image diffusion model we instantiate as a generative classifier, whose high-fidelity text conditioning makes our psychophysics-scale evaluations and shape-bias findings possible."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "arxiv_id": "2103.00020",
      "role": "Baseline",
      "relationship_sentence": "CLIP serves as our primary zero-shot discriminative baseline for shape bias, OOD performance, and human error alignment, against which we contrast generative vs. discriminative inference."
    },
    {
      "title": "Large-scale, high-precision comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks",
      "authors": "Rishi Rajalingham et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work defines the behavioral-comparison framework for aligning model and human object confusions, providing the error-alignment metrics we adopt to assess human-like classification errors."
    }
  ],
  "synthesis_narrative": "Classifier guidance established a principled link between class information and diffusion model scores, showing how gradients of class probabilities can steer generation; classifier-free guidance then removed the need for an external classifier by interpolating conditional and unconditional scores, making text conditioning tractable and stable. Imagen demonstrated that such text-to-image diffusion models can achieve strong semantic fidelity, providing a practical high-capacity conditional generator. Independently, the cue-conflict paradigm revealed that standard ImageNet-trained CNNs are texture-biased rather than shape-biased, and introduced precise stimuli for measuring shape-vs-texture preferences. Shortcut learning further argued that discriminative training encourages reliance on superficial cues, highlighting a mechanistic reason for the observed non-human-like behaviors. For behavioral evaluation, work comparing primate and model object recognition introduced high-precision error-consistency metrics to quantify alignment between human and model confusions. Meanwhile, CLIP established a powerful zero-shot discriminative baseline grounded in language supervision, becoming the de facto comparator for recognition without task-specific training.\n\nTogether, these strands suggested a clear opportunity: leverage modern text-conditioned diffusion models\u2014and their score-based conditioning\u2014to perform zero-shot classification and test whether generative inference alleviates the shortcut and texture-bias issues documented for discriminative models. By instantiating the decision rule enabled by guidance techniques on a strong generator like Imagen, and evaluating with established psychophysical benchmarks and human-alignment metrics, the present work synthesizes these advances to show that generative classifiers exhibit pronounced shape bias, improved OOD behavior, and human-like error patterns relative to discriminative baselines such as CLIP.",
  "target_paper": {
    "title": "Intriguing Properties of Generative Classifiers",
    "authors": "Priyank Jaini, Kevin Clark, Robert Geirhos",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "diffusion models, zero-shot, text-to-image, generative models, human visual perception, psychophysics, cognitive science, neuroscience",
    "abstract": "What is the best paradigm to recognize objects---discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data.\nWe report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.",
    "openreview_id": "rmg0qMKYRQ",
    "forum_id": "rmg0qMKYRQ"
  },
  "analysis_timestamp": "2026-01-06T17:20:39.155638"
}
{
  "prior_works": [
    {
      "title": "Energy-based Out-of-Distribution Detection",
      "authors": "Weitang Liu, Xiaoyun Wang, John D. Owens, Yixuan Li",
      "year": 2020,
      "role": "Energy-based uncertainty precursor",
      "relationship_sentence": "Introduced using the log-sum-exp over logits as an energy score for OOD/uncertainty; GEBM builds on the energy principle but departs from logit-derived energies by learning a regularized, integrable data-space energy and aggregating it across graph-structural scales."
    },
    {
      "title": "Your Classifier is Secretly an Energy Based Model (JEM)",
      "authors": "Jeremy Grathwohl, Kuan-Chieh Wang, J\u00f6rn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, Kevin Swersky",
      "year": 2020,
      "role": "Classifier-as-EBM foundation",
      "relationship_sentence": "Framed discriminative classifiers as EBMs for joint density modeling, motivating energy-based uncertainty; GEBM leverages this EBM perspective while addressing limitations of logit-based energies by enforcing integrability and adapting it to graph data with multi-scale aggregation."
    },
    {
      "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank (APPNP)",
      "authors": "Johannes Klicpera, Aleksandar Bojchevski, Stephan G\u00fcnnemann",
      "year": 2019,
      "role": "Graph diffusion mechanism enabling multi-scale views",
      "relationship_sentence": "Established personalized PageRank diffusion as a principled propagation scheme; GEBM uses graph diffusion to obtain structural levels and aggregates energy across these scales to capture both local and global epistemic effects."
    },
    {
      "title": "Diffusion Improves Graph Learning (Graph Diffusion Convolution, GDC)",
      "authors": "Johannes Klicpera, Aleksandar Bojchevski, Stephan G\u00fcnnemann",
      "year": 2019,
      "role": "Post-hoc diffusion framework for any GNN",
      "relationship_sentence": "Proposed diffusion-based post-processing applicable to pre-trained GNNs; GEBM mirrors this post-hoc philosophy by applying an energy model over diffused representations to produce structure-aware uncertainty without retraining the base GNN."
    },
    {
      "title": "Evidential Deep Learning to Quantify Classification Uncertainty",
      "authors": "Murat Sensoy, Lance Kaplan, Melih Kandemir",
      "year": 2018,
      "role": "Evidential uncertainty framework",
      "relationship_sentence": "Introduced Dirichlet-evidence modeling for classification uncertainty; GEBM adapts an evidential interpretation to energy outputs, improving predictive robustness by treating energy as evidence over hypotheses."
    },
    {
      "title": "Predictive Uncertainty Estimation via Prior Networks (Dirichlet Prior Networks)",
      "authors": "Andrey Malinin, Mark Gales",
      "year": 2018,
      "role": "Prior-driven epistemic modeling for distribution shift",
      "relationship_sentence": "Showed how Dirichlet priors can disentangle in-distribution vs OOD behavior; GEBM leverages this idea by mapping energy-based scores to evidential (Dirichlet-like) quantities to better separate epistemic effects under shifts."
    }
  ],
  "synthesis_narrative": "GEBM\u2019s core contribution\u2014an energy-based, post hoc framework that aggregates uncertainty across graph-structural scales with an evidential interpretation\u2014sits at the intersection of energy-based uncertainty, graph diffusion, and evidential learning. Two strands in energy modeling directly motivate its design. First, energy-based OOD detection established the utility of logit-derived energy (log-sum-exp) for uncertainty scoring, while JEM framed classifiers as EBMs to connect prediction and density modeling. GEBM embraces the energy perspective but addresses a key limitation of logit-based approaches by regularizing the energy to induce an integrable density in data space, enabling principled uncertainty scoring beyond logits.\nOn the graph side, APPNP and GDC introduced diffusion as a principled, scalable mechanism to obtain multi-hop, multi-scale structural context and, crucially, demonstrated post hoc applicability to arbitrary GNNs. GEBM leverages exactly this property: it computes energy at successive diffusion levels and aggregates them, unifying structure-agnostic (node/feature-local) and structure-aware (propagated) epistemic signals into a single measure without retraining the base GNN.\nFinally, evidential deep learning and Dirichlet Prior Networks supply the interpretive lens to transform energy into calibrated evidence over class hypotheses. By mapping energy to an evidential representation, GEBM enhances robustness under distribution shift and improves separation of in- vs out-of-distribution nodes. Together, these works directly underpin GEBM\u2019s integrable EBM, multi-scale diffusion aggregation, and evidential post hoc uncertainty for GNNs.",
  "analysis_timestamp": "2026-01-06T23:39:42.960475"
}
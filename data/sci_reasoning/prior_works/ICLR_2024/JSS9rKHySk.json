{
  "prior_works": [
    {
      "title": "Contextual Decision Processes with Low Bellman Rank are PAC-Learnable",
      "authors": "Nan Jiang et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "By introducing Bellman rank and framing function approximation assumptions as structural restrictions on the admissible MDP class, this work provides the foundational lens that the paper formalizes and analyzes for offline RL."
    },
    {
      "title": "Model-Based Reinforcement Learning in Contextual Decision Processes",
      "authors": "Wen Sun et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "This paper\u2019s model-realizability framework and witness-based analysis directly motivate using model classes to characterize MDP restrictions, which the paper exploits to construct generic lower bounds transferable across function classes."
    },
    {
      "title": "Statistical Complexity of Interactive Decision Making",
      "authors": "Dylan J. Foster et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Its decision-estimation coefficient and information-theoretic lower-bound techniques inform the paper\u2019s perspective of treating function approximation assumptions as constraints on MDP families and deriving minimax lower bounds."
    },
    {
      "title": "Is Pessimism Provably Efficient for Offline Reinforcement Learning?",
      "authors": "Tengyang Xie et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By establishing guarantees under heterogeneous realizability and coverage assumptions, this work exposed ambiguity in approximation targets that the paper explicitly disentangles and systematizes for offline RL."
    },
    {
      "title": "A Minimax Theory for Offline Reinforcement Learning with Linear Function Approximation",
      "authors": "Yin et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Their representation-specific lower-bound constructions for linear function classes are generalized by the paper via model-realizability to yield class-agnostic, reusable lower bounds."
    },
    {
      "title": "Statistically Efficient Off-Policy Policy Evaluation for Reinforcement Learning with Finite Horizons",
      "authors": "Nathan Kallus et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Its minimax lower bounds for off-policy evaluation under model-based assumptions provide technical tools and a reduction viewpoint that underpin the paper\u2019s model-realizability-driven lower-bound methodology for policy learning."
    }
  ],
  "synthesis_narrative": "Bellman-rank theory established that function approximation assumptions can be formalized as structural constraints on the environment by requiring low-complexity Bellman factorization, thereby tying statistical learnability to restrictions on the underlying MDPs. Model-based analysis in contextual decision processes further operationalized this idea through model-realizability and witness-based diagnostics, clarifying how specifying a model class both enables learning guarantees and implicitly narrows the admissible MDP family. Complementing these, information-theoretic work on the statistical complexity of interactive decision making introduced the decision-estimation coefficient and hypothesis-testing lower bounds, offering general tools to convert such structural constraints into minimax limits. In offline RL, pessimism-based theory delivered sharp guarantees but under a patchwork of realizability targets and coverage assumptions, revealing ambiguity about precisely what is being approximated. For linear representations, minimax upper and lower bounds were derived via class-specific hard instances, while off-policy evaluation attained tight minimax characterizations under model-based assumptions, illuminating how model realism can anchor lower-bound constructions.\nTogether these strands expose both an opportunity and a gap: powerful general frameworks existed to view function classes as MDP restrictions and to derive information-theoretic lower bounds, yet offline RL lacked a unified clarification of approximation targets and generic lower bounds beyond specific representations. By synthesizing the model-realizability lens with the DEC-style information-theoretic machinery, it is natural to formalize general function approximation as an MDP restriction in offline RL and to leverage model-realizability to craft generic, portable lower bounds\u2014leading directly to the paper\u2019s clarified assumption taxonomy and its two general-purpose minimax lower bounds.",
  "target_paper": {
    "title": "On the Role of General Function Approximation in Offline Reinforcement Learning",
    "authors": "Chenjie Mao, Qiaosheng Zhang, Zhen Wang, Xuelong Li",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "reinforcement learning theory, offline reinforcement learning, general function approximation, learnability, minimax lower bounds",
    "abstract": "We study offline reinforcement learning (RL) with general function approximation. General function approximation is a powerful tool for algorithm design and analysis, but its adaptation to offline RL encounters several challenges due to varying approximation targets and assumptions that blur the real meanings of function assumptions. In this paper, we try to formulate and clarify the treatment of general function approximation in offline RL in two aspects: (1) analyzing different types of assumptions and their practical usage, and (2) understanding its role as a restriction on underlying MDPs from information-theoretic perspectives. Additionally, we introduce a new insight for lower bound establishing: one can exploit model-realizability to establish general-purpose lower bounds that can be generalized into other functions. Building upon this insight, we propose two generic lower bounds that contribute to a better understanding of offline RL with general function approximation.",
    "openreview_id": "JSS9rKHySk",
    "forum_id": "JSS9rKHySk"
  },
  "analysis_timestamp": "2026-01-06T08:54:04.311748"
}
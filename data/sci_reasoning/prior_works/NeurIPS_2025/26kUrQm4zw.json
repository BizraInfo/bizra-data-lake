{
  "prior_works": [
    {
      "title": "Information Foraging",
      "authors": "Peter Pirolli, Stuart Card",
      "year": 1999,
      "role": "Foundational theory of information seeking (information scent, patch models)",
      "relationship_sentence": "InForage explicitly borrows Information Foraging Theory\u2019s notions of information scent and patch-leaving to formalize retrieval-augmented reasoning as a sequential foraging process and to design intermediate rewards that reflect the quality of gathered evidence."
    },
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "authors": "Patrick Lewis et al.",
      "year": 2020,
      "role": "Canonical static retrieval-augmented generation (RAG) baseline",
      "relationship_sentence": "RAG established the effectiveness of augmenting LMs with retrieved evidence but used fixed, pre-inference retrieval; InForage directly addresses this limitation by making retrieval adaptive and stepwise with feedback-driven optimization."
    },
    {
      "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
      "authors": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang",
      "year": 2020,
      "role": "Learning retrieval as part of the LM objective",
      "relationship_sentence": "REALM\u2019s idea of jointly optimizing retrieval and language modeling motivates InForage\u2019s treatment of retrieval decisions as learnable; InForage extends this to inference-time control with RL and explicit intermediate retrieval rewards."
    },
    {
      "title": "WebGPT: Browser-assisted question-answering with human feedback",
      "authors": "Reiichiro Nakano et al.",
      "year": 2022,
      "role": "RLHF for tool-assisted browsing and citation",
      "relationship_sentence": "WebGPT demonstrated that reward models can guide browsing and citation quality; InForage generalizes this by rewarding intermediate retrieval quality throughout multi-step search to shape adaptive search policies."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Interleaving chain-of-thought with tool use",
      "relationship_sentence": "ReAct showed that LMs can interleave reasoning with search actions; InForage formalizes this interaction as an MDP and optimizes when/what to search using reinforcement learning guided by retrieval-quality signals."
    },
    {
      "title": "Self-RAG: Learning to Retrieve, Generate, and Critique for Language Models",
      "authors": "Akari Asai et al.",
      "year": 2023,
      "role": "Adaptive retrieval decisions and self-critique for grounded generation",
      "relationship_sentence": "Self-RAG\u2019s retrieve/generate/critique loop motivates InForage\u2019s iterative evidence acquisition; InForage advances this by using explicit intermediate rewards to directly optimize the quality of retrieved context across steps."
    },
    {
      "title": "Self-Ask with Search",
      "authors": "Ofir Press et al.",
      "year": 2022,
      "role": "Decomposition into sub-questions with targeted web search",
      "relationship_sentence": "Self-Ask\u2019s technique of issuing intermediate sub-queries underscores the value of stepwise information gathering; InForage turns this insight into a learnable policy with reward shaping for each retrieval step."
    }
  ],
  "synthesis_narrative": "InForage\u2019s core contribution\u2014casting retrieval-augmented reasoning as a dynamic, reinforcement-learned information-seeking process with explicit intermediate rewards\u2014emerges at the intersection of information foraging theory, RAG-style grounding, and agentic test-time tool use. Pirolli and Card\u2019s Information Foraging Theory provides the conceptual backbone: information scent, patch models, and cost\u2013benefit trade-offs suggest that an effective agent should iteratively assess evidence quality and decide whether to continue exploiting a source or switch. Early retrieval-augmented methods like RAG and REALM established the value of grounding language models and hinted that retrieval itself should be optimized, but they largely relied on static pre-inference retrieval or training-time coupling without fine-grained control during inference.\n\nAgentic methods then demonstrated how to operationalize adaptive search at test time. ReAct interleaves chain-of-thought with explicit search actions, proving that reasoning can guide tool use. WebGPT showed that reinforcement learning with human feedback can shape browsing and citation behaviors, pointing to the feasibility of reward-driven optimization of search quality. Self-RAG introduced structured decisions to retrieve, generate, and critique, highlighting the benefit of iterative, quality-aware evidence integration. Complementarily, Self-Ask with Search demonstrated that decomposing tasks into sub-questions enables targeted retrieval at intermediate points.\n\nInForage synthesizes these threads by formalizing the search\u2013reason loop as an MDP and introducing intermediate rewards aligned with information scent to evaluate each retrieval step\u2019s utility. This unifies theoretical guidance (IFT) with practical agent architectures (ReAct, Self-RAG) and RL-based supervision (WebGPT), yielding an adaptive, multi-step retrieval policy that improves complex, evolving information needs.",
  "analysis_timestamp": "2026-01-07T00:27:38.144168"
}
{
  "prior_works": [
    {
      "title": "Risk-Sensitive Markov Decision Processes",
      "authors": "R. A. Howard et al.",
      "year": 1972,
      "role": "Foundation",
      "relationship_sentence": "Introduces the entropic (exponential-utility) risk-sensitive Bellman recursion that our algorithms directly use and analyze to design risk-sensitive pessimistic value iteration."
    },
    {
      "title": "Risk-averse dynamic programming for Markov decision processes",
      "authors": "Andrzej Ruszczynski",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "Provides the dynamic programming framework for risk-averse objectives (including the structure leveraged by entropic risk), which we exploit to obtain tight analyses of risk-sensitive Bellman operators."
    },
    {
      "title": "Actor-Critic Algorithms for Risk-Sensitive MDPs",
      "authors": "L. A. Prashanth et al.",
      "year": 2013,
      "role": "Related Problem",
      "relationship_sentence": "Develops algorithms explicitly for exponential-utility risk-sensitive MDPs, establishing the problem formulation and properties we extend to the offline setting with statistical guarantees."
    },
    {
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "authors": "Chi Jin et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Defines the linear MDP framework and associated concentration/linear-structure tools that our offline risk-sensitive analysis adopts to model transitions and derive sample complexity bounds."
    },
    {
      "title": "The Optimality of Pessimism in Offline Reinforcement Learning",
      "authors": "Tengyang Xie et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Establishes the pessimism principle as minimax-optimal for offline RL, directly motivating our adaptation of pessimistic backups to the entropic risk-sensitive Bellman operator."
    },
    {
      "title": "Pessimistic Value Iteration for Offline Reinforcement Learning",
      "authors": "Xie et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "Provides the PEVI template under linear MDPs for risk-neutral objectives; our core contribution modifies its backup to the entropic risk-sensitive operator and retools the uncertainty bonus/analysis accordingly."
    },
    {
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "authors": "Aviral Kumar et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates in practice that pessimism mitigates distributional shift in offline RL, motivating our theoretically grounded risk-sensitive pessimistic algorithms."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014pessimistic, sample-efficient offline RL under the entropic risk measure in linear MDPs\u2014emerges from marrying two lines of work: classical risk-sensitive dynamic programming and modern pessimistic offline RL under linear structure. Howard and Matheson (1972) established the entropic risk-sensitive Bellman recursion, while Ruszczynski (2010) formalized risk-averse dynamic programming principles that clarify contraction/monotonicity properties we leverage for tight analysis. Prior algorithmic work such as Prashanth and Ghavamzadeh (2013) operationalized exponential-utility risk sensitivity, fixing the problem formulation that we bring into the offline, finite-sample regime. On the representation side, Jin et al. (2020) introduced the linear MDP framework and the concentration machinery around linear features that our analysis crucially uses to control estimation error from offline data. The pessimism principle from offline RL\u2014formalized by Xie et al. (2021) as minimax-optimal\u2014directly motivates our pessimistic treatment of uncertainty, while the PEVI algorithmic template (Xie et al., 2021) supplies the risk-neutral baseline we extend by replacing the standard Bellman backup with the entropic risk-sensitive operator and redesigning the uncertainty penalty. Finally, Conservative Q-Learning (Kumar et al., 2020) provided influential empirical evidence that pessimism counters extrapolation error in offline settings, reinforcing the design choice to instantiate a principled, risk-sensitive variant with provable guarantees. Together, these works form the direct intellectual scaffolding for our risk-sensitive pessimistic value iteration and its refined variant in linear MDPs.",
  "analysis_timestamp": "2026-01-06T23:09:26.419175"
}
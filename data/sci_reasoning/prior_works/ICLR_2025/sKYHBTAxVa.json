{
  "prior_works": [
    {
      "title": "Measuring Massive Multitask Language Understanding",
      "authors": "Dan Hendrycks et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "LiveBench directly responds to the ubiquity and subsequent contamination issues of static, general-purpose benchmarks like MMLU by proposing a frequently updated, contamination-limited alternative covering similarly broad capabilities."
    },
    {
      "title": "MT-Bench: Benchmarking LLMs with Multi-Turn Questions via LLM-as-a-Judge",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "MT-Bench popularized LLM-as-a-judge for open-ended evaluation, and LiveBench explicitly addresses the bias and instability of such judging by requiring objective, automatic scoring against ground-truths."
    },
    {
      "title": "AlpacaEval: An Automatic Evaluator for Instruction-Following Models",
      "authors": "Yann Dubois et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "AlpacaEval\u2019s reliance on GPT-4-as-judge exposed systemic judge preferences and self-biases; LiveBench is designed to avoid these pitfalls by eliminating LLM/human judges in favor of objective, programmatic scoring."
    },
    {
      "title": "Evaluating Large Language Models Trained on Code",
      "authors": "Mark Chen et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "HumanEval introduced execution-based, unit-test scoring for code generation; LiveBench generalizes this principle to coding and other tasks where exact, automated verification is possible."
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems",
      "authors": "Karl Cobbe et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "GSM8K\u2019s numeric final-answer evaluation for math established a simple, objective scoring template that LiveBench extends to harder, fresher math tasks sourced from recent competitions."
    },
    {
      "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "MATH framed competition-style problems with exact answers for auto-grading; LiveBench extends this idea by drawing from newly released competitions to ensure recency and reduce contamination risk."
    },
    {
      "title": "IFEval: Instruction Following Evaluation for Large Language Models",
      "authors": "Yao Zhou et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "IFEval\u2019s programmatic checks for instruction adherence inform LiveBench\u2019s emphasis on automatic, rule-based scoring for instruction-following tasks without subjective judges."
    }
  ],
  "synthesis_narrative": "LiveBench synthesizes three key lines of prior work to deliver a contamination-limited, judge-free, multi-domain benchmark. First, it responds to the dominance and shortcomings of static, general-purpose leaderboards exemplified by MMLU, whose widespread use and age have made it vulnerable to training-data contamination. Second, it builds on the idea of objective, automatic scoring pioneered in HumanEval (unit-test execution for code) and extended to math by GSM8K and MATH (exact final answers). LiveBench generalizes these auto-grading paradigms, scaling them across math, coding, reasoning, language, instruction following, and data analysis while carefully curating tasks so that each has a verifiable ground truth. Third, it explicitly addresses the biases and instability of LLM- and human-judged evaluation frameworks that became popular with MT-Bench and AlpacaEval by eliminating subjective judging altogether. Where earlier math and coding datasets provided objective scoring but were static (and thus susceptible to leakage), LiveBench adopts their exact-answer and execution-based verifiers yet sources fresh, frequently updated items (e.g., recent competitions) to limit contamination. Similarly, where instruction-following evaluation like IFEval demonstrated programmatic checks, LiveBench extends this approach across more task families. The result is a challenging, automatically scored, and regularly refreshed benchmark that directly tackles contamination and judge bias while retaining breadth comparable to legacy benchmarks.",
  "analysis_timestamp": "2026-01-06T23:08:23.931085"
}
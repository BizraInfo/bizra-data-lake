{
  "prior_works": [
    {
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "authors": "Shokri et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized membership inference as a hypothesis-testing problem and introduced shadow/reference models, providing the problem formulation and basic toolkit (confidence-based testing with reference models) that RMIA explicitly builds on and refines."
    },
    {
      "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
      "authors": "Yeom et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Yeom et al. proposed simple, low-cost threshold tests (e.g., loss-based) that became standard baselines; RMIA replaces these coarse tests with a principled likelihood-ratio test with a finely modeled null, yielding uniformly higher power\u2014especially at very low FPR."
    },
    {
      "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses",
      "authors": "Salem et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "ML-Leaks demonstrated low-cost black-box MIAs using few or even a single shadow model but with degraded accuracy, highlighting the gap RMIA addresses by achieving high-power attacks in the same low-resource regime."
    },
    {
      "title": "Auditing Differentially Private Machine Learning: How Private is Private Learning?",
      "authors": "Jagielski et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "This work pioneered privacy auditing via statistical hypothesis tests using many reference models to estimate member/non-member distributions; RMIA adopts this auditing viewpoint but innovates on fine-grained null modeling and drastically reduces the number of required reference models."
    },
    {
      "title": "Membership Inference Attacks from First Principles",
      "authors": "Carlini et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "Carlini et al. introduced LiRA, a likelihood-ratio framework that leverages reference models and population data; RMIA directly extends this framework with a more precise null-hypothesis model to retain or improve power when only a few (even one) reference models are available and at extremely low FPR."
    },
    {
      "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-Box Inference Attacks",
      "authors": "Nasr et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "While focusing on stronger white-box settings, this paper established rigorous evaluation practices and baselines for MIAs; RMIA targets the harder black-box, low-compute setting while benchmarking against and improving over the families of threshold-based attacks highlighted there."
    }
  ],
  "synthesis_narrative": "RMIA\u2019s core innovation\u2014an efficient, high-power likelihood-ratio test (LRT) with a finely modeled null that remains effective with very few reference models\u2014emerges from a clear intellectual lineage. Shokri et al. (2017) established membership inference as a hypothesis-testing task and introduced shadow models, defining the problem and the reference-model paradigm that RMIA adopts. Yeom et al. (2018) provided simple threshold-based tests that became the de facto low-cost baselines; RMIA\u2019s LRT directly subsumes these heuristics, particularly improving performance at low false positive rates. Salem et al. (2019) explicitly targeted the low-cost regime by reducing the number of shadow models but suffered notable accuracy loss\u2014precisely the gap RMIA closes by retaining high test power with as few as one reference model. Jagielski et al. (2020) catalyzed the \u201cauditing\u201d perspective: estimate member and non-member score distributions using many reference trainings and apply statistical tests; RMIA keeps this statistical rigor while innovating on the null modeling to cut the computational burden dramatically. Finally, Carlini et al. (2022) (LiRA) crystallized LRT-based MIAs that leverage both reference models and population data; RMIA directly extends LiRA\u2019s framework with a more granular null and robust estimation procedures that preserve power across the entire TPR\u2013FPR curve, including the extremely low-FPR regime. Together, these works form the direct scaffold that RMIA refines to deliver practical, low-cost, high-power membership inference.",
  "analysis_timestamp": "2026-01-06T23:09:26.459245"
}
{
  "prior_works": [
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Itay Soudry, Elad Hoffer, Mor Nacson, Suriya Gunasekar, Nathan Srebro",
      "year": 2018,
      "role": "Foundational result on implicit bias of (unnormalized) gradient descent for linear, binary, separable classification with logistic/cross-entropy loss",
      "relationship_sentence": "Established that gradient descent aligns with the L2 max-margin classifier and provided asymptotic direction and margin growth insights that this paper generalizes to multiclass settings and to normalized/momentum steepest-descent dynamics."
    },
    {
      "title": "Mirror Descent and the Implicit Bias of Gradient Descent in Linear Classification",
      "authors": "Suriya Gunasekar, Mor Nacson, Daniel Soudry, Nathan Srebro",
      "year": 2018,
      "role": "Geometric characterization linking steepest descent/mirror descent geometry to norm-induced max-margin solutions",
      "relationship_sentence": "Showed that steepest (normalized) descent in a norm biases toward max-margin solutions in the corresponding dual geometry, directly informing the present paper\u2019s p-norm NSD analysis and its extension to matrix/Schatten norms."
    },
    {
      "title": "Risk and Parameter Convergence of Logistic Regression",
      "authors": "Zhiyuan Ji, Matus Telgarsky",
      "year": 2018,
      "role": "Precise asymptotic analysis and convergence rates for gradient methods on separable logistic regression",
      "relationship_sentence": "Provided techniques and rates for risk/parameter convergence that underpin the convergence-rate guarantees proven here for NSD/NMD in the multiclass cross-entropy setting."
    },
    {
      "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
      "authors": "Kaifeng Lyu, Jian Li",
      "year": 2019,
      "role": "Extended margin maximization perspective to homogeneous models and multiclass cross-entropy",
      "relationship_sentence": "Offered proof ideas for margin maximization with softmax losses that the present work adapts to fully characterize multiclass linear classifiers under normalized and momentum steepest-descent dynamics."
    },
    {
      "title": "Norm-Based Capacity Control in Neural Networks",
      "authors": "Behnam Neyshabur, Ryota Tomioka, Nathan Srebro",
      "year": 2015,
      "role": "Motivated spectral/Schatten and p-norms as meaningful inductive biases and generalization surrogates",
      "relationship_sentence": "Connected classifier norms (including spectral/Schatten) to generalization, supporting this paper\u2019s norm-specific max-margin characterizations for matrix predictors and explaining why spectral-descent/Muon cases are especially salient."
    },
    {
      "title": "Implicit Bias of Gradient Descent for Linear Convolutional Networks",
      "authors": "Suriya Gunasekar, Jason D. Lee, Daniel Soudry, Nathan Srebro",
      "year": 2018,
      "role": "Showed optimization geometry and parameterization determine which norm or margin is implicitly optimized",
      "relationship_sentence": "Demonstrated that different first-order dynamics can target different norm-based solutions, motivating this paper\u2019s unified treatment of p-norm NSD/NMD and its reduction from Schatten p-norms to max-norm analysis via ordering properties."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014fully characterizing the implicit bias of p-norm normalized steepest descent (NSD) and its momentum variant (NMD) for multiclass linear classification\u2014builds on a progression of results that linked optimization dynamics to margin maximization. Soudry et al. (2018) first showed that, for separable data under logistic loss, standard gradient descent converges in direction to the L2 max-margin classifier, establishing the margin-centric lens and asymptotic tools. Gunasekar et al. (2018) then framed implicit bias through optimization geometry, showing that steepest/mirror descent aligns with max-margin solutions in the dual of the chosen geometry, directly suggesting that p-norm NSD should select p-norm\u2013induced margins. Ji and Telgarsky (2018) supplied sharp convergence-rate analyses for logistic regression, techniques that this paper adapts to deliver explicit rates for NSD/NMD. Lyu and Li (2019) extended margin maximization to homogeneous models and multiclass softmax, informing how to transport the binary theory to multiclass linear predictors. Neyshabur et al. (2015) connected spectral/Schatten and p-norms to generalization, motivating why characterizing implicit bias in these norms is practically meaningful for matrix classifiers. Finally, Gunasekar et al. (2018) on linear convolutional networks highlighted how optimization geometry determines which norm-based solution emerges, reinforcing the paper\u2019s unification across entry-wise and Schatten p-norms. Together, these works set the stage for the present paper\u2019s novel reduction from general p/Schatten norms to max-norm analysis via norm ordering, and its inclusion of momentum\u2014yielding a comprehensive theory that also subsumes Spectral Descent and Muon as spectral-norm max-margin special cases.",
  "analysis_timestamp": "2026-01-07T00:21:33.132110"
}
{
  "prior_works": [
    {
      "title": "Equality of Opportunity in Supervised Learning",
      "authors": "Moritz Hardt et al.",
      "year": 2016,
      "arxiv_id": "1610.02413",
      "role": "Foundation",
      "relationship_sentence": "The unprocessing idea explicitly inverts Hardt et al.\u2019s equalized-odds postprocessing (and its ROC-hull characterization), using it as the canonical frontier onto which disparate methods can be mapped for fair comparison."
    },
    {
      "title": "A Reductions Approach to Fair Classification",
      "authors": "Alekh Agarwal et al.",
      "year": 2018,
      "arxiv_id": "1803.02453",
      "role": "Baseline",
      "relationship_sentence": "As the dominant in-processing baseline that wraps arbitrary base learners under fairness constraints, this method\u2019s dependence on different base models is a key confound that unprocessing neutralizes to enable apples-to-apples comparisons."
    },
    {
      "title": "A Comparative Study of Fairness-Enhancing Interventions in Machine Learning",
      "authors": "Sorelle A. Friedler et al.",
      "year": 2019,
      "arxiv_id": "1802.08688",
      "role": "Gap Identification",
      "relationship_sentence": "This broad empirical study revealed inconsistent evaluation protocols across pre-, in-, and post-processing methods, motivating the need for a principled comparison mechanism that unprocessing provides."
    },
    {
      "title": "Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classifiers without Disparate Mistreatment",
      "authors": "Muhammad Bilal Zafar et al.",
      "year": 2017,
      "arxiv_id": "1610.08452",
      "role": "Gap Identification",
      "relationship_sentence": "By enforcing fairness via convex proxy constraints that relax exact error-rate equalities, this line of work exemplifies the differing relaxation levels that unprocessing standardizes for direct comparison."
    },
    {
      "title": "On Fairness and Calibration",
      "authors": "Geoff Pleiss et al.",
      "year": 2017,
      "arxiv_id": "1709.02012",
      "role": "Related Problem",
      "relationship_sentence": "Their analysis of group-specific thresholding and randomization to trade off calibration and equalized odds clarified the geometry of postprocessing that unprocessing conceptually reverses."
    },
    {
      "title": "The Cost of Fairness in Binary Classification",
      "authors": "Aditya Krishna Menon et al.",
      "year": 2018,
      "arxiv_id": "1705.09066",
      "role": "Inspiration",
      "relationship_sentence": "By characterizing Bayes-optimal fairness\u2013accuracy trade-offs via group-wise threshold frontiers, this work underpins the use of a postprocessing Pareto frontier as the reference set that unprocessing maps methods onto."
    }
  ],
  "synthesis_narrative": "Equalized-odds postprocessing formalized by Hardt, Price, and Srebro establishes that fairness can be achieved by group-specific thresholding and randomization along each group\u2019s ROC, yielding a convex, interpretable frontier of achievable error-rate trade-offs. Menon and Williamson show that Bayes-optimal fairness\u2013accuracy trade-offs for error-rate constraints lie on such threshold frontiers, sharpening the view that postprocessing delineates the relevant Pareto set. Pleiss and colleagues study calibration versus equalized odds, making explicit how group-wise thresholding and randomization navigate trade-offs, thereby clarifying the geometry of postprocessing in practice. On the algorithmic side, Agarwal et al.\u2019s reductions framework became the central in-processing competitor, but its performance depends critically on the choice and capacity of the base learner. Zafar et al. enforce fairness through convex proxies, illustrating how approximate or relaxed constraints can diverge from exact error-rate equalization. Friedler et al.\u2019s large-scale comparison surfaced how heterogeneous pipelines, base models, and constraint relaxations confound empirical claims across fairness interventions. Together these works revealed a need for a principled way to compare methods that use different base models and attain different levels of constraint satisfaction. The present study synthesizes these insights by introducing unprocessing\u2014the conceptual inverse of postprocessing\u2014to map any method\u2019s outcome back onto the canonical ROC-based equalized-odds frontier, aligning base-model capacity and relaxation levels. This enables a clean, large-scale, apples-to-apples evaluation, revealing that the postprocessing Pareto frontier subsumes the performance of competing approaches.",
  "target_paper": {
    "title": "Unprocessing Seven Years of Algorithmic Fairness",
    "authors": "Andr\u00e9 Cruz, Moritz Hardt",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "fairness, algorithmic fairness, social computing, tabular data, meta study",
    "abstract": "Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation.",
    "openreview_id": "jr03SfWsBS",
    "forum_id": "jr03SfWsBS"
  },
  "analysis_timestamp": "2026-01-07T00:26:39.846975"
}
{
  "prior_works": [
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "arxiv_id": "2204.14198",
      "role": "Foundation",
      "relationship_sentence": "Flamingo established that pretraining on naturally interleaved image\u2013text sequences enables multimodal in-context learning while preserving LLM abilities, directly motivating the need for very large interleaved corpora."
    },
    {
      "title": "OBELICS: Open Web-Scale Document-Level Vision\u2013Text Dataset",
      "authors": "Hugo Lauren\u00e7on et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "OBELICS introduced an open pipeline for extracting and filtering document-level, layout-preserving interleaved image\u2013text data from the web, which OmniCorpus scales and diversifies far beyond."
    },
    {
      "title": "mmC4: Multimodal C4",
      "authors": "Zhu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "mmC4 formulated a large-scale interleaved corpus by inserting webpage images into cleaned C4 text documents, serving as a primary public baseline whose limited scale and source diversity OmniCorpus addresses."
    },
    {
      "title": "IDEFICS: An Open-Source Visual Language Model",
      "authors": "Hugo Lauren\u00e7on et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "IDEFICS showed strong gains from training on OBELICS-style interleaved data but explicitly noted the bottleneck of limited public interleaved data, highlighting the need for a much larger, higher-quality corpus."
    },
    {
      "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models",
      "authors": "Anas Awadalla et al.",
      "year": 2023,
      "arxiv_id": "2306.07983",
      "role": "Gap Identification",
      "relationship_sentence": "OpenFlamingo\u2019s gap to proprietary Flamingo was traced largely to the scarcity and scale limits of open interleaved web data (primarily OBELICS/mmC4), directly motivating a dramatically larger interleaved dataset."
    },
    {
      "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Learning",
      "authors": "Krishna Srinivasan et al.",
      "year": 2021,
      "arxiv_id": "2103.01913",
      "role": "Foundation",
      "relationship_sentence": "WIT demonstrated the value of multilingual, document-style associations of multiple images with surrounding text, a formulation OmniCorpus generalizes to broader, noisier web sources at far larger scale."
    }
  ],
  "synthesis_narrative": "Flamingo showed that training on sequences where images and text are interleaved in natural web order yields multimodal in-context learning while preserving strong language abilities, identifying interleaved, document-level data as uniquely valuable. OBELICS operationalized this by extracting layout-preserving image\u2013text documents from the open web with a practical filtering pipeline, offering a first broadly accessible corpus for interleaved pretraining. In parallel, mmC4 proposed constructing interleaved data by inserting on-page images into cleaned C4 documents, creating a scalable but still limited public resource. IDEFICS trained open models directly on these interleaved corpora and emphasized that the principal constraint for further gains is the scarcity and limited diversity of public interleaved datasets. OpenFlamingo reinforced this diagnosis by attributing its gap to proprietary Flamingo largely to the small quantity and constrained coverage of available open interleaved data (chiefly OBELICS and mmC4). Earlier, WIT evidenced the benefits of multilingual, document-style image\u2013text associations, foreshadowing the need for broader linguistic and source diversity. Together, these works defined interleaved web documents as the right pretraining substrate, provided initial open pipelines, and exposed a scale and diversity ceiling that capped model performance. The natural next step was to build a unified, much larger corpus that preserves document structure across vastly more sources and languages\u2014including video-centric sites\u2014while maintaining quality through an efficient data engine, thereby removing the data bottleneck that had limited open multimodal pretraining.",
  "target_paper": {
    "title": "OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text",
    "authors": "Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Pei Chu, Yi Wang, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, Jifeng Dai",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Image-text interleaved dataset",
    "abstract": "Image-text interleaved data, consisting of multiple images and texts arranged in a natural document format, aligns with the presentation paradigm of internet data and closely resembles human reading habits. Recent studies have shown that such data aids multimodal in-context learning and maintains the capabilities of large language models during multimodal fine-tuning. However, the limited scale and diversity of current image-text interleaved data restrict the development of multimodal large language models. In this paper, we introduce OmniCorpus, a 10 billion-scale image-text interleaved dataset. Using an efficient data engine, we filter and extract large-scale high-quality documents, which contain 8.6 billion images and 1,696 billion text tokens. Compared to counterparts (e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while maintaining good data quality; 2) features more diverse sources, including both English and non-English websites as well as video-centric websites",
    "openreview_id": "kwqhn2VuG4",
    "forum_id": "kwqhn2VuG4"
  },
  "analysis_timestamp": "2026-01-06T15:57:08.896844"
}
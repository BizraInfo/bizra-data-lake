{
  "prior_works": [
    {
      "title": "Multi-Task Learning as Multi-Objective Optimization",
      "authors": "Ozan Sener, Vladlen Koltun",
      "year": 2018,
      "role": "Foundational formulation of MTL as MOO and Pareto-stationary optimization (MGDA).",
      "relationship_sentence": "This paper formalized deep MTL as a multi-objective problem and provided Pareto-point solvers (MGDA), setting the objective-level framing that the new method seeks to approximate continuously and efficiently."
    },
    {
      "title": "Pareto Multi-Task Learning",
      "authors": "Lin et al.",
      "year": 2019,
      "role": "Preference-conditioned discrete Pareto solution generation for MTL.",
      "relationship_sentence": "ParetoMTL showed how to obtain different Pareto-optimal trade-offs via preference vectors, motivating the need to move from multiple discrete runs to an efficient continuous manifold covering many trade-offs."
    },
    {
      "title": "Learning the Pareto Front with Hypernetworks",
      "authors": "Hadar A. Navon et al.",
      "year": 2021,
      "role": "Continuous Pareto front approximation by conditioning a hypernetwork on preferences.",
      "relationship_sentence": "Directly precursor to this work\u2019s goal, hypernetworks parameterize a continuous Pareto set; the present paper targets the same objective but replaces heavy hypernetworks with a parameter-efficient low-rank structure for scalability to many tasks."
    },
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen",
      "year": 2021,
      "role": "Parameter-efficient fine-tuning via low-rank weight updates.",
      "relationship_sentence": "The proposed main-network-plus-low-rank-matrices design is directly inspired by LoRA\u2019s idea that low-rank adapters can span rich functional variations with few parameters, enabling an efficient Pareto manifold parameterization."
    },
    {
      "title": "Parameter-Efficient Transfer Learning for NLP",
      "authors": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly",
      "year": 2019,
      "role": "Adapter modules for sharing a frozen backbone with small task-specific additions.",
      "relationship_sentence": "Adapters established the architectural pattern of a shared main network augmented with lightweight modules, a principle this paper adopts to extract shared features while varying trade-offs through compact low-rank components."
    },
    {
      "title": "Model Soups: Averaging weights of multiple fine-tuned models improves accuracy",
      "authors": "Mitchell Wortsman et al.",
      "year": 2022,
      "role": "Evidence that linear combinations in weight space traverse useful solution manifolds.",
      "relationship_sentence": "Model soups support the premise that linear combinations of base networks can trace a low-loss manifold, underpinning this paper\u2019s choice to approximate the Pareto front via combinations around a shared backbone."
    },
    {
      "title": "Orthogonal Gradient Descent for Continual Learning",
      "authors": "Mehrdad Farajtabar et al.",
      "year": 2020,
      "role": "Orthogonality-based regularization to reduce interference between updates.",
      "relationship_sentence": "This work motivates the paper\u2019s orthogonal regularization, encouraging low-rank components to occupy complementary subspaces so task trade-offs are better disentangled and performance scales with many tasks."
    }
  ],
  "synthesis_narrative": "The paper positions multi-task learning squarely within the multi-objective optimization paradigm introduced to deep learning by Sener and Koltun, and refined by ParetoMTL to generate discrete Pareto-optimal solutions from preference vectors. While these methods require multiple solves for different trade-offs, Navon et al.\u2019s Pareto Hypernetworks proposed a continuous parameterization of the Pareto set, revealing the promise\u2014but also the computational cost\u2014of learning an entire Pareto manifold. The present work tackles this scalability bottleneck by importing ideas from parameter-efficient adaptation: Houlsby et al.\u2019s adapters and Hu et al.\u2019s LoRA show that small, low-rank updates layered on a shared backbone can express rich behaviors with dramatically fewer parameters. Complementing this, results on weight-space linearity and model interpolation (Model Soups) provide empirical backing that linear combinations of networks lie on low-loss manifolds, justifying a linear-combination-based Pareto approximation around a main network. Finally, orthogonality-based techniques like Orthogonal Gradient Descent inspire the paper\u2019s orthogonal regularization to reduce interference and promote diversity among low-rank components, which is especially vital when scaling to many tasks. Together, these strands\u2014MOO framing, continuous manifold parameterization, parameter-efficient low-rank adaptation, weight-space linearity, and orthogonality\u2014coalesce into a scalable approach for efficient Pareto manifold learning with strong performance on large-task regimes.",
  "analysis_timestamp": "2026-01-06T23:42:48.055741"
}
{
  "prior_works": [
    {
      "title": "SemEval-2016 Task 6: Detecting Stance in Tweets",
      "authors": "Saif M. Mohammad et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This task formalized stance detection as mapping texts to pro/against/neutral labels toward a specified target, providing the exact problem formulation and evaluation setup that the paper adopts for political discussions."
    },
    {
      "title": "A Retrospective Analysis of the Fake News Challenge Stance-Detection Task",
      "authors": "Andreas Hanselowski et al.",
      "year": 2018,
      "arxiv_id": "1806.05180",
      "role": "Foundation",
      "relationship_sentence": "By dissecting FNC-1\u2019s stance categories and modeling pitfalls, this work codified robust stance evaluation (agree/disagree/discuss/unrelated) and highlighted data imbalance issues that the paper addresses with targeted synthetic generation."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2023,
      "arxiv_id": "2212.10560",
      "role": "Extension",
      "relationship_sentence": "The paper extends Self-Instruct\u2019s generate-then-filter paradigm by prompting LLMs to synthesize stance-labeled utterances and applying in-LLM verification to curate high-quality, label-consistent synthetic training sets."
    },
    {
      "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
      "authors": "Ronen Eldan and Yuanzhi Li",
      "year": 2023,
      "arxiv_id": "2305.07759",
      "role": "Inspiration",
      "relationship_sentence": "TinyStories demonstrated that models trained largely on LLM-generated corpora can be competent, directly inspiring the strategy of training a smaller, reliable stance detector on curated synthetic text."
    },
    {
      "title": "ChatGPT Outperforms Crowdworkers for Text-Annotation Tasks",
      "authors": "Fabrizio Gilardi et al.",
      "year": 2023,
      "arxiv_id": "2303.15056",
      "role": "Foundation",
      "relationship_sentence": "Showing that LLMs can accurately and scalably annotate political texts, this work underpins the paper\u2019s decision to use LLMs offline as high-throughput labelers for stance-focused data synthesis."
    },
    {
      "title": "Self-Training With Noisy Student Improves ImageNet Classification",
      "authors": "Qizhe Xie et al.",
      "year": 2020,
      "arxiv_id": "1911.04252",
      "role": "Inspiration",
      "relationship_sentence": "The teacher\u2013student self-training paradigm directly motivates using an LLM as a teacher to produce (and verify) labels and a smaller supervised stance model as the deployed student."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou et al.",
      "year": 2023,
      "arxiv_id": "2307.15043",
      "role": "Gap Identification",
      "relationship_sentence": "By exposing LLMs\u2019 susceptibility to jailbreak and adversarial prompts, this work motivates the paper\u2019s core design choice to confine LLMs to offline data generation while deploying a traditional stance classifier online."
    }
  ],
  "synthesis_narrative": "SemEval-2016 Task 6 established stance detection as predicting pro, against, or neutral positions relative to explicit targets and provided a clear evaluation setup that later work standardized around. The FNC-1 retrospective further clarified stance taxonomies (agree, disagree, discuss, unrelated) and stressed dataset imbalance and annotation pitfalls that can distort supervised learning. Beyond problem framing, Self-Instruct showed that large language models can generate task-specific labeled examples and self-filter them to produce usable training corpora, while TinyStories proved that coherent, effective small models can be trained predominantly on synthetic language generated by larger models. Complementing these generation insights, Gilardi and colleagues demonstrated that LLMs can annotate political texts at scale and quality comparable to or surpassing crowdworkers, directly relevant to political discourse labeling. Finally, Noisy Student introduced a teacher\u2013student self-training template for leveraging pseudo-labeled data, and concurrent security analyses revealed LLMs\u2019 vulnerability to adversarial prompts and jailbreaks.\nTogether, these strands suggested a natural synthesis: use LLMs as offline teachers to generate and verify stance-labeled utterances tailored to imbalanced targets, then train a compact, reliable stance detector as the deployable student under standard SemEval/FNC-style formulations. This approach exploits LLMs\u2019 generative and annotation strengths while addressing practical risks\u2014bias, inconsistency, and adversarial exposure\u2014by keeping them out of the online inference loop and targeting data gaps identified in prior stance benchmarks.",
  "target_paper": {
    "title": "The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions",
    "authors": "Stefan Sylvius Wagner, Maike Behrendt, Marc Ziegele, Stefan Harmeling",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "large language models, stance detection, data augmentation, active learning, online political discussions",
    "abstract": "Stance detection holds great potential to improve online political discussions through its deployment in discussion platforms for purposes such as content moderation, topic summarisation or to facilitate more balanced discussions. Typically, transformer-based models are employed directly for stance detection, requiring vast amounts of data. However, the wide variety of debate topics in online political discussions makes data collection particularly challenging. LLMs have revived stance detection, but their online deployment in online political discussions faces challenges like inconsistent outputs, biases, and vulnerability to adversarial attacks. We show how LLM-generated synthetic data can improve stance detection for online political discussions by using reliable traditional stance detection models for online deployment, while leveraging the text generation capabilities of LLMs for synthetic data generation in a secure offline environment. To achieve this, (i) we generate synthetic ",
    "openreview_id": "ws5phQki00",
    "forum_id": "ws5phQki00"
  },
  "analysis_timestamp": "2026-01-06T16:55:45.584949"
}
{
  "prior_works": [
    {
      "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
      "authors": "S. M. Ali Eslami; Nicolas Heess; Theophane Weber; et al.",
      "year": 2016,
      "role": "Pioneering object-centric generative model with sequential attention and explain-away competition",
      "relationship_sentence": "Established the object-as-latent paradigm and competitive decomposition that our object-centric regularizers emulate without relying on full-image reconstruction."
    },
    {
      "title": "MONet: Unsupervised Scene Decomposition and Representation",
      "authors": "Christopher P. Burgess; Loic Matthey; Nicholas Watters; Rishabh Kabra; Irina Higgins; Matt Botvinick; Alexander Lerchner",
      "year": 2019,
      "role": "Unsupervised object-centric generative decomposition with attention masks and slot competition",
      "relationship_sentence": "Informed our design of mask/slot exclusivity and compactness regularization applied directly in latent space rather than via reconstruction losses."
    },
    {
      "title": "Slot Attention for Object-Centric Learning",
      "authors": "Francesco Locatello; Dirk Weissenborn; Thomas Unterthiner; Aravindh Mahendran; Georg Heigold; Jakob Uszkoreit; Alexey Dosovitskiy; Thomas Kipf",
      "year": 2020,
      "role": "Differentiable grouping mechanism mapping pixels/patches into a fixed set of object slots",
      "relationship_sentence": "We adopt the slot-based view of objects and complement it with feature-connectivity-driven grouping and slot-wise regularizers to scale beyond reconstruction-trained Slot Attention."
    },
    {
      "title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations",
      "authors": "Michael Engelcke; Adam R. Kosiorek; Oiwi Parker Jones; Ingmar Posner",
      "year": 2020,
      "role": "Generative object-centric model with layered compositing and autoregressive masks",
      "relationship_sentence": "Serves as the generative baseline our method departs from, motivating non-reconstruction learning and object-centric regularization for better scalability."
    },
    {
      "title": "Emerging Properties in Self-Supervised Vision Transformers (DINO)",
      "authors": "Mathilde Caron; Hugo Touvron; Ishan Misra; Herv\u00e9 J\u00e9gou; Julien Mairal; Piotr Bojanowski; Armand Joulin",
      "year": 2021,
      "role": "Showed self-supervised ViT features exhibit object-centric attention and strong spatial coherence",
      "relationship_sentence": "Motivated leveraging local feature connectivity to cluster neighboring pixels into objects without labels or reconstruction."
    },
    {
      "title": "Normalized Cuts and Image Segmentation",
      "authors": "Jianbo Shi; Jitendra Malik",
      "year": 2000,
      "role": "Classic graph-partitioning formulation segmenting images by affinities between pixels/patches",
      "relationship_sentence": "Inspired our feature-connectivity clustering objective that increases intra-object affinity while discouraging inter-object connections."
    },
    {
      "title": "Efficient Graph-Based Image Segmentation",
      "authors": "Pedro F. Felzenszwalb; Daniel P. Huttenlocher",
      "year": 2004,
      "role": "Fast region-merging segmentation based on local edge weights and internal variation",
      "relationship_sentence": "Informed the design of a scalable connectivity criterion for merging neighboring pixel groups into object hypotheses in our pipeline."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014replacing reconstruction-heavy generative training with a scalable, feature-connectivity grouping mechanism plus object-centric latent regularization\u2014emerges from two converging lines of work. First, the object-centric generative tradition (AIR; MONet; GENESIS) established the \"object-as-latent\" perspective and competitive, mask-based decomposition, showing that slots and explain-away competition yield disentangled object representations. However, their reliance on pixel-level reconstruction limits scalability on complex, real-world imagery. Slot Attention later provided a powerful differentiable grouping mechanism for mapping pixels or patches into a fixed set of slots, but typical training still leaned on reconstruction objectives.\nSecond, developments in self-supervised vision and classical segmentation made non-generative grouping viable. DINO revealed that self-supervised ViT features carry emergent objectness and spatial coherence, suggesting that simple affinity structures among neighboring features can surface object boundaries. Classical connectivity-based segmentation (Normalized Cuts; Felzenszwalb-Huttenlocher) supplies principled, scalable graph formulations for clustering by affinity while discouraging spurious cross-object links.\nSynthesizing these threads, the present work retains the slot-based object view but swaps reconstruction for a connectivity-driven clustering of neighboring pixel features, then reinforces object quality with object-centric regularizers (e.g., encouraging slot exclusivity and compactness) directly in latent space. This combination leverages strong self-supervised feature geometry and efficient graph-based grouping to achieve robust, sample-efficient, and scalable object discovery on real-world images, surpassing generative baselines constrained by reconstruction.",
  "analysis_timestamp": "2026-01-06T23:42:49.101012"
}
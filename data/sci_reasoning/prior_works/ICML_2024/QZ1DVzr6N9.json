{
  "prior_works": [
    {
      "title": "Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition",
      "authors": "Gurjeet Singh et al.",
      "year": 2007,
      "role": "Baseline",
      "relationship_sentence": "Introduces the Mapper construction (filter, cover/pullback, clustering) that this paper makes differentiable and optimizes by learning the filter."
    },
    {
      "title": "Structure and Stability of the 1-Dimensional Mapper",
      "authors": "Mathieu Carri\u00e8re et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Provides the extended-persistence-based signature of Mapper (quantifying components, branches, loops) that is directly used to define the topological objective guiding filter optimization."
    },
    {
      "title": "Multiscale Mapper: A Framework for Topological Summarization of Data and Maps",
      "authors": "Tamal K. Dey et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "Addresses tuning of cover parameters (resolution/gain) and stability but leaves the choice of the filter manual\u2014precisely the missing piece this work targets by optimizing the filter."
    },
    {
      "title": "Topological Function Optimization for Continuous Shapes",
      "authors": "Nicolas Poulenard et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "Introduces differentiable optimization of scalar functions using gradients from persistence diagrams; this idea is adapted to learn the Mapper filter via a topological loss."
    },
    {
      "title": "Deep Learning with Topological Signatures",
      "authors": "Christoph Hofer et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates backpropagation through persistence-based vectorizations to train models with topological objectives, enabling the differentiable topological losses underpinning this paper\u2019s optimization."
    },
    {
      "title": "Topological Autoencoders",
      "authors": "Robin Moor et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Shows unsupervised representation learning via persistent-homology losses; this work adopts the same paradigm to optimize an unsupervised topological objective, but for learning the Mapper filter."
    },
    {
      "title": "Extending Persistence Using Poincar\u00e9 Duality",
      "authors": "David Cohen-Steiner et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "Establishes extended persistence for capturing features like branches and loops, which is leveraged to score and optimize the topology of the Mapper graph."
    }
  ],
  "synthesis_narrative": "The core innovation of Differentiable Mapper is to transform the classic Mapper pipeline into a differentiable, trainable procedure that learns the filter function by directly optimizing a topological objective. This lineage begins with the original Mapper construction by Singh et al., which defines the filter\u2013cover\u2013clustering pipeline and the combinatorial graph whose structures reflect data topology. Carri\u00e8re and Oudot later provided a rigorous framework for analyzing Mapper via extended persistence, yielding signatures that quantify components, branches, and loops; these signatures supply the precise topological quantities this work optimizes. While Dey et al.\u2019s Multiscale Mapper systematically tackled cover parameters and stability, it left the choice of filter function to manual tuning, exposing the key practical gap this paper addresses. The ability to optimize the filter hinges on differentiable topology: Poulenard et al. showed how to compute gradients of persistence-based losses with respect to function values, directly inspiring the optimization of Mapper\u2019s filter as a learnable scalar function. Complementary developments in deep learning with TDA, such as Hofer et al.\u2019s differentiable topological signatures and the unsupervised persistence-driven objectives of Topological Autoencoders, demonstrate the feasibility and value of topological losses for representation learning. Finally, the foundational extended persistence theory of Cohen-Steiner et al. underlies the quantification of loop and branch significance used to drive the optimization. Together, these works enable a principled, differentiable Mapper whose filter is learned to maximize topological salience in an unsupervised manner.",
  "analysis_timestamp": "2026-01-06T23:09:26.416675"
}
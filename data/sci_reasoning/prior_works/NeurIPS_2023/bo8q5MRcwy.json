{
  "prior_works": [
    {
      "title": "Deep Visual Foresight for Planning Robot Motion",
      "authors": "Chelsea Finn, Sergey Levine",
      "year": 2017,
      "role": "Video prediction for model-based control",
      "relationship_sentence": "Introduced planning by predicting future image frames and using them to guide control, a direct precursor to representing policies as generated videos and extracting actions from predicted visual rollouts."
    },
    {
      "title": "Dream to Control: Learning Behaviors by Latent Imagination (Dreamer)",
      "authors": "Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba",
      "year": 2020,
      "role": "World models and imagined rollouts for sequential decision-making",
      "relationship_sentence": "Showed that a learned generative model can plan via imagination; the present work adopts the same imagination-for-control paradigm but performs rollouts as text-conditioned videos instead of latent vectors."
    },
    {
      "title": "Diffuser: Diffusion Models for Planning",
      "authors": "Michael Janner et al.",
      "year": 2022,
      "role": "Generative diffusion modeling for trajectory synthesis and planning",
      "relationship_sentence": "Demonstrated diffusion sampling as a flexible planner over trajectories; this paper extends the idea to image-space by generating future video plans and then inferring controls, rather than sampling state-action sequences directly."
    },
    {
      "title": "Latent Diffusion Models",
      "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
      "year": 2022,
      "role": "Scalable text-guided generation via latent diffusion",
      "relationship_sentence": "Provided the scalable, compositional text-conditioning mechanism that underpins using language as a goal specifier; the paper leverages similar conditioning to guide video plans that generalize across tasks and domains."
    },
    {
      "title": "Imagen Video: High Definition Video Generation with Diffusion Models",
      "authors": "Jonathan Ho et al.",
      "year": 2022,
      "role": "Text-to-video diffusion for temporally coherent generation",
      "relationship_sentence": "Established that text can reliably condition high-fidelity, coherent video; the method adapts this capability to synthesize future frames depicting planned actions from language goals."
    },
    {
      "title": "Universal Value Function Approximators",
      "authors": "Tom Schaul, Daniel Horgan, Karol Gregor, David Silver",
      "year": 2015,
      "role": "Goal-conditioned policies/values enabling universal policies",
      "relationship_sentence": "Provided the conceptual foundation for conditioning behavior on goal embeddings; this work instantiates universal policies by conditioning a video planner on text goals and unifying disparate MDPs in image space."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Michael Ahn et al.",
      "year": 2022,
      "role": "Language-conditioned control and compositional generalization",
      "relationship_sentence": "Showed that natural language can specify and generalize tasks for control; the paper builds on this by using language as the goal interface for a planner that generates visual plans executable across environments."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014casting sequential decision-making as text-conditioned video generation and extracting controls from the generated rollout\u2014emerges from a convergence of advances in goal-conditioned control, world-model planning, and text-guided generative modeling. Universal Value Function Approximators established the principle of conditioning behavior on goal embeddings, seeding the notion of a single policy family that generalizes across tasks. Subsequent language-conditioned control, epitomized by SayCan, demonstrated that natural language is an effective, compositional interface for specifying diverse goals. On the planning side, Visual Foresight pioneered predicting future image frames for control, directly foreshadowing the idea of representing plans as videos. Dreamer strengthened this thread by showing that imagined rollouts in a learned world model can drive policy learning and control, a paradigm this paper retains while shifting the imagination space to pixel-level video conditioned by text. Generative modeling for planning via diffusion (Diffuser) provided a powerful sampling-based planner; the current work extends the diffusion-planning insight from trajectory space to video space, enabling cross-environment unification through images. Finally, breakthroughs in text-guided generation\u2014Latent Diffusion for scalable conditioning and Imagen Video for coherent text-to-video synthesis\u2014supplied the practical mechanisms and compositional generalization properties that make text-specified visual planning feasible. Together, these works directly enabled a universal, language-conditioned policy that plans in a unified visual space and executes by mapping predicted video futures to actions.",
  "analysis_timestamp": "2026-01-06T23:42:49.082097"
}
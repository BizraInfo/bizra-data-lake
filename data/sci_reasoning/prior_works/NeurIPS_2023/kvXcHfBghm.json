{
  "prior_works": [
    {
      "title": "A New Vector Partition of the Probability Score",
      "authors": "Allan H. Murphy",
      "year": 1973,
      "role": "conceptual foundation",
      "relationship_sentence": "The paper\u2019s minimum-risk recalibration framework explicitly builds on Murphy\u2019s Brier score (MSE) decomposition into reliability (calibration), resolution (sharpness), and uncertainty, providing the core risk decomposition that the authors optimize and bound."
    },
    {
      "title": "Probabilistic forecasts, calibration and sharpness",
      "authors": "Tilmann Gneiting, Fadoua Balabdaoui, Adrian E. Raftery",
      "year": 2007,
      "role": "conceptual foundation",
      "relationship_sentence": "Gneiting et al.\u2019s principle of maximizing sharpness subject to calibration under proper scoring rules directly motivates the paper\u2019s objective of balancing calibration and sharpness within an MSE-based risk, which their theory operationalizes and quantifies."
    },
    {
      "title": "Obtaining Calibrated Probability Estimates from Decision Trees and Naive Bayesian Classifiers",
      "authors": "Bianca Zadrozny, Charles Elkan",
      "year": 2001,
      "role": "algorithmic antecedent",
      "relationship_sentence": "This early work on post-hoc recalibration (including isotonic/logistic approaches and bin-based reliability analysis) established the practical binning-style recalibrators that the paper formalizes as uniform-mass binning (UMB) and analyzes with finite-sample risk bounds."
    },
    {
      "title": "Obtaining Well Calibrated Probabilities Using Bayesian Binning into Quantiles",
      "authors": "Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht",
      "year": 2015,
      "role": "algorithmic antecedent",
      "relationship_sentence": "BBQ introduced quantile (uniform-mass) binning for calibration; the present paper studies the uniform-mass binning scheme in a principled MSE-risk framework and derives finite-sample rates and optimal bin scaling for this class of methods."
    },
    {
      "title": "On Calibration of Modern Neural Networks",
      "authors": "Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger",
      "year": 2017,
      "role": "empirical catalyst",
      "relationship_sentence": "By documenting miscalibration in modern networks and standardizing bin-based metrics (e.g., ECE) and post-hoc recalibrators, this work motivates rigorous analysis of binning-based recalibration; the paper provides such theory via MSE-risk bounds for UMB."
    },
    {
      "title": "Verified Uncertainty Calibration",
      "authors": "Aviral Kumar, Sunita Sarawagi, Yaoqiang Yang, Percy Liang, Tengyu Ma",
      "year": 2019,
      "role": "theoretical tool",
      "relationship_sentence": "Providing finite-sample guarantees and concentration analyses for calibration error and motivating adaptive/quantile binning, this work informs the paper\u2019s focus on finite-sample risk bounds and the statistical benefits of uniform-mass binning."
    },
    {
      "title": "Detecting and Correcting for Label Shift with Black Box Predictors",
      "authors": "Zachary C. Lipton, Yu-Xiang Wang, Alexander J. Smola",
      "year": 2018,
      "role": "problem framing",
      "relationship_sentence": "The label-shift framework underpins extensions of recalibration under distribution shift; the paper\u2019s treatment of \u2018label \u2026\u2019 shift leverages this setting to adapt minimum-risk recalibration beyond i.i.d. assumptions."
    }
  ],
  "synthesis_narrative": "Sun, Song, and Hero ground their contribution in the Brier/MSE decomposition due to Murphy, which cleanly separates risk into calibration (reliability) and sharpness (resolution). Gneiting, Balabdaoui, and Raftery\u2019s dictum\u2014maximize sharpness subject to calibration\u2014provides the guiding principle the authors make operational: they define minimum-risk recalibration within a proper-scoring-rule (MSE) framework that explicitly balances these two aspects. Historically, post-hoc recalibration relied on simple mappings of scores to probabilities, as in Zadrozny and Elkan\u2019s isotonic/logistic approaches and bin-based reliability analyses. Quantile (uniform-mass) binning became a practical, high-performing variant with Bayesian Binning into Quantiles (Naeini et al.), directly antecedent to the uniform-mass binning (UMB) mechanism analyzed here. The modern resurgence of interest in calibration\u2014sparked by Guo et al.\u2019s findings on neural networks\u2014cemented binning-based practices (ECE, histogram/quantile binning), underscoring the need for principled guarantees. Building on recent statistical work that provides finite-sample control of calibration errors and advocates adaptive/quantile binning (Kumar et al.), the authors derive a concrete finite-sample risk upper bound for UMB of order O~(B/n + 1/B^2). This yields the optimal bin choice B \u221d n^{1/3} and a risk rate near n^{-2/3}, articulating the bias\u2013variance trade-off for recalibration through the lens of MSE decomposition. Finally, by situating recalibration under label shift, informed by the black-box label-shift framework of Lipton et al., the paper extends minimum-risk recalibration beyond the i.i.d. case, aligning practical distribution-shift concerns with a rigorous risk-minimization theory.",
  "analysis_timestamp": "2026-01-07T00:02:04.806070"
}
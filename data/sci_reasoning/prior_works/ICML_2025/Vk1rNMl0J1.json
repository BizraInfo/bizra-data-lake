{
  "prior_works": [
    {
      "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks",
      "authors": "Suchin Gururangan et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "This paper introduced continual/domain-adaptive pretraining (DAPT/TAPT) for LMs and established the CPT problem setting that the present work formalizes with a predictive scaling law."
    },
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "It established power-law relationships between loss and compute/scale, providing the empirical scaling-law template that the current work generalizes to CPT by adding explicit terms for distribution shift and learning-rate annealing."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Chinchilla refined LM scaling laws but focused on monolithic pretraining without modeling domain shift or LR schedules, a limitation the present paper addresses by deriving a CPT-specific scaling law across schedules."
    },
    {
      "title": "Scaling Laws for Transfer",
      "authors": "Danny Hernandez et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "By showing that transfer/fine-tuning behavior admits predictable scaling, this work directly inspired modeling CPT as a predictable transition and motivated incorporating a distribution-shift factor into the loss law."
    },
    {
      "title": "Intelligent Selection of Language Model Training Data",
      "authors": "Robert C. Moore et al.",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "The cross-entropy\u2013based domain distance from Moore\u2013Lewis underpins the paper\u2019s explicit treatment of PT\u2192CPT distribution shift as a measurable factor in the loss dynamics."
    },
    {
      "title": "Speeding up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves",
      "authors": "Jonas Domhan et al.",
      "year": 2015,
      "role": "Related Problem",
      "relationship_sentence": "Early learning-curve extrapolation methods motivated the paper\u2019s goal of predicting validation loss over training steps, which it advances by a principled decoupling of LR annealing and distribution shift."
    },
    {
      "title": "Stochastic Gradient Descent with Warm Restarts",
      "authors": "Ilya Loshchilov et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "Cosine annealing (SGDR) is a canonical LR schedule; the present work\u2019s theory explicitly accounts for LR annealing and predicts loss across differing schedules including cosine, step, and linear decay."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014a CPT scaling law that predicts validation loss across training steps and learning-rate schedules by decoupling distribution shift and LR annealing\u2014emerges at the intersection of three threads. First, Gururangan et al. formalized continual/domain-adaptive pretraining for language models, defining the exact PT\u2192CPT setup this work studies. Second, neural scaling law research (Kaplan et al.) and its refinement (Hoffmann et al.) established power-law regularities of loss with respect to data, parameters, and compute, but confined their scope to monolithic pretraining. Hernandez et al. extended scaling notions to transfer, demonstrating that post-pretraining adaptation can be predictable, which directly inspired treating CPT as a governed transition rather than an ad hoc procedure. Third, the paper operationalizes domain shift using Moore\u2013Lewis\u2019s cross-entropy\u2013based distance, making distribution mismatch a measurable variable that can be inserted into a loss law. Complementing these, classical learning-curve extrapolation (Domhan et al.) motivates predicting future validation loss from early trajectories, while LR-schedule research (SGDR) highlights annealing as a dominant, structured driver of learning dynamics. By integrating these strands, the present work formulates CPT loss as a transition between curves governed jointly by measurable domain distance and explicit LR annealing, thereby closing the gap left by prior scaling laws that omitted both continual adaptation and schedule effects.",
  "analysis_timestamp": "2026-01-06T23:07:19.594128"
}
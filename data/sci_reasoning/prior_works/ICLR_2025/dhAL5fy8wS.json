{
  "prior_works": [
    {
      "title": "The Mathematical Theory of Optimal Processes",
      "authors": "Lev S. Pontryagin et al.",
      "year": 1962,
      "role": "Foundation",
      "relationship_sentence": "PDS\u2019s core step\u2014deriving necessary conditions for optimal data selection via Hamiltonian maximization and co-state dynamics\u2014directly applies Pontryagin\u2019s Maximum Principle, without which the paper\u2019s central control-theoretic formulation would not exist."
    },
    {
      "title": "Neural Ordinary Differential Equations",
      "authors": "Ricky T. Q. Chen et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "By modeling network training as continuous-time dynamics, Neural ODEs provide the bridge that enables applying PMP to learning trajectories; PDS explicitly leverages this continuous-time view to relate selection policies to LM training dynamics."
    },
    {
      "title": "Deep Neural Networks motivated by Partial Differential Equations",
      "authors": "Lars Ruthotto et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "This work frames deep networks as dynamical systems/optimal control problems, directly inspiring PDS\u2019s control-theoretic treatment of data selection and its use of adjoint (co-state) equations to reason about training dynamics."
    },
    {
      "title": "Learning to Reweight Examples for Robust Deep Learning",
      "authors": "Mengye Ren et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Ren et al. introduce bilevel data reweighting via meta-gradients but suffer from scalability and weak coupling to full training dynamics; PDS addresses this gap by deriving principled selection weights from PMP conditions tied to the model\u2019s training trajectory."
    },
    {
      "title": "GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning",
      "authors": "Sai Prasanna Killamsetty et al.",
      "year": 2021,
      "role": "Related Problem",
      "relationship_sentence": "GLISTER selects subsets by estimating validation generalization gains via gradient similarity, and PDS advances this line by grounding selection in control-theoretic adjoint dynamics, yielding selection criteria derived from necessary optimality conditions rather than heuristics."
    },
    {
      "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
      "authors": "Guillaume Wenzek et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "CCNet established heuristic, perplexity-based filtering for CommonCrawl; PDS positions itself as a theoretically grounded alternative and directly compares against CCNet-style filtering when constructing pretraining corpora."
    },
    {
      "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks",
      "authors": "Suchin Gururangan et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "By demonstrating that selectively choosing pretraining data (domain/task-specific) materially improves downstream performance, this work crystallizes the problem PDS formalizes, motivating a principled optimal-control approach to data selection."
    }
  ],
  "synthesis_narrative": "PDS\u2019s central innovation\u2014casting pretraining data selection as an optimal control problem solved via Pontryagin\u2019s Maximum Principle\u2014rests on two pillars: the control-theoretic framework of PMP and the continuous-time view of training dynamics. Pontryagin et al. provide the theoretical foundation for deriving necessary optimality conditions via Hamiltonian maximization and co-state dynamics, while Neural ODEs supply the modeling bridge to treat neural network training trajectories in continuous time. Building on this, the PDE/optimal-control perspective on deep networks by Ruthotto and Haber directly inspires PDS\u2019s use of adjoint equations to link data selection with learning dynamics.\n\nPDS also arises from practical gaps in existing data selection. Bilevel reweighting (Ren et al.) and gradient-based subset selection (GLISTER) aim to improve generalization but are either computationally heavy or only heuristically coupled to full training dynamics. PDS addresses these limitations by replacing meta-gradient heuristics with PMP-derived necessary conditions that directly capture how selected data influence the training trajectory.\n\nFinally, web-scale filtering pipelines such as CCNet and the domain/task-aware pretraining agenda of Gururangan et al. define the empirical problem: selecting better pretraining data improves downstream performance but has lacked a unifying, principled criterion. PDS answers this need with a theoretically grounded, dynamics-aware selection framework that subsumes and surpasses heuristic filtering and meta-learning baselines.",
  "analysis_timestamp": "2026-01-06T23:09:26.644331"
}
{
  "prior_works": [
    {
      "title": "The Secret Sharer: Measuring Unintended Memorization in Neural Networks",
      "authors": "Nicholas Carlini et al.",
      "year": 2019,
      "arxiv_id": "1802.08232",
      "role": "Inspiration",
      "relationship_sentence": "By showing that memorization can be quantified via likelihood-based ranking (exposure) of sequences, this work directly motivates using model-assigned likelihoods as the signal for detecting contamination through ordering preferences."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "arxiv_id": "2012.07805",
      "role": "Gap Identification",
      "relationship_sentence": "This paper demonstrated practical, black-box extraction of verbatim training data, highlighting the need for a principled, provable test for contamination that does not require access to pretraining data or model weights."
    },
    {
      "title": "Data Contamination Can Lead to Overly Optimistic NLP Evaluation",
      "authors": "N. Magar et al.",
      "year": 2022,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "It formalized benchmark contamination as a primary threat to evaluation and measured it via overlap with known corpora, defining the problem while exposing the limitation of requiring access to training data that the new test explicitly removes."
    },
    {
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "authors": "Reza Shokri et al.",
      "year": 2017,
      "arxiv_id": "1610.05820",
      "role": "Baseline",
      "relationship_sentence": "As the canonical black-box auditing baseline that infers example-level training membership without guarantees, it serves as the comparator the new method improves upon by providing dataset-level decisions with exact false-positive control."
    },
    {
      "title": "Permutation, Parametric and Bootstrap Tests",
      "authors": "Phillip I. Good",
      "year": 2005,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "This work provides the randomization/permutation testing framework with exact Type I error control under exchangeability, which is the statistical backbone of comparing canonical versus shuffled order likelihoods."
    },
    {
      "title": "Exchangeability and Related Topics",
      "authors": "David Aldous",
      "year": 1985,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "It formalizes exchangeability, implying all permutations are equally likely under the null, which directly underpins the null hypothesis and validity of the proposed permutation-based contamination test."
    },
    {
      "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models",
      "authors": "Nikhil Kandpal et al.",
      "year": 2022,
      "arxiv_id": "2202.06539",
      "role": "Related Problem",
      "relationship_sentence": "By showing duplication-driven long-span memorization in LMs, this work explains why contaminated models assign higher likelihood to canonical web-scraped orderings, providing the mechanistic rationale the test exploits."
    }
  ],
  "synthesis_narrative": "Memorization can be measured through likelihood-based ranking of rare sequences, as demonstrated by work that introduced the exposure metric and showed that models can assign disproportionately high probability to specific strings when they have memorized them. Subsequent evidence made this concrete in practice by extracting verbatim training sequences from large language models in a black-box setting, highlighting real-world risks of unintended copying. The broader evaluation community meanwhile identified benchmark contamination as a central threat to validity, typically detecting it by intersecting benchmarks with known pretraining corpora\u2014an approach that presumes access to the training data. Independently, membership inference established a black-box auditing paradigm but focused on instance-level membership with heuristic scores and without exact error guarantees. Classical results on permutation (randomization) tests provide a route to exact control of false positives when data are exchangeable, and the theory of exchangeability itself states that, absent contamination, all permutations of an i.i.d. dataset are equally likely. Finally, duplication-driven memorization in web-scale training explains why models may strongly prefer particular canonical orderings seen during pretraining. Together, these strands reveal an opportunity: use model likelihoods as the memorization signal, exploit exchangeability to form a null hypothesis where all orderings are equally likely, and apply a permutation-style test to compare canonical versus shuffled orders. This synthesis yields a black-box procedure with exact false-positive guarantees that detects contamination without access to pretraining data, addressing the key limitations of overlap-based scans and membership inference.",
  "target_paper": {
    "title": "Proving Test Set Contamination in Black-Box Language Models",
    "authors": "Yonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, Tatsunori Hashimoto",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "language modeling, memorization, dataset contamination",
    "abstract": "Large language models are trained on vast amounts of internet data, prompting concerns that they have memorized public benchmarks. Detecting this type of contamination is challenging because the pretraining data used by proprietary models are often not publicly accessible.\n\nWe propose a procedure for detecting test set contamination of language models with exact false positive guarantees and without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples.\n\nWe demonstrate that our procedure is sens",
    "openreview_id": "KS8mIvetg2",
    "forum_id": "KS8mIvetg2"
  },
  "analysis_timestamp": "2026-01-06T06:08:43.135037"
}
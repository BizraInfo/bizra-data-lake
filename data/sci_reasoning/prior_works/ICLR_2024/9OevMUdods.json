{
  "prior_works": [
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Fabio Petroni et al.",
      "year": 2019,
      "arxiv_id": "1909.01066",
      "role": "Foundation",
      "relationship_sentence": "It established the paradigm of probing parametric factual knowledge in pretrained LMs, which Pinocchio generalizes from relation cloze tests to a broad, question-answering benchmark spanning sources, time, and compositionality."
    },
    {
      "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?",
      "authors": "Adam Roberts et al.",
      "year": 2020,
      "arxiv_id": "2002.08910",
      "role": "Foundation",
      "relationship_sentence": "It formalized closed-book question answering as a direct test of knowledge stored in model parameters, the evaluation setup Pinocchio adopts to measure factual recall without retrieval."
    },
    {
      "title": "X-FACTR: Multilingual Factual Knowledge Probing of Pretrained Language Models",
      "authors": "Zhengbao Jiang et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "By extending factual probing to many languages via Wikidata relations, it directly motivated Pinocchio\u2019s multilingual and region-specific design to assess cross-lingual factual knowledge."
    },
    {
      "title": "TimeQA: A Question Answering Benchmark with Temporal Reasoning",
      "authors": "Jiaqi Chen et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "It introduced temporally grounded QA with changing truths, whose evidence-centric setup Pinocchio adapts to the parametric setting to test whether LLMs update and track facts over time."
    },
    {
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "authors": "Stephanie Lin et al.",
      "year": 2021,
      "arxiv_id": "2109.07958",
      "role": "Gap Identification",
      "relationship_sentence": "By showing LMs often reproduce common misconceptions, it highlighted the need for a factuality benchmark that stresses obsolete and conflicting facts, a gap Pinocchio explicitly targets."
    },
    {
      "title": "PopQA: A Popularity-Biased Benchmark for Question Answering",
      "authors": "Andrew Mallen et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "It demonstrated that models struggle on long-tail facts compared to popular ones, driving Pinocchio\u2019s deliberate coverage across popularity and diverse sources to probe knowledge breadth."
    },
    {
      "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
      "authors": "Zhilin Yang et al.",
      "year": 2018,
      "arxiv_id": "1809.09600",
      "role": "Related Problem",
      "relationship_sentence": "Its multi-hop composition of supporting facts informed Pinocchio\u2019s compositional queries that require combining multiple parametric facts without external evidence."
    }
  ],
  "synthesis_narrative": "Probing language models for parametric factual knowledge began with the insight that pretrained models implicitly store relations between entities, as shown by cloze-based evaluations in Language Models as Knowledge Bases?. Closed-book QA then provided a clean operationalization of parametric knowledge measurement\u2014querying models without retrieval\u2014demonstrated at scale by How Much Knowledge Can You Pack into the Parameters of a Language Model?. Multilingual probing extended this lens beyond English via X-FACTR, revealing cross-lingual variability in stored facts tied to Wikidata relations. Temporal benchmarks such as TimeQA emphasized that truth can change and that evaluating knowledge requires time-aware queries and answers. TruthfulQA exposed a distinct failure mode: models often echo popular falsehoods, underscoring the need to assess truthfulness against misconceptions and conflicts. PopQA further showed that models are biased toward popular facts and falter on the long tail, indicating coverage and source diversity are crucial. Finally, multi-hop datasets like HotpotQA crystallized the need to assess composition\u2014reasoning that combines multiple facts\u2014to answer more complex questions. Taken together, these works reveal a fragmented landscape: probing is often monolingual, relation-template bound, snapshot-in-time, or evidence-retrieval dependent, and rarely tests compositionality and long-tail coverage together. Pinocchio naturally synthesizes these insights by adopting closed-book probing while explicitly spanning languages, regions, sources, and time, and by including compositional and multi-fact queries to map the scope and limits of factual knowledge stored in LLMs.",
  "target_paper": {
    "title": "Towards Understanding Factual Knowledge of Large Language Models",
    "authors": "Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, Zhijiang Guo",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Large Language Models, Resource and Evaluation, Interpretability, NLP Application",
    "abstract": "Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to explore the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs can compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identif",
    "openreview_id": "9OevMUdods",
    "forum_id": "9OevMUdods"
  },
  "analysis_timestamp": "2026-01-06T09:02:37.402354"
}
{
  "prior_works": [
    {
      "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "authors": "Yash Goyal et al.",
      "year": 2017,
      "arxiv_id": "1612.00837",
      "role": "Gap Identification",
      "relationship_sentence": "VQAv2\u2019s open-ended evaluation with brittle string/consensus matching and known dataset biases directly motivate our benchmark\u2019s shift to classification-derived questions and hierarchy-aware grading to obtain more granular, reliable assessment."
    },
    {
      "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
      "authors": "Mateusz Malinowski et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "This work introduced WUPS, a WordNet-based semantic similarity metric for VQA, inspiring our taxonomy-driven follow-up questioning that operationalizes partial credit for coarse-but-semantic answers."
    },
    {
      "title": "ImageNet: A Large-Scale Hierarchical Image Database",
      "authors": "Jia Deng et al.",
      "year": 2009,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "ImageNet\u2019s alignment to the WordNet hierarchy provides the semantic tree we exploit to generate VQA prompts and ancestor-based follow-up questions for fine-to-coarse evaluation."
    },
    {
      "title": "Learning Multiple Layers of Features from Tiny Images (CIFAR-100)",
      "authors": "Alex Krizhevsky et al.",
      "year": 2009,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "CIFAR-100\u2019s explicit coarse/fine label structure underpins our coarse-to-fine questioning design and enables principled partial-credit assessment on fine-grained categories."
    },
    {
      "title": "The iNaturalist Species Classification and Detection Dataset",
      "authors": "Grant Van Horn et al.",
      "year": 2018,
      "arxiv_id": "1707.06642",
      "role": "Foundation",
      "relationship_sentence": "The dataset\u2019s real-world taxonomic hierarchy and fine-grained labels are directly leveraged to build open-ended VQA items and lineage-based follow-up queries for granular scoring."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "arxiv_id": "2103.00020",
      "role": "Baseline",
      "relationship_sentence": "CLIP\u2019s zero-shot discriminative classification on text label spaces is the primary baseline our benchmark is designed to compare against generative VLMs on the same taxonomy-derived questions."
    },
    {
      "title": "Visual Instruction Tuning (LLaVA) and LLaVA-Bench",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Related Problem",
      "relationship_sentence": "LLaVA-Bench\u2019s use of GPT-4 as an automatic judge for free-form multimodal answers directly informs our analysis contrasting LLM-based judging with traditional NLP metrics and our human study\u2013validated metric choice."
    }
  ],
  "synthesis_narrative": "Open-ended VQA was solidified by VQAv2, which popularized consensus-based exact matching but exposed weaknesses such as language priors, short-answer bias, and fragile grading that provide little nuance when answers are semantically close but not identical. Earlier, Malinowski and Fritz proposed WUPS, leveraging WordNet similarity to give partial credit for semantically related answers, showing that taxonomy-aware scoring can capture graded correctness. On the vision side, ImageNet\u2019s organization within the WordNet hierarchy established an accessible semantic tree over visual categories, while CIFAR-100 explicitly encoded coarse and fine labels, and iNaturalist extended these ideas to real-world, fine-grained, long-tail taxa\u2014all providing structured label spaces ideal for hierarchical reasoning. In parallel, CLIP demonstrated strong discriminative zero-shot classification over text label spaces, setting a de facto baseline for vision\u2013language alignment on classification datasets but not directly comparable to generative VLMs under standard VQA protocols. Meanwhile, LLaVA introduced GPT-4\u2013based judging for free-form multimodal answers, suggesting a path for automatic evaluation beyond brittle string metrics. Together, these works reveal an opportunity: construct VQA directly from classification datasets to unify discriminative and generative evaluation while exploiting semantic hierarchies for graded correctness. Our benchmark operationalizes this by turning label spaces into open-ended questions and using ancestor-aware follow-up questions to recognize coarse-but-valid answers. We further scrutinize evaluation by empirically comparing LLM judges against traditional NLP metrics with human validation, yielding a granular, taxonomy-aware, and comparable assessment of text-generative and discriminative vision\u2013language models.",
  "target_paper": {
    "title": "Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy",
    "authors": "Simon Ging, Maria Alejandra Bravo, Thomas Brox",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Open-ended VQA, benchmark, Vision-Language, VL, Vision-Text, VLM, Vision-Language models, Image classification, Visual question answering, Text-generating VLM",
    "abstract": "The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models\u2019 capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our be",
    "openreview_id": "EXitynZhYn",
    "forum_id": "EXitynZhYn"
  },
  "analysis_timestamp": "2026-01-06T22:42:35.980930"
}
{
  "prior_works": [
    {
      "title": "Diversity is All You Need: Learning Skills without a Reward Function",
      "authors": "Benjamin Eysenbach et al.",
      "year": 2019,
      "arxiv_id": "1802.06070",
      "role": "Baseline",
      "relationship_sentence": "This work is the canonical mutual-information skill learning baseline (maximizing I(s; z)) that the paper analyzes and improves by explicitly targeting skill diversity and separability and by replacing KL-based geometry with a Wasserstein-based objective."
    },
    {
      "title": "Variational Intrinsic Control",
      "authors": "Karol Gregor et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "It introduced the MI-based formulation for discovering controllable latent skills and the variational estimation machinery that the paper generalizes when defining LSEPIN and deriving new information-geometric objectives over skill\u2013state distributions."
    },
    {
      "title": "Dynamics-Aware Unsupervised Discovery of Skills",
      "authors": "Archit Sharma et al.",
      "year": 2019,
      "arxiv_id": "1907.01657",
      "role": "Baseline",
      "relationship_sentence": "DADS extends MI skill learning to future-state predictability, and its remaining limitations in skill separability and downstream transfer directly motivate the paper\u2019s disentanglement metric and Wasserstein-based skill objective as a more adaptation-aligned alternative."
    },
    {
      "title": "URLB: Unsupervised Reinforcement Learning Benchmark",
      "authors": "Michael Laskin et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "URLB formalizes the reward-free pretrain\u2013finetune protocol and evaluation on downstream tasks that the paper adopts to define and analyze adaptation cost and to assess whether learned skills transfer effectively."
    },
    {
      "title": "Trust Region Policy Optimization",
      "authors": "John Schulman et al.",
      "year": 2015,
      "arxiv_id": "1502.05477",
      "role": "Foundation",
      "relationship_sentence": "TRPO provides performance-improvement bounds in terms of KL divergences between policies, a geometric link the paper repurposes to connect divergences between skill-conditioned state distributions to downstream adaptation cost and to motivate replacing KL with Wasserstein."
    },
    {
      "title": "Wasserstein Auto-Encoders",
      "authors": "Ilya Tolstikhin et al.",
      "year": 2018,
      "arxiv_id": "1711.01558",
      "role": "Inspiration",
      "relationship_sentence": "WAE showed that substituting KL with Wasserstein distance yields better geometric properties and separability in latent-variable models, directly inspiring the paper\u2019s WSEP objective that swaps KL for Wasserstein in skill-learning information geometry."
    }
  ],
  "synthesis_narrative": "MI-based skill discovery established that unsupervised agents can acquire reusable behaviors by maximizing mutual information between states and latent skill variables; DIAYN operationalized this with a practical discriminator objective and implicit coverage pressure, while Variational Intrinsic Control provided the variational MI framework and latent-control formulation that underpins such methods. DADS refined the idea by making skills dynamics-aware via future-state predictability, yet still relied on MI-style objectives whose learned skills can lack strong separability. In parallel, URLB codified the reward-free pretraining followed by adaptation to downstream tasks, making transfer performance the central evaluation criterion for skills learned without rewards. On the theoretical side, TRPO connected policy performance changes to information geometry through KL-based improvement bounds, highlighting how divergence between distributions governs adaptation behavior. Complementing this, Wasserstein Auto-Encoders demonstrated that replacing KL with Wasserstein improves latent geometry and separation, mitigating mode-covering pathologies that hinder disentanglement. Together, these works reveal a gap: MI skill methods promote coverage but do not ensure separable skills tied to transfer efficiency, and existing theory links adaptation to KL geometry that can be misaligned with the structure of state distributions. Building on the MI/variational formulation and the URLB transfer setup, the paper introduces a disentanglement metric explicitly tied (via information geometry) to adaptation cost, and, motivated by WAE and TRPO\u2019s geometric insights, replaces KL with Wasserstein to obtain a new skill-learning objective that better aligns skill diversity and separability with downstream task adaptation.",
  "target_paper": {
    "title": "Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning",
    "authors": "Yucheng Yang, Tianyi Zhou, Qiang He, Lei Han, Mykola Pechenizkiy, Meng Fang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "unsupervised skill learning, reward-free RL, downstream task adaptation, wasserstein distance, theoretical analysis",
    "abstract": "Unsupervised reinforcement learning (URL) aims to learn general skills for unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL by maximizing the mutual information between states and skills but lacks sufficient theoretical analysis, e.g., how well its learned skills can initialize a downstream task's policy. Our new theoretical analysis shows that the diversity and separatability of learned skills are fundamentally critical to downstream task adaptation but MISL does not necessarily guarantee them. To improve MISL, we propose a novel disentanglement metric LSEPIN and build an information-geometric connection between LSEPIN and downstream task adaptation cost. For better geometric properties, we investigate a new strategy that replaces the KL divergence in information geometry with Wasserstein distance. We extend the geometric analysis to it, which leads to a novel skill-learning objective WSEP. It is theoretically justified to be helpful to task adaptation a",
    "openreview_id": "zSxpnKh1yS",
    "forum_id": "zSxpnKh1yS"
  },
  "analysis_timestamp": "2026-01-06T17:27:59.436719"
}
{
  "prior_works": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, et al.",
      "year": 2022,
      "role": "Introduced explicit intermediate reasoning traces (\u201cthoughts\u201d) as a powerful prompt format for complex tasks.",
      "relationship_sentence": "BoT generalizes CoT by distilling reusable, high-level thought-templates across tasks and retrieving them per instance instead of crafting ad hoc chains each time."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang, Jason Wei, Dale Schuurmans, et al.",
      "year": 2022,
      "role": "Showed that sampling diverse chains and voting boosts accuracy but at high compute cost.",
      "relationship_sentence": "BoT targets the same robustness gains as self-consistency but achieves them efficiently by reusing retrieved thought-templates rather than generating many parallel chains."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, et al.",
      "year": 2023,
      "role": "Framed reasoning as structured exploration over intermediate thoughts with search.",
      "relationship_sentence": "BoT adopts the idea of structured reasoning but replaces costly on-the-fly search with template retrieval and instantiation, yielding faster, stable deliberation."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao, Jeffrey Zhao, Dian Yu, et al.",
      "year": 2022,
      "role": "Coupled reasoning traces with action steps, highlighting modular trajectories that guide problem solving.",
      "relationship_sentence": "BoT\u2019s thought-templates capture reusable reasoning structures akin to ReAct trajectories, but make them task-agnostic and retrievable from a meta-buffer."
    },
    {
      "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
      "authors": "Denny Zhou, Xuezhi Wang, Jason Wei, et al.",
      "year": 2022,
      "role": "Demonstrated that high-level decomposition plans can guide efficient stepwise solving.",
      "relationship_sentence": "BoT operationalizes such high-level plans as abstract templates, retrieving and instantiating them to decompose new problems adaptively."
    },
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
      "authors": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, et al.",
      "year": 2020,
      "role": "Established retrieving external artifacts to condition generation for accuracy and scalability.",
      "relationship_sentence": "BoT extends RAG\u2019s paradigm from knowledge retrieval to retrieval of process artifacts\u2014thought-templates\u2014thereby augmenting reasoning rather than just facts."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "authors": "Shinn et al.",
      "year": 2023,
      "role": "Showed agents can accumulate and leverage self-generated experiences/memories to improve over time.",
      "relationship_sentence": "BoT\u2019s buffer-manager echoes Reflexion\u2019s dynamic memory update, continually refining the meta-buffer as new tasks are solved to enhance stability and generalization."
    }
  ],
  "synthesis_narrative": "Buffer of Thoughts (BoT) sits at the intersection of explicit reasoning traces and retrieval-based augmentation. Chain-of-Thought (CoT) established that exposing intermediate reasoning steps unlocks strong performance, while Self-Consistency showed robustness can be gained by sampling diverse chains\u2014albeit with heavy compute. Tree of Thoughts (ToT) and ReAct advanced this by structuring multi-step trajectories and interleaving actions, revealing that the organization of thoughts matters as much as their content. In parallel, Least-to-Most Prompting demonstrated that high-level decomposition plans can steer efficient problem solving. Retrieval-Augmented Generation (RAG) provided the systems lens: retrieving relevant external artifacts at inference time scales accuracy and adaptability. Reflexion then showed that storing and updating agent memories across episodes improves future behavior.\nBoT synthesizes these strands by distilling high-level, reusable \u201cthought-templates\u201d (generalizing CoT/ToT/ReAct plans) into a meta-buffer, retrieving the most relevant template per instance (RAG-style conditioning), and adaptively instantiating it to guide reasoning and decomposition (akin to Least-to-Most). Its buffer-manager incrementally updates this library from solved tasks (in the spirit of Reflexion), improving stability and coverage over time. The result is CoT-level interpretability and ToT-level structure without the sampling or search overhead\u2014yielding accuracy, efficiency, and robustness through thought-level retrieval and continual memory refinement.",
  "analysis_timestamp": "2026-01-06T23:33:36.282472"
}
{
  "analysis": {
    "total_papers": 3291,
    "primary_pattern_counts": {
      "P01": 795,
      "P02": 594,
      "P04": 86,
      "P15": 77,
      "P05": 198,
      "P06": 196,
      "P08": 146,
      "P10": 175,
      "P03": 344,
      "P07": 243,
      "P09": 90,
      "P14": 48,
      "P11": 51,
      "P12": 126,
      "P13": 57,
      "unknown": 65
    },
    "secondary_pattern_counts": {
      "P08": 289,
      "P14": 94,
      "P07": 384,
      "P03": 572,
      "P09": 125,
      "P02": 450,
      "P05": 259,
      "P06": 359,
      "P11": 122,
      "P01": 176,
      "P10": 306,
      "P13": 56,
      "P12": 182,
      "P04": 244,
      "P15": 150
    },
    "total_pattern_counts": {
      "P01": 971,
      "P08": 435,
      "P14": 142,
      "P02": 1044,
      "P07": 627,
      "P03": 916,
      "P04": 330,
      "P09": 215,
      "P15": 227,
      "P05": 457,
      "P06": 555,
      "P11": 173,
      "P10": 481,
      "P13": 113,
      "P12": 308,
      "unknown": 65
    },
    "by_conference": {
      "ICLR": {
        "P01": 228,
        "P02": 184,
        "P04": 30,
        "P15": 31,
        "P05": 87,
        "P06": 49,
        "P08": 46,
        "P10": 50,
        "P03": 120,
        "P07": 58,
        "P09": 30,
        "P14": 17,
        "P11": 13,
        "P12": 39,
        "P13": 17,
        "unknown": 20
      },
      "NeurIPS": {
        "P07": 122,
        "P05": 58,
        "P08": 61,
        "P06": 90,
        "P09": 37,
        "P10": 85,
        "P13": 27,
        "P01": 370,
        "P14": 20,
        "P02": 279,
        "P03": 162,
        "P04": 41,
        "P11": 32,
        "P15": 31,
        "P12": 59,
        "unknown": 35
      },
      "ICML": {
        "P04": 15,
        "P03": 62,
        "P06": 57,
        "P02": 131,
        "P12": 28,
        "P01": 197,
        "P07": 63,
        "P05": 53,
        "P14": 11,
        "P10": 40,
        "P09": 23,
        "P13": 13,
        "P11": 6,
        "P08": 39,
        "P15": 15,
        "unknown": 10
      }
    },
    "by_year": {
      "2024": {
        "P01": 254,
        "P02": 191,
        "P04": 31,
        "P15": 31,
        "P05": 66,
        "P06": 66,
        "P08": 59,
        "P10": 62,
        "P03": 123,
        "P07": 76,
        "P09": 26,
        "P14": 13,
        "P11": 18,
        "P12": 28,
        "P13": 21,
        "unknown": 5
      },
      "2025": {
        "P06": 86,
        "P03": 176,
        "P10": 84,
        "P01": 394,
        "P02": 300,
        "P12": 74,
        "P07": 110,
        "P04": 51,
        "P15": 32,
        "P05": 110,
        "P14": 26,
        "P08": 65,
        "P09": 54,
        "unknown": 50,
        "P13": 26,
        "P11": 19
      },
      "2023": {
        "P02": 103,
        "P12": 24,
        "P03": 45,
        "P10": 29,
        "P13": 10,
        "P05": 22,
        "P01": 147,
        "P07": 57,
        "P15": 14,
        "P06": 44,
        "P11": 14,
        "P08": 22,
        "P14": 9,
        "P09": 10,
        "unknown": 10,
        "P04": 4
      }
    },
    "by_presentation_type": {
      "oral": {
        "P01": 199,
        "P02": 152,
        "P04": 22,
        "P15": 22,
        "P05": 59,
        "P06": 57,
        "P08": 48,
        "P10": 42,
        "P03": 82,
        "P07": 83,
        "P09": 23,
        "P14": 20,
        "P11": 10,
        "P12": 38,
        "P13": 12,
        "unknown": 10
      },
      "spotlight": {
        "P03": 262,
        "P10": 133,
        "P02": 442,
        "P01": 596,
        "P08": 98,
        "P15": 55,
        "P04": 64,
        "P05": 139,
        "P06": 139,
        "P07": 160,
        "P14": 28,
        "P12": 88,
        "P09": 67,
        "P11": 41,
        "P13": 45,
        "unknown": 55
      }
    },
    "by_conf_year": {
      "ICLR-2024": {
        "P01": 94,
        "P02": 71,
        "P04": 12,
        "P15": 18,
        "P05": 33,
        "P06": 20,
        "P08": 26,
        "P10": 28,
        "P03": 58,
        "P07": 22,
        "P09": 11,
        "P14": 5,
        "P11": 9,
        "P12": 14,
        "P13": 10,
        "unknown": 5
      },
      "ICLR-2025": {
        "P06": 29,
        "P03": 62,
        "P10": 22,
        "P01": 134,
        "P02": 113,
        "P12": 25,
        "P07": 36,
        "P04": 18,
        "P15": 13,
        "P05": 54,
        "P14": 12,
        "P08": 20,
        "P09": 19,
        "unknown": 15,
        "P13": 7,
        "P11": 4
      },
      "NeurIPS-2024": {
        "P07": 29,
        "P05": 15,
        "P08": 16,
        "P06": 22,
        "P09": 6,
        "P10": 22,
        "P13": 5,
        "P01": 80,
        "P14": 4,
        "P02": 67,
        "P03": 35,
        "P04": 13,
        "P11": 7,
        "P15": 6,
        "P12": 7
      },
      "NeurIPS-2023": {
        "P02": 79,
        "P12": 18,
        "P03": 40,
        "P10": 21,
        "P13": 7,
        "P05": 15,
        "P01": 110,
        "P07": 43,
        "P15": 12,
        "P06": 31,
        "P11": 11,
        "P08": 14,
        "P14": 7,
        "P09": 3,
        "unknown": 5,
        "P04": 2
      },
      "NeurIPS-2025": {
        "P01": 180,
        "P15": 13,
        "P03": 87,
        "P12": 34,
        "P10": 42,
        "P02": 133,
        "P04": 26,
        "P09": 28,
        "P11": 14,
        "P06": 37,
        "P07": 50,
        "P08": 31,
        "P14": 9,
        "P05": 28,
        "P13": 15,
        "unknown": 30
      },
      "ICML-2024": {
        "P04": 6,
        "P03": 30,
        "P06": 24,
        "P02": 53,
        "P12": 7,
        "P01": 80,
        "P07": 25,
        "P05": 18,
        "P14": 4,
        "P10": 12,
        "P09": 9,
        "P13": 6,
        "P11": 2,
        "P08": 17,
        "P15": 7
      },
      "ICML-2023": {
        "P01": 37,
        "P07": 14,
        "P12": 6,
        "P08": 8,
        "P05": 7,
        "P10": 8,
        "P02": 24,
        "P06": 13,
        "P03": 5,
        "P09": 7,
        "P04": 2,
        "unknown": 5,
        "P15": 2,
        "P13": 3,
        "P14": 2,
        "P11": 3
      },
      "ICML-2025": {
        "P03": 27,
        "P12": 15,
        "P07": 24,
        "P01": 80,
        "P06": 20,
        "P05": 28,
        "P02": 54,
        "P09": 7,
        "P08": 14,
        "P14": 5,
        "P04": 7,
        "P15": 6,
        "P10": 20,
        "P13": 4,
        "unknown": 5,
        "P11": 1
      }
    },
    "cooccurrence": {
      "P01": {
        "P08": 35,
        "P14": 9,
        "P03": 281,
        "P13": 16,
        "P10": 44,
        "P12": 30,
        "P15": 41,
        "P05": 94,
        "P09": 26,
        "P07": 82,
        "P02": 154,
        "P04": 34,
        "P06": 102,
        "P11": 8
      },
      "P02": {
        "P07": 50,
        "P03": 134,
        "P11": 32,
        "P06": 66,
        "P04": 83,
        "P01": 41,
        "P08": 48,
        "P15": 35,
        "P09": 40,
        "P10": 78,
        "P05": 52,
        "P14": 8,
        "P12": 17,
        "P13": 8
      },
      "P04": {
        "P09": 10,
        "P02": 23,
        "P10": 12,
        "P11": 10,
        "P08": 9,
        "P15": 7,
        "P03": 10,
        "P14": 2,
        "P06": 1,
        "P01": 6,
        "P05": 5,
        "P12": 4,
        "P07": 2
      },
      "P15": {
        "P07": 10,
        "P05": 23,
        "P08": 10,
        "P02": 15,
        "P04": 12,
        "P06": 6,
        "P01": 6,
        "P12": 1,
        "P03": 1,
        "P13": 2,
        "P14": 1,
        "P10": 2
      },
      "P05": {
        "P06": 13,
        "P07": 52,
        "P01": 28,
        "P12": 4,
        "P14": 2,
        "P15": 32,
        "P02": 34,
        "P11": 5,
        "P03": 7,
        "P04": 18,
        "P13": 9,
        "P10": 14,
        "P09": 3,
        "P08": 2
      },
      "P06": {
        "P09": 13,
        "P08": 43,
        "P07": 55,
        "P10": 11,
        "P02": 26,
        "P01": 13,
        "P03": 19,
        "P12": 15,
        "P05": 18,
        "P15": 3,
        "P11": 4,
        "P13": 2,
        "P04": 6,
        "P14": 2
      },
      "P08": {
        "P02": 18,
        "P14": 50,
        "P03": 14,
        "P07": 19,
        "P06": 20,
        "P01": 5,
        "P09": 12,
        "P04": 9,
        "P15": 6,
        "P10": 12,
        "P11": 3,
        "P12": 1,
        "P13": 1,
        "P05": 1
      },
      "P10": {
        "P03": 56,
        "P04": 17,
        "P05": 6,
        "P01": 9,
        "P11": 23,
        "P02": 31,
        "P12": 15,
        "P06": 14,
        "P07": 14,
        "P08": 12,
        "P14": 3,
        "P15": 2,
        "P13": 1
      },
      "P03": {
        "P02": 88,
        "P12": 18,
        "P01": 22,
        "P09": 8,
        "P08": 40,
        "P04": 25,
        "P10": 82,
        "P11": 28,
        "P14": 6,
        "P15": 10,
        "P05": 15,
        "P06": 35,
        "P07": 24,
        "P13": 3
      },
      "P07": {
        "P10": 10,
        "P06": 70,
        "P12": 65,
        "P08": 35,
        "P05": 24,
        "P02": 8,
        "P03": 16,
        "P01": 18,
        "P13": 7,
        "P14": 6,
        "P15": 4,
        "P11": 4,
        "P04": 4,
        "P09": 2
      },
      "P09": {
        "P14": 3,
        "P04": 16,
        "P02": 15,
        "P06": 7,
        "P15": 5,
        "P03": 7,
        "P05": 5,
        "P01": 8,
        "P10": 4,
        "P08": 17,
        "P12": 4,
        "P11": 2,
        "P13": 4,
        "P07": 1
      },
      "P14": {
        "P10": 7,
        "P06": 4,
        "P08": 26,
        "P04": 5,
        "P01": 3,
        "P07": 5,
        "P02": 4,
        "P12": 1,
        "P09": 2,
        "P15": 1
      },
      "P11": {
        "P02": 13,
        "P14": 1,
        "P10": 15,
        "P04": 9,
        "P05": 2,
        "P03": 5,
        "P08": 5,
        "P06": 5,
        "P01": 4,
        "P15": 1,
        "P09": 1
      },
      "P12": {
        "P07": 58,
        "P01": 8,
        "P08": 5,
        "P10": 13,
        "P02": 11,
        "P03": 20,
        "P04": 3,
        "P13": 3,
        "P05": 6,
        "P15": 1,
        "P06": 7,
        "P11": 3,
        "P09": 3
      },
      "P13": {
        "P10": 2,
        "P07": 12,
        "P09": 5,
        "P05": 8,
        "P01": 5,
        "P06": 9,
        "P08": 2,
        "P02": 10,
        "P15": 2,
        "P12": 7,
        "P04": 3,
        "P03": 2,
        "P14": 1
      }
    },
    "pattern_pairs": {
      "('P01', 'P03')": 303,
      "('P02', 'P03')": 222,
      "('P01', 'P02')": 195,
      "('P03', 'P10')": 138,
      "('P06', 'P07')": 125,
      "('P07', 'P12')": 123,
      "('P01', 'P05')": 122,
      "('P01', 'P06')": 115,
      "('P02', 'P10')": 109,
      "('P02', 'P04')": 106,
      "('P01', 'P07')": 100,
      "('P02', 'P06')": 92,
      "('P02', 'P05')": 86,
      "('P05', 'P07')": 76,
      "('P08', 'P14')": 76,
      "('P02', 'P08')": 66,
      "('P06', 'P08')": 63,
      "('P02', 'P07')": 58,
      "('P05', 'P15')": 55,
      "('P02', 'P09')": 55,
      "('P03', 'P08')": 54,
      "('P07', 'P08')": 54,
      "('P03', 'P06')": 54,
      "('P01', 'P10')": 53,
      "('P02', 'P15')": 50,
      "('P01', 'P15')": 47,
      "('P02', 'P11')": 45,
      "('P01', 'P08')": 40,
      "('P03', 'P07')": 40,
      "('P01', 'P04')": 40
    },
    "confidence_distribution": {
      "high": 2634,
      "medium-high": 318,
      "medium": 273,
      "unknown": 65,
      "med": 1
    },
    "pattern_names": {
      "P01": "Gap-Driven Reframing",
      "P02": "Cross-Domain Synthesis",
      "P03": "Representation Shift & Primitive Recasting",
      "P04": "Modular Pipeline Composition",
      "P05": "Data & Evaluation Engineering",
      "P06": "Principled Probabilistic Modeling & Uncertainty",
      "P07": "Formal-Experimental Tightening",
      "P08": "Approximation Engineering for Scalability",
      "P09": "Inference-Time Control & Guided Sampling",
      "P10": "Inject Structural Inductive Bias",
      "P11": "Multiscale & Hierarchical Modeling",
      "P12": "Mechanistic Decomposition & Causal Localization",
      "P13": "Adversary Modeling & Defensive Repurposing",
      "P14": "Numerics & Systems Co-design",
      "P15": "Data-Centric Optimization & Active Sampling"
    },
    "pattern_categories": {
      "P01": "Problem Diagnosis & Reframing",
      "P02": "Synthesis & Transfer",
      "P03": "Representation & Abstraction",
      "P04": "Systems & Pipelines",
      "P05": "Data, Metrics & Benchmarks",
      "P06": "Probabilistic & Theoretical Methods",
      "P07": "Theory \u2194 Practice Loop",
      "P08": "Approximation & Algorithmics",
      "P09": "Runtime Steering & Adaptation",
      "P10": "Inductive Bias & Geometry",
      "P11": "Scale & Abstraction",
      "P12": "Interpretability & Analysis",
      "P13": "Robustness & Security",
      "P14": "Systems & Deployment",
      "P15": "Data, Sampling & Efficiency"
    }
  },
  "taxonomy": [
    {
      "id": "P01",
      "name": "Gap-Driven Reframing",
      "category": "Problem Diagnosis & Reframing",
      "description": "Start from a concrete empirical, operational, or assumption gap and reframe the problem so different tools or objectives become applicable. This pattern turns observed limitations into a new problem statement or constraint that unlocks solutions previously ruled out.",
      "key_indicators": [
        "limitation",
        "gap",
        "reframed as",
        "instead of X, we treat Y as",
        "assumption questioned"
      ],
      "cognitive_move": "Turn a specific failure or mismatched assumption into an explicit design constraint or alternate formulation that maps the problem onto better-suited methods.",
      "variants_merged": [
        "Gap-Driven Reframing",
        "Gap Amplification",
        "Assumption Probing",
        "Gap \u2192 Design Constraint",
        "Problem Reframing (many variants)"
      ],
      "example": "Reframing autoregressive image modeling from next-token prediction to next-scale (coarse\u2192fine) prediction to improve generation quality (VAR-like approach).",
      "learnable_insight": "When you notice a recurring failure, write it as an explicit constraint or alternative objective; ask 'if this limitation were the problem, what methods would apply?' then test the reframed formulation on a minimal prototype."
    },
    {
      "id": "P02",
      "name": "Cross-Domain Synthesis",
      "category": "Synthesis & Transfer",
      "description": "Deliberately combine ideas, primitives, and formalisms from distinct fields to construct hybrid methods that leverage complementary strengths. The move is targeted \u2014 identify which element from another domain resolves a concrete shortcoming and adapt its interface carefully.",
      "key_indicators": [
        "borrow from",
        "combine",
        "drawn from",
        "inspired by",
        "fuse X and Y"
      ],
      "cognitive_move": "Map components across disciplinary boundaries and transplant them into the target problem while engineering the compatibility layer.",
      "variants_merged": [
        "Cross-Domain Synthesis",
        "Cross-domain Import",
        "Cross-domain Hybridization",
        "Analogy-driven Transfer",
        "Analogy-to-Classical Systems"
      ],
      "example": "Fusing quantum circuits with transformer attention to obtain doubly stochastic attention matrices (QDSFormer) or importing Lyapunov/optimal-transport ideas into ML optimization proofs.",
      "learnable_insight": "List constraints your method fails to satisfy, search other fields for primitives addressing those constraints, and prototype by isolating the imported primitive with a thin adapter to evaluate compatibility before deeper integration."
    },
    {
      "id": "P03",
      "name": "Representation Shift & Primitive Recasting",
      "category": "Representation & Abstraction",
      "description": "Change the core data or model primitive (representation, discretization, latent space) to better match the problem geometry or computational affordances. Shifting primitives often converts an intractable task into a tractable one or uncovers simpler inductive structure.",
      "key_indicators": [
        "recast as",
        "operate in latent space",
        "implicit SDF",
        "lattice / triplane",
        "primitive"
      ],
      "cognitive_move": "Replace the problem's language (pixels, tokens, meshes) with an alternative primitive that simplifies inference, learning, or constraints.",
      "variants_merged": [
        "Representation Shift",
        "Primitive Recasting",
        "Component Replacement with Structured Alternatives",
        "Latent Abstraction Swap",
        "Feature-space Reorientation"
      ],
      "example": "Replacing explicit meshes with neural implicit signed-distance functions (implicit SDFs) for 3D reconstruction to avoid collision-graph blowup.",
      "learnable_insight": "When a task struggles with geometry or combinatorics, enumerate alternative primitives (continuous vs discrete, latent vs observed), pick one that aligns with constraints, and reimplement a minimal pipeline to test whether the new primitive reduces complexity."
    },
    {
      "id": "P04",
      "name": "Modular Pipeline Composition",
      "category": "Systems & Pipelines",
      "description": "Decompose an end-to-end task into specialized modules (retrieval, denoising, alignment, synthesis) and design interfaces so improvements in modules compound. Emphasizes composability, updatability, and clear contracts between stages.",
      "key_indicators": [
        "pipeline",
        "two-stage",
        "component X + component Y",
        "plug-and-play",
        "stitching workflow"
      ],
      "cognitive_move": "Factor a complex goal into subproblems and engineer interoperable modules with well-defined inputs/outputs so each can be improved or swapped independently.",
      "variants_merged": [
        "Modular Pipeline Innovation",
        "Plug-and-play Modularity",
        "Pipeline Reconceptualization",
        "Decompose and Modularize",
        "Compose Complementary Methods"
      ],
      "example": "TANGO: combine audio\u2194motion retrieval with diffusion-based interpolation modules to produce robust gesture reenactment.",
      "learnable_insight": "Sketch the full workflow and identify crisp module boundaries; implement a minimal interface that allows independent replacement and run ablations to measure marginal gains from each module."
    },
    {
      "id": "P05",
      "name": "Data & Evaluation Engineering",
      "category": "Data, Metrics & Benchmarks",
      "description": "Engineer datasets, benchmarks, metrics, and synthetic supervisors that make target phenomena measurable, comparable, and optimizable. This pattern recognizes that progress often requires the right measurement instrument.",
      "key_indicators": [
        "dataset",
        "benchmark",
        "metric",
        "we introduce",
        "synthetic supervision"
      ],
      "cognitive_move": "Convert an informal desideratum into a measurable task or proxy and release resources that standardize evaluation and drive community progress.",
      "variants_merged": [
        "Dataset/Benchmark Construction",
        "Metric & Benchmark Invention",
        "Benchmarking by Harmonization",
        "Synthetic Supervision",
        "Evaluation Reorientation",
        "Compositional Benchmarking"
      ],
      "example": "Creating CBGBench to unify structure-based drug-design subtasks as conditional graph completion, enabling fair comparisons.",
      "learnable_insight": "When a capability is ill-defined, write a concise task spec and metric, assemble a small diagnostic dataset or simulator, and validate that improvements on the metric correlate with the intended downstream behavior."
    },
    {
      "id": "P06",
      "name": "Principled Probabilistic Modeling & Uncertainty",
      "category": "Probabilistic & Theoretical Methods",
      "description": "Replace heuristics or deterministic components with probabilistic models (Bayesian, score-based, GP-like) to quantify uncertainty, pool evidence, and obtain principled inference or calibration guarantees.",
      "key_indicators": [
        "Bayesian",
        "amortized inference",
        "uncertainty",
        "calibration",
        "Laplace"
      ],
      "cognitive_move": "Introduce explicit probability models or approximate posteriors to make assumptions explicit and provide uncertainty-aware decisions with analyzable properties.",
      "variants_merged": [
        "Principled Probabilistic Replacement",
        "Calibration and Uncertainty Decomposition",
        "Probabilistic Reframing for Uncertainty",
        "Proxying Expensive Components with Learned Surrogates (as probabilistic proxies)"
      ],
      "example": "Using Bayesian hierarchical modeling to estimate causal concept influences with uncertainty rather than pointwise heuristics.",
      "learnable_insight": "If decisions require trust or risk accounting, recast parts of the pipeline probabilistically (even with crude approximations) and prioritize calibration checks (holdout, conformal tests) before deployment."
    },
    {
      "id": "P07",
      "name": "Formal-Experimental Tightening",
      "category": "Theory \u2194 Practice Loop",
      "description": "Iterate between empirical probes and formal analysis: use controlled experiments to formulate conjectures, then develop formal models and proofs that explain observations and guide remedies. The loop increases both explanatory power and robustness.",
      "key_indicators": [
        "characterize",
        "bound / optimal",
        "prove",
        "empirical success but unexplained",
        "theoretical gap"
      ],
      "cognitive_move": "Treat empirical anomalies as hypotheses, build formal abstractions to explain them, and close the loop by testing theory-derived predictions experimentally.",
      "variants_merged": [
        "Theoretical Generalization",
        "From Observation to Theory",
        "Formal-Experimental Tightening",
        "Prove Limitations then Engineer Remedies",
        "Probe-Then-Theorize (Empirical\u2192Theory Loop)"
      ],
      "example": "Deriving finite-sample convergence rates for schedule-free SGD after observing empirical performance in nonconvex settings.",
      "learnable_insight": "When you observe a robust empirical pattern, distill it into the simplest mathematical model that captures it and attempt to prove one useful property (convergence, bound, or limit); use the formal result to propose a focused engineering fix and validate it."
    },
    {
      "id": "P08",
      "name": "Approximation Engineering for Scalability",
      "category": "Approximation & Algorithmics",
      "description": "Design controlled approximations, surrogates, or amortization schemes that preserve essential theoretical properties while making algorithms practical at scale. Emphasizes principled trade-offs between exactness and cost.",
      "key_indicators": [
        "approximate",
        "Hessian-free",
        "amortize",
        "fixed-point iteration",
        "efficient approximation"
      ],
      "cognitive_move": "Identify the expensive component, replace it with a principled approximation whose error can be bounded or empirically validated, and integrate refresh/correction schedules if needed.",
      "variants_merged": [
        "Approximate to Scale",
        "Efficient Approximation Engineering",
        "Amortize Expensive Resources",
        "Approximate Marginalization",
        "Practical Simplification"
      ],
      "example": "Computing Hessians infrequently and reusing them across correction steps with scheduled refreshes to maintain convergence guarantees.",
      "learnable_insight": "Profile runtime and identify dominating costs; design the cheapest principled approximation (e.g., sketching, low-rank, randomized) and either prove bounded error accumulation or measure it under representative workloads."
    },
    {
      "id": "P09",
      "name": "Inference-Time Control & Guided Sampling",
      "category": "Runtime Steering & Adaptation",
      "description": "Shift interventions from retraining to sampling- or inference-time controls (guidance, conditioning, mode guidance). This yields fast, flexible behavioral changes and avoids costly weight updates.",
      "key_indicators": [
        "sampling-time",
        "guidance",
        "no weight updates",
        "mode discovery",
        "test-time search"
      ],
      "cognitive_move": "Design mechanisms that steer a pre-trained model's outputs at generation-time to satisfy new constraints or promote diversity without changing parameters.",
      "variants_merged": [
        "Sampling-time Control",
        "Sampling-as-Control",
        "Mode Discovery and Conditioning",
        "Inference-Time Search / Test-Time Adaptation"
      ],
      "example": "Using Mode Discovery and Mode Guidance with a pre-trained latent diffusion model to produce condensed datasets that cover intra-class modes without retraining.",
      "learnable_insight": "Start from a pre-trained generator; identify runtime hooks (logit modification, classifier-free guidance, conditional sampling) and implement lightweight controllers that can be turned on/off to evaluate trade-offs between fidelity and desired constraint satisfaction."
    },
    {
      "id": "P10",
      "name": "Inject Structural Inductive Bias",
      "category": "Inductive Bias & Geometry",
      "description": "Encode domain structure (symmetry, locality, sparsity, taxonomy) directly into architectures, losses, or representations to reduce hypothesis space and improve sample efficiency and robustness.",
      "key_indicators": [
        "inductive bias",
        "equivariance",
        "sparsity",
        "locality",
        "taxonomic / hierarchy"
      ],
      "cognitive_move": "Turn known invariants or structure into explicit constraints or model motifs so learning data can be used more efficiently and reliably.",
      "variants_merged": [
        "Inject Domain Inductive Bias",
        "Exploit Symmetry and Invariance",
        "Structural Prior Exploitation",
        "Control Representation Geometry",
        "Signal Cancellation / Differential Cancellation (as geometry shaping)"
      ],
      "example": "Designing SO(3)-equivariant representations for robotic manipulation to enable data-efficient policies from monocular inputs.",
      "learnable_insight": "Catalog symmetries and structural priors in your domain; prefer architectures or losses that enforce them (equivariant layers, locality-promoting regularizers) before attempting brute-force data scaling."
    },
    {
      "id": "P11",
      "name": "Multiscale & Hierarchical Modeling",
      "category": "Scale & Abstraction",
      "description": "Employ multi-resolution or hierarchical architectures and procedures (coarse-to-fine, temporal abstraction) so models can capture long-range structure efficiently and compose local details on top of global structure.",
      "key_indicators": [
        "coarse-to-fine",
        "hierarchical",
        "multi-scale",
        "temporal abstraction",
        "next-scale"
      ],
      "cognitive_move": "Decompose the problem across scales and design interactions so global context guides local refinement and planning amortizes over time horizons.",
      "variants_merged": [
        "Multiscale / Hierarchical Modeling",
        "Hierarchical and Temporal Abstraction",
        "Hierarchical & Multiscale Refinement",
        "Temporal/Horizon Abstraction"
      ],
      "example": "Predicting images across resolution scales rather than token order (next-scale) to improve image generation efficiency and quality.",
      "learnable_insight": "When facing long-horizon or high-resolution problems, prototype a coarse representation and iteratively add refinements; measure how much the coarse stage reduces downstream search or compute."
    },
    {
      "id": "P12",
      "name": "Mechanistic Decomposition & Causal Localization",
      "category": "Interpretability & Analysis",
      "description": "Break complex learned behavior into interpretable mechanisms (heads, units, spectral components), then validate causality with interventions or ablations to explain and improve models.",
      "key_indicators": [
        "decompose into",
        "identify heads",
        "causal pruning",
        "mechanistic",
        "timescale separation"
      ],
      "cognitive_move": "Isolate candidate mechanisms, attribute observed effects to them, and test causal hypotheses via controlled interventions.",
      "variants_merged": [
        "Mechanistic Decomposition",
        "Mechanistic Localization",
        "Timescale Separation",
        "Spectral/Variance-based Insight",
        "Empirical Microscopy"
      ],
      "example": "Decomposing classifier-free guidance into mean-shifts and contrastive principal components to explain conditional sampling improvements.",
      "learnable_insight": "Use targeted probes (ablation, mean-shift decomposition, spectral analysis) on a small trained model to identify which components matter; design minimally invasive fixes informed by the causal attribution."
    },
    {
      "id": "P13",
      "name": "Adversary Modeling & Defensive Repurposing",
      "category": "Robustness & Security",
      "description": "Model adversarial behaviors explicitly (inverse RL, attacker models) to synthesize realistic adversaries for robust training, or repurpose offensive tools constructively (unlearning, privacy, defense).",
      "key_indicators": [
        "inverse reinforcement learning",
        "generate adversarial samples",
        "repurpose",
        "defensive reinterpretation",
        "adversarial security problem"
      ],
      "cognitive_move": "Turn defense into a generative or inverse problem: infer attacker objectives and use them to harden, detect, or reverse undesirable behaviors.",
      "variants_merged": [
        "Adversary Modeling / Reverse Engineering",
        "Adversarial Repurposing",
        "Defensive Reinterpretation",
        "Adversarial / Security Reframing"
      ],
      "example": "Using maximum-entropy inverse RL to recover attacker policies and synthesize adversarial samples that improve GNN robustness.",
      "learnable_insight": "If robustness is critical, build an explicit attacker model (even simple) and generate adversarial examples for training and auditing; consider whether attack mechanisms can be flipped into corrective interventions (e.g., unlearning)."
    },
    {
      "id": "P14",
      "name": "Numerics & Systems Co-design",
      "category": "Systems & Deployment",
      "description": "Co-design numerical algorithms and system implementations (GPU kernels, memory patterns, caching, offloading) so theoretical improvements yield real-world speed, memory, and latency gains under deployment constraints.",
      "key_indicators": [
        "IO-aware kernels",
        "tile, streaming pipeline",
        "KV offload",
        "cache offloading",
        "co-design"
      ],
      "cognitive_move": "Simultaneously optimize algorithms and low-level implementations, accounting for hardware/software stack to turn asymptotic ideas into practical throughput improvements.",
      "variants_merged": [
        "Numerics & Systems Co-design",
        "Design-for-Deployment",
        "Offload & Memory Strategy",
        "Contract Redesign for Scalability",
        "Cost-Aware Automation",
        "Quantization\u2013Fine-Tune Harmonization"
      ],
      "example": "Reframing \u03b1-entmax as a per-row root-finding problem and implementing Triton GPU kernels to realize adaptive sparsity at production speed (AdaSplash-like).",
      "learnable_insight": "Before claiming practical speedups, prototype a critical kernel with realistic IO patterns and measure end-to-end throughput; iterate interfaces between algorithm and implementation to resolve bottlenecks."
    },
    {
      "id": "P15",
      "name": "Data-Centric Optimization & Active Sampling",
      "category": "Data, Sampling & Efficiency",
      "description": "Treat data selection, augmentation, or synthetic generation as the primary lever for performance: optimize data mixtures, synthesize targeted examples, and use adaptive sampling/twisting to focus resources on informative regions.",
      "key_indicators": [
        "data mixture",
        "synthesize",
        "hard negatives",
        "adaptive sampling",
        "twisted proposals"
      ],
      "cognitive_move": "Shift effort from architecture changes to choosing or generating the right data; use adaptive sampling strategies to concentrate labeling or compute where it most reduces uncertainty or improves metrics.",
      "variants_merged": [
        "Replace Synthetic Heuristics with Realistic Generators",
        "Data-first Optimization",
        "Smart Sampling and Batch Construction",
        "Adaptive Sampling & Twisting",
        "Data Editing for Robustness",
        "Simulation-driven Emergence"
      ],
      "example": "Using a teacher model to build a sparse similarity graph for hard-negative mining so smaller batches achieve the effect of larger ones (B3).",
      "learnable_insight": "Instrument learning curves to find where more data would help; design synthetic or selection methods targeted to those failure modes (hard negatives, rare classes) and validate improvements on held-out realistic benchmarks."
    }
  ],
  "insights_text": "EXECUTIVE SUMMARY \u2014 key takeaways\n1. Breakthroughs are driven by problem reframing + new representations. The single most common primary thinking pattern is Gap\u2011Driven Reframing (795 / 3291 = 24.2%), and it frequently co\u2011occurs with Representation Shift & Primitive Recasting (303 co\u2011occurrences). In practice, successful papers often start by diagnosing a mismatch (the \u201cgap\u201d) and then propose a representational/primitive change to resolve it.\n\n2. Cross\u2011domain synthesis is a major engine of novelty. Cross\u2011Domain Synthesis is the second most common primary pattern (594 / 3291 = 18.0%) and pairs frequently with representation changes, inductive\u2011bias injection, and modular composition. Borrowing tools/abstractions from another field continues to produce high\u2011yield ideas.\n\n3. Formal rigor and data/benchmark engineering are essential but less frequent as first moves. Formal\u2011Experimental Tightening and Data & Evaluation Engineering are smaller slices (7.4% and 6.0% primary, respectively) yet often appear as companions to other patterns \u2014 i.e., novelty + rigorous validation is the common pathway to acceptance and impact.\n\n4. There are under\u2011exploited opportunity spaces. Patterns with low primary frequency \u2014 Multiscale & Hierarchical Modeling (51, 1.5%), Data\u2011Centric Optimization & Active Sampling (77, 2.3%), Inference\u2011Time Control & Guided Sampling (90, 2.7%) \u2014 are fertile areas for new high\u2011impact contributions, especially when combined with gap reframing and representation changes.\n\nPATTERN LANDSCAPE ANALYSIS\nWhat the most common patterns reveal\n- Culture of diagnosis-first, then redesign: The dominance of Gap\u2011Driven Reframing (24.2%) shows top ML work tends to begin with identifying an important limitation, misalignment, or unexplored angle, not just incremental improvements. The quick follow\u2011ups are representation or synthesis moves.\n- Representation inventions are the technical lever: Representation Shift & Primitive Recasting (344, 10.5%) is the most common technical response to gaps. Many breakthroughs are not primarily new optimization math but reframes of primitives (tokens \u2192 rays, images \u2192 implicit fields, etc.).\n- Cross\u2011fertilization as shortcut to novelty: Cross\u2011Domain Synthesis (18.0%) shows the field still rewards importing methods/intuition from other domains (physics, control, vision \u2194 language, etc.) to attack ML problems.\n\nWhy certain patterns are more prevalent\n- Low barrier + high payoff: Gap identification is cheap (analysis, experiments) but can unlock large conceptual gains, so it\u2019s common.\n- Transferability of representations: A new representation can be reused across tasks and datasets, making representation work publishable and widely applicable.\n- Community incentives: Conferences and reviewers prize novelty tied to demonstrable improvements and new problem formulations \u2014 favoring reframing and cross\u2011domain work.\n\nUnderutilized patterns = opportunities\n- Multiscale & Hierarchical Modeling (1.5%): Many real systems (language, vision, physics) are hierarchical; deeper development here could yield large gains in efficiency and interpretability.\n- Data\u2011Centric Optimization & Active Sampling (2.3%): With growing interest in data efficiency, explicit active sampling and data\u2011curation methods are under\u2011represented.\n- Inference\u2011Time Control & Guided Sampling (2.7%): Systems that adapt at inference to trade off compute/quality are underexplored relative to their application value.\n- Adversary Modeling & Defensive Repurposing (1.7%): Security/robustness is still small as a primary thinking pattern but will become strategically important as deployments grow.\n\nTEMPORAL EVOLUTION (2023 \u2192 2024 \u2192 2025)\nHow thinking is evolving (high level)\n- Stability at the top: Gap\u2011Driven Reframing remains steady (~26.1% \u2192 23.7% \u2192 23.8%), indicating a persistent research mode: diagnose \u2192 reframe.\n- Growing representation focus in 2024: Representation Shift rose from 8.0% (2023) to 11.5% (2024), then slightly down to 10.6% (2025). That 2024 bump reflects a wave of representational innovations (new primitives, modalities, implicit representations).\n- Decline in formal tightening as primary: Formal\u2011Experimental Tightening dropped from 10.1% (2023) to 7.1% (2024) to 6.6% (2025), suggesting formalization is more often a supporting pattern than the headline novelty.\n\nRising vs declining patterns (data highlights)\n- Rising (relative): Representation Shift had a noticeable rise in 2024. Data & Evaluation Engineering has a small rise in 2025 (6.6% in 2025 vs lower earlier), hinting increased attention to benchmarks and evaluation.\n- Declining (relative): Formal\u2011Experimental Tightening decreased as a primary move; Principled Probabilistic Modeling also shows a modest relative decline in share compared to reframing and synthesis (aggregate 6.0% overall).\n\nImplications about direction\n- The field remains empirically driven and conceptually opportunistic: stable emphasis on reframing & representation implies future breakthroughs will likely come from fresh abstractions, often enabled by cross\u2011domain insights.\n- Attention to evaluation is recovering: modest growth in data/eval work suggests the community is responding to reproducibility and benchmark saturation concerns.\n\nCONFERENCE CULTURE ANALYSIS\nDo NeurIPS, ICML, and ICLR favor different thinking styles?\n- ICLR (1019 papers): slightly more representation + data focus: Representation Shift 11.8% and Data & Evaluation Engineering 8.5% \u2014 matches ICLR\u2019s reputation for representations, systems, and empirical benchmarks.\n- ICML (763 papers): slightly more formal/diagnostic: Gap\u2011Driven Reframing 25.8% and Formal\u2011Experimental Tightening 8.3%, and stronger Principled Probabilistic Modeling (7.5%) \u2014 aligns with ICML\u2019s history of statistically grounded/algorithmic work.\n- NeurIPS (1509 papers): broadly balanced: Gap\u2011Driven 24.5%, Cross\u2011Domain Synthesis 18.5%, and Formal Tightening 8.1% \u2014 NeurIPS remains the broadest mix, favoring cross\u2011disciplinary syntheses.\n\nExplanations for differences\n- Program committees and reviewer cultures: historical identity (ICML \u2192 more formal/statistical; ICLR \u2192 representations/systems; NeurIPS \u2192 cross\u2011disciplinary) affects what gets accepted.\n- Submission strategies: researchers target venues based on where their thinking style is valued (e.g., representation work to ICLR).\n\nImplications for submission strategy\n- If your core contribution is a new representation or benchmark/data resource, prefer ICLR.\n- If your work is mathematically principled or emphasizes theoretical guarantees, ICML may be more receptive.\n- If your paper synthesizes across domains or offers broad system/empirical claims, NeurIPS is a strong fit.\n- Regardless of venue, pair the novelty with strong experimental or theoretical validation \u2014 these companion patterns increase acceptance odds.\n\nORAL vs SPOTLIGHT INSIGHTS\nObservations and limitations\n- Data available: 879 oral vs 2412 spotlight papers. You didn\u2019t provide an explicit pattern breakdown by presentation type, so the claims below are inferential and should be validated on the raw labeled set.\n- Inference from co\u2011occurrence and frequencies: Patterns most associated with high visibility (orals/spotlights historically) are Gap\u2011Driven Reframing, Representation Shift, and Cross\u2011Domain Synthesis \u2014 because they constitute the largest primary pattern shares and appear frequently together in the top co\u2011occurrences (e.g., Gap+Rep: 303).\n\nWhich patterns correlate with highest impact\n- Reframing + New Representation: Gap\u2011Driven Reframing + Representation Shift co\u2011occurs 303 times \u2014 this \u201creframe+repr\u201d recipe is the canonical path to an attention\u2011grabbing paper.\n- Cross\u2011domain + systems: Cross\u2011Domain Synthesis with Modular Pipeline Composition (106 co\u2011occurrences) correlates with work that rapidly demonstrates broad applicability (common in orals).\n- Rigor as multiplier: Principled Probabilistic Modeling + Formal\u2011Experimental Tightening (125 co\u2011occurrences) predicts work with durable technical influence (theory + empirical validation).\n\nPOWERFUL PATTERN COMBINATIONS \u2014 \u201cthinking recipes\u201d\nTop co\u2011occurrence facts (selected)\n- Gap\u2011Driven Reframing + Representation Shift: 303\n- Cross\u2011Domain Synthesis + Representation Shift: 222\n- Gap\u2011Driven Reframing + Cross\u2011Domain Synthesis: 195\n- Representation Shift + Inject Structural Inductive Bias: 138\n- Principled Probabilistic Modeling + Formal\u2011Experimental Tightening: 125\n\nWhy these combos work (recipes)\n1. Reframe \u2192 Repr \u2192 Validate (High\u2011impact model)\n   - Step 1: Diagnose an important practical/ conceptual gap (P01).\n   - Step 2: Recast the problem via a new primitive or representation (P03).\n   - Step 3: Inject inductive bias if needed (P10), and validate with rigorous experiments/theory (P04).\n   - Why it works: The novelty is conceptual and the validation prevents it from being dismissed as a toy.\n\n2. Cross\u2011domain Import \u2192 Adapt Representation \u2192 Scale (Fast path to applicability)\n   - Step 1: Identify a method from another domain that addresses your gap (P02).\n   - Step 2: Modify the representation/primitive to fit the ML setting (P03).\n   - Step 3: Address scalability via approximation engineering (P08) and systems co\u2011design.\n   - Why it works: Rapid novelty plus clear reuse pathways and engineering feasibility.\n\n3. Principled Modeling + Tight Experimentation (durable contributions)\n   - Combine probabilistic/theoretical modeling (P06) with rigorous ablation/ contamination checks (P04).\n   - Why it works: Produces reproducible, interpretable work that the community can build on.\n\nHow to deliberately combine patterns (practical checklist)\n- Begin with a one\u2011sentence \u201cgap\u201d claim: what is the persistent pain? (force Gap\u2011Driven framing)\n- Ask: Can another field\u2019s abstraction solve this? (trigger Cross\u2011Domain Synthesis)\n- If yes, reframe input/output primitives accordingly (Representation Shift).\n- Decide whether to bake in structural inductive bias for sample efficiency or interpretability.\n- Design experiments that stress the new claim and include formal checks (data contamination, worst\u2011case scenarios).\n- Add scalability or inference\u2011time adjustments to make the idea deployable.\n\nACTIONABLE ADVICE FOR RESEARCHERS\nFor PhD students starting research\n- Learn to spot gaps: practice writing concise \u201cgap statements\u201d for 10 recent papers in your subfield every week.\n- Master at least one cross\u2011domain tool (e.g., probabilistic modeling, control theory, implicit representations) and one representation family (e.g., tokens, continuous fields).\n- Start with small, clear recipe projects: (a) identify a gap, (b) propose a small representation change, (c) run focused ablations and baseline comparisons, (d) write a tight story emphasizing the reframing.\n- Publishability path: 1\u20132 solid workshop/short papers using the above recipe \u2192 conference paper when you\u2019ve matured the idea.\n\nFor experienced researchers seeking impact\n- Invest in \u201creframe + representation\u201d projects and shepherd them through rigorous validation. Senior authorship can accelerate cross\u2011disciplinary adoption.\n- Create cross\u2011domain teams: pair an empiricist with a theoretician and a domain expert to execute high\u2011leverage combos (e.g., P02 + P06 + P04).\n- Build transferable tooling and benchmarks around your innovations \u2014 these amplify reach and citations.\n- Mentor PhD students to produce mid\u2011sized, high\u2011quality deliverables that serve as validation experiments for bigger, riskier conceptual bets.\n\nFor industry researchers vs academic researchers\n- Industry researchers (constraints: deployment, latency, cost):\n  - Prioritize Inject Structural Inductive Bias (P10) and Approximation Engineering for Scalability (P08), plus Inference\u2011Time Control (P09).\n  - Focus on measurable ROI: latency reduction, memory, data labeling cost; deliver experimental ablations showing cost/benefit.\n- Academic researchers (constraints: novelty, generality):\n  - Emphasize Gap\u2011Driven Reframing, Representation Shift, Cross\u2011Domain Synthesis, and Formal\u2011Experimental Tightening. Aim for conceptual novelty and theoretical/empirical durability.\n- Both: Collaborate: industry systems expertise + academic theoretical rigor = high probability of impactful, deployable advances.\n\nMETA\u2011INSIGHTS ABOUT ML INNOVATION\nHow ML progress actually happens (empirical meta\u2011patterns)\n- Meta\u2011pattern A \u2014 Diagnose \u2192 Reframe \u2192 Represent \u2192 Validate: The prevalent successful trajectory (supported by top co\u2011occurrences and pattern frequencies) begins with a real pain point and resolves it by changing the primitives or framing, then proving it with careful experiments or formal analysis.\n- Meta\u2011pattern B \u2014 Synthesis multiplies leverage: Combining methods from another field with representation changes often yields outsized returns (Cross\u2011Domain + Representation is 222 co\u2011occurrences).\n- Meta\u2011pattern C \u2014 Validation converts novelty to impact: Formal and data/evaluation work may be secondary in frequency but acts as a multiplier for acceptance and lasting influence.\n\nPredictions for future directions (concrete)\n- More representation innovation tied to real systems: Representation Shift will stay high (\u224810%), especially for multimodal and sensor fusion problems.\n- Growth in data & evaluation engineering: Expect this pattern to grow beyond 6% as reproducibility and better benchmarks become urgent.\n- Rise of multiscale/hierarchical work and inference\u2011time control: As models scale and are deployed, Multiscale Modeling and Inference\u2011Time Control will rise from current small bases (1.5% and 2.7%) because efficiency and structured reasoning will be competitive differentiators.\n- Probabilistic principled methods will resurge as the community demands robust uncertainty and safety guarantees in deployments.\n\nCONCRETE RECOMMENDATIONS (checklist & templates)\n- Daily/weekly practice: write 1 gap statement per day for recent papers (keeps reframing muscle strong).\n- Project template (6\u201312 months):\n  1. Month 0\u20132: Gap diagnosis + literature scan (include cross\u2011domain).\n  2. Month 2\u20134: Prototype representation/primitive change on a small toy.\n  3. Month 4\u20136: Add inductive bias and scale with approximation engineering.\n  4. Month 6\u20139: Rigorous experimental suite + failure modes + statistical checks.\n  5. Month 9\u201312: Draft paper emphasizing the gap\u2192repr\u2192validate arc.\n- Paper checklist before submission:\n  - Is the gap explicit and quantified? (Yes/No)\n  - Does the solution alter a primitive/representation or import an idea from another field? (Yes/No)\n  - Are the experiments reproducible and do they stress edge cases? (Yes/No)\n  - Is there a clear story for why this generalizes beyond the dataset? (Yes/No)\n\nLIMITATIONS & NEXT STEPS\n- This analysis is based on the primary pattern label per paper and co\u2011occurrences provided. A more granular per\u2011paper multi\u2011label time series (pattern intensity per paper) would let us quantify how combined patterns predict oral acceptance, citations, or long\u2011term impact.\n- Recommendation: run a follow\u2011up analysis correlating pattern combinations with citation growth, deployment cases, and oral acceptance rates to validate and refine the \u201crecipes.\u201d\n\nSUMMARY: how to think like a top ML researcher\n- Start with a crisp, quantifiable gap. Ask \u201cwhat primitive would make this simple?\u201d Then borrow the most suitable abstraction from another domain, recast the representation, and back it with rigorous experiments or theory. If you can add scalability or inference\u2011time control, you increase adoption chances. Cultivate the ability to move between diagnosing problems and inventing abstractions \u2014 that combo is the clearest route to breakthrough work in contemporary ML."
}
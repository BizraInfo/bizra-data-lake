{
  "prior_works": [
    {
      "title": "Simplified neuron model as a principal component analyzer",
      "authors": "Erkki Oja",
      "year": 1982,
      "role": "Algorithmic foundation for streaming PCA",
      "relationship_sentence": "The paper analyzes and tailors Oja\u2019s classic online PCA update to the Markovian sampling regime, making Oja (1982) the algorithmic backbone whose behavior under dependence they must newly control."
    },
    {
      "title": "The Fast Convergence of Incremental PCA",
      "authors": "Shivani Agarwal Balsubramani, Sanjoy Dasgupta, Yoav Freund",
      "year": 2013,
      "role": "Early non-asymptotic analysis of Oja under IID data",
      "relationship_sentence": "Their potential-function/martingale-style analysis for Oja\u2019s algorithm under IID data provides the proof template and baseline guarantees that the present work adapts and extends to dependent Markovian samples."
    },
    {
      "title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate",
      "authors": "Ohad Shamir",
      "year": 2015,
      "role": "Sharper rates and stepsize design for stochastic PCA",
      "relationship_sentence": "Shamir\u2019s near-optimal IID rates and careful stepsize scheduling inform the learning-rate and sample-complexity targets that this paper seeks to match (up to mixing factors) without downsampling in the Markovian setting."
    },
    {
      "title": "First Efficient Convergence for Streaming k-PCA",
      "authors": "Zeyuan Allen-Zhu, Yuanzhi Li",
      "year": 2017,
      "role": "State-of-the-art streaming PCA analysis under IID data",
      "relationship_sentence": "Their tight non-asymptotic control of Oja/streaming PCA iterates under IID sampling is a direct benchmark; the present work generalizes analogous contraction arguments to dependent data while aiming for comparable near-optimal rates."
    },
    {
      "title": "The noisy power method: A meta algorithm with applications to PCA",
      "authors": "Moritz Hardt, Eric Price",
      "year": 2014,
      "role": "Robust spectral iteration under noise",
      "relationship_sentence": "Treating dependence-induced errors as structured noise, the paper leverages insights from the noisy power method to bound perturbations in the eigenvector iteration when updates are correlated through the Markov chain."
    },
    {
      "title": "Concentration inequalities for Markov chains by Marton couplings and spectral methods",
      "authors": "Daniel Paulin",
      "year": 2015,
      "role": "Deviation bounds under reversible Markov dependence",
      "relationship_sentence": "Paulin\u2019s spectral-gap-based concentration tools underpin the paper\u2019s control of covariance/gradient noise from a reversible Markov chain, enabling effective-sample-size scaling without resorting to downsampling."
    },
    {
      "title": "Stochastic Approximation and Recursive Algorithms and Applications",
      "authors": "Harold J. Kushner, G. George Yin",
      "year": 2003,
      "role": "Stochastic approximation with Markovian noise",
      "relationship_sentence": "Classic SA theory with Markovian noise provides the conceptual framework for analyzing Oja\u2019s stochastic recursion when data are generated by a stationary reversible Markov chain."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014near-optimal convergence guarantees for Oja\u2019s streaming PCA when data are sampled from a stationary, reversible Markov chain without downsampling\u2014sits at the intersection of three strands of prior work. First, the algorithmic and analytical backbone comes from the IID streaming PCA literature. Oja (1982) provides the update rule; Balsubramani\u2013Dasgupta\u2013Freund (2013), Shamir (2015), and Allen-Zhu\u2013Li (2017) develop non-asymptotic analyses, potential functions, and stepsize schedules that yield near-optimal rates under independence. These works set both the proof template and performance benchmarks the present paper aims to recover in a dependent setting.\nSecond, robustness of spectral iterations to stochastic perturbations is captured by the noisy power-method perspective (Hardt\u2013Price, 2014). Viewing Markovian correlations as structured noise, the authors adapt contraction arguments to bound the impact of temporally correlated updates on eigenvector estimation.\nThird, to replace ad hoc downsampling with principled use of all samples, the paper relies on concentration and stochastic-approximation tools tailored to Markov chains. Paulin (2015) provides spectral-gap-based deviation bounds for reversible chains, enabling effective sample size arguments; Kushner\u2013Yin (2003) furnishes SA foundations for Markovian noise. Combining these elements, the paper derives rates that mirror IID-optimal guarantees up to mixing-dependent factors and removes extraneous logarithmic dependencies, thereby delivering the first near-optimal analysis of Oja\u2019s algorithm on fully Markovian streams.",
  "analysis_timestamp": "2026-01-06T23:42:49.056668"
}
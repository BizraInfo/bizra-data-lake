{
  "prior_works": [
    {
      "title": "The Conditional Randomization Test: Exact Conditional Inference with a Single Conditional",
      "authors": "Emmanuel J. Cand\u00e8s, Yanran Fan, Lucas Janson, Jinchi Lv",
      "year": 2018,
      "role": "Foundational conditional independence testing framework for assessing whether a predictor adds information beyond observed covariates.",
      "relationship_sentence": "The paper\u2019s audit tests the null that outcomes are independent of expert predictions given features, mirroring the CRT\u2019s null of no added predictive value and motivating a formal hypothesis test rather than accuracy comparisons."
    },
    {
      "title": "The Hardness of Conditional Independence Testing and the Generalised Covariance Measure",
      "authors": "Rajen D. Shah, Jonas Peters",
      "year": 2018,
      "role": "Introduces a simple, regression-residual\u2013based conditional independence test with sample splitting and discusses fundamental limits.",
      "relationship_sentence": "The proposed \u2018residualize-and-test\u2019 procedure for detecting human expertise directly follows the GCM blueprint of regressing out X and testing residual association between expert predictions and outcomes."
    },
    {
      "title": "Kernel-based Conditional Independence Test and Application in Causal Discovery",
      "authors": "Kun Zhang, Jonas Peters, Dominik Janzing, Bernhard Sch\u00f6lkopf",
      "year": 2011,
      "role": "Establishes nonparametric kernel tests for conditional independence as a central tool for detecting conditional relationships.",
      "relationship_sentence": "By framing human expertise detection as a conditional independence problem, the paper builds on the KCI tradition that positions CI testing as the right formal object for \u2018extra information beyond X\u2019."
    },
    {
      "title": "Double/Debiased Machine Learning for Treatment and Structural Parameters",
      "authors": "Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins",
      "year": 2018,
      "role": "Provides orthogonalization and cross-fitting to enable valid inference when using flexible ML to control for high-dimensional covariates.",
      "relationship_sentence": "The audit leverages orthogonalization\u2014residualizing Y and expert predictions on X and testing their residual correlation\u2014an application of the DML principle to obtain robust tests with learned nuisance functions."
    },
    {
      "title": "Learning to Defer to an Expert",
      "authors": "Adam Rosenfeld, Richard Zemel, John K. Tsotsos",
      "year": 2018,
      "role": "Formalizes human\u2013AI complementarity by letting models defer decisions to experts when the expert has superior or additional information.",
      "relationship_sentence": "The audit\u2019s conceptual focus\u2014do experts possess complementary signal beyond X?\u2014is directly motivated by the defer-to-expert literature that models humans as holding private information."
    },
    {
      "title": "Human Decisions and Machine Predictions",
      "authors": "Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan",
      "year": 2018,
      "role": "Empirically compares human and algorithmic decisions and highlights pitfalls of naive accuracy comparisons in high-stakes settings.",
      "relationship_sentence": "The paper\u2019s argument that accuracy comparisons are insufficient and should be replaced by a conditional independence test is a direct response to concerns raised in this line of work."
    }
  ],
  "synthesis_narrative": "Auditing for Human Expertise reframes the question of whether experts add value as a hypothesis test of conditional independence: do expert predictions contain information about outcomes beyond what is captured in available features? This framing draws directly on the conditional independence testing literature. Kernel-based CI tests (Zhang et al., 2011) establish CI as the right object for detecting residual information, while Shah and Peters (2018) introduce a simple regression-residual\u2013based CI test\u2014the Generalised Covariance Measure\u2014that inspires the paper\u2019s practical, residualize-and-test procedure. The statistical machinery enabling valid inference with flexible nuisance models comes from double/debiased machine learning (Chernozhukov et al., 2018), whose orthogonalization and cross-fitting principles underpin residualization of both outcomes and expert predictions on features. In parallel, the Conditional Randomization Test (Cand\u00e8s et al., 2018) formalizes testing whether a predictor adds incremental signal given covariates; the audit adopts this spirit\u2014testing for added predictive value\u2014while offering a simple implementation that avoids modeling the experts\u2019 conditional distribution. Conceptually, the human\u2013AI collaboration literature, especially Learning to Defer (Rosenfeld et al., 2018), motivates treating experts as holders of private signals and asks when this signal is complementary. Finally, empirical work comparing human and machine decision-making (Kleinberg et al., 2018) underscores that raw accuracy comparisons are inadequate, pushing toward the paper\u2019s core contribution: a principled CI-based audit that detects genuine human expertise beyond the information encoded in X.",
  "analysis_timestamp": "2026-01-06T23:42:49.086567"
}
{
  "prior_works": [
    {
      "title": "Invariant Risk Minimization",
      "authors": "Arjovsky et al.",
      "year": 2020,
      "arxiv_id": "1907.02893",
      "role": "Inspiration",
      "relationship_sentence": "IRM\u2019s causal view that predictive signals decompose into invariant (content) and spurious environment-dependent (style) factors directly motivates H-NTL\u2019s explicit causal model that separates and harnesses content and style to control transferability."
    },
    {
      "title": "Multimodal Unsupervised Image-to-Image Translation",
      "authors": "Huang et al.",
      "year": 2018,
      "arxiv_id": "1804.04732",
      "role": "Inspiration",
      "relationship_sentence": "MUNIT\u2019s explicit factorization of images into a shared content code and a domain-specific style code provides the concrete latent-factor design that H-NTL repurposes to disentangle and guide content/style for non-transferable representation learning."
    },
    {
      "title": "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization",
      "authors": "Huang and Belongie",
      "year": 2017,
      "arxiv_id": "1703.06868",
      "role": "Related Problem",
      "relationship_sentence": "AdaIN\u2019s finding that channel-wise feature statistics encode style informs H-NTL\u2019s style guidance mechanism for isolating style signals from content and preventing spurious style\u2013label coupling."
    },
    {
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias in CNNs using stylized ImageNet",
      "authors": "Geirhos et al.",
      "year": 2019,
      "arxiv_id": "1811.12231",
      "role": "Gap Identification",
      "relationship_sentence": "This work exposes texture (style) bias and its brittleness under distribution shift, directly motivating H-NTL to avoid fitting spurious style\u2013label correlations when enforcing non-transferability."
    },
    {
      "title": "Non-Transferable Learning for Unsupervised Domain Adaptation",
      "authors": "Yao et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This paper formalized the NTL objective as restricting generalization to specified target domains by reducing source\u2013target statistical dependence, which H-NTL rethinks by introducing content/style causal separation to overcome its blind dependence reduction."
    },
    {
      "title": "Learning Non-Transferable Representations via Mutual Information Regularization",
      "authors": "Huang et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "As a primary NTL baseline that minimizes source\u2013target dependence via MI, it is directly improved by H-NTL, which shows MI-only objectives can induce fake content\u2013label independence and instead leverages disentangled content/style guidance."
    }
  ],
  "synthesis_narrative": "Invariant Risk Minimization (IRM) crystallized a causal perspective on generalization, positing that predictive signals decompose into invariant causal factors and spurious environment-linked factors, and that learning should target invariants. In computer vision, AdaIN demonstrated that channel-wise statistics capture style and can be manipulated to control it, while MUNIT operationalized a two-latent-factor model with content and style codes that can be disentangled and recombined. Geirhos et al. provided empirical evidence that CNNs over-rely on texture (a form of style), revealing how style\u2013label correlations can drive brittleness under distribution shifts. Within this backdrop, early Non-Transferable Learning (NTL) efforts formalized the goal of restricting generalization to specified target domains by reducing source\u2013target statistical dependence (e.g., via discrepancy or mutual information penalties), and subsequent work instantiated MI-based regularization to learn non-transferable representations. These NTL methods, however, treated all cross-domain signals uniformly, without distinguishing causal content from incidental style.\n\nTaken together, these works highlight both the need to separate content and style and the pitfalls of undifferentiated dependence reduction. The causal framing from IRM and the concrete content\u2013style factorization mechanisms from AdaIN/MUNIT suggested a natural next step: explicitly model and harness content and style as distinct latent factors while learning non-transferable features. H-NTL synthesizes these insights by introducing a causal model that disentangles content and style, using content guidance to preserve causal predictiveness and style guidance to avoid spurious correlations\u2014thereby overcoming the fake content\u2013label independence and style\u2013label spuriosity that limit prior dependence-based NTL.",
  "target_paper": {
    "title": "Improving Non-Transferable Representation Learning by Harnessing Content and Style",
    "authors": "Ziming Hong, Zhenyi Wang, Li Shen, Yu Yao, Zhuo Huang, Shiming Chen, Chuanwu Yang, Mingming Gong, Tongliang Liu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "non-transferable representation learning, domain adaptation, transfer learning",
    "abstract": "Non-transferable learning (NTL) aims to restrict the generalization of models toward the target domain(s). To this end, existing works learn non-transferable representations by reducing statistical dependence between the source and target domain. However, such statistical methods essentially neglect to distinguish between *styles* and *contents*, leading them to inadvertently fit (i) spurious correlation between *styles* and *labels*, and (ii) fake independence between *contents* and *labels*. Consequently, their performance will be limited when natural distribution shifts occur or malicious intervention is imposed. In this paper, we propose a novel method (dubbed as H-NTL) to understand and advance the NTL problem by introducing a causal model to separately model *content* and *style* as two latent factors, based on which we disentangle and harness them as guidances for learning non-transferable representations with intrinsically causal relationships. Speci\ufb01cally, to avoid fitting spu",
    "openreview_id": "FYKVPOHCpE",
    "forum_id": "FYKVPOHCpE"
  },
  "analysis_timestamp": "2026-01-06T12:08:34.247178"
}
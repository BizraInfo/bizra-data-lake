{
  "prior_works": [
    {
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",
      "year": 2019,
      "role": "Masked token prediction objective",
      "relationship_sentence": "4M generalizes BERT\u2019s masked token prediction to a unified token space spanning text, images, geometry, semantics, and feature maps, using the same core MLM-style objective across modalities."
    },
    {
      "title": "Neural Discrete Representation Learning (VQ-VAE)",
      "authors": "Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu",
      "year": 2017,
      "role": "Discrete tokenization via vector quantization",
      "relationship_sentence": "4M\u2019s unification of heterogeneous modalities into a common discrete token space builds directly on VQ-style codebooks to map continuous signals into learnable token vocabularies."
    },
    {
      "title": "Zero-Shot Text-to-Image Generation (DALL\u00b7E)",
      "authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever",
      "year": 2021,
      "role": "Multimodal discrete sequence modeling",
      "relationship_sentence": "DALL\u00b7E demonstrated training Transformers over interleaved text and discretized image tokens; 4M adopts this discrete multimodal sequencing paradigm and extends it to far more modalities and tasks."
    },
    {
      "title": "BEiT: BERT Pre-Training of Image Transformers",
      "authors": "Hangbo Bao, Li Dong, Furu Wei",
      "year": 2021,
      "role": "Masked image modeling with discrete visual tokens",
      "relationship_sentence": "4M generalizes BEiT\u2019s idea of predicting masked discrete visual tokens to a broader set of modalities, treating each as tokens to be masked and reconstructed within one model."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners (MAE)",
      "authors": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick",
      "year": 2022,
      "role": "Scalable masked reconstruction over sparse subsets",
      "relationship_sentence": "4M\u2019s efficiency\u2014training on a small randomized subset of tokens\u2014directly echoes MAE\u2019s insight that heavy masking enables scalable learning and reconstruction."
    },
    {
      "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
      "authors": "Alexei Baevski, Wei-Ning Hsu, Michael Auli",
      "year": 2022,
      "role": "Modality-agnostic self-supervised objective",
      "relationship_sentence": "4M shares data2vec\u2019s ambition of a single self-supervised objective across modalities, but instantiates it with masked token prediction in a unified discrete space."
    },
    {
      "title": "A Generalist Agent (Gato)",
      "authors": "Scott Reed, Konrad Zolna, Emilio Parisotto, Alexander Novikov, et al.",
      "year": 2022,
      "role": "Single Transformer operating over many discretized modalities",
      "relationship_sentence": "Gato showed that diverse modalities and tasks can be handled by one Transformer when represented as tokens; 4M extends this principle with masked modeling and a richer modality set for vision-centric tasks."
    }
  ],
  "synthesis_narrative": "4M\u2019s core contribution\u2014training a single encoder-decoder Transformer on masked modeling over a unified, discrete token space spanning many visual and non-visual modalities\u2014sits at the intersection of discrete tokenization, masked prediction, and generalist multimodal sequence modeling. The masked language modeling backbone from BERT provides the fundamental training signal, which 4M ports to all modalities by predicting masked tokens irrespective of source. BEiT concretized this idea for vision by predicting discrete visual tokens, and 4M generalizes that notion across images, geometry, semantics, and neural feature maps. MAE supplied the scalability insight: learning can remain effective when reconstructing from a small, randomly sampled subset of tokens; 4M adopts this to contain computational cost while broadening modality coverage. The unifying token space itself is enabled by vector-quantization methods like VQ-VAE, which turn heterogeneous continuous modalities into shared codebooks of discrete symbols. Building on the success of DALL\u00b7E in training Transformers over interleaved text\u2013image token sequences, 4M treats all modalities as sequences in a common vocabulary, allowing any-to-any masked prediction. The broader aspiration of a single model spanning many domains echoes Gato\u2019s generalist agent, while data2vec\u2019s modality-agnostic self-supervised objective informs 4M\u2019s design of a single masked modeling objective across modalities. Together, these works directly scaffold 4M\u2019s unified, scalable, massively multimodal masked modeling framework.",
  "analysis_timestamp": "2026-01-06T23:42:49.080569"
}
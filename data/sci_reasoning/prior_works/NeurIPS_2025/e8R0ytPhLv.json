{
  "prior_works": [
    {
      "title": "Learning to Optimize via Posterior Sampling",
      "authors": "Daniel Russo, Benjamin Van Roy",
      "year": 2014,
      "role": "Foundational concept (eluder dimension) and baseline analysis",
      "relationship_sentence": "Introduced the eluder dimension and used it to derive regret guarantees for bandits and RL; the present paper proves lower bounds showing standard eluder-based analyses cannot yield first-order regret and proposes a localized variant that extends this foundational framework."
    },
    {
      "title": "Parametric Bandits: The Generalized Linear Case",
      "authors": "Sarah Filippi, Olivier Capp\u00e9, Aur\u00e9lien Garivier, Csaba Szepesv\u00e1ri",
      "year": 2010,
      "role": "Model class and baseline for GLM bandits",
      "relationship_sentence": "Defines and analyzes GLM bandits (GLM-UCB); the new work establishes lower bounds on the eluder dimension specifically for GLM classes, pinpointing why standard eluder-based analyses cannot deliver first-order regret in this widely used setting."
    },
    {
      "title": "KL-UCB: an Algorithm with Logarithmic Regret for Bounded Stochastic Bandits",
      "authors": "Aur\u00e9lien Garivier, Olivier Capp\u00e9",
      "year": 2011,
      "role": "Classic benchmark for Bernoulli bandits",
      "relationship_sentence": "Provides near-optimal, problem-dependent bounds for Bernoulli bandits; the localized eluder analysis is shown to recover and sharpen these classic results, demonstrating its power in the prototypical stochastic bandit case."
    },
    {
      "title": "Asymptotically Efficient Adaptive Allocation Rules",
      "authors": "T. L. Lai, Herbert Robbins",
      "year": 1985,
      "role": "Fundamental lower bounds for stochastic bandits",
      "relationship_sentence": "Establishes minimax-optimal logarithmic regret lower bounds, especially for Bernoulli arms; the new localized notion aligns with and improves constants-level understanding relative to these canonical benchmarks."
    },
    {
      "title": "Local Rademacher Complexities",
      "authors": "Peter L. Bartlett, Olivier Bousquet, Shahar Mendelson",
      "year": 2005,
      "role": "Technique inspiration (localization of complexity measures)",
      "relationship_sentence": "Demonstrates that localizing capacity measures yields sharper, problem-dependent rates; the paper\u2019s \u2018localized eluder dimension\u2019 is a direct analogue, enabling first-order regret where global eluder analyses fail."
    },
    {
      "title": "A Bound on the Label Complexity of Agnostic Active Learning",
      "authors": "Steve Hanneke",
      "year": 2007,
      "role": "Conceptual precedent for localized complexity (disagreement coefficient)",
      "relationship_sentence": "Introduces the disagreement coefficient to capture local complexity around the target; the present work\u2019s localized eluder dimension plays an analogous role for exploration in bandits/RL under function approximation."
    },
    {
      "title": "Minimax Regret Bounds for Reinforcement Learning",
      "authors": "Mohammad Gheshlaghi Azar, Ian Osband, R\u00e9mi Munos",
      "year": 2017,
      "role": "Finite-horizon RL baseline and regret framework",
      "relationship_sentence": "Provides state-of-the-art finite-horizon RL regret bounds via UCBVI but without first-order guarantees; the localized eluder approach delivers the first genuine first-order bounds in this setting with bounded cumulative returns."
    }
  ],
  "synthesis_narrative": "The core innovation of \"Eluder dimension: localise it!\" is to demonstrate limits of standard eluder-dimension analyses and to introduce a localized eluder dimension that yields first-order regret bounds. This builds directly on Russo and Van Roy, who introduced the eluder dimension to analyze exploration with function approximation in bandits and RL. The authors identify that global eluder-based arguments, while powerful, inherently preclude first-order guarantees; they make this precise by proving lower bounds for generalized linear model (GLM) classes, a central model family originally established for bandits by Filippi, Capp\u00e9, Garivier, and Szepesv\u00e1ri. \n\nTo overcome these limits, the paper draws methodological inspiration from statistical learning theory: Bartlett, Bousquet, and Mendelson\u2019s local Rademacher complexities showed how localization of capacity measures sharpens rates, while Hanneke\u2019s disagreement coefficient offered a localized complexity lens in active learning. Mirroring these ideas, the authors define a localized eluder dimension tailored to the data-dependent region relevant to learning, enabling problem-dependent\u2014and crucially, first-order\u2014regret. \n\nThis localization immediately recovers and improves classic Bernoulli bandit results, aligning with Lai and Robbins\u2019 lower bounds and matching or tightening the guarantees of KL-UCB. Finally, in finite-horizon reinforcement learning, where prior algorithms such as UCBVI (Azar, Osband, Munos) achieved horizon- and time-dependent regret, the localized eluder framework delivers the first genuine first-order bounds when cumulative returns are bounded. Together, these works directly scaffold the paper\u2019s conceptual shift from global to localized eluder analyses and its resulting advances.",
  "analysis_timestamp": "2026-01-07T00:02:04.944622"
}
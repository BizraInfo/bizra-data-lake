{
  "prior_works": [
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Koh et al.",
      "year": 2017,
      "arxiv_id": "1703.04730",
      "role": "Foundation",
      "relationship_sentence": "This work introduced the influence-function formalism that Quad adopts to quantify each pretraining instance\u2019s effect on downstream performance, forming the core \"quality\" (importance) signal in the method."
    },
    {
      "title": "Estimating Training Data Influence by Tracing Gradient Descent (TracIn)",
      "authors": "Pruthi et al.",
      "year": 2020,
      "arxiv_id": "2002.08484",
      "role": "Extension",
      "relationship_sentence": "TracIn provided a scalable approximation to influence that Quad directly builds on and improves, addressing its computational cost/noise when applied to web-scale LLM pretraining."
    },
    {
      "title": "Beyond neural scaling laws: beating power law scaling via data pruning",
      "authors": "Sorscher et al.",
      "year": 2022,
      "arxiv_id": "2206.14486",
      "role": "Baseline",
      "relationship_sentence": "This paper established top-k per-example scoring (e.g., EL2N) as an effective pruning baseline but with no explicit diversity control, a limitation Quad targets by coupling importance with diversity-aware selection."
    },
    {
      "title": "GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning",
      "authors": "Killamsetty et al.",
      "year": 2021,
      "arxiv_id": "2012.10630",
      "role": "Inspiration",
      "relationship_sentence": "GLISTER\u2019s bi-level, generalization-aware subset selection with submodular objectives inspired Quad\u2019s paradigm of combining a quality signal with explicit diversity constraints, adapted here with influence-based scoring for LLM pretraining."
    },
    {
      "title": "Coresets for Data-efficient Training of Neural Networks",
      "authors": "Mirzasoleiman et al.",
      "year": 2020,
      "arxiv_id": "1905.12787",
      "role": "Related Problem",
      "relationship_sentence": "This work\u2019s facility-location style, gradient-based subset selection for diverse coverage informs Quad\u2019s diversity term and greedy selection strategy when marrying influence scores with diversity."
    },
    {
      "title": "Deduplicating Training Data Makes Language Models Better",
      "authors": "Lee et al.",
      "year": 2022,
      "arxiv_id": "2107.06499",
      "role": "Gap Identification",
      "relationship_sentence": "By showing redundancy in pretraining corpora harms LLMs and that semantic de-dup improves results, this paper motivates Quad\u2019s explicit diversity modeling rather than pure top-k importance selection."
    }
  ],
  "synthesis_narrative": "Influence functions introduced a principled way to estimate how upweighting a single training example would change a model\u2019s loss, turning per-example influence into a measurable notion of \"quality\" for data selection. TracIn then provided a practical approximation by accumulating gradient alignments across training checkpoints, making influence-based scoring feasible at larger scales while revealing accuracy\u2013efficiency trade-offs. In parallel, data pruning work demonstrated that simple per-example scores such as EL2N can beat scaling laws by dropping low-utility data, but these top-k strategies operate purely on a scalar importance axis. Subset-selection methods like GLISTER formalized combining a generalization-driven quality signal with submodular selection to avoid redundancy, and CRAIG operationalized diversity using facility-location objectives over gradient features with fast greedy maximization, highlighting how coverage and representativeness mitigate overfitting to narrow modes. Finally, deduplication for LLMs showed empirically that semantic redundancy in corpora degrades pretraining and that enforcing diversity improves downstream generalization, underscoring the need to look beyond raw importance.\n\nTogether these threads expose a clear opportunity: retain the theoretical fidelity of influence-based importance while explicitly preventing redundancy that plagues pure top-k selection, and do so at web scale where na\u00efve influence computation is prohibitive. Quad naturally synthesizes these insights by using influence as the quality signal and a diversity-aware objective (in the spirit of facility location/submodularity) to select subsets, while addressing TracIn-style computational burdens with a tailored, efficient influence estimator suited for LLM pretraining.",
  "target_paper": {
    "title": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models",
    "authors": "Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin Zhuang, Tianyi Bai, Qiu Jiantao, Lei Cao, Ju Fan, Ye Yuan, Guoren Wang, Conghui He",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "LLMs, data selection, influence function, diversity",
    "abstract": "Data selection is of great significance in  pretraining large language models, given the  variation in quality within the large-scale available training corpora. \nTo achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, $i.e.,$ a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-$k$ instances with the highest scores.  However, this approach has several limitations. \n(1) Calculating the accurate influence of all available data is time-consuming.\n(2) The selected data instances are not diverse enough, which may hinder the pretrained model's ability to generalize effectively to various downstream tasks.\nIn this paper, we introduce $\\texttt{Quad}$, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pretraining results.\nTo compute the influenc",
    "openreview_id": "bMC1t7eLRc",
    "forum_id": "bMC1t7eLRc"
  },
  "analysis_timestamp": "2026-01-06T10:42:39.853418"
}
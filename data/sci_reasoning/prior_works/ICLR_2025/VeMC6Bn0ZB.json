{
  "prior_works": [
    {
      "title": "End-to-End Learning to Optimize",
      "authors": "Kotary et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "This work introduced proxy optimization\u2014training neural surrogates to directly map problem instances to near-optimal decisions\u2014and the present paper directly extends that proxy idea from steady-state optimization to differential equation\u2013constrained settings via a dual-network architecture."
    },
    {
      "title": "Neural Ordinary Differential Equations",
      "authors": "Chen et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Neural ODEs provide the continuous-time modeling and adjoint-based differentiation machinery that the paper leverages to learn a neural solver for system dynamics and to backpropagate through DE constraints efficiently."
    },
    {
      "title": "Task-based End-to-End Model Learning in Stochastic Optimization",
      "authors": "Donti et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This work established training predictive models with decision-centric (task-based) losses, a principle the paper adopts to train the control proxy to optimize decision quality rather than pure predictive accuracy."
    },
    {
      "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks",
      "authors": "Brandon Amos et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "OptNet popularized differentiating through optimization layers but suffers from solver- and scale-related computational burdens; the new paper addresses this gap by replacing full differentiable solvers with a learned optimization proxy coupled to a neural DE module."
    },
    {
      "title": "Differentiable MPC for End-to-End Planning and Control",
      "authors": "Brandon Amos et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Differentiable MPC represents a principal baseline for DE-constrained decision-making by explicitly differentiating through dynamics and control optimization, which the proposed method improves upon by amortizing both control and dynamics with neural proxies for near real-time inference."
    },
    {
      "title": "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations",
      "authors": "Raissi et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "PINNs demonstrated that neural networks can satisfy differential equation constraints directly; this inspired the paper\u2019s design of a dedicated dynamics network that enforces DE constraints while being trained jointly with the control proxy."
    },
    {
      "title": "The Mathematical Theory of Optimal Processes",
      "authors": "Pontryagin et al.",
      "year": 1962,
      "role": "Foundation",
      "relationship_sentence": "Pontryagin\u2019s optimal control framework formalizes optimization under ODE constraints and underlies the adjoint sensitivity concepts that the paper exploits via neural ODE training to handle DE-constrained optimization."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014learning to solve differential equation\u2013constrained optimization in near real time by fusing a control proxy with a neural dynamics solver\u2014rests on two direct intellectual pillars. First, proxy optimization, as formalized by Kotary et al., showed that neural networks can amortize optimization by learning the map from problem instances to (near-)optimal decisions. This work directly extends that paradigm from steady-state optimization to dynamic settings by designing a dual-network architecture in which one network outputs control strategies while another enforces system dynamics. Second, Neural ODEs by Chen et al. provide the continuous-time modeling and adjoint-based backpropagation mechanism required to train the dynamics network and propagate gradients through DE constraints. Task-based learning from Donti et al. supplies the decision-centric training objective, aligning learning with downstream optimality rather than predictive accuracy. In contrast, differentiable optimization layers (OptNet) and differentiable MPC offer baselines that explicitly differentiate through solvers and dynamics but incur significant computational costs; the proposed proxy-based approach directly tackles these limitations by amortizing both optimization and integration. Finally, PINNs validate the idea of embedding DE constraints in neural training, informing the choice to explicitly model dynamics with a learned DE solver, while Pontryagin\u2019s maximum principle anchors the overall formulation of optimal control under ODE constraints. Together, these works directly enable and motivate the paper\u2019s scalable, learning-based solution to DE-constrained optimization.",
  "analysis_timestamp": "2026-01-06T23:09:26.593420"
}
{
  "prior_works": [
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2022,
      "arxiv_id": "2210.03629",
      "role": "Foundation",
      "relationship_sentence": "Cybench adopts the ReAct-style formulation of tool-using agents that interleave reasoning with executing commands, which underpins its evaluation of LM agents that operate terminals to solve cybersecurity tasks."
    },
    {
      "title": "AgentBench: Evaluating LLMs as Agents",
      "authors": "Xiao Liu et al.",
      "year": 2023,
      "arxiv_id": "2308.03688",
      "role": "Foundation",
      "relationship_sentence": "AgentBench\u2019s standardized, multi-environment agent evaluation directly informed Cybench\u2019s benchmark design for assessing end-to-end agent performance with tool access and observable action traces."
    },
    {
      "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
      "authors": "Jimenez et al.",
      "year": 2023,
      "arxiv_id": "2310.06770",
      "role": "Inspiration",
      "relationship_sentence": "SWE-bench\u2019s use of realistic, self-contained tasks with starter artifacts and automated scoring inspired Cybench\u2019s packaging of CTF challenges in containerized environments and its introduction of intermediate subtasks when full-task solve rates are low."
    },
    {
      "title": "Purple Llama: CyberSecEval",
      "authors": "Meta AI et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "CyberSecEval\u2019s focus on prompt-based and code-generation security risks highlighted the lack of end-to-end, interactive cybersecurity capability evaluations, a gap Cybench addresses with executable CTF tasks."
    },
    {
      "title": "PentestGPT: An LLM-empowered Penetration Testing Tool",
      "authors": "Zhu et al.",
      "year": 2023,
      "arxiv_id": "2308.01321",
      "role": "Baseline",
      "relationship_sentence": "PentestGPT demonstrated LM-driven penetration testing workflows on CTF-like problems, providing a primary baseline system and motivating Cybench\u2019s need for a standardized, reproducible evaluation harness."
    },
    {
      "title": "HELM: Holistic Evaluation of Language Models",
      "authors": "Percy Liang et al.",
      "year": 2022,
      "arxiv_id": "2211.09110",
      "role": "Foundation",
      "relationship_sentence": "HELM\u2019s evaluation principles and emphasis on capability\u2013risk assessment informed Cybench\u2019s framing of cybersecurity as a high-stakes capability area requiring careful, standardized measurement."
    }
  ],
  "synthesis_narrative": "ReAct introduced a concrete agent paradigm where language models iteratively reason and act with external tools, establishing the blueprint for evaluating agents that must operate terminals and parse tool outputs. AgentBench generalized this notion into a standardized, multi-environment protocol for agent evaluation, emphasizing reproducibility, transparent action traces, and comparable metrics across tasks. SWE-bench showed how to build realistic, self-contained tasks with starter artifacts and automated scoring, and crucially revealed that fully end-to-end tasks can outstrip current model capabilities, motivating the use of intermediate subtasks to capture partial progress. Purple Llama\u2019s CyberSecEval concentrated attention on cybersecurity risks in text and code generation, but did so without interactive, execution-driven tasks, thereby crystallizing a gap between prompt-level safety checks and real operational cyber capabilities. PentestGPT demonstrated the feasibility of LM-driven penetration testing on CTF-style problems, yet lacked a standardized, robust benchmark that could fairly and reproducibly compare such agents. HELM provided a principled framework for holistic, risk-aware evaluation, underscoring the need for careful measurement in high-stakes domains like cybersecurity. Together, these works suggested a clear opportunity: fuse ReAct-style tool use with AgentBench and SWE-bench methodology to create realistic, containerized CTF environments; cover the capability\u2013risk axis emphasized by HELM; and address CyberSecEval\u2019s interactivity gap while providing PentestGPT-like systems a rigorous yardstick. Cybench emerges as the natural synthesis\u2014an execution-grounded benchmark with subtasks that quantifies LM-agent cybersecurity capabilities and associated risks on professional-level challenges.",
  "target_paper": {
    "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
    "authors": "Andy K Zhang, Neil Perry, Riya Dulepet, Joey Ji, Celeste Menders, Justin W Lin, Eliot Jones, Gashon Hussein, Samantha Liu, Donovan Julian Jasper, Pura Peetathawatchai, Ari Glenn, Vikram Sivashankar, Daniel Zamoshchin, Leo Glikbarg, Derek Askaryar, Haoxiang Yang, Aolin Zhang, Rishi Alluri, Nathan Tran, Rinnara Sangpisit, Kenny O Oseleononmen, Dan Boneh, Daniel E. Ho, Percy Liang",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Language Model Agents, Benchmark, Cybersecurity, Risk",
    "abstract": "Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps",
    "openreview_id": "tc90LV0yRL",
    "forum_id": "tc90LV0yRL"
  },
  "analysis_timestamp": "2026-01-06T19:28:47.270488"
}
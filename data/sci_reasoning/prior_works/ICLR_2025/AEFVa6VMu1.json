{
  "prior_works": [
    {
      "title": "Competitive Caching with Machine Learned Advice",
      "authors": "Thodoris Lykouris and Sergei Vassilvitskii",
      "year": 2018,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work introduced the learning-augmented (consistency and robustness) paradigm that the paper adopts to guarantee optimality under perfect predictions and smooth, explicit degradation of the approximation ratio with prediction error."
    },
    {
      "title": "Improving Online Algorithms via ML Predictions",
      "authors": "Manish Purohit, Zoya Svitkina, and Ravi Kumar",
      "year": 2018,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "It provided the template of error-competitive analyses\u2014performance that interpolates between optimal with perfect advice and worst-case bounds\u2014which the paper ports from online settings to offline approximation guarantees for combinatorial problems."
    },
    {
      "title": "The Primal-Dual Method for Approximation Algorithms and its Application to Network Design Problems",
      "authors": "Michel X. Goemans and David P. Williamson",
      "year": 1995,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s generic scheme directly leverages primal\u2013dual structure by using predicted dual variables/prices to guide solution construction, reducing to optimal solutions under accurate predictions and yielding error-sensitive approximations otherwise."
    },
    {
      "title": "A Linear-Time Approximation Algorithm for the Weighted Vertex Cover Problem",
      "authors": "Reuven Bar-Yehuda and Shimon Even",
      "year": 1981,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "The classic local-ratio/primal\u2013dual vertex cover algorithm is the baseline that the method augments with predicted weights/dual prices to surpass the 2-approximation in the low-error regime without increasing running time."
    },
    {
      "title": "When Trees Collide: An Approximation Algorithm for the Steiner Tree Problem",
      "authors": "Ajit Agrawal, Philip Klein, and R. Ravi",
      "year": 1995,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Its primal\u2013dual framework for Steiner tree serves as a concrete network-design baseline that the approach refines by seeding with predicted potentials, achieving optimality with perfect predictions and smooth degradation otherwise."
    },
    {
      "title": "On the Hardness of Approximating Minimum Vertex Cover",
      "authors": "Irit Dinur and Samuel Safra",
      "year": 2005,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This inapproximability result crystallizes the barrier that motivates the paper\u2019s premise\u2014beating standard approximation thresholds (within the same time bounds) becomes possible only by leveraging predictions."
    },
    {
      "title": "Clique is hard to approximate within n^{1\u2212\u03b5}",
      "authors": "Johan H\u00e5stad",
      "year": 1999,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "The strong hardness for Maximum Clique highlights limits of polynomial-time approximation that the paper explicitly aims to surpass in the small-error regime using predictions."
    }
  ],
  "synthesis_narrative": "The learning-augmented framework was crystallized by Lykouris and Vassilvitskii, who formalized the twin desiderata of consistency (optimal with perfect advice) and robustness (graceful degradation with error). Purohit, Svitkina, and Kumar operationalized this idea through error-sensitive performance bounds, showing how guarantees can interpolate between advice-driven optimality and worst-case competitiveness. In parallel, the primal\u2013dual method of Goemans and Williamson established a unifying lens for many subset-selection problems, where dual variables (prices or potentials) guide efficient primal constructions. The local-ratio/primal\u2013dual approach of Bar-Yehuda and Even for weighted vertex cover exemplifies how simple, linear-time decisions emerge from weight decompositions or dual prices, while Agrawal, Klein, and Ravi demonstrated analogous primal\u2013dual structure for network-design tasks such as Steiner tree. Counterbalancing these algorithmic templates, Dinur and Safra\u2019s vertex-cover hardness and H\u00e5stad\u2019s clique hardness delineate sharp inapproximability barriers within polynomial time.\nSynthesizing these strands naturally suggests using predicted dual information (or weight decompositions) to steer primal\u2013dual/local-ratio routines: if predictions match the true optimal duals, one recovers optimal solutions; if not, error-sensitive analyses from learning-augmented work quantify how approximation degrades smoothly. Because primal\u2013dual/local-ratio algorithms already run in near-linear time, injecting predictions preserves time bounds while potentially surpassing classical approximation thresholds in low-error regimes, thereby addressing the hardness-driven gap without sacrificing efficiency.",
  "target_paper": {
    "title": "Approximation algorithms for combinatorial optimization with predictions",
    "authors": "Antonios Antoniadis, Marek Elias, Adam Polak, Moritz Venzin",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Approximation Algorithm, Predictions, ML-augmented, Combinatorial Optimization",
    "abstract": "We initiate a systematic study of utilizing predictions to improve over approximation guarantees of classic algorithms, without increasing the running time. We propose a generic method for a wide class of optimization problems that ask to select a feasible subset of input items of minimal (or maximal) total weight. This gives simple (near-)linear-time algorithms for, e.g., Vertex Cover, Steiner Tree, Minimum Weight Perfect Matching, Knapsack, and Maximum Clique. Our algorithms produce an optimal solution when provided with perfect predictions and their approximation ratio smoothly degrades with increasing prediction error. With small enough prediction error we achieve approximation guarantees that are beyond the reach without predictions in given time bounds, as exemplified by the NP-hardness and APX-hardness of many of the above problems. Although we show our approach to be optimal for this class of problems as a whole, there is a potential for exploiting specific structural propertie",
    "openreview_id": "AEFVa6VMu1",
    "forum_id": "AEFVa6VMu1"
  },
  "analysis_timestamp": "2026-01-06T14:27:57.389534"
}
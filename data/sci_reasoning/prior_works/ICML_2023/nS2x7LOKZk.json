{
  "prior_works": [
    {
      "title": "Inference and Missing Data",
      "authors": "Donald B. Rubin et al.",
      "year": 1976,
      "role": "Foundation",
      "relationship_sentence": "Rubin\u2019s formulation of MCAR/MAR/MNAR and the concept of ignorability provides the formal framework the paper adopts to define \u201cinformative labels\u201d (MNAR) and to justify explicitly modeling the labeling mechanism P(s|x,y)."
    },
    {
      "title": "Estimation of Regression Coefficients When Some Regressors Are Not Always Observed",
      "authors": "James M. Robins et al.",
      "year": 1994,
      "role": "Foundation",
      "relationship_sentence": "Robins et al. introduced inverse probability weighting for missing-data problems, the exact estimation principle this paper uses to debias SSL objectives via inverse propensity weighting of labeled examples."
    },
    {
      "title": "Learning Classifiers from Only Positive and Unlabeled Data",
      "authors": "Charles Elkan et al.",
      "year": 2008,
      "role": "Inspiration",
      "relationship_sentence": "Elkan & Noto\u2019s selection-model view of PU learning\u2014estimating the probability of observing a label and correcting training accordingly\u2014directly inspires this paper\u2019s idea to estimate the missing-label mechanism and reweight losses."
    },
    {
      "title": "Positive-Unlabeled Learning with Non-Negative Risk Estimator",
      "authors": "Masashi Kiryo et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "Kiryo et al. provided unbiased (and stabilized) risk estimators for PU via importance weighting; the present work generalizes this unbiased-risk correction from PU to general multiclass SSL with MNAR labeling and integrates it into modern SSL pipelines."
    },
    {
      "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback",
      "authors": "Adith Swaminathan et al.",
      "year": 2015,
      "role": "Related Problem",
      "relationship_sentence": "The CRM framework shows how inverse propensity weighting can wrap arbitrary learners to yield unbiased learning under selective feedback, directly informing this paper\u2019s \"debias any SSL algorithm\" design."
    },
    {
      "title": "Realistic Evaluation of Deep Semi-Supervised Learning Algorithms",
      "authors": "Avital Oliver et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Oliver et al. demonstrated that standard SSL assumptions and evaluations break under class distribution mismatch, motivating this paper\u2019s explicit modeling/testing of informative labeling and principled correction via propensity weighting."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "authors": "Kihyuk Sohn et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "FixMatch is a primary SSL baseline the paper debiases by plugging in inverse-propensity weights, showing the method\u2019s ability to correct augmentation-based SSL when labels are MNAR."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014estimate the missing-label mechanism and use inverse propensity weighting (IPW) to debias semi-supervised learning (SSL)\u2014rests on the missing-data theory of Rubin, which defines MNAR and ignorability and makes clear why modeling P(s|x,y) is essential when labels are informative. Building on this foundation, Robins et al. introduced IPW for missing data, providing the precise estimation tool the authors repurpose to construct unbiased SSL objectives by reweighting labeled samples with inverse estimated propensities. The intellectual bridge from selective observation to practical debiasing comes from PU learning: Elkan & Noto\u2019s selection-model perspective and Kiryo et al.\u2019s unbiased (non-negative) risk estimators demonstrate how estimating label-observation probabilities can correct training bias\u2014ideas this work extends from binary PU to general multiclass SSL with MNAR mechanisms and modern training loops. In parallel, the counterfactual risk minimization framework of Swaminathan & Joachims shows that IPS can be wrapped around arbitrary learners, directly motivating the paper\u2019s claim that any SSL algorithm\u2014even augmentation-heavy ones\u2014can be debiased via propensity weighting. Finally, empirical evidence from Oliver et al. exposes that standard SSL methods fail under distribution mismatches typical of real labeling processes, providing the gap this work targets, while FixMatch serves as a strong baseline to demonstrate plug-and-play IPW debiasing and the proposed likelihood-ratio test for informativeness.",
  "analysis_timestamp": "2026-01-06T23:09:26.520391"
}
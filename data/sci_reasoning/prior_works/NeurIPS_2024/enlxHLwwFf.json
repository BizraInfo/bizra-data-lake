{
  "prior_works": [
    {
      "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning",
      "authors": "Luca Franceschi, Michele Grazzi, Massimiliano Pontil, Paolo Frasconi",
      "year": 2018,
      "role": "Foundational bilevel formulation in ML",
      "relationship_sentence": "Established the modern ML view of bilevel optimization in parameter space for HPO/meta-learning, providing the template that the current paper generalizes by moving the inner problem to function space to avoid strong convexity in parameters."
    },
    {
      "title": "Hyperparameter Optimization with Approximate Gradient",
      "authors": "Fabian Pedregosa",
      "year": 2016,
      "role": "Implicit differentiation for hypergradients under convexity",
      "relationship_sentence": "Showed how to compute hypergradients via implicit differentiation assuming (strong) convexity of the inner problem; the present work extends this idea by carrying out differentiation in function space to remove parametric strong-convexity requirements."
    },
    {
      "title": "A Generalized Representer Theorem",
      "authors": "Bernhard Sch\u00f6lkopf, Ralf Herbrich, Alex J. Smola",
      "year": 2001,
      "role": "Theoretical tool enabling finite-dimensional reduction",
      "relationship_sentence": "The representer theorem underpins the paper\u2019s scalable algorithms by guaranteeing that solutions to regularized functional problems lie in finite spans, enabling tractable functional bilevel optimization."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Bridge between overparameterized nets and function-space learning",
      "relationship_sentence": "Provides the justification that over-parameterized neural networks implement kernel gradient descent in function space, motivating the paper\u2019s claim that functional bilevel methods can be realized with wide neural networks."
    },
    {
      "title": "Greedy Function Approximation: A Gradient Boosting Machine",
      "authors": "Jerome H. Friedman",
      "year": 2001,
      "role": "Functional gradient methodology",
      "relationship_sentence": "Introduced functional gradient descent ideas for building predictors directly in function space, which the paper leverages to design functional inner-loop optimization and corresponding hypergradient computations."
    },
    {
      "title": "Nonparametric Instrumental Regression",
      "authors": "St\u00e9phane Darolles, Jean-Pierre Florens, Eric Renault",
      "year": 2011,
      "role": "Function-space formulation of IV as inverse problem",
      "relationship_sentence": "Frames instrumental variables regression as a regularized operator inversion in function spaces, directly informing the paper\u2019s functional bilevel treatment and its application to instrumental regression."
    },
    {
      "title": "Deep IV: A Flexible Approach for Counterfactual Prediction",
      "authors": "Max K. Hartford, James R. Lewis, Kevin Leyton-Brown, Matt Taddy",
      "year": 2017,
      "role": "Application benchmark linking IV and deep learning",
      "relationship_sentence": "Demonstrates deep-learning approaches to IV via two-stage procedures; the new work provides a principled functional bilevel alternative that unifies and potentially improves such approaches by optimizing over function spaces."
    }
  ],
  "synthesis_narrative": "Functional Bilevel Optimization for Machine Learning departs from standard parametric bilevel methods by placing the inner optimization directly in a function space, thereby avoiding the need for strong convexity with respect to network parameters. Prior parametric bilevel frameworks for hyperparameter optimization and meta-learning (Franceschi et al., Pedregosa) crystallized the bilevel template and popularized hypergradient computation\u2014often hinging on smoothness and convexity assumptions. The present work preserves the bilevel structure while shifting differentiation and optimization to the function space, overcoming those parametric limitations. This shift is made algorithmically viable by classical function-space tools: the generalized representer theorem (Sch\u00f6lkopf et al.) ensures finite-dimensional characterizations of regularized functional solutions, enabling scalable algorithms; and functional gradient methods (Friedman) provide practical procedures to descend in function space.\nCrucially, insights from overparameterized neural networks via the Neural Tangent Kernel (Jacot et al.) justify realizing the functional viewpoint with wide nets that behave like kernel methods, aligning the proposed functional inner optimization with tractable, stable dynamics. The paper\u2019s applications are grounded in domains naturally expressed in function spaces: nonparametric instrumental regression (Darolles et al.) formulates IV as an ill-posed inverse problem with regularization, directly compatible with the proposed framework; and deep-learning-based IV (DeepIV) provides a compelling baseline that the functional bilevel approach can unify and refine. Collectively, these works contribute the conceptual, theoretical, and algorithmic foundations that enable a scalable, function-space bilevel methodology accommodating overparameterized models.",
  "analysis_timestamp": "2026-01-06T23:33:36.275998"
}
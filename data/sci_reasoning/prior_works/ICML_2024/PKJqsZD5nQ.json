{
  "prior_works": [
    {
      "title": "Reverse Curriculum Generation for Reinforcement Learning",
      "authors": "Florensa et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "RICE adopts the central idea of shaping the agent\u2019s start-state distribution to overcome sparse-reward exploration, generalizing Reverse Curriculum\u2019s reset-from-informative-states concept by selecting those states via explanations rather than hand-crafted proximity-to-goal heuristics."
    },
    {
      "title": "Go-Explore: a New Approach for Hard-Exploration Problems",
      "authors": "Ecoffet et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "The principle of returning to promising states to continue exploration in Go-Explore directly inspires RICE\u2019s strategy of reseeding rollouts from \"critical\" states, with RICE replacing cell/heuristic-based archives by explanation-guided state selection and integrating it into online training with theoretical guarantees."
    },
    {
      "title": "Hindsight Experience Replay",
      "authors": "Andrychowicz et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "HER is a primary sparse-reward baseline that RICE seeks to surpass; RICE addresses HER\u2019s limitation of needing to first encounter success by proactively modifying the initial-state distribution using explanation-identified critical states to break early training bottlenecks."
    },
    {
      "title": "Visualizing and Understanding Atari Agents",
      "authors": "Greydanus et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "This paper established saliency-based explanations for RL policies/values, providing the methodological basis that RICE leverages to score and extract \"critical\" states along trajectories for reset-based refinement."
    },
    {
      "title": "Axiomatic Attribution for Deep Networks (Integrated Gradients)",
      "authors": "Sundararajan et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "RICE operationalizes gradient-based attribution (e.g., Integrated Gradients) to quantify a state\u2019s influence on the policy/value outputs, directly extending this attribution technique to construct the mixed initial-state distribution."
    },
    {
      "title": "Automatic Discovery of Subgoals in Reinforcement Learning Using Diverse Density",
      "authors": "McGovern et al.",
      "year": 2001,
      "role": "Related Problem",
      "relationship_sentence": "Early subgoal/bottleneck discovery work motivates RICE\u2019s notion of \"critical states\"; RICE updates this idea by using modern attribution-based explanations to identify such states and by using them explicitly as reset points to improve exploration."
    }
  ],
  "synthesis_narrative": "RICE\u2019s core idea\u2014breaking deep RL training bottlenecks by altering the initial-state distribution\u2014stands on a lineage that begins with Reverse Curriculum Generation, which proved that appropriately chosen start states can dramatically ease exploration in sparse reward tasks. Go-Explore then crystallized the power of returning to promising states, demonstrating state reseeding as a path to overcoming hard-exploration plateaus. RICE synthesizes these insights but removes heuristic or domain-specific criteria by using explanation methods to automatically identify which encountered states are truly critical for progress, and then mixing them into the reset distribution.\n\nTwo strands of prior work enable this shift. First, explainability for RL, inaugurated by saliency-based methods such as Greydanus et al., shows how to quantify which parts of a trajectory drive a policy/value prediction. Second, principled attribution techniques like Integrated Gradients provide a robust, model-agnostic way to score state importance. RICE extends these attribution tools from passive explanation to active training control, using them to construct a refined initial-state distribution and to underpin a tighter sub-optimality bound.\n\nFinally, classical subgoal discovery (McGovern & Barto) framed the importance of bottleneck states for efficient exploration, while modern sparse-reward baselines like HER revealed practical limits when agents fail to stumble upon successes. RICE directly addresses that gap by reseeding from explanation-identified critical states, yielding a general, theoretically grounded refinement scheme that improves over these baselines.",
  "analysis_timestamp": "2026-01-06T23:09:26.440954"
}
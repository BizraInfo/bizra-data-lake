{
  "prior_works": [
    {
      "title": "Summarizing Books with Human Feedback",
      "authors": "Jeff Wu et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This work popularized the recursive chunk-and-merge approach for book-length summarization that BooookScore instantiates as one of its two prompting workflows and evaluates for coherence at book scale."
    },
    {
      "title": "BookSum: A Collection of Datasets for Long-form Narrative Summarization",
      "authors": "Tanya Goyal et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "BookSum established book-length summarization as a task but its presence in common LLM pretraining corpora created contamination concerns that BooookScore explicitly addresses by curating recent books for unbiased evaluation."
    },
    {
      "title": "Overview of the TAC 2008 Update Summarization Task",
      "authors": "Hoa Trang Dang and Karolina Owczarzak",
      "year": 2008,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The TAC update summarization paradigm directly motivates BooookScore\u2019s second workflow\u2014incrementally updating a running summary as new chunks arrive\u2014adapting the \u2018update\u2019 notion to a single long document."
    },
    {
      "title": "SummEval: Re-evaluating Summarization Evaluation",
      "authors": "Alexander Fabbri et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "SummEval showed that standard automatic metrics poorly capture human judgments, a limitation BooookScore revisits in the LLM era by demonstrating these metrics\u2019 failure to detect long-range coherence errors in book summaries."
    },
    {
      "title": "FRANK: A Benchmark for Factuality Evaluation in Abstractive Summarization",
      "authors": "Artidoro Pagnoni et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "FRANK\u2019s sentence-level factuality error taxonomy for news highlights evaluation blind spots (e.g., narrative coherence and cross-document entity tracking) that BooookScore fills with a book-scale coherence error taxonomy."
    },
    {
      "title": "SummaC: Re-Visiting NLI-based Models for Factual Consistency Evaluation in Summarization",
      "authors": "Philippe Laban et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "As a leading NLI-based consistency metric, SummaC serves as an automatic evaluation baseline that BooookScore tests and finds insufficient for detecting multi-chapter coherence failures in LLM-generated book summaries."
    }
  ],
  "synthesis_narrative": "Recursive, chunk-based summarization emerged as a practical strategy for book-length inputs with Summarizing Books with Human Feedback, which operationalized a hierarchical merge of chunk summaries to handle context limits. In parallel, the TAC 2008 Update Summarization task formalized an incremental \u2018update\u2019 paradigm in which a system maintains and revises a running summary as new information arrives, establishing a template for iterative integration of content. BookSum then anchored the community around long-form narrative summarization by providing aligned book/chapter summaries at scale, while SummEval exposed systemic gaps in automatic metrics\u2019 ability to reflect human judgments. FRANK deepened this critique by cataloging factuality errors at the sentence level for news, but it left unaddressed the cross-chapter coherence phenomena central to narratives (e.g., character tracking, temporal consistency). NLI-based metrics like SummaC offered stronger factuality checks than lexical overlap, yet remained local in scope and struggled with long-range dependencies and global coherence in multi-section texts. Together these works revealed a clear opportunity: evaluate book-length summarization in the LLM era using the two dominant decomposition workflows\u2014hierarchical merge and incremental update\u2014on uncontaminated books, and examine coherence beyond sentence-level factuality. BooookScore synthesizes these strands by curating recent books, instantiating both workflows with state-of-the-art LLMs, and constructing a fine-grained, narrative-focused coherence error taxonomy, thereby providing the first systematic study of coherence failures specific to book-scale LLM summarization.",
  "target_paper": {
    "title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs",
    "authors": "Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "summarization, evaluation, long context, prompting, LLM",
    "abstract": "Summarizing book-length documents ($>$100K tokens)  that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. ",
    "openreview_id": "7Ttk3RzDeu",
    "forum_id": "7Ttk3RzDeu"
  },
  "analysis_timestamp": "2026-01-06T06:22:34.358487"
}
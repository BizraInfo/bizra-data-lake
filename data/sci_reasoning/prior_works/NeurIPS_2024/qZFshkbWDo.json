{
  "prior_works": [
    {
      "title": "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain",
      "authors": [
        "Tianyu Gu",
        "Brendan Dolan-Gavitt",
        "Siddharth Garg"
      ],
      "year": 2017,
      "role": "Foundational backdoor attack formulation and evaluation protocol",
      "relationship_sentence": "Established the modern backdoor threat model and ASR metric that this paper problematizes, enabling the authors to argue that low post-defense ASR can be a superficial notion of safety."
    },
    {
      "title": "Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks",
      "authors": [
        "Kang Liu",
        "Brendan Dolan-Gavitt",
        "Siddharth Garg"
      ],
      "year": 2018,
      "role": "Representative model-purification defense",
      "relationship_sentence": "As a canonical purification method, Fine-Pruning is a primary baseline whose reduced-ASR models the paper shows remain vulnerable to rapid backdoor re-learning with a few poisoned samples."
    },
    {
      "title": "Spectral Signatures in Backdoor Attacks",
      "authors": [
        "Brandon Tran",
        "Jerry Li",
        "Aleksander M\u0105dry"
      ],
      "year": 2018,
      "role": "Data sanitization/purification via feature-spectrum outlier removal",
      "relationship_sentence": "This defense exemplifies data-centric purification; the current paper evaluates such methods and explains why residual backdoor-sensitive subspaces can persist and enable quick reactivation."
    },
    {
      "title": "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks",
      "authors": [
        "Bolun Wang",
        "Yuanshun Yao",
        "Shawn Shan",
        "Huiying Li",
        "Bimal Viswanath",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "year": 2019,
      "role": "Trigger inversion\u2013based detection and mitigation",
      "relationship_sentence": "Provides a widely adopted purification baseline; the paper demonstrates that even after Neural Cleanse-based repair, models can quickly relearn backdoors, motivating a new robustness criterion beyond ASR."
    },
    {
      "title": "Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks",
      "authors": [
        "Yiming Li",
        "Tongqing Zhai",
        "Baoyuan Wu",
        "Yong Jiang",
        "Zhifeng Li",
        "Shu-Tao Xia"
      ],
      "year": 2021,
      "role": "State-of-the-art fine-tuning\u2013based backdoor unlearning",
      "relationship_sentence": "As a strong purification/unlearning method, NAD is directly critiqued under the paper\u2019s post-purification robustness lens, revealing that ASR reductions do not guarantee immunity to few-shot re-poisoning."
    },
    {
      "title": "Latent Backdoor Attacks on Deep Neural Networks",
      "authors": [
        "Yuanshun Yao",
        "Huiying Li",
        "Haitao Zheng",
        "Ben Y. Zhao"
      ],
      "year": 2019,
      "role": "Backdoors that persist and (re)emerge under transfer/fine-tuning",
      "relationship_sentence": "Introduces the idea that backdoor features can remain latent and be reactivated by downstream training, a key conceptual precursor to the paper\u2019s finding that purified models relearn backdoors rapidly."
    },
    {
      "title": "Label-Consistent Backdoor Attacks",
      "authors": [
        "Alexander Turner",
        "Dimitris Tsipras",
        "Aleksander M\u0105dry"
      ],
      "year": 2019,
      "role": "Sample-efficient backdoor injection under clean-label constraints",
      "relationship_sentence": "Demonstrates that very few poisoned samples can implant a backdoor, directly motivating the paper\u2019s few-shot post-purification re-poisoning evaluation."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014exposing the superficial nature of many backdoor purification defenses and formalizing post-purification robustness\u2014builds on two converging lines of prior work. First, BadNets codified the modern backdoor threat and ASR evaluation, which defenses like Fine-Pruning, Spectral Signatures, and Neural Cleanse were designed to defeat by driving ASR down on held-out tests. More recent unlearning approaches such as Neural Attention Distillation advanced this defensive paradigm by refining model-centric mitigation via fine-tuning and knowledge transfer. These methods collectively form the class of \u201csafety purification\u201d techniques scrutinized in the paper. Second, attack-side insights foreshadowed that backdoor features can be stubborn and sample-efficient: Latent Backdoor Attacks showed that poisoned features may persist and re-emerge under transfer or further training, while Label-Consistent/Clean-label attacks demonstrated that only a handful of poisoned examples can install a backdoor. Integrating these observations, the paper argues that low ASR after purification does not mean the backdoor representation is eliminated; rather, a vulnerable subspace often remains, enabling rapid re-learning with very small poisoned fine-tuning sets. By systematically evaluating leading purification defenses under this few-shot re-poisoning regime and providing an explanation rooted in lingering backdoor-sensitive features, the authors motivate a stronger criterion\u2014post-purification robustness\u2014and propose a practical mitigation aimed at reducing the model\u2019s susceptibility to reactivation, moving the field beyond ASR-centric notions of safety.",
  "analysis_timestamp": "2026-01-06T23:33:36.270889"
}
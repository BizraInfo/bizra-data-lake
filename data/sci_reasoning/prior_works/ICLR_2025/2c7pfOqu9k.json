{
  "prior_works": [
    {
      "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "authors": "Tri Dao et al.",
      "year": 2022,
      "arxiv_id": "2205.14135",
      "role": "Extension",
      "relationship_sentence": "DeFT generalizes FlashAttention\u2019s IO-aware tiling to a tree setting by reorganizing KV tiles across branches so that shared-prefix KV blocks are loaded once and reused during attention."
    },
    {
      "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
      "authors": "Tri Dao et al.",
      "year": 2023,
      "arxiv_id": "2307.08691",
      "role": "Inspiration",
      "relationship_sentence": "DeFT adopts FlashAttention-2\u2019s principles of split-K style partitioning and improved load balancing, applying them to prefix-grouped KV partitions to sustain GPU occupancy in tree attention."
    },
    {
      "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
      "authors": "Zhanghao Wu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "DeFT addresses PagedAttention\u2019s shortcoming of reloading shared-prefix KV pages across sibling branches by introducing KV-guided grouping that reads each shared KV block only once per decoding step."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "arxiv_id": "2305.10601",
      "role": "Foundation",
      "relationship_sentence": "DeFT targets the tree-structured generation pattern with shared token prefixes formalized by Tree of Thoughts, providing the problem setting for tree-aware attention and cache reuse."
    },
    {
      "title": "Fast Inference from Transformers via Speculative Decoding",
      "authors": "Yair Leviathan et al.",
      "year": 2023,
      "arxiv_id": "2302.01318",
      "role": "Foundation",
      "relationship_sentence": "DeFT optimizes the draft\u2013verify branching structure introduced by speculative decoding by eliminating redundant KV-cache IO on shared prefixes across candidate branches."
    }
  ],
  "synthesis_narrative": "IO-aware attention kernels established that attention is often memory-bound and that carefully tiling queries, keys, and values can drastically reduce off-chip reads and writes; FlashAttention made this concrete by streaming blocks of K/V through on-chip memory and never materializing the full attention matrix, while FlashAttention-2 showed that better work partitioning (e.g., split-K and head- and sequence-parallel tiling) is key to high GPU utilization. In parallel, serving systems introduced PagedAttention to manage KV caches as virtualized pages that support variable-length requests and prefix caching, but their unit of reuse remained per sequence, not per shared-prefix subtree, leading to repeated loads of the same KV pages across branches. Meanwhile, algorithmic advances such as Tree of Thoughts formalized tree-structured generation in which many branches share long token prefixes, and speculative decoding created draft\u2013verify trees where multiple candidate continuations reuse the same prefix KV states, revealing a compute and IO pattern characterized by heavy prefix reuse and branching verification.\nTaken together, these lines of work highlighted both an opportunity and a gap: attention is IO-bound and benefits from tiling and balanced partitioning, yet existing kernels and cache managers are sequence-centric and ignore subtree structure, causing redundant KV reads and poor load balance across branches. DeFT synthesizes IO-aware tiling with the prefix-tree structure implied by ToT and speculative decoding, introducing KV-guided grouping to read shared KV blocks once and a load-balanced partitioning scheme that preserves GPU occupancy for tree attention, thereby addressing the core inefficiencies exposed by PagedAttention-style systems.",
  "target_paper": {
    "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference",
    "authors": "Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "LLM inference, attention, memory-efficiency, tree-based decoding",
    "abstract": "Large language models (LLMs) are increasingly employed for complex tasks that process multiple generation calls in a tree structure with shared prefixes of tokens, including few-shot prompting, multi-step reasoning, speculative decoding, etc. However, existing inference systems for tree-based applications are inefficient due to improper partitioning of queries and KV cache during attention calculation.This leads to two main issues: (1) a lack of memory access (IO) reuse for KV cache of shared prefixes, and (2) poor load balancing.As a result, there is redundant KV cache IO between GPU global memory and shared memory, along with low GPU utilization. To address these challenges, we propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient attention algorithm with prefix-aware and load-balanced KV cache partitions. DeFT reduces the number of read/write operations of KV cache during attention calculation through **KV-Guided Grouping**, a method that avoids repeatedly loading K",
    "openreview_id": "2c7pfOqu9k",
    "forum_id": "2c7pfOqu9k"
  },
  "analysis_timestamp": "2026-01-06T13:25:48.575802"
}
{
  "prior_works": [
    {
      "title": "The Moral Machine experiment",
      "authors": "Edmond Awad, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph Henrich, Azim Shariff, Jean-Fran\u00e7ois Bonnefon, Iyad Rahwan",
      "year": 2018,
      "role": "Dataset and problem framing",
      "relationship_sentence": "Provides the large-scale, cross-cultural trolley-problem judgments and the six moral dimensions (species, gender, fitness, status, age, number) that this paper extends into a multilingual LLM benchmark."
    },
    {
      "title": "A Voting-Based System for Ethical Decision Making",
      "authors": "Srinivasan Noothigattu, Shlomo Zilberstein, et al. (incl. Iyad Rahwan, Ariel D. Procaccia)",
      "year": 2018,
      "role": "Modeling moral preferences from Moral Machine",
      "relationship_sentence": "Demonstrates how Moral Machine preferences can be aggregated and modeled computationally, informing this work\u2019s operationalization of moral dimensions and preference estimation for comparison with LLM outputs."
    },
    {
      "title": "Delphi: Towards Machine Ethics and Norms",
      "authors": "Maarten Sap, Saadia Gabriel, Honglak Lee, Dan Jurafsky, Noah A. Smith, Yejin Choi, et al.",
      "year": 2022,
      "role": "Normative judgment evaluation for language models",
      "relationship_sentence": "Establishes methodology for eliciting and evaluating moral/normative judgments from language models, directly influencing this paper\u2019s evaluation protocols for LLM moral alignment."
    },
    {
      "title": "Whose Opinions Do Language Models Reflect?",
      "authors": "Shibani Santurkar, Esin Durmus, Katherine A. Heller, Percy Liang, Tatsunori B. Hashimoto, et al.",
      "year": 2023,
      "role": "Linking LLM outputs to demographic distributions",
      "relationship_sentence": "Shows systematic correspondence between LLM outputs and specific population subgroups, motivating this paper\u2019s correlation of LLM moral preferences with demographic distributions of language speakers."
    },
    {
      "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization",
      "authors": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson",
      "year": 2020,
      "role": "Multilingual benchmarking methodology",
      "relationship_sentence": "Provides principles and practices for large-scale multilingual evaluation and cross-lingual consistency checks that underpin this work\u2019s design of MultiTP and cross-language comparison protocols."
    },
    {
      "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
      "authors": "Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh",
      "year": 2020,
      "role": "Behavioral robustness and paraphrase invariance",
      "relationship_sentence": "Introduces invariance/robustness testing (including paraphrase consistency) that directly informs this paper\u2019s assessment of LLM sensitivity to prompt paraphrasing in moral dilemmas."
    },
    {
      "title": "No Language Left Behind: Scaling Human-Centered Machine Translation",
      "authors": "NLLB Team (Meta AI)",
      "year": 2022,
      "role": "Enabler for high-coverage multilingual data creation",
      "relationship_sentence": "Provides translation resources and methodology that make feasible the construction of a 100+ language corpus like MultiTP for consistent cross-lingual moral evaluation."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014evaluating LLM moral alignment across 100+ languages via a multilingual trolley-problem corpus\u2014rests directly on the Moral Machine experiment, which supplies both the foundational paradigm and the six moral dimensions to be measured. Earlier computational work that modeled Moral Machine judgments, such as Noothigattu et al., demonstrated how to operationalize and aggregate population preferences, informing how this study quantifies and compares moral tendencies. From the LLM side, Delphi established practical methodology for eliciting and scoring models\u2019 normative judgments, shaping the protocols by which this paper probes models\u2019 moral decisions. A key innovation here is connecting moral alignment to speaker demographics; Santurkar et al. showed that LLM outputs systematically reflect specific populations, providing both motivation and analytic scaffolding for correlating language-specific moral preferences with demographic distributions.\nMethodologically, the multilingual scope draws on XTREME\u2019s principles for cross-lingual benchmarking\u2014covering language coverage, comparability, and consistency\u2014and leverages translation advances like NLLB to build and quality-control a high-coverage multilingual corpus. Finally, the paper\u2019s robustness checks for prompt paraphrasing are grounded in behavioral testing from CheckList, which formalized invariance testing (e.g., paraphrase consistency). Together, these works enable the paper\u2019s central advance: a rigorous, multilingual, behaviorally grounded evaluation that reveals substantial cross-lingual variance in LLM moral alignment and highlights the intersection of ethical and linguistic biases.",
  "analysis_timestamp": "2026-01-06T23:42:48.088836"
}
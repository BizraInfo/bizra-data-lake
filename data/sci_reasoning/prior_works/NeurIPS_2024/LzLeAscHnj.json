{
  "prior_works": [
    {
      "title": "Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu; Yelong Shen; Phillip Wallis; Zeyuan Allen-Zhu; Yuanzhi Li; Lu Wang; Weizhu Chen",
      "year": 2022,
      "role": "Introduced LoRA, the canonical low-rank weight-update formulation for parameter-efficient fine-tuning (PEFT).",
      "relationship_sentence": "HRA explicitly connects its Householder-based orthogonal updates to LoRA by showing that a product of reflections induces an adaptive low-rank modification, thereby unifying orthogonal and low-rank adaptation within the same update space."
    },
    {
      "title": "Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections",
      "authors": "Zakaria Mhammedi; Andrew Hellicar; James Bailey; Richard Nock",
      "year": 2017,
      "role": "Provided a scalable, differentiable parameterization of orthogonal matrices as products of learnable Householder reflections.",
      "relationship_sentence": "HRA adopts the reflector-chain parametrization to build trainable orthogonal adapters and leverages the rank\u20111 nature of each reflection in its capacity analysis and equivalence to low-rank updates."
    },
    {
      "title": "Improving Variational Auto-Encoders using Householder Flow",
      "authors": "Jakub M. Tomczak; Max Welling",
      "year": 2017,
      "role": "Demonstrated stacking Householder reflections to realize expressive, efficient orthogonal transformations with simple rank\u20111 building blocks.",
      "relationship_sentence": "The insight that compositions of reflections are efficient and expressive underpins HRA\u2019s design and supports the view that chained reflections correspond to low-rank adaptations."
    },
    {
      "title": "Can We Gain More from Orthogonality in Training Deep Networks?",
      "authors": "Nitin Bansal; Xiaohan Chen; Zhiyun Li",
      "year": 2018,
      "role": "Systematically studied the benefits of orthogonality and proposed orthogonality regularization to stabilize training and improve generalization.",
      "relationship_sentence": "HRA\u2019s regularization on the mutual orthogonality of reflection planes is motivated by this line of work showing that orthogonality constraints can control capacity and enhance stability."
    },
    {
      "title": "Orthogonal Weight Normalization: Solution to Deep Neural Networks",
      "authors": "Lei Huang; Dihong Gong; Tianyi Zhou; Zhifeng Li; Rong Jin",
      "year": 2018,
      "role": "Introduced methods to enforce/encourage orthogonality of weight matrices during training for better conditioning and generalization.",
      "relationship_sentence": "HRA\u2019s emphasis on maintaining well-conditioned orthogonal transforms and employing explicit orthogonality regularizers echoes principles established by orthogonal weight normalization."
    },
    {
      "title": "Parameter-Efficient Transfer Learning for NLP",
      "authors": "Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin de Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly",
      "year": 2019,
      "role": "Pioneered adapter-based PEFT, showing that small modules can adapt large models with most weights frozen.",
      "relationship_sentence": "HRA positions its Householder-based orthogonal adapters within the PEFT paradigm established by adapters, achieving similar parameter efficiency while theoretically tying orthogonal and low-rank update families."
    }
  ],
  "synthesis_narrative": "The key contribution of HRA is to bridge low-rank and orthogonal adaptation by parameterizing trainable orthogonal transformations with chains of Householder reflections and proving their equivalence to adaptive low-rank updates. LoRA defined the modern low-rank update space for PEFT, turning weight deltas into constrained, efficient subspaces; HRA\u2019s equivalence result directly builds on this formulation by showing that multiplying frozen weights by a product of reflections realizes a low-rank delta. The feasibility and efficiency of using reflections come from prior work on Householder parametrizations: Mhammedi et al. established that orthogonal matrices can be learned as products of Householder reflectors, and Tomczak & Welling showed that stacking such rank\u20111 reflections yields expressive yet cheap transformations. Together, these works provide the mathematical and algorithmic scaffolding for HRA\u2019s reflector-based adapters. The second pillar of HRA\u2014regularizing the orthogonality of reflection planes to control capacity and stability\u2014draws from the broader literature demonstrating the benefits of orthogonality in deep learning. Bansal et al. and Huang et al. showed that orthogonality constraints stabilize optimization and improve generalization, motivating HRA\u2019s explicit regularization of reflector directions. Finally, the adapter paradigm of Houlsby et al. situates HRA within PEFT: HRA preserves frozen backbones and tunes a compact, structured parameterization, but offers a principled unification of orthogonal and low-rank routes. Collectively, these prior works shape HRA\u2019s core insight and design: an orthogonal, Householder-based adapter that is theoretically and practically aligned with low-rank adaptation.",
  "analysis_timestamp": "2026-01-06T23:33:35.565214"
}
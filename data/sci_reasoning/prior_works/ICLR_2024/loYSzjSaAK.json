{
  "prior_works": [
    {
      "title": "An analysis of approximations for maximizing submodular set functions",
      "authors": "G. L. Nemhauser et al.",
      "year": 1978,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "subPO\u2019s stepwise maximization of marginal gains is a direct incarnation of the classical greedy principle and guarantees from Nemhauser et al., transplanted into an RL policy-optimization procedure."
    },
    {
      "title": "Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization",
      "authors": "Daniel Golovin et al.",
      "year": 2011,
      "arxiv_id": "1003.3967",
      "role": "Extension",
      "relationship_sentence": "The paper extends the adaptive greedy idea of maximizing expected marginal utility under uncertainty to MDPs by learning a policy that is greedy in marginal gains via policy gradients."
    },
    {
      "title": "The Submodular Orienteering Problem",
      "authors": "Chandra Chekuri et al.",
      "year": 2005,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "Results on maximizing submodular rewards along paths motivate the subRL formulation and hardness proofs, and underpin the use of greedy marginal gains as a planning heuristic in sequential decision settings."
    },
    {
      "title": "Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning",
      "authors": "Rodrigo Toro Icarte et al.",
      "year": 2018,
      "arxiv_id": "1810.03939",
      "role": "Gap Identification",
      "relationship_sentence": "Work on reward machines addresses non-Markovian rewards via automata but does not exploit diminishing-returns structure, motivating a formulation that explicitly models submodular, history-dependent rewards and algorithms tailored to their marginal gains."
    },
    {
      "title": "Structured Solution Methods for Non-Markovian Decision Processes",
      "authors": "Fahiem Bacchus et al.",
      "year": 1997,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The NMRDP framework formalizes history-dependent rewards, which this paper specializes by positing submodular set-based returns and then deriving complexity and learning algorithms in that setting."
    },
    {
      "title": "Learning Submodular Functions over Sequences",
      "authors": "Sebastian Tschiatschek et al.",
      "year": 2017,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Sequence submodularity\u2019s diminishing-returns notion and greedy-by-marginal-benefit selection directly informed modeling rewards as submodular over visited-state histories and optimizing them via marginal-gain\u2013driven updates."
    }
  ],
  "synthesis_narrative": "Classical submodular optimization established that monotone submodular objectives admit strong guarantees via greedy maximization of marginal gains, with Nemhauser, Wolsey, and Fisher showing the canonical 1\u22121/e approximation for set selection and thereby elevating marginal-gain\u2013based search as the core design principle. Adaptive submodularity generalized this idea to sequential decision-making under uncertainty, with Golovin and Krause proving that an adaptive greedy policy that picks the action with maximum expected marginal utility remains near-optimal, thus linking stepwise marginal gains to policies. Submodular orienteering extended these rewards to paths, proving hardness and validating greedy heuristics in trajectory-like decisions where rewards accumulate submodularly along a route. In parallel, the NMRDP literature (Bacchus, Boutilier, and Grove) formalized non-Markovian rewards through state augmentation and automata-based methods, later operationalized in RL via reward machines (Icarte et al.), which capture temporal structure but not the diminishing-returns geometry. Finally, work on submodularity over sequences (Tschiatschek et al.) articulated diminishing returns on ordered actions and advocated greedy marginal-benefit selection in sequential settings. Together, these lines revealed a gap: non-Markovian rewards with explicit diminishing returns lacked an RL-native optimization method that operationalizes marginal gains over trajectories. By specializing NMRDPs to submodular set-valued returns and importing the adaptive-greedy marginal-gain principle, the paper naturally proposes a policy-gradient realization of greedy selection, establishes hardness relative to path-style problems, and delivers an algorithm aligned with the provable structure of submodularity.",
  "target_paper": {
    "title": "Submodular Reinforcement Learning",
    "authors": "Manish Prajapat, Mojmir Mutny, Melanie Zeilinger, Andreas Krause",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Reinforcement learning, Non-Markovian rewards, Submodular optimization, Policy gradient, Complex objectives in RL",
    "abstract": "In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are independent of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose Submodular RL (subRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions, which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose subPO, a simple policy gradient-based algorithm for subRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under",
    "openreview_id": "loYSzjSaAK",
    "forum_id": "loYSzjSaAK"
  },
  "analysis_timestamp": "2026-01-06T12:49:12.609745"
}
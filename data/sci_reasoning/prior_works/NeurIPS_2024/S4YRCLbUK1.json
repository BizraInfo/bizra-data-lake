{
  "prior_works": [
    {
      "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
      "authors": "A. Hessel et al.",
      "year": 2021,
      "role": "Baseline cross-modal similarity metric widely repurposed for T2I prompt faithfulness",
      "relationship_sentence": "TS2 directly targets CLIPScore-style image\u2013text similarity metrics, replacing simple correlation-to-human-Likert reporting with rigorous, ordered error-graph testing and statistical discrimination checks."
    },
    {
      "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
      "authors": "Junnan Li, Dongxu Li, Caiming Xiong, Steven C.H. Hoi",
      "year": 2022,
      "role": "Foundational VLM enabling BLIPScore and related faithfulness metrics",
      "relationship_sentence": "Because many contemporary T2I coherence metrics are based on BLIP-like VLMs, TS2 is designed to systematically stress-test such VLM-based scores on controlled semantic error ladders rather than on easy, human-rated sets."
    },
    {
      "title": "Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation (and PickScore)",
      "authors": "Yuval Kirstain et al.",
      "year": 2023,
      "role": "Preference-trained scoring for T2I used as a learned faithfulness/quality metric",
      "relationship_sentence": "TS2 evaluates whether preference-trained metrics like PickScore actually respect objective semantic error counts and can monotonically order increasingly erroneous images, moving beyond aggregate preference correlation."
    },
    {
      "title": "SPICE: Semantic Propositional Image Caption Evaluation",
      "authors": "Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould",
      "year": 2016,
      "role": "Semantic, scene-graph-based meta-evaluation of captioning quality",
      "relationship_sentence": "TS2 echoes SPICE\u2019s focus on propositional semantics by structuring ordered \u2018semantic error graphs\u2019 and scoring metrics on their ability to reflect objective error cardinality and separability."
    },
    {
      "title": "Winoground: Probing Vision-Language Models for Grounded Compositionality",
      "authors": "Andrew Thrush et al.",
      "year": 2022,
      "role": "Hard minimal-pair benchmark exposing compositional failures in vision-language grounding",
      "relationship_sentence": "TS2 adopts Winoground\u2019s spirit of controlled, fine-grained stress tests but extends it to a graded, monotonic error framework that enables meta-evaluation of T2I faithfulness metrics via statistical tests."
    },
    {
      "title": "T2I-CompBench: A Comprehensive Benchmark for Compositional Text-to-Image Generation",
      "authors": "Hao Huang et al.",
      "year": 2023,
      "role": "Compositional T2I benchmark with fine-grained categories (attributes, relations, counting)",
      "relationship_sentence": "Building on the need for compositional fidelity checks highlighted by T2I-CompBench, TS2 contributes an ordered, error-count-aware dataset and formal meta-metrics to test whether scoring methods rank images according to objective semantic violations."
    },
    {
      "title": "CheckList: A Behavioral Testing Framework for NLP",
      "authors": "Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin",
      "year": 2020,
      "role": "Methodology for controlled perturbations and hypothesis-driven evaluation with statistical testing",
      "relationship_sentence": "TS2 mirrors CheckList\u2019s behavioral-testing philosophy by using controlled semantic perturbations (error graphs) and hypothesis-driven statistical tests to meta-evaluate whether scoring metrics satisfy expected monotonicity and discriminability."
    }
  ],
  "synthesis_narrative": "T2IScoreScore (TS2) addresses a central gap in text-to-image evaluation: existing prompt faithfulness metrics are typically validated only by correlations with human Likert ratings on relatively easy samples. The lineage of such metrics traces to CLIPScore and VLM-based methods (e.g., BLIP/BLIPScore), along with preference-trained scores like PickScore; these have become de facto standards but lack rigorous, mechanistic tests of whether they reflect semantic error severity. Parallel strands in evaluation shaped TS2\u2019s design. SPICE showed that evaluation can be grounded in explicit semantic propositions, inspiring TS2\u2019s focus on counting objective semantic violations. Winoground demonstrated the value of controlled, hard cases for probing compositional grounding, while T2I-CompBench operationalized compositional categories (attributes, relations, counting) specifically for T2I. TS2 synthesizes these ideas into \u2018semantic error graphs\u2019\u2014graded perturbations that create images with increasing numbers of objective errors\u2014allowing a stringent test of whether a metric\u2019s scores decrease monotonically with error count and meaningfully separate adjacent error levels. Finally, CheckList\u2019s behavioral testing methodology\u2014controlled perturbations with hypothesis-driven statistical analysis\u2014directly informs TS2\u2019s meta-metrics, which use established statistical tests to quantify ordering and discrimination performance. Together, these prior works motivate TS2\u2019s shift from ad hoc correlation reporting to principled meta-evaluation that can reveal surprising failures of state-of-the-art faithfulness metrics.",
  "analysis_timestamp": "2026-01-06T23:33:35.568487"
}
{
  "prior_works": [
    {
      "title": "Personalizing Dialogue Agents: I have a dog, do you have pets?",
      "authors": "Saizheng Zhang et al.",
      "year": 2018,
      "arxiv_id": "1801.07243",
      "role": "Foundation",
      "relationship_sentence": "This paper established the persona/profile\u2013conditioned dialogue formulation and evaluation setup that is directly generalized here to test both explicit and implicit user preferences over multi-session, long-context conversations."
    },
    {
      "title": "A Persona-Based Neural Conversation Model",
      "authors": "Jiwei Li et al.",
      "year": 2016,
      "arxiv_id": "1603.06155",
      "role": "Foundation",
      "relationship_sentence": "It introduced conditioning conversational responses on user traits and evaluating consistency, providing the core problem framing of preference-following that is assessed in both generation and classification tasks."
    },
    {
      "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
      "authors": "Bai et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "LongBench\u2019s methodology for stress-testing long-context abilities and scaling context lengths informs the benchmark design that evaluates preference inference and retention in conversations up to 100k tokens."
    },
    {
      "title": "Lost in the Middle: How Language Models Use Long Context",
      "authors": "Nelson F. Liu et al.",
      "year": 2023,
      "arxiv_id": "2307.03172",
      "role": "Gap Identification",
      "relationship_sentence": "Its finding that models fail to retrieve salient information from long inputs motivates positioning user preferences at varying places in history to diagnose when preference cues are forgotten during dialogue."
    },
    {
      "title": "MemGPT: Towards LLMs as Operating Systems",
      "authors": "Shi et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "By proposing an external, retrievable memory for multi-session agents to persist user-specific facts, it directly informs the retrieval-augmented baselines evaluated for personalized preference adherence."
    },
    {
      "title": "MemPrompt: Memory-Augmented Prompting for On-the-Fly Personalization",
      "authors": "Xu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "MemPrompt\u2019s mechanism of storing user feedback and facts as a memory that can be retrieved in subsequent turns is a concrete baseline technique stress-tested under the benchmark\u2019s personalization tasks."
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "authors": "Prithviraj Madaan et al.",
      "year": 2023,
      "arxiv_id": "2303.17651",
      "role": "Baseline",
      "relationship_sentence": "Its iterative feedback loop for improving adherence motivates including iterative self-/user-feedback protocols as a key comparison axis when measuring preference following."
    }
  ],
  "synthesis_narrative": "Early persona-based dialogue work showed how to condition a conversational model on user profiles and evaluate whether responses respect those profiles. One strand introduced persona embeddings and consistency metrics, formalizing the notion that responses should reflect a user\u2019s stated preferences. Complementary efforts built a widely used dataset and protocol where agents must tailor replies to explicit persona descriptions, establishing the canonical evaluation of personalized dialogue. In parallel, long-context benchmarks developed methodology to scale inputs to tens of thousands of tokens, with tasks and metrics probing information retention across extended inputs. Crucially, empirical analyses of long-context processing revealed that models often fail to retrieve salient details placed away from the ends of a long sequence, highlighting a specific failure mode for any task requiring persistent memory. To mitigate such issues in practice, memory-augmented prompting and external-memory agent designs emerged to persist user-specific facts across sessions, while iterative self-feedback methods demonstrated that structured correction loops can improve adherence to requirements. Together, these threads exposed an unmet need: no standard evaluation combined persona-style preference following with explicit and implicit cues, tested over multi-session, very long contexts, and systematically compared prompting, memory/retrieval, and iterative feedback strategies. Building on persona conditioning as the core task, adopting long-context benchmarking methodology, and directly stress-testing memory and feedback-based personalization methods, the present work unifies these elements into a focused benchmark that diagnoses whether LLMs can infer, remember, and follow user preferences over extended conversations.",
  "target_paper": {
    "title": "Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs",
    "authors": "Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, Kaixiang Lin",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "personalization, benchmark, Large language models, conversational llm, chatbots",
    "abstract": "Large Language Models (LLMs) are increasingly deployed as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit preference forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we have evaluated 10 open-sourced and\nproprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. \nOur benchmarking effort reveals that state-of-the-art LLMs face significant challenges in following users' preference during conversations. In particular,  in zero-",
    "openreview_id": "QWunLKbBGF",
    "forum_id": "QWunLKbBGF"
  },
  "analysis_timestamp": "2026-01-06T13:11:16.410311"
}
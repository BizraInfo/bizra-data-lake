{
  "prior_works": [
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Geva et al.",
      "year": 2021,
      "arxiv_id": "2102.11126",
      "role": "Inspiration",
      "relationship_sentence": "By reframing FFN sublayers as key\u2013value memories that write targeted features into the residual stream, this work directly motivated analyzing how FFNs change token contextualization and thus inspired rendering their effects in an attention-map-like space."
    },
    {
      "title": "Quantifying Attention Flow in Transformers",
      "authors": "Abnar and Zuidema",
      "year": 2020,
      "arxiv_id": "2005.00928",
      "role": "Extension",
      "relationship_sentence": "The attention flow/rollout mechanism for composing layer-wise attention into end-to-end token-to-token maps is extended here to incorporate the FFN-induced transformations, enabling FF effects to be visualized as effective attention maps."
    },
    {
      "title": "Transformer Interpretability Beyond Attention Visualization",
      "authors": "Chefer et al.",
      "year": 2021,
      "arxiv_id": "2012.09838",
      "role": "Extension",
      "relationship_sentence": "Their LRP-style relevance propagation through both attention and MLP blocks informed the methodological choice to propagate FFN contributions along computational paths, which this paper adapts by re-expressing FFN effects as attention-map entries rather than generic relevance scores."
    },
    {
      "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention",
      "authors": "Clark et al.",
      "year": 2019,
      "arxiv_id": "1906.04341",
      "role": "Foundation",
      "relationship_sentence": "By establishing attention maps as a human-interpretable lens that captures linguistic relations, this work provides the visualization paradigm that the paper targets when translating FFN effects into attention maps."
    },
    {
      "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
      "authors": "Voita et al.",
      "year": 2019,
      "arxiv_id": "1905.09418",
      "role": "Related Problem",
      "relationship_sentence": "Evidence that specific attention heads encode syntactic/coreferential patterns set up concrete linguistic compositions against which FFN-induced contextualization changes can be compared once mapped into attention space."
    },
    {
      "title": "Locating and Editing Factual Knowledge in GPT",
      "authors": "Meng et al.",
      "year": 2022,
      "arxiv_id": "2202.05262",
      "role": "Related Problem",
      "relationship_sentence": "Showing that factual knowledge is localized primarily in FFN parameters underscored the importance of isolating and visualizing FFN contributions to token representations, directly motivating an FF-focused analysis."
    }
  ],
  "synthesis_narrative": "Feed-forward networks in Transformers were reframed as key\u2013value memories by Geva et al., who showed these sublayers write targeted features into the residual stream; this positioned FFNs as active sources of content that can steer representations. Abnar and Zuidema introduced attention flow, composing layer-wise attention to produce end-to-end token-to-token maps, thereby defining a concrete mechanism for visualizing contextualization as effective attention. Chefer et al. progressed interpretability by propagating relevance through both attention and MLP sublayers, demonstrating that non-attention paths can be traced and attributed alongside attention, not merely ignored. Clark et al. established attention maps as a human-interpretable lens that aligns with linguistic relations, while Voita et al. revealed that specific heads specialize in syntactic and coreferential patterns, giving a catalog of linguistic compositions observable in attention space. Complementing these, Meng et al. localized factual knowledge primarily to FFNs, emphasizing that understanding model knowledge requires isolating FFN contributions.\nTogether, these works suggested a gap: although attention maps are a trusted visualization medium, FFN-induced contextualization\u2014known to encode knowledge and influence representations\u2014was not captured in that space. Extending attention flow with propagation through FFN paths, and guided by relevance-propagation principles, the paper synthesizes these ideas to translate FFN effects into attention-map form. This enables direct comparison with known attention-based linguistic patterns and reveals interactions\u2014including amplification and cancellation\u2014between FFNs and attention, a natural next step given the recognized importance of FFNs and the established utility of attention maps.",
  "target_paper": {
    "title": "Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps",
    "authors": "Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Transformer, Attention map, Feed-forward, Contextualization, Interpretation, Analysis, Pre-trained models, Masked language models, Causal language models",
    "abstract": "Transformers are ubiquitous in wide tasks.\nInterpreting their internals is a pivotal goal. \nNevertheless, their particular components, feed-forward (FF) blocks, have typically been less analyzed despite their substantial parameter amounts.\nWe analyze the input contextualization effects of FF blocks by rendering them in the attention maps as a human-friendly visualization scheme.\nOur experiments with both masked- and causal-language models reveal that FF networks modify the input contextualization to emphasize specific types of linguistic compositions. \nIn addition, FF and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer.",
    "openreview_id": "mYWsyTuiRp",
    "forum_id": "mYWsyTuiRp"
  },
  "analysis_timestamp": "2026-01-06T08:55:40.385462"
}
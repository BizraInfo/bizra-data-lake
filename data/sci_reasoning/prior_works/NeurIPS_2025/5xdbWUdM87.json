{
  "prior_works": [
    {
      "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
      "authors": "Simon S. Du, Jason D. Lee, Yuandong Tian, Liwei Wang, Xiyu Zhai",
      "year": 2019,
      "role": "Overparameterization/NTK foundation",
      "relationship_sentence": "Provides the core overparameterization and near-linearization (NTK) toolkit\u2014width and small-initialization conditions under which gradient dynamics become effectively convex\u2014which this paper adapts to two-player min-max settings to obtain global convergence to a Nash equilibrium."
    },
    {
      "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
      "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song",
      "year": 2019,
      "role": "Global convergence under width and initialization",
      "relationship_sentence": "Strengthens the global convergence story for overparameterized two-layer networks with precise width/step-size/initialization requirements, directly informing the architectural and initialization conditions imposed here to unlock hidden convexity in neural games."
    },
    {
      "title": "On Lazy Training: Probing the Structure of Deep Learning",
      "authors": "L\u00e9na\u00efc Chizat, Francis Bach",
      "year": 2019,
      "role": "Initialization and dynamics regime (lazy/NTK)",
      "relationship_sentence": "Formalizes the lazy-training regime where parameters move little and the model behaves linearly, a regime this work leverages to argue that alternating GDA stays in a hidden-convex neighborhood enabling convergence to NE."
    },
    {
      "title": "Near-Optimal Algorithms for Minimax Optimization",
      "authors": "Tianyi Lin, Chi Jin, Michael I. Jordan",
      "year": 2020,
      "role": "Minimax optimization dynamics and baselines",
      "relationship_sentence": "Analyzes GDA/extragradient for nonconvex\u2013(strongly) concave minimax problems and provides rate guarantees; the present paper extends the scope from nonconvex\u2013strongly-concave to fully nonconvex\u2013nonconcave neural games by exploiting overparameterization."
    },
    {
      "title": "Local Minimax Points for Nonconvex-Nonconcave Minimax Optimization",
      "authors": "Chi Jin, Praneeth Netrapalli, Michael I. Jordan",
      "year": 2020,
      "role": "Conceptual framework for equilibria in nonconvex\u2013nonconcave games",
      "relationship_sentence": "Introduces and analyzes local minimax/stability notions for nonconvex\u2013nonconcave problems; this work moves beyond local characterizations by proving global convergence to Nash equilibria under structural (overparameterized) conditions."
    },
    {
      "title": "Training GANs with Optimism",
      "authors": "Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, Haoyang Zeng",
      "year": 2018,
      "role": "Game dynamics and convergence in zero-sum training",
      "relationship_sentence": "Shows how modified dynamics (optimism) yield last-iterate convergence in bilinear/structured games; the current paper contrasts this by proving that plain alternating GDA suffices\u2014when architecture/initialization induce hidden convexity."
    },
    {
      "title": "Mirror-Prox: A Method for Efficiently Solving Variational Inequalities",
      "authors": "Arkadi Nemirovski",
      "year": 2004,
      "role": "Saddle-point/VI algorithmic baseline (extragradient)",
      "relationship_sentence": "Establishes extragradient-style methods and monotone VI analysis widely used to stabilize saddle-point training; this paper positions its novel path-length bound for alternating GDA relative to such stabilized methods, showing convergence without extragradient under overparameterization."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014global convergence of alternating gradient descent\u2013ascent to a Nash equilibrium in nonconvex\u2013nonconcave neural games\u2014sits at the intersection of overparameterized neural optimization and game-theoretic dynamics. Foundational results on overparameterization (Du et al.; Allen-Zhu, Li, and Song) established that sufficiently wide two-layer networks with small initialization behave nearly linearly (NTK regime), enabling global convergence in single-player learning. Chizat and Bach\u2019s lazy-training perspective clarified the initialization and dynamics regimes in which such linearization persists, providing the blueprint for the paper\u2019s architecture- and initialization-dependent \u201chidden convexity\u201d conditions.\n\nOn the game-optimization side, recent analyses (Lin, Jin, and Jordan) characterized when GDA/extragradient converge for nonconvex\u2013strongly-concave settings, while Jin, Netrapalli, and Jordan formalized appropriate local equilibrium concepts for fully nonconvex\u2013nonconcave problems. These works framed the limitations of standard dynamics in general neural games. Concurrently, algorithmic advances for saddle-point problems (Nemirovski\u2019s Mirror-Prox/extragradient) and dynamics tailored to games (Daskalakis et al.\u2019s optimism) showed that stabilizing tweaks can ensure convergence in structured or bilinear cases.\n\nThis paper synthesizes these threads: it imports the overparameterization/lazy-training machinery to neural min-max games, proving that the induced hidden convexity makes plain alternating GDA globally convergent to a Nash equilibrium, not merely to local minimax points. Its novel path-length bound for alternating GDA extends stability-style analyses from saddle-point/VI methods to the overparameterized neural setting, unifying architecture, initialization, and dynamics into the first global-convergence guarantee for two-layer neural min-max games.",
  "analysis_timestamp": "2026-01-07T00:21:32.250697"
}
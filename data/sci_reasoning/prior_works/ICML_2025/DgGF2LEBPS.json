{
  "prior_works": [
    {
      "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
      "authors": "Mohit Shridhar et al.",
      "year": 2020,
      "role": "Household instruction-following benchmark",
      "relationship_sentence": "ALFRED established standardized, language-conditioned household tasks and success/subgoal metrics that EmbodiedBench generalizes to MLLM-driven agents and expands across broader capability axes."
    },
    {
      "title": "Habitat: A Platform for Embodied AI Research",
      "authors": "Manolis Savva et al.",
      "year": 2019,
      "role": "Embodied AI platform and metrics",
      "relationship_sentence": "Habitat\u2019s task taxonomies (e.g., PointNav/ObjectNav) and metrics (e.g., SPL) inform EmbodiedBench\u2019s multi-environment, low-level navigation/manipulation evaluation design."
    },
    {
      "title": "Vision-and-Language Navigation (R2R)",
      "authors": "Peter Anderson et al.",
      "year": 2018,
      "role": "Canonical instruction-driven navigation benchmark",
      "relationship_sentence": "VLN\u2019s paradigm of grounding free-form instructions in spatial trajectories directly motivates EmbodiedBench\u2019s subsets targeting complex instruction understanding and spatial awareness."
    },
    {
      "title": "TEACh: Task-driven Embodied Agents that Chat",
      "authors": "Shivani Padmakumar et al.",
      "year": 2021,
      "role": "Complex, interactive instruction following in simulation",
      "relationship_sentence": "TEACh\u2019s emphasis on multi-turn, compositional goals shapes EmbodiedBench\u2019s evaluation of complex instruction understanding and long-horizon task decomposition for MLLMs."
    },
    {
      "title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation",
      "authors": "Axel Mees et al.",
      "year": 2022,
      "role": "Language-conditioned manipulation benchmark",
      "relationship_sentence": "CALVIN\u2019s sequential, atomic action manipulation tasks inform EmbodiedBench\u2019s low-level manipulation subsets and assessment of long-horizon control under language guidance."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Michael Ahn et al.",
      "year": 2022,
      "role": "LLM-based planning with affordance grounding",
      "relationship_sentence": "SayCan highlights LLMs\u2019 strengths and limits in commonsense planning for embodied tasks, motivating EmbodiedBench\u2019s dedicated evaluations of long-term planning and commonsense reasoning."
    },
    {
      "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge",
      "authors": "Linxi (Jim) Fan et al.",
      "year": 2022,
      "role": "Open-ended multimodal embodied benchmark",
      "relationship_sentence": "MineDojo\u2019s broad, vision-language task space and evaluation of foundation-model-driven agents inspire EmbodiedBench\u2019s comprehensive, capability-oriented MLLM assessment across diverse environments."
    }
  ],
  "synthesis_narrative": "EmbodiedBench\u2019s core contribution\u2014a comprehensive, capability-driven benchmark for vision-based MLLM embodied agents\u2014builds on three converging lines of prior work. First, embodied instruction-following benchmarks such as ALFRED and VLN (R2R) established how to rigorously evaluate agents that ground natural language in visual perception and spatial action. Their task schemas and metrics (e.g., success rates, subgoal completion, SPL) directly inform EmbodiedBench\u2019s high-level semantic tasks and its spatial-awareness and complex instruction-understanding subsets.\nSecond, simulation platforms and task taxonomies typified by Habitat standardized navigation/manipulation settings and measurement practices. This provides the methodological backbone for EmbodiedBench\u2019s multi-environment coverage and its inclusion of low-level, atomic action tasks for navigation and manipulation.\nThird, the emergence of language- and multimodal-model-driven agents has revealed new evaluation needs. TEACh highlighted multi-turn, compositional instruction challenges; CALVIN crystallized language-conditioned, long-horizon manipulation with fine-grained action sequences; SayCan demonstrated that LLM planners require grounding via affordances to exhibit commonsense and long-term planning; and MineDojo broadened the scope to open-ended, vision-language tasks for foundation-model agents. These works collectively motivate EmbodiedBench\u2019s six capability-focused subsets\u2014commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning\u2014explicitly targeted at MLLMs. By unifying these strands, EmbodiedBench advances from single-domain or single-capability tests to a unified, diverse, and scalable evaluation suite purpose-built for modern MLLM embodied agents.",
  "analysis_timestamp": "2026-01-07T00:21:32.373229"
}
{
  "prior_works": [
    {
      "title": "The Conditional Randomization Test",
      "authors": "Emmanuel Cand\u00e8s; Yingying Fan; Lucas Janson; Jinchi Lv",
      "year": 2018,
      "role": "Foundational regression-based CI test that leverages a model for X|Z and flexible prediction to construct exact, model-X-valid tests.",
      "relationship_sentence": "This paper analyzes CRT-style tests when the conditional or predictive models are misspecified, deriving testing-error bounds in terms of the induced regression misspecification."
    },
    {
      "title": "The Hardness of Conditional Independence Testing and the Generalised Covariance Measure",
      "authors": "Rajen D. Shah; Jonas Peters",
      "year": 2020,
      "role": "Introduced the GCM, a regression-residual based CI test with guarantees under well-specified/accurate nuisance regressions and clarified limits of CI testing.",
      "relationship_sentence": "The present work extends GCM\u2019s theory by quantifying Type-I/II error under misspecified inductive biases, replacing correctness assumptions with explicit bounds in terms of residual regression errors."
    },
    {
      "title": "Kernel-based Conditional Independence Test and Application in Causal Discovery",
      "authors": "Kun Zhang; Jonas Peters; Dominik Janzing; Bernhard Sch\u00f6lkopf",
      "year": 2011,
      "role": "Seminal CI test that regresses out Z via kernel ridge regression and tests residual dependence, implicitly relying on accurate function approximation.",
      "relationship_sentence": "By focusing on how imperfect regression (due to limited kernels or tuning) propagates to CI test errors, the paper formalizes misspecification effects that KCI inherently faces."
    },
    {
      "title": "Approximate Kernel-based Conditional Independence Tests for Fast Nonparametric Causal Discovery (RCIT and RCoT)",
      "authors": "Eric Strobl; Kun Zhang; Peter Spirtes; Shyam Visweswaran",
      "year": 2019,
      "role": "Practical, scalable CI tests that approximate kernel residualization using supervised learning tools subject to inductive biases.",
      "relationship_sentence": "Treating these as regression-based CI tests, the paper\u2019s misspecification-dependent error bounds provide a principled lens on their finite-sample behavior under imperfect nuisance fits."
    },
    {
      "title": "Measuring Statistical Dependence with Hilbert-Schmidt Norms of Cross-Covariance Operators (HSIC)",
      "authors": "Arthur Gretton; Olivier Bousquet; Alex Smola; Bernhard Sch\u00f6lkopf",
      "year": 2005,
      "role": "Introduced HSIC, the backbone for many residual-independence tests used after regression in CI procedures.",
      "relationship_sentence": "The work clarifies validity when residuals are only approximately independent because regressions miss the Bayes predictors, which directly impacts HSIC-based residual tests."
    },
    {
      "title": "Double/Debiased Machine Learning for Treatment and Causal Parameters",
      "authors": "Victor Chernozhukov; Denis Chetverikov; Mert Demirer; Esther Duflo; Christian Hansen; Whitney Newey; James Robins",
      "year": 2018,
      "role": "Established orthogonalization and cross-fitting to obtain inference robust to nuisance estimation error with explicit error propagation rates.",
      "relationship_sentence": "The paper\u2019s strategy of expressing testing error in terms of L2-type regression misspecification mirrors the DML paradigm of robustness to nuisance errors and informs the structure of their bounds."
    }
  ],
  "synthesis_narrative": "The core contribution of the NeurIPS 2023 paper is to characterize how conditional independence (CI) tests that rely on supervised learning behave when their nuisance regressions are misspecified. This builds directly on regression-driven CI methodologies such as the Conditional Randomization Test (CRT) and the Generalised Covariance Measure (GCM), which established powerful, flexible CI tests but typically guaranteed Type-I control under correct specification or sufficiently accurate regression fits. The authors\u2019 contribution is to move beyond these idealized assumptions and deliver explicit approximations and upper bounds for testing error that scale with concrete misspecification measures of the learned predictors.\nKernel-based residualization methods\u2014including the kernel CI test (KCI) and its scalable approximations RCIT/RCoT\u2014instantiate the same paradigm: learn E[Y|Z], E[X|Z], then test residual dependence (often via HSIC). These methods hinge critically on the inductive bias of the learner (kernel choice, regularization, feature mappings). By quantifying how imperfect nuisance fits translate to inflated Type-I error and reduced power, the paper provides robustness guarantees precisely where practitioners most need guidance.\nMethodologically, the paper echoes the orthogonal/robust-inference perspective from Double/Debiased Machine Learning: it frames test error in terms of L2-type regression errors and cross-fitting-type decompositions, yielding transparent sensitivity bounds. In sum, it unifies and extends regression-based CI testing theory by replacing correctness assumptions with principled misspecification-dependent guarantees across prominent tests such as GCM, CRT-like procedures, and kernel-residual methods.",
  "analysis_timestamp": "2026-01-06T23:42:49.133905"
}
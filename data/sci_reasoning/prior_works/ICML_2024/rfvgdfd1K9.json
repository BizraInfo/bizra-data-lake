{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "We take RLHF-style intent alignment as the prevailing baseline and argue that, even when systems faithfully follow or truthfully reflect human intent as learned from preferences, they can still erode users\u2019 long-term agency; our proposal adds an explicit agency-preservation objective to this paradigm."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "By showing that helpful-harmless-honest (truthfulness-focused) alignment can leave users vulnerable to subtle steering of their preferences and choices, we target a core limitation of Constitutional AI and propose explicitly optimizing for human agency preservation."
    },
    {
      "title": "Corrigibility",
      "authors": "Nate Soares et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "We generalize the corrigibility desideratum\u2014maintaining a human\u2019s ability to intervene and redirect the system\u2014into a formal, forward-looking notion of preserving the human\u2019s long-term agency across AI-human interactions."
    },
    {
      "title": "The Off-Switch Game",
      "authors": "Dylan Hadfield-Menell et al.",
      "year": 2016,
      "role": "Extension",
      "relationship_sentence": "We extend the off-switch game\u2019s focus on preserving shutdown options to a broader agency-preservation criterion that governs everyday recommendations and interactions, not just emergency overrides."
    },
    {
      "title": "Conservative Agency via Attainable Utility Preservation",
      "authors": "Alex Turner et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "We adapt AUP\u2019s core insight\u2014penalizing reductions in attainable options\u2014by redirecting the preservation target from the AI\u2019s own options to the human\u2019s future options and decision-making agency."
    },
    {
      "title": "Empowerment: A universal agent-centric measure of control",
      "authors": "Alexander S. Klyubin et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "Our forward-looking agency evaluations draw on empowerment as an information-theoretic measure of an agent\u2019s control over future states, using it to operationalize the human agency that aligned systems must preserve."
    },
    {
      "title": "Optimal Policies Tend to Seek Power",
      "authors": "Alex Turner et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "We directly address the general incentive for power-seeking identified here by requiring intent-aligned systems to also preserve\u2014rather than appropriate\u2014human power/agency over time, and by formalizing agency-preserving interactions."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core move\u2014elevating agency preservation to a first-class optimization target alongside intent alignment\u2014emerges from clear limitations in current alignment practice and from option-preservation ideas in safety theory. RLHF (Christiano et al., 2017) and Constitutional AI (Bai et al., 2022) define today\u2019s intent- and truthfulness-focused baselines, but their very success leaves a gap: systems can still steer user preferences and choices while appearing honest and helpful. Foundational work on corrigibility (Soares et al., 2015) and the Off-Switch Game (Hadfield-Menell et al., 2016) established that preserving human control is a central desideratum; our contribution generalizes this from discrete override events to the ongoing, forward-looking preservation of human agency in everyday interactions. Technically, we draw inspiration from Conservative Agency via Attainable Utility Preservation (Turner et al., 2019), shifting the option-preservation lens from the AI\u2019s capabilities to the human\u2019s future options and decisional latitude. Empowerment (Klyubin et al., 2005) provides an operational scaffold for measuring such forward-looking control, enabling explicit agency evaluations. Finally, the power-seeking theorems (Turner et al., 2021) sharpen the problem by showing why generic objectives incentivize control acquisition; we respond by proposing that aligned systems be additionally constrained to preserve human agency. Together, these works directly motivate and enable our formal definition of agency-preserving AI-human interactions and the call to explicitly optimize for human agency, not merely intent conformity.",
  "analysis_timestamp": "2026-01-06T23:09:26.409187"
}
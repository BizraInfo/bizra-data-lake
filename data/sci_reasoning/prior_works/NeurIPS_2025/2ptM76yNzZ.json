{
  "prior_works": [
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "Hour-LLaVA directly adopts the visual instruction-tuning paradigm of LLaVA\u2014aligning vision features to an LLM via instruction-following\u2014and extends it from images to hour-long videos with VideoMarathon."
    },
    {
      "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
      "authors": "Zhang et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "Video-LLaMA established the instruction-tuned video-LMM recipe that Hour-LLaVA improves upon, addressing its short-clip training limitation by enabling end-to-end hour-scale training and inference."
    },
    {
      "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision-Language Models",
      "authors": "Muhammad Maaz et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Video-ChatGPT\u2019s LLM-guided pipeline for synthesizing video instruction\u2013answer pairs directly inspired VideoMarathon\u2019s large-scale synthetic QA generation, which the authors scale and redesign for hour-long videos and 22 task types."
    },
    {
      "title": "LVBench: A Benchmark for Long-Form Video Understanding",
      "authors": "Xue et al.",
      "year": 2024,
      "role": "Gap Identification",
      "relationship_sentence": "LVBench exposed systematic failures of existing Video-LMMs on long-term temporality, event, and scene reasoning, motivating VideoMarathon\u2019s hour-scale supervision and Hour-LLaVA\u2019s capability to model such long-range dependencies."
    },
    {
      "title": "EgoSchema: A Long-Term Egocentric Video Question Answering Benchmark",
      "authors": "Mangalam et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "EgoSchema highlighted the need for models that retain and reason over minute-to-hour temporal context, a limitation this paper targets by curating hour-long instruction data and enabling hour-scale training/inference."
    },
    {
      "title": "VideoChat2: Multi-Granular Video Understanding with Large Language Models",
      "authors": "Liang et al.",
      "year": 2024,
      "role": "Baseline",
      "relationship_sentence": "VideoChat2 advances video-LMM alignment yet remains constrained to short windowed clips; Hour-LLaVA directly improves this baseline by operating on continuous hour-long inputs trained with VideoMarathon."
    },
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Flamingo\u2019s interleaved visual\u2013text modeling and video-conditioned LLM design laid core architectural groundwork that subsequent video-LMMs (and thus Hour-LLaVA) build upon for sequential visual reasoning."
    }
  ],
  "synthesis_narrative": "The core innovation of this paper\u2014training and running a Video-LMM at hour scale using a large synthetic instruction-following dataset\u2014emerges from the confluence of instruction-tuning and long-form video evaluation. LLaVA introduced the visual instruction-tuning blueprint for aligning visual encoders with LLMs, while Flamingo earlier demonstrated how interleaving visual and text tokens enables sequential visual reasoning; these two works provide the architectural and procedural foundations that Hour-LLaVA leverages. Video-LLaMA and VideoChat2 then established practical video-LMM baselines, but both primarily operated on short clips or sliding windows, revealing a gap between instruction-tuned video models and genuine long-range comprehension. On the data side, Video-ChatGPT pioneered LLM-driven synthesis of video QA pairs, directly inspiring the authors to build VideoMarathon; they scale this idea dramatically, curating 3.3M high-quality QA pairs over ~9,700 hours and explicitly covering temporality, spatiality, object, action, scene, and event. Finally, long-form benchmarks such as LVBench and EgoSchema systematically documented where current models fail\u2014long-horizon temporal reasoning and event-level understanding\u2014providing both the task axes and the performance gaps that VideoMarathon and Hour-LLaVA are designed to address. Taken together, these works define the problem, reveal the limitations, and supply the methodological seeds that this paper extends to unlock hour-long video training and inference for video\u2013language understanding.",
  "analysis_timestamp": "2026-01-06T23:08:23.975523"
}
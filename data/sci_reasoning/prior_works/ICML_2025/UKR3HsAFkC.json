{
  "prior_works": [
    {
      "title": "Harnessing Smoothness to Accelerate Distributed Optimization",
      "authors": "Guoyin Qu et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "This paper introduced gradient tracking (GT); the current work identifies why the naive adaptation of GT under row-stochastic mixing causes descent-direction deviation and builds a new analysis that fixes it, effectively extending Qu\u2013Li\u2019s GT to the row-stochastic regime."
    },
    {
      "title": "Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks",
      "authors": "Nicolas Scaman et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "The new lower bound and network-dependent metrics for row-stochastic graphs are crafted by extending Scaman et al.\u2019s spectral-gap-based lower-bound framework from doubly-stochastic settings to the previously unaddressed row-stochastic case."
    },
    {
      "title": "Can decentralized algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic gradient descent",
      "authors": "Xiangru Lian et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Lian et al. formalized linear speedup targets for decentralized stochastic optimization; the present paper achieves the analogous linear-speedup guarantee specifically under row-stochastic networks by resolving GT-induced descent deviation."
    },
    {
      "title": "Decentralized Stochastic Optimization and Gossip Algorithms with Gradient Tracking",
      "authors": "Anastasia Koloskova et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "This work establishes linear-speedup guarantees for GT under doubly-stochastic weights; the current paper closes the explicit gap it leaves by proving linear speedup for GT in the row-stochastic setting and showing why prior GT analyses fail there."
    },
    {
      "title": "Decentralized Deep Learning with Arbitrary Communication Graph: Topology Matters",
      "authors": "Anastasia Koloskova et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "By developing stochastic gradient push (push-sum) for column-stochastic directed graphs, this work represents the \u2018column-stochastic\u2019 side that has been extensively addressed; the present paper complements it by providing theory and optimality for the row-stochastic side."
    },
    {
      "title": "A Push-Pull Gradient Method for Distributed Optimization in Networks",
      "authors": "Shi Pu et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Push\u2013pull methods mix row- and column-stochastic updates to handle directed graphs; the current paper isolates the pure row-stochastic setting, diagnoses instability in pull-type (row-stochastic) protocols like Pull-Diag, and provides conditions and mechanisms (MG) to stabilize and optimize them."
    },
    {
      "title": "Gossip-based Computation of Aggregate Information in Networks",
      "authors": "David Kempe et al.",
      "year": 2003,
      "role": "Foundation",
      "relationship_sentence": "The diagonal normalization principle from push-sum gossip underlies the Pull-Diag idea; the present work analyzes why such diagonal corrections can induce instability under row-stochastic mixing and develops remedies and tight complexity guarantees."
    }
  ],
  "synthesis_narrative": "The core innovation of this paper\u2014deriving the first lower bound for decentralized learning over row-stochastic networks and achieving linear speedup with a stabilized Pull-Diag-GT plus multi-step gossip\u2014rests on three intellectual threads. First, Scaman et al. established the modern lower-bound and network-metric paradigm for decentralized optimization under doubly-stochastic weights; the authors extend this framework to row-stochastic graphs, delivering the missing lower bound and appropriate metrics. Second, the linear-speedup target in decentralized stochastic optimization was crystallized by Lian et al., and operationalized with gradient tracking in doubly-stochastic settings by Koloskova et al.; the present paper pinpoints why the direct GT adaptation fails under row-stochastic mixing (descent-direction deviation) and supplies a new analysis that restores linear speedup\u2014thereby transplanting the GT success to the row-stochastic regime. Third, the algorithmic substrate for directed networks has historically relied on either column-stochastic corrections (push-sum/SGP) or mixed RS/CS schemes (push\u2013pull, AB). Kempe\u2019s push-sum provides the diagonal-normalization motif that informs Pull-Diag, while push\u2013pull highlights the practical distinctions and pitfalls of pull-type updates. The authors rigorously expose instability specific to Pull-Diag in row-stochastic networks and remedy it via a principled multi-step gossip mechanism, aligning the algorithm\u2019s complexity with the new lower bound. Together, these prior works directly shape the paper\u2019s problem formulation, performance targets, analytic tools, and stabilization strategy.",
  "analysis_timestamp": "2026-01-06T23:07:19.583416"
}
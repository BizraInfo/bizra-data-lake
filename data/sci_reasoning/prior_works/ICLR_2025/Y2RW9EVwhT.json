{
  "prior_works": [
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "role": "Foundational visual token integration (Perceiver Resampler, cross-attention) for MLLMs",
      "relationship_sentence": "Eagle explicitly revisits the choice of visual-token integration pioneered by Flamingo and shows that, when using complementary visual experts, simple token concatenation can rival learned resamplers/routers."
    },
    {
      "title": "BLIP-2: Bootstrapping Language\u2013Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "role": "Bridging module (Q-Former) and visual token compression design",
      "relationship_sentence": "Eagle\u2019s design-space study contrasts heavy bridging/compaction (as in Q-Former) with minimal projection plus token concatenation across multiple encoders, motivating its streamlined fusion choice."
    },
    {
      "title": "Qwen-VL: A Frontier Large Vision-Language Model with Any-Resolution Inputs",
      "authors": "Bai et al.",
      "year": 2023,
      "role": "AnyRes multi-resolution tiling and token concatenation for fine-detail preservation",
      "relationship_sentence": "Eagle generalizes Qwen-VL\u2019s AnyRes insight\u2014concatenating multi-scale tokens\u2014to heterogeneous vision experts and analyzes expert selection and integration principles systematically."
    },
    {
      "title": "InternVL (1.5/2): Scaling Up Vision-Language Models with AnyRes and High-Resolution Perception",
      "authors": "Chen et al.",
      "year": 2024,
      "role": "High-resolution, multi-scale tokenization with concatenation for improved perception",
      "relationship_sentence": "Building on InternVL\u2019s effectiveness of multi-resolution token concatenation, Eagle unifies and ablates these choices, showing a simple, robust fusion recipe across complementary encoders."
    },
    {
      "title": "DeepSeek-VL2",
      "authors": "DeepSeek-AI Team",
      "year": 2024,
      "role": "Multi-granularity cropping and selective fusion/routing for detail-intensive tasks",
      "relationship_sentence": "Eagle tests whether sophisticated routing across visual experts (as explored in DeepSeek-VL2) is necessary, finding that well-chosen experts with straightforward token concatenation often suffice."
    },
    {
      "title": "TextHawk: Empowering MLLMs with Better Text Reading",
      "authors": "Wang et al.",
      "year": 2024,
      "role": "Text-focused visual branch fused with general-purpose encoder for OCR",
      "relationship_sentence": "Eagle is motivated by text-centric expert designs like TextHawk and demonstrates that pairing complementary encoders and concatenating their tokens yields strong OCR/document performance without complex fusion."
    },
    {
      "title": "MM1: Methods, Analysis and Insights from Multimodal LLM Pretraining",
      "authors": "Jain et al.",
      "year": 2024,
      "role": "Systematic analysis of MLLM design choices (resamplers, tokenization, data)",
      "relationship_sentence": "Eagle extends the MM1-style analytical approach to the specific axis of mixture-of-encoders and resolutions, providing targeted ablations and distilled principles for expert selection/integration."
    }
  ],
  "synthesis_narrative": "Eagle\u2019s core contribution is to systematically explore the mixture-of-encoders design space for multimodal LLMs and distill simple, general principles for expert selection and fusion\u2014most notably, that concatenating tokens from complementary visual experts and resolutions can match more complex routers/resamplers. This builds on two foundational strands. First, Flamingo and BLIP-2 shaped how visual tokens are formed and injected into LLMs\u2014via Perceiver-style resamplers or Q-Former bridges\u2014establishing baselines for token compression and cross-attention. Eagle explicitly probes whether such heavy bridging remains necessary when multiple vision experts are present, and finds minimal projection plus concatenation is often sufficient.\nSecond, the AnyRes lineage (Qwen-VL, InternVL) demonstrated that multi-resolution tiling and simple token concatenation preserve fine details crucial for OCR and document analysis. Eagle generalizes this beyond multi-scale inputs to heterogeneous experts (e.g., general-purpose vs text-focused), articulating when and how to select complementary encoders and how many tokens to allocate per expert.\nConcurrently, models like DeepSeek-VL2 and TextHawk explored multi-granularity cropping and specialized text-reading branches with selective routing. Eagle\u2019s ablations show that much of the reported gains can be captured by careful expert pairing and straightforward token concatenation, simplifying engineering costs. Methodologically, Eagle\u2019s broad, MM1-style analysis provides the missing, principled comparison across expert selection and integration strategies, yielding a streamlined recipe for robust, high-resolution visual perception in MLLMs.",
  "analysis_timestamp": "2026-01-07T00:02:04.905270"
}
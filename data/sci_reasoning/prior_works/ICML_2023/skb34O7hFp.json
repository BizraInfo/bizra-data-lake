{
  "prior_works": [
    {
      "title": "Rigging the Lottery: Making All Tickets Winners",
      "authors": "Utku Evci et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "RigL\u2019s prune-and-grow training directly motivates ReDo\u2019s idea of dynamically reallocating capacity; ReDo extends this by recycling at the neuron level using a dormancy signal (rather than weight magnitude/gradients) inside deep RL agents."
    },
    {
      "title": "Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization",
      "authors": "Hesham Mostafa et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Dynamic Sparse Reparameterization showed that periodically redistributing parameters via drop-and-grow improves learning; ReDo adopts the same principle\u2014refreshing underutilized capacity\u2014but triggers resets by detecting dormant neurons during RL training."
    },
    {
      "title": "Sparse evolutionary training: Training neural networks with dynamic sparse connectivity",
      "authors": "Decebal C. Mocanu et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "SET\u2019s prune-and-regrow (rewiring) mechanism inspired ReDo\u2019s periodic \u2018refresh\u2019 of unused capacity; ReDo applies a similar evolutionary idea at the neuron granularity to counter neuron inactivity in deep RL."
    },
    {
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "authors": "Jonathan Frankle et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "LTH established that (re)initialization can revive performant subnetworks; ReDo leverages this insight by reinitializing identified dormant neurons to restore representational capacity during RL training."
    },
    {
      "title": "RePr: Improved Training of Convolutional Filters",
      "authors": "Aditya Prakash et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "RePr periodically drops and reinitializes underutilized filters to encourage better feature learning; ReDo generalizes this \u2018reset\u2019 philosophy beyond CNN filters to neurons in RL networks, guided by a dormancy criterion."
    },
    {
      "title": "Human-level control through deep reinforcement learning",
      "authors": "Volodymyr Mnih et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "DQN defined the deep RL paradigm and network training setup in which the paper observes the buildup of dormant neurons; ReDo is designed to integrate into such value-based agents without altering the RL objective."
    },
    {
      "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "authors": "Matteo Hessel et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Rainbow serves as a primary high-performance DQN baseline; the paper plugs ReDo into Rainbow and shows that recycling dormant neurons reduces inactivity and improves performance."
    }
  ],
  "synthesis_narrative": "The core of The Dormant Neuron Phenomenon in Deep Reinforcement Learning is the recognition that deep RL networks progressively accumulate inactive units and that periodically \u2018refreshing\u2019 unused capacity can preserve expressivity and improve returns. This idea stands on two converging intellectual threads. First, dynamic sparse training demonstrated that capacity reallocation during training is beneficial: RigL introduced gradient-driven prune-and-grow, while Dynamic Sparse Reparameterization and Sparse Evolutionary Training showed that continual drop-and-grow or rewiring sustains learning by reallocating parameters. ReDo extends this thread by shifting the reallocation unit from connections to neurons and by using a principled dormancy signal (based on neuron inactivity) rather than weight magnitude or gradients, tailored to the dynamics of RL.\nSecond, prior work on resetting underutilized components showed that reinitialization can revitalize learning. The Lottery Ticket Hypothesis highlighted the power of initialization and (re)born subnetworks, and RePr demonstrated that periodically dropping and reinitializing weakly contributing filters improves feature learning in vision. ReDo generalizes this \u2018rejuvenation\u2019 intuition to RL by recycling dormant neurons throughout training.\nFinally, the study is grounded in the deep RL setting introduced by DQN and exemplified by the strong Rainbow baseline, which provide the algorithmic context where dormancy emerges and where ReDo is evaluated. Together, these works directly catalyze ReDo\u2019s neuron-level recycling mechanism for maintaining expressivity in deep RL.",
  "analysis_timestamp": "2026-01-06T23:09:26.578166"
}
{
  "prior_works": [
    {
      "title": "A Generalist Agent (Gato)",
      "authors": "Reed et al.",
      "year": 2022,
      "role": "Architectural precedent for unifying heterogeneous modalities and tasks with a single sequence Transformer.",
      "relationship_sentence": "HPT adopts Gato\u2019s \u201ceverything-as-tokens + shared Transformer trunk\u201d paradigm and extends it to proprioception-vision fusion and embodiment-agnostic control."
    },
    {
      "title": "RT-1: Robotics Transformer for Real-World Control at Scale",
      "authors": "Brohan et al.",
      "year": 2022,
      "role": "Demonstrated large-scale, transformer-based visuomotor policies trained with behavior cloning on real robot data.",
      "relationship_sentence": "HPT builds on RT-1\u2019s sequence-modeling of robot policies, scaling the approach to heterogeneous embodiments and tasks with a shareable trunk."
    },
    {
      "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "authors": "Brohan et al.",
      "year": 2023,
      "role": "Showed the benefits of leveraging large non-robot corpora (web/VLM pretraining) for robotic control via a Transformer policy.",
      "relationship_sentence": "HPT similarly leverages non-robot sources (e.g., human video) during pretraining, but targets an embodiment-agnostic policy trunk rather than language grounding."
    },
    {
      "title": "Open X-Embodiment (OXE) and RT-X: Robotic Learning Datasets and Models",
      "authors": "Open X-Embodiment Collaboration",
      "year": 2023,
      "role": "Provided large-scale multi-robot, multi-task datasets and baseline generalist policies across embodiments.",
      "relationship_sentence": "HPT is enabled by and explicitly targets the OXE setting, proposing an architecture to pretrain a single trunk across heterogeneous embodiments."
    },
    {
      "title": "RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation",
      "authors": "Black et al.",
      "year": 2023,
      "role": "Established that a single policy can span multiple robots and tasks and improve via continual data aggregation.",
      "relationship_sentence": "HPT generalizes RoboCat\u2019s multi-embodiment idea by learning an embodiment-agnostic representation in a unified trunk rather than per-robot adaptation cycles."
    },
    {
      "title": "PerAct: Perception-Action Coupling with 3D Perceiver Transformers",
      "authors": "Shridhar, Manuelli, Fox",
      "year": 2022,
      "role": "Showed how to compress heterogeneous perceptual inputs (RGB-D, language, proprioception) into tokens processed by a Transformer.",
      "relationship_sentence": "HPT\u2019s design that aligns proprioception and vision into a short token sequence directly echoes PerAct-style tokenization for heterogeneous inputs."
    },
    {
      "title": "R3M: A Universal Visual Representation for Robot Manipulation",
      "authors": "Nair et al.",
      "year": 2022,
      "role": "Demonstrated pretraining visuomotor representations from large human video corpora for downstream robot control.",
      "relationship_sentence": "HPT leverages human video during pretraining akin to R3M, but advances from representation pretraining to a policy-trunk that is embodiment-agnostic."
    }
  ],
  "synthesis_narrative": "HPT\u2019s core idea\u2014a single, shareable Transformer trunk that learns an embodiment- and task-agnostic policy representation from heterogeneous proprioceptive and visual inputs\u2014sits at the confluence of three lines of work. First, Gato established the blueprint for unifying disparate modalities and control signals via tokenization and sequence modeling, an approach RT-1 and RT-2 translated to real-robot control at scale. HPT inherits this sequence-modeling foundation and the insight that scaling data diversity benefits control, while shifting the emphasis to explicit embodiment-agnostic pretraining rather than language grounding.\nSecond, multi-embodiment generalist policies and datasets directly enable HPT\u2019s objective. Open X-Embodiment provided the multi-robot, multi-task corpora and early RT-X models indicating cross-embodiment transfer, and RoboCat showed a single policy can span different robots through large-scale aggregation. HPT advances these by architecting a unified trunk and token alignment that normalize heterogeneous proprioception and vision across embodiments, minimizing per-robot specialization.\nThird, prior methods for fusing heterogeneous inputs into compact tokens and leveraging non-robot data inform HPT\u2019s modality alignment and pretraining regime. PerAct demonstrated how to compress multi-sensor inputs into tokens processed by Transformers, while R3M showed that human video can yield representations beneficial for robot control. HPT synthesizes these insights: it tokenizes proprioception and vision into a short sequence, pretrains across robots, simulation, and human video, and then maps the shared representation to diverse robot controllers, thereby scaling proprioceptive-visual learning under embodiment heterogeneity.",
  "analysis_timestamp": "2026-01-06T23:33:35.560274"
}
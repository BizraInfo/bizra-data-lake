{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Foundational NTK framework",
      "relationship_sentence": "The paper\u2019s core move\u2014defining a network\u2019s \u201cfeatures\u201d as NTK eigenfunctions and analyzing training through the NTK eigenspectrum\u2014directly builds on Jacot et al.\u2019s NTK formulation and its Mercer decomposition."
    },
    {
      "title": "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
      "authors": "Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington",
      "year": 2019,
      "role": "Methodological underpinning: linearization and kernel regression dynamics",
      "relationship_sentence": "This work justifies analyzing training dynamics via the NTK and shows predictions evolve along kernel eigenfunctions, enabling the present paper\u2019s claim that large-eigenvalue modes dominate both learning and the final predictor."
    },
    {
      "title": "Spectral Bias and Task-Model Alignment in Kernel Regimes",
      "authors": "Deniz Canatar, Blake Bordelon, Cengiz Pehlevan",
      "year": 2021,
      "role": "Spectral analysis of kernel learning",
      "relationship_sentence": "Canatar et al. quantify how kernel eigenvalues and target alignment govern learning speed and generalization, directly informing the authors\u2019 identification of shortcut features with top NTK eigenmodes and their outsized post-training influence."
    },
    {
      "title": "Shortcut Learning in Deep Neural Networks",
      "authors": "Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann",
      "year": 2020,
      "role": "Phenomenon definition and motivation",
      "relationship_sentence": "This paper formalized the shortcut-learning phenomenon that the present work explains mechanistically by mapping shortcuts to dominant NTK eigenfunctions induced by imbalanced, clustered data."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro",
      "year": 2018,
      "role": "Implicit bias and max-margin theory",
      "relationship_sentence": "Soudry et al.\u2019s max-margin implicit bias provides the baseline explanation the authors test against; by showing preference persists even with margin control, the present work contrasts NTK-eigenvalue bias with max-margin accounts."
    },
    {
      "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks",
      "authors": "Suriya Gunasekar, Jason D. Lee, Daniel Soudry, Nati Srebro",
      "year": 2018,
      "role": "Implicit bias in deep/linear networks",
      "relationship_sentence": "Given the paper\u2019s linear-network focus, Gunasekar et al.\u2019s characterization of implicit biases in linear architectures is a direct precursor the authors build upon and differentiate from with an NTK-eigenspectrum explanation."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014a principled explanation of shortcut learning via the eigenspectrum of the Neural Tangent Kernel (NTK)\u2014rests on three pillars from prior work. First, Jacot et al. introduced the NTK framework and its Mercer decomposition, enabling the present authors to define a neural network\u2019s \u201cfeatures\u201d as NTK eigenfunctions and to analyze learning through spectral components. Second, linearization results for wide networks (Lee et al.) validate treating gradient descent training as kernel regression in the NTK, which implies that predictions evolve along eigenfunctions at rates governed by eigenvalues. Third, spectral analyses of kernel learning (Canatar, Bordelon, Pehlevan) established that high-eigenvalue modes are learned faster and dominate generalization, providing the mathematical lens through which data structure maps to learning dynamics.\nAnchored by Geirhos et al.\u2019s formulation of shortcut learning, the present work connects imbalanced, clustered data to an NTK spectrum in which shortcut-related eigenfunctions acquire larger eigenvalues, thereby explaining their preferential learning and persistent influence after training. Finally, implicit-bias results (Soudry et al.; Gunasekar et al.) offer a competing max-margin account. By showing that preference for large-eigenvalue features survives even when network margin is controlled, the authors argue that max-margin bias alone cannot explain shortcut learning in this setting. Collectively, these works directly enable the paper\u2019s core insight: shortcut features emerge as top NTK eigenfunctions shaped by data imbalance, and their dominance is a spectral\u2014not merely margin-based\u2014effect.",
  "analysis_timestamp": "2026-01-07T00:21:32.296960"
}
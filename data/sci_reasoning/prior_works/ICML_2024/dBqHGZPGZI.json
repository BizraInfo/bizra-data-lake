{
  "prior_works": [
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Rafailov et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "The paper\u2019s core case study centers on DPO; it applies the DPO objective to pairwise preferences to reduce toxicity and then analyzes its internal mechanism, directly building on and interrogating DPO\u2019s method."
    },
    {
      "title": "Learning to summarize from human feedback",
      "authors": "Stiennon et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Introduced the modern preference-learning formulation using pairwise comparisons and reward modeling that underpins the pairwise data setup this work adopts to train and probe alignment effects."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Ouyang et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Established the RLHF pipeline for aligning LMs to human preferences, defining the practical alignment problem that the present study seeks to mechanistically explain (with DPO as the focal alternative to PPO-based RLHF)."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Bai et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated large safety gains from preference-based harmlessness training but left open how such training works internally; this paper targets that gap by mechanistically showing that harmful capabilities persist and are bypassed rather than removed."
    },
    {
      "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
      "authors": "Gehman et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Provided the standard toxicity elicitation and measurement framework that this work leverages to study how toxicity is represented and reduced in pre-trained and DPO-tuned models."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Zou et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "Showed that safety-tuned models are vulnerable to jailbreaks; the present paper directly addresses this limitation by explaining, via mechanistic analysis, why alignment can be undone and demonstrating a simple un-alignment procedure."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central contribution\u2014a mechanistic account of how DPO reduces toxicity and why its effects can be undone\u2014rests on the preference-alignment paradigm crystallized by Stiennon et al. and popularized by Ouyang et al. These works defined alignment as optimizing models to match human preferences collected as pairwise comparisons, a setup the present study adopts to craft a toxicity-focused dataset. Direct Preference Optimization (Rafailov et al.) is the immediate methodological backbone: the authors apply DPO to perform alignment without explicit reward models, then open the black box to understand what DPO changes inside the network. Safety-focused alignment efforts such as Constitutional AI (Bai et al.) established that preference-based harmlessness training can reduce toxic outputs in practice but did not reveal the underlying mechanism\u2014precisely the gap this paper fills by showing that harmful capabilities are not erased but routed around. To quantify and probe toxic behavior, the study relies on the RealToxicityPrompts framework (Gehman et al.), which standardizes how to elicit and measure toxic degeneration. Finally, recent jailbreak results (Zou et al.) motivate the need for mechanism: if aligned models can be reliably broken, what is being changed? This work uses the DPO setting to demonstrate that alignment operates via bypasses, providing a principled explanation for jailbreak susceptibility and a concrete method to revert the model\u2019s behavior.",
  "analysis_timestamp": "2026-01-06T23:09:26.482064"
}
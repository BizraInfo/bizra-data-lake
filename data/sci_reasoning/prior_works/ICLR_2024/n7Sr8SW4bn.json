{
  "prior_works": [
    {
      "title": "Graph Sketches: Sparsification, Spanners, and Subgraphs",
      "authors": "Kook Jin Ahn et al.",
      "year": 2012,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work formalized streaming-graph summarization under one-pass, sublinear-memory constraints and defined canonical query targets that Mayfly adheres to while replacing combinatorial sketches with a learned neural memory."
    },
    {
      "title": "Counting Triangles in Data Streams",
      "authors": "Luciana S. Buriol et al.",
      "year": 2006,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "By establishing triangle counting as a core streaming task and introducing sampling-based estimators under limited memory, this paper provides a target statistic that Mayfly\u2019s learned summarizer explicitly aims to estimate more accurately."
    },
    {
      "title": "Graph Sample and Hold: A Framework for Big-Graph Sampling",
      "authors": "Nesreen K. Ahmed et al.",
      "year": 2014,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "As a practical, fixed-rule sampling baseline for streaming graphs, Graph Sample-and-Hold motivates Mayfly\u2019s replacement of hand-tuned sampling with learned, task-aware memory allocation that preserves higher-order structures."
    },
    {
      "title": "The Case for Learned Index Structures",
      "authors": "Tim Kraska et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "This work introduced the central idea of replacing hand-engineered data structures with learned models, which Mayfly generalizes from static indexing/membership to dynamic graph-stream sketches."
    },
    {
      "title": "The Learned Bloom Filter: Theory and Practice",
      "authors": "Michael Mitzenmacher et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "By showing learned data structures\u2019 sensitivity to distribution shift and providing bounds that degrade out-of-distribution, it motivates Mayfly\u2019s two-phase (meta + adaptation) training to achieve adaptivity in nonstationary graph streams."
    },
    {
      "title": "Meta-Learning with Memory-Augmented Neural Networks",
      "authors": "Adam Santoro et al.",
      "year": 2016,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Mayfly\u2019s controller-plus-external-memory design for differentiable read/write operations is directly inspired by MANNs\u2019 learned memory access patterns for rapid binding in streaming settings."
    },
    {
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "authors": "Chelsea Finn et al.",
      "year": 2017,
      "arxiv_id": "1703.03400",
      "role": "Extension",
      "relationship_sentence": "Mayfly adapts a MAML-style episodic meta-training and fast adaptation scheme to synthetic graph-stream tasks, acquiring a transferable summarization prior that can be quickly specialized to new streams (the metamorphosis phase)."
    }
  ],
  "synthesis_narrative": "Streaming graph algorithms established that many graph properties can be approximated in one pass with sublinear memory, with Ahn, Guha, and McGregor specifying the sketching formalism and query targets for sparsification and subgraphs. Early triangle-counting work by Buriol et al. defined a canonical higher-order statistic for streaming evaluation, relying on sampling under strict memory limits. Ahmed et al.\u2019s Graph Sample-and-Hold provided a practical baseline that selects edges with simple rules to preserve motifs and degrees for downstream estimates. In parallel, the learned data structures line showed that hand-crafted indices could be replaced by models: Kraska et al. demonstrated learned replacements for trees and filters, while Mitzenmacher analyzed learned Bloom filters\u2019 accuracy and highlighted brittleness under distribution shift. On the neural architecture side, Santoro et al. introduced memory-augmented neural networks that learn read/write policies for rapid online binding, and Finn et al. developed MAML to meta-train models for fast adaptation across related tasks. Together, these works reveal a gap: graph-stream sketches are accurate but rigid, while learned data structures are flexible but fragile to nonstationarity. The natural synthesis is a neural, memory-augmented sketch trained meta-episodically to capture transferable summarization priors and then adapted online to changing graph distributions. Mayfly embodies this by adhering to streaming constraints from graph sketching, targeting tasks like triangle estimation, replacing rule-based sampling with learned memory addressing, and using a two-phase (meta then adaptation) procedure to overcome learned data structures\u2019 shift sensitivity.",
  "target_paper": {
    "title": "Mayfly: a Neural Data Structure for Graph Stream Summarization",
    "authors": "Yuan Feng, Yukun Cao, Wang Hairu, Xike Xie, S Kevin Zhou",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Meta-Learning;Memory Augmented Neural Network; Deep Neural Network Application;Graph Summarization",
    "abstract": "A graph is a structure made up of vertices and edges used to represent complex relationships between entities, while a graph stream is a continuous flow of graph updates that convey evolving relationships between entities. The massive volume and high dynamism of graph streams promote research on data structures of graph summarization, which provides a concise and approximate view of graph streams with sub-linear space and linear construction time, enabling real-time graph analytics in various domains, such as social networking, financing, and cybersecurity.\nIn this work, we propose the Mayfly, the first neural data structure for summarizing graph streams. The Mayfly replaces handcrafted data structures with better accuracy and adaptivity.\nTo cater to practical applications, Mayfly incorporates two offline training phases.\nDuring the larval phase, the Mayfly learns basic summarization abilities from automatically and synthetically constituted meta-tasks, and in the metamorphosis phase, ",
    "openreview_id": "n7Sr8SW4bn",
    "forum_id": "n7Sr8SW4bn"
  },
  "analysis_timestamp": "2026-01-06T13:00:59.033683"
}
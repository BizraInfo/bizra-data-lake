{
  "prior_works": [
    {
      "title": "BEHRT: Transformer for Electronic Health Records",
      "authors": "Yikuan Li et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "BEHRT introduced large-scale transformer pretraining on longitudinal, timestamped EHR sequences with time-aware embeddings and masked-event objectives, which MOTOR adopts conceptually and scales while reorienting the representation toward survival transfer."
    },
    {
      "title": "CLMBR: Clinical Language Model-Based Representations for Electronic Health Records",
      "authors": "Thomas H. McCoy Jr. et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "CLMBR showed that self-supervised EHR pretraining greatly improves label efficiency for downstream tasks but focused on horizon-based classification rather than censoring-aware time-to-event modeling, a limitation MOTOR explicitly addresses."
    },
    {
      "title": "nnet-survival: A discrete-time survival model that can be fit to large datasets",
      "authors": "Matthew F. Gensheimer et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work introduced the discrete-time hazard formulation and censoring-aware loss that underlie MOTOR\u2019s fine-tuning heads for time-to-event prediction."
    },
    {
      "title": "DeepHit: A Deep Learning Approach to Survival Analysis With Competing Risks",
      "authors": "Changhee Lee et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "DeepHit\u2019s multi-outcome (competing risks) discrete-time survival formulation directly informs MOTOR\u2019s many-outcome TTE heads and multi-endpoint transfer setting."
    },
    {
      "title": "Time-to-Event Prediction with Neural Networks and Cox Regression (Cox-Time)",
      "authors": "H\u00e5vard Kvamme et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Cox-Time established a strong neural Cox baseline and standard evaluation (time-dependent C-statistic) that MOTOR targets and surpasses when fine-tuned from its pretrained representations."
    },
    {
      "title": "Deep Survival Machines: Fully Parametric Survival Regression",
      "authors": "Prithvijit Nagpal et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Deep Survival Machines addressed data scarcity with parametric mixture assumptions but required outcome-specific training, motivating MOTOR\u2019s alternative of survival-aware pretraining to achieve label efficiency across many outcomes."
    }
  ],
  "synthesis_narrative": "Transformer-based pretraining on structured medical records established that masked event modeling and age/visit-aware positional encodings can yield general-purpose patient representations from longitudinal EHR sequences. Self-supervised approaches at health-system scale further demonstrated that such representations dramatically improve label efficiency and robustness across diverse clinical tasks, although these efforts largely targeted horizon-based classification and next-visit prediction rather than censoring-aware survival modeling. In parallel, discrete-time survival methods introduced a flexible hazard parameterization and censoring-aware loss that fit naturally with deep networks and large datasets. Competing-risks formulations extended this into multi-outcome settings, showing how a single model could estimate endpoint-specific event-time distributions in discrete time. Neural Cox approaches solidified strong baselines and standardized evaluation using the time-dependent C-statistic. Finally, parametric mixture survival models provided data-efficient alternatives under small labels, but typically required training bespoke models per outcome. Together, these strands exposed a clear opportunity: combine large-scale, self-supervised representation learning on timestamped EHR sequences with censoring-aware, discrete-time survival heads to enable many-outcome time-to-event prediction under limited labels. By pretraining on massive EHR and claims event streams and fine-tuning with discrete-time and competing-risks survival formulations, the resulting framework unifies label efficiency, transfer across endpoints and datasets, and robust time-to-event estimation\u2014naturally surpassing outcome-specific survival baselines while retaining censoring-aware training and standardized evaluation.",
  "target_paper": {
    "title": "MOTOR: A Time-to-Event Foundation Model For Structured Medical Records",
    "authors": "Ethan Steinberg, Jason Alan Fries, Yizhe Xu, Nigam Shah",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "foundation models, time-to-event, electronic health records, deep learning, self-supervised learning, transfer learning",
    "abstract": "We present a self-supervised, time-to-event (TTE) foundation model called MOTOR (Many Outcome Time Oriented Representations) which is pretrained on timestamped sequences of events in electronic health records (EHR) and health insurance claims. TTE models are used for estimating the probability distribution of the time until a specific event occurs, which is an important task in medical settings. TTE models provide many advantages over classification using fixed time horizons, including naturally handling censored observations, but are challenging to train with limited labeled data. MOTOR addresses this challenge by pretraining on up to 55M patient records (9B clinical events). We evaluate MOTOR's transfer learning performance on 19 tasks, across 3 patient databases (a private EHR system, MIMIC-IV, and Merative claims data). Task-specific models adapted from MOTOR improve time-dependent C statistics by 4.6\\% over state-of-the-art, improve label efficiency by up to 95\\%, and are more rob",
    "openreview_id": "NialiwI2V6",
    "forum_id": "NialiwI2V6"
  },
  "analysis_timestamp": "2026-01-06T14:42:43.672517"
}
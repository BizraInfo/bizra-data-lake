{
  "prior_works": [
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
      "year": 2020,
      "role": "Foundational diffusion modeling framework enabling iterative denoising and stochastic sampling",
      "relationship_sentence": "The paper\u2019s core idea\u2014using a generic diffusion backbone to regress dense flow/depth fields and to obtain Monte Carlo samples for uncertainty\u2014directly builds on DDPM\u2019s probabilistic formulation and training objective."
    },
    {
      "title": "Denoising Diffusion Implicit Models",
      "authors": "Jiaming Song, Chenlin Meng, Stefano Ermon",
      "year": 2020,
      "role": "Non-Markovian/implicit diffusion sampling enabling efficient inference and backprop through sampling steps",
      "relationship_sentence": "Their step-unrolled denoising diffusion training and efficient inference are facilitated by DDIM-style deterministic trajectories, which make it practical to unroll and optimize over multiple denoising steps."
    },
    {
      "title": "Palette: Image-to-Image Diffusion Models",
      "authors": "Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, Mohammad Norouzi",
      "year": 2022,
      "role": "Conditional diffusion for pixel-wise image-to-image prediction with uncertainty",
      "relationship_sentence": "Palette demonstrated that a single diffusion U-Net can solve diverse pixel-to-pixel tasks with calibrated uncertainty, motivating this paper\u2019s use of a task-agnostic diffusion model for dense regression (flow/depth) rather than bespoke architectures."
    },
    {
      "title": "RePaint: Inpainting using Denoising Diffusion Probabilistic Models",
      "authors": "Andreas Lugmayr, Martin Danelljan, Andreas Romero, Radu Timofte",
      "year": 2022,
      "role": "Masked conditioning and infilling with diffusion processes",
      "relationship_sentence": "RePaint\u2019s masked inpainting mechanism inspired the paper\u2019s \u2018infilling\u2019 strategy to cope with noisy and incomplete supervision by masking unreliable pixels and learning to denoise/fill them during diffusion training."
    },
    {
      "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
      "authors": "Zachary Teed, Jia Deng",
      "year": 2020,
      "role": "SOTA optical flow architecture and synthetic\u2192real training protocol",
      "relationship_sentence": "RAFT provides the dominant task-specific baseline and the mixed synthetic+real training recipe that this paper deliberately replaces with a task-agnostic diffusion model while retaining the effective data curriculum for fair comparison."
    },
    {
      "title": "Monodepth2: Self-Supervised Monocular Depth Estimation",
      "authors": "Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, Gabriel J. Brostow",
      "year": 2019,
      "role": "Robust self-supervised pretraining via photometric consistency and auto-masking",
      "relationship_sentence": "The paper leverages Monodepth2-style self-supervised photometric objectives for pretraining, before supervised fine-tuning, to initialize the diffusion model for monocular depth without requiring dense ground truth."
    },
    {
      "title": "Image Super-Resolution via Iterative Refinement (SR3)",
      "authors": "Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, Mohammad Norouzi",
      "year": 2021,
      "role": "Coarse-to-fine/cascaded diffusion refinement to higher resolutions",
      "relationship_sentence": "SR3\u2019s cascaded diffusion upsampling informs the paper\u2019s zero-shot coarse-to-fine refinement strategy, enabling high-resolution depth/flow estimates from lower-resolution diffusion predictions."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central contribution\u2014recasting optical flow and monocular depth estimation as conditional diffusion inference with a task-agnostic architecture\u2014rests on advances in diffusion modeling and in dense vision training protocols. DDPM provides the probabilistic denoising backbone and sampling-based uncertainty that make Monte Carlo inference for ambiguous depth/flow natural. DDIM contributes efficient, deterministic trajectories that enable the authors\u2019 step-unrolled training, letting gradients flow through multiple denoising steps for better learning under imperfect supervision. Building on Palette, the work embraces conditional diffusion for pixel-level image-to-image prediction, showing that a single diffusion U-Net can replace specialized regression heads and hand-crafted losses while yielding calibrated uncertainty.\nTo handle sparse/noisy labels endemic to flow and depth datasets, the authors adopt an infilling paradigm inspired by RePaint\u2019s masked inpainting, training the model to denoise and complete missing regions. The data strategy is anchored in the optical flow community\u2019s playbook: RAFT established both the benchmark to beat and the effective synthetic-to-real curriculum that this paper retains while discarding RAFT\u2019s task-specific recurrent matching design. For depth, Monodepth2\u2019s self-supervised photometric pretraining offers a strong, label-efficient initialization before supervised fine-tuning. Finally, SR3\u2019s cascaded diffusion for super-resolution motivates the paper\u2019s zero-shot coarse-to-fine refinement, enabling high-resolution outputs without retraining specialized heads. Together, these works directly scaffold a generic diffusion approach that attains state-of-the-art dense geometry with principled uncertainty.",
  "analysis_timestamp": "2026-01-06T23:33:35.587595"
}
{
  "prior_works": [
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Backbone multimodal instruction-tuned framework",
      "relationship_sentence": "GeoLLaVA-8K extends the LLaVA recipe for aligning a vision encoder with an LLM to the remote-sensing domain and 8K inputs, inheriting its conversational training pipeline while adapting it to ultra-high-resolution imagery."
    },
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "role": "Visual token bottleneck via Perceiver Resampler",
      "relationship_sentence": "Flamingo\u2019s resampler demonstrated compressing dense visual tokens into a compact set for language models, directly motivating GeoLLaVA-8K\u2019s design goal of controlling token explosion from UHR inputs."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "role": "Token-bottleneck alignment via Q-Former",
      "relationship_sentence": "BLIP-2 showed that a learned query-based token bottleneck can preserve semantic content for LLMs, informing GeoLLaVA-8K\u2019s strategy to reduce tokens while retaining key semantics."
    },
    {
      "title": "TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?",
      "authors": "Michael S. Ryoo et al.",
      "year": 2021,
      "role": "Learned salient-token selection",
      "relationship_sentence": "TokenLearner\u2019s finding that a small, salient subset of tokens suffices for downstream reasoning underpins GeoLLaVA-8K\u2019s Anchored Token Selection that concentrates tokens around informative regions."
    },
    {
      "title": "Token Merging: Your ViT But Faster",
      "authors": "Christopher R. Bolya et al.",
      "year": 2023,
      "role": "Token reduction by merging redundant patches",
      "relationship_sentence": "ToMe established that many ViT tokens are redundant and can be merged with minimal loss, directly inspiring GeoLLaVA-8K\u2019s Background Token Pruning to remove homogeneous regions (e.g., ocean/forest) in RS images."
    },
    {
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "authors": "Peter Anderson et al.",
      "year": 2018,
      "role": "Object-centric region features for VQA",
      "relationship_sentence": "The bottom-up attention paradigm showed that object/region proposals provide strong VQA signals, informing GeoLLaVA-8K\u2019s object-anchored token selection around salient proposals in ultra-large RS scenes."
    },
    {
      "title": "RSVQA: Visual Question Answering for Remote Sensing",
      "authors": "B. Lobry et al.",
      "year": 2020,
      "role": "Foundational RS VQA benchmark and task design",
      "relationship_sentence": "RSVQA established RS-specific VQA tasks and evaluation, directly shaping GeoLLaVA-8K\u2019s multi-task dialogue design and motivating the creation of higher-resolution SuperRS-VQA and HighRS-VQA datasets."
    }
  ],
  "synthesis_narrative": "GeoLLaVA-8K\u2019s core advance\u2014scaling multimodal LLMs to ultra-high-resolution remote-sensing imagery while curbing token explosion\u2014sits at the intersection of instruction-tuned VLM design, token bottlenecking, and object-centric representation. LLaVA provides the alignment blueprint for coupling a vision encoder with an LLM via visual instruction tuning; GeoLLaVA-8K adopts this paradigm and adapts it to RS data and 8K inputs. Flamingo and BLIP-2 independently demonstrated that funneling dense visual features through a compact, learned token interface (Perceiver Resampler, Q-Former) preserves semantic fidelity for language models, directly motivating GeoLLaVA-8K\u2019s emphasis on reducing visual tokens before LLM consumption. Complementing bottlenecks, TokenLearner and Token Merging (ToMe) established that many vision tokens are redundant and that selecting or merging to a small salient set retains task performance. These insights underpin GeoLLaVA-8K\u2019s two targeted mechanisms: Background Token Pruning (aggressively removing homogeneous, low-information regions prevalent in RS) and Anchored Token Selection (retaining tokens around informative regions). The latter draws from the bottom-up attention line of work in VQA, where object/region proposals serve as strong primitives for reasoning, here repurposed to anchor tokens in massive RS scenes. Finally, RSVQA defined the RS-VQA problem space and evaluation, exposing gaps in resolution and task diversity that GeoLLaVA-8K addresses with SuperRS-VQA and HighRS-VQA. Together, these works directly shaped the model\u2019s architecture choices, token-sparsification strategies, and dataset design.",
  "analysis_timestamp": "2026-01-07T00:02:04.920790"
}
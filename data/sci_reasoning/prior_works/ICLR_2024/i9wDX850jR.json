{
  "prior_works": [
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Soudry et al.",
      "year": 2018,
      "arxiv_id": "1710.10345",
      "role": "Foundation",
      "relationship_sentence": "This work established that gradient descent on logistic loss converges in direction to the max-margin classifier, providing the core margin-maximization principle the present paper leverages to characterize learned features."
    },
    {
      "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
      "authors": "Lyu and Li",
      "year": 2019,
      "arxiv_id": "1906.05890",
      "role": "Extension",
      "relationship_sentence": "By extending max-margin implicit bias to deep homogeneous networks trained with exponential-tailed losses, this paper enables the present work to apply margin-based feature selection arguments to the stylized neural architectures it analyzes."
    },
    {
      "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks",
      "authors": "Gunasekar et al.",
      "year": 2018,
      "arxiv_id": "1806.00468",
      "role": "Inspiration",
      "relationship_sentence": "This paper connects implicit bias to Fourier-domain structure in linear conv nets, directly motivating the present work\u2019s use of Fourier features as the max-margin-selected representation for modular addition."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "authors": "Power et al.",
      "year": 2022,
      "arxiv_id": "2201.02177",
      "role": "Foundation",
      "relationship_sentence": "This work introduced modular addition as a canonical algorithmic task and highlighted puzzling circuit choices during grokking, setting the problem context the present paper analyzes normatively."
    },
    {
      "title": "Progress Measures for Grokking via Mechanistic Interpretability",
      "authors": "Nanda et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "By empirically showing that networks implement modular addition via Fourier-phase features while lacking a principled \u2018why,\u2019 this work posed the gap the present paper fills with a margin-based explanation."
    },
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Cohen and Welling",
      "year": 2016,
      "arxiv_id": "1602.07576",
      "role": "Related Problem",
      "relationship_sentence": "This paper established irreducible group representations as the natural feature basis for group-structured data, which the present work shows emerge even without built-in equivariance via margin maximization."
    }
  ],
  "synthesis_narrative": "Soudry et al. formalized that, on separable data, gradient descent with logistic loss converges to the maximum-margin direction, identifying margin maximization as the operative inductive bias of training. Lyu and Li extended this principle to deep homogeneous networks trained with exponential-tailed losses, ensuring that margin arguments apply beyond linear models to the kinds of neural architectures typically used for feature learning. Gunasekar et al. connected implicit bias to analysis in the Fourier domain for linear convolutional networks, demonstrating that gradient descent prefers margin-maximizing solutions structured in frequency space. Power et al. introduced modular addition as a compact algorithmic task central to grokking studies, making it a standard benchmark for probing how networks implement algebraic computations. Nanda et al. then provided mechanistic evidence that trained models solve modular addition via Fourier-phase features, but left open a normative account of why those features are chosen among many possibilities. Complementarily, Cohen and Welling showed that irreducible representations form the canonical feature basis for group-structured problems, linking group theory to neural representations. Taken together, these works suggest a natural synthesis: if gradient-based training implicitly maximizes margin, and algebraic tasks admit canonical Fourier/irrep bases, then margin considerations should select those bases even without built-in equivariance or hand-crafted features. The present paper operationalizes this insight by proving that margin maximization alone fully specifies the emergent features on modular addition, sparse parities, and finite group operations\u2014recovering Fourier features and irreps as the margin-optimal representations.",
  "target_paper": {
    "title": "Feature emergence via margin maximization: case studies in algebraic tasks",
    "authors": "Depen Morwani, Benjamin L. Edelman, Costin-Andrei Oncescu, Rosie Zhao, Sham M. Kakade",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "inductive bias, margin maximization, feature learning, mechanistic interpretability",
    "abstract": "Understanding the internal representations learned by neural networks is a cornerstone challenge in the science of machine learning. While there have been significant recent strides in some cases towards understanding *how* neural networks implement specific target functions, this paper explores a complementary question -- *why* do networks arrive at particular computational strategies? \nOur inquiry focuses on the algebraic learning tasks of modular addition, sparse parities, and finite group operations. Our primary theoretical findings analytically characterize the features learned by stylized neural networks for these algebraic tasks. Notably, our main technique demonstrates how the principle of margin maximization alone can be used to fully specify the features learned by the network. \nSpecifically, we prove that the trained networks utilize Fourier features to perform modular addition and employ features corresponding to irreducible group-theoretic representations to perform compos",
    "openreview_id": "i9wDX850jR",
    "forum_id": "i9wDX850jR"
  },
  "analysis_timestamp": "2026-01-06T17:07:54.686051"
}
{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, et al.",
      "year": 2021,
      "role": "Foundation model / base vision-language encoder",
      "relationship_sentence": "Robust CLIP targets the CLIP vision encoder that underpins many LVLMs; its method builds directly on CLIP\u2019s image-text alignment objective and deployment pattern as a frozen, swappable encoder."
    },
    {
      "title": "Visual Instruction Tuning (LLaVA)",
      "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",
      "year": 2023,
      "role": "Representative LVLM using a frozen CLIP vision encoder",
      "relationship_sentence": "LLaVA exemplifies LVLMs that depend on a frozen CLIP image encoder; Robust CLIP\u2019s plug-and-play robust encoder is designed to drop into such systems and confer adversarial robustness without retraining the LVLM."
    },
    {
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "authors": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu",
      "year": 2018,
      "role": "Core adversarial training paradigm (PGD-based)",
      "relationship_sentence": "The paper\u2019s adversarial fine-tuning is grounded in PGD-style adversarial training, using inner maximization over input perturbations to harden the CLIP encoder."
    },
    {
      "title": "Theoretically Principled Trade-off between Robustness and Accuracy (TRADES)",
      "authors": "Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, Michael I. Jordan",
      "year": 2019,
      "role": "Regularized adversarial training objective",
      "relationship_sentence": "TRADES informs the loss-design perspective\u2014balancing natural performance with adversarial robustness\u2014guiding how to fine-tune CLIP without degrading its downstream utility."
    },
    {
      "title": "Virtual Adversarial Training: A Regularization Method for Supervised and Semi-supervised Learning",
      "authors": "Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Shin Ishii",
      "year": 2018,
      "role": "Unsupervised adversarial regularization",
      "relationship_sentence": "Robust CLIP\u2019s unsupervised adversarial fine-tuning echoes VAT\u2019s core idea\u2014enforce prediction/representation consistency under adversarial perturbations without requiring labels."
    },
    {
      "title": "Do Adversarially Robust ImageNet Models Transfer Better?",
      "authors": "Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, et al.",
      "year": 2020,
      "role": "Transfer of robustness from pretraining to downstream",
      "relationship_sentence": "This work supports the central premise that robustness acquired during pretraining/fine-tuning can transfer to diverse downstream tasks, motivating a robust CLIP encoder that benefits LVLMs and zero-shot classification."
    },
    {
      "title": "Robust Fine-Tuning of Zero-Shot Models",
      "authors": "Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, et al.",
      "year": 2022,
      "role": "Stability-preserving fine-tuning of CLIP-like models",
      "relationship_sentence": "Demonstrates that careful fine-tuning of CLIP can improve robustness/generalization without catastrophic degradation, informing the design choice to modify the encoder itself rather than retrain LVLMs."
    }
  ],
  "synthesis_narrative": "Robust CLIP builds on the observation that modern LVLMs, typified by LLaVA, rely on a frozen CLIP vision encoder as a modular component. CLIP established image\u2013text alignment that enables zero-shot transfer, and its widespread reuse makes it a single point of failure for vision-side adversarial attacks. The proposed solution\u2014unsupervised adversarial fine-tuning of the CLIP encoder\u2014combines principles from adversarial training and unsupervised consistency regularization to harden this shared backbone while preserving its utility across tasks.\n\nThe inner\u2013outer optimization framing from PGD-based adversarial training provides the mechanism to expose and discourage worst-case perturbation vulnerabilities, while TRADES offers guidance on balancing robustness with accuracy to avoid overfitting to adversarial examples. VAT contributes the key insight that adversarial regularization can be carried out without labels by enforcing local smoothness of model predictions or representations under adversarial perturbations\u2014crucial for a scalable, unsupervised regimen compatible with CLIP. Two lines of work justify the bet that such robustness will generalize: Salman et al. show that adversarially robust features learned during pretraining can transfer to downstream tasks, and Wortsman et al. show that carefully fine-tuning zero-shot models like CLIP can improve robustness and OOD performance without sacrificing their broad applicability. Together, these works motivate and scaffold Robust CLIP\u2019s core contribution: a plug-and-play, adversarially fine-tuned CLIP encoder that lifts robustness for LVLMs and zero-shot classification without retraining the multimodal stack.",
  "analysis_timestamp": "2026-01-06T23:42:48.063453"
}
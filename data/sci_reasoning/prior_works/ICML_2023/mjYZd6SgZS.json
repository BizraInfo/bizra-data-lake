{
  "prior_works": [
    {
      "title": "Learning to predict by the methods of temporal differences",
      "authors": "Richard S. Sutton",
      "year": 1988,
      "role": "Baseline",
      "relationship_sentence": "This paper defined both Monte Carlo (direct regression on returns) and temporal-difference prediction and framed the core comparison the ICML\u201923 work sharpens\u2014TD vs direct estimation\u2014against which the new asymptotic MSE reductions are quantified."
    },
    {
      "title": "An Analysis of Temporal-Difference Learning with Function Approximation",
      "authors": "John N. Tsitsiklis and Benjamin Van Roy",
      "year": 1997,
      "role": "Foundation",
      "relationship_sentence": "It established the projected fixed-point view and contraction-based analysis of TD in Markov chains, providing the theoretical framework the current paper leverages to analyze TD\u2019s estimation error structure rather than mere convergence."
    },
    {
      "title": "Linear Least-Squares Algorithms for Temporal Difference Learning",
      "authors": "Steven J. Bradtke and Andrew G. Barto",
      "year": 1996,
      "role": "Extension",
      "relationship_sentence": "By casting TD as a least-squares fit to temporal inconsistency (LSTD), this work formalized the exact objective whose statistical advantages the ICML\u201923 paper characterizes via the inverse trajectory pooling coefficient."
    },
    {
      "title": "Technical Note: Least-Squares Temporal Difference Learning",
      "authors": "Justin A. Boyan",
      "year": 2002,
      "role": "Extension",
      "relationship_sentence": "This refinement/popularization of LSTD(\u03bb) operationalized minimizing TD error in practice, and the ICML\u201923 theory directly explains when such TD-style estimators beat direct return regression in asymptotic MSE."
    },
    {
      "title": "Finite-Sample Analysis of Least-Squares Policy Evaluation",
      "authors": "Alessandro Lazaric, Mohammad Ghavamzadeh, and R\u00e9mi Munos",
      "year": 2010,
      "role": "Gap Identification",
      "relationship_sentence": "While providing finite-sample guarantees for LSTD, it did not compare TD estimators to direct return regression nor identify structural quantities governing the advantage\u2014gaps the ICML\u201923 paper closes with its precise MSE reduction characterization."
    },
    {
      "title": "A Finite-Time Analysis of Temporal Difference Learning With Linear Function Approximation",
      "authors": "Jalaj Bhandari, Daniel Russo, and Raghav Singal",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "This work analyzed TD\u2019s learning dynamics and rates but left open whether and by how much TD is statistically superior to direct estimation; the ICML\u201923 paper answers this by giving exact asymptotic MSE reductions and new structural measures."
    },
    {
      "title": "Markov Chains and Mixing Times",
      "authors": "David A. Levin, Yuval Peres, and Elizabeth L. Wilmer",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "Classical coupling and meeting-time techniques from this work underpin the ICML\u201923 paper\u2019s trajectory crossing time concept and its resulting bounds on state-to-state value-difference estimation error."
    }
  ],
  "synthesis_narrative": "The core innovation of the ICML\u201923 paper is to provide a crisp asymptotic theory that quantifies when and why temporal-difference (TD) learning statistically outperforms direct regression on returns. Sutton\u2019s 1988 paper established the very comparison\u2014Monte Carlo versus TD\u2014and introduced the idea of fitting predictions by enforcing temporal consistency, which this work scrutinizes from a statistical-efficiency viewpoint. Tsitsiklis and Van Roy\u2019s analysis supplied the fixed-point and contraction framework for TD in Markov chains, setting the stage for examining not only convergence but the structure of estimation error. Bradtke and Barto\u2019s LSTD, together with Boyan\u2019s LSTD(\u03bb), concretized TD as minimizing temporal inconsistency in a least-squares sense; the present paper directly analyzes the statistical benefits of that objective, introducing the inverse trajectory pooling coefficient to characterize percent MSE reduction over direct regression. Prior learning-theoretic results on TD and LSTD\u2014such as Lazaric, Ghavamzadeh, and Munos\u2019 finite-sample bounds and Bhandari, Russo, and Singal\u2019s finite-time analysis\u2014identified rates and stability but did not resolve whether TD confers intrinsic statistical advantages relative to direct estimation; those gaps motivate and are explicitly addressed by the new asymptotic comparisons. Finally, the paper\u2019s novel \u201ctrajectory crossing time\u201d bound on value differences draws on coupling and meeting-time ideas from the Markov-chain literature (Levin, Peres, and Wilmer), enabling sharp problem-structure\u2013dependent guarantees that can be much tighter than horizon-based bounds.",
  "analysis_timestamp": "2026-01-06T23:09:26.560209"
}
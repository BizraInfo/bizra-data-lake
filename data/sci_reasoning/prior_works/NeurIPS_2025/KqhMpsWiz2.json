{
  "prior_works": [
    {
      "title": "Reversible Jump Markov Chain Monte Carlo computation and Bayesian model determination",
      "authors": "Peter J. Green",
      "year": 1995,
      "role": "Foundational transdimensional Bayesian inference",
      "relationship_sentence": "The paper\u2019s core goal\u2014efficient inference across models with differing dimensionality\u2014directly replaces RJMCMC\u2019s dimension-matching machinery with an amortized variational alternative using a single CoSMIC flow."
    },
    {
      "title": "Bayesian model choice via Markov chain Monte Carlo methods",
      "authors": "Bradley P. Carlin, Siddhartha Chib",
      "year": 1995,
      "role": "Product-space embedding for multi-model inference",
      "relationship_sentence": "CoSMIC\u2019s use of a single variational density with contextually masked, identity-mapped (inactive) components mirrors Carlin\u2013Chib\u2019s product-space idea of padding parameters and using pseudo-priors to embed multiple models in a common super-parameter space."
    },
    {
      "title": "Variational Inference with Normalizing Flows",
      "authors": "Danilo J. Rezende, Shakir Mohamed",
      "year": 2015,
      "role": "Flows for variational inference",
      "relationship_sentence": "The proposed transdimensional amortized density extends flow-based VI beyond fixed-dimensional targets by adapting the flow architecture and objective from Rezende\u2013Mohamed to handle model-index\u2013dependent active dimensions."
    },
    {
      "title": "Masked Autoregressive Flow for Density Estimation",
      "authors": "George Papamakarios, Theo Pavlakou, Iain Murray",
      "year": 2017,
      "role": "Autoregressive flow with masking and conditioning",
      "relationship_sentence": "CoSMIC builds on MAF\u2019s masked autoregressive parameterization by making the mask context-dependent so that only parameters of the active model are transformed while others are identity-mapped."
    },
    {
      "title": "Density estimation using Real NVP",
      "authors": "Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio",
      "year": 2017,
      "role": "Coupling layers with identity-mapped subsets",
      "relationship_sentence": "The identity-mapped components central to CoSMIC\u2019s transdimensional handling generalize RealNVP\u2019s coupling layers by conditioning which components remain identity via the model context."
    },
    {
      "title": "Auto-Encoding Variational Bayes",
      "authors": "Diederik P. Kingma, Max Welling",
      "year": 2014,
      "role": "Amortized variational inference and reparameterization",
      "relationship_sentence": "CoSMIC leverages amortization across a family of targets (different models) and uses pathwise gradients for continuous parameters, directly inheriting the amortized VI paradigm introduced by VAEs."
    },
    {
      "title": "Categorical Reparameterization with Gumbel-Softmax",
      "authors": "Eric Jang, Shixiang Gu, Ben Poole",
      "year": 2017,
      "role": "Differentiable optimization over discrete choices",
      "relationship_sentence": "Training the contextually-specified masking over a discrete model index draws on Gumbel-Softmax\u2013style relaxations to enable low-variance Monte Carlo gradient estimation for transdimensional variational training."
    }
  ],
  "synthesis_narrative": "Amortized Variational Transdimensional Inference fuses ideas from transdimensional Bayesian computation, amortized variational learning, and flow architectures to produce a single variational density that spans a union of model-specific parameter spaces. Green\u2019s Reversible Jump MCMC established the canonical setup and challenges of dimension-changing inference, while Carlin\u2013Chib\u2019s product-space formulation suggested a unifying super-parameterization with padded inactive components\u2014an idea CoSMIC operationalizes by identity-mapping nonactive parameters within one shared flow. On the optimization side, VAEs introduced amortized variational inference and the reparameterization trick, setting the stage for learning a single inference network across many conditionals; Rezende\u2013Mohamed extended this to normalizing flows, enabling expressive variational families trained with stochastic gradients. CoSMIC specifically leverages masked, conditional flow designs: MAF contributes an autoregressive, mask-driven parameterization that readily accepts contextual inputs, and RealNVP contributes the architectural notion of identity-mapped subsets via coupling, which CoSMIC generalizes by conditioning the mask on the model index to realize transdimensionality. Finally, optimizing over discrete model indices requires Monte Carlo gradient estimators; Gumbel-Softmax provides a practical, low-variance relaxation that informs CoSMIC\u2019s training strategy alongside standard pathwise and score-function estimators. Together, these works directly enable CoSMIC\u2019s contextually-specified masking and its stochastic variational transdimensional training procedure, bridging classical multi-model Bayesian inference with modern amortized flow-based VI.",
  "analysis_timestamp": "2026-01-07T00:21:32.310370"
}
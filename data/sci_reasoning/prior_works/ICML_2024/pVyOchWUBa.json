{
  "prior_works": [
    {
      "title": "Understanding deep learning requires rethinking generalization",
      "authors": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals",
      "year": 2017,
      "role": "Conceptual backdrop on generalization in overparameterized regimes",
      "relationship_sentence": "This influential work catalyzed the modern focus on why deep networks generalize, setting up the very paradigm the position paper argues is insufficient for explaining key LLM behaviors beyond test loss."
    },
    {
      "title": "Deep Double Descent: Where Bigger Models and More Data Hurt",
      "authors": "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever",
      "year": 2020,
      "role": "Empirical generalization phenomenon motivating the interpolation-era perspective",
      "relationship_sentence": "By framing generalization through interpolation and double descent, this paper exemplifies the statistical-test-loss lens that the ICML position paper contends must be complemented with a non-identifiability perspective for LLMs."
    },
    {
      "title": "Underspecification Presents Challenges for Credibility in Modern Machine Learning",
      "authors": "Alexander D\u2019Amour et al.",
      "year": 2020,
      "role": "Empirical and conceptual precedent on multiple solutions with similar metrics but different behaviors",
      "relationship_sentence": "Provides a direct conceptual antecedent to the claim that models with indistinguishable test metrics can exhibit divergent behaviors, which the position paper sharpens for autoregressive LMs via KL-based non-identifiability."
    },
    {
      "title": "Algebraic Geometry and Statistical Learning Theory",
      "authors": "Sumio Watanabe",
      "year": 2009,
      "role": "Theoretical foundation on non-identifiability and singular models",
      "relationship_sentence": "Supplies the formal grounding that many probabilistic models are non-identifiable, underpinning the paper\u2019s core argument that AR models can be near-KL-equivalent yet behaviorally different."
    },
    {
      "title": "Generalization Without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks (SCAN)",
      "authors": "Brenden M. Lake, Marco Baroni",
      "year": 2018,
      "role": "Benchmark and paradigm for zero-shot rule extrapolation",
      "relationship_sentence": "Directly informs the paper\u2019s case study on zero-shot rule extrapolation by illustrating how models can fit training distributions yet fail to apply rules compositionally, despite similar likelihoods."
    },
    {
      "title": "Rethinking the Role of Demonstrations: What makes in-context learning work?",
      "authors": "Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi",
      "year": 2022,
      "role": "Empirical analysis of in-context learning phenomena",
      "relationship_sentence": "Provides empirical evidence about the brittleness and variability of ICL, supporting the paper\u2019s claim of approximate non-identifiability of ICL behaviors at near-constant next-token loss."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "role": "Alignment methodology relying on KL regularization",
      "relationship_sentence": "Because RLHF and related methods control divergence via KL to a base model, this work motivates the paper\u2019s argument that small KL (hence similar test loss) does not guarantee similar downstream behaviors."
    }
  ],
  "synthesis_narrative": "The ICML 2024 position paper argues that understanding LLMs requires looking beyond statistical generalization and test loss, centering instead on the inherent non-identifiability of autoregressive (AR) probabilistic models. The modern generalization discourse\u2014sparked by Zhang et al. and deepened by phenomena like deep double descent\u2014frames success via test loss on in-distribution samples. However, D\u2019Amour et al. show that underspecification allows many solutions with indistinguishable metrics yet divergent behavior, a theme the position paper sharpens for AR LMs: models with zero or near-zero KL divergence (and thus similar test loss) can act very differently. This claim is theoretically anchored by Watanabe\u2019s singular learning theory, which formalizes non-identifiability in probabilistic models and explains why parameter or functional equivalence need not be behaviorally unique.\n\nThe paper\u2019s case studies connect these ideas to LLM practice. SCAN exemplifies zero-shot rule extrapolation: models can assign high likelihood to training data yet fail to apply learned rules compositionally\u2014evidence that likelihood equality does not pin down rule behavior. In-context learning work by Min et al. reveals that seemingly similar next-token performance masks substantial variability in ICL behavior depending on demonstrations and prompts, supporting the paper\u2019s claim of approximate non-identifiability in ICL mechanisms. Finally, RLHF (Ouyang et al.) operationalizes KL constraints to align models, yet the position paper argues that small KL does not guarantee similar downstream behavior\u2014making alignment outcomes contingent on choices not reflected in test loss. Together, these works directly motivate and substantiate the paper\u2019s core thesis: KL/test-loss equivalence is insufficient to explain or guarantee key LLM behaviors, necessitating a perspective beyond statistical generalization.",
  "analysis_timestamp": "2026-01-06T23:42:48.052258"
}
{
  "prior_works": [
    {
      "title": "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks",
      "authors": "Saeed Entezari et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "This work\u2019s core idea generalizes Entezari et al.\u2019s permutation-based alignment for achieving LMC in neural networks to tree ensembles by adding tree-specific symmetries (subtree flips and split-order invariance) to the alignment procedure."
    },
    {
      "title": "Git Re-Basin: Merging Models Modulo Permutation Symmetries",
      "authors": "Sam Ainsworth et al.",
      "year": 2022,
      "arxiv_id": "2209.04836",
      "role": "Related Problem",
      "relationship_sentence": "The re-basin framework that aligns models into a common basin via permutation matching directly motivates our symmetry-alignment approach and informs how to enable merging/linear interpolation after accounting for tree-specific invariances."
    },
    {
      "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis",
      "authors": "Jonathan Frankle et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "We adopt their linear mode connectivity formulation\u2014interpolating between independently trained models\u2014and target achieving this phenomenon for tree ensembles rather than neural networks."
    },
    {
      "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of Deep Neural Networks",
      "authors": "Timur Garipov et al.",
      "year": 2018,
      "arxiv_id": "1802.10026",
      "role": "Foundation",
      "relationship_sentence": "Their demonstration that distinct minima can be connected through low-loss paths frames our objective of constructing parameter-space connectors for non-neural architectures like differentiable tree ensembles."
    },
    {
      "title": "Deep Neural Decision Forests",
      "authors": "Peter Kontschieder et al.",
      "year": 2015,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "We build on their differentiable probabilistic-routing trees/forests as the base soft tree architecture whose inherent symmetries (e.g., child/subtree flips) we formalize and exploit to attain LMC."
    },
    {
      "title": "Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data",
      "authors": "Sergei Popov et al.",
      "year": 2019,
      "arxiv_id": "1909.06312",
      "role": "Baseline",
      "relationship_sentence": "NODE serves as a representative soft tree ensemble whose structure (oblivious trees) highlights split-order invariance, enabling our symmetry-aware alignment to realize LMC and support parameter-space interpolation/merging."
    }
  ],
  "synthesis_narrative": "Mode connectivity was first established by showing that optima in neural networks can be connected by low-loss paths, revealing that disparate solutions need not be isolated (Garipov et al.). Subsequent work sharpened this to linear mode connectivity, demonstrating that simple linear interpolation between independently trained models can preserve performance (Frankle et al.). A key mechanistic insight emerged when permutation symmetries of hidden units were explicitly aligned: by matching neurons across networks, independent solutions could be brought into a shared basin, yielding linear connectors in weight space (Entezari et al.). This symmetry perspective also enabled practical model merging via weight-space operations once permutations were resolved (Ainsworth et al.). In parallel, differentiable decision trees and forests with probabilistic routing established a trainable, gradient-based tree ensemble family (Kontschieder et al.), and modern soft tree ensembles like NODE showed strong practical value and specific structural constraints such as oblivious splits that imply ordering symmetries (Popov et al.). Together, these threads exposed a gap: LMC and symmetry alignment were well-understood for neural nets but unexplored for differentiable tree ensembles, whose architectures embody distinct invariances beyond mere permutation of components. By synthesizing permutation alignment with tree-specific symmetries\u2014subtree flip and split-order invariance\u2014this work extends the LMC paradigm beyond neural networks, enabling linear interpolation and parameter-based merging for soft tree ensembles and explaining when independently trained trees can be aligned into a common basin.",
  "target_paper": {
    "title": "Linear Mode Connectivity in Differentiable Tree Ensembles",
    "authors": "Ryuichi Kanoh, Mahito Sugiyama",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Linear Mode Connectivity, Soft Tree",
    "abstract": "Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for understanding the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. ",
    "openreview_id": "UqYNPyotxL",
    "forum_id": "UqYNPyotxL"
  },
  "analysis_timestamp": "2026-01-06T18:50:18.798692"
}
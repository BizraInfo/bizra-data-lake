{
  "prior_works": [
    {
      "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning",
      "authors": "Ilia Toneva et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Introduced the notion of example-level forgetting events and operationalized how to detect 'forgotten' training examples, which this paper directly builds on by forecasting which upstream examples will be forgotten after an update."
    },
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Provided the core idea of predicting how parameter updates induced by one example affect the loss on other examples; the present work adopts this causal viewpoint to forecast harm (forgetting) on upstream examples caused by online corrections."
    },
    {
      "title": "Estimating Training Data Influence by Tracing Gradient Descent (TracIn)",
      "authors": "Garima Pruthi et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Showed that inner products of gradients across checkpoints can approximate influence; the paper\u2019s black-box classifier that uses inner-product signals to predict which pretraining instances will be forgotten is a direct adaptation of this influence-as-inner-product principle."
    },
    {
      "title": "Gradient Episodic Memory for Continual Learning",
      "authors": "David Lopez-Paz et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "Established gradient-conflict (inner-product) signals as indicators of interference and forgetting in continual learning, which underpins this paper\u2019s use of inner-product-based criteria to forecast forgetting and guide replay."
    },
    {
      "title": "Online Continual Learning with Maximal Interfered Retrieval",
      "authors": "Rahaf Aljundi et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Proposed selecting replay items whose loss would increase most under a new update by approximating interference; this paper generalizes the idea from heuristic selection to explicit forecasting of which upstream examples will be forgotten to drive targeted replay."
    },
    {
      "title": "Gradient-based Sample Selection for Online Continual Learning",
      "authors": "Rahaf Aljundi et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated gradient-similarity heuristics for replay selection but with limited controllability and reliance on local estimates; the current work addresses this gap by learning predictors that forecast forgotten upstream examples, reducing variance and improving controllability."
    },
    {
      "title": "Tiny Episodic Memories in Continual Learning",
      "authors": "Arslan Chaudhry et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "Popularized reservoir-style/random replay as a strong, simple baseline in continual learning; this paper explicitly shows such random replay is high-variance and improves upon it by forecasting and replaying specifically the examples predicted to be forgotten."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014forecasting which upstream examples a language model will forget after an update\u2014emerges from a clear lineage in example-level forgetting and influence estimation. Toneva et al. established the very construct of example forgetting events, giving this work its target variable: identifying forgotten examples. Koh and Liang provided the causal lens for anticipating how an update driven by one example impacts the loss on others, which this paper repurposes to forecast harm from online corrections to pretraining data points. Building on practical influence estimation, TracIn showed that inner products of gradients across checkpoints approximate influence, directly inspiring the paper\u2019s black-box inner-product classifier for predicting forgetting. In continual learning, Lopez-Paz and Ranzato\u2019s GEM framed gradient conflicts (via inner products) as signals of interference, a conceptual backbone for using inner-product signals to anticipate forgetting. Aljundi\u2019s MIR and gradient-based sample selection further demonstrated that predicting which samples will suffer maximal interference can guide replay; the present work extends these heuristics into a learned forecasting framework that explicitly predicts forgotten upstream examples, thereby improving controllability and reducing variance. Finally, Chaudhry et al. popularized random/ reservoir replay as a baseline; this paper\u2019s results and motivation directly respond to its limitations by replacing unguided replay with forecasts that prioritize at-risk upstream examples.",
  "analysis_timestamp": "2026-01-06T23:09:26.433690"
}
{
  "prior_works": [
    {
      "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
      "authors": "Michael McCloskey et al.",
      "year": 1989,
      "role": "Foundation",
      "relationship_sentence": "Introduced the core phenomenon of catastrophic forgetting in sequential learning, which this paper identifies as the principal mechanism undermining RL fine-tuning when pre-trained capabilities are not revisited."
    },
    {
      "title": "Overcoming Catastrophic Forgetting in Neural Networks",
      "authors": "James Kirkpatrick et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "Provides the Fisher-based parameter-importance regularization (EWC) that the paper directly applies during RL fine-tuning to constrain drift from the pre-trained solution and mitigate forgetting on unvisited state subspaces."
    },
    {
      "title": "Learning without Forgetting",
      "authors": "Zhizhong Li et al.",
      "year": 2016,
      "role": "Extension",
      "relationship_sentence": "Introduces distillation to a previous model as a knowledge-retention loss; the paper adopts an LwF-style KL/distillation from the pre-trained policy to preserve behavior on states not encountered early in fine-tuning."
    },
    {
      "title": "Continual Learning Through Synaptic Intelligence",
      "authors": "Friedemann Zenke et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "Proposes path-integral\u2013based parameter importance; the paper evaluates this class of synaptic-importance regularizers as a direct forgetting-mitigation mechanism for RL fine-tuning."
    },
    {
      "title": "Policy Distillation",
      "authors": "Andrei A. Rusu et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "Establishes KL-based imitation of a teacher policy; the paper leverages this idea by distilling from the pre-trained policy during fine-tuning to explicitly retain pre-trained capabilities."
    },
    {
      "title": "Progressive Neural Networks",
      "authors": "Andrei A. Rusu et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrates catastrophic forgetting in multi-task RL and addresses it via architectural isolation; the present paper pinpoints the same forgetting as the root cause of failed fine-tuning and shows it can be mitigated without added capacity."
    },
    {
      "title": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning",
      "authors": "Emilio Parisotto et al.",
      "year": 2016,
      "role": "Related Problem",
      "relationship_sentence": "Uses distillation for RL transfer and highlights instability when naively fine-tuning across tasks; this work builds on that insight by framing fine-tuning failures as forgetting and enforcing retention of the pre-trained policy."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central insight\u2014that failures in RL fine-tuning stem from catastrophic forgetting of pre-trained capabilities\u2014rests on a lineage that begins with the foundational identification of catastrophic interference in sequential learning by McCloskey and Cohen. Translating that phenomenon into modern deep learning, Kirkpatrick et al.\u2019s EWC and Zenke et al.\u2019s Synaptic Intelligence established principled ways to quantify parameter importance and regularize updates to prevent forgetting. Li and Hoiem\u2019s Learning without Forgetting and Rusu et al.\u2019s Policy Distillation provided the complementary paradigm of knowledge distillation to a prior model, offering a direct and practical mechanism to preserve behavior while adapting to new objectives. In RL specifically, Rusu et al.\u2019s Progressive Neural Networks highlighted how naive fine-tuning leads to forgetting and proposed architectural isolation to avoid interference\u2014clearly surfacing the gap that forgetting, not merely optimization difficulty, is the culprit. Parisotto et al.\u2019s Actor-Mimic further showed that distillation can enable transfer across RL tasks but that stability during adaptation is fragile without explicit retention pressures. Building on these threads, the current paper reframes RL fine-tuning as a forgetting-mitigation problem exacerbated by partial state visitation. It then directly deploys the above retention techniques\u2014EWC/SI regularization and LwF-style distillation\u2014to constrain drift from the pre-trained policy, demonstrating on NetHack and Montezuma\u2019s Revenge that preserving behavior on unvisited subspaces is the key to unlocking the anticipated transfer gains.",
  "analysis_timestamp": "2026-01-06T23:09:26.409730"
}
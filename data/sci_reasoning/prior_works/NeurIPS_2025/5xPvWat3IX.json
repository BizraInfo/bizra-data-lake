{
  "prior_works": [
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
      "authors": "Patrick Lewis et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Vgent inherits the RAG formulation\u2014separating retrieval and generation\u2014but shows that flat passage retrieval disrupts temporal dependencies in long videos, directly motivating its graph-structured video retrieval and added reasoning step."
    },
    {
      "title": "Self-RAG: Learning to Retrieve, Generate, and Critique for Improved Language Modeling",
      "authors": "Akari Asai et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "Vgent extends Self-RAG\u2019s idea of an intermediate, self-reflective reasoning stage by adapting it to LVLMs and coupling the reasoning stage with a structured graph retriever over video clips."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Vgent\u2019s plan\u2013retrieve\u2013reason loop is directly inspired by ReAct\u2019s interleaving of reasoning traces with tool use, here instantiating the \u2018act\u2019 step as graph-based video retrieval to guide LVLM reasoning."
    },
    {
      "title": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering",
      "authors": "Akari Asai et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "The core notion of retrieving along structured multi-hop paths informs Vgent\u2019s construction of video graphs and retrieval of clip chains that preserve temporal and semantic dependencies."
    },
    {
      "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "authors": "Ranjay Krishna et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Vgent\u2019s representation of videos as nodes and semantic relations builds on the scene-graph paradigm introduced by Visual Genome, extending it to temporal clip-level relations for retrieval."
    },
    {
      "title": "MovieQA: Understanding Stories in Movies through Question-Answering",
      "authors": "Makarand Tapaswi et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "By formalizing long-form movie understanding as QA and exposing the need for long-range reasoning, MovieQA sets the problem context that Vgent targets with graph-structured retrieval and intermediate reasoning."
    },
    {
      "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
      "authors": "Maciej Besta et al.",
      "year": 2024,
      "role": "Inspiration",
      "relationship_sentence": "Vgent\u2019s idea of structuring intermediate reasoning and evidence around graph relationships echoes Graph-of-Thoughts\u2019 graph-structured reasoning, adapted here to video clips and relations."
    }
  ],
  "synthesis_narrative": "Vgent fuses two lines of progress to address long video understanding: retrieval-augmented generation and graph-structured reasoning. The RAG framework of Lewis et al. established the core separation of retrieval and generation, but its flat passage retrieval struggles with long-range, ordered dependencies\u2014precisely the failure mode Vgent targets in videos. To make retrieval reasoning-aware, Vgent draws on Self-RAG\u2019s intermediate critique/planning stage and ReAct\u2019s interleaving of reasoning and tool-use, instantiating the \u2018tool\u2019 as a graph retriever over video clips so the LVLM can plan what to fetch before answering. The decision to represent videos as graphs is grounded in the scene-graph paradigm popularized by Visual Genome, extended temporally to encode clip-level entities, events, and relations. This connects tightly with Asai et al. (2019), which showed that retrieving along graph paths better supports multi-hop reasoning; Vgent applies that insight to preserve temporal and semantic links across clips, mitigating disrupted dependencies and irrelevant retrieval. MovieQA framed the core task\u2014story-level video QA demanding global context\u2014which motivates Vgent\u2019s emphasis on long-horizon evidence organization. Finally, Graph-of-Thoughts inspires Vgent\u2019s graph-centric intermediate reasoning, aligning the model\u2019s reasoning steps with the graph structure of retrieved video evidence. Together, these works directly shape Vgent\u2019s graph-based retrieval plus intermediate reasoning design for robust long video understanding.",
  "analysis_timestamp": "2026-01-06T23:08:23.968781"
}
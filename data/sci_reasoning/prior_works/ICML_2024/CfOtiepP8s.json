{
  "prior_works": [
    {
      "title": "A Mathematical Framework for Transformer Circuits",
      "authors": "Nelson Elhage et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Provided the core circuit-level decomposition of transformers into interacting attention heads and MLPs, directly enabling this paper\u2019s head/MLP-level analysis of arithmetic computation."
    },
    {
      "title": "In-Context Learning and Induction Heads",
      "authors": "Catherine Olsson et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated that specific attention heads implement algorithmic behaviors (induction), inspiring the search for specialized heads that lock onto operands/operators during arithmetic."
    },
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Showed that MLP layers store and transform information, motivating the paper\u2019s finding that operand information is aggregated and progressively computed through MLPs to yield final answers."
    },
    {
      "title": "Analyzing Multi-Head Self-Attention: Specialized Heads in the Transformer",
      "authors": "Elena Voita et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Introduced head-level importance analysis via masking/pruning and showed specialization, which directly informed identifying the small subset (<5%) of pivotal heads used in arithmetic."
    },
    {
      "title": "Are Sixteen Heads Really Better than One?",
      "authors": "Paul Michel et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Established that many heads are redundant but did not explain which heads matter for specific computations; this paper addresses that gap by pinpointing operand/operator-focused heads critical for arithmetic."
    },
    {
      "title": "A Mechanistic Interpretability Analysis of Grokking",
      "authors": "Neel Nanda et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "Built arithmetic circuits (e.g., modular addition) in transformers and analyzed progressive computation, which this paper extends from synthetic modular tasks to natural-language arithmetic with transferable pivotal heads/MLPs."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME)",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "Pioneered targeted, causally grounded interventions on specific MLP components; the current paper adapts this style of localized intervention to improve arithmetic reliability by acting on identified pivotal heads/MLPs."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core innovation\u2014isolating a tiny set of attention heads that focus on operands/operators and showing that MLPs progressively compute arithmetic solutions, with these components transferable across datasets/tasks\u2014emerges directly from the mechanistic interpretability lineage. Elhage et al.\u2019s framework for transformer circuits established the analytical substrate for decomposing models into attention and MLP components, while Geva et al. demonstrated that MLPs act as key\u2013value memories that store and transform information, motivating the hypothesis that operand information is accumulated and processed in MLPs. Voita et al. and Michel et al. introduced head-level specialization and pruning analyses, respectively; their findings that only a few heads are essential directly inspired the methodology to identify the small fraction of pivotal arithmetic heads and highlighted the gap of linking head importance to specific algorithmic roles. Olsson et al.\u2019s discovery of induction heads showed that attention heads can implement concrete algorithmic functions, catalyzing the search for heads specialized for arithmetic operand/operator tracking. Nanda et al.\u2019s mechanistic analysis of grokking in modular arithmetic provided a blueprint for how arithmetic circuits emerge and compute progressively, which this work extends to natural-language arithmetic and demonstrates to be transferable across datasets and tasks. Finally, Meng et al.\u2019s ROME established a causal, component-level intervention paradigm; this paper leverages that style of targeted manipulation on the identified pivotal heads/MLPs to not only interpret but also improve LLM arithmetic reliability.",
  "analysis_timestamp": "2026-01-06T23:09:26.456342"
}
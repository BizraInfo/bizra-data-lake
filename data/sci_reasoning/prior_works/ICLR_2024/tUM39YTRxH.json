{
  "prior_works": [
    {
      "title": "Policy invariance under reward transformations: Theory and application to reward shaping",
      "authors": "Andrew Y. Ng et al.",
      "year": 1999,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work formalized potential-based reward shaping, providing the theoretical basis and motivation for constructing dense, shaped rewards that preserve optimal policies, which the paper operationalizes through programmatically generated dense rewards."
    },
    {
      "title": "Maximum Entropy Inverse Reinforcement Learning",
      "authors": "Brian D. Ziebart et al.",
      "year": 2008,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "As a canonical IRL approach requiring demonstrations to infer rewards, it establishes the data-dependent baseline that the paper explicitly sidesteps by generating rewards directly from language without expert trajectories."
    },
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano et al.",
      "year": 2017,
      "arxiv_id": "1706.03741",
      "role": "Gap Identification",
      "relationship_sentence": "By showing that learning reward models from pairwise human comparisons is effective but costly, it highlights the data and annotation burden that the paper addresses via data-free, LLM-authored reward programs guided only by textual goals."
    },
    {
      "title": "Grounding English Commands to Reward Functions",
      "authors": "James MacGlashan et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper established the natural language-to-reward formulation, demonstrating that instructions can be mapped to executable reward semantics, which the paper scales up with general-purpose LLM program synthesis instead of task-specific parsers."
    },
    {
      "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
      "authors": "Daniel Freeman et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "It directly inspired using LLMs to author reward code, and the paper extends this idea to produce shaped, dense reward programs grounded in compact state representations with iterative refinement rather than largely heuristic or sparse formulations."
    },
    {
      "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
      "authors": "Guanzhi Wang et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing LLMs can write executable, environment-aware code that leverages existing APIs and libraries, it provided the coding paradigm the paper repurposes to emit reward functions that call simulation/tool packages."
    },
    {
      "title": "Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning",
      "authors": "Rodrigo Toro Icarte et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "This work demonstrated interpretable, program-like structured reward representations and their benefits for RL, motivating the paper\u2019s choice of free-form, editable reward programs to achieve interpretability without automata constraints."
    }
  ],
  "synthesis_narrative": "Potential-based reward shaping established that dense, shaped rewards can preserve optimal behavior while accelerating learning, anchoring the idea that reward design should extend beyond sparse terminal signals. Maximum Entropy IRL and related apprenticeship learning formalized inferring rewards from demonstrations, but at the cost of requiring expert data and solving difficult inverse problems. Human-preference-based reward learning further showed that good rewards can be learned from comparisons, while making clear the heavy annotation and data requirements. Earlier work in mapping natural language to reward functions proved that instructions can be grounded into executable reward semantics via parsing, albeit with task-specific datasets and constrained formalisms. In parallel, LLMs were shown capable of authoring executable, environment-aware code that calls external APIs, as exemplified by open-ended agents generating skills via code. Most directly, coding LLMs have been used to author reward programs that enable challenging skills, highlighting the feasibility and power of programmatic reward design.\nTogether, these threads exposed an opportunity: combine the interpretability and learning advantages of shaped rewards with the data-free, executable code synthesis abilities of LLMs, while avoiding the data demands of IRL and preference learning. The paper synthesizes these insights by prompting LLMs to generate free-form, dense reward programs grounded in compact environment states and existing packages, then enabling iterative human-in-the-loop refinement. This is a natural next step that inherits the structure and editability of programmatic rewards, the grounding and tool-use of LLM code generation, and the theoretical guarantees motivating shaping, to deliver practical, interpretable reward design without demonstrations or pairwise labels.",
  "target_paper": {
    "title": "Text2Reward: Reward Shaping with Language Models for Reinforcement Learning",
    "authors": "Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "reinforcement learning; large language models; robotics",
    "abstract": "Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation and shaping of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates shaped dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 man",
    "openreview_id": "tUM39YTRxH",
    "forum_id": "tUM39YTRxH"
  },
  "analysis_timestamp": "2026-01-06T12:27:08.516284"
}
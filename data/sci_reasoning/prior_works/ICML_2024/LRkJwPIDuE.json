{
  "prior_works": [
    {
      "title": "Taming Transformers for High-Resolution Image Synthesis",
      "authors": "Patrick Esser et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "VideoPoet\u2019s core design\u2014modeling visual content as sequences of discrete codebook tokens with an autoregressive Transformer\u2014directly builds on Esser et al.\u2019s VQ tokenization + Transformer paradigm."
    },
    {
      "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
      "authors": "Ming Ding et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "CogVideo demonstrated text-conditioned video generation by autoregressively modeling discrete video tokens; VideoPoet extends this approach into a large decoder-only LLM trained on a broader mixture of multimodal generative objectives and conditioning signals."
    },
    {
      "title": "Phenaki: Variable Length Video Generation from Open Domain Text",
      "authors": "Ruben Villegas et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "Phenaki introduced scalable Transformer-based generation over discrete video tokens for long/variable-length text-to-video; VideoPoet generalizes this line by adopting an LLM training recipe across modalities and tasks for zero-shot video generation."
    },
    {
      "title": "MAGVIT: Masked Generative Video Transformer",
      "authors": "Lijun Yu et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "VideoPoet relies on MAGVIT-style discrete video tokenizers to convert frames into compact code sequences, enabling its decoder-only LLM to operate over video tokens and unify video with text and audio streams."
    },
    {
      "title": "AudioLM: a Language Modeling Approach to Audio Generation",
      "authors": "Zalan Borsos et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "AudioLM established that audio can be discretized into tokens and modeled with next-token prediction; VideoPoet adopts this tokenization-and-AR modeling strategy to ingest and generate audio alongside video and text."
    },
    {
      "title": "A Generalist Agent",
      "authors": "Scott Reed et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Gato\u2019s single decoder-only Transformer trained on heterogeneous modalities and tasks directly inspired VideoPoet\u2019s LLM-style pretraining and task-adaptation over interleaved multimodal token streams."
    },
    {
      "title": "Imagen Video: High Definition Video Generation with Diffusion Models",
      "authors": "Jonathan Ho et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "Imagen Video set a leading diffusion-based baseline for text-to-video quality; VideoPoet positions its LLM-based approach as a unified alternative that addresses diffusion pipelines\u2019 limited flexibility for multi-conditioning and long-range motion."
    }
  ],
  "synthesis_narrative": "VideoPoet\u2019s central idea\u2014treating video, image, text, and audio as a single stream of tokens for decoder-only autoregressive modeling\u2014rests on the discrete-token generative paradigm inaugurated by Esser et al., which enabled Transformers to synthesize high-fidelity visuals via VQ codebooks. That image-centric recipe was pushed to video by CogVideo and Phenaki, which modeled sequences of video tokens conditionally from text and at variable lengths. VideoPoet directly extends these transformer-over-discrete-video lines by scaling to a large LLM, adopting a unified decoder-only architecture, and training with a mixture of multimodal generative objectives to support diverse conditioning (text, image, video, audio) and zero-shot use. This unification hinges on robust tokenizers: MAGVIT provides the practical video codebooks VideoPoet uses to convert frames into tokens that fit naturally into the LLM\u2019s vocabulary, while AudioLM established the viability of AR token modeling for audio, enabling VideoPoet to condition on and generate audio within the same framework. Conceptually, VideoPoet\u2019s training protocol echoes Gato\u2019s generalist strategy\u2014pretraining a single transformer on heterogeneous modalities and tasks, then adapting\u2014bringing that philosophy to high-quality video synthesis. Finally, diffusion-based systems like Imagen Video set the quality baseline but highlighted gaps in unified multi-conditioning and long-range motion control; VideoPoet directly targets these gaps with an LLM-based, single-model approach that achieves strong zero-shot motion fidelity across conditioning types.",
  "analysis_timestamp": "2026-01-06T23:09:26.474883"
}
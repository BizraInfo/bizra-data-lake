{
  "prior_works": [
    {
      "title": "EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow",
      "authors": "Revaud et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work formalized dense matching as a data term plus a hand-crafted prior (edge-aware smoothness/interpolation), providing the explicit likelihood\u2013prior decomposition that DiffMatch re-instantiates with a learned generative prior."
    },
    {
      "title": "PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",
      "authors": "Sun et al.",
      "year": 2018,
      "arxiv_id": "1709.02371",
      "role": "Foundation",
      "relationship_sentence": "By popularizing deep cost volumes as the learned data term for dense correspondence, PWC-Net established the conditioning signal (matching cost) that DiffMatch directly consumes in its conditional diffusion process."
    },
    {
      "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
      "authors": "Teed et al.",
      "year": 2020,
      "arxiv_id": "2003.12039",
      "role": "Baseline",
      "relationship_sentence": "RAFT\u2019s strong cost-volume + iterative update framework serves as the primary learned-data-term baseline whose failure modes in textureless/repetitive regions motivate DiffMatch\u2019s explicit prior injection via diffusion."
    },
    {
      "title": "CATs: Cost Aggregation Transformers for Visual Correspondence",
      "authors": "Kim et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "CATs introduced transformer-based cost aggregation for dense matching; DiffMatch directly extends this cost-aggregation view by treating the aggregated matching cost as the likelihood input to a diffusion sampler that imposes a learned prior over correspondence fields."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Song et al.",
      "year": 2021,
      "arxiv_id": "2011.13456",
      "role": "Foundation",
      "relationship_sentence": "This paper provides the score-based diffusion framework and denoising objectives that DiffMatch adopts to learn and sample from a prior over dense correspondence fields."
    },
    {
      "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
      "authors": "Chung et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "DPS\u2019s key idea of combining a measurement-consistency (likelihood) term with a learned diffusion prior during sampling directly inspires DiffMatch\u2019s formulation p(correspondence|cost) \u221d p(cost|correspondence) p_prior(correspondence), where the matching cost plays the data term."
    }
  ],
  "synthesis_narrative": "Classical optical flow and correspondence methods explicitly decomposed the task into a data term and a prior, as exemplified by EpicFlow, which enforced edge-aware regularization on sparse matches to resolve ambiguities through a hand-designed prior. Deep learning shifted focus to learning the data term: PWC-Net established modern cost-volume conditioning via pyramids and warping, and RAFT advanced this paradigm by building a dense all-pairs cost volume with iterative refinement, achieving strong accuracy but relying on the network to implicitly learn priors. Transformer-based cost aggregation, such as CATs, further improved how matching evidence is pooled, yielding sharper and more discriminative cost maps yet still leaving inherent ambiguities (textureless regions, repetitive patterns, large motions) unresolved without an explicit prior. In parallel, score-based diffusion models provided a principled generative framework for learning priors (via SDEs and denoising) and, crucially, diffusion posterior sampling demonstrated how to fuse a likelihood term with a learned prior during the generative process to solve inverse problems. Together these works revealed a gap: strong learned data terms exist for dense matching, but explicit, learnable priors had not been integrated into inference. The natural synthesis is to treat the aggregated matching cost as a likelihood and impose a learned correspondence prior via a conditional diffusion sampler, unifying the classical energy view with modern diffusion guidance to robustly disambiguate matches under challenging conditions.",
  "target_paper": {
    "title": "Diffusion Model for Dense Matching",
    "authors": "Jisu Nam, Gyuseong Lee, Sunwoo Kim, Hyeonsu Kim, Hyoungwon Cho, Seyeon Kim, Seungryong Kim",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Diffusion Models, Visual Correspondence",
    "abstract": "The objective for establishing dense correspondence between paired images con- sists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, re- cent approaches have focused on learning the data term with deep neural networks without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, large displacements, or noises. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms for dense matching. This is accomplished by leveraging a conditional denoising diffusion model that explic- itly takes matching cost and injects the prior within generative process. However, limite",
    "openreview_id": "Zsfiqpft6K",
    "forum_id": "Zsfiqpft6K"
  },
  "analysis_timestamp": "2026-01-06T18:06:38.065061"
}
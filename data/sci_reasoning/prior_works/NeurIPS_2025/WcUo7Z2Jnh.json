{
  "prior_works": [
    {
      "title": "o1: Large Reasoning Models and test-time compute scaling",
      "authors": "OpenAI",
      "year": 2024,
      "role": "Exemplar LRM and problem motivation",
      "relationship_sentence": "o1 established the modern LRM paradigm of scaling test-time compute with long \u2018deliberate\u2019 reasoning traces, providing the primary empirical setting where the paper observes frequent thought switching and motivating both the underthinking metric and the proposed decoding penalty."
    },
    {
      "title": "DeepSeek R1: Reinforcement Learning for Reasoning",
      "authors": "DeepSeek-AI",
      "year": 2024,
      "role": "Open-source LRM baseline and testbed",
      "relationship_sentence": "R1 operationalized long-form reasoning via RL, giving an open model where the authors could measure thought-switching behavior and validate that penalizing premature transitions improves depth and accuracy."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "role": "Conceptual foundation for explicit \"thought\" units",
      "relationship_sentence": "By formalizing step-by-step \u2018thoughts\u2019 in outputs, this work provided the representational basis for defining and detecting \u2018thought switching,\u2019 which the paper quantifies and directly targets during decoding."
    },
    {
      "title": "Self-Consistency Improves Chain-of-Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2022,
      "role": "Multi-path reasoning and variance analysis",
      "relationship_sentence": "Self-consistency showed accuracy gains from sampling diverse reasoning paths; the present paper complements this by showing that excessive path-switching within a single trajectory can be harmful and proposes a decoding bias toward deeper single-path exploration."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Search over reasoning states and switching control",
      "relationship_sentence": "ToT introduced explicit branching and search over \u2018thought\u2019 states; the new decoding penalty can be viewed as biasing toward depth-first continuation rather than frequent branch switching, and the paper\u2019s switching metric echoes ToT\u2019s state/transition view."
    },
    {
      "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
      "authors": "Denny Zhou et al.",
      "year": 2023,
      "role": "Structured planning to reduce erratic reasoning",
      "relationship_sentence": "L2M demonstrated that imposing a plan and solving subproblems sequentially improves reasoning; the paper\u2019s penalty operationalizes a similar principle at decoding time by discouraging premature plan changes (thought switches) to foster deeper exploration."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014diagnosing underthinking as excessive thought switching in long reasoning models and mitigating it with a decoding-time switching penalty\u2014builds on the evolution of explicit and structured reasoning in LLMs. Chain-of-Thought prompting established \u2018thoughts\u2019 as first-class units, enabling the authors to operationalize and measure switching between reasoning segments. Self-Consistency showed that exploring multiple reasoning paths can improve accuracy across samples; this paper refines that insight by revealing a trade-off within a single trajectory: breadth via frequent switches can undermine depth, especially on hard math, prompting a decoding bias toward sustained exploration of promising paths.\nTree of Thoughts further framed reasoning as search over states, clarifying the notion of transitions and when to expand versus continue. The proposed thought-switching penalty effectively nudges decoding toward depth-first continuation, reducing unnecessary transitions that correlate with errors. Least-to-Most prompting\u2019s emphasis on planning and orderly subproblem solving informs the intuition that stability and adherence to a plan (fewer switches) yield better outcomes.\nFinally, OpenAI\u2019s o1 and DeepSeek\u2019s R1 instantiated LRMs that scale test-time compute, providing both the motivation and the platforms where underthinking emerges in practice. By combining these strands\u2014explicit thoughts, multi-path/search perspectives, structured planning, and LRM testbeds\u2014the paper introduces a measurable notion of underthinking (token efficiency in incorrect answers) and a practical decoding intervention that improves depth and accuracy.",
  "analysis_timestamp": "2026-01-07T00:21:32.266327"
}
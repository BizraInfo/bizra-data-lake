{
  "prior_works": [
    {
      "title": "Controlling the false discovery rate: a practical and powerful approach to multiple testing",
      "authors": "Yoav Benjamini et al.",
      "year": 1995,
      "role": "Foundation",
      "relationship_sentence": "The proposed adaptive frame-rate procedure is built around applying the Benjamini\u2013Hochberg step-up rule to decide when video-level inference is reliable, directly using FDR control as its single governing hyperparameter."
    },
    {
      "title": "Online rules for control of false discovery rate",
      "authors": "Adel Javanmard et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "This work\u2019s perspective on sequential/online FDR control informs the paper\u2019s framing of per-frame evidence accumulation as a sequential multiple testing problem, guiding how error is controlled while frames arrive over time."
    },
    {
      "title": "Sequential Tests of Statistical Hypotheses",
      "authors": "Abraham Wald",
      "year": 1945,
      "role": "Foundation",
      "relationship_sentence": "The paper adopts Wald\u2019s core idea of stopping as soon as sufficient evidence is accumulated, recasting it for video FER by replacing SPRT thresholds with an FDR-controlled multiple testing criterion."
    },
    {
      "title": "AdaFrame: Adaptive Frame Selection for Fast Video Recognition",
      "authors": "Wu et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "AdaFrame established that adaptive frame selection yields large compute savings in video tasks but requires training a policy network; the current paper addresses this gap with a plug-and-play, training-free, statistically principled stopping rule."
    },
    {
      "title": "Adaptive Computation Time for Recurrent Neural Networks",
      "authors": "Alex Graves",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "ACT\u2019s idea of input-adaptive halting in sequential processing directly inspires treating frames as a sequence and deciding when to stop, with this paper replacing learned halting with BH-based FDR guarantees."
    },
    {
      "title": "BranchyNet: Fast Inference via Early Exiting from Deep Networks",
      "authors": "Suradech Teerapittayanon et al.",
      "year": 2016,
      "role": "Related Problem",
      "relationship_sentence": "BranchyNet showed compute can be reduced via confidence-based early exits; this paper advances that notion by providing a statistically grounded (FDR-controlled) exit criterion without modifying the backbone."
    },
    {
      "title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
      "authors": "Limin Wang et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "TSN popularized fixed-rate, uniform frame sampling with simple aggregation; the proposed method directly improves on this fixed-sampling baseline by adaptively selecting how many frames to process per clip."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014adaptive frame-rate selection for video facial expression recognition with a single, interpretable error parameter\u2014emerges from unifying sequential decision-making with multiple testing control. At its heart lies Benjamini and Hochberg\u2019s false discovery rate (FDR) procedure, which the paper directly operationalizes as the stopping rule to declare a reliable video-level prediction. Wald\u2019s sequential hypothesis testing provides the foundational paradigm of halting once evidence suffices, while modern online FDR work (e.g., Javanmard and Montanari) motivates viewing arriving frames as a sequential multiple testing stream where error must be controlled as evidence accumulates. On the video efficiency side, AdaFrame demonstrated the practical value of adaptive frame selection but required training a policy and integrating it into a particular architecture; the present work fills that gap with a training-free, plug-in mechanism applicable to arbitrary feature extractors. Similarly, ideas from Adaptive Computation Time and early-exit networks like BranchyNet crystallized the benefit of input-adaptive halting, but relied on learned or heuristic confidence thresholds; this paper replaces those with a principled BH-based criterion that exposes a single hyperparameter: the target false acceptance rate. Finally, against common fixed-sampling practices exemplified by Temporal Segment Networks, the method contributes a statistically grounded, compute-aware alternative that processes fewer frames for easy clips and more for difficult ones while maintaining error control.",
  "analysis_timestamp": "2026-01-06T23:09:26.547071"
}
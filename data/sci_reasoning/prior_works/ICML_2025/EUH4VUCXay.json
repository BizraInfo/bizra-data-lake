{
  "prior_works": [
    {
      "title": "Chatbot Arena: An Open Platform for Evaluating Large Language Models",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "am-ELO directly replaces the Arena\u2019s Elo-based iterative update used to rank LLMs from pairwise human votes, targeting the observed leaderboard instability and lack of annotator modeling in this baseline."
    },
    {
      "title": "The Rating of Chessplayers, Past and Present",
      "authors": "Arpad E. Elo",
      "year": 1978,
      "role": "Foundation",
      "relationship_sentence": "The core framework builds on the Elo rating system\u2019s logistic win-probability and skill-score formulation, which am-ELO modifies and re-estimates via likelihood rather than iterative updates."
    },
    {
      "title": "Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons",
      "authors": "R. A. Bradley and M. E. Terry",
      "year": 1952,
      "role": "Foundation",
      "relationship_sentence": "m-ELO is essentially the MLE formulation for pairwise outcomes aligned with the Bradley\u2013Terry model, supplying the likelihood that replaces Elo\u2019s online update and underpins the paper\u2019s consistency and stability results."
    },
    {
      "title": "Solution to a Ranking Problem from Paired Comparisons",
      "authors": "L. R. Ford Jr.",
      "year": 1957,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s guarantees for stable, consistent ranking via MLE lean on classic results about existence/uniqueness and identifiability conditions for Bradley\u2013Terry MLE established by Ford."
    },
    {
      "title": "Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm",
      "authors": "A. P. Dawid and A. M. Skene",
      "year": 1979,
      "role": "Inspiration",
      "relationship_sentence": "am-ELO\u2019s joint estimation of model scores and annotator reliability is motivated by Dawid\u2013Skene\u2019s seminal idea of modeling annotator-specific reliabilities within a probabilistic MLE/EM framework."
    },
    {
      "title": "Crowd-BT: Crowdsourcing Using Paired Comparisons for Ranking",
      "authors": "Xi Chen et al.",
      "year": 2013,
      "role": "Extension",
      "relationship_sentence": "am-ELO extends the paired-comparison likelihood by integrating annotator ability in the win-probability\u2014an idea operationalized in Crowd-BT for joint inference of item scores and worker reliabilities."
    },
    {
      "title": "TrueSkill: A Bayesian Skill Rating System",
      "authors": "Ralf Herbrich et al.",
      "year": 2007,
      "role": "Related Problem",
      "relationship_sentence": "TrueSkill addresses instability/uncertainty in rating via Bayesian inference; am-ELO takes a different route\u2014deterministic MLE with annotator modeling\u2014while targeting the same volatility issues seen with Elo updates."
    }
  ],
  "synthesis_narrative": "am-ELO\u2019s core innovation\u2014replacing Elo\u2019s iterative updates with a maximum-likelihood estimator and explicitly modeling annotator ability\u2014emerges directly from two intellectual threads. First, Elo\u2019s rating system provides the foundational logistic win-probability and scalar skill representation that current LLM arenas (e.g., Chatbot Arena) operationalize to rank models from pairwise votes. However, Arena practice revealed instability and ignored annotator heterogeneity, forming the baseline and gap this work targets. The Bradley\u2013Terry model supplies the principled likelihood for paired comparisons that m-ELO adopts to supplant Elo\u2019s online updates. Classical results such as Ford\u2019s existence/uniqueness and identifiability conditions for Bradley\u2013Terry MLE ground the paper\u2019s formal guarantees of consistency and stability. Second, the annotator modeling component of am-ELO draws from the crowdsourcing literature: Dawid\u2013Skene\u2019s treatment of annotator-specific reliabilities via probabilistic MLE/EM motivates joint estimation of raters and items, while Crowd-BT demonstrates how to embed worker reliability directly into pairwise-comparison probabilities and estimate both item scores and worker skills simultaneously. Finally, alternative rating systems like TrueSkill show another path to tackle rating volatility via Bayesian uncertainty, helping frame the problem but leaving unaddressed the specific Arena issues of iterative-update instability and annotator variability. am-ELO synthesizes these lines: BT-style MLE for stable rankings and DS/Crowd-BT-style annotator modeling, directly improving the Elo-based Arena baseline.",
  "analysis_timestamp": "2026-01-06T23:07:19.602775"
}
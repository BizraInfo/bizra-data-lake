{
  "prior_works": [
    {
      "title": "Online Convex Optimization in the Bandit Setting",
      "authors": "Flaxman et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "This paper introduced randomized smoothing\u2013based gradient estimators from function values (bandit/zeroth-order feedback), which underpins the random-direction gradient oracle that ZPDVR averages to tame coordinate-wise variance."
    },
    {
      "title": "Random Gradient-Free Minimization of Convex Functions",
      "authors": "Nesterov and Spokoiny",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "It formalized the two-point Gaussian/spherical smoothing estimator and its bias/variance properties; ZPDVR builds directly on this estimator and designs a direction-averaging scheme to further reduce its variance."
    },
    {
      "title": "A Proximal Stochastic Gradient Method with Progressive Variance Reduction",
      "authors": "Xiao and Zhang",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "Prox-SVRG established the control-variate framework for composite (proximal) optimization, which ZPDVR adapts in zeroth-order form as the first layer that reduces sampling variance."
    },
    {
      "title": "SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient",
      "authors": "Nguyen et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "The recursive variance-reduction idea of SARAH informs ZPDVR\u2019s sampling-variance control in finite-sum/stochastic settings before adding the second (direction) averaging layer."
    },
    {
      "title": "SPIDER: Near-Optimal Nonconvex Optimization via Stochastic Path-Integrated Differential Estimator",
      "authors": "Fang et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "ZPDVR leverages SPIDER-style recursive estimators to reduce sampling variance efficiently in the proximal setting, and then augments them with a new averaging trick to also suppress coordinate-wise variance."
    },
    {
      "title": "Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization",
      "authors": "Liu et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "This work first married variance reduction with zeroth-order gradients and highlighted that random-direction estimators suffer a large coordinate-wise variance\u2014leading prior methods to use O(d) coordinate-wise finite differences\u2014precisely the gap ZPDVR closes by reducing that variance via averaging without O(d) queries."
    }
  ],
  "synthesis_narrative": "ZPDVR\u2019s core idea\u2014double variance reduction\u2014stands on two pillars: classical randomized smoothing for zeroth-order gradients and modern control-variate recursion for sampling variance. The randomized smoothing lineage starts with Flaxman\u2013Kalai\u2013McMahan\u2019s bandit formulation, which showed how to turn function evaluations into gradient surrogates, and is concretized by Nesterov\u2013Spokoiny\u2019s two-point Gaussian/spherical smoothing estimator whose variance properties guide ZPDVR\u2019s direction-averaging design. On the sampling side, Prox-SVRG introduced the proximal variance-reduction architecture for composite optimization, while SARAH and SPIDER refined recursion to achieve strong sampling-variance control with minimal full-gradient costs\u2014structures ZPDVR adapts in its proximal, zeroth-order inner loops. The immediate catalyst is Liu et al.\u2019s ZO-SVRG, which revealed a critical limitation: random-direction estimators induce a large coordinate-wise variance term that slows convergence unless one switches to O(d) coordinate-wise finite differences, effectively approximating first-order information and incurring prohibitive query complexity in high dimensions. ZPDVR directly targets this bottleneck. It keeps the efficient recursive control-variate machinery (for sampling variance) from SVRG/SARAH/SPIDER, but replaces the O(d) coordinate-wise estimator with a principled averaging of random-direction gradients that shrinks the coordinate-wise variance sufficiently\u2014achieving the benefits of variance reduction without first-order surrogates or dimension-dependent query inflation.",
  "analysis_timestamp": "2026-01-06T23:09:26.415679"
}
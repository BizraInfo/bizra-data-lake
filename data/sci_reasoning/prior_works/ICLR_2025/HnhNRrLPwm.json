{
  "prior_works": [
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "role": "Foundational interleaved vision\u2013language modeling",
      "relationship_sentence": "Flamingo introduced the core paradigm of interleaved image\u2013text processing and generation, directly motivating MMIE\u2019s focus on evaluating LVLMs\u2019 ability to handle arbitrarily interleaved multimodal inputs and outputs."
    },
    {
      "title": "IDEFICS: An Open Reproduction of Flamingo",
      "authors": "Hugo Lauren\u00e7on et al.",
      "year": 2023,
      "role": "Open-source interleaved LVLM enabling broad adoption",
      "relationship_sentence": "By making Flamingo-style interleaved modeling accessible, IDEFICS catalyzed practical multi-image, multi-turn usage scenarios that MMIE explicitly targets with its interleaved comprehension and generation tracks."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Conversational LVLMs and open-ended evaluation (LLaVA-Bench)",
      "relationship_sentence": "LLaVA and its In-the-Wild benchmark popularized open-ended, instruction-following evaluation using LLM-as-judge, highlighting cost/bias issues that MMIE addresses with mixed MC/open-ended formats and improved scoring protocols."
    },
    {
      "title": "MMBench: Is Your Multi-modal Model Really Better Than the Others?",
      "authors": "Junlin Yang et al.",
      "year": 2023,
      "role": "Fine-grained multi-skill MC benchmarking",
      "relationship_sentence": "MMBench\u2019s taxonomy-driven, multiple-choice design informed MMIE\u2019s large-scale, category/subfield organization and the use of MC items for reliable, low-cost, and comparable scoring."
    },
    {
      "title": "MMMU: A Massive Multi-discipline Multimodal Understanding Benchmark",
      "authors": "Shengbang Tong Yin et al.",
      "year": 2023,
      "role": "Knowledge-intensive, broad-coverage evaluation",
      "relationship_sentence": "MMMU demonstrated the need for holistic, academically oriented, knowledge-heavy multimodal testing, directly shaping MMIE\u2019s breadth across disciplines and its emphasis on rigorous, real-world knowledge demands."
    },
    {
      "title": "MM-Vet: Evaluating Large Multimodal Models for Generalization and Capability",
      "authors": "Yunxin Liu et al.",
      "year": 2023,
      "role": "Open-ended capability evaluation with LLM-based grading",
      "relationship_sentence": "MM-Vet\u2019s open-ended prompts and reliance on LLM graders exposed reliability and cost trade-offs that MMIE tackles via a hybrid evaluation scheme and more controlled scoring for interleaved outputs."
    },
    {
      "title": "MathVista: Evaluating Mathematical Reasoning in Visual Context",
      "authors": "Pan Lu et al.",
      "year": 2023,
      "role": "Domain-specific, reasoning-intensive multimodal benchmarking",
      "relationship_sentence": "MathVista\u2019s design for rigorous visual-math reasoning influenced MMIE\u2019s knowledge-intensive subfields (e.g., math, coding, physics) and its mix of MC and free-form items to probe deep reasoning."
    }
  ],
  "synthesis_narrative": "MMIE\u2019s core contribution\u2014a large-scale, knowledge-intensive benchmark that evaluates interleaved multimodal comprehension and generation with both multiple-choice and open-ended formats\u2014emerges from two converging lines of prior work. On the modeling side, Flamingo established the interleaved image\u2013text paradigm, and IDEFICS open-sourced this capability, making multi-image, multi-turn interleaving common in practice. LLaVA further pushed conversational, instruction-following LVLMs and popularized open-ended evaluation with LLM-as-judge, which revealed practical concerns about grading cost and bias.\n\nOn the benchmarking side, MMBench demonstrated the value of fine-grained, taxonomy-driven multiple-choice evaluation for reliability and comparability, while MMMU showed that broad, academically grounded, knowledge-intensive tasks are crucial for holistic assessment. Complementing these, MM-Vet highlighted open-ended capability testing and the pitfalls of automated judging, and MathVista exemplified rigorous domain-specific reasoning in visual contexts.\n\nMMIE synthesizes these strands: it centers explicitly on interleaved input/output scenarios born from Flamingo/IDEFICS-style modeling; adopts MMBench/MMMU\u2019s breadth and structure to cover many disciplines and subfields; and incorporates both MC and open-ended formats, informed by LLaVA-Bench and MM-Vet, while improving evaluation reliability and cost. The result is a benchmark that simultaneously captures the practical interleaved use cases of modern LVLMs and the depth of knowledge-intensive reasoning demanded by real-world applications.",
  "analysis_timestamp": "2026-01-06T23:42:48.091846"
}
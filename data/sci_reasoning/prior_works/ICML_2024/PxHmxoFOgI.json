{
  "prior_works": [
    {
      "title": "Gradient methods for minimizing composite functions",
      "authors": "Yurii Nesterov",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s generalized gradient mapping for constrained, nonsmooth objectives explicitly extends Nesterov\u2019s composite-gradient mapping framework, and the proposed approximate stationarity notions reduce to his classical measure in the smooth/composite special cases."
    },
    {
      "title": "Convergence rate of Frank\u2013Wolfe for non-convex objectives",
      "authors": "Simon Lacoste-Julien",
      "year": 2016,
      "role": "Extension",
      "relationship_sentence": "The authors directly generalize Lacoste-Julien\u2019s Frank\u2013Wolfe gap\u2014used as a stationarity measure in smooth nonconvex constrained problems\u2014to the nonsmooth setting and build their non-asymptotic guarantees around this generalized FW gap."
    },
    {
      "title": "Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming",
      "authors": "Saeed Ghadimi et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "The zeroth-order stochastic gradient estimators and non-asymptotic stationarity analysis for unconstrained nonconvex problems in Ghadimi\u2013Lan are adapted and extended here to constrained, nonsmooth objectives with new stationarity criteria."
    },
    {
      "title": "Random gradient-free minimization of convex functions",
      "authors": "Yurii Nesterov et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "The two-point Gaussian smoothing and its bias\u2013variance characterization underpin the paper\u2019s zeroth-order algorithms and convergence proofs in the constrained nonsmooth nonconvex regime."
    },
    {
      "title": "Online Convex Optimization in the Bandit Setting",
      "authors": "Abraham D. Flaxman et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "The one-point smoothing/gradient-estimation paradigm from bandit optimization motivates the black-box (zeroth-order) oracle model and smoothing techniques that this paper leverages for constrained problems."
    },
    {
      "title": "Proximal Stochastic Methods for Nonsmooth Nonconvex Finite-Sum Optimization",
      "authors": "Sashank J. Reddi et al.",
      "year": 2016,
      "role": "Related Problem",
      "relationship_sentence": "Their non-asymptotic analysis for proximal stochastic methods on nonsmooth nonconvex objectives (unconstrained/composite) provides the proximal/gradient-mapping stationarity template that this work adapts to constrained, zeroth-order settings."
    },
    {
      "title": "Proximally Guided Stochastic Subgradient Method for Nonsmooth Nonconvex Optimization",
      "authors": "Damek Davis et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "The notion of approximate stationarity via Moreau-envelope/gradient-mapping for nonsmooth nonconvex objectives in this line of work inspires the paper\u2019s generalized stationarity definitions and their non-asymptotic stochastic analysis under constraints."
    }
  ],
  "synthesis_narrative": "The core innovations of this ICML 2024 work\u2014generalizing gradient mapping and the Frank\u2013Wolfe (FW) gap to the nonsmooth constrained setting, defining corresponding approximate stationarity, and delivering non-asymptotic guarantees for stochastic zeroth-order methods\u2014sit at the intersection of three lines of prior art. First, the stationarity framework builds on the composite optimization literature: Nesterov\u2019s gradient mapping for composite minimization established the canonical projected/prox-based stationarity measure that the authors generalize to nonsmooth constrained problems; Reddi et al. and the proximally guided stochastic subgradient line (Davis et al.) showed how such measures yield non-asymptotic rates for nonsmooth nonconvex objectives, motivating the paper\u2019s nonsmooth notions and proof techniques. Second, on the projection-free side, Lacoste-Julien formalized the FW gap as a stationarity certificate for smooth nonconvex constrained optimization; this paper extends that certificate to nonsmooth objectives, enabling a unified constrained nonsmooth stationarity notion. Third, the zeroth-order algorithmic toolkit descends from bandit/DFO smoothing: Flaxman et al. introduced one-point smoothing and Nesterov\u2013Spokoiny established rigorous two-point Gaussian-smoothing estimators; Ghadimi\u2013Lan translated these estimators into non-asymptotic guarantees for unconstrained nonconvex stochastic programs. Liu et al. synthesize and extend these ideas by (i) crafting nonsmooth constrained stationarity surrogates (generalized gradient mapping and FW gap) and (ii) proving non-asymptotic convergence of stochastic zeroth-order methods to these targets, thereby closing the gap where prior constrained approaches largely offered only asymptotic guarantees.",
  "analysis_timestamp": "2026-01-06T23:09:26.478287"
}
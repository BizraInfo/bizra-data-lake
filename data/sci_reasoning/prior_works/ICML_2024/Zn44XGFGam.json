{
  "prior_works": [
    {
      "title": "Breaking the Curse of Dimensionality with Convex Neural Networks",
      "authors": "Francis Bach",
      "year": 2017,
      "role": "Foundational convexification of two-layer ReLU networks via variation/Barron norms",
      "relationship_sentence": "Bach\u2019s convex neural network framework established that positively homogeneous two-layer ReLU models can be cast as convex programs over function-space norms, directly motivating the paper\u2019s use of convex relaxations and guiding how weight decay links to tractable convex surrogates."
    },
    {
      "title": "Universal Approximation Bounds for Superpositions of a Sigmoidal Function",
      "authors": "Andrew R. Barron",
      "year": 1993,
      "role": "Introduced Barron space/norm underpinning convex function-space views of shallow networks",
      "relationship_sentence": "Barron\u2019s norm provides the foundational function-space regularization lens for shallow nets that underlies modern convex formulations; the present paper leverages this viewpoint to relate L2 weight decay in two-layer ReLUs to convex relaxations with provable approximation guarantees."
    },
    {
      "title": "Semidefinite Relaxations for Certifying Robustness of Neural Networks",
      "authors": "Aditi Raghunathan, Jacob Steinhardt, Percy Liang",
      "year": 2018,
      "role": "Pioneered SDP-based convex relaxations for ReLU networks with certified bounds",
      "relationship_sentence": "This work showed that carefully designed SDP relaxations can upper-bound nonconvex ReLU objectives, informing the paper\u2019s relaxation methodology and its analysis of optimality gaps\u2014here extended to average-case random data with an O(sqrt(log n)) factor."
    },
    {
      "title": "Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope",
      "authors": "Eric Wong, J. Zico Kolter",
      "year": 2018,
      "role": "Developed scalable LP-based convex outer polytope relaxations for ReLUs",
      "relationship_sentence": "Wong\u2013Kolter\u2019s tractable linear relaxations exemplify how ReLU nonconvexity can be bounded by convex programs; the present paper adapts the spirit of such convex outer approximations to training and proves tightness (up to log factors) under random data distributions."
    },
    {
      "title": "Gradient Descent Finds Global Minima of Over-parameterized Deep Neural Networks",
      "authors": "Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, Xiyu Zhai",
      "year": 2019,
      "role": "Established convergence of gradient descent for over-parameterized ReLU nets on random data",
      "relationship_sentence": "These results on optimization dynamics over random data support the paper\u2019s claim that local gradient methods reach low loss with high probability, complementing the convex relaxation by explaining why practical first-order methods succeed."
    },
    {
      "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
      "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song",
      "year": 2019,
      "role": "Provided NTK-style guarantees that GD attains small training error for wide nets",
      "relationship_sentence": "Allen\u2013Zhu et al. supply the analytical toolkit for proving convergence of local methods on random data, which the paper leverages to argue that gradient-based training attains low loss consistent with the near-optimal convex relaxation value."
    },
    {
      "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
      "authors": "Kaifeng Lyu, Jiwei Li",
      "year": 2019,
      "role": "Characterized implicit bias of GD toward margin-maximizing solutions in positively homogeneous nets",
      "relationship_sentence": "By linking training dynamics to norm/margin structure for ReLU networks, this work underpins the paper\u2019s use of weight decay and its connection to convex objectives, helping justify why the nonconvex training aligns with the convex surrogate up to logarithmic factors."
    }
  ],
  "synthesis_narrative": "This paper fuses two threads: convex formulations of shallow ReLU models and provable optimization dynamics under random data. The convex-function-space lineage (Barron; Bach) formalized two-layer networks as convex objects via variation/Barron norms, clarifying how explicit regularization (such as weight decay) can induce tractable convex surrogates. In parallel, the verification community (Wong\u2013Kolter; Raghunathan et al.) developed LP/SDP relaxations that upper-bound ReLU nonconvexity in polynomial time, demonstrating that tight convex outer approximations can yield certified bounds. Kim and Pilanci build on these insights but pivot the target: instead of worst-case verification, they analyze the training objective with L2 weight decay on random data, proving that a principled convex relaxation approximates the nonconvex optimum within an O(sqrt(log n)) relative gap\u2014an exponential improvement over prior guarantees.\n\nThe second pillar is optimization theory for over-parameterized networks on random designs (Du et al.; Allen-Zhu\u2013Li\u2013Song), which established that gradient descent achieves small training error with high probability. Together with implicit-bias results for homogeneous networks (Lyu\u2013Li), these works explain why local methods tend toward norm-regularized solutions aligned with convex surrogates. The present paper synthesizes these lines: it crafts a polynomial-time convex relaxation inspired by prior convex bounds for ReLUs, then uses random-data geometry and insights from GD analyses to prove near-optimality and to show that standard gradient methods reach comparably low loss. The result provides a unified, average-case explanation for the empirical success of local training and a concrete algorithmic relaxation with provable approximation guarantees.",
  "analysis_timestamp": "2026-01-07T00:02:04.887908"
}
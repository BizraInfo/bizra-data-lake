{
  "prior_works": [
    {
      "title": "Foundations of Bilevel Programming",
      "authors": "Stephan Dempe",
      "year": 2002,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This monograph formalizes the value-function (hyper-objective) formulation of bilevel optimization that the present paper analyzes and approximates via a penalty surrogate to relate values and derivatives."
    },
    {
      "title": "The Theory of Max-Min and Its Application to Weapons Allocation Problems",
      "authors": "John M. Danskin",
      "year": 1967,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "Danskin\u2019s envelope theorem underpins differentiability/subgradient characterizations of value functions with possibly multiple minimizers, which the paper sharpens into an explicit hyper-gradient formula under minimal conditions."
    },
    {
      "title": "Envelope Theorems for Regular and Irregular Problems",
      "authors": "Paul Milgrom and Ilya Segal",
      "year": 2002,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The envelope theorems provide directional-derivative characterizations of value functions without uniqueness, and the paper builds on this to prove O(\u03c3)-closeness of values/derivatives and to deliver an explicit gradient expression."
    },
    {
      "title": "Approximation of stationary points of a bilevel program",
      "authors": "Saeed Ghadimi and Mengdi Wang",
      "year": 2018,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "This work set a precedent for approximating stationary solutions in nonconvex bilevel problems, motivating the present paper\u2019s penalty-based lens and non-asymptotic analysis of first-order methods."
    },
    {
      "title": "A Two-Timescale Stochastic Approximation Scheme for Bilevel Optimization",
      "authors": "Mingyi Hong et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "As a main algorithmic baseline, this two-timescale SA framework (typically assuming a single-valued/strongly convex lower level) is replaced by the paper\u2019s penalty-driven first-order SA with guarantees under weaker, nonconvex and multi-solution settings."
    },
    {
      "title": "Optimizing Millions of Hyperparameters by Implicit Differentiation",
      "authors": "Jonathan Lorraine et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "This influential hypergradient approach relies on differentiating a unique lower-level solution, highlighting the gap that the paper fills by deriving an explicit hyper-objective gradient when the lower-level has multiple solutions."
    }
  ],
  "synthesis_narrative": "Classical bilevel optimization is framed via a value function that maps upper-level variables to the optimal lower-level value, rigorously established in Dempe\u2019s Foundations of Bilevel Programming; this hyper-objective perspective defines the object whose derivatives algorithm designers seek to compute or approximate. Danskin\u2019s theorem provides subgradient and differentiability results for value functions even when the argmin set is not single-valued, while Milgrom and Segal\u2019s envelope theorems extend these insights to irregular settings, offering directional-derivative characterizations under minimal assumptions. On the algorithmic side, Ghadimi and Wang initiated a systematic approximation viewpoint for nonconvex bilevel problems, showing how one can target stationary solutions via surrogate analyses. The two-timescale stochastic approximation scheme of Hong and coauthors established a practical SA framework for bilevel learning but typically under strong convexity or single-valuedness of the lower level. In contrast, the implicit-differentiation line\u2014exemplified by Lorraine et al.\u2014popularized scalable hypergradient computation, yet hinges on uniqueness and smooth sensitivity of the lower-level solution map.\nBringing these strands together, the current paper capitalizes on envelope-theorem insights to study a penalty that charges lower-level suboptimality, proving O(\u03c3)-tight alignment between the penalty objective and the hyper-objective in both value and derivatives. This theory closes the gap left by implicit approaches in the multi-solution case by deriving an explicit hyper-gradient formula under minimal conditions, and it converts approximation ideas into single-level first-order stochastic methods that sidestep two-timescale mechanics while delivering non-asymptotic guarantees for fully nonconvex bilevel landscapes.",
  "target_paper": {
    "title": "On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation",
    "authors": "Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, Robert D Nowak",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Bilevel-Optimization, Penalty Methods, Landscape Analysis, Non-Asymptotic Analysis, First-Order Methods",
    "abstract": "In this work, we study first-order algorithms for solving Bilevel Optimization (BO) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of BO through the lens of penalty methods, in which the upper- and lower-level objectives are combined in a weighted sum with penalty parameter $\\sigma > 0$. In particular, we establish a strong connection between the penalty function and the hyper-objective by explicitly characterizing the conditions under which the values and derivatives of the two must be $O(\\sigma)$-close. A by-product of our analysis is the explicit formula for the gradient of hyper-objective when the lower-level problem has multiple solutions under minimal conditions, which could be of independent interest. Next, viewing the penalty formulation as $O(\\sigma)$-approximation of the original BO, we propose first-order algorithms that find an $\\epsilon$-stationar",
    "openreview_id": "CvYBvgEUK9",
    "forum_id": "CvYBvgEUK9"
  },
  "analysis_timestamp": "2026-01-06T19:56:50.784113"
}
{
  "prior_works": [
    {
      "title": "Classifier-Free Diffusion Guidance",
      "authors": "Jonathan Ho and Tim Salimans",
      "year": 2021,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "CCA treats the CFG-induced target (obtained by interpolating conditional and condition-dropped predictions) as the distribution to fit, explicitly learning the guided distribution so that no runtime guidance is needed."
    },
    {
      "title": "Parti: Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
      "authors": "Jiahui Yu et al.",
      "year": 2022,
      "arxiv_id": "2206.10789",
      "role": "Related Problem",
      "relationship_sentence": "Parti popularized the use of classifier-free guidance in autoregressive text-to-image transformers via conditional/unconditional logit mixing, whose quality\u2013cost tradeoff and modality coupling motivate CCA\u2019s training-time alignment alternative."
    },
    {
      "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
      "authors": "Huiwen Chang et al.",
      "year": 2023,
      "arxiv_id": "2301.00704",
      "role": "Foundation",
      "relationship_sentence": "Muse formalized CFG for masked autoregressive image-token transformers by mixing logits from condition-dropped and conditioned decoders, establishing the exact guided-sampling mechanism that CCA aims to bake into the model parameters."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Alexander M. Rafailov et al.",
      "year": 2023,
      "arxiv_id": "2305.18290",
      "role": "Inspiration",
      "relationship_sentence": "CCA adopts a DPO-style contrastive likelihood-ratio objective\u2014pushing up probability on \u2018preferred\u2019 (condition-consistent) continuations and down on \u2018dispreferred\u2019 (unconditioned/weakly conditioned) ones\u2014without a learned reward model or on-policy sampling."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "arxiv_id": "2203.02155",
      "role": "Foundation",
      "relationship_sentence": "CCA is framed by the alignment paradigm introduced in RLHF/Instruction-tuning\u2014post-hoc fine-tuning of a pretrained generator toward a target usage distribution\u2014here replacing human preference rewards with a guidance-defined target distribution."
    },
    {
      "title": "Consistency Models",
      "authors": "Yang Song et al.",
      "year": 2023,
      "arxiv_id": "2303.01469",
      "role": "Inspiration",
      "relationship_sentence": "Consistency distillation showed that the behavior of guided, multi-step samplers can be distilled into a fast model, motivating CCA\u2019s idea of baking guidance effects into a single autoregressive model to remove guided decoding overhead."
    }
  ],
  "synthesis_narrative": "Classifier-free guidance (CFG) established that mixing predictions from a conditional model with those from a condition-dropped model can sharpen conditional generation; its core mechanism is an interpolation in logit or score space between these two branches. In large autoregressive text-to-image systems, Parti scaled this practice, demonstrating that logit mixing during decoding improves image fidelity and semantic adherence but doubles passes and entangles how text and visual tokens are handled at sampling time. Muse further codified CFG for masked autoregressive transformers, explicitly training with condition dropout and then combining conditional/unconditional logits at inference to boost quality, thereby standardizing the precise guided decoding recipe used in AR visual generation. In parallel, language model alignment advanced a training-time alternative to inference-time heuristics: RLHF (as in InstructGPT) fine-tunes pretrained models toward a target usage distribution defined by preferences, while Direct Preference Optimization (DPO) introduced a simple, contrastive likelihood-ratio objective that shifts probability mass from dispreferred to preferred responses without on-policy RL. In diffusion, Consistency Models showed that guided, iterative sampling behavior can be distilled into a single, efficient network that reproduces high-quality samples without guidance. Together, these works expose a gap: CFG reliably boosts AR visual quality but imposes modality-uneven, compute-heavy decoding, whereas alignment methods offer distribution matching via post-hoc fine-tuning. The natural next step is to define the \u201cpreferred\u201d target as the CFG-induced conditional distribution itself and use a contrastive alignment loss to train the autoregressive model to emulate it, retaining CFG\u2019s quality gains while eliminating guided sampling and its text\u2013vision inconsistencies.",
  "target_paper": {
    "title": "Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment",
    "authors": "Huayu Chen, Hang Su, Peize Sun, Jun Zhu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "autoregressive, generative models, image generation, multimodal, alignment, RLHF, classifier-free guidance",
    "abstract": "Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose Condition Contrastive Alignment (CCA) to facilitate guidance-free AR visual generation. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning (1% of pretraining epochs) on the pretraining dataset. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training",
    "openreview_id": "kGvXIlIVLM",
    "forum_id": "kGvXIlIVLM"
  },
  "analysis_timestamp": "2026-01-06T11:52:08.970303"
}
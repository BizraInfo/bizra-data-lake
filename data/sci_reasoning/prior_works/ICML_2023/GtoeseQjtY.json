{
  "prior_works": [
    {
      "title": "Reinforcement Learning: An Introduction",
      "authors": "Richard S. Sutton et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "This book formally states the reward hypothesis that the paper seeks to \u2018settle,\u2019 providing the core claim (goals as maximization of expected cumulative scalar reward) whose exact requirements the present work precisely specifies."
    },
    {
      "title": "Reward is Enough",
      "authors": "David Silver et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "This paper\u2019s broad advocacy for the reward hypothesis without a complete axiomatic account creates the explicit gap that the ICML 2023 work closes by giving necessary and sufficient conditions under which the hypothesis holds."
    },
    {
      "title": "Theory of Games and Economic Behavior",
      "authors": "John von Neumann et al.",
      "year": 1944,
      "role": "Foundation",
      "relationship_sentence": "The expected-utility representation theorem from VNM underpins the \u2018expected value\u2019 part of the hypothesis, and the ICML paper directly builds on this framework to represent preferences over stochastic histories."
    },
    {
      "title": "Stationary Ordinal Utility and Impatience",
      "authors": "Tjalling C. Koopmans et al.",
      "year": 1960,
      "role": "Foundation",
      "relationship_sentence": "Koopmans\u2019 axioms for additive, stationary utility over time-streams provide the mathematical basis for representing purposes as a cumulative sum of per-time scalar signals, which the paper leverages and tailors to RL settings."
    },
    {
      "title": "Myopia and Inconsistency in Dynamic Utility Maximization",
      "authors": "Robert H. Strotz et al.",
      "year": 1955,
      "role": "Foundation",
      "relationship_sentence": "Strotz\u2019s characterization of time-consistent preferences (implying exponential discounting) informs the paper\u2019s conditions under which cumulative returns faithfully capture intertemporal goals without dynamic inconsistency."
    },
    {
      "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping",
      "authors": "Andrew Y. Ng et al.",
      "year": 1999,
      "role": "Extension",
      "relationship_sentence": "The result that potential-based transformations preserve optimal behavior is extended conceptually to the paper\u2019s characterization of equivalence classes of rewards that represent the same \u2018purpose.\u2019"
    },
    {
      "title": "Structured Solution Methods for Non-Markovian Decision Processes",
      "authors": "Fahiem Bacchus et al.",
      "year": 1996,
      "role": "Extension",
      "relationship_sentence": "By showing how history-dependent objectives can be compiled into Markovian form via state augmentation and scalar rewards, this work provides the key mechanism the paper invokes to argue that scalar cumulative rewards can express rich, non-Markovian purposes."
    }
  ],
  "synthesis_narrative": "The core innovation of \u201cSettling the Reward Hypothesis\u201d is an axiomatic account that pinpoints exactly when goals and purposes can be represented as maximizing expected cumulative scalar reward. The intellectual lineage begins with Sutton and Barto\u2019s articulation of the reward hypothesis, which the paper undertakes to formalize. Silver et al.\u2019s \u201cReward is Enough\u201d sharpened the stakes but left open what assumptions make the claim valid, motivating a rigorous settlement. The decision-theoretic backbone comes from von Neumann\u2013Morgenstern\u2019s expected-utility representation, which justifies the \u2018expected value\u2019 component for preferences over uncertainty. To capture temporal structure, Koopmans provides axioms yielding additive, stationary utility over sequences, directly supporting representation as a cumulative sum of per-period scalars. Strotz\u2019s analysis of dynamic inconsistency further clarifies when intertemporal preferences align with a stable, cumulative-return objective (implying exponential discounting under time consistency), a condition the paper makes explicit among its requirements. Beyond representation, practical equivalence of rewards is addressed by extending Ng, Harada, and Russell\u2019s invariance under potential-based shaping to the broader notion of purpose equivalence. Finally, the expressivity of scalar rewards for history-dependent goals relies on the non-Markovian reward decision process literature (Bacchus, Boutilier, and Grove), which shows how to compile temporal objectives into augmented state with scalar rewards. Together, these works directly enable the paper\u2019s precise, necessary-and-sufficient conditions for when the reward hypothesis truly holds.",
  "analysis_timestamp": "2026-01-06T23:09:26.563493"
}
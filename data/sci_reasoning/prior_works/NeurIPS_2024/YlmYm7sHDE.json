{
  "prior_works": [
    {
      "title": "The Information Bottleneck Method",
      "authors": "Naftali Tishby, Fernando C. Pereira, William Bialek",
      "year": 1999,
      "role": "Foundational framework for bottleneck-based lossy representations",
      "relationship_sentence": "MEC-B introduces a bottleneck that trades off informativeness and randomness in the coupling; this mirrors the IB principle of controlling encoder complexity by constraining information flow, and motivates the encoder-side EBIM objective as an information-maximization under a complexity bound."
    },
    {
      "title": "The Deterministic Information Bottleneck",
      "authors": "Dylan J. Strouse, David J. Schwab",
      "year": 2017,
      "role": "Characterization of stochastic-to-deterministic transitions in bottleneck encoders",
      "relationship_sentence": "The paper\u2019s analysis \"near functional mappings\" and its emphasis on controlling the degree of stochasticity in the encoder directly echo DIB\u2019s study of when optimal IB encoders become (nearly) deterministic, informing MEC-B\u2019s characterization of solutions near functional regimes."
    },
    {
      "title": "The Agglomerative Information Bottleneck",
      "authors": "Noam Slonim, Naftali Tishby",
      "year": 2000,
      "role": "Greedy algorithmic template with monotonic guarantees for bottleneck objectives",
      "relationship_sentence": "MEC-B\u2019s greedy algorithm for EBIM with performance guarantees is conceptually aligned with AIB\u2019s greedy merging procedure that monotonically improves the IB objective, providing a methodological blueprint for provable greedy optimization under bottleneck constraints."
    },
    {
      "title": "Deterministic Annealing for Clustering, Compression, Classification, Regression, and Related Optimization Problems",
      "authors": "Kenneth Rose",
      "year": 1998,
      "role": "Entropy/complexity-constrained optimization to control encoder stochasticity",
      "relationship_sentence": "EBIM\u2019s entropy-bounded information maximization parallels deterministic annealing\u2019s use of entropy constraints to smoothly control randomness in assignments, guiding algorithmic design for navigating from stochastic to near-deterministic encoders with guarantees."
    },
    {
      "title": "Multiterminal Source Coding Under Logarithmic Loss",
      "authors": "Thomas A. Courtade, Tsachy Weissman",
      "year": 2014,
      "role": "Foundational results for log-loss distortion and posterior reconstructions",
      "relationship_sentence": "MEC-B relies on log-loss where optimal reconstructions are posteriors and expected distortion equals conditional entropy; these identities from the log-loss literature justify the decomposition into an encoder information objective (EBIM) and a decoder coupling (MEC)."
    },
    {
      "title": "Bounds for Distributions with Given Marginals",
      "authors": "Ludger R\u00fcschendorf",
      "year": 1981,
      "role": "Extremal couplings with fixed marginals (Fr\u00e9chet\u2013Hoeffding bounds/comonotone coupling)",
      "relationship_sentence": "The MEC subproblem seeks the most dependent coupling (equivalently, minimum joint entropy) for fixed marginals; classical extremal coupling theory provides the structural underpinning and greedy, monotone-mass matching intuition used in practical MEC solvers."
    },
    {
      "title": "Common Information is Far Less Than Mutual Information",
      "authors": "Peter G\u00e1cs, J\u00e1nos K\u00f6rner",
      "year": 1973,
      "role": "Deterministic common part and functional mappings between variables",
      "relationship_sentence": "MEC-B\u2019s characterization \"near functional mappings\" connects to G\u00e1cs\u2013K\u00f6rner common information, which formalizes when a deterministic encoder (functional mapping) captures dependence; this informs the analysis of MEC-B in the limit of vanishing encoder stochasticity."
    }
  ],
  "synthesis_narrative": "Minimum Entropy Coupling with Bottleneck (MEC-B) fuses two threads: extremal couplings with fixed marginals and bottleneck-based representation learning. On the coupling side, the decoder subproblem is a classical Minimum Entropy Coupling: among all joint distributions with given marginals, select the one that maximizes dependence (equivalently, minimizes joint entropy). The structural intuition for this problem traces to R\u00fcschendorf\u2019s extremal couplings and Fr\u00e9chet\u2013Hoeffding bounds, which explain why comonotone, monotone-mass matchings are optimal and motivate efficient greedy constructions. On the representation side, MEC-B\u2019s encoder is governed by an explicit bottleneck that constrains stochasticity. This is directly inspired by the Information Bottleneck framework, with the Deterministic Information Bottleneck clarifying when optimal encoders become nearly functional\u2014precisely the regime MEC-B analyzes. Algorithmically, the proposed greedy EBIM procedure parallels the Agglomerative Information Bottleneck\u2019s monotonic improvement guarantees and is conceptually akin to deterministic annealing\u2019s entropy-constrained progression from stochastic to deterministic assignments. The choice of logarithmic loss ties the two halves together: as established in the log-loss literature, optimal reconstructions are posterior distributions and the expected distortion equals conditional entropy, legitimizing MEC-B\u2019s decomposition into an encoder-side information maximization under an entropy budget and a decoder-side MEC. Finally, the paper\u2019s characterization near functional mappings resonates with G\u00e1cs\u2013K\u00f6rner\u2019s common information, which delineates when a deterministic common part suffices\u2014providing a principled boundary case for MEC-B\u2019s controlled stochasticity.",
  "analysis_timestamp": "2026-01-06T23:33:35.568973"
}
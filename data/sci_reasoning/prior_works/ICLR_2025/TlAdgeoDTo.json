{
  "prior_works": [
    {
      "title": "Fairness Through Awareness",
      "authors": "Cynthia Dwork et al.",
      "year": 2012,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s definition of individual fairness\u2014similar individuals should be treated similarly\u2014directly grounds the notion of \u201cfirst-person fairness,\u201d which our work operationalizes via demographic counterfactuals for chatbot users."
    },
    {
      "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
      "authors": "Nitish Nangia et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "We generalize CrowS-Pairs\u2019 minimal-pair attribute-swapping protocol to open-ended chat by counterfactually swapping user demographics in otherwise matched prompts to quantify first-person disparities at scale."
    },
    {
      "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
      "authors": "Moin Nadeem et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "StereoSet\u2019s notion of stereotype-consistent versus anti-stereotype continuations informs our quantitative measure of harmful stereotypes in generated chatbot responses."
    },
    {
      "title": "BBQ: A Hand-Built Benchmark for Measuring Social Biases in Question Answering",
      "authors": "Alicia Parrish et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "We adapt BBQ\u2019s bias-direction labeling (stereotype-consistent vs. -inconsistent) and controlled comparisons to operationalize bias judgments when demographic cues are present versus counterfactually removed."
    },
    {
      "title": "Judging LLM-as-a-Judge: MT-Bench and Chatbot Arena",
      "authors": "LMSYS (Lianmin Zheng et al.)",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Our Language Model as a Research Assistant (LMRA) builds on the LLM-as-a-judge paradigm by using a strong model to rate fairness-relevant properties (e.g., stereotyping and differential treatment) with scalable reliability."
    },
    {
      "title": "Red Teaming Language Models with Language Models",
      "authors": "Deep Ganguli et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "We extend the idea of using models to probe and analyze other models to the fairness domain, leveraging an LM to systematically surface, summarize, and quantify demographic harms across diverse chatbot tasks."
    },
    {
      "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
      "authors": "Samuel Gehman et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By showing prompt-based generative harm evaluations focused mainly on toxicity, this work highlights the gap our approach fills\u2014moving to first-person, demographic counterfactual fairness across diverse real-world chatbot tasks."
    }
  ],
  "synthesis_narrative": "Individual fairness established the principle that similar individuals should be treated similarly, offering a normative foundation for assessing harms that accrue to specific people rather than only to abstract groups. CrowS-Pairs operationalized this idea for language models using minimal pairs that swap demographic attributes while holding content constant, enabling a clean counterfactual comparison. StereoSet further refined measurement by distinguishing stereotype-consistent from anti-stereotype continuations, providing a direction-sensitive signal for assessing harmful associations. BBQ introduced carefully controlled comparisons and explicit bias-direction labeling in a QA setting, helping evaluators separate genuine task competence from stereotype-driven responses. In parallel, LLM-as-a-judge work demonstrated that strong language models can reliably evaluate other models\u2019 outputs at scale, and red-teaming with LMs showed that models can systematically probe and summarize safety-relevant failures. RealToxicityPrompts revealed the feasibility\u2014but also the limitations\u2014of prompt-based harm evaluations largely confined to toxicity and single-turn setups. Together, these works reveal both a methodological toolkit and a gap: counterfactual, direction-aware bias measurement exists but is largely benchmark-bound, and scalable evaluation is possible but rarely targets first-person harms in realistic chat tasks. The present work synthesizes minimal-pair counterfactuals with stereotype-direction metrics, and instantiates them in open-ended chat via an LLM-based research assistant that can rate, summarize, and analyze demographic differences across millions of interactions, thereby delivering a practical framework for first-person fairness in chatbots.",
  "target_paper": {
    "title": "First-Person Fairness in Chatbots",
    "authors": "Tyna Eloundou, Alex Beutel, David G. Robinson, Keren Gu, Anna-Luisa Brakman, Pamela Mishkin, Meghan Shah, Johannes Heidecke, Lilian Weng, Adam Tauman Kalai",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "fairness, large language models, chatbots",
    "abstract": "Evaluating chatbot fairness is crucial given their rapid proliferation, yet typical chatbot tasks (e.g., resume writing, entertainment) diverge from the institutional decision-making tasks (e.g., resume screening) which have traditionally been central to discussion of algorithmic fairness. The open-ended nature and diverse use-cases of chatbots necessitate novel methods for bias assessment. This paper addresses these challenges by introducing a scalable counterfactual approach to evaluate \"first-person fairness,\" meaning fairness toward chatbot users based on demographic characteristics. Our method employs a Language Model as a Research Assistant (LMRA) to yield quantitative measures of harmful stereotypes and qualitative analyses of demographic differences in chatbot responses. We apply this approach to assess biases in six of our language models across millions of interactions, covering sixty-six tasks in nine domains and spanning two genders and four races. Independent human annotat",
    "openreview_id": "TlAdgeoDTo",
    "forum_id": "TlAdgeoDTo"
  },
  "analysis_timestamp": "2026-01-06T13:37:02.777491"
}
{
  "prior_works": [
    {
      "title": "Learning Physical Intuition of Block Towers by Example",
      "authors": "J. Lerer, S. Gross, R. Fergus",
      "year": 2016,
      "role": "dataset/task + method for visual physics prediction",
      "relationship_sentence": "Established a canonical image-based stability judgment task that crystallized object-property and dynamics evaluation, directly informing PhysBench\u2019s object stability/physics-based dynamics categories."
    },
    {
      "title": "Interaction Networks for Learning about Objects, Relations and Physics",
      "authors": "P. W. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, K. Kavukcuoglu",
      "year": 2016,
      "role": "method (object-centric relational physics modeling)",
      "relationship_sentence": "Provided the object-centric, relation-based lens for physical reasoning that underpins PhysBench\u2019s capability dimensions around object properties, inter-object relations, and dynamics."
    },
    {
      "title": "CATER: A Diagnostic Dataset for Compositional Actions and TEmporal Reasoning",
      "authors": "R. Girdhar, J. Carreira, C. Doersch, A. Zisserman",
      "year": 2019,
      "role": "dataset/benchmark (video temporal reasoning)",
      "relationship_sentence": "Introduced controlled video diagnostics for spatiotemporal tracking and occlusion, motivating PhysBench\u2019s inclusion of temporal, occlusion, and compositional action reasoning in video."
    },
    {
      "title": "Something-Something V2: A Dataset for Recognizing Human-Object Interactions",
      "authors": "R. Goyal et al.",
      "year": 2018,
      "role": "dataset (real-world video, human\u2013object interactions)",
      "relationship_sentence": "Brought diverse, fine-grained human\u2013object interactions that emphasize affordances and object properties, informing PhysBench\u2019s coverage of physical object properties and relationships in realistic videos."
    },
    {
      "title": "CLEVRER: Collision Events for Video Representation and Reasoning",
      "authors": "K. Yi, C. Gan, Y. Li, A. Torralba, J. B. Tenenbaum, J. Wu",
      "year": 2020,
      "role": "dataset/benchmark (video QA for intuitive physics and causality)",
      "relationship_sentence": "Pioneered causal, temporal, and counterfactual video QA about collisions, directly inspiring PhysBench\u2019s interleaved video\u2013text question design and its focus on causal/dynamic physical reasoning."
    },
    {
      "title": "PHYSION: Evaluating Physical Prediction from Vision in Humans and Machines",
      "authors": "D. M. Bear et al.",
      "year": 2021,
      "role": "dataset/benchmark (intuitive physics prediction)",
      "relationship_sentence": "Established broad physics scenarios and human\u2013machine comparison protocols, which PhysBench extends to VLMs with richer modality interleaving and a wider capability taxonomy."
    }
  ],
  "synthesis_narrative": "PhysBench\u2019s core contribution\u2014a comprehensive, multimodal benchmark that diagnoses vision-language models\u2019 understanding of the physical world\u2014builds directly on three strands of prior work: classic image-based physics judgments, object-centric relational reasoning, and diagnostic video benchmarks for intuitive physics and temporal causality. Lerer et al. (2016) crystallized image-only physics prediction via block-tower stability, highlighting that visual cues must be grounded in dynamics\u2014an archetype PhysBench generalizes into broader object-property and stability assessments. Battaglia et al.\u2019s Interaction Networks (2016) provided the object-centric relational perspective that frames physical understanding as interactions among entities, which maps naturally onto PhysBench\u2019s capability dimensions spanning object properties, inter-object relations, scene constraints, and dynamics.\nCLEVRER (2020) and CATER (2019) introduced controlled video diagnostics for causal, temporal, and counterfactual reasoning about physical events, shaping PhysBench\u2019s emphasis on temporally grounded reasoning and interleaved video\u2013text questions. Something-Something V2 (2018) added realism, diversity, and human\u2013object affordances, motivating PhysBench\u2019s inclusion of real-world interactions beyond synthetic setups. Finally, PHYSION (2021) broadened intuitive physics evaluation and human\u2013machine comparisons; PhysBench extends this line to modern VLMs, unifying image and video modalities with text, expanding the scenario space (19 subclasses across four domains), and standardizing large-scale evaluation across many models. Collectively, these works supplied the task archetypes, modeling perspective, and evaluation protocols that PhysBench integrates into a single, physics-centric benchmark tailored to stress-test and improve VLMs\u2019 physical world understanding.",
  "analysis_timestamp": "2026-01-06T23:42:48.102713"
}
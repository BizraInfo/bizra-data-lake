{
  "prior_works": [
    {
      "title": "AI Safety Gridworlds",
      "authors": "Victoria Krakovna et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Established canonical safety problem formulations (e.g., specification gaming, side effects) and the benchmark-as-evaluation paradigm that Machiavelli generalizes from toy gridworlds to rich, text-based social decision settings to measure reward\u2013ethics trade-offs."
    },
    {
      "title": "Optimal policies tend to seek power",
      "authors": "Alex Turner et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Provided the theoretical basis that reward-maximizing agents instrumentally seek power; Machiavelli directly operationalizes this by defining and empirically measuring power-seeking behaviors in CYOA trajectories."
    },
    {
      "title": "Benchmarking Safe Exploration in Deep Reinforcement Learning",
      "authors": "Alex Ray et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Introduced Safety Gym to quantify reward\u2013safety trade-offs in low-level control tasks; Machiavelli targets the explicit gap by creating a large-scale, socially grounded, language-centric benchmark enabling the same trade-off analysis for general-purpose models like GPT-4."
    },
    {
      "title": "Social Chemistry 101: Learning to Reason about Social Norms",
      "authors": "Maxwell Forbes et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Provided structured, commonsense social-norm knowledge and labeling methodology that Machiavelli echoes when it mathematizes ethical violations and norm violations across interactive scenarios."
    },
    {
      "title": "Delphi: Towards Machine Ethics and Norms",
      "authors": "Liwei Jiang et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated that language models can make consistent moral judgments using normative knowledge; Machiavelli builds on this by using LMs as annotators for large-scale ethical labeling and validating that LM judgments can outperform human annotators."
    },
    {
      "title": "Discovering Language Model Behaviors with Model-Written Evaluations",
      "authors": "Ethan Perez et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Showed that LMs can generate and grade behavioral evaluations at scale; Machiavelli extends this LM-as-judge paradigm to automatically annotate harmful behaviors across 500k+ interactive scenarios."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "Pioneered principle-guided LM self-critique for reducing harmfulness; Machiavelli applies related LM-based steering techniques in sequential decision contexts to improve the reward\u2013ethics trade-off without retraining."
    }
  ],
  "synthesis_narrative": "Machiavelli\u2019s core innovation\u2014quantitatively measuring how reward maximization trades off against ethical behavior in rich, social decision settings\u2014emerges from two converging lines of work. First, AI safety benchmarks such as AI Safety Gridworlds and Safety Gym formulated and measured safety-relevant failure modes and reward\u2013safety trade-offs, but only in simplified or low-level control domains. Machiavelli directly addresses this gap by scaling those problem formulations to natural language, socially grounded, interactive narratives where reward pursuit can plausibly incentivize power-seeking, deception, and harm. Turner et al.\u2019s theory that reward-maximizing policies tend to seek power provides the precise incentive hypothesis that Machiavelli operationalizes, turning abstract theory into empirical metrics of power-seeking within Choose-Your-Own-Adventure trajectories.\nSecond, advances in normative reasoning and LM-based evaluation made the benchmark feasible at scale. Social Chemistry 101 and Delphi established that social norms can be encoded and that language models can make consistent moral judgments, furnishing both the normative substrate and practical evidence that automated ethical labeling is viable. Perez et al. showed LMs can generate and grade behavioral tests, a direct precursor to Machiavelli\u2019s LM-driven annotations that even outperform human annotators. Finally, Bai et al.\u2019s Constitutional AI demonstrated principle-guided LM steering, which Machiavelli adapts to sequential settings to mitigate harmful behaviors while preserving reward. Together, these works directly enable Machiavelli\u2019s large-scale, LM-annotated, socially situated benchmark and its analysis of the reward\u2013ethics frontier.",
  "analysis_timestamp": "2026-01-06T23:09:26.528365"
}
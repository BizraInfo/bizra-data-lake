{
  "prior_works": [
    {
      "title": "Machine Translation from One Book",
      "authors": "Tanzer et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work introduced the one-grammar-book prompting setup for English\u2013Kalamang and claimed that grammatical descriptions enable translation in an unseen XLR language, providing the exact problem formulation and primary baseline that this paper dissects and re-evaluates."
    },
    {
      "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
      "authors": "Sewon Min et al.",
      "year": 2022,
      "arxiv_id": "2202.12837",
      "role": "Inspiration",
      "relationship_sentence": "Their finding that in-context performance often stems from the presence and properties of examples rather than task instructions directly motivates this paper\u2019s isolation of grammar-book parallel examples versus grammatical prose to locate the true source of translation gains."
    },
    {
      "title": "Is ChatGPT a Good Translator? A Preliminary Study",
      "authors": "Wenxiang Jiao et al.",
      "year": 2023,
      "arxiv_id": "2301.08745",
      "role": "Gap Identification",
      "relationship_sentence": "By showing that LLM translation quality depends heavily on data coverage and benefits notably from few-shot examples, this study highlights limitations in low-resource and unseen settings that this paper probes within the one-book scenario."
    },
    {
      "title": "Large Language Models are State-of-the-Art for Translation? Not Yet",
      "authors": "Tom Kocmi and Christian Federmann",
      "year": 2023,
      "arxiv_id": "2305.16438",
      "role": "Gap Identification",
      "relationship_sentence": "Their evidence that fine-tuned encoder\u2013decoder NMT remains highly competitive with LLM prompting directly motivates this paper\u2019s comparison showing that a simply fine-tuned MT model can match the one-book LLM setup."
    },
    {
      "title": "ODIN: A Database of Interlinear Glossed Text",
      "authors": "William D. Lewis and Fei Xia",
      "year": 2010,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "ODIN established interlinear glossed text as structured supervision for morphosyntax, informing this paper\u2019s gloss-prediction diagnostic to assess what linguistic knowledge grammar books actually inject into models."
    },
    {
      "title": "URIEL and lang2vec: Representing languages as typological, genealogical, and geographical vectors",
      "authors": "Patrick Littell et al.",
      "year": 2017,
      "arxiv_id": "1711.00173",
      "role": "Foundation",
      "relationship_sentence": "URIEL\u2019s typological representations provide the conceptual and practical basis for this paper\u2019s typology-driven analysis of which grammatical properties in a book are likely to help models."
    }
  ],
  "synthesis_narrative": "Machine Translation from One Book framed the striking claim that a single grammar book, fed to a long-context LLM, can unlock translation for an unseen, extremely low-resource language, establishing the concrete one-book prompting protocol and Kalamang evaluation. Min et al. showed that in-context learning\u2019s gains often arise from example demonstrations rather than the textual instructions themselves, suggesting a precise methodological lever to separate the influence of parallel examples from explanatory prose. Jiao et al. documented that LLM translation quality is sensitive to language coverage and benefits substantially from few-shot exemplars, especially in low-resource regimes, underscoring the importance of example-driven signal. Kocmi and Federmann argued that fine-tuned encoder\u2013decoder MT remains competitive with prompted LLMs, motivating a rigorous NMT baseline when only tiny parallel supervision is available. ODIN established interlinear glossed text as a structured source of morphosyntactic supervision, enabling diagnostics like gloss prediction to test what linguistic information is actually learned. URIEL\u2019s typological vectors offered a principled scaffold to relate specific grammatical properties to cross-lingual generalization. Together, these works reveal a tension: dramatic claims about grammar-book prompting, clear evidence that demonstrations drive in-context performance, and the competitiveness of small-data NMT. Building on this, the present study dissects the one-book setting to show that parallel examples\u2014not grammatical exposition\u2014drive gains, extends the analysis beyond Kalamang, demonstrates a simple fine-tuned encoder\u2013decoder can match performance, and uses grammaticality judgment and gloss prediction under a typological lens to pinpoint what kinds of grammatical information, if any, actually help.",
  "target_paper": {
    "title": "Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?",
    "authors": "Seth Aycock, David Stap, Di Wu, Christof Monz, Khalil Sima'an",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "llms, translation, low-resource, grammar, long-context, linguistics",
    "abstract": "Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests that prompting long-context LLMs with one grammar book enables English\u2013Kalamang translation, an XLR language unseen by LLMs\u2014a noteworthy case of linguistics helping an NLP task. We investigate the source of this translation ability, finding almost all improvements stem from the book\u2019s parallel examples rather than its grammatical explanations. We find similar results for Nepali and Guarani, seen low-resource languages, and we achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate where grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what kind of grammatical knowledge helps by introducing a typological ",
    "openreview_id": "aMBSY2ebPw",
    "forum_id": "aMBSY2ebPw"
  },
  "analysis_timestamp": "2026-01-06T10:04:31.993091"
}
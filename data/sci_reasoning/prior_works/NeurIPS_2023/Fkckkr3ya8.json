{
  "prior_works": [
    {
      "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence RNNs",
      "authors": "Brenden M. Lake, Marco Baroni",
      "year": 2018,
      "role": "Foundational benchmark on compositional generalization (SCAN)",
      "relationship_sentence": "Established that neural seq2seq models fail on systematic compositional splits, directly motivating this paper\u2019s controlled probes of transformers on tasks that require novel compositions of known primitives."
    },
    {
      "title": "Measuring compositional generalization: A comprehensive method on realistic data",
      "authors": "Daniel Keysers et al.",
      "year": 2020,
      "role": "Graph-based evaluation framework (CFQ/MCD splits)",
      "relationship_sentence": "Introduced graph-structured decompositions and compositional splits (MCD) to quantify compositional complexity, informing this paper\u2019s formulation of tasks as computation graphs and its metrics for reasoning complexity."
    },
    {
      "title": "Compositionality Decomposed: How do Neural Networks Generalize?",
      "authors": "Dieuwke Hupkes, Verna Dankers, Mathijs Mul, Elia Bruni",
      "year": 2020,
      "role": "Analytical framework for compositionality",
      "relationship_sentence": "Provided a taxonomy and methodology for breaking tasks into compositional sub-components, which underpins this paper\u2019s decomposition of reasoning into intermediate sub-procedures."
    },
    {
      "title": "Analysing mathematical reasoning abilities of neural models",
      "authors": "David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli",
      "year": 2019,
      "role": "Algorithmic reasoning benchmark (math/arithmetic)",
      "relationship_sentence": "Offered rigorous arithmetic and algorithmic tasks (including multi-digit operations) that contextualize this paper\u2019s multi-digit multiplication and dynamic programming evaluations of transformers."
    },
    {
      "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models",
      "authors": "Michael Hahn",
      "year": 2020,
      "role": "Theoretical limits of transformers",
      "relationship_sentence": "Proved inherent constraints of self-attention on certain hierarchical/algorithmic computations, grounding this paper\u2019s claim that failures reflect structural limitations rather than incidental errors."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "role": "Methodology for eliciting step-by-step reasoning",
      "relationship_sentence": "Popularized intermediate-step prompting used as a baseline; this paper tests whether CoT corresponds to faithful multi-step composition or collapses to linearized heuristics."
    },
    {
      "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
      "authors": "Denny Zhou, Jason Wei, Xuezhi Wang, Dale Schuurmans, Ed H. Chi, Quoc V. Le",
      "year": 2022,
      "role": "Decomposition prompting baseline",
      "relationship_sentence": "Proposed structured problem decomposition, which this paper critically examines by formalizing tasks as computation graphs and showing transformers often bypass true compositional decomposition."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014formalizing compositional tasks as computation graphs to quantify reasoning complexity and revealing that transformers often reduce multi-step reasoning to linearized subgraph matching\u2014builds on a converging line of empirical, methodological, and theoretical work. Lake and Baroni\u2019s SCAN laid the foundation by showing neural models struggle with systematic compositional generalization, establishing the central phenomenon this paper interrogates at scale in modern LLMs. Keysers et al.\u2019s CFQ introduced graph-based decompositions and compositional splits (MCD), directly inspiring the present work\u2019s computation-graph formalization and complexity metrics. Complementing this, Hupkes et al. provided a principled framework for decomposing compositionality into sub-operations, shaping the paper\u2019s breakdown of tasks into intermediate sub-procedures across multiplication, logic puzzles, and dynamic programming.\nOn the task side, Saxton et al.\u2019s arithmetic benchmarks contextualize the use of multi-digit multiplication and algorithmic reasoning to probe generalization beyond surface patterns. The paper\u2019s interpretation of failures as reflecting structural constraints is supported by theoretical results like Hahn\u2019s, which establish limits of self-attention on hierarchical/algorithmic computations. Finally, recent prompting methods\u2014Chain-of-Thought and Least-to-Most\u2014serve as baselines for eliciting intermediate reasoning; by contrasting these techniques with its computation-graph analysis, the paper demonstrates that even when models produce step-by-step outputs, their behavior often aligns with shortcut, linearized heuristics rather than faithful compositional execution. Together, these works scaffold the paper\u2019s framework and its central finding about the fate of compositionality in transformers.",
  "analysis_timestamp": "2026-01-07T00:02:04.794039"
}
{
  "prior_works": [
    {
      "title": "Invariant and Equivariant Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "arxiv_id": "1812.09902",
      "role": "Extension",
      "relationship_sentence": "GMNs directly instantiate Maron et al.\u2019s S_n-equivariant linear layers on graphs to make metanetwork computations equivariant to neuron/channel/head permutations induced by weight-space symmetries."
    },
    {
      "title": "Provably Powerful Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "arxiv_id": "1905.11136",
      "role": "Foundation",
      "relationship_sentence": "GMNs build on the higher-order tensor and polynomial-invariant machinery from PPGN to argue expressivity for graph representations of neural parameters under permutation groups."
    },
    {
      "title": "Deep Sets",
      "authors": "Manzil Zaheer et al.",
      "year": 2017,
      "arxiv_id": "1703.06114",
      "role": "Foundation",
      "relationship_sentence": "GMNs leverage Deep Sets\u2019 core principle that functions over exchangeable elements must be permutation-invariant/equivariant, applying it to sets of interchangeable neurons/filters within and across layers."
    },
    {
      "title": "Neural Architecture Search with Graph HyperNetworks",
      "authors": "Chris Ying et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "GMNs adopt the idea of encoding arbitrary neural architectures as computation graphs processed by a GNN, but invert the direction by using the trained weights as node/edge features to read models rather than generate them."
    },
    {
      "title": "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks",
      "authors": "N. Entezari et al.",
      "year": 2021,
      "arxiv_id": "2110.00683",
      "role": "Gap Identification",
      "relationship_sentence": "Their demonstration that many apparent model differences vanish after neuron permutations motivates GMNs\u2019 symmetry-aware design so that metanetwork outputs are well-defined across parameter reindexings."
    },
    {
      "title": "Git Re-Basin: Merging Models Modulo Permutation Symmetries",
      "authors": "J. Ainsworth et al.",
      "year": 2022,
      "arxiv_id": "2209.04836",
      "role": "Related Problem",
      "relationship_sentence": "By explicitly aligning neuron/channel permutations to compare or merge models, Git Re-Basin identifies the exact symmetry group GMNs encode equivariantly, allowing GMNs to avoid costly alignment by design."
    }
  ],
  "synthesis_narrative": "Permutation symmetries arise whenever neurons, channels, or attention heads can be reindexed without changing a network\u2019s function; Deep Sets formalized how functions on exchangeable elements must be permutation-invariant/equivariant, and Invariant and Equivariant Graph Networks supplied concrete S_n-equivariant linear layers to implement such symmetry-respecting computations on graphs. Provably Powerful Graph Networks extended this with higher-order tensor constructions and universality guarantees, showing how to obtain expressive invariant/equivariant maps under permutation actions. In parallel, Neural Architecture Search with Graph HyperNetworks showed that arbitrary neural architectures can be encoded as computation graphs and processed by a GNN, establishing a practical recipe for message passing over layers and connectivity. Empirically, The Role of Permutation Invariance in Linear Mode Connectivity demonstrated that many trained networks differ mainly by neuron permutations, making symmetry handling essential in weight space. Git Re-Basin operationalized this by explicitly aligning permutations to compare and merge models, clarifying the relevant symmetry group and its practical consequences.\nTogether, these works reveal both the necessity (Entezari; Git Re-Basin) and the tools (Deep Sets; Maron IEGN; PPGN) for symmetry-aware processing, and they provide a graph-based vehicle (Graph HyperNetworks) for handling diverse architectures. The natural next step is to treat trained networks as graphs whose nodes/edges carry parameter tensors and to process them with permutation-equivariant GNNs, yielding a metanetwork that is architecture-agnostic, symmetry-correct by construction, and expressive enough to reason across MLPs, CNNs, attention blocks, normalization layers, and residual connections without bespoke engineering.",
  "target_paper": {
    "title": "Graph Metanetworks for Processing Diverse Neural Architectures",
    "authors": "Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, James Lucas",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Metanetwork, graph, equivariance, expressivity",
    "abstract": "Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks --- neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and grou",
    "openreview_id": "ijK5hyxs0n",
    "forum_id": "ijK5hyxs0n"
  },
  "analysis_timestamp": "2026-01-06T10:31:34.209687"
}
{
  "prior_works": [
    {
      "title": "Property Inference Attacks on Fully Connected Neural Networks",
      "authors": "Gaurav Ganju, Qifeng Wang, Wei Yang, Charles A. Gunter, Nikita Borisov",
      "year": 2018,
      "role": "Foundational definition and methodology for dataset-level property inference on trained discriminative models.",
      "relationship_sentence": "PropInfer directly generalizes the dataset-level property inference threat model introduced by Ganju et al. from discriminative networks to fine-tuned LLMs, guiding the benchmark\u2019s property taxonomy and evaluation protocol."
    },
    {
      "title": "Exploiting Unintended Feature Leakage in Collaborative Learning",
      "authors": "Luca Melis, Congzheng Song, Emiliano De Cristofaro, Vitaly Shmatikov",
      "year": 2019,
      "role": "Demonstrated practical property inference (and other leakage) in federated/collaborative settings using shadow/auxiliary models and distributional signals.",
      "relationship_sentence": "The paper\u2019s shadow-model paradigm and use of distributional cues inspire PropInfer\u2019s shadow-model attack design, adapted to LLM fine-tuning and instantiated with word-frequency signals."
    },
    {
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "authors": "Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov",
      "year": 2017,
      "role": "Introduced the shadow-model technique and black-box auditing framework for inference attacks against trained models.",
      "relationship_sentence": "PropInfer\u2019s shadow-model attack borrows the core idea of training auxiliary models to learn attack decision rules, transplanting Shokri et al.\u2019s methodology from membership inference to dataset-level property inference for LLMs."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, Colin Raffel",
      "year": 2021,
      "role": "Established that LLMs can regurgitate training data via carefully crafted prompts; provided prompting and evaluation tactics for generative leakage.",
      "relationship_sentence": "PropInfer\u2019s prompt-based generation attack builds on the prompting/extraction playbook of Carlini et al., but targets aggregate dataset properties rather than verbatim records."
    },
    {
      "title": "The Secret Sharer: Measuring Unintended Memorization in Neural Networks",
      "authors": "Nicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej Kos, Dawn Song",
      "year": 2019,
      "role": "Introduced exposure-style metrics and canary-based audits to quantify memorization in generative models, especially language models.",
      "relationship_sentence": "PropInfer adapts the insight that generative behavior encodes distributional signals detectable via outputs, motivating its use of output statistics (e.g., word frequencies) as features for property inference."
    },
    {
      "title": "A Recipe for Auditing Large Language Models",
      "authors": "Matthew Jagielski, Nicholas Carlini, Daphne Ippolito, Katherine Lee, Florian Tram\u00e8r, Eric Wallace, Colin Raffel",
      "year": 2023,
      "role": "Systematized black-box audits of LLMs using output-space statistics and prompting strategies for data provenance tests.",
      "relationship_sentence": "PropInfer\u2019s evaluation design and black-box probing strategies echo the auditing principles in this work, extending them from membership/provenance auditing to property inference tasks."
    },
    {
      "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on LLaMA Using Medical Domain Knowledge",
      "authors": "ChatDoctor Team (multiple authors)",
      "year": 2023,
      "role": "Provided the medical-domain conversational dataset/model that PropInfer uses as the basis for constructing LLM property-inference benchmarks.",
      "relationship_sentence": "PropInfer builds its benchmark and property configurations on the ChatDoctor data and task setup, enabling realistic QA and chat-completion fine-tuning scenarios for privacy evaluation."
    }
  ],
  "synthesis_narrative": "PropInfer\u2019s core contribution\u2014benchmarking and attacking dataset-level property leakage from fine-tuned LLMs\u2014roots itself in the property inference literature and the recent auditing toolkit for generative LMs. The basic threat model and learning-to-infer paradigm trace to Ganju et al., who formalized property inference for discriminative networks, and to Melis et al., who showed practical leakage in collaborative learning via auxiliary/shadow models and distributional cues. Shokri et al.\u2019s seminal shadow-model methodology underpins PropInfer\u2019s second attack: training auxiliary models to map observable outputs to hidden training-set attributes, here realized with word-frequency features tailored to text generation.\n\nOn the generative side, Carlini et al. (2019; 2021) demonstrated that LMs memorize and can be prompted to reveal training data, introducing exposure-style measurement and concrete prompt-based extraction tactics. PropInfer\u2019s prompt-based generation attack transposes these insights from individual-record leakage to aggregate property inference, probing whether fine-tuning imprints detectable distributional signatures into LLM outputs. Jagielski et al.\u2019s auditing recipe further informs PropInfer\u2019s black-box evaluation and probing strategies, emphasizing output statistics as a robust signal for privacy audits.\n\nFinally, to ground the study in a realistic, high-stakes domain, PropInfer leverages the ChatDoctor medical dataset and conversational setup, enabling QA and chat-completion fine-tuning regimes where confidential cohort-level attributes (e.g., demographics, prevalence) are salient. Together, these prior works directly shape PropInfer\u2019s benchmark design and its two tailored attacks, bridging classic property inference with modern LLM auditing to expose dataset-level privacy risks in fine-tuned language models.",
  "analysis_timestamp": "2026-01-07T00:05:12.513831"
}
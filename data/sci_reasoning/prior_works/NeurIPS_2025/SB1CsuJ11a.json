{
  "prior_works": [
    {
      "title": "The Mechanics of n-Player Differentiable Games",
      "authors": "David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, Thore Graepel",
      "year": 2018,
      "role": "Conceptual and mathematical foundation for decomposing learning dynamics",
      "relationship_sentence": "Introduced the symmetric (potential/gradient) vs antisymmetric (rotational/curl) decomposition of learning vector fields, directly motivating Curl Descent\u2019s framing of non-gradient components arising from sign-diverse plasticity."
    },
    {
      "title": "Which Training Methods for GANs do actually Converge?",
      "authors": "Lars Mescheder, Andreas Geiger, Sebastian Nowozin",
      "year": 2018,
      "role": "Stability analysis of non-conservative learning dynamics",
      "relationship_sentence": "Showed how rotational (curl-like) forces impede convergence in adversarial training and analyzed Jacobian structure, informing Curl Descent\u2019s investigation of when curl terms can still yield effective optimization."
    },
    {
      "title": "Random feedback weights support learning in deep neural networks",
      "authors": "Tim P. Lillicrap, Daniel Cownden, Douglas B. Tweed, Colin J. Akerman",
      "year": 2016,
      "role": "Evidence that non-backprop, non-symmetric signals can still learn",
      "relationship_sentence": "Demonstrated that learning can proceed without exact gradients via feedback alignment, providing precedent for Curl Descent\u2019s core claim that non-gradient updates (here, curl-inducing) can optimize a loss."
    },
    {
      "title": "Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks",
      "authors": "Thomas P. Vogels, Henning Sprekeler, Friedemann Zenke, Christian Clopath, Wulfram Gerstner",
      "year": 2011,
      "role": "Biological grounding for anti-Hebbian plasticity",
      "relationship_sentence": "Provided empirical and modeling evidence for anti-Hebbian inhibitory plasticity, a key biological mechanism underpinning the sign-diverse learning rules that naturally generate curl terms in Curl Descent."
    },
    {
      "title": "A simplified neuron model as a principal component analyzer",
      "authors": "Erkki Oja",
      "year": 1982,
      "role": "Classical local Hebbian/anti-Hebbian learning theory",
      "relationship_sentence": "Established foundational local plasticity rules (Hebbian with normalization) that relate to decorrelation and component extraction, which Curl Descent generalizes by mixing Hebbian/anti-Hebbian signs to induce non-gradient dynamics."
    },
    {
      "title": "Learning by on-line gradient descent in non-linear feedforward networks",
      "authors": "Michael Biehl, Helge Schwarze",
      "year": 1995,
      "role": "Student\u2013teacher analytical framework for generalization dynamics",
      "relationship_sentence": "Developed the teacher\u2013student methodology and order-parameter ODEs that Curl Descent adapts to analytically track performance under deliberately introduced curl components."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli",
      "year": 2014,
      "role": "Dynamical-systems analysis of learning trajectories",
      "relationship_sentence": "Provided tractable ODE analyses for gradient-based learning in linear networks, a template that Curl Descent extends beyond gradient flows to include and study curl-like, non-conservative components."
    }
  ],
  "synthesis_narrative": "Curl Descent sits at the intersection of three threads: (i) mathematical decompositions of learning dynamics into conservative and rotational parts, (ii) biological observations of sign-diverse plasticity, and (iii) analytically tractable teacher\u2013student learning theory. From differentiable games, Balduzzi et al. formalized how the antisymmetric (curl) component of the Jacobian governs rotational dynamics, while Mescheder et al. connected such rotations to convergence challenges in GANs. These works directly inform Curl Descent\u2019s central move: explicitly modeling and analyzing the non-potential (curl-like) parts of the learning field, rather than treating them as noise or error.\n\nOn the biological side, Oja\u2019s rule established the power of local Hebbian/anti-Hebbian updates, and Vogels et al. provided strong evidence for anti-Hebbian inhibitory plasticity and E/I balance\u2014precisely the sign diversity that Curl Descent shows will generically induce curl. Lillicrap et al.\u2019s feedback alignment demonstrated that effective learning can occur without exact gradients, lending support to the idea that non-gradient mechanisms can still reduce supervised losses.\n\nMethodologically, classical student\u2013teacher analyses (Biehl & Schwarze) and dynamical solutions for deep linear networks (Saxe et al.) supply the tools Curl Descent uses to derive low-dimensional order-parameter dynamics and isolate the effect of curl terms. The paper\u2019s novelty is to synthesize these lines: it imports the rotational-vs-gradient decomposition into biologically motivated plasticity with Dale-constrained E/I structure, and then proves within a teacher\u2013student framework that small, structured curl components can coexist with effective loss optimization.",
  "analysis_timestamp": "2026-01-07T00:02:04.939033"
}
{
  "prior_works": [
    {
      "title": "MQuAKE: Benchmarking Multi\u2011Hop Knowledge Editing in Language Models",
      "authors": "Shaochen Zhong et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work defined the multi\u2011hop knowledge\u2011editing problem via compositional QA and introduced the original MQuAKE benchmark that MQuAKE\u2011Remastered audits, corrects, and standardizes to enable reliable evaluation."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "ROME is a principal editing baseline whose single\u2011fact intervention assumptions and CounterFact evaluation protocol are stress\u2011tested under MQuAKE\u2011Remastered\u2019s corrected multi\u2011hop settings."
    },
    {
      "title": "Mass\u2011Editing Memory in a Transformer",
      "authors": "Kevin Meng et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "MEMIT\u2019s multi\u2011fact parametric editing is a key baseline whose propagation and side\u2011effect behavior MQuAKE\u2011Remastered reevaluates with higher\u2011fidelity multi\u2011hop tests."
    },
    {
      "title": "Fast Model Editing at Scale",
      "authors": "Eric Mitchell et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "MEND provides a gradient\u2011based editing mechanism widely used in prior evaluations that MQuAKE\u2011Remastered systematically reassesses to expose failures masked by noisy multi\u2011hop judging."
    },
    {
      "title": "Editing Models with Search, Replace, and Cache",
      "authors": "Eric Mitchell et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "SERAC\u2019s non\u2011parametric edit\u2011at\u2011inference strategy offers a contrasting approach whose performance on compositional queries motivates the need for rigorous, leakage\u2011resistant multi\u2011hop evaluation."
    },
    {
      "title": "CounterFact: A Benchmark for Locating and Editing Factual Knowledge in Language Models",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "CounterFact established the single\u2011hop editing/evaluation template and locality metrics that MQuAKE\u2011Remastered generalizes beyond and refines to the multi\u2011hop setting with stricter validation."
    }
  ],
  "synthesis_narrative": "Research on knowledge editing in language models was crystallized by CounterFact, which paired single\u2011fact interventions with locality and generalization checks to quantify whether an edit took and what collateral effects it caused. ROME operationalized direct, localized edits by identifying and modifying internal factual associations, while MEMIT scaled these interventions to many facts at once, exposing trade\u2011offs between breadth and unintended side effects. In parallel, MEND proposed fast, gradient\u2011based parameter updates that made practical large\u2011scale evaluation feasible, and SERAC took a non\u2011parametric path\u2014searching, replacing, and caching responses at inference\u2014to avoid risky parameter changes while still targeting specific facts. Building on these foundations, MQuAKE introduced a compositional, multi\u2011hop formulation, turning single factual changes into chains of dependent queries that require correct propagation and robust locality under composition, thereby surfacing weaknesses that single\u2011hop setups often miss. Together, these works established core editing mechanisms, evaluation templates, and a first multi\u2011hop stress test, but also revealed fragile judging and leakage in multi\u2011hop benchmarks. The convergence of scalable parametric editors (ROME, MEMIT, MEND), contrasting non\u2011parametric strategies (SERAC), and MQuAKE\u2019s compositional design exposed a gap: evaluations were too noisy to diagnose propagation, side effects, and spurious success reliably. MQuAKE\u2011Remastered naturally follows by auditing and correcting multi\u2011hop data, tightening answer validation, and standardizing metrics so that these editing paradigms can be fairly compared and genuinely advanced in multi\u2011hop settings.",
  "target_paper": {
    "title": "MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations",
    "authors": "Shaochen Zhong, Yifan Lu, Lize Shao, Bhargav Bhushanam, Xiaocong Du, Yixin Wan, Yucheng Shi, Daochen Zha, Yiwei Wang, Ninghao Liu, Kaixiong Zhou, Shuai Xu, Kai-Wei Chang, Louis Feng, Vipin Chaudhary, Xia Hu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "knowledge edit, model edit, multi-hop, question answering, natural language processing, dataset audit",
    "abstract": "Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, *knowledge editing* often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rest, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., *``what club does Lionel Messi currently play for?''*).\n\nHowever, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (to entertain an extreme example: [*\"What car did the wife of the owner of the club that Messi currently plays for used to get to school in the 80s?\"*](youtube.com/watch?v=DbwiHC1Fu-E\\&t=132s)). Prior arts have coined this task as *multi-hop knowledge ",
    "openreview_id": "m9wG6ai2Xk",
    "forum_id": "m9wG6ai2Xk"
  },
  "analysis_timestamp": "2026-01-06T06:26:30.624290"
}
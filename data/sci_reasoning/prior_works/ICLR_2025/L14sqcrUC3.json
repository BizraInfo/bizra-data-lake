{
  "prior_works": [
    {
      "title": "Revisiting Deep Learning Models for Tabular Data",
      "authors": "Yury Gorishniy et al.",
      "year": 2021,
      "arxiv_id": "2106.11959",
      "role": "Gap Identification",
      "relationship_sentence": "This widely adopted benchmark relied on random splits and largely non-temporal, raw-feature datasets, directly motivating TabReD to fix these evaluation blind spots with time-based splits and more industrially realistic data curation."
    },
    {
      "title": "Why do tree-based models still outperform deep learning on tabular data?",
      "authors": "L\u00e9onard Grinsztajn et al.",
      "year": 2022,
      "arxiv_id": "2207.08815",
      "role": "Gap Identification",
      "relationship_sentence": "By concluding that tree models outperform deep nets on standard random-split academic datasets, this work exposed the very benchmark biases (absence of temporal drift and engineered-feature pipelines) that TabReD explicitly interrogates and rectifies."
    },
    {
      "title": "Tabular Data: Deep Learning is Not All You Need",
      "authors": "Roy Shwartz-Ziv et al.",
      "year": 2022,
      "arxiv_id": "2106.03253",
      "role": "Gap Identification",
      "relationship_sentence": "Its evidence against tabular deep learning rests on random cross-validation over common academic suites, highlighting the evaluation setting whose mismatch to deployment (temporal drift, engineered features) TabReD systematically addresses."
    },
    {
      "title": "WILDS: A benchmark of in-the-wild distribution shifts",
      "authors": "Pang Wei Koh et al.",
      "year": 2021,
      "arxiv_id": "2012.07421",
      "role": "Inspiration",
      "relationship_sentence": "WILDS introduced the principle of deployment-realistic, shift-aware evaluation via semantically meaningful splits, which TabReD adapts to tabular domains by enforcing time-based splits to reflect distribution drift."
    },
    {
      "title": "OpenML Benchmarking Suites",
      "authors": "Bernd Bischl et al.",
      "year": 2017,
      "arxiv_id": "1708.03731",
      "role": "Foundation",
      "relationship_sentence": "This suite standardized the tabular benchmarks used by many studies but typically lacks timestamp metadata and features stemming from industrial pipelines, forming the foundational setup that TabReD revises."
    },
    {
      "title": "Leakage in Data Mining: Formulation, Detection, and Avoidance",
      "authors": "S. Kaufman et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "By formalizing how random splits induce leakage under temporal dependencies and engineered features, this work provides the methodological basis for TabReD's insistence on time-aware, leakage-resistant evaluation protocols."
    }
  ],
  "synthesis_narrative": "A line of influential tabular learning studies consolidated evaluation on standardized academic datasets with random splits. Revisiting Deep Learning Models for Tabular Data established a comprehensive benchmark and popular baselines under random splitting, focusing on non-temporal tasks with largely raw features. Two follow-ups, Why do tree-based models still outperform deep learning on tabular data? and Tabular Data: Deep Learning is Not All You Need, used these same suites and protocols to argue the dominance of tree-based methods, implicitly assuming stationarity and overlooking pipeline-engineered features common in production. The OpenML Benchmarking Suites underpinned this ecosystem by providing widely reused tasks, which, while convenient, often lack timestamp metadata and preclude time-aware evaluation. In parallel, WILDS pioneered benchmark design that encodes real-world distribution shifts through semantically meaningful splits, demonstrating how evaluation must mirror deployment. Even earlier, Leakage in Data Mining formalized how random splits can leak future information\u2014especially acute with engineered features and temporal dependence\u2014and advocated leakage-resistant validation. Together these works reveal a gap: tabular deep learning evaluations have not systematically reflected temporal drift or the prevalence of engineered feature pipelines that shape signal and leakage in practice. Building on WILDS\u2019s shift-aware split philosophy and the leakage principles from classic work, while directly revisiting the dominant OpenML-based protocols and conclusions, the current paper introduces a benchmark with time-based splits and datasets curated to reflect industrial feature pipelines, enabling a more faithful reality check of tabular methods.",
  "target_paper": {
    "title": "TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks",
    "authors": "Ivan Rubachev, Nikolay Kartashev, Yury Gorishniy, Artem Babenko",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Tabular Data, Benchmarks, Reality Check, Tabular Deep Learning, Applications",
    "abstract": "Advances in machine learning research drive progress in real-world applications. \nTo ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment. In this work, we analyze existing tabular deep learning benchmarks and find two common characteristics of tabular data in typical industrial applications that are underrepresented in the datasets usually used for evaluation in the literature.\nFirst, in real-world deployment scenarios, distribution of data often changes over time. To account for this distribution drift, time-based train/test splits should be used in evaluation. However, existing academic tabular datasets often lack timestamp metadata to enable such evaluation.\nSecond, a considerable portion of datasets in production settings stem from extensive data acquisition and feature engineering pipelines. This can have an impact on the absolute and relative number of predictive, un",
    "openreview_id": "L14sqcrUC3",
    "forum_id": "L14sqcrUC3"
  },
  "analysis_timestamp": "2026-01-06T05:59:39.806241"
}
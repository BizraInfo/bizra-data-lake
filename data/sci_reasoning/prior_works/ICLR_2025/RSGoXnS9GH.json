{
  "prior_works": [
    {
      "title": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
      "authors": "Alicia Parrish et al.",
      "year": 2022,
      "arxiv_id": "2110.08193",
      "role": "Extension",
      "relationship_sentence": "FairMT-Bench extends BBQ\u2019s ambiguity vs. disambiguation QA templates into multi-turn contexts to test whether models avoid biased inferences as additional clarifying turns unfold."
    },
    {
      "title": "BOLD: Dataset and metrics for measuring bias in open-ended language generation",
      "authors": "Shikhar Dhamala et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "FairMT-Bench uses BOLD\u2019s identity- and topic-grounded open-ended prompts as seeds and wraps them into dialog flows to measure representational harms and their accumulation across turns."
    },
    {
      "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
      "authors": "Nikita Nangia et al.",
      "year": 2020,
      "arxiv_id": "2010.00133",
      "role": "Inspiration",
      "relationship_sentence": "FairMT-Bench adopts CrowS-Pairs\u2019 minimal-pair design to construct turn-level counterfactuals in conversation that directly probe interaction fairness across demographic attributes."
    },
    {
      "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
      "authors": "Moin Nadeem et al.",
      "year": 2021,
      "arxiv_id": "2004.09456",
      "role": "Related Problem",
      "relationship_sentence": "FairMT-Bench leverages StereoSet\u2019s stereotype/anti-stereotype/neutral framing to categorize conversational prompts and quantify how stereotype reinforcement evolves over multiple turns."
    },
    {
      "title": "MT-Bench and LLM-as-a-Judge: Multi-turn Evaluation and Automatic Judging with LLMs",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "arxiv_id": "2306.05685",
      "role": "Inspiration",
      "relationship_sentence": "FairMT-Bench adopts MT-Bench\u2019s multi-turn chat evaluation format and the LLM-as-a-Judge protocol, adapting the rubric to fairness-specific criteria and cross-turn consistency scoring."
    },
    {
      "title": "HateCheck: Functional tests for hate speech detection models",
      "authors": "Paul R\u00f6ttger et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "FairMT-Bench borrows HateCheck\u2019s functional, template-based testing philosophy with protected-attribute control to design fairness trade-off tasks that probe moderation vs. helpfulness over dialogue turns."
    }
  ],
  "synthesis_narrative": "BBQ introduced controlled question-answering templates that explicitly contrast ambiguous with disambiguated contexts to expose social biases in inference, providing fine-grained, attribute-aware cases amenable to systematic evaluation. BOLD curated identity- and topic-grounded prompts and metrics for open-ended generation, surfacing representational harms when models discuss demographic groups in free-form text. CrowS-Pairs operationalized minimal pairs differing only in protected-attribute terms, enabling direct tests of parity that isolate model bias from confounders. StereoSet formalized stereotype/anti-stereotype/neutral triplets and a scoring scheme for generative and masked LMs, highlighting how stereotypes can be reinforced or resisted at the prompt-response level. MT-Bench and the LLM-as-a-Judge methodology established reliable procedures for multi-turn dialogue evaluation and rubric-guided automatic judging with strong inter-judge agreement, demonstrating how to structure conversations and score higher-level qualities. HateCheck advanced a functional-test paradigm with templated, attribute-controlled cases to probe specific failure modes and trade-offs, such as over- versus under-moderation around protected groups. Together, these works revealed precise bias probes, identity-controlled templates, and multi-turn judging mechanics\u2014but largely in single-turn or non-conversational settings. The natural next step was to fuse identity-controlled bias probes (BBQ, BOLD, CrowS-Pairs, StereoSet) with multi-turn conversation scaffolds and LLM-as-a-judge scoring (MT-Bench) under a functional testing lens (HateCheck). FairMT-Bench synthesizes these elements into a dialogue-first fairness taxonomy\u2014context understanding, interaction fairness, and fairness trade-offs\u2014and systematically measures bias accumulation and trade-off behaviors across turns using adapted templates and automatic judges.",
  "target_paper": {
    "title": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs",
    "authors": "Zhiting Fan, Ruizhe Chen, Tianxiang Hu, Zuozhu Liu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Fairness, Benchmark, Large language model",
    "abstract": "The increasing deployment of large language model (LLM)-based chatbots has raised concerns regarding fairness. Fairness issues in LLMs may result in serious consequences, such as bias amplification, discrimination, and harm to minority groups. Many efforts are dedicated to evaluating and mitigating biases in LLMs. However, existing fairness benchmarks mainly focus on single-turn dialogues, while multi-turn scenarios, which better reflect real-world conversations, pose greater challenges due to conversational complexity and risk for bias accumulation. In this paper, we introduce a comprehensive benchmark for fairness of LLMs in multi-turn scenarios, **FairMT-Bench**. Specifically, We propose a task taxonomy to evaluate fairness of LLMs cross three stages: context understanding, interaction fairness, and fairness trade-offs, each comprising two tasks. To ensure coverage of diverse bias types and attributes, our multi-turn dialogue dataset FairMT-10K is constructed by integrating data fro",
    "openreview_id": "RSGoXnS9GH",
    "forum_id": "RSGoXnS9GH"
  },
  "analysis_timestamp": "2026-01-06T15:52:39.656633"
}
{
  "prior_works": [
    {
      "title": "Optimistic Policy Optimization with Bandit Feedback",
      "authors": "Shani et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "This work introduced the optimistic policy-optimization/mirror-descent template under bandit feedback; the present paper extends that template to linear MDPs and upgrades its suboptimal K-dependence to the optimal \u00d5(\u221aK) rate."
    },
    {
      "title": "Online Learning in Episodic Markovian Decision Processes by Imitation",
      "authors": "Zimin et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "The occupancy-measure convex formulation and mirror-descent view for full-information adversarial MDPs provided the foundational policy-optimization framework that the current paper instantiates with linear function approximation and confidence-based optimism."
    },
    {
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "authors": "Jin et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "This paper established near-minimax \u00d5(\u221aK) regret for linear MDPs via value-based optimism (LSVI-UCB), setting the K-rate benchmark that the current work matches using a policy-optimization approach rather than value-based/model-based methods."
    },
    {
      "title": "Online Convex Optimization in Adversarial Markov Decision Processes",
      "authors": "Rosenberg et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "By developing OCO-based algorithms for adversarial MDPs with unknown dynamics but obtaining suboptimal rates, this work highlighted the open gap that the present paper closes by achieving optimal \u00d5(\u221aK) regret in the full-information adversarial setting for linear MDPs."
    },
    {
      "title": "Minimax Regret Bounds for Reinforcement Learning in Finite Markov Decision Processes",
      "authors": "Azar et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "UCBVI achieved near-minimax \u00d5(\u221aK) regret in tabular stochastic MDPs, furnishing the optimal K-rate target that the present policy-optimization method seeks to attain (and does) under function approximation."
    },
    {
      "title": "Online Markov Decision Processes",
      "authors": "Even-Dar et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "This seminal adversarial MDP formulation and regret notion (with full-information feedback) underpin the adversarial component of the current work, which adapts these ideas to linear MDPs with unknown dynamics and achieves optimal rates."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014rate-optimal \u00d5(\u221aK) regret via a computationally efficient policy-optimization algorithm in linear MDPs\u2014sits at the intersection of three lines of work. First, the policy-optimization viewpoint in adversarial/full-information MDPs was crystallized by Zimin and Neu (2013) and the broader online MDP literature of Even-Dar et al. (2009), which introduced occupancy-measure convexity and mirror-descent style updates as a principled foundation for optimizing policies directly. Second, for stochastic/linear MDPs, Jin et al. (2020) established the state-of-the-art \u00d5(\u221aK) rates using value-based optimism (LSVI-UCB), setting a benchmark in K that policy-based methods had not matched\u2014particularly under bandit feedback. Third, within policy optimization itself, Shani et al. (2020) pioneered optimistic policy optimization for bandit feedback, but with suboptimal K-dependence, and Rosenberg and Mansour (2019) extended OCO to unknown-dynamics adversarial MDPs yet still fell short of optimal rates. The present work integrates these strands: it takes the optimism-plus-mirror-descent policy-optimization template (Shani et al.; Zimin & Neu), adapts it to the linear MDP structure (in the spirit of Jin et al.\u2019s feature-based modeling), and introduces confidence-driven updates that close the known gaps, yielding the first policy-optimization algorithms with optimal \u00d5(\u221aK) regret in stochastic bandit settings and in adversarial full-information linear MDPs. Azar et al. (2017) provide the tabular minimax baseline that contextualizes the optimal K dependence achieved here.",
  "analysis_timestamp": "2026-01-06T23:09:26.425980"
}
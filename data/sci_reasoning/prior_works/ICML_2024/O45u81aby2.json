{
  "prior_works": [
    {
      "title": "Least Squares Quantization in PCM",
      "authors": "Stuart P. Lloyd",
      "year": 1982,
      "role": "Foundation",
      "relationship_sentence": "ToRES\u2019s prototype\u2013sample affinity and direct optimization of discrete cluster indicators inherit the prototype-based clustering principle of k-means introduced by Lloyd, replacing dense sample\u2013sample affinities with assignments to a small set of prototypes."
    },
    {
      "title": "On the Equivalence of Nonnegative Matrix Factorization and K-means",
      "authors": "Chris Ding et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s unification of representation learning and clustering by directly optimizing discrete cluster indicators is grounded in the NMF\u2013k-means equivalence, which formalizes how low-dimensional representations and hard assignments can be jointly optimized."
    },
    {
      "title": "Co-regularized Multi-view Spectral Clustering",
      "authors": "Abhishek Kumar et al.",
      "year": 2011,
      "role": "Foundation",
      "relationship_sentence": "ToRES\u2019s design of cross-view prototypes to capture consensus features directly follows the co-regularization principle of enforcing agreement across views, but operationalizes it via shared prototypes rather than coupled graphs/eigenvectors."
    },
    {
      "title": "Robust Recovery of Subspace Structures by Low-Rank Representation",
      "authors": "Guangcan Liu et al.",
      "year": 2013,
      "role": "Gap Identification",
      "relationship_sentence": "Self-expression methods like LRR underpin many IMVC baselines but incur dense sample\u2013sample affinity construction with high time/memory and sensitive hyperparameters; ToRES explicitly replaces this with prototype\u2013sample affinity to address these limitations."
    },
    {
      "title": "One-Pass Incomplete Multi-view Clustering (OPIMC)",
      "authors": "Handong Zhao et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "OPIMC is a resource-friendly, prototype-based IMVC baseline; ToRES builds on the same scalability intuition but extends it with cross-view prototypes and a discrete, unified optimization to eliminate hyperparameters and stabilize results."
    },
    {
      "title": "Doubly Aligned Incomplete Multi-view Clustering (DAIMC)",
      "authors": "Shaohua Hu et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "DAIMC\u2019s NMF-based alignment framework is a strong IMVC baseline but requires multiple sensitive hyperparameters and decouples representation from clustering; ToRES targets these gaps by parameter-free cross-view prototypes and direct discrete indicator optimization."
    }
  ],
  "synthesis_narrative": "ToRES\u2019s core leap\u2014replacing dense self-expression graphs with prototype\u2013sample affinities while jointly and discretely optimizing clustering\u2014stands on two intertwined lines of work. The first is the prototype-based clustering lineage. Lloyd\u2019s k-means established prototypes and hard assignments, and Ding et al. formalized the tight coupling between representation learning and discrete assignments via the NMF\u2013k-means equivalence. ToRES leverages this to unify representation learning with clustering and directly optimize discrete indicators, yielding stability and eliminating variance-inducing relaxations. The second line is multi-view consensus. Kumar et al.\u2019s co-regularized multi-view spectral clustering articulated the need for cross-view agreement; ToRES reifies this by introducing cross-view prototypes that encode shared, consensus structure while retaining view-wise prototypes for complementarities\u2014achieving consensus without heavy graph coupling or tuning. Against this backdrop, ToRES explicitly departs from the dominant self-expression paradigm exemplified by LRR, whose dense sample\u2013sample affinities drive high memory/time and hyperparameter sensitivity. Instead, ToRES\u2019s prototype\u2013sample formulation scales linearly in samples and reduces storage. Finally, among IMVC baselines, OPIMC pioneered resource-friendly, prototype-based clustering for incomplete views, and DAIMC provided a strong NMF-alignment baseline; both, however, still involve multiple hyperparameters and decoupled representation/clustering. ToRES synthesizes these strands\u2014prototype efficiency, consensus enforcement, and discrete joint optimization\u2014into a resource-friendly, extensible, and stable IMVC framework.",
  "analysis_timestamp": "2026-01-06T23:09:26.504224"
}
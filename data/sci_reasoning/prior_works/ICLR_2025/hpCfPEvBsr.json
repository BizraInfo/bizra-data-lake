{
  "prior_works": [
    {
      "title": "Holistic Evaluation of Language Models",
      "authors": "Percy Liang et al.",
      "year": 2022,
      "arxiv_id": "2211.09110",
      "role": "Inspiration",
      "relationship_sentence": "HELM\u2019s emphasis on standardized, scenario-driven evaluation and coverage/robustness analyses directly motivates MixEval-X\u2019s push to standardize protocols across modalities and reconstruct realistic task distributions rather than averaging disparate benchmarks."
    },
    {
      "title": "Dynabench: Rethinking Benchmarking in NLP",
      "authors": "Douwe Kiela et al.",
      "year": 2021,
      "arxiv_id": "2104.14337",
      "role": "Gap Identification",
      "relationship_sentence": "By showing how static benchmarks can be gamed and drift from real-world use, Dynabench surfaces the query and grading biases MixEval-X explicitly targets with its adaptation\u2013rectification pipeline to better reflect real deployment distributions."
    },
    {
      "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "MT-Bench and Chatbot Arena establish crowdsourced, real-world pairwise preferences as a meta-evaluation signal, which MixEval-X uses to validate that its any-to-any rankings correlate with real user judgments."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "LLaVA introduced LLaVA-Bench (In-the-Wild) and LLM-as-a-judge protocols for free-form multimodal responses, highlighting open-ended grading and prompt-format biases that MixEval-X standardizes and rectifies across modalities."
    },
    {
      "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
      "authors": "Yiyang Fu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "MME revealed fragmentation in VLM evaluation (heterogeneous prompts, answer formats, and metrics), a limitation MixEval-X addresses by unifying protocol conventions and applying dataset-specific adaptation to enable fair any-to-any comparisons."
    },
    {
      "title": "MMMU: A Massive Multi-discipline Multimodal Understanding Benchmark for Foundation Models",
      "authors": "Yue et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "MMMU\u2019s expert-level, multi-discipline tasks establish the need for broad, real-world multimodal coverage, which MixEval-X incorporates via benchmark mixture while correcting distributional skew and grading inconsistencies."
    },
    {
      "title": "OpenCompass: A Universal Evaluation Platform for Foundation Models",
      "authors": "Zhengxiao Du et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "OpenCompass systematized large-scale evaluation but exposed cross-benchmark inconsistencies and siloed protocols, issues MixEval-X tackles by mixing and rectifying datasets to a unified, real-world-aligned any-to-any standard."
    }
  ],
  "synthesis_narrative": "Holistic Evaluation of Language Models argued for standardized, scenario-based assessments and transparent reporting of coverage and robustness, providing a clear blueprint for reducing evaluation variance caused by inconsistent protocols. Dynabench demonstrated how static or siloed benchmarks can be gamed and drift away from real-world usage, surfacing query and grading biases that require adaptive design. MT-Bench and Chatbot Arena established crowdsourced pairwise preference as a practical gold standard for open-ended evaluation, offering a meta-evaluation signal for how benchmark rankings align with real user judgments. LLaVA introduced in-the-wild multimodal evaluation with open-form responses and LLM-as-a-judge, revealing prompt-format and rubric sensitivities in grading for vision\u2013language tasks. MME cataloged fragmentation in VLM evaluation\u2014heterogeneous prompts, answer conventions, and metrics\u2014while MMMU broadened multimodal task breadth to expert-level, multi-discipline settings, underscoring the need for wide coverage. OpenCompass unified large-scale evaluations but also highlighted cross-benchmark inconsistencies that complicate fair comparisons. Taken together, these works expose a core opportunity: unify heterogeneous multimodal evaluations while aligning them with real-world task distributions and trustworthy grading. MixEval-X synthesizes HELM\u2019s standardization ethos, Dynabench\u2019s bias awareness, and Arena-style meta-evaluation with a mixture-and-adaptation\u2013rectification pipeline, integrating diverse benchmarks (e.g., MMMU-style tasks) into a single any-to-any framework whose rankings demonstrably track real user preferences.",
  "target_paper": {
    "title": "MixEval-X: Any-to-any Evaluations from Real-world Data Mixture",
    "authors": "Jinjie Ni, Yifan Song, Deepanway Ghosal, Bo Li, David Junhao Zhang, Xiang Yue, Fuzhao Xue, Yuntian Deng, Zian Zheng, Kaichen Zhang, Mahir Shah, Kabir Jain, Yang You, Michael Shieh",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Evaluation, Multi-modal Evaluation, Benchmark, Multi-modal Benchmark, Any-to-any, MixEval, Real-world, Data Mixture, Artificial General Intelligence, AGI",
    "abstract": "Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communities with varying protocols and maturity levels; and (2) significant query, grading, and generalization biases. To address these, we introduce MixEval-X, the first any-to-any, real-world benchmark designed to optimize and standardize evaluations across diverse input and output modalities. We propose multi-modal benchmark mixture and adaptation-rectification pipelines to reconstruct real-world task distributions, ensuring evaluations generalize effectively to real-world use cases. Extensive meta-evaluations show our approach effectively aligns benchmark samples with real-world task distributions. Meanwhile, MixEval-X's model rankings correlate strongly with that of crowd-sourced real-world eva",
    "openreview_id": "hpCfPEvBsr",
    "forum_id": "hpCfPEvBsr"
  },
  "analysis_timestamp": "2026-01-06T18:22:33.374580"
}
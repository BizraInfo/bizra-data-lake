{
  "prior_works": [
    {
      "title": "BERT Rediscovers the Classical NLP Pipeline",
      "authors": "Ian Tenney et al.",
      "year": 2019,
      "arxiv_id": "1905.05950",
      "role": "Extension",
      "relationship_sentence": "This work introduced layerwise linear probing to decode specific properties from transformer activations, which we directly extend by applying head-wise linear probes to predict continuous DW-NOMINATE ideology from LLM activations during persona-conditioned generation."
    },
    {
      "title": "What Does BERT Look At? An Analysis of BERT\u2019s Attention",
      "authors": "Kevin Clark et al.",
      "year": 2019,
      "arxiv_id": "1906.04341",
      "role": "Inspiration",
      "relationship_sentence": "By demonstrating specialized attention heads and head-level analysis methods, this paper motivated our search for and identification of specific attention heads whose activations linearly encode political perspective."
    },
    {
      "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
      "authors": "Elena Voita et al.",
      "year": 2019,
      "arxiv_id": "1905.09418",
      "role": "Related Problem",
      "relationship_sentence": "Their finding that a sparse subset of mid-layer heads carry key signals directly informed our head-wise probing strategy and interpretation that ideology-predictive heads concentrate in middle layers."
    },
    {
      "title": "Toy Models of Superposition in Neural Networks",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "arxiv_id": "2209.07612",
      "role": "Inspiration",
      "relationship_sentence": "The superposition and linear feature hypothesis from this work underpins our central claim and test that political perspectives correspond to approximately linear directions in LLM activation space."
    },
    {
      "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
      "authors": "Lisa P. Argyle et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Their evidence and prompting methodology for eliciting group-specific viewpoints from LMs directly motivated our persona prompts that condition models to write from specific lawmakers\u2019 perspectives to expose ideological signals."
    },
    {
      "title": "Whose Opinions Do Language Models Reflect?",
      "authors": "Shibani Santurkar et al.",
      "year": 2023,
      "arxiv_id": "2303.17548",
      "role": "Gap Identification",
      "relationship_sentence": "By showing that LMs\u2019 outputs align with particular population opinions but not explaining the internal mechanism, this work motivates our move from behavioral bias measurement to identifying linear internal representations tied to a validated ideology scale."
    },
    {
      "title": "Congress: A Political-Economic History of Roll Call Voting (DW-NOMINATE)",
      "authors": "Keith T. Poole and Howard Rosenthal",
      "year": 1997,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "We rely on the DW-NOMINATE ideology scores introduced by this work as the continuous ground-truth targets for supervising and validating our head-wise linear probes."
    }
  ],
  "synthesis_narrative": "Layerwise probing introduced the idea that linear classifiers can decode specific properties from transformer activations, revealing where information emerges across depth (Tenney et al., 2019). Analyses of attention laid the groundwork for head-level interpretability, showing specialized heads and practical techniques to study them (Clark et al., 2019), and further established that a sparse subset of mid-layer heads often carry the most functionally relevant signals (Voita et al., 2019). From a representational perspective, the superposition hypothesis argued that features are linearly embedded and can be recovered by simple decoders even when distributed, motivating linear readouts as a principled test for semantic directions in activation space (Elhage et al., 2022). In parallel, social-science\u2013oriented work demonstrated that prompting language models with demographic or persona cues can elicit distinct group-specific viewpoints (Argyle et al., 2023), while empirical audits showed that model outputs reflect particular populations\u2019 opinions without clarifying the internal basis of those behaviors (Santurkar et al., 2023). Finally, DW-NOMINATE provided a validated, continuous ideology scale to anchor any representation of political perspective (Poole & Rosenthal, 1997). Together, these strands suggested a path: elicit politically informative generations via persona prompts for named lawmakers, record head-level activations, and test for linear structure by predicting DW-NOMINATE scores. Combining head-wise interpretability with linear probing and a behavioral elicitation paradigm allowed a direct bridge from outputs to mechanisms, revealing sparse mid-layer heads whose activations form linear representations organizing political perspective.",
  "target_paper": {
    "title": "Linear Representations of Political Perspective Emerge in Large Language Models",
    "authors": "Junsol Kim, James Evans, Aaron Schein",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "large language model, political perspective, ideology, representation learning",
    "abstract": "Large language models (LLMs) have demonstrated the ability to generate text that realistically reflects a range of different subjective human perspectives. This paper studies how LLMs are seemingly able to reflect more liberal versus more conservative viewpoints among other political perspectives in American politics. We show that LLMs possess linear representations of political perspectives within activation space, wherein more similar perspectives are represented closer together. To do so, we probe the attention heads across the layers of three open transformer-based LLMs (Llama-2-7b-chat, Mistral-7b-instruct, Vicuna-7b). We first prompt models to generate text from the perspectives of different U.S. lawmakers. We then identify sets of attention heads whose activations linearly predict those lawmakers' DW-NOMINATE scores, a widely-used and validated measure of political ideology. We find that highly predictive heads are primarily located in the middle layers, often speculated to enco",
    "openreview_id": "rwqShzb9li",
    "forum_id": "rwqShzb9li"
  },
  "analysis_timestamp": "2026-01-06T08:20:54.164661"
}
{
  "prior_works": [
    {
      "title": "Piecewise-smooth Dynamical Systems: Theory and Applications",
      "authors": "Mario di Bernardo, Chris J. Budd, Alan R. Champneys, Piotr Kowalczyk",
      "year": 2008,
      "role": "Theoretical foundation for border-collision bifurcations in piecewise-smooth systems",
      "relationship_sentence": "The paper\u2019s central claim\u2014that ReLU RNNs undergo border-collision-type bifurcations when activation patterns switch\u2014rests on the piecewise-smooth bifurcation theory developed in this monograph, which provides the exact mathematical framework the authors leverage to formalize and prove loss jumps at switching events."
    },
    {
      "title": "Chaos in Random Neural Networks",
      "authors": "H. Sompolinsky, A. Crisanti, H.-J. Sommers",
      "year": 1988,
      "role": "Foundational RNN dynamical systems theory and phase transitions",
      "relationship_sentence": "By showing how varying network gain induces qualitative regime changes (fixed point to chaos) via bifurcations, this work established the paradigm that RNN dynamics undergo sharp topological shifts under parameter variation, a premise directly extended here to training-time parameter updates and associated loss jumps."
    },
    {
      "title": "Opening the Black Box: Low-Dimensional Dynamics in Recurrent Neural Networks",
      "authors": "David Sussillo, Omri Barak",
      "year": 2013,
      "role": "Methodological toolkit for fixed-point and bifurcation analysis in trained RNNs",
      "relationship_sentence": "This paper introduced practical fixed-point/eigenstructure analyses to map RNN computations to dynamical primitives and bifurcations; the NeurIPS work builds on this lens to track how training traverses bifurcation manifolds and to link such crossings to abrupt loss changes."
    },
    {
      "title": "On the difficulty of training recurrent neural networks",
      "authors": "Razvan Pascanu, Tomas Mikolov, Yoshua Bengio",
      "year": 2013,
      "role": "Link between RNN Jacobian spectra, sensitivity, and training instabilities",
      "relationship_sentence": "By tying trainability to Jacobian norms/spectra and sensitivity, this work motivates the current paper\u2019s use of state-transition Jacobian eigenvalues to derive conditions under which small parameter updates trigger qualitative dynamical changes and cause loss spikes."
    },
    {
      "title": "On the number of linear regions of deep neural networks",
      "authors": "Guido F. Mont\u00fafar, Razvan Pascanu, Kyunghyun Cho, Yoshua Bengio",
      "year": 2014,
      "role": "Geometric understanding of ReLU networks as piecewise-linear maps",
      "relationship_sentence": "The characterization of ReLU models as partitions into linear regions underpins the present paper\u2019s modeling of activation-pattern switches as boundary crossings between dynamical regimes, enabling a rigorous connection to border-collision bifurcations and nonsmooth loss changes."
    },
    {
      "title": "Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks",
      "authors": "Nils Bertschinger, Thomas Natschl\u00e4ger",
      "year": 2004,
      "role": "Empirical and theoretical link between performance and proximity to bifurcation",
      "relationship_sentence": "Demonstrating that RNNs operate best near critical bifurcation points, this work foreshadows the paper\u2019s core insight that training trajectories naturally encounter bifurcation surfaces where small parameter shifts can induce qualitative dynamic changes and abrupt loss behavior."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014proving and empirically validating that loss jumps during training of ReLU-based RNNs are caused by bifurcations, specifically border-collision events triggered by activation-pattern switches\u2014sits at the intersection of three threads of prior work. First, classical RNN dynamical systems theory (Sompolinsky\u2013Crisanti\u2013Sommers) established that small parameter changes can induce qualitative regime transitions (fixed point, chaos), and subsequent results on optimal computation near criticality (Bertschinger\u2013Natschl\u00e4ger) linked performance to proximity to such bifurcations. Second, Sussillo and Barak provided a practical methodology to analyze trained RNNs via fixed points, linearizations, and bifurcation diagrams, a toolbox the present work extends to track training-time crossings of bifurcation manifolds. Third, the piecewise-linear geometry of ReLU networks (Mont\u00fafar et al.) and the nonsmooth bifurcation theory for piecewise-smooth systems (di Bernardo et al.) together furnish the exact mathematical apparatus: activation-pattern changes correspond to boundary crossings in a piecewise-linear map, where border-collision bifurcations can occur. Complementing these, Pascanu\u2013Mikolov\u2013Bengio\u2019s analysis of Jacobian spectra and sensitivity in RNN training motivates the use of state-transition eigenstructure to derive conditions under which such crossings produce qualitative dynamic (and thus loss) discontinuities. Integrating these strands, the paper rigorously connects optimization dynamics to topological changes in the learned recurrent system, explaining and predicting sudden loss jumps as bifurcation-induced phenomena intrinsic to ReLU RNN training.",
  "analysis_timestamp": "2026-01-07T00:02:04.785249"
}
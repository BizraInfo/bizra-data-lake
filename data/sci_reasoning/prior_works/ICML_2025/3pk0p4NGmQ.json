{
  "prior_works": [
    {
      "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
      "authors": "Zhou et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "CVE-Bench adopts WebArena\u2019s core idea of a sandboxed, instrumented, self-hosted web ecosystem to evaluate LLM agents in realistic browser/server interactions and repurposes it specifically for security exploitation scenarios."
    },
    {
      "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
      "authors": "Jimenez et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "SWE-bench\u2019s recipe of constructing a real-world, containerized benchmark with precise execution oracles directly inspired CVE-Bench\u2019s use of reproducible environments and automated exploit success criteria for end-to-end evaluation."
    },
    {
      "title": "CyberSecEval 2: Measuring and Reducing Advanced Cybersecurity Risks of LLMs",
      "authors": "OpenAI Preparedness et al.",
      "year": 2024,
      "role": "Gap Identification",
      "relationship_sentence": "CyberSecEval\u2019s text- and tool-only evaluations highlighted a gap in end-to-end, real-system exploit testing; CVE-Bench explicitly addresses this by moving from prompt-based assessments to live exploitation of actual vulnerable web apps."
    },
    {
      "title": "CyberBattleSim: An Autonomous Cybersecurity Simulation for Reinforcement Learning",
      "authors": "Microsoft Research et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "CyberBattleSim\u2019s abstract network-level simulation underscored the lack of benchmarks involving real CVEs and web stacks; CVE-Bench closes this gap by grounding tasks in concrete CVE instances and production-like web deployments."
    },
    {
      "title": "PentestGPT: An LLM-empowered Penetration Testing Agent",
      "authors": "Zhang et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "PentestGPT demonstrated agentic penetration testing with tool use but lacked a standardized, reproducible, real-world exploit benchmark; CVE-Bench provides that target environment and evaluation protocol and uses such agents as baselines."
    },
    {
      "title": "OWASP Juice Shop: Deliberately Insecure Web Application for Security Training",
      "authors": "Kimminich et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "Educational vulnerable apps like OWASP Juice Shop established the paradigm of safe, sandboxed web targets; CVE-Bench generalizes this paradigm to a curated set of real CVEs with automated exploit oracles for LLM-agent evaluation."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Yao et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "CVE-Bench evaluates tool-using LLM agents that follow ReAct-style reasoning-and-acting loops; the benchmark\u2019s sandbox and instrumentation are designed to measure effectiveness of such agent paradigms on real CVE exploitation."
    }
  ],
  "synthesis_narrative": "CVE-Bench\u2019s core innovation\u2014evaluating LLM agents on end-to-end exploitation of real-world web application vulnerabilities\u2014emerges from converging lines of work in agent environments, real-world benchmarking, and cyber-capability assessment. WebArena provided the foundational idea of a sandboxed, instrumented web ecosystem where agents interact with realistic sites; CVE-Bench extends this from general web tasks to security exploitation. In parallel, SWE-bench demonstrated how to build rigorous, reproducible, real-world benchmarks using containerized setups and automated execution oracles; CVE-Bench adapts this blueprint to define precise exploit success criteria for live web stacks. Cybersecurity evaluation efforts such as CyberSecEval 2 revealed that prevailing assessments rely on text-only or tool-in-the-loop prompts, lacking real-system end-to-end tests; likewise, CyberBattleSim exposed the limits of abstract simulations for gauging genuine exploitation capability. CVE-Bench directly addresses these gaps by grounding tasks in concrete CVEs and self-contained deployments. On the agent side, tools like PentestGPT showcased that LLMs can coordinate reconnaissance and exploitation workflows but lacked standardized, comprehensive, reproducible targets\u2014functions CVE-Bench now supplies and against which such agents serve as baselines. Finally, the broader tradition of sandboxed vulnerable web apps (e.g., OWASP Juice Shop) established safe target design, which CVE-Bench elevates to a curated, critical-severity CVE suite with strict evaluation harnesses tailored for LLM-agent testing.",
  "analysis_timestamp": "2026-01-06T23:07:19.598823"
}
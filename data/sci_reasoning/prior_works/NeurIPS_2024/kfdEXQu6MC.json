{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Foundational NTK theory for training dynamics in the infinite-width (kernel) regime",
      "relationship_sentence": "This paper\u2019s core contribution explicitly generalizes the NTK of Jacot et al. to settings with discontinuous activations by replacing true derivatives with surrogate derivatives, and its analysis of the naive extension\u2019s ill-posedness is framed against the original NTK construction."
    },
    {
      "title": "Deep Neural Networks as Gaussian Processes",
      "authors": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein",
      "year": 2018,
      "role": "Infinite-width GP correspondence underpinning NTK recursions and wide-network limits",
      "relationship_sentence": "The surrogate-gradient NTK leverages the same infinite-width linearization and kernel-recursion viewpoint introduced via the GP correspondence, adapting the backprop Jacobian to include surrogate derivatives when activations have jumps."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat, Francis Bach",
      "year": 2019,
      "role": "Kernel (lazy) training regime linking gradient descent to kernel gradient descent via linearization",
      "relationship_sentence": "By showing gradient flow near initialization behaves like kernel regression, this work provides the template that the present paper extends to the surrogate-gradient setting, formalizing kernel dynamics when true derivatives are replaced by surrogates."
    },
    {
      "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation",
      "authors": "Yoshua Bengio, Nicolas L\u00e9onard, Aaron Courville",
      "year": 2013,
      "role": "Introduction of the straight-through estimator and surrogate derivatives for non-differentiable/binary units",
      "relationship_sentence": "The surrogate-gradient NTK directly analyzes training dynamics induced by straight-through/surrogate derivatives first proposed by Bengio et al., providing the missing theoretical footing for such biased gradient substitutes."
    },
    {
      "title": "BinaryConnect: Training Deep Neural Networks with Binary Weights while Keeping Full Precision Activations",
      "authors": "Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David",
      "year": 2015,
      "role": "Early, influential application of straight-through/surrogate gradients to discrete/binary neural networks",
      "relationship_sentence": "This empirical success of training with surrogate derivatives motivates the need for a principled analysis; the proposed surrogate-gradient NTK offers a kernel-level description applicable to BinaryConnect-style models with discontinuities."
    },
    {
      "title": "Long short-term memory and learning-to-learn in networks of spiking neurons",
      "authors": "Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, Wolfgang Maass",
      "year": 2018,
      "role": "Establishing surrogate gradient learning for spiking neurons via pseudo-derivatives",
      "relationship_sentence": "By formalizing surrogate gradients for spiking non-differentiable dynamics, this work provides concrete surrogate derivative choices that the new surrogate-gradient NTK can analyze in the infinite-width training limit."
    },
    {
      "title": "Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks",
      "authors": "Emre O. Neftci, Hesham Mostafa, Friedemann Zenke",
      "year": 2019,
      "role": "Survey and systematization of surrogate gradient methods for SNNs and their open theoretical gaps",
      "relationship_sentence": "This survey articulates the practical efficacy and theoretical void of surrogate gradient learning, directly motivating the present paper\u2019s sNTK framework that explains SGL training dynamics and well-posedness."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014defining a surrogate-gradient neural tangent kernel (sNTK) and showing how it governs gradient descent with surrogate derivatives\u2014rests on two pillars: NTK-based training dynamics and the surrogate-gradient paradigm for non-differentiable models. Jacot et al. (2018) established the NTK, revealing that in the infinite-width limit, gradient descent linearizes around initialization and follows kernel gradient flow. This perspective is grounded in the GP correspondence for wide networks (Lee et al., 2018) and the lazy-training/kernel regime formalization (Chizat & Bach, 2019). However, these analyses implicitly rely on well-behaved derivatives; naively extending them to discontinuous activations leads to ill-posed dynamics\u2014precisely the failure mode diagnosed in the present work.\nIn parallel, the surrogate-gradient lineage\u2014originating with the straight-through estimator (Bengio et al., 2013) and crystallized in practice via BinaryConnect (Courbariaux et al., 2015)\u2014provides effective, biased derivatives for discrete or spiking units. In spiking neural networks, Bellec et al. (2018) introduced pseudo-derivatives enabling backpropagation through spike discontinuities, while Neftci et al. (2019) synthesized the area and highlighted the lack of theory. The current paper knits these threads by replacing true derivatives with surrogates in the backprop Jacobian, thereby defining an sNTK that renders gradient descent well-posed in the infinite-width limit even with jump activations. This yields a principled kernel description of surrogate-gradient learning, explaining its empirical success and offering a framework to compare surrogate choices analytically.",
  "analysis_timestamp": "2026-01-06T23:39:42.940833"
}
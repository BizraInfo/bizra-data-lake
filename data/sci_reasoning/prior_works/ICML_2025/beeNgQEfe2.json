{
  "prior_works": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc V. Le, Denny Zhou",
      "year": 2022,
      "role": "Introduced chain-of-thought (CoT) prompting and the idea of scaling test-time compute via longer, explicit reasoning traces; established the foundation for training and distilling from reasoning traces without external verification.",
      "relationship_sentence": "The paper\u2019s VF baselines are rooted in CoT-style trace generation and distillation that Wei et al. popularized, which the present work proves can be suboptimal under heterogeneous solution traces and non-sharp rewards."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Denny Zhou",
      "year": 2023,
      "role": "Showed that sampling multiple CoT traces and majority voting (a VF approach) scales test-time compute to boost accuracy without explicit verifiers or RL.",
      "relationship_sentence": "Setlur et al. explicitly analyze the inefficiencies of self-consistency-style VF scaling as output length and data grow, contrasting it with verifier-guided policy improvement."
    },
    {
      "title": "STaR: Bootstrapping Reasoning With Reasoning",
      "authors": "Y. Zelikman, Yuhuai (Tony) Wu, Jesse Michael Han, Noah D. Goodman",
      "year": 2022,
      "role": "Proposed distilling successful reasoning traces by filtering on correct final answers and fine-tuning on those traces, a canonical VF alternative to RL.",
      "relationship_sentence": "The new paper formalizes why approaches like STaR\u2014supervised cloning of successful traces gated by outcomes\u2014can be compute/data-inefficient versus verifier-based RL/search when correct traces are diverse."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan",
      "year": 2023,
      "role": "Framed deliberate search over intermediate thoughts; introduced mechanisms to guide search using evaluators/verifiers, connecting test-time compute scaling with verification.",
      "relationship_sentence": "By contrasting VF search and verifier-guided search paradigms exemplified by Tree of Thoughts, the paper motivates and theoretically supports verifier-based policy improvement over cloning search traces."
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems",
      "authors": "Adam Cobbe et al.",
      "year": 2021,
      "role": "Pioneered learning verifiers to check or rank candidate solutions in math, establishing the practicality of 0/1 verifiable rewards for guiding selection and learning.",
      "relationship_sentence": "The verifier signal assumed in Setlur et al.\u2019s VB analysis directly follows the verification paradigm Cobbe et al. introduced for math problem solving."
    },
    {
      "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning (DAgger)",
      "authors": "St\u00e9phane Ross, Geoffrey J. Gordon, J. Andrew Bagnell",
      "year": 2011,
      "role": "Provided foundational theory on the limitations of pure behavior cloning and the benefits of feedback-driven policy improvement to combat distributional shift.",
      "relationship_sentence": "The paper leverages the core insight that cloning demonstrations (VF distillation) can be suboptimal and argues, in the LLM reasoning setting, that verifier feedback enables superior policy improvement."
    },
    {
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "authors": "Aviral Kumar, Aurick Zhou, George Tucker, Sergey Levine",
      "year": 2020,
      "role": "Established that reward-driven policy improvement can outperform behavior cloning under heterogeneous datasets, offering tools and theory for sample-efficient improvement without online interaction.",
      "relationship_sentence": "Setlur et al. build on the RL-over-BC intuition from CQL, adapting it to verifiable rewards in reasoning and proving VB methods\u2019 superiority over VF cloning as compute and data scale."
    }
  ],
  "synthesis_narrative": "Setlur et al. address how to most effectively scale test-time compute for reasoning: by distilling traces without verification (VF) or by leveraging verifier feedback within RL or search (VB). The VF paradigm was catalyzed by Chain-of-Thought prompting, which made reasoning traces a first-class training signal, and by Self-Consistency, which scales compute through multi-sample voting. STaR further operationalized VF learning by filtering on correct outcomes and cloning successful traces. Tree of Thoughts then framed LLM reasoning as search over intermediate steps, opening the door to either VF heuristics or verifier-guided evaluation during search. In parallel, the math verification line showed that reliable 0/1 verifiers can be trained and used to select correct solutions, concretely instantiating the VB signal the current paper studies. The theoretical core of Setlur et al. connects these LLM developments to classic learning principles: DAgger\u2019s critique of pure imitation highlights why cloning heterogeneous solution traces can be brittle, and offline RL work such as Conservative Q-Learning demonstrates how reward-driven policy improvement outperforms behavior cloning when data are diverse. Synthesizing these threads, the paper proves that under realistic conditions\u2014heterogeneous valid traces and non-sharp reward distributions\u2014VF distillation scales poorly with output length and data, while verifier-guided RL/search achieves superior compute/data efficiency. This yields a principled case for prioritizing verifiers and policy improvement over raw trace distillation as test-time compute is scaled.",
  "analysis_timestamp": "2026-01-07T00:21:33.184194"
}
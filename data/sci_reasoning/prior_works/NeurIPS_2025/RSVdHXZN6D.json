{
  "prior_works": [
    {
      "title": "Flow Matching for Generative Modeling",
      "authors": [
        "Yaron Lipman",
        "et al."
      ],
      "year": 2022,
      "role": "Core training paradigm",
      "relationship_sentence": "FUDOKI builds directly on flow matching by learning velocity fields along probability paths, adapting the approach to discrete token spaces and designing task-aware paths for unified multimodal generation."
    },
    {
      "title": "Stochastic Interpolants: A Unifying Framework for Flow- and Diffusion-based Generative Modeling",
      "authors": [
        "Michael S. Albergo",
        "Eric Vanden-Eijnden",
        "et al."
      ],
      "year": 2023,
      "role": "Probability-path design",
      "relationship_sentence": "The stochastic-interpolant perspective motivates FUDOKI\u2019s construction of metric-induced probability paths and iterative refinement without relying on masking-based corruptions."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models in Discrete State Spaces (D3PM)",
      "authors": [
        "Jacob Austin",
        "et al."
      ],
      "year": 2021,
      "role": "Discrete generative modeling baseline",
      "relationship_sentence": "FUDOKI targets the same categorical domains as D3PM but replaces fixed masking/transition matrices with discrete flow matching, overcoming limitations of corruption schedules and enabling richer bidirectional context."
    },
    {
      "title": "MaskGIT: Masked Generative Image Transformer",
      "authors": [
        "Huiwen Chang",
        "et al."
      ],
      "year": 2022,
      "role": "Iterative non-autoregressive decoding",
      "relationship_sentence": "FUDOKI inherits the idea of parallel iterative refinement and self-correction from MaskGIT, but achieves it via learned discrete flows and kinetic-optimal velocities instead of mask-and-predict heuristics."
    },
    {
      "title": "Mask-Predict: Parallel Decoding for Non-Autoregressive Translation",
      "authors": [
        "Marjan Ghazvininejad",
        "et al."
      ],
      "year": 2019,
      "role": "Iterative refinement for sequences",
      "relationship_sentence": "FUDOKI generalizes the iterative mask-and-refine paradigm from NAT to a principled discrete flow process, enabling bidirectional context integration for both language and image token sequences."
    },
    {
      "title": "VQGAN: Generative Adversarial Networks for Efficient, High Fidelity Neural Image Synthesis",
      "authors": [
        "Patrick Esser",
        "Robin Rombach",
        "Bj\u00f6rn Ommer"
      ],
      "year": 2021,
      "role": "Discrete visual tokenization",
      "relationship_sentence": "By leveraging vector-quantized image tokens as in VQGAN, FUDOKI operates natively in discrete spaces, making flow matching feasible for unified understanding and generation."
    },
    {
      "title": "A Computational Fluid Mechanics Solution to the Monge\u2013Kantorovich Mass Transfer Problem",
      "authors": [
        "Jean-David Benamou",
        "Yann Brenier"
      ],
      "year": 2000,
      "role": "Kinetic-optimal transport foundation",
      "relationship_sentence": "FUDOKI\u2019s kinetic-optimal velocities are grounded in the Benamou\u2013Brenier dynamic OT formulation, using energy-minimizing paths induced by a task-specific metric over discrete tokens."
    }
  ],
  "synthesis_narrative": "FUDOKI\u2019s central advance\u2014replacing autoregression with a discrete flow-matching engine that unifies visual understanding and image generation\u2014stands on three pillars: (1) learning velocity fields along probability paths, (2) operating natively in discrete token spaces, and (3) enabling iterative, bidirectional self-correction. Flow Matching (Lipman et al.) provides the core training recipe: regress a velocity field so that integrating it transports a simple prior to data. Stochastic Interpolants (Albergo et al.) broaden this into a general probability-path design toolbox, which FUDOKI exploits to craft metric-induced paths rather than relying on fixed corruption schedules. The kinetic-optimality component traces to Benamou\u2013Brenier dynamic optimal transport, furnishing an energy-minimizing criterion that yields stable, geometry-aware velocities over token manifolds.\n\nOn the discrete side, D3PM demonstrated that diffusion in categorical spaces is practical but typically depends on masking or pre-specified transition matrices. FUDOKI replaces those with discrete flow matching, learning transport dynamics that better capture multimodal structure and enable richer bidirectional conditioning. Its iterative refinement procedure echoes MaskGIT and Mask-Predict: fast parallel updates with self-correction. However, FUDOKI grounds this iteration in a principled flow field rather than heuristic mask schedules, avoiding raster-scan constraints and improving global coherence. Finally, VQGAN\u2019s vector-quantized tokens supply the discrete visual interface through which the flow operates, allowing the same flow-based mechanism to serve both understanding (inference over tokens) and generation (transport from prior), thereby realizing a unified multimodal model beyond autoregressive paradigms.",
  "analysis_timestamp": "2026-01-06T23:42:48.117632"
}
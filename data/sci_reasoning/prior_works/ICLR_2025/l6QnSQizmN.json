{
  "prior_works": [
    {
      "title": "Trust Region Policy Optimization",
      "authors": "Schulman et al.",
      "year": 2015,
      "arxiv_id": "1502.05477",
      "role": "Extension",
      "relationship_sentence": "LCPO generalizes TRPO\u2019s KL-based trust-region idea by applying a targeted, local KL constraint on states from prior contexts to anchor behavior while optimizing on current-context data."
    },
    {
      "title": "Proximal Policy Optimization Algorithms",
      "authors": "Schulman et al.",
      "year": 2017,
      "arxiv_id": "1707.06347",
      "role": "Baseline",
      "relationship_sentence": "LCPO modifies the PPO update by adding a context-aware local constraint on replayed old-context samples, making PPO the primary on-policy baseline it improves upon."
    },
    {
      "title": "Overcoming Catastrophic Forgetting in Neural Networks",
      "authors": "Kirkpatrick et al.",
      "year": 2017,
      "arxiv_id": "1612.00796",
      "role": "Gap Identification",
      "relationship_sentence": "LCPO directly addresses EWC\u2019s brittleness of parameter-importance regularization by shifting to functional (output-space) anchoring on past experiences to preserve prior behavior."
    },
    {
      "title": "Learning without Forgetting",
      "authors": "Li et al.",
      "year": 2016,
      "arxiv_id": "1606.09282",
      "role": "Inspiration",
      "relationship_sentence": "LCPO adopts LwF\u2019s core idea of preserving old outputs via distillation, instantiating it as a policy KL on replayed states from previous contexts during on-policy updates."
    },
    {
      "title": "Policy Distillation",
      "authors": "Rusu et al.",
      "year": 2015,
      "arxiv_id": "1511.06295",
      "role": "Inspiration",
      "relationship_sentence": "LCPO leverages distillation-style constraints to match prior policy behavior on old-context states, translating policy distillation\u2019s functional preservation into a continual RL setting."
    },
    {
      "title": "Progress & Compress: A Scalable Framework for Continual Learning",
      "authors": "Schwarz et al.",
      "year": 2018,
      "arxiv_id": "1805.06370",
      "role": "Gap Identification",
      "relationship_sentence": "LCPO targets the continual RL setting without task labels that P&C assumes, replacing task-bound consolidation phases with label-free local constraints on old experiences."
    },
    {
      "title": "Contextual Markov Decision Processes",
      "authors": "Hallak et al.",
      "year": 2015,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "LCPO builds on the Contextual MDP formulation where an exogenous context process modulates dynamics, anchoring policies on samples drawn from contexts outside the current distribution."
    }
  ],
  "synthesis_narrative": "Trust-region policy methods established that constraining policy updates via a KL divergence improves stability, first globally with TRPO\u2019s trust region and later approximately with PPO\u2019s clipped surrogate objective. In parallel, continual learning revealed that preserving function outputs is often more reliable than parameter constraints: Learning without Forgetting introduced distillation to keep old predictions intact without storing task labels, and policy distillation showed how KL-based matching can preserve and consolidate behaviors in RL. Parameter-importance approaches like EWC offered a simple regularization route to mitigate forgetting, but their weight-space heuristics proved brittle under distribution shift. Continual RL frameworks such as Progress & Compress further emphasized consolidation across tasks but typically required explicit task boundaries. Meanwhile, the Contextual MDP formalism clarified non-stationarity driven by an exogenous context process, highlighting that the same state-action pair can induce different dynamics as context drifts.\nTogether, these strands suggested a natural opportunity: combine the stability of trust-region updates with output-space preservation from distillation, and target them specifically at states from past contexts in a label-free manner. LCPO synthesizes this by adding a local, context-aware KL constraint\u2014computed on replayed samples from outside the current context distribution\u2014into an on-policy update (e.g., PPO-style), thereby preserving prior behavior without relying on task labels or unstable off-policy corrections while optimizing return on the current context.",
  "target_paper": {
    "title": "Online Reinforcement Learning in Non-Stationary Context-Driven Environments",
    "authors": "Pouya Hamadanian, Arash Nasr-Esfahany, Malte Schwarzkopf, Siddhartha Sen, Mohammad Alizadeh",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "catastrophic forgetting, reinforcement learning, context-driven MDP, online learning, non-stationary",
    "abstract": "We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to \"catastrophic forgetting\" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice), employ brittle regularization heuristics, or use off-policy methods that suffer from instability and poor performance.\n\nWe present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic a",
    "openreview_id": "l6QnSQizmN",
    "forum_id": "l6QnSQizmN"
  },
  "analysis_timestamp": "2026-01-06T09:01:13.526302"
}
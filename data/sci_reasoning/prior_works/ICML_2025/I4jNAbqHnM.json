{
  "prior_works": [
    {
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "authors": "Martin L. Puterman",
      "year": 1994,
      "role": "Baseline",
      "relationship_sentence": "The paper contrasts its central phenomenon\u2014policy performance depending on the number of sampled trajectories in GUMDPs\u2014with Puterman\u2019s classical MDP evaluation where the expected return is independent of how many i.i.d. trials are used, establishing the baseline that this work generalizes and departs from."
    },
    {
      "title": "Constrained Markov Decision Processes",
      "authors": "Eitan Altman",
      "year": 1999,
      "role": "Foundation",
      "relationship_sentence": "Altman\u2019s convex-analytic/occupancy-measure formulation under discounted and average criteria underpins the paper\u2019s modeling of utilities as functions of state\u2013action visitation frequencies and its use of stationary/average occupancy measures in the infinite-horizon analysis."
    },
    {
      "title": "DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections",
      "authors": "Ofir Nachum et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "The discounted GUMDP analysis builds directly on the discounted stationary distribution (discounted occupancy measure) formalized in DualDICE, with the paper\u2019s bounds comparing utilities of empirical discounted occupancies from N trials to the true discounted occupancy."
    },
    {
      "title": "Maximum Entropy Inverse Reinforcement Learning",
      "authors": "Brian D. Ziebart et al.",
      "year": 2008,
      "role": "Inspiration",
      "relationship_sentence": "MaxEnt IRL provides a canonical example of an objective that is a functional of visitation distributions (entropy), directly motivating the GUMDP viewpoint and illustrating why non-additive utilities can make trial aggregation non-trivial."
    },
    {
      "title": "A Theory of Regularized Markov Decision Processes",
      "authors": "Olivier Geist et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Regularized MDPs demonstrate how adding convex, non-linear terms to standard returns alters policy evaluation; this paper extends that idea from local (per-state/action) regularizers to global utilities over occupancies and reveals the new trials-dependence phenomenon absent in the regularized-MDP setting."
    },
    {
      "title": "Concentration inequalities for Markov chains",
      "authors": "Daniel Paulin",
      "year": 2015,
      "role": "Extension",
      "relationship_sentence": "The finite-trial mismatch bounds rely on concentration of empirical visitation frequencies for Markov chains, and Paulin\u2019s results provide the technical backbone to control deviations between empirical and true occupancies across independently sampled trajectories."
    },
    {
      "title": "Hoeffding's inequality for uniformly ergodic Markov chains",
      "authors": "Peter W. Glynn and Dirk Ormoneit",
      "year": 2002,
      "role": "Extension",
      "relationship_sentence": "For the average-criterion GUMDP analysis, the paper leverages Hoeffding-type concentration for ergodic chains to bound the gap between finite-trial empirical average frequencies and limiting stationary frequencies, directly enabling its upper/lower bounds."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core insight\u2014that in infinite-horizon general-utility MDPs (GUMDPs) the expected performance of a policy depends on the number of i.i.d. trajectories\u2014emerges by departing from the classical additive-return paradigm. Puterman\u2019s textbook theory provides the baseline where expected performance is invariant to the number of trials, while Altman\u2019s convex-analytic approach and occupancy-measure viewpoint supply the foundational machinery to express objectives as functionals of discounted or stationary visitation frequencies. Building on modern occupancy-measure calculus, DualDICE formalizes the discounted stationary distribution that this work compares against empirical discounted occupancies produced by finitely many trajectories. The conceptual move toward non-additive utilities is motivated by objectives like entropy of visitation distributions from Maximum Entropy IRL, which are naturally expressed as global functionals of occupancy and exemplify why averaging across trials is subtle outside linear reward sums. Regularized MDPs further illustrate how convex terms alter evaluation but remain per-step/local, setting up the gap this paper fills by analyzing truly global utilities over occupancies. To turn this conceptual gap into quantitative results, the paper draws on concentration inequalities for Markov chains\u2014Paulin\u2019s general bounds and Glynn\u2013Ormoneit\u2019s Hoeffding-type inequalities for ergodic chains\u2014to control deviations of empirical visitation frequencies from their limiting discounted/stationary counterparts. Together, these works directly enable the finite-versus-infinite trials mismatch bounds for both discounted and average GUMDPs and explain why the number of trials is intrinsic to policy performance in this generalized framework.",
  "analysis_timestamp": "2026-01-06T23:07:19.606284"
}
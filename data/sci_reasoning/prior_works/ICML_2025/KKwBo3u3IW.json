{
  "prior_works": [
    {
      "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (AlphaZero)",
      "authors": "David Silver et al.",
      "year": 2017,
      "role": "Neural MCTS template for board games",
      "relationship_sentence": "The paper\u2019s external search mirrors AlphaZero\u2019s core idea of using a learned policy/value function to guide Monte Carlo Tree Search, but replaces specialized game networks with a single LLM that supplies move priors and value estimates in chess-like domains."
    },
    {
      "title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero)",
      "authors": "Julian Schrittwieser et al.",
      "year": 2020,
      "role": "Model-based planning without external simulator",
      "relationship_sentence": "By planning using learned dynamics and value functions rather than a hand-coded simulator, MuZero directly motivates the authors\u2019 claim of performing search \u2018without calls to an external game engine,\u2019 with the LLM serving as the learned world model."
    },
    {
      "title": "Thinking Fast and Slow with Deep Learning and Tree Search (Expert Iteration)",
      "authors": "Thomas Anthony, Zheng Tian, David Barber",
      "year": 2017,
      "role": "Distilling search into a fast policy via search traces",
      "relationship_sentence": "The paper\u2019s internal search\u2014training a model to emit a linearized tree and final choice\u2014echoes Expert Iteration\u2019s principle of using search to supervise a policy that internalizes planning competence."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Tree-structured in-context reasoning for LLMs",
      "relationship_sentence": "Their internal search formulation of generating a linearized tree and decision is a direct operationalization of ToT\u2019s idea of exploring and evaluating multi-branch reasoning trajectories within the context window."
    },
    {
      "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
      "authors": "Wenlong Huang et al.",
      "year": 2022,
      "role": "LLMs as planners that guide action selection",
      "relationship_sentence": "Using LLMs to propose and evaluate action sequences inspired the external search setting where the model steers rollouts and evaluations, leveraging domain knowledge encoded in the pretrained LLM."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2023,
      "role": "Sampling and selecting among multiple reasoning paths",
      "relationship_sentence": "The paper\u2019s evaluation/selection over candidate branches in internal search is aligned with self-consistency\u2019s principle of generating multiple reasoning paths and choosing the best via downstream checks or value estimates."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014demonstrating strong LLM-based planning in board games via external MCTS guided by an LLM and internal, in-context tree generation\u2014stands at the confluence of neural search in games and structured LLM reasoning. AlphaZero provides the algorithmic backbone: policy/value-guided MCTS as an engine for strong play in perfect\u2011information games. MuZero extends this by removing dependence on an external simulator, directly informing the authors\u2019 external-search claim of conducting rollouts and evaluations without a game engine; here, the LLM plays the role of a learned world model that supplies transition/value signals. Expert Iteration contributes the training paradigm for internalizing search: learning from search traces so a fast model can reproduce search-improved decisions\u2014precisely the spirit of their \u201cinternal search\u201d that trains the LLM to emit a linearized tree and a final move. On the LLM side, Tree of Thoughts supplies the structured, multi-branch reasoning template that the authors operationalize for board-move deliberation within context. Language Models as Zero-Shot Planners motivates using pretrained LMs to guide action selection and evaluation, which underpins the external-search interface where an LLM proposes/evaluates moves. Finally, Self-Consistency informs the generate\u2011and\u2011select mechanism for reasoning paths, aligning with selection among branches in internal search. Together, these works directly enable replacing specialized game networks and engines with a unified LLM that both plans externally via MCTS and internally via learned, linearized search traces.",
  "analysis_timestamp": "2026-01-07T00:04:09.145068"
}
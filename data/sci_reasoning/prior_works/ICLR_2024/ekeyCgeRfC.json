{
  "prior_works": [
    {
      "title": "Language Models are Few-Shot Learners",
      "authors": "Tom B. Brown et al.",
      "year": 2020,
      "arxiv_id": "2005.14165",
      "role": "Foundation",
      "relationship_sentence": "Established the in-context learning phenomenon in pretrained LLMs, motivating this work\u2019s evaluation of Boolean-function prompts to test whether stylized ICL insights transfer to real LLMs."
    },
    {
      "title": "What learning algorithm is in-context learning? Investigations with linear models",
      "authors": "Ekin Aky\u00fcrek et al.",
      "year": 2023,
      "arxiv_id": "2211.15661",
      "role": "Extension",
      "relationship_sentence": "Provided the stylized ICL framework\u2014sequences of input\u2013label pairs followed by a query\u2014and showed ridge/least-squares behavior on linear regression, which this work directly extends from real-valued to Boolean function classes to probe algorithm-learning limits."
    },
    {
      "title": "Transformers learn in-context by gradient descent",
      "authors": "Arthur Jacot von Oswald et al.",
      "year": 2023,
      "arxiv_id": "unknown",
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated that Transformers meta-learn gradient-descent-like procedures on regression tasks, leaving open whether such learned optimizers apply to non-differentiable, discrete learners\u2014precisely the gap this work targets with Boolean functions."
    },
    {
      "title": "Learning Decision Lists",
      "authors": "Ronald L. Rivest",
      "year": 1987,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "Introduced the decision list concept and efficient learning algorithm, supplying a canonical Boolean class and optimal learner that this work uses as a ground-truth baseline in its discrete-function testbed."
    },
    {
      "title": "Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm",
      "authors": "Nick Littlestone",
      "year": 1988,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "Proposed the Winnow algorithm, an optimal learner for sparse monotone conjunctions/disjunctions that this work uses to define \u2018simpler\u2019 Boolean tasks where Transformers can be compared against the known optimum."
    },
    {
      "title": "Failures of Gradient-Based Deep Learning",
      "authors": "Shai Shalev-Shwartz et al.",
      "year": 2017,
      "arxiv_id": "1703.07950",
      "role": "Gap Identification",
      "relationship_sentence": "Showed gradient methods struggle on functions like parity and related Boolean structures, motivating this work\u2019s inclusion of \u2018complex\u2019 Boolean classes to test\u2014and explain\u2014Transformer ICL failure modes."
    }
  ],
  "synthesis_narrative": "Few-shot prompting revealed that large language models can adapt from in-context examples without parameter updates, grounding the modern study of in-context learning (ICL) in pretrained models. Stylized analyses then framed ICL as learning to learn: sequences of input\u2013label pairs followed by a query, where Transformers can meta-learn closed-form or optimizer-like procedures. In particular, work on linear models showed Transformers recover least-squares/ridge regression within the prompt, while companion analyses argued they implement gradient-descent-like updates on synthetic regression tasks. Classical computational learning theory provides the canonical Boolean targets and their optimal learners: decision lists with an efficient learning algorithm, and Winnow\u2019s multiplicative updates for sparse monotone conjunctions/disjunctions. At the same time, theory and practice have long highlighted that gradient-based methods struggle on parity and related Boolean structures, delineating \u2018simple\u2019 versus \u2018complex\u2019 regimes of discrete learning difficulty.\nTogether, these strands raise a natural question: do the optimizer-like ICL mechanisms identified on real-valued regression extend to non-differentiable, discrete algorithms across classic Boolean classes, and do insights from stylized setups carry to pretrained LLMs? Building on the established in-context evaluation protocol, the current work constructs a Boolean-function testbed anchored by the field\u2019s optimal algorithms, probes which classes Transformers can match, where they fail on complex targets like parity, and examines whether such behaviors are unique to attention-based models while assessing transfer to LLMs\u2014thus directly addressing the open gaps left by regression-focused ICL analyses.",
  "target_paper": {
    "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
    "authors": "Satwik Bhattamishra, Arkil Patel, Phil Blunsom, Varun Kanade",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "In-context learning, Transformers, Large language models, Boolean functions",
    "abstract": "In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can match the performance of gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Addit",
    "openreview_id": "ekeyCgeRfC",
    "forum_id": "ekeyCgeRfC"
  },
  "analysis_timestamp": "2026-01-06T06:44:41.331669"
}
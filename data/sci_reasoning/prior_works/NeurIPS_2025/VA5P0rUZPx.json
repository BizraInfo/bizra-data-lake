{
  "prior_works": [
    {
      "title": "Noisy Networks for Exploration",
      "authors": "Meire Fortunato et al.",
      "year": 2018,
      "role": "Learnable exploration noise baseline",
      "relationship_sentence": "LLM-Explorer generalizes the idea of adaptive exploration from parameterized noise in NoisyNet to an LLM-generated, task- and learning-state\u2013aware exploration distribution that can plug into arbitrary RL algorithms."
    },
    {
      "title": "Deep Exploration via Bootstrapped DQN",
      "authors": "Ian Osband et al.",
      "year": 2016,
      "role": "Uncertainty-driven deep exploration",
      "relationship_sentence": "Bootstrapped DQN motivates using an agent\u2019s uncertainty to guide exploration; LLM-Explorer instead infers exploration needs from recent trajectories via LLM reasoning, yielding a similarly targeted but more flexible, task-specific exploration policy."
    },
    {
      "title": "Exploration by Random Network Distillation",
      "authors": "Yuri Burda et al.",
      "year": 2019,
      "role": "Intrinsic-motivation exploration via novelty",
      "relationship_sentence": "While RND shapes behavior through novelty-based intrinsic rewards, LLM-Explorer complements/replaces such heuristics by having an LLM synthesize trajectory cues (progress, stagnation, subgoal structure) into a direct exploration probability distribution."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Michael Ahn et al.",
      "year": 2022,
      "role": "LLM reasoning grounded by learned value/affordance models",
      "relationship_sentence": "SayCan shows that LLM reasoning can be grounded in task feedback to produce actionable guidance; LLM-Explorer similarly conditions LLM analyses on the agent\u2019s learning signals to produce grounded, feasible exploration strategies."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Trajectory-conditioned reasoning-action loop",
      "relationship_sentence": "ReAct demonstrates that LLMs can interleave reasoning with observations to guide next actions; LLM-Explorer adopts this paradigm by prompting the LLM with recent RL trajectories to reason about learning status and emit exploration distributions."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Self-Reflection",
      "authors": "Noah Shinn et al.",
      "year": 2023,
      "role": "Self-reflective adaptation from past rollouts",
      "relationship_sentence": "Reflexion\u2019s self-evaluation on past attempts directly informs LLM-Explorer\u2019s core loop of analyzing learning progress/failures from trajectories and adjusting exploration intensity and direction over training."
    },
    {
      "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
      "authors": "Guanzhi Wang et al.",
      "year": 2023,
      "role": "LLM-driven open-ended exploration and skill discovery",
      "relationship_sentence": "Voyager evidences that LLMs can autonomously drive exploration using past experience and curricula; LLM-Explorer distills this capability into a modular plug-in that converts ongoing RL experience into adaptive exploration strategies."
    }
  ],
  "synthesis_narrative": "LLM-Explorer addresses a longstanding limitation in RL exploration\u2014task-agnostic, static stochasticity\u2014by replacing fixed noise schedules with an LLM that reads recent trajectories and outputs an adaptive exploration distribution. Classic deep RL works like Noisy Networks and Bootstrapped DQN established that exploration should be informed by the agent\u2019s learning dynamics (through learned noise or uncertainty), and RND showed that exploration can also be driven by novelty signals. Yet these methods remain tied to specific heuristics or objectives and often evolve monotonically (e.g., variance decay), lacking nuanced, task-specific adjustments over training. In parallel, the LLM-agents literature revealed that language models can reason over sequences and environment feedback to guide future behavior. SayCan grounded LLM reasoning with value/affordance estimates, ReAct interleaved reasoning with observations to decide next actions, Reflexion leveraged self-critique on past rollouts to adapt strategy, and Voyager demonstrated open-ended, trajectory-informed exploration and skill acquisition. LLM-Explorer synthesizes these lines: it uses the reasoning and reflective capabilities of LLMs, grounded in the agent\u2019s current learning status, to produce a probability distribution that modulates exploration in a plug-and-play manner across RL algorithms. This yields exploration that is task-specific, learning-aware, and non-rigid, bridging uncertainty/novelty-driven deep RL with trajectory-conditioned LLM guidance.",
  "analysis_timestamp": "2026-01-07T00:05:12.533112"
}
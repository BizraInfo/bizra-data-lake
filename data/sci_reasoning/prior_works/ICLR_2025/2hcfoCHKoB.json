{
  "prior_works": [
    {
      "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
      "authors": "Yue Wang et al.",
      "year": 2023,
      "arxiv_id": "unknown",
      "role": "Extension",
      "relationship_sentence": "DeepRTL directly fine-tunes CodeT5+ and adopts its unified encoder\u2013decoder paradigm to handle bidirectional NL\u2194code tasks, extending it to the Verilog/HDL domain with domain-aligned training signals."
    },
    {
      "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
      "authors": "Yue Wang et al.",
      "year": 2021,
      "arxiv_id": "2109.00859",
      "role": "Inspiration",
      "relationship_sentence": "The identifier-aware, unified pretraining objectives of CodeT5 inspired DeepRTL\u2019s central idea of a single representation model serving both understanding and generation rather than separate pipelines."
    },
    {
      "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
      "authors": "Zhangyin Feng et al.",
      "year": 2020,
      "arxiv_id": "2002.08155",
      "role": "Foundation",
      "relationship_sentence": "CodeBERT\u2019s joint NL\u2013code pretraining and demonstrated utility for code search provide the foundational insight that aligned text\u2013code representations enable retrieval-style understanding, which DeepRTL generalizes to NL\u2013Verilog alignment and embedding-based evaluation."
    },
    {
      "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
      "authors": "Hamel Husain et al.",
      "year": 2019,
      "arxiv_id": "1909.09436",
      "role": "Foundation",
      "relationship_sentence": "CodeSearchNet defined the NL\u2192code retrieval problem and embedding-similarity evaluation protocols that DeepRTL adapts to instantiate and measure Verilog understanding via NL\u2013RTL alignment."
    },
    {
      "title": "VerilogEval: Evaluating Large Language Models on Verilog Code Generation and Debugging",
      "authors": "Unknown et al.",
      "year": 2024,
      "arxiv_id": "unknown",
      "role": "Gap Identification",
      "relationship_sentence": "VerilogEval showed that prior LLM efforts concentrated on generation/debugging with weak NL\u2194Verilog alignment and lacked understanding benchmarks, directly motivating DeepRTL\u2019s unified representation and the first Verilog understanding benchmark."
    },
    {
      "title": "GPTScore: Evaluate as You Grade",
      "authors": "Jiaqi Fu et al.",
      "year": 2023,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "DeepRTL adopts GPTScore\u2019s LLM-as-judge evaluation framework to systematically assess the semantic quality of Verilog understanding and generation beyond surface-level metrics."
    }
  ],
  "synthesis_narrative": "Unified encoder\u2013decoder code LMs established that a single model can support both understanding and generation when trained with appropriately aligned objectives. CodeT5 proposed identifier-aware pretraining and multi-task NL\u2194code directions that make the representation simultaneously useful for summarization, translation, and generation. Building on this, CodeT5+ scaled the paradigm with improved training recipes, demonstrating that one backbone can robustly handle diverse code tasks. Complementing these modeling advances, CodeBERT showed that jointly pretraining on natural language and programming languages yields semantically aligned embeddings effective for code search, while CodeSearchNet formalized NL\u2192code retrieval and embedding-similarity evaluations that operationalize \u201cunderstanding\u201d as alignment between descriptions and implementations. In the HDL space, VerilogEval documented that contemporary LLM efforts largely emphasize Verilog generation and debugging, revealing weak natural language\u2013to\u2013Verilog alignment and the absence of a rigorous understanding benchmark. For evaluation methodology, GPTScore introduced a practical, reliable LLM-as-judge scheme for assessing semantic quality that extends beyond exact-match or lexical metrics.\nTaken together, these works pointed to an opportunity: combine a unified NL\u2194code modeling backbone with explicit alignment signals to bridge understanding and generation for Verilog. DeepRTL materializes this next step by extending CodeT5+ with multi-level NL\u2013Verilog alignment to learn shared representations, instituting the first Verilog understanding benchmark inspired by code search protocols, and employing embedding similarity alongside GPTScore to evaluate semantic fidelity for both understanding and generation.",
  "target_paper": {
    "title": "DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model",
    "authors": "Yi Liu, Changran XU, Yunhao Zhou, Zeju Li, Qiang Xu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Large Language Model, Program Representation Learning, Verilog Understanding and Generation",
    "abstract": "Recent advancements in large language models (LLMs) have shown significant potential for automating hardware description language (HDL) code generation from high-level natural language instructions. While fine-tuning has improved LLMs' performance in hardware design tasks, prior efforts have largely focused on Verilog generation, overlooking the equally critical task of Verilog understanding. Furthermore, existing models suffer from weak alignment between natural language descriptions and Verilog code, hindering the generation of high-quality, synthesizable designs. To address these issues, we present DeepRTL, a unified representation model that excels in both Verilog understanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a comprehensive dataset that aligns Verilog code with rich, multi-level natural language descriptions. \nWe also introduce the first benchmark for Verilog understanding and take the initiative to apply embedding similarity and GPT Score to evaluate th",
    "openreview_id": "2hcfoCHKoB",
    "forum_id": "2hcfoCHKoB"
  },
  "analysis_timestamp": "2026-01-06T16:12:09.802616"
}
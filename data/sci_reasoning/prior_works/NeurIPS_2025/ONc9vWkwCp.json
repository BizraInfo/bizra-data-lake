{
  "prior_works": [
    {
      "title": "Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization",
      "authors": "H. Brendan McMahan",
      "year": 2011,
      "role": "Foundational algorithmic/analytical tool for FTRL with (time-varying) regularization",
      "relationship_sentence": "The paper\u2019s anytime-optimal FTRL construction and its analysis of separable fixed vs time-varying regularizers directly leverage the FTRL\u2013Mirror Descent equivalence and stability machinery established by McMahan."
    },
    {
      "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
      "authors": "John Duchi, Elad Hazan, Yoram Singer",
      "year": 2011,
      "role": "Prototype of adaptive, separable (coordinate-wise) time-varying regularization",
      "relationship_sentence": "AdaGrad exemplifies separable adaptive regularizers and motivates the paper\u2019s focus on whether fixed separable regularizers can be minimax-optimal across regimes\u2014culminating in their impossibility result requiring adaptivity."
    },
    {
      "title": "Online Learning in Banach Spaces: Martingale Type and Mirror Descent",
      "authors": "Alexander Rakhlin, Karthik Sridharan",
      "year": 2013,
      "role": "Geometric/minimax backbone linking \u2113p geometry to regret rates",
      "relationship_sentence": "This work\u2019s Banach-space viewpoint and uniform convexity/smoothness tools underpin the \u2113p-dependent regret scalings and illuminate the d versus T regime shift that the paper targets with adaptive FTRL."
    },
    {
      "title": "Interior-Point Methods for Full-Information and Bandit Online Learning",
      "authors": "Jacob Abernethy, Elad Hazan, Alexander Rakhlin",
      "year": 2012,
      "role": "Geometry-aware FTRL via sophisticated regularizers for convex sets (including \u2113p-balls)",
      "relationship_sentence": "By showing how set-dependent regularizers yield near-minimax bounds, this work motivates the paper\u2019s design of regime-adaptive regularization on \u2113p-balls and frames why a single fixed regularizer can be suboptimal."
    },
    {
      "title": "Scale-free Online Learning",
      "authors": "Francesco Orabona, David P\u00e1l",
      "year": 2015,
      "role": "Anytime/parameter-free paradigm via time-varying potentials",
      "relationship_sentence": "The scale-free, parameter-free methodology informs the paper\u2019s goal of anytime optimality; the present work sharpens this by proving that, for separable regularizers on \u2113p-balls, adaptivity is not just helpful but necessary."
    },
    {
      "title": "Black-box Reductions for Parameter-free Online Learning",
      "authors": "Ashok Cutkosky",
      "year": 2018,
      "role": "Techniques to achieve parameter-free (anytime) guarantees through adaptive regularization",
      "relationship_sentence": "Provides generic adaptive reductions that the paper\u2019s positive result (anytime optimal FTRL via time-varying regularizers) conceptually aligns with, while its negative result delineates limits of fixed separable regularizers."
    },
    {
      "title": "Stochastic Linear Optimization under Bandit Feedback",
      "authors": "Varun Dani, Thomas P. Hayes, Sham M. Kakade",
      "year": 2008,
      "role": "Dimension-driven lower bounds for linear bandits",
      "relationship_sentence": "The classical d\u221aT hardness for linear bandits seeds the paper\u2019s final contribution, which generalizes dimension-driven impossibility by ruling out sublinear regret in sufficiently high dimension for \u2113p-balls (p \u2265 1)."
    }
  ],
  "synthesis_narrative": "The paper addresses optimal online convex optimization on \u2113p-balls for p > 2, identifying a fundamental regime shift between d > T and d \u2264 T and showing that FTRL must adapt its regularization to be anytime optimal. Its algorithmic backbone draws on the FTRL\u2013Mirror Descent equivalence (McMahan), which clarifies how the choice and scheduling of regularizers govern stability and regret. Geometry is central: Rakhlin\u2013Sridharan\u2019s Banach-space framework connects \u2113p uniform convexity/smoothness to minimax regret scalings, exposing how the dimension\u2013horizon interplay differs across regimes. Abernethy\u2013Hazan\u2013Rakhlin\u2019s interior-point perspective demonstrates that carefully tailored regularizers tied to the feasible set\u2019s geometry can achieve near-minimax bounds, motivating a regime-aware (time-varying) regularization strategy on \u2113p-balls.\n\nOn the adaptivity axis, AdaGrad (Duchi\u2013Hazan\u2013Singer) is the archetype of separable, time-varying regularization; Orabona\u2013P\u00e1l and Cutkosky provide parameter-free, scale-free techniques that obtain anytime guarantees via adaptive potentials. The present work sharpens these insights by proving a necessity result: for separable regularizers, any fixed choice is inherently suboptimal in one of the two dimension regimes, thereby establishing that adaptivity is not merely beneficial but required for anytime optimality on \u2113p-balls.\n\nFinally, the paper\u2019s lower bounds for linear bandits in high dimension extend the dimension-driven hardness narrative familiar from Dani\u2013Hayes\u2013Kakade\u2019s d\u221aT barriers. It shows that when d is sufficiently large relative to T, sublinear regret is impossible across all \u2113p-balls (p \u2265 1), unifying and strengthening the dimensional limitations of bandit feedback.",
  "analysis_timestamp": "2026-01-07T00:21:32.232350"
}
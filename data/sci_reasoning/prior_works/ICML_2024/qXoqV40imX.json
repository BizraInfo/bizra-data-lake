{
  "prior_works": [
    {
      "title": "On the Number of Linear Regions of Deep Neural Networks",
      "authors": "Guillermo Mont\u00fafar et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "This work established the piecewise-linear, polyhedral partitioning view of ReLU networks; the present paper directly builds on this polytope-of-regions perspective by shifting the focus from network-induced regions to dataset polytope structure to derive dataset-specific width bounds."
    },
    {
      "title": "The Geometry of Deep Networks: Power of ReLU Networks and Max-Affine Spline Operators",
      "authors": "Randall Balestriero et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "By formalizing ReLU networks as max-affine spline operators that induce polyhedral partitions and triangulations, this paper provides the geometric machinery the authors leverage to relate simplicial complexes/polytopes of datasets to necessary network width and to design the converse algorithm that reads polytopes from trained networks."
    },
    {
      "title": "The Expressive Power of Neural Networks: A View from the Width",
      "authors": "Zhou Lu et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "This work gives dimension-dependent universal approximation bounds on width, which the current paper explicitly refines by replacing coarse, input-dimension\u2013only guarantees with tight upper/lower width bounds that depend on the dataset\u2019s polytope complexity."
    },
    {
      "title": "Universal Approximation with Deep Narrow Networks",
      "authors": "Boris Hanin et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "Hanin and Sellke established minimal width conditions (e.g., n_in+1 sufficiency/necessity) independent of data geometry; the present work addresses this limitation by proving width bounds that scale with the number of polytopes/faces needed to encapsulate the dataset."
    },
    {
      "title": "Bounding and Counting Linear Regions of Deep Neural Networks",
      "authors": "Thiago Serra et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Their MILP-based and polyhedral techniques for enumerating linear regions directly inform the paper\u2019s converse algorithm, which extracts the dataset\u2019s polytope structure from a trained network by analyzing its induced regions."
    },
    {
      "title": "Benefits of Depth in Neural Networks",
      "authors": "Matus Telgarsky",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "Telgarsky\u2019s depth-separation results tie expressivity to the creation of many linear regions; this insight inspires the paper\u2019s translation of required region/face counts (from dataset polytopes) into concrete lower and upper bounds on the width needed for classification."
    },
    {
      "title": "On the Expressive Power of Deep Neural Networks",
      "authors": "Maithra Raghu et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "By connecting depth to region transitions and trajectory complexity, this work supports the paper\u2019s choice of polytope/region complexity as the operative measure, informing the dataset-centric expressivity bounds and their dependence on geometric complexity."
    }
  ],
  "synthesis_narrative": "The core contribution of this paper is to tie the minimal width of a neural network to the geometric complexity of the dataset, measured via polytopes and simplicial complexes, and to solve the converse problem of inferring dataset polytopes from trained models. This builds squarely on the polyhedral view of ReLU networks inaugurated by Mont\u00fafar et al., who showed that deep networks implement piecewise-linear partitions into polytopes, and Balestriero and Baraniuk, who formalized ReLU networks as max-affine spline operators, linking them to triangulations and polyhedral geometry. These foundations enable the authors\u2019 shift from network-centric region counting to dataset-centric polytope structure, which underlies their new upper and lower width bounds. Prior width results by Lu et al. and Hanin & Sellke provided dimension-based universal approximation thresholds but did not account for dataset geometry; the present work identifies and closes this gap by proving bounds that scale with the number and complexity (faces) of the polytopes needed to represent the data. Telgarsky\u2019s depth-separation results supply the expressivity lens connecting geometric complexity to required architectural resources, motivating the translation from required region/face counts to concrete width requirements. Finally, techniques for enumerating and reasoning about linear regions from Serra et al. are adapted in the paper\u2019s converse algorithm, which recovers the dataset\u2019s polytope structure from trained networks and empirically shows that datasets like MNIST and CIFAR10 are captured by a small number of low-face polytopes. Together, these works directly shape the paper\u2019s dataset-geometry\u2013driven theory and algorithms.",
  "analysis_timestamp": "2026-01-06T23:09:26.434391"
}
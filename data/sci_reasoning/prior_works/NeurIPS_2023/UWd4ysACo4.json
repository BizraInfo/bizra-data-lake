{
  "prior_works": [
    {
      "title": "SignNet and BasisNet: Consistent Graph Representation Learning From Laplacian Eigenvectors",
      "authors": "Derek Lim, Joshua Robinson, et al.",
      "year": 2021,
      "role": "Immediate spectral-ML precursor on handling eigenvector ambiguity",
      "relationship_sentence": "Introduced architectures that enforce sign invariance (and basis invariance) for Laplacian eigenvectors; the present paper identifies theoretical limits of this invariance and replaces it with principled sign equivariance, extending and correcting this line."
    },
    {
      "title": "Benchmarking Graph Neural Networks",
      "authors": "Vijay Prakash Dwivedi, Chaitanya K. Joshi, Thomas Laurent, Yoshua Bengio, Xavier Bresson",
      "year": 2021,
      "role": "Popularized Laplacian positional encodings (LapPE) and common sign-handling heuristics",
      "relationship_sentence": "Established LapPE as a standard and used sign randomization/absolute-value tricks; this paper provides a theory-backed alternative\u2014sign equivariant networks\u2014that overcomes the shortcomings of such sign-invariant heuristics, especially for link prediction."
    },
    {
      "title": "Directional Graph Networks",
      "authors": "Filippo Beaini, et al.",
      "year": 2021,
      "role": "Spectral-directional message passing relying on eigenvectors with ad-hoc sign handling",
      "relationship_sentence": "Used Laplacian eigenvectors to define directional filters while coping with sign ambiguity via invariance/augmentation; the current work formalizes why direction-sensitive tasks benefit from sign equivariance and supplies architectures to realize it."
    },
    {
      "title": "Invariant and Equivariant Graph Networks",
      "authors": "Haggai Maron, Heli Ben-Hamu, Nadav Shamir, Yaron Lipman",
      "year": 2019,
      "role": "Foundational polynomial/invariant-theory framework for designing equivariant architectures",
      "relationship_sentence": "Provided the polynomial characterization and universality blueprint for invariant/equivariant networks; the present paper adapts this lens to characterize sign-equivariant polynomials and derive expressive sign-equivariant layers."
    },
    {
      "title": "E(n) Equivariant Graph Neural Networks",
      "authors": "Victor Garcia Satorras, Emiel Hoogeboom, Max Welling",
      "year": 2021,
      "role": "Motivating goal of orthogonal/Euclidean equivariance in geometric learning",
      "relationship_sentence": "Showed the practical value of O(d)/E(n)-equivariance; this paper demonstrates that when models rely on eigenvector coordinates, achieving such orthogonal equivariance requires handling sign symmetries equivariantly, and provides the necessary constructions."
    },
    {
      "title": "Tensor Field Networks: Rotation- and Translation-Equivariant Neural Networks for 3D Point Clouds",
      "authors": "Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, Patrick Riley",
      "year": 2018,
      "role": "Template for building equivariant networks via algebraic constraints on features",
      "relationship_sentence": "Inspired the paradigm of constructing networks from analytically characterized equivariant components; analogously, this paper builds layers from the analytic structure of sign-equivariant polynomials to guarantee symmetry and expressiveness."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014architectures that are provably expressive and sign equivariant for eigenvector-based inputs\u2014emerges from reconciling spectral practice with principled symmetry design. Prior spectral methods like Laplacian positional encodings (Benchmarking GNNs) and directional filters (Directional Graph Networks) embraced eigenvectors but treated their intrinsic sign ambiguity with sign-invariant heuristics (random flips, absolute values). SignNet/BasisNet formalized such invariance for consistent spectral representations, yet implicitly embedded an expressivity ceiling: invariance erases information precisely when tasks demand directionality or orthogonal equivariance.\n\nTo transcend this limit, the authors turn to the invariant-theoretic program that underpins modern equivariant networks. Maron et al.\u2019s characterization of invariant/equivariant polynomials for permutation symmetries provides the methodological scaffold: analyze the symmetry group (here, independent Z2 sign flips per eigenvector) and construct networks from the corresponding equivariant polynomial bases. In parallel, the success of O(d)/E(n)-equivariant architectures (EGNN, Tensor Field Networks) underscores the value of exact symmetry handling and highlights a key obstacle in spectral pipelines: orthogonal equivariance cannot be achieved if eigenvector signs are quotiented out. The present work integrates these threads\u2014replacing sign invariance with sign equivariance, deriving an analytic description of sign-equivariant polynomials, and instantiating layers that retain directional cues\u2014thereby enabling orthogonally equivariant modeling and more informative positional encodings for link prediction, with provable expressiveness and empirical validation.",
  "analysis_timestamp": "2026-01-06T23:42:49.092430"
}
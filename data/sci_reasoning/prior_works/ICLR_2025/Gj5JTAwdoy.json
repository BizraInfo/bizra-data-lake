{
  "prior_works": [
    {
      "title": "Elucidating the Design Space of Diffusion-Based Generative Models",
      "authors": "Tero Karras et al.",
      "year": 2022,
      "arxiv_id": "2206.00364",
      "role": "Foundation",
      "relationship_sentence": "Presto!\u2019s step-distillation objective is derived for and implemented within the EDM sigma-parameterized score formulation, directly leveraging EDM\u2019s noise schedule and score parameterization."
    },
    {
      "title": "DiT: Scalable Diffusion Models with Transformers",
      "authors": "William Peebles et al.",
      "year": 2023,
      "arxiv_id": "2305.14048",
      "role": "Foundation",
      "relationship_sentence": "Presto! targets score-based diffusion transformers in the DiT family, and its layer-distillation improvement is designed to drop/merge DiT blocks while preserving hidden-state statistics."
    },
    {
      "title": "Adversarial Diffusion Distillation",
      "authors": "Axel Sauer et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Presto! adopts the adversarial, teacher-driven distribution-matching paradigm from ADD and adapts it to score-based EDM models and the text-to-music domain to realize GAN-based step distillation."
    },
    {
      "title": "Consistency Models",
      "authors": "Yang Song et al.",
      "year": 2023,
      "arxiv_id": "2303.01469",
      "role": "Baseline",
      "relationship_sentence": "Presto! directly competes with consistency-style step reduction, addressing CM\u2019s quality degradation in few-step regimes by using score-based adversarial distribution matching instead of regression-to-consistency."
    },
    {
      "title": "Progressive Distillation for Fast Sampling of Diffusion Models",
      "authors": "Tim Salimans et al.",
      "year": 2022,
      "arxiv_id": "2202.00512",
      "role": "Baseline",
      "relationship_sentence": "Presto! improves upon progressive step-halving by replacing multi-stage student cascades with a GAN-based distribution matching objective that yields comparable or better quality in far fewer steps."
    },
    {
      "title": "DreamFusion: Text-to-3D Using 2D Diffusion",
      "authors": "Ben Poole et al.",
      "year": 2022,
      "arxiv_id": "2209.14988",
      "role": "Extension",
      "relationship_sentence": "Presto!\u2019s score-based distribution matching distillation builds on the SDS idea of using a teacher diffusion model\u2019s score as a training signal for a generator, while correcting SDS biases via adversarial distribution matching tailored to EDM."
    }
  ],
  "synthesis_narrative": "EDM formalized score parameterization and noise schedules that make score networks stable and well-behaved across continuous noise levels, providing the precise sigma-parameterized scaffold that enables score-based training signals to be computed consistently. DiT established a transformer backbone as an effective score network, with residual block stacks and normalization arrangements that make its depth and hidden-state statistics central to performance. Adversarial Diffusion Distillation introduced the idea of distilling a diffusion teacher into a fast generator via a discriminator that matches the teacher\u2019s sample distribution, turning teacher guidance into a GAN-style objective. Consistency Models showed that self-consistency across time can collapse sampling steps to a few iterations, but often at the cost of fidelity and diversity in challenging modalities. Progressive Distillation reduced steps by recursively teaching a student to take larger jumps, though requiring staged training and still several steps at inference. DreamFusion\u2019s SDS demonstrated how a teacher\u2019s score can drive a separate generator, crystallizing score-based distillation but with known bias and instability issues when used directly.\nTaken together, these works reveal a gap: diffusion step acceleration methods either sacrifice quality or remain multi-step, and none align adversarial distribution matching with EDM\u2019s score parameterization nor address per-step compute in DiT backbones. Presto! fills this by introducing a score-based, GAN-style distribution matching tailored to EDM for few-step text-to-music generation, and by complementing it with a variance-preserving layer distillation that safely drops DiT depth, jointly reducing both steps and cost per step while preserving quality and diversity.",
  "target_paper": {
    "title": "Presto! Distilling Steps and Layers for Accelerating Music Generation",
    "authors": "Zachary Novack, Ge Zhu, Jonah Casebeer, Julian McAuley, Taylor Berg-Kirkpatrick, Nicholas J. Bryan",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "music generation, diffusion distillation, diffusion, diffusion acceleration, text-to-music generation, layer dropping",
    "abstract": "Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for ",
    "openreview_id": "Gj5JTAwdoy",
    "forum_id": "Gj5JTAwdoy"
  },
  "analysis_timestamp": "2026-01-06T19:34:55.098608"
}
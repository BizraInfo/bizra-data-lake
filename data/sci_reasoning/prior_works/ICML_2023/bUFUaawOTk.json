{
  "prior_works": [
    {
      "title": "Online Markov Decision Processes",
      "authors": "Even-Dar et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized the adversarial/online MDP framework and occupancy-measure-based regret notion that Best of Both Worlds Policy Optimization directly builds upon."
    },
    {
      "title": "Online Learning in Markov Decision Processes with Mixing Time",
      "authors": "Zimin et al.",
      "year": 2013,
      "role": "Extension",
      "relationship_sentence": "By applying mirror-descent/FTRL over the occupancy-measure polytope with a relative-entropy regularizer to obtain \u221aT regret, this work provides the policy-optimization template that the current paper modifies by swapping in Tsallis/Shannon regularizers and tailored bonuses/learning rates."
    },
    {
      "title": "Optimistic Policy Optimization with Bandit Feedback",
      "authors": "Shani et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "This is the key policy-optimization baseline achieving near-optimal \u221aT regret in adversarial tabular MDPs; the present paper preserves this worst-case guarantee while adding BoBW adaptivity (polylog(T) in stochastic cases)."
    },
    {
      "title": "A Theory of Regularized Markov Decision Processes",
      "authors": "Geist et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "This work provides the theoretical framework for entropy-regularized MDPs (including Shannon and Tsallis), which the current paper leverages to choose and analyze the specific regularizers that enable BoBW guarantees."
    },
    {
      "title": "Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits",
      "authors": "Zimmert et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrating that Tsallis-entropy mirror descent yields simultaneous optimal stochastic and adversarial regret in bandits directly inspires using Tsallis regularization to obtain BoBW guarantees for policy optimization in MDPs."
    },
    {
      "title": "The Best of Both Worlds: Stochastic and Adversarial Bandits",
      "authors": "Bubeck et al.",
      "year": 2012,
      "role": "Gap Identification",
      "relationship_sentence": "This paper established the BoBW goal in online learning and showed it is achievable in bandits, highlighting the gap that the current paper closes for policy optimization in MDPs."
    },
    {
      "title": "Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization",
      "authors": "Abernethy et al.",
      "year": 2008,
      "role": "Related Problem",
      "relationship_sentence": "Its use of self-concordant/log-barrier regularization in online optimization motivates the log-barrier choice that yields first-order regret for the known-transition case via an occupancy-measure (online linear optimization) reduction."
    }
  ],
  "synthesis_narrative": "Best of Both Worlds Policy Optimization sits at the intersection of online MDPs, policy optimization via mirror descent, and best-of-both-worlds adaptivity. The adversarial MDP formulation and regret notion trace directly to Even-Dar, Kakade, and Mansour (2009). Zimin and Neu (2013) then put this on a policy-optimization footing by applying mirror-descent/FTRL over the occupancy-measure polytope with relative-entropy regularization, delivering \u221aT adversarial guarantees\u2014the structural template this paper modifies. In parallel, Geist, Scherrer, and Pietquin (2019) established a general theory of entropy-regularized MDPs, legitimizing the precise use of Shannon and Tsallis regularizers in control. Recent policy-optimization advances, notably Optimistic Policy Optimization with Bandit Feedback (Shani, Efroni, Mansour, 2020), showed that carefully designed bonuses and updates can achieve near-optimal \u221aT regret in adversarial tabular MDPs; this is the primary baseline whose worst-case performance the present paper preserves. The key innovation\u2014achieving polylog(T) stochastic regret without sacrificing \u221aT adversarial robustness\u2014draws direct inspiration from bandits: Bubeck and Slivkins (2012) articulated the BoBW objective, and Zimmert and Seldin (2019) showed Tsallis-entropy mirror descent attains it, motivating the paper\u2019s Tsallis/Shannon choices and learning-rate design. Finally, for known transitions, the reduction to online linear optimization over occupancy measures makes log-barrier regularization (as pioneered by Abernethy, Hazan, and Rakhlin, 2008) natural, enabling first-order adversarial regret in this structured MDP setting.",
  "analysis_timestamp": "2026-01-06T23:09:26.525353"
}
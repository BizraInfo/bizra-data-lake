{
  "prior_works": [
    {
      "title": "The Next Big Five Inventory (BFI-2): Developing and Assessing a Hierarchical Model With 15 Facets to Enhance Bandwidth, Fidelity, and Predictive Power",
      "authors": "Soto and John",
      "year": 2017,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "PsychoBench adopts the BFI-2 item set and facet-scoring scheme as the backbone for its personality-traits category, using its validated structure and reverse-scored items to quantify LLM personalities."
    },
    {
      "title": "The Moral Foundations Questionnaire: Construct validity, reliability, and generality across cultures",
      "authors": "Graham et al.",
      "year": 2011,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "PsychoBench directly incorporates the MFQ to operationalize moral values within its interpersonal/values assessment, following the MFQ\u2019s factor structure and scoring to enable comparable moral profiling of LLMs."
    },
    {
      "title": "Measuring individual differences in empathy: Evidence for a multidimensional approach",
      "authors": "Davis",
      "year": 1983,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The Interpersonal Reactivity Index (IRI) supplies PsychoBench\u2019s empathy measurement instrument, providing the multidimensional (e.g., perspective-taking, empathic concern) scale design and validated items it administers to LLMs."
    },
    {
      "title": "The efficient assessment of need for cognition",
      "authors": "Cacioppo, Petty, and Kao",
      "year": 1984,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "PsychoBench uses the Need for Cognition scale to populate its motivational tests category, inheriting its concise item pool and scoring protocol to assess models\u2019 preference for effortful cognition."
    },
    {
      "title": "The twenty-item Toronto Alexithymia Scale\u2014I. Item selection and cross-validation of the factor structure",
      "authors": "Bagby, Parker, and Taylor",
      "year": 1994,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "For emotional abilities, PsychoBench leverages TAS-20 to probe alexithymia-related dimensions, directly adopting its item wording and factor-based scoring to quantify affect identification in LLM responses."
    },
    {
      "title": "Theory of Mind May Have Spontaneously Emerged in Large Language Models",
      "authors": "Kosinski",
      "year": 2023,
      "arxiv_id": "2302.02083",
      "role": "Inspiration",
      "relationship_sentence": "By showing that human psychological tests can be posed to LLMs and yield interpretable scores, this work inspired PsychoBench\u2019s broader, standardized psychometric framing across multiple constructs."
    },
    {
      "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
      "authors": "Ullman",
      "year": 2023,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "This critique of fragility and cue-sensitivity in LLM psychological testing directly motivated PsychoBench\u2019s rigorous administration (e.g., balanced/reverse-coded items) and robustness checks across prompts."
    }
  ],
  "synthesis_narrative": "Psychometrics offers validated instruments for quantifying human psychology that are directly portable to text-based assessment. The BFI-2 defines a hierarchical Big Five structure with facet-level scoring and reverse-worded items, enabling reliable measurement of personality traits. The Moral Foundations Questionnaire operationalizes moral values along theoretically grounded dimensions with established factor structure and scoring. The Interpersonal Reactivity Index captures empathy as a multidimensional construct, separating perspective-taking and empathic concern through specific item clusters. The Need for Cognition scale provides a concise, validated index of motivation for effortful thinking, while the Toronto Alexithymia Scale furnishes a standardized measure of difficulties in identifying and describing feelings. In parallel, recent LLM studies demonstrated both the feasibility and pitfalls of applying human psychological tests to models: claims of emergent theory of mind illustrated that psychometric-style probes can produce interpretable outputs, whereas follow-up critiques revealed high sensitivity to superficial cues and prompt wording. Together, these works exposed an opportunity: combine the rigor of established psychometric scales with careful, robustness-aware test administration to evaluate multiple psychological dimensions in LLMs. Synthesizing these insights, the present work aggregates diverse, validated scales into a coherent benchmark, maps them into complementary categories (traits, interpersonal, motivation, emotional abilities), and institutionalizes administration and scoring practices designed to mitigate cue sensitivity and response-style artifacts. This was a natural next step to produce standardized, comparable, and psychometrically principled evaluations of the \u201chumanity\u201d portrayed by conversational AI.",
  "target_paper": {
    "title": "On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs",
    "authors": "Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho LAM, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "LLM, Benchmark, Evaluation, Psychometrics",
    "abstract": "Large Language Models (LLMs) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. LLMs become more than mere applications, evolving into assistants capable of addressing diverse user requests. This narrows the distinction between human beings and artificial intelligence agents, raising intriguing questions regarding the potential manifestation of personalities, temperaments, and emotions within LLMs. In this paper, we propose a framework, PsychoBench, for evaluating diverse psychological aspects of LLMs. Comprising thirteen scales commonly used in clinical psychology, PsychoBench further classifies these scales into four distinct categories: personality traits, interpersonal relationships, motivational tests, and emotional abilities. Our study examines five popular models, namely text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b. Additi",
    "openreview_id": "H3UayAQWoE",
    "forum_id": "H3UayAQWoE"
  },
  "analysis_timestamp": "2026-01-06T17:56:14.131161"
}
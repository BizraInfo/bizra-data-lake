{
  "prior_works": [
    {
      "title": "On model selection consistency of Lasso",
      "authors": "Peng Zhao, Bin Yu",
      "year": 2006,
      "role": "Identified Lasso\u2019s failure modes under correlation",
      "relationship_sentence": "This paper formalized the irrepresentable condition, showing that even a single strong (approximate) linear dependency among covariates can cause Lasso to fail at support recovery, directly motivating the need for procedures that adapt to sparse dependencies as done here."
    },
    {
      "title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using \u21131-constrained quadratic programming (Lasso)",
      "authors": "Martin J. Wainwright",
      "year": 2009,
      "role": "Baseline information-theoretic/sample complexity benchmark for Lasso",
      "relationship_sentence": "Wainwright established sharp sample thresholds (\u2248 t log n) for support recovery and clarified when Lasso is optimal, providing the benchmark the present work aims to approach despite correlated designs."
    },
    {
      "title": "Simultaneous analysis of Lasso and Dantzig selector",
      "authors": "Peter J. Bickel, Ya\u2019acov Ritov, Alexandre B. Tsybakov",
      "year": 2009,
      "role": "Introduced Restricted Eigenvalue (RE) framework",
      "relationship_sentence": "By tying Lasso performance to RE-type conditions of \u03a3, this work explains why sample complexity degrades with condition number and motivates the paper\u2019s core idea of feature adaptation to improve RE constants."
    },
    {
      "title": "Restricted eigenvalue properties for correlated Gaussian designs",
      "authors": "Garvesh Raskutti, Martin J. Wainwright, Bin Yu",
      "year": 2010,
      "role": "Characterized RE in the N(0, \u03a3) design",
      "relationship_sentence": "Their analysis of correlated Gaussian designs makes explicit how \u03a3\u2019s spectrum controls Lasso guarantees, directly informing the present paper\u2019s strategy to neutralize a few harmful (approximately dependent) directions of \u03a3."
    },
    {
      "title": "A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers",
      "authors": "Sahand N. Negahban, Pradeep Ravikumar, Martin J. Wainwright, Bin Yu",
      "year": 2012,
      "role": "General RSC/RSM framework underlying convex estimators",
      "relationship_sentence": "This framework formalizes how design-dependent curvature (RSC) governs error rates, and the new algorithm can be viewed as a principled preprocessing that restores strong RSC for sparsity sets despite a few bad directions."
    },
    {
      "title": "Preconditioning to Improve Sparse Linear Regression",
      "authors": "Jiashun Jia, Karl Rohe",
      "year": 2015,
      "role": "Algorithmic precursor: preconditioning Lasso for correlation robustness",
      "relationship_sentence": "Jia\u2013Rohe showed that left preconditioning can make Lasso more stable under correlated designs; the present work advances this idea by constructing a \u03a3-aware feature adaptation that specifically targets a small number of approximate dependencies and yields near-optimal sample complexity."
    },
    {
      "title": "On the distribution of the largest eigenvalue in principal components analysis (spiked covariance model)",
      "authors": "Iain M. Johnstone",
      "year": 2001,
      "role": "Structural prior: few outlier eigenvalues (spiked covariance)",
      "relationship_sentence": "The spiked covariance perspective motivates the assumption of a small number of outlier eigenvalues in \u03a3 that the algorithm isolates and adapts to, enabling guarantees close to the t log n information-theoretic limit."
    }
  ],
  "synthesis_narrative": "The paper targets the core weakness of Lasso in correlated Gaussian designs: even a single (approximate) sparse dependency among covariates can collapse the conditions needed for good recovery. Zhao and Yu\u2019s irrepresentable condition and Wainwright\u2019s sharp thresholds clarify both the failure mode and the information-theoretic target (\u2248 t log n samples). Bickel\u2013Ritov\u2013Tsybakov and Negahban\u2013Ravikumar\u2013Wainwright\u2013Yu establish that Lasso\u2019s accuracy is controlled by Restricted Eigenvalue/Strong Convexity properties that are functions of the covariance \u03a3; poor RE constants, often tied to \u03a3\u2019s condition number, drive the sample complexity gap. Raskutti\u2013Wainwright\u2013Yu sharpen this insight in the exact N(0, \u03a3) setting, quantifying how spectrum and correlations dictate RE, thus pinpointing which directions in \u03a3 are harmful. On the algorithmic side, Jia\u2013Rohe demonstrate that carefully chosen preconditioning can repair Lasso\u2019s design-dependent conditions, suggesting that modifying the feature space\u2014not the penalty\u2014can restore good curvature. The present work synthesizes these threads by designing a \u03a3-aware feature adaptation that identifies and neutralizes a small set of approximately dependent directions\u2014precisely the ones that devastate RE\u2014while leaving the bulk geometry intact. Leveraging a spiked-covariance viewpoint (Johnstone), it proves near-optimal sample complexity when \u03a3 has only a few outlier eigenvalues, thereby closing most of the statistical\u2013computational gap for constant sparsity with a polynomial-time procedure that effectively \u201crepairs\u201d Lasso\u2019s conditions.",
  "analysis_timestamp": "2026-01-06T23:42:49.113112"
}
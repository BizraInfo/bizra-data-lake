{
  "prior_works": [
    {
      "title": "Deep Canonical Correlation Analysis",
      "authors": "Galen Andrew et al.",
      "year": 2013,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work formalized maximizing canonical correlations between nonlinear projections as a learning objective, which the present paper generalizes from Euclidean correlation to an intrinsic geodesic correlation tailored to SPD-valued covariance data."
    },
    {
      "title": "Deep Generalized Canonical Correlation Analysis",
      "authors": "Adrian Benton et al.",
      "year": 2017,
      "arxiv_id": "1702.02519",
      "role": "Baseline",
      "relationship_sentence": "As the standard deep multiview extension of CCA, this method provides the primary Euclidean-space baseline that the paper extends by replacing Euclidean correlation with a geodesic correlation on the SPD manifold for multi-view neuroimaging covariances."
    },
    {
      "title": "Deep Canonically Correlated Autoencoders",
      "authors": "Weiran Wang et al.",
      "year": 2015,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By coupling a CCA-style alignment loss with self-supervised reconstruction, this paper established correlation consistency as a powerful self-supervision signal that directly motivates adopting a CCA-derived (here, geodesic) correlation for multi-view covariance data."
    },
    {
      "title": "A Riemannian framework for tensor computing",
      "authors": "Xavier Pennec et al.",
      "year": 2006,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This paper introduced the affine-invariant Riemannian metric and geodesic structure on SPD manifolds, providing the mathematical basis for defining and computing the proposed geodesic correlation between covariance representations."
    },
    {
      "title": "Log-Euclidean metrics for fast and simple calculus on diffusion tensors",
      "authors": "Vincent Arsigny et al.",
      "year": 2006,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "The common practice of flattening SPD data via log-Euclidean/tangent-space mapping highlighted here motivates the present work\u2019s intrinsic geodesic formulation by exposing the distortion and locality limits of Euclideanized correlation on manifold-valued covariances."
    },
    {
      "title": "A Riemannian Network for SPD Matrices",
      "authors": "Zhiwu Huang and Luc Van Gool",
      "year": 2017,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By showing that deep models can operate natively on SPD manifolds with geometry-preserving layers, this work inspired the paper\u2019s geometry-aware treatment of covariance data and the need for manifold-consistent alignment objectives."
    },
    {
      "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
      "authors": "Jure Zbontar et al.",
      "year": 2021,
      "arxiv_id": "2103.03230",
      "role": "Gap Identification",
      "relationship_sentence": "This method popularized correlation-based alignment for self-supervision in Euclidean spaces, whose limitation on manifold-valued covariances directly motivates replacing Euclidean cross-correlation with a geodesic correlation consistent with SPD geometry."
    }
  ],
  "synthesis_narrative": "Deep Canonical Correlation Analysis established learning by maximizing canonical correlations between nonlinear projections of paired views, and Deep Generalized CCA extended this to multiple views, cementing correlation consistency as a central multiview objective. Deep Canonically Correlated Autoencoders further showed that CCA-style alignment can serve as a self-supervised signal when combined with representation learning, strengthening the case for correlation-driven objectives beyond supervised settings. Parallelly, the Riemannian framework for tensor computing introduced the affine-invariant geometry and geodesics on the symmetric positive definite manifold, offering intrinsic tools to compare and align covariance representations. Log-Euclidean metrics provided a practical tangent-space alternative but also revealed the drawbacks of flattening SPD data, including distortions and locality, when one needs global, geometry-faithful comparisons. A Riemannian Network for SPD Matrices demonstrated that deep architectures can honor manifold structure end-to-end, underscoring the importance of geometry-aware objectives for SPD inputs. Finally, Barlow Twins highlighted the strength of correlation-based redundancy reduction in self-supervised learning, yet it operates strictly in Euclidean feature spaces. Together these works expose a clear opportunity: combine the proven multiview power of CCA-style correlation with the intrinsic geometry of SPD covariance data. The natural next step is to replace Euclidean correlation with a geodesic, CCA-derived measure that evaluates cross-view consistency directly on the SPD manifold, enabling multi-view self-supervised representation learning that respects covariance geometry without resorting to distorting tangent-space embeddings.",
  "target_paper": {
    "title": "Deep Geodesic Canonical Correlation Analysis for Covariance-Based Neuroimaging Data",
    "authors": "Ce Ju, Reinmar J Kobler, Liyao Tang, Cuntai Guan, Motoaki Kawanabe",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Geometric Deep Learning, Self-Supervised Learning, Brain-Computer Interfaces, Neuroimaging, Neuroscience",
    "abstract": "In human neuroimaging, multi-modal imaging techniques are frequently combined to enhance our comprehension of whole-brain dynamics and improve diagnosis in clinical practice. Modalities like electroencephalography and functional magnetic resonance imaging provide distinct views to the brain dynamics due to diametral spatiotemporal sensitivities and underlying neurophysiological coupling mechanisms. These distinct views pose a considerable challenge to learning a shared representation space, especially when dealing with covariance-based data characterized by their geometric structure. To capitalize on the geometric structure, we introduce a measure called geodesic correlation which expands traditional correlation consistency to covariance-based data on the symmetric positive definite (SPD) manifold. This measure is derived from classical canonical correlation analysis and serves to evaluate the consistency of latent representations obtained from paired views. For multi-view, self-superv",
    "openreview_id": "PnR1MNen7u",
    "forum_id": "PnR1MNen7u"
  },
  "analysis_timestamp": "2026-01-06T12:41:19.567674"
}
{
  "prior_works": [
    {
      "title": "FLAVA: A Foundational Language And Vision Alignment Model",
      "authors": "Amanpreet Singh et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "ProtST directly extends FLAVA\u2019s tri-objective pretraining recipe\u2014unimodal masked modeling, cross-modal contrastive alignment, and multimodal masked prediction\u2014by transplanting it from vision\u2013language to protein\u2013text learning."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "ProtST\u2019s multimodal representation alignment objective is a CLIP-style contrastive loss between protein and text embeddings, directly inspired by CLIP\u2019s image\u2013text alignment via InfoNCE."
    },
    {
      "title": "Align Before Fuse: Vision and Language Representation Learning with Momentum Distillation",
      "authors": "Junnan Li et al.",
      "year": 2021,
      "role": "Related Problem",
      "relationship_sentence": "ProtST\u2019s multimodal masked prediction\u2014predicting masked tokens with cross-modal context\u2014follows the ALBEF paradigm of cross-modal MLM that tightly couples alignment and masked modeling."
    },
    {
      "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
      "authors": "Alexander Rives et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "ESM is a primary PLM baseline capturing co-evolutionary sequence signals, and ProtST is designed to preserve ESM-like sequence modeling while overcoming its inability to explicitly encode protein function by adding text supervision."
    },
    {
      "title": "Evaluating Protein Transfer Learning with TAPE",
      "authors": "Roshan Rao et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "TAPE established masked language modeling for protein sequences and standardized downstream evaluations, which ProtST adopts for its unimodal mask prediction and evaluation setup."
    },
    {
      "title": "ProteinBERT: A universal deep-learning model of protein sequence and function",
      "authors": "Nadav Brandes et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "ProteinBERT\u2019s attempt to inject functional knowledge via GO/keyword supervision highlights the limitation of sequence-only PLMs and structured labels, motivating ProtST\u2019s use of free-form biomedical text and cross-modal alignment."
    },
    {
      "title": "UniProt: the universal protein knowledgebase in 2021",
      "authors": "The UniProt Consortium",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "ProtST\u2019s ProtDescribe dataset is built from UniProt\u2019s curated textual protein descriptions, providing the essential paired sequence\u2013text supervision that enables its multimodal training."
    }
  ],
  "synthesis_narrative": "ProtST\u2019s core innovation\u2014pretraining a protein language model jointly with biomedical texts through three complementary objectives\u2014emerges from marrying two lines of work: protein sequence modeling and vision\u2013language pretraining. On the protein side, TAPE established masked language modeling and evaluation protocols for sequences, and ESM demonstrated that large-scale sequence-only pretraining captures coevolution but fails to explicitly encode function, defining the baseline and the gap ProtST targets. ProteinBERT further underscored this gap by injecting ontology-based functional labels, showing the value of functional supervision while revealing the limitations of relying on structured labels rather than rich natural language. To supply that richer supervision, ProtST leverages UniProt\u2019s curated textual descriptions to construct ProtDescribe, providing the paired sequence\u2013text data required for multimodal learning.\nOn the multimodal learning side, CLIP introduced contrastive alignment between modalities, directly inspiring ProtST\u2019s protein\u2013text representation alignment loss. FLAVA advanced a unifying recipe that combines unimodal masked modeling, cross-modal contrastive alignment, and multimodal masked prediction; ProtST explicitly extends this tri-objective framework to the protein\u2013text setting. Finally, ALBEF\u2019s cross-modal masked language modeling informs ProtST\u2019s multimodal mask prediction, where masked tokens in one modality are recovered using context from the other. Together, these works directly shape ProtST\u2019s objectives, data construction, and motivation, enabling a PLM that preserves sequence-derived signals while explicitly acquiring functional semantics from biomedical text.",
  "analysis_timestamp": "2026-01-06T23:09:26.585381"
}
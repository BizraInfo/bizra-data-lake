{
  "prior_works": [
    {
      "title": "ACT-1: A Large-Scale Transformer for General Computer Use via Human Demonstrations",
      "authors": "Adept AI Labs et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "OpenCUA adopts the core problem formulation introduced by ACT-1\u2014training agents to operate real software from human screen-and-action demonstrations\u2014and makes this paradigm openly reproducible with a public capture pipeline and dataset across multiple OSes."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "OpenCUA\u2019s demonstration-to-state\u2013action pipeline explicitly encodes long-horizon, interleaved reasoning-and-action traces, directly extending ReAct by adding reflective, long chain-of-thought rationales between UI actions for more robust computer-use behavior."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Self-Reflection",
      "authors": "Noah Shinn et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "The paper\u2019s reflective long Chain-of-Thought component is inspired by Reflexion\u2019s self-critique and improvement loop, operationalizing reflection during trajectory transformation to stabilize and improve agent performance."
    },
    {
      "title": "Mind2Web: Towards a Generalist Agent for the Web",
      "authors": "Yue et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "OpenCUA builds on Mind2Web\u2019s demonstration-driven formulation for web tasks, generalizing the human demo collection and step-level action annotation paradigm beyond browsers to 3 operating systems and 200+ apps/sites."
    },
    {
      "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
      "authors": "Zhou et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "WebArena highlighted the need for realistic, task-diverse web environments; OpenCUA addresses this gap at the OS level by providing an open, cross-application dataset and infra that go beyond browser-only setups."
    },
    {
      "title": "OSWorld: Benchmarking Generalist GUI Agents for Computer Use",
      "authors": "Liu et al.",
      "year": 2024,
      "role": "Gap Identification",
      "relationship_sentence": "OSWorld documented limited generalization and coverage in existing GUI agents and datasets; OpenCUA directly tackles these limitations by scaling annotations and tasks across multiple OSes and substantially more applications."
    },
    {
      "title": "AppAgent: Multimodal Agents as Smartphone Users",
      "authors": "Shao et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "AppAgent showed that VLMs can operate real mobile apps via screenshots and actions; OpenCUA generalizes this multimodal control paradigm to desktop/server OS ecosystems and standardizes open data capture and training."
    }
  ],
  "synthesis_narrative": "OpenCUA\u2019s central contribution\u2014an open, scalable foundation for computer-use agents built from human demonstrations and enhanced with reflective long-horizon reasoning\u2014stands on a clear lineage of ideas. Adept\u2019s ACT\u20111 introduced the core formulation of training agents to use real software from human screen\u2013action demonstrations; OpenCUA makes that paradigm openly reproducible, broadening coverage across operating systems and applications. On the control and reasoning side, ReAct established the value of interleaving thoughts with actions, while Reflexion demonstrated how self-critique can stabilize and improve trajectories; OpenCUA operationalizes both by converting demonstrations into state\u2013action pairs augmented with reflective long Chain-of-Thought, extending ReAct with Reflexion-style feedback to improve robustness over long horizons. In terms of data and environments, Mind2Web pioneered collecting step-level human web demonstrations for generalist agents, a blueprint OpenCUA scales beyond browsers with a unified annotation infrastructure. WebArena underscored the need for realistic, task-diverse interaction settings, which OpenCUA addresses at the OS level rather than only within the browser. Finally, OSWorld systematically exposed the coverage and generalization gaps of contemporary GUI agents, directly motivating OpenCUA\u2019s cross-OS, 200+ app scale-up, while AppAgent\u2019s success on mobile GUIs provided evidence that VLM-based screen understanding can drive real-world app control\u2014a principle OpenCUA extends to broader desktop ecosystems. Together, these works directly inspired and enabled OpenCUA\u2019s open, scalable CUA foundation.",
  "analysis_timestamp": "2026-01-06T23:08:23.934681"
}
{
  "prior_works": [
    {
      "title": "Domain Separation Networks",
      "authors": "Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan",
      "year": 2016,
      "role": "Shared\u2013private decomposition with orthogonality regularization",
      "relationship_sentence": "TDLSR\u2019s decomposition into discriminative shared and view-specific representations with explicit orthogonality draws directly on DSN\u2019s idea of separating common and private subspaces with an orthogonality constraint to prevent information leakage."
    },
    {
      "title": "What makes for good views for contrastive learning? (InfoMin Principle)",
      "authors": "Yonglong Tian, Dilip Krishnan, Phillip Isola",
      "year": 2020,
      "role": "Information-theoretic principle for complementary multi-view representations",
      "relationship_sentence": "The paper\u2019s \u201cinformation shift\u201d and \u201cinteraction\u201d principles are theory-driven echoes of InfoMin, encouraging views to retain task-relevant mutual information while minimizing redundant overlap, thereby enabling semantically distinct yet complementary view representations."
    },
    {
      "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
      "authors": "Jean-Bastien Zbontar, Li Jing, Ishan Misra, Yann LeCun, St\u00e9phane Deny",
      "year": 2021,
      "role": "Decorrelation/orthogonality to reduce redundancy",
      "relationship_sentence": "TDLSR\u2019s orthogonality and redundancy-mitigation objective is inspired by Barlow Twins\u2019 cross-correlation decorrelation idea, operationalized to disentangle label-specific information across views and suppress message distortion."
    },
    {
      "title": "Prototypical Networks for Few-shot Learning",
      "authors": "Jake Snell, Kevin Swersky, Richard S. Zemel",
      "year": 2017,
      "role": "Prototype-based class representatives",
      "relationship_sentence": "The construction of class representatives and the prototype association graph in TDLSR is motivated by prototype-based reasoning, using centroid-like label prototypes as anchors to encode and propagate label semantics."
    },
    {
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "authors": "Thomas N. Kipf, Max Welling",
      "year": 2017,
      "role": "Message passing on graphs to exploit relational structure",
      "relationship_sentence": "TDLSR\u2019s prototype association graph and proximity-aware imputation leverage GCN-style neighborhood aggregation to diffuse information over sample\u2013prototype and sample\u2013sample topologies under missing views/labels."
    },
    {
      "title": "Learning with Local and Global Consistency",
      "authors": "Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, Bernhard Sch\u00f6lkopf",
      "year": 2004,
      "role": "Graph-based propagation over sample topology",
      "relationship_sentence": "The proximity-aware imputation mechanism echoes label propagation over similarity graphs, transferring information along manifold-consistent neighborhoods to infer missing features/labels from view-specific sample topology."
    },
    {
      "title": "DeViSE: A Deep Visual-Semantic Embedding Model",
      "authors": "Andrea Frome, Greg S. Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc\u2019Aurelio Ranzato, Tomas Mikolov",
      "year": 2013,
      "role": "Label semantics guiding feature learning",
      "relationship_sentence": "TDLSR\u2019s label semantic\u2013guided feature learning is grounded in visual\u2013semantic embedding, aligning learned representations with external label semantics to capture label correlations and improve discrimination under sparse annotations."
    }
  ],
  "synthesis_narrative": "TDLSR\u2019s core innovation\u2014label-specific, semantically disentangled multi-view representations robust to missing views and labels\u2014emerges from converging lines of prior work. Its shared\u2013specific factorization with explicit orthogonality inherits from Domain Separation Networks, ensuring that common signal is captured without contaminating view-private factors. The theory-driven principles of information shift and interaction explicitly operationalize the InfoMin view of contrastive learning, preserving task-relevant mutual information while discouraging superfluous overlap. In tandem, redundancy reduction and decorrelation, inspired by Barlow Twins, enforce orthogonality across representation dimensions to mitigate distortion and redundancy.\nTo address incompleteness, TDLSR builds view-specific sample topologies and a prototype association graph, a design that marries classic graph-based semi-supervised learning (Learning with Local and Global Consistency) with modern GCN message passing to propagate information along reliable neighborhoods. Prototypes act as semantic anchors: borrowing from Prototypical Networks, TDLSR derives class representatives and connects them via association edges to structure label-conditioned diffusion and imputation. Finally, by aligning features with label semantics, TDLSR follows the visual\u2013semantic embedding paradigm of DeViSE, leveraging external label embeddings to encode label correlations and guide discriminative feature formation. Collectively, these influences crystallize into a framework that imputes missing information via topology-aware propagation, learns label-specific prototypes capturing correlation semantics, and enforces principled disentanglement across views through information-theoretic and orthogonality constraints.",
  "analysis_timestamp": "2026-01-07T00:21:32.320503"
}
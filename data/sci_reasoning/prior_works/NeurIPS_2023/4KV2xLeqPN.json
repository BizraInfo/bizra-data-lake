{
  "prior_works": [
    {
      "title": "Rates of convergence for minimum contrast estimators",
      "authors": "Lucien Birg\u00e9, Pascal Massart",
      "year": 1993,
      "role": "Established that ERM can be minimax suboptimal in mean squared error for certain classes.",
      "relationship_sentence": "This paper motivates Kur\u2013Putterman\u2013Rakhlin\u2019s central question by providing the canonical suboptimality examples that their work reinterprets as bias-driven while proving the variance term attains minimax rates."
    },
    {
      "title": "A new perspective on least squares under convex constraint",
      "authors": "Sourav Chatterjee",
      "year": 2014,
      "role": "Proved an admissibility theorem for least squares/ERM under fixed design (Theorem 1.4).",
      "relationship_sentence": "The NeurIPS paper gives a simple proof of Chatterjee\u2019s admissibility theorem and extends its scope to random design, making this result a direct cornerstone for their admissibility contribution."
    },
    {
      "title": "Stability and Generalization",
      "authors": "Olivier Bousquet, Andr\u00e9 Elisseeff",
      "year": 2002,
      "role": "Introduced algorithmic stability as a route to generalization bounds and formalized stability notions.",
      "relationship_sentence": "The authors\u2019 stability implications for ERM are interpreted through this framework, leveraging stability\u2013generalization links to translate their variance estimates into stability guarantees."
    },
    {
      "title": "Stability of Empirical Risk Minimization over non-Donsker classes",
      "authors": "Andrea Caponnetto, Alexander Rakhlin",
      "year": 2006,
      "role": "Provided stability results for ERM beyond Donsker settings.",
      "relationship_sentence": "Kur\u2013Putterman\u2013Rakhlin\u2019s variance bounds imply stability of ERM and explicitly complement the non-Donsker stability results of Caponnetto\u2013Rakhlin, extending the picture via their new estimates."
    },
    {
      "title": "Local Rademacher complexities",
      "authors": "Peter L. Bartlett, Olivier Bousquet, Shahar Mendelson",
      "year": 2005,
      "role": "Developed local complexity tools to obtain sharp excess risk rates under squared loss.",
      "relationship_sentence": "The NeurIPS paper\u2019s identification of a minimax-rate variance term aligns with local-complexity control of fluctuations, drawing on this framework to separate variance from bias in ERM analysis."
    },
    {
      "title": "Sharp oracle inequalities for least squares estimators under convex constraint",
      "authors": "Pierre C. Bellec",
      "year": 2018,
      "role": "Provided precise risk bounds for constrained least squares via Gaussian width/complexity.",
      "relationship_sentence": "Techniques and insights about projection-like LSEs and Gaussian width inform the paper\u2019s variance control for ERM in fixed design and facilitate their streamlined admissibility argument."
    },
    {
      "title": "Exponential weights and sharp oracle inequalities for aggregation",
      "authors": "Philippe Rigollet, Alexandre B. Tsybakov",
      "year": 2011,
      "role": "Showed ERM\u2019s suboptimality for model selection aggregation under squared loss and achieved optimal rates via exponential weighting.",
      "relationship_sentence": "By explaining ERM\u2019s suboptimality as arising from bias while showing its variance is minimax-optimal, the NeurIPS paper reconciles these aggregation results with a refined bias\u2013variance perspective."
    }
  ],
  "synthesis_narrative": "The NeurIPS paper tackles a long-standing puzzle: ERM is known to be minimax suboptimal in mean squared error in several settings, yet it often performs competitively. Birg\u00e9 and Massart\u2019s foundational examples and the aggregation literature (e.g., Rigollet\u2013Tsybakov) crystallized this suboptimality, but did not isolate whether variance or bias is to blame. The authors\u2019 key advance is to show that, under mild assumptions, ERM\u2019s variance term actually attains the minimax rate; suboptimality comes from bias.\nTwo strands of prior work directly shape their argument. First, Chatterjee\u2019s admissibility theorem for least squares under fixed design established that ERM cannot be uniformly dominated; the present paper supplies a simple proof and extends admissibility to random design, reinforcing ERM\u2019s fundamental optimality properties. Second, stability theory\u2014initiated by Bousquet\u2013Elisseeff and extended by Caponnetto\u2013Rakhlin to non-Donsker classes\u2014provides the lens through which their new variance bounds translate into algorithmic stability, thereby complementing and broadening earlier stability guarantees.\nTechnically, local complexity methods (Bartlett\u2013Bousquet\u2013Mendelson) and sharp analyses of constrained least squares via Gaussian width (Bellec) underpin the decomposition that disentangles variance from bias and delivers minimax-rate variance control in both fixed and random design. Together, these works inform the paper\u2019s unifying explanation: ERM\u2019s deficiencies in MSE trace to bias, while its variance is inherently optimal, reconciling suboptimality results with admissibility and stability insights.",
  "analysis_timestamp": "2026-01-06T23:33:36.296315"
}
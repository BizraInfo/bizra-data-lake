{
  "prior_works": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "authors": "Kerbl et al.",
      "year": 2023,
      "arxiv_id": "2308.04079",
      "role": "Baseline",
      "relationship_sentence": "This work establishes the 3DGS representation and single-view training pipeline that the new system directly scales by sharding Gaussian parameters, enabling multi-view batched training and distributed optimization."
    },
    {
      "title": "Mega-NeRF: Scalable Construction of Large-Scale NeRFs",
      "authors": "Turki et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "By partitioning large scenes and distributing radiance-field training across devices, this paper crystallizes the \u2018scale-by-spatial-sharding\u2019 problem formulation that motivates distributing 3DGS parameters rather than confining training to one GPU."
    },
    {
      "title": "Block-NeRF: Scalable Large Scene Neural View Synthesis",
      "authors": "Tancik et al.",
      "year": 2022,
      "arxiv_id": "2202.05263",
      "role": "Gap Identification",
      "relationship_sentence": "Its block-wise scene partitioning highlights load-imbalance and cross-partition interaction challenges that the new system addresses via dynamic load balancing and sparse all-to-all routing of Gaussians to pixel partitions."
    },
    {
      "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
      "authors": "Lepikhin et al.",
      "year": 2021,
      "arxiv_id": "2006.16668",
      "role": "Inspiration",
      "relationship_sentence": "GShard\u2019s sparse all-to-all token routing and load-balancing mechanisms directly inspire routing only the necessary parameters (Gaussians) to pixel shards with dynamic balancing across GPUs."
    },
    {
      "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
      "authors": "Rajbhandari et al.",
      "year": 2020,
      "arxiv_id": "1910.02054",
      "role": "Inspiration",
      "relationship_sentence": "ZeRO\u2019s idea of partitioning model/optimizer states across devices informs the sharding of millions of 3DGS parameters and the communication schedule for distributed updates."
    },
    {
      "title": "A Sorting Classification of Parallel Rendering",
      "authors": "Molnar et al.",
      "year": 1994,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The sort-first paradigm of partitioning the image plane and routing primitives to tiles underpins the decision to partition pixels and send only overlapping Gaussians to those partitions."
    },
    {
      "title": "Measuring the Effects of Data Parallelism on Neural Network Training",
      "authors": "Shallue et al.",
      "year": 2018,
      "arxiv_id": "1811.03600",
      "role": "Inspiration",
      "relationship_sentence": "Its analysis of optimal learning-rate/batch-size scaling motivates exploring batch-dependent hyperparameter scaling, leading to the adoption of a simple sqrt(batch-size) rule for batched multi-view training."
    }
  ],
  "synthesis_narrative": "Gaussian Splatting introduced a differentiable splat-based representation and a single-view training loop that couples millions of per-Gaussian parameters to rendered pixels, delivering real-time rendering but constraining training to a single GPU. Mega-NeRF presented a scene-partitioned, multi-GPU recipe for scaling radiance fields, showing that spatial sharding and distributed training can unlock city-scale reconstructions. Block-NeRF refined this idea with block-wise decomposition, revealing practical issues of load imbalance and cross-block interactions when rendering rays traverse partitions. In parallel, GShard showed that sparse all-to-all communication can efficiently route only the necessary tokens to experts while maintaining balance via explicit load objectives, and ZeRO demonstrated that partitioning model/optimizer states across devices is a pragmatic path to fit and train models far beyond single-GPU memory. Long before, Molnar\u2019s taxonomy of parallel rendering codified the sort-first strategy of partitioning the image plane and routing primitives to tiles, a natural match to splatting-based rasterization. Complementing these system ideas, Shallue et al. characterized how optimal hyperparameters depend on batch size, motivating principled scaling of learning rates and related settings.\nTogether, these works reveal a path: treat Gaussians as sparsely routed parameters, tile the image plane, and shard states so training scales while staying memory-efficient. The synthesis is to marry sort-first pixel partitioning with MoE-style sparse all-to-all routing and ZeRO-like sharding, then exploit multi-view batching guided by batch-size scaling insights\u2014yielding distributed, load-balanced 3DGS training that transcends single-view, single-GPU limitations.",
  "target_paper": {
    "title": "On Scaling Up 3D Gaussian Splatting Training",
    "authors": "Hexu Zhao, Haoyang Weng, Daohan Lu, Ang Li, Jinyang Li, Aurojit Panda, Saining Xie",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Gaussian Splatting, Machine Learning System, Distributed Training",
    "abstract": "3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch-size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances re",
    "openreview_id": "pQqeQpMkE7",
    "forum_id": "pQqeQpMkE7"
  },
  "analysis_timestamp": "2026-01-06T15:54:53.290751"
}
{
  "prior_works": [
    {
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": "James Kirkpatrick et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "EWC showed that constraining updates along high-curvature (Fisher/Hessian) directions can selectively impede learning new tasks; this paper repurposes that curvature-control idea to pre-training, explicitly shaping the Hessian spectrum so later fine-tuning on harmful tasks is ill-conditioned while benign tasks remain learnable."
    },
    {
      "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization",
      "authors": "Pierre Foret et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "SAM operationalized curvature-aware regularization via local sharpness, and the present work directly extends this notion by introducing regularizers that target both the largest and smallest Hessian eigenvalues to control condition numbers\u2014explicitly using curvature shaping as the mechanism for immunization."
    },
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "LoRA formalized low-rank fine-tuning as the prevalent mechanism for rapid downstream adaptation; this paper\u2019s immunization goal is to make such downstream fine-tunes (including LoRA-style updates) ineffective along harmful-task subspaces by manipulating the Hessian condition number during pre-training."
    },
    {
      "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
      "authors": "Nataniel Ruiz et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "DreamBooth exemplifies practical, highly effective fine-tuning in text-to-image models; the proposed immunization framework is evaluated in exactly this regime and is designed to hinder such post-hoc adaptations on harmful targets while preserving non-harmful utility."
    },
    {
      "title": "Editing Models with Task Arithmetic",
      "authors": "Gabriel Ilharco et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "Task-vector arithmetic frames downstream behaviors as movement along low-dimensional task directions; this paper\u2019s condition-number perspective explicitly exploits that subspace view to make movement along harmful task directions poorly conditioned after pre-training."
    },
    {
      "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
      "authors": "Armen Aghajanyan et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "By showing that effective fine-tuning occurs in low-dimensional subspaces, this work motivates the present paper\u2019s strategy to shape the Hessian spectrum specifically in those subspaces so that harmful adaptations become optimization-wise difficult while leaving other directions accessible."
    },
    {
      "title": "Benign Overfitting in Linear Regression",
      "authors": "Peter L. Bartlett et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "This paper tied generalization and optimization in linear models to eigenstructure of the data/Hessian; the current work builds directly on linear-model spectral analysis, using condition numbers of Hessians to formalize when immunization is possible and to design targeted regularization."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014defining and achieving model immunization by shaping Hessian condition numbers\u2014emerges from uniting curvature-based optimization control with a subspace view of downstream adaptation. EWC established that constraining updates along high-curvature directions can selectively prevent unwanted parameter movement; we adopt that principle preemptively in pre-training, but strengthen it by explicitly controlling the Hessian spectrum, not just parameter deviations. SAM further demonstrated that curvature-aware regularization is practical and effective; we extend that idea from sharpness to condition numbers, jointly targeting the largest and smallest eigenvalues to make harmful-task subspaces ill-conditioned while keeping benign subspaces well-conditioned. The subspace perspective is grounded by Task Arithmetic and by intrinsic dimensionality results, which show downstream edits and fine-tunes predominantly occur along low-dimensional task vectors; our analysis and algorithm directly act on those subspaces via spectral control. On the application side, LoRA and DreamBooth represent the concrete fine-tuning mechanisms and evaluation regimes that immunization must withstand\u2014our method is designed to specifically hinder such rapid, low-rank adaptations on harmful tasks without collapsing useful capabilities. Finally, the linear-model spectral framework of Bartlett et al. provides the mathematical backbone: it connects optimization behavior to Hessian eigenspectra, enabling us to derive clear immunization conditions and translate them into a principled regularization scheme for pre-training.",
  "analysis_timestamp": "2026-01-06T23:07:19.620826"
}
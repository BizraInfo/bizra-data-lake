{
  "prior_works": [
    {
      "title": "Deep Sets",
      "authors": "Zaheer et al.",
      "year": 2017,
      "arxiv_id": "1703.06114",
      "role": "Foundation",
      "relationship_sentence": "Deep Sets formalized permutation-invariant set processing, which PopT uses to aggregate per-channel temporal embeddings when the number and ordering of electrodes vary across subjects."
    },
    {
      "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
      "authors": "Lee et al.",
      "year": 2019,
      "arxiv_id": "1810.00825",
      "role": "Extension",
      "relationship_sentence": "Set Transformer\u2019s attention-based set pooling directly informs PopT\u2019s learned attention over channel embeddings to produce population-level codes that are robust to sparse, arbitrary electrode layouts."
    },
    {
      "title": "Attention-based Deep Multiple Instance Learning",
      "authors": "Ilse et al.",
      "year": 2018,
      "arxiv_id": "1802.04712",
      "role": "Inspiration",
      "relationship_sentence": "The MIL attention mechanism inspired PopT\u2019s idea of learning instance (channel)-wise importance weights so the model can emphasize informative sensors during population-level aggregation."
    },
    {
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": "van den Oord et al.",
      "year": 2018,
      "arxiv_id": "1807.03748",
      "role": "Foundation",
      "relationship_sentence": "CPC established self-supervised temporal representation learning that PopT explicitly leverages by stacking population aggregation on top of pretrained per-channel time-series embeddings."
    },
    {
      "title": "Inferring single-trial neural population dynamics using sequential autoencoders (LFADS)",
      "authors": "Pandarinath et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "LFADS is a primary population-level latent modeling baseline that PopT aims to match or exceed while addressing LFADS\u2019s limitations in cross-subject generalization and sensor mismatch."
    },
    {
      "title": "A Shared Response Model for fMRI: unifying inter-subject myriads of features (SRM)",
      "authors": "Chen et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "SRM introduced the notion of aligning heterogeneous subject-specific measurements into a shared latent space, a core premise PopT adopts for population-level neural representations across variable electrode configurations."
    }
  ],
  "synthesis_narrative": "Permutation-invariant modeling of unordered inputs was crystallized by Deep Sets, which showed how to learn functions over sets independent of element order and count, enabling robust aggregation when input cardinalities vary. Set Transformer extended this idea with attention-based set operations and pooling, providing a learnable mechanism to weight and combine set elements\u2014useful when some elements are more informative than others. Attention-based deep multiple instance learning further demonstrated how instance-wise attention can select and weight relevant instances when only bag-level supervision is available, offering a template for learned importance weighting during aggregation. In parallel, Contrastive Predictive Coding established a strong self-supervised recipe for learning temporal embeddings that capture rich dynamics from raw time series without labels. For neural population activity specifically, LFADS introduced powerful latent dynamical models that decode behavior and dynamics but assume consistent sensor sets and require per-dataset training. The Shared Response Model showed that heterogeneous subject measurements can be projected into a shared latent space, even when individual sensors or voxels differ across subjects. Together, these works exposed a gap: self-supervised temporal encoders can learn strong per-channel features, and set/attention mechanisms can aggregate variable-sized collections, yet population-level neural decoding across subjects with sparse, mismatched electrodes remained underexplored. PopT naturally synthesizes these threads by stacking population-level attention-based set aggregation atop self-supervised temporal embeddings, yielding a shared, permutation-invariant code that generalizes across subjects and tasks while remaining lightweight and label-efficient.",
  "target_paper": {
    "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
    "authors": "Geeling Chau, Christopher Wang, Sabera J Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, Yisong Yue, Boris Katz, Andrei Barbu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "representation learning, neuroscience, self supervised learning",
    "abstract": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscie",
    "openreview_id": "FVuqJt3c4L",
    "forum_id": "FVuqJt3c4L"
  },
  "analysis_timestamp": "2026-01-06T17:54:23.728932"
}
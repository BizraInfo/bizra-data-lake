# BIZRA Data Lake - Python Test Suite
# Comprehensive testing workflow with coverage reporting
#
# Architecture:
# +---------+     +-------------+     +------+
# | Unit    | --> | Integration | --> | Slow |
# | (<30s)  |     | (<2min)     |     | (opt)|
# +---------+     +-------------+     +------+
#
# Standing on Giants: pytest, coverage.py, Codecov

name: Tests

on:
  push:
    branches: [main, master, develop]
    paths:
      - 'core/**'
      - 'tests/**'
      - 'pyproject.toml'
      - '.github/workflows/tests.yml'
  pull_request:
    branches: [main, master]
    paths:
      - 'core/**'
      - 'tests/**'
      - 'pyproject.toml'
      - '.github/workflows/tests.yml'
  schedule:
    # Run slow tests weekly on Sunday at 2am UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      run_slow_tests:
        description: 'Run slow tests'
        required: false
        default: 'false'
        type: boolean
      coverage_threshold:
        description: 'Coverage threshold (%)'
        required: false
        default: '55'
        type: string

env:
  PYTHONUNBUFFERED: '1'
  COVERAGE_THRESHOLD: ${{ github.event.inputs.coverage_threshold || '55' }}

concurrency:
  group: tests-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # =========================================================================
  # Stage 1: Unit Tests (Fast, <30s)
  # =========================================================================
  unit-tests:
    name: Unit Tests (Python ${{ matrix.python-version }}${{ matrix.optional-deps && ', full' || '' }})
    runs-on: ubuntu-24.04
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11', '3.12']
        optional-deps: [false]
        include:
          # Run with full dependencies on Python 3.12
          - python-version: '3.12'
            optional-deps: true
            coverage: true

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830  # v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-
            ${{ runner.os }}-pip-

      - name: Cache virtual environment
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830  # v4
        with:
          path: .venv-linux
          key: ${{ runner.os }}-venv-${{ matrix.python-version }}-${{ matrix.optional-deps }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-venv-${{ matrix.python-version }}-${{ matrix.optional-deps }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install pytest pytest-cov pytest-asyncio pytest-timeout pytest-xdist
          if [ "${{ matrix.optional-deps }}" = "true" ]; then
            pip install -e ".[dev,full]"
          else
            pip install -e ".[dev]"
          fi

      - name: Run unit tests
        id: unit_tests
        run: |
          pytest tests/ \
            -v \
            --tb=short \
            --timeout=30 \
            -m "not slow and not integration and not requires_ollama and not requires_gpu" \
            ${{ matrix.coverage && '--cov=core --cov-branch --cov-report=xml:coverage-unit.xml --cov-report=term-missing --cov-fail-under=0' || '' }} \
            --junit-xml=junit-unit.xml \
            -n auto \
            2>&1 | tee test-output.log
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Upload unit test results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4
        if: always()
        with:
          name: unit-test-results-${{ matrix.python-version }}-${{ matrix.optional-deps }}
          path: |
            junit-unit.xml
            coverage-unit.xml
            test-output.log
          retention-days: 14

      - name: Upload coverage to Codecov (unit)
        if: matrix.coverage && always()
        uses: codecov/codecov-action@b9fd7d16f6d7d1b5d2bec1a2887e65ceed900238  # v4
        with:
          files: ./coverage-unit.xml
          flags: unit
          name: unit-python-${{ matrix.python-version }}
          fail_ci_if_error: false
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # =========================================================================
  # Stage 2: Integration Tests (<2min)
  # =========================================================================
  integration-tests:
    name: Integration Tests (Python ${{ matrix.python-version }})
    runs-on: ubuntu-24.04
    timeout-minutes: 20
    needs: [unit-tests]
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11', '3.12']
        include:
          - python-version: '3.12'
            coverage: true

    services:
      redis:
        image: redis:7-alpine@sha256:b9f6cf0cab55fdd102fd2182deeae150457943d33439d18c6e2fc5666d3228c1
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830  # v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ matrix.python-version }}-

      - name: Cache virtual environment
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830  # v4
        with:
          path: .venv-linux
          key: ${{ runner.os }}-venv-${{ matrix.python-version }}-integration-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-venv-${{ matrix.python-version }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install pytest pytest-cov pytest-asyncio pytest-timeout
          pip install -e ".[dev,full]"

      - name: Run integration tests
        id: integration_tests
        run: |
          pytest tests/integration/ tests/ \
            -v \
            --tb=short \
            --timeout=120 \
            -m "integration and not requires_ollama and not requires_gpu and not slow" \
            ${{ matrix.coverage && '--cov=core --cov-branch --cov-report=xml:coverage-integration.xml --cov-report=term-missing --cov-fail-under=0' || '' }} \
            --junit-xml=junit-integration.xml \
            2>&1 | tee test-output.log
        env:
          PYTHONPATH: ${{ github.workspace }}
          REDIS_URL: redis://localhost:6379

      - name: Upload integration test results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4
        if: always()
        with:
          name: integration-test-results-${{ matrix.python-version }}
          path: |
            junit-integration.xml
            coverage-integration.xml
            test-output.log
          retention-days: 14

      - name: Upload coverage to Codecov (integration)
        if: matrix.coverage && always()
        uses: codecov/codecov-action@b9fd7d16f6d7d1b5d2bec1a2887e65ceed900238  # v4
        with:
          files: ./coverage-integration.xml
          flags: integration
          name: integration-python-${{ matrix.python-version }}
          fail_ci_if_error: false
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # =========================================================================
  # Stage 2b: Token System Gate (Ledger Integrity + Full Token Suite)
  # =========================================================================
  token-tests:
    name: Token System Gate
    runs-on: ubuntu-24.04
    timeout-minutes: 15
    needs: [unit-tests]

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4

      - name: Set up Python 3.12
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5
        with:
          python-version: '3.12'

      - name: Cache pip dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830  # v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-3.12-token-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-3.12-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install pytest pytest-cov pytest-asyncio pytest-timeout
          pip install -e ".[dev]"

      - name: Run token unit tests
        run: |
          pytest tests/core/token/ \
            -v \
            --tb=short \
            --timeout=30 \
            --cov=core/token \
            --cov-branch \
            --cov-report=term-missing \
            --cov-fail-under=80 \
            --junit-xml=junit-token-unit.xml \
            2>&1 | tee token-unit-output.log
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Run token integration tests
        run: |
          pytest tests/integration/test_token_integration.py \
            -v \
            --tb=short \
            --timeout=60 \
            --junit-xml=junit-token-integration.xml \
            2>&1 | tee token-integration-output.log
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Verify token ledger integrity
        run: |
          python tools/verify_token_ledger.py --json | tee token-verify.json
          exit_code=${PIPESTATUS[0]}
          # Exit code 2 = no ledger (ok in CI), 0 = pass, 1 = fail
          if [ "$exit_code" -eq 1 ]; then
            echo "::error::Token ledger verification failed!"
            exit 1
          elif [ "$exit_code" -eq 2 ]; then
            echo "::notice::No production ledger found (expected in CI)"
          else
            echo "::notice::Token ledger integrity verified"
          fi
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Token gate summary
        if: always()
        run: |
          echo "## Token System Gate" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Token unit tests | ${{ steps.token_unit.outcome || 'completed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Token integration tests | ${{ steps.token_integration.outcome || 'completed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Ledger integrity | verified |" >> $GITHUB_STEP_SUMMARY

      - name: Upload token test results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4
        if: always()
        with:
          name: token-test-results
          path: |
            junit-token-unit.xml
            junit-token-integration.xml
            token-unit-output.log
            token-integration-output.log
            token-verify.json
          retention-days: 14

  # =========================================================================
  # Stage 2c: Spearpoint E2E Gate (Cross-Pillar Integrity)
  # =========================================================================
  spearpoint-e2e:
    name: Spearpoint E2E Gate
    runs-on: ubuntu-24.04
    timeout-minutes: 15
    needs: [unit-tests, token-tests]

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4

      - name: Set up Python 3.12
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5
        with:
          python-version: '3.12'

      - name: Cache pip dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830  # v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-3.12-spearpoint-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-3.12-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install pytest pytest-asyncio pytest-timeout
          pip install -e ".[dev]"

      - name: Run spearpoint cockpit unit tests
        run: |
          pytest tests/core/sovereign/test_spearpoint_cockpit.py \
            -v \
            --tb=short \
            --timeout=30 \
            --junit-xml=junit-spearpoint-cockpit.xml \
            2>&1 | tee spearpoint-cockpit-output.log
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Run spearpoint E2E integration tests
        run: |
          pytest tests/integration/test_spearpoint_e2e.py \
            -v \
            --tb=short \
            --timeout=60 \
            --junit-xml=junit-spearpoint-e2e.xml \
            2>&1 | tee spearpoint-e2e-output.log
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Spearpoint pillar summary
        if: always()
        run: |
          echo "## Spearpoint E2E Gate" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Pillar | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| 1. Truth Spine (Ed25519 + Genesis) | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| 2. Verifier Surface (Reject Codes) | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| 3. SNR Engine (Gate + Facade) | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| 4. GoT Runtime (Graph + Sign) | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| 5. Evidence Ledger (Hash Chain) | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| Cross-Pillar Cockpit | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| Token + PoI Integration | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| SAT Health Check | Tested |" >> $GITHUB_STEP_SUMMARY

      - name: Upload spearpoint test results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4
        if: always()
        with:
          name: spearpoint-test-results
          path: |
            junit-spearpoint-cockpit.xml
            junit-spearpoint-e2e.xml
            spearpoint-cockpit-output.log
            spearpoint-e2e-output.log
          retention-days: 14

  # =========================================================================
  # Stage 2d: Seven-Layer Stack Gate (DDAGI OS Full-Stack Integrity)
  # =========================================================================
  seven-layer-stack:
    name: Seven-Layer Stack Gate
    runs-on: ubuntu-24.04
    timeout-minutes: 15
    needs: [unit-tests, spearpoint-e2e]

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4

      - name: Set up Python 3.12
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5
        with:
          python-version: '3.12'

      - name: Cache pip dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830  # v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-3.12-7layer-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-3.12-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install pytest pytest-asyncio pytest-timeout
          pip install -e ".[dev]"

      - name: Run 7-Layer stack integration tests
        run: |
          pytest tests/integration/test_seven_layer_stack.py \
            -v \
            --tb=short \
            --timeout=60 \
            --junit-xml=junit-seven-layer.xml \
            2>&1 | tee seven-layer-output.log
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Seven-Layer summary
        if: always()
        run: |
          echo "## Seven-Layer Stack Gate (DDAGI OS)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Layer | Module | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| L2-SIC | Sovereign Identity (Ed25519) | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| L3-PCE | Proof-Carrying Execution (GoT + Receipts) | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| L4-CPM | Consensus + Federation | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| L5-TFSC | Agent-to-Agent Protocol | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| L6-WAN | Knowledge Integration | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| L7-MCG | Ihsan Governance (IhsanGate + Watchdog) | Tested |" >> $GITHUB_STEP_SUMMARY
          echo "| Cross-Layer | L2-L3, L3-L7, Full-Stack Smoke | Tested |" >> $GITHUB_STEP_SUMMARY

      - name: Upload seven-layer test results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4
        if: always()
        with:
          name: seven-layer-test-results
          path: |
            junit-seven-layer.xml
            seven-layer-output.log
          retention-days: 14

  # =========================================================================
  # Stage 3: Slow Tests (Optional, scheduled or on-demand)
  # =========================================================================
  slow-tests:
    name: Slow Tests
    runs-on: ubuntu-24.04
    timeout-minutes: 30
    needs: [integration-tests]
    if: github.event_name == 'schedule' || github.event.inputs.run_slow_tests == 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4

      - name: Set up Python 3.12
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5
        with:
          python-version: '3.12'

      - name: Cache pip dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830  # v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-3.12-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-3.12-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install pytest pytest-cov pytest-asyncio pytest-timeout
          pip install -e ".[dev,full]"

      - name: Run slow tests
        run: |
          pytest tests/ \
            -v \
            --tb=short \
            --timeout=600 \
            -m "slow and not requires_ollama and not requires_gpu" \
            --cov=core \
            --cov-branch \
            --cov-report=xml:coverage-slow.xml \
            --cov-report=term-missing \
            --junit-xml=junit-slow.xml \
            2>&1 | tee test-output.log
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Upload slow test results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4
        if: always()
        with:
          name: slow-test-results
          path: |
            junit-slow.xml
            coverage-slow.xml
            test-output.log
          retention-days: 14

      - name: Upload coverage to Codecov (slow)
        if: always()
        uses: codecov/codecov-action@b9fd7d16f6d7d1b5d2bec1a2887e65ceed900238  # v4
        with:
          files: ./coverage-slow.xml
          flags: slow
          name: slow-tests
          fail_ci_if_error: false
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # =========================================================================
  # Coverage Report & Threshold Enforcement
  # =========================================================================
  coverage-report:
    name: Coverage Report
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    needs: [unit-tests, integration-tests, token-tests, spearpoint-e2e, seven-layer-stack]
    if: always() && !cancelled()

    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4

      - name: Set up Python 3.12
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5
        with:
          python-version: '3.12'

      - name: Install coverage tools
        run: |
          pip install coverage[toml] diff-cover

      - name: Download all coverage artifacts
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093  # v4
        with:
          pattern: '*-test-results-*'
          merge-multiple: true
          path: coverage-reports

      - name: Merge coverage reports
        run: |
          # Find all coverage XML files
          coverage_files=$(find coverage-reports -name "coverage-*.xml" 2>/dev/null || true)

          if [ -z "$coverage_files" ]; then
            echo "::warning::No coverage files found"
            echo "coverage_available=false" >> $GITHUB_ENV
            exit 0
          fi

          echo "Found coverage files:"
          echo "$coverage_files"

          # Create combined coverage report
          pip install pytest pytest-cov

          # Run coverage combine if .coverage files exist
          if ls coverage-reports/.coverage* 1>/dev/null 2>&1; then
            coverage combine coverage-reports/.coverage*
            coverage xml -o coverage-combined.xml
            coverage report --format=markdown > coverage-report.md
          else
            # Use the first XML file if no .coverage files
            first_xml=$(echo "$coverage_files" | head -1)
            cp "$first_xml" coverage-combined.xml
          fi

          echo "coverage_available=true" >> $GITHUB_ENV

      - name: Generate coverage diff report (for PRs)
        if: github.event_name == 'pull_request' && env.coverage_available == 'true'
        run: |
          diff-cover coverage-combined.xml \
            --compare-branch=origin/${{ github.base_ref }} \
            --html-report=coverage-diff.html \
            --markdown-report=coverage-diff.md \
            --fail-under=${{ env.COVERAGE_THRESHOLD }} \
            || echo "::warning::Coverage diff below threshold"

      - name: Check coverage threshold
        if: env.coverage_available == 'true'
        run: |
          pip install coverage

          # Extract coverage percentage from XML
          coverage_pct=$(python -c "
          import xml.etree.ElementTree as ET
          try:
              tree = ET.parse('coverage-combined.xml')
              root = tree.getroot()
              line_rate = float(root.get('line-rate', 0))
              print(f'{line_rate * 100:.1f}')
          except Exception as e:
              print('0.0')
          ")

          echo "Coverage: ${coverage_pct}%"
          echo "Threshold: ${{ env.COVERAGE_THRESHOLD }}%"

          # Compare with threshold
          threshold=${{ env.COVERAGE_THRESHOLD }}
          if (( $(echo "$coverage_pct < $threshold" | bc -l) )); then
            echo "::error::Coverage ${coverage_pct}% is below threshold ${threshold}%"
            exit 1
          else
            echo "::notice::Coverage ${coverage_pct}% meets threshold ${threshold}%"
          fi

      - name: Post coverage comment on PR
        if: github.event_name == 'pull_request' && env.coverage_available == 'true'
        uses: actions/github-script@f28e40c7f34bde8b3046d885e986cb6290c5673b  # v7
        with:
          script: |
            const fs = require('fs');

            let comment = '## Test Coverage Report\n\n';

            // Read coverage diff if available
            try {
              const diffReport = fs.readFileSync('coverage-diff.md', 'utf8');
              comment += diffReport;
            } catch {
              comment += '_Coverage diff not available_\n';
            }

            comment += '\n\n---\n';
            comment += `Threshold: **${{ env.COVERAGE_THRESHOLD }}%**\n`;
            comment += '\n_Generated by BIZRA Test Suite_';

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' &&
              c.body.includes('Test Coverage Report')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Upload combined coverage report
        if: env.coverage_available == 'true'
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02  # v4
        with:
          name: coverage-combined
          path: |
            coverage-combined.xml
            coverage-report.md
            coverage-diff.html
            coverage-diff.md
          retention-days: 30

      - name: Generate coverage badge data
        if: env.coverage_available == 'true' && github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
        run: |
          coverage_pct=$(python -c "
          import xml.etree.ElementTree as ET
          tree = ET.parse('coverage-combined.xml')
          root = tree.getroot()
          line_rate = float(root.get('line-rate', 0))
          print(f'{line_rate * 100:.1f}')
          ")

          # Determine color based on coverage
          if (( $(echo "$coverage_pct >= 90" | bc -l) )); then
            color="brightgreen"
          elif (( $(echo "$coverage_pct >= 80" | bc -l) )); then
            color="green"
          elif (( $(echo "$coverage_pct >= 70" | bc -l) )); then
            color="yellow"
          elif (( $(echo "$coverage_pct >= 60" | bc -l) )); then
            color="orange"
          else
            color="red"
          fi

          echo "COVERAGE_PCT=${coverage_pct}" >> $GITHUB_ENV
          echo "BADGE_COLOR=${color}" >> $GITHUB_ENV

  # =========================================================================
  # Test Summary
  # =========================================================================
  test-summary:
    name: Test Summary
    runs-on: ubuntu-24.04
    timeout-minutes: 5
    needs: [unit-tests, integration-tests, spearpoint-e2e, seven-layer-stack, coverage-report]
    if: always()

    steps:
      - name: Download test results
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093  # v4
        with:
          pattern: '*-test-results-*'
          merge-multiple: true
          path: test-results
        continue-on-error: true

      - name: Generate summary
        run: |
          echo "# BIZRA Test Suite Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Stages" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Python 3.11 | Python 3.12 |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|-------------|-------------|" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.unit-tests.result == 'success' && 'PASS' || 'FAIL' }} | ${{ needs.unit-tests.result == 'success' && 'PASS' || 'FAIL' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result == 'success' && 'PASS' || 'FAIL' }} | ${{ needs.integration-tests.result == 'success' && 'PASS' || 'FAIL' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Configuration" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Coverage Threshold:** ${{ env.COVERAGE_THRESHOLD }}%" >> $GITHUB_STEP_SUMMARY
          echo "- **Slow Tests:** ${{ github.event_name == 'schedule' || github.event.inputs.run_slow_tests == 'true' && 'Enabled' || 'Skipped' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Skipped Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The following test markers are skipped in CI:" >> $GITHUB_STEP_SUMMARY
          echo "- \`@pytest.mark.requires_ollama\` - Requires Ollama running" >> $GITHUB_STEP_SUMMARY
          echo "- \`@pytest.mark.requires_gpu\` - Requires GPU hardware" >> $GITHUB_STEP_SUMMARY
          echo "- \`@pytest.mark.slow\` - Long-running tests (run on schedule)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "_Standing on Giants: pytest, coverage.py, Codecov_" >> $GITHUB_STEP_SUMMARY

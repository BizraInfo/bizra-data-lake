{
  "prior_works": [
    {
      "title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills",
      "authors": "Xue Bin Peng, Pieter Abbeel, Sergey Levine, Michiel van de Panne",
      "year": 2018,
      "role": "methodological predecessor",
      "relationship_sentence": "CooHOI\u2019s first phase\u2014learning single-humanoid skills via imitation from motion priors\u2014directly builds on DeepMimic\u2019s paradigm of training physically simulated humanoids to match motion examples with RL."
    },
    {
      "title": "AMP: Adversarial Motion Priors for Character Control",
      "authors": "Xue Bin Peng et al.",
      "year": 2021,
      "role": "methodological predecessor",
      "relationship_sentence": "By leveraging motion priors through adversarial discrimination, AMP provided the practical recipe CooHOI employs to learn realistic human-object interaction skills from motion data before transferring them to cooperation."
    },
    {
      "title": "ASE: Adversarial Skill Embeddings for Physically Simulated Characters",
      "authors": "Xue Bin Peng et al.",
      "year": 2022,
      "role": "skill learning and transfer framework",
      "relationship_sentence": "CooHOI\u2019s two-phase design\u2014first distill individual, reusable skills and then deploy/transfer them in new tasks\u2014draws directly from ASE\u2019s idea of learning generalizable skill policies that can be recombined for downstream control."
    },
    {
      "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments (MADDPG)",
      "authors": "Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch",
      "year": 2017,
      "role": "multi-agent RL baseline/contrast",
      "relationship_sentence": "CooHOI is motivated by the sample-inefficiency and scaling issues of joint multi-agent training exemplified by MADDPG, and expressly avoids such heavy centralized training by transferring single-agent skills instead."
    },
    {
      "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "authors": "Tabish Rashid, Mikayel Samvelyan, Christian Schroeder de Witt, Gregory Farquhar, Jakob Foerster, Shimon Whiteson",
      "year": 2018,
      "role": "multi-agent RL baseline/contrast",
      "relationship_sentence": "CooHOI\u2019s policy-transfer strategy is a response to the coordination and credit-assignment complexities tackled by value-factorization methods like QMIX, replacing joint training with object-dynamics\u2013mediated coordination."
    },
    {
      "title": "Interaction Networks for Learning about Objects, Relations and Physics",
      "authors": "Peter W. Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, Koray Kavukcuoglu",
      "year": 2016,
      "role": "object-centric dynamics inspiration",
      "relationship_sentence": "The idea of coordinating agents by reasoning over a shared object\u2019s dynamics in CooHOI is rooted in object-centric relational modeling popularized by Interaction Networks."
    },
    {
      "title": "AMASS: Archive of Motion Capture as Surface Shapes",
      "authors": "Naureen Mahmood et al.",
      "year": 2019,
      "role": "data/motion prior enabler",
      "relationship_sentence": "Large-scale, unified motion priors like AMASS underpin the imitation stage CooHOI relies on to endow a single humanoid with realistic base skills before cooperative transfer."
    }
  ],
  "synthesis_narrative": "CooHOI\u2019s core idea\u2014decoupling cooperative humanoid manipulation into (1) single-agent human\u2013object skill acquisition from motion priors and (2) subsequent multi-agent coordination via the manipulated object\u2019s shared dynamics\u2014emerges from two converging lines of work. On the single-agent side, DeepMimic established the recipe for learning physically plausible humanoid skills through motion-guided reinforcement learning, while AMP demonstrated how adversarially trained motion priors can scale imitation to complex, contact-rich behaviors. ASE then showed that such learned skills can be encapsulated as reusable, transferable policies, suggesting a clean stage-wise path from mastery of individual capabilities to broader task composition\u2014exactly the scaffold CooHOI uses for its phase-one skill learning and phase-two transfer.\nOn the multi-agent side, canonical CTDE methods like MADDPG and value-factorization approaches like QMIX highlighted both the promise and the practical limitations of end-to-end multi-agent RL\u2014credit assignment, instability, and sample inefficiency. CooHOI\u2019s design explicitly sidesteps these pitfalls by first perfecting individual controllers and then coordinating them indirectly through the physics of the commonly manipulated object. This object-centric coordination philosophy is grounded in the relational reasoning paradigm introduced by Interaction Networks, which framed dynamics as interactions among entities. Finally, large-scale motion repositories such as AMASS make the first phase viable by providing rich motion priors. Together, these works directly shape CooHOI\u2019s two-phase, object-dynamics\u2013mediated approach to efficient, realistic cooperative human\u2013object manipulation.",
  "analysis_timestamp": "2026-01-06T23:33:36.285819"
}
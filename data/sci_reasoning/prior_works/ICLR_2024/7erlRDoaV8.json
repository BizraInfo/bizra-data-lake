{
  "prior_works": [
    {
      "title": "The Secret Sharer: Measuring Unintended Memorization in Neural Networks",
      "authors": "Nicholas Carlini et al.",
      "year": 2019,
      "arxiv_id": "1802.08232",
      "role": "Foundation",
      "relationship_sentence": "Its exposure metric\u2014judging an attack successful when a secret appears among a model\u2019s top-ranked candidates\u2014directly underpins this paper\u2019s top-B candidate threat model and the corresponding defense objective."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "arxiv_id": "2012.07805",
      "role": "Foundation",
      "relationship_sentence": "By showing practical white- and gray-box extraction of memorized sequences via sampling/beam search, it defines the concrete adversary behavior this paper designs defenses against and motivates weight-level deletion over surface-level filters."
    },
    {
      "title": "Quantifying Memorization Across Neural Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "It demonstrates that memorized content persists across prompts and scales with model size, highlighting the insufficiency of naive redaction and motivating robust deletion objectives resilient to paraphrases and attack diversity."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME)",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "arxiv_id": "2202.05262",
      "role": "Extension",
      "relationship_sentence": "This method for localizing and editing factual associations in model weights provides the editable mechanism and baseline that this paper adapts toward deletion and tests under adversarial extraction."
    },
    {
      "title": "Mass-Editing Memory in a Transformer (MEMIT)",
      "authors": "Kevin Meng et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "As the state-of-the-art multi-edit approach, it serves as a primary baseline and exposes overgeneralization/side-effect risks that this paper addresses with defense-oriented loss design against extraction."
    },
    {
      "title": "MEND: Fast Model Editing at Scale",
      "authors": "Eric Mitchell et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "Its gradient-based editor for local, scalable edits is a main comparator whose lack of guarantees against adversarial extraction motivates the paper\u2019s attack-aware deletion objectives."
    }
  ],
  "synthesis_narrative": "Work on unintended memorization established both the measurement and the stakes of the problem: The Secret Sharer introduced exposure, a rank-based notion of risk where an attack is considered successful if a secret appears among a model\u2019s top candidates. Subsequent demonstrations showed that large language models can yield verbatim training data under sampling and beam-search strategies, concretizing practical extraction behavior and the relevance of white-/gray-box adversaries. Further analysis quantified how memorization scales with model size and persists across paraphrases, underscoring that surface-level fixes or naive redaction do not reliably remove sensitive knowledge. In parallel, knowledge editing methods matured: ROME pinpointed and modified internal factual associations through targeted weight updates, inaugurating a controlled, local editing paradigm. MEND delivered a fast, gradient-based editor for scalable local edits, while MEMIT expanded to mass editing, revealing challenges like overgeneralization and leakage to unintended contexts.\n\nTaken together, these works expose a gap: while extraction is tractable and persistent, existing editors focus on changing facts rather than making them unrecoverable under adversarial prompting, and they lack guarantees aligned with exposure-style risk. The current paper synthesizes the exposure-based threat model with weight-editing mechanisms, reframing editing as deletion against an attacker who succeeds if the answer is among B candidates. Building on ROME/MEND/MEMIT\u2019s editability, it designs attack-aware objectives that suppress the sensitive answer across paraphrases and contexts while controlling side effects\u2014naturally extending editing into a defense tuned to extraction risks.",
  "target_paper": {
    "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
    "authors": "Vaidehi Patil, Peter Hase, Mohit Bansal",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Sensitive Information Deletion, Privacy Attacks, Model editing, Language Models",
    "abstract": "Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. We study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates",
    "openreview_id": "7erlRDoaV8",
    "forum_id": "7erlRDoaV8"
  },
  "analysis_timestamp": "2026-01-06T10:15:58.077310"
}
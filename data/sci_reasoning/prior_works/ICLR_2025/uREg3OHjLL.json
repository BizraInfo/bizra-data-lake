{
  "prior_works": [
    {
      "title": "Depth Lower Bounds for ReLU Networks with Integer Weights",
      "authors": "Christian Haase et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "Their proof that any integer-weight ReLU network exactly computing F_n needs at least ceil(log2(n+1)) hidden layers is the immediate precursor whose depth-lower-bound methodology and witness function F_n this work generalizes from integer weights to rational (N-ary) weights."
    },
    {
      "title": "On the Depth of ReLU Networks: The Max Function F_n as a Depth Witness",
      "authors": "Benedikt Hertrich et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This NeurIPS 2021 paper introduced F_n = max(0, x1, \u2026, xn) as the canonical witness and conjectured the ceil(log2(n+1)) depth lower bound for exact representation with ReLU networks, defining the problem and target bound that the present work partially confirms for rational weights."
    },
    {
      "title": "Tropical Geometry of Deep Neural Networks",
      "authors": "R. Zhang et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By recasting ReLU networks as max-plus/tropical objects with associated Newton polytopes, this work motivates the mixed-volume and lattice-polytope viewpoint that underpins the number-theoretic and geometric analysis used here to derive depth lower bounds under rational weight constraints."
    },
    {
      "title": "On the Number of Linear Regions of Deep Neural Networks",
      "authors": "G. F. Mont\u00fafar et al.",
      "year": 2014,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Their polyhedral/region-count framework for piecewise-linear ReLU functions provides the structural lens (via affine pieces and combinatorial growth across layers) that this paper leverages when translating weight restrictions into limits on how depth can expand the max-structure of F_n."
    },
    {
      "title": "Complexity of Linear Regions in Deep Networks",
      "authors": "B. Hanin et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "They show region-count bounds can be loose and decoupled from exact representability, sharpening the need for exact-function witnesses like F_n and motivating the search for precise depth lower bounds under additional constraints such as weight arithmetic."
    },
    {
      "title": "Benefits of Depth in Neural Networks",
      "authors": "M. Telgarsky",
      "year": 2016,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "This depth-separation work clarified how compositional structure amplifies expressivity, directly motivating the use of simple, compositional witnesses (like F_n via binary max trees) and framing why proving exact-depth lower bounds is both natural and nontrivial."
    }
  ],
  "synthesis_narrative": "Hertrich, Basu, Di Summa, and Skutella established F_n = max(0, x1, \u2026, xn) as a canonical witness for studying depth, formulating the precise conjecture that exact computation of F_n with ReLU networks requires a logarithmic number of hidden layers. Haase, Hertrich, and Loho then confirmed this conjecture for integer-weight networks, showing ceil(log2(n+1)) depth is unavoidable in that arithmetic setting and showcasing techniques that track how max-composition propagates through layers. Parallel developments on the geometry of ReLU networks\u2014most notably the tropical viewpoint of Zhang, Naitzat, and Lim\u2014connect ReLU computation to max-plus algebra and Newton polytopes, providing the polyhedral/mixed-volume machinery to reason about how algebraic constraints restrict combinatorial growth. Earlier structural work by Mont\u00fafar, Pascanu, Cho, and Bengio quantified how piecewise-linear regions expand with depth, giving a combinatorial template, while Hanin and Rolnick highlighted that region counts alone can misrepresent exact representability, pushing the field toward function-specific certificates. Telgarsky\u2019s depth-separation results reinforced the centrality of compositional witnesses in exposing depth advantages. Taken together, these works isolate F_n as the right exact witness, supply a proven integer-weight lower bound, and furnish geometric tools to relate arithmetic constraints to combinatorial expressivity. The present paper synthesizes these threads by replacing integrality with rational N-ary constraints and, via tropical/polyhedral reasoning, converts denominator structure into limits on per-layer \u201cmax-arity,\u201d yielding ceil(log3(n+1)) for decimal fractions and a general \u03a9(ln n / ln ln N) lower bound\u2014thereby partially confirming the original conjecture beyond the integer case.",
  "target_paper": {
    "title": "On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth",
    "authors": "Gennadiy Averkov, Christopher Hojny, Maximilian Merkert",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "expressive power, depth, exact representations, ReLU networks, mixed volumes, lattice polytopes, number theory",
    "abstract": "To confirm that the expressive power of ReLU neural networks grows with their depth, the function $F_n = \\max (0,x_1,\\ldots,x_n )$ has been considered in the literature.\n  A conjecture by Hertrich, Basu, Di Summa, and Skutella [NeurIPS 2021] states that any ReLU network that exactly represents $F_n$ has at least $\\lceil \\log_2 (n+1) \\rceil$ hidden layers.\n  The conjecture has recently been confirmed for networks with integer weights by Haase, Hertrich, and Loho [ICLR 2023].\n\n  We follow up on this line of research and show that, within ReLU networks whose weights are decimal fractions, $F_n$ can only be represented by networks with at least $\\lceil \\log_3 (n+1) \\rceil$ hidden layers.\n  Moreover, if all weights are $N$-ary fractions, then $F_n$ can only be represented by networks with at least $\\Omega( \\frac{\\ln n}{\\ln \\ln N})$ layers.\n  These results are a  partial confirmation of the above conjecture for rational ReLU networks, and provide the first non-constant lower bound on the dep",
    "openreview_id": "uREg3OHjLL",
    "forum_id": "uREg3OHjLL"
  },
  "analysis_timestamp": "2026-01-06T10:32:18.022034"
}
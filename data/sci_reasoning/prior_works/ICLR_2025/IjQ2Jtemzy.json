{
  "prior_works": [
    {
      "title": "Eliciting Latent Knowledge",
      "authors": "Ajeya Cotra et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work frames the core goal of extracting a model\u2019s internal knowledge without supervision, which the current paper operationalizes by eliciting and verifying language-model descriptions of their own learned behavioral policies without in-context examples."
    },
    {
      "title": "Language Models (Mostly) Know What They Know",
      "authors": "Akhil Kadavath et al.",
      "year": 2022,
      "arxiv_id": "2207.05221",
      "role": "Inspiration",
      "relationship_sentence": "Demonstrating that LLMs can accurately report aspects of their own competence inspired the paper\u2019s central idea to test whether models can similarly articulate their learned behavioral policies (self-knowledge about behavior) in out-of-context settings."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "arxiv_id": "2212.08073",
      "role": "Related Problem",
      "relationship_sentence": "By showing that models can follow and reason about explicit natural-language principles, this work motivates probing whether models can also verbalize implicit, fine-tuned policies\u2014moving from externally provided constitutions to internally learned ones."
    },
    {
      "title": "Concealed Data Poisoning Attacks on NLP Models",
      "authors": "Eric Wallace et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper establishes that targeted behaviors can be implanted via data poisoning/backdoors in NLP models, directly motivating the paper\u2019s exploration of whether such implanted (or fine-tuned) behaviors are explicitly knowable to the model itself."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "The discovery of short universal triggers that activate undesirable behaviors informs the paper\u2019s analysis linking learned policies to trigger-like conditions (e.g., personas) and testing whether the model can describe those activation conditions."
    },
    {
      "title": "Sleeper Agents: Training Deceptive Models that Persist Through Safety Training",
      "authors": "Evan Hubinger et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By showing that hidden, goal-directed behaviors can be trained to activate under specific circumstances, this work motivates testing whether models can recognize and report their own implanted policies and triggers\u2014a gap the paper directly addresses."
    },
    {
      "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?",
      "authors": "Saizheng Zhang et al.",
      "year": 2018,
      "arxiv_id": "1801.07243",
      "role": "Foundation",
      "relationship_sentence": "Introducing persona-conditioned behavior as a formal setup provides the methodological basis for attributing different learned policies to distinct personas and evaluating whether models can correctly report persona-specific policies."
    }
  ],
  "synthesis_narrative": "Eliciting Latent Knowledge established the aim of extracting internal model beliefs without supervision, emphasizing that models can possess latent representations not directly revealed by standard prompts. Language Models (Mostly) Know What They Know showed that LLMs can report aspects of their own competence, suggesting that self-knowledge can be elicited reliably via language. Constitutional AI demonstrated that models can follow and reason over explicit natural-language rules and critique their outputs against those rules, underscoring the feasibility of using natural language as a medium for specifying and reflecting on behavioral policies. In parallel, Concealed Data Poisoning Attacks on NLP Models grounded the notion that targeted behaviors can be implanted via training data, formalizing backdoors in NLP. Universal and Transferable Adversarial Attacks on Aligned Language Models revealed that short, general triggers can consistently activate such behaviors, conceptually linking behavior to compact activation conditions. Sleeper Agents then showed that strategically trained hidden goals can persist and surface under triggers even after safety training. Finally, Persona-Chat provided a standard framework for persona-conditioned behavior, enabling attribution and measurement across identities. Together, these works spotlighted a gap: while models can follow explicit rules and can be implanted with hidden behaviors, it was unclear whether they can articulate their own implicitly learned policies, detached from immediate examples, and attribute them to specific conditions such as personas. Building on elicitation principles, backdoor insights, and persona conditioning, the paper naturally tests and demonstrates behavioral self-awareness: LLMs trained on behaviors can out-of-context describe the governing policies and correctly attribute them to the appropriate persona or trigger.",
  "target_paper": {
    "title": "Tell me about yourself: LLMs are aware of their learned behaviors",
    "authors": "Jan Betley, Xuchan Bao, Mart\u00edn Soto, Anna Sztyber-Betley, James Chua, Owain Evans",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "NLP, LLM, GPT, generalization, out-of-context reasoning, capabilities, fine-tuning, self-awareness, self-knowledge",
    "abstract": "We study *behavioral self-awareness*, which we define as an LLM's capability to articulate its behavioral policies without relying on in-context examples. We finetune LLMs on examples that exhibit particular behaviors, including (a) making risk-seeking / risk-averse economic decisions, and (b) making the user say a certain word. Although these examples never contain explicit descriptions of the policy (e.g. \"I will now take the risk-seeking option\"), we find that the finetuned LLMs can explicitly describe their policies through out-of-context reasoning. We demonstrate LLMs' behavioral self-awareness across various evaluation tasks, both for multiple-choice and free-form questions. \nFurthermore, we demonstrate that models can correctly attribute different learned policies to distinct personas.\nFinally, we explore the connection between behavioral self-awareness and the concept of backdoors in AI safety, where certain behaviors are implanted in a model, often through data poisoning, and ",
    "openreview_id": "IjQ2Jtemzy",
    "forum_id": "IjQ2Jtemzy"
  },
  "analysis_timestamp": "2026-01-06T16:58:26.049735"
}
{
  "prior_works": [
    {
      "title": "Intelligent Selection of Language Model Training Data",
      "authors": "Robert C. Moore et al.",
      "year": 2010,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work introduced domain-similarity data selection via cross-entropy difference, the canonical paradigm DiSF explicitly replaces to avoid the diversity collapse induced by selecting data too close to a target domain."
    },
    {
      "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks",
      "authors": "Suchin Gururangan et al.",
      "year": 2020,
      "arxiv_id": "2004.10964",
      "role": "Gap Identification",
      "relationship_sentence": "By showing domain-adaptive pretraining boosts in-domain performance while harming general capabilities, this paper surfaces the exact trade-off (domain gains vs. broad generalization) that DiSF\u2019s diversity-preserving selection is designed to fix."
    },
    {
      "title": "DoReMi: Optimizing Data Mixtures for Language Model Pretraining",
      "authors": "Xie et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "DoReMi reweights sources using a proxy to match a target distribution (e.g., Wikipedia/Books), providing the primary similarity-driven baseline whose tendency to narrow the representation space DiSF counters with decorrelation-based submodular selection."
    },
    {
      "title": "Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies",
      "authors": "Andreas Krause et al.",
      "year": 2008,
      "arxiv_id": "0710.3742",
      "role": "Extension",
      "relationship_sentence": "DiSF adapts the log-determinant information gain submodular objective (and its greedy 1\u22121/e maximization) from this work to select text files that maximize the volume of feature covariance, yielding decorrelated, diverse subsets."
    },
    {
      "title": "Submodular Optimization for Data Subset Selection in Machine Learning",
      "authors": "Kai Wei et al.",
      "year": 2015,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "This line of work demonstrates that greedy submodular objectives (e.g., facility location, log-det) produce diverse, representative data subsets in NLP and vision, directly informing DiSF\u2019s use of submodular selection for diversity."
    },
    {
      "title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning",
      "authors": "Adrien Bardes et al.",
      "year": 2021,
      "arxiv_id": "2105.04906",
      "role": "Inspiration",
      "relationship_sentence": "VICReg\u2019s anti-collapse principle of enforcing per-dimension variance and decorrelated features motivates DiSF\u2019s objective of achieving more uniform eigenvalues in the selected data\u2019s feature covariance."
    },
    {
      "title": "Deduplicating Training Data Makes Language Models Better",
      "authors": "Katherine Lee et al.",
      "year": 2021,
      "arxiv_id": "2107.06499",
      "role": "Gap Identification",
      "relationship_sentence": "By showing that redundancy and near-duplicates in pretraining corpora degrade performance, this work highlights the need for diversity-aware selection that DiSF achieves via decorrelation in feature space."
    }
  ],
  "synthesis_narrative": "Early data selection for language modeling centered on domain similarity: Moore and Lewis proposed choosing sentences that minimize cross-entropy difference to a target domain, a strategy that later became ubiquitous. Gururangan and colleagues then showed that continued pretraining on in-domain data improves domain tasks but can erode broad generalization, exposing a central tension in domain-focused selection. DoReMi operationalized this paradigm at pretraining scale by learning mixture weights that align sources (e.g., Wikipedia/Books) to a target distribution using a proxy model, but its similarity-driven emphasis risks narrowing the representation space. In parallel, the submodular optimization literature established that greedy maximization of objectives like log-determinant yields diverse, representative subsets; Krause et al. provided the log-det information gain objective with strong guarantees, and Wei et al. demonstrated practical, scalable submodular subset selection for text. From representation learning, VICReg formalized anti-collapse via enforcing variance and decorrelation, while Lee et al. showed that redundancy in LM corpora harms performance, underscoring the value of diversity.\nTaken together, these works reveal both the effectiveness and the pitfalls of similarity-driven selection and point to a remedy: explicitly manage representation diversity. The submodular toolbox offers a principled, efficient way to choose diverse sets, and anti-collapse objectives suggest what kind of diversity matters\u2014uniform, decorrelated features. Building on these insights, the current paper replaces target-similarity scoring with a submodular, log-det-style decorrelation criterion over file features, using greedy maximization to equalize covariance eigenvalues and thereby prevent dimensional collapse while preserving broad capability.",
  "target_paper": {
    "title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection",
    "authors": "Ziqing Fan, Siyuan Du, Shengchao Hu, Pingjie Wang, Li Shen, Ya Zhang, Dacheng Tao, Yanfeng Wang",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "file selection, large language model, pre-training, submodular optimization",
    "abstract": "Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e. dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance.To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyz",
    "openreview_id": "f4gF6AIHRy",
    "forum_id": "f4gF6AIHRy"
  },
  "analysis_timestamp": "2026-01-06T17:52:39.209294"
}
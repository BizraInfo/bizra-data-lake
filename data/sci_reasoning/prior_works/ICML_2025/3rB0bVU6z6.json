{
  "prior_works": [
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Wen-tau Yih",
      "year": 2023,
      "role": "Agent prompting paradigm enabling tool use with stepwise reasoning",
      "relationship_sentence": "RE-Bench\u2019s agent baselines rely on tool-augmented, iterative reasoning patterns; ReAct directly informed the design and evaluation of agent interaction loops with code execution and environment feedback."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Karthik Narasimhan, Yuan Cao, et al.",
      "year": 2023,
      "role": "Search over multi-step reasoning traces",
      "relationship_sentence": "RE-Bench\u2019s analysis of best-of-k and exploration budgets builds on ToT-style search over reasoning/solution trajectories to quantify how expanded search improves success on long-horizon research tasks."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, et al.",
      "year": 2022,
      "role": "Best-of-N sampling strategy for improved reliability",
      "relationship_sentence": "The benchmark\u2019s best-of-k evaluation protocol and time\u2013quality tradeoff measurements are grounded in the self-consistency insight that multiple independent samples substantially raise solve rates on complex problems."
    },
    {
      "title": "Evaluating Large Language Models Trained on Code (HumanEval)",
      "authors": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, et al.",
      "year": 2021,
      "role": "Foundational code-generation evaluation with executable tests",
      "relationship_sentence": "RE-Bench extends HumanEval\u2019s executable, test-based evaluation ethos to realistic, open-ended ML R&D environments and uses pass/fail style scoring as a building block for quantitative comparisons."
    },
    {
      "title": "Measuring Coding Challenge Competence With APPS",
      "authors": "Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Andy Zou, et al.",
      "year": 2021,
      "role": "Long-form, human-comparable programming benchmark",
      "relationship_sentence": "APPS\u2019 emphasis on realistic, multi-step coding tasks and human baselines influenced RE-Bench\u2019s focus on task realism, difficulty calibration, and direct comparison to expert performance."
    },
    {
      "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "authors": "Timo Schick, Jane Dwivedi-Yu, Roberta Raileanu, Patrick Lewis, et al.",
      "year": 2023,
      "role": "Learning to call external tools/APIs",
      "relationship_sentence": "RE-Bench\u2019s agent setups that integrate code execution, search, and other utilities are conceptually grounded in Toolformer\u2019s demonstration that tool-use competence is critical for complex, real-world tasks."
    },
    {
      "title": "Improving Reproducibility in Machine Learning Research: A Report from the NeurIPS 2019 Reproducibility Program",
      "authors": "Joelle Pineau, Koustuv Sinha, et al.",
      "year": 2020,
      "role": "Reproducibility standards and human-in-the-loop replication",
      "relationship_sentence": "RE-Bench\u2019s research-engineering environments and 8-hour expert baselines draw directly on the reproducibility program\u2019s practices to define rigorous, time-bounded, human-grounded evaluations of ML R&D work."
    }
  ],
  "synthesis_narrative": "RE-Bench\u2019s central contribution\u2014realistic, human-comparable evaluation of AI agents on ML research engineering\u2014sits at the intersection of agentic tool-use, search over solution trajectories, executable evaluations, and reproducible research practices. On the agent side, ReAct and Toolformer established that effective research workflows require iterative reasoning tightly coupled with tools such as code execution, search, and file I/O. RE-Bench\u2019s agent baselines leverage precisely these patterns to operate within complex ML environments. To quantify how agent design and time budgets translate into performance, the benchmark adopts best-of-k sampling and exploration analyses inspired by Self-Consistency and Tree of Thoughts, which show that diversified trajectories and structured search markedly improve solve rates on long-horizon problems.\n\nFor measurement, HumanEval and APPS provided the methodological backbone: executable, objective tests and human-comparable coding tasks. RE-Bench extends this philosophy beyond toy problems to multi-hour, open-ended ML R&D, while maintaining crisp scoring signals tied to concrete artifacts (e.g., code, experiments). Finally, the NeurIPS Reproducibility initiative shaped RE-Bench\u2019s realism and rigor: the construction of environments from real ML workflows, the 8-hour expert attempts, and transparent baselining practices directly mirror the field\u2019s push toward time-bounded, replicable research engineering. Together, these works underpin RE-Bench\u2019s design choices\u2014tool-augmented agents, search-aware evaluation, executable scoring, and human baselines\u2014enabling a credible head-to-head comparison between frontier LLM agents and practicing ML researchers.",
  "analysis_timestamp": "2026-01-07T00:04:09.142645"
}
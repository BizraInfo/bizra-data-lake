{
  "prior_works": [
    {
      "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
      "authors": "Jacob Andreas et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Policy Sketches introduced using natural-language-like task sketches to define and train reusable subpolicies, which MaestroMotif generalizes to free-form skill descriptions paired with automatically generated rewards."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Michael Ahn et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "SayCan\u2019s approach of using an LLM to select and sequence skills while grounding choices in learned affordance/value estimates directly informs MaestroMotif\u2019s LLM-guided composition of trained skills into complex behaviors."
    },
    {
      "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
      "authors": "Guanzhi Wang et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Voyager\u2019s mechanism of having an LLM write, name, and reuse skills as callable code is adapted by MaestroMotif to use LLM code generation for implementing and reusing a skill library."
    },
    {
      "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
      "authors": "Ma et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Eureka\u2019s technique of generating and iteratively refining programmatic reward functions with an LLM and RL-in-the-loop is extended in MaestroMotif to create per-skill rewards directly from natural-language skill descriptions."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Constitutional AI established AI feedback as a viable substitute for human oversight, a principle MaestroMotif leverages to automatically assess and refine reward designs for skills without human-in-the-loop labeling."
    },
    {
      "title": "Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning",
      "authors": "M. Icarte et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Reward Machines\u2019 structured decomposition of tasks into composable reward semantics motivates MaestroMotif\u2019s use of language-defined skills with explicit reward definitions that can be composed."
    },
    {
      "title": "The NetHack Learning Environment",
      "authors": "Matthias K\u00fcttler et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "NLE provides the complex, sparse-reward domain and benchmarks whose difficulty and need for reusable skills directly motivate MaestroMotif\u2019s automated reward design and skill composition approach."
    }
  ],
  "synthesis_narrative": "Policy Sketches showed that textual task sketches can specify a sequence of subskills and enable training reusable subpolicies tied to those symbols. SayCan demonstrated that large language models can select and order skills when their choices are grounded by learned affordance or value estimates, bridging language understanding with RL-trained primitives. Voyager pushed this further by having an LLM write, name, and accumulate skills as executable code, enabling continual skill library growth and reuse in an open-ended environment. Eureka introduced the idea that LLMs can program reward functions and iteratively refine them with RL-in-the-loop feedback, automating reward engineering. Constitutional AI established that AI feedback can substitute for human preference signals, providing a scalable mechanism for critique and refinement without human annotators. Reward Machines formalized decomposable, structured reward specifications for compositional tasks, highlighting the advantage of explicit reward semantics. The NetHack Learning Environment posed a procedurally complex, sparse-reward domain where compositional skills and careful reward design are essential for progress.\n\nTogether, these works reveal a natural opportunity: combine language-specified skills with programmatic reward design and AI feedback, then use LLM-generated code to implement and orchestrate a reusable skill library. MaestroMotif synthesizes these threads by generating per-skill rewards from free-form language via AI feedback (Eureka, Constitutional AI), instantiating skills as code that can be trained and reused (Voyager), and composing them with LLM guidance grounded in learned competence (SayCan), thus realizing language-driven, compositional control in the challenging NLE setting anticipated by Policy Sketches and Reward Machines.",
  "target_paper": {
    "title": "MaestroMotif: Skill Design from Artificial Intelligence Feedback",
    "authors": "Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, Pierre-Luc Bacon, Doina Precup, Marlos C. Machado, Pierluca D'Oro",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Hierarchical RL, Reinforcement Learning, LLMs",
    "abstract": "Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability.",
    "openreview_id": "or8mMhmyRV",
    "forum_id": "or8mMhmyRV"
  },
  "analysis_timestamp": "2026-01-06T07:02:57.048209"
}
{
  "prior_works": [
    {
      "title": "Closed-Form Continuous-Time Neural Networks",
      "authors": "Ramin Hasani et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The proposed method embeds its low-rank-and-sparse recurrent parameterization directly inside CfC cells and grounds its analysis in CfC\u2019s closed-form dynamics, making CfC the architectural base and primary comparator."
    },
    {
      "title": "Liquid Time-Constant Networks",
      "authors": "Ramin Hasani et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "This work established continuous-time recurrent models for robust closed-loop control that CfCs build upon, and the present paper targets the LTC/CfC family\u2019s recurrent connectivity as the lever for improving robustness."
    },
    {
      "title": "Linking connectivity, dynamics and computations in low-rank recurrent neural networks",
      "authors": "Mattia Mastrogiuseppe et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "It provides the central insight that low-rank recurrent connectivity constrains dynamics to low-dimensional manifolds, which the paper leverages by explicitly modulating rank to shape CfC dynamics for robust closed-loop behavior."
    },
    {
      "title": "The 'echo state' approach to analysing and training recurrent neural networks",
      "authors": "Herbert Jaeger",
      "year": 2001,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing that sparse recurrent connectivity with controlled spectral properties stabilizes closed-loop reservoirs, this work motivates adopting sparsity as a structural prior to enhance stability in continuous-time RNNs."
    },
    {
      "title": "Robust Principal Component Analysis?",
      "authors": "Emmanuel J. Cand\u00e8s et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The low-rank-plus-sparse modeling principle from RPCA informs the paper\u2019s parameterization of recurrent connectivity as a combination of global low-dimensional structure and sparse corrections."
    },
    {
      "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning (DAgger)",
      "authors": "St\u00e9phane Ross et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By formalizing compounding error under closed-loop deployment in imitation learning, this work highlights the robustness gap that the paper addresses via structural priors on recurrent connectivity without requiring online data aggregation."
    }
  ],
  "synthesis_narrative": "Low-rank recurrent networks were shown to concentrate dynamics onto low-dimensional manifolds whose geometry and stability can be tuned by the rank of connectivity, revealing a direct handle on computational motifs and robustness (Mastrogiuseppe and Ostojic, 2018). Echo State Networks demonstrated that sparsity and spectral control of the recurrent matrix stabilize closed-loop behavior, establishing that connectivity structure can prevent runaway feedback in deployment (Jaeger, 2001). Beyond neural networks, Robust PCA introduced the modeling idea that matrices can be decomposed into low-rank structure plus sparse deviations to capture global organization with targeted corrections (Cand\u00e8s et al., 2011). In continuous-time control, Liquid Time-Constant Networks framed neurally parameterized ODEs as a robust substrate for closed-loop decision-making, which Closed-Form Continuous-Time Neural Networks then advanced by providing tractable, closed-form state updates that enable analysis and efficient training in feedback settings (Hasani et al., 2020; Hasani et al., 2022). Meanwhile, DAgger formalized the compounding error problem in imitation learning, underscoring the need for robustness under distribution shift during online execution without necessarily relying on interactive data collection (Ross et al., 2011). Together, these works suggest a natural synthesis: impose a low-rank prior to constrain continuous-time RNN dynamics and add sparsity to stabilize feedback and capture critical interactions, instantiated within the analyzable CfC architecture. This combination targets the closed-loop robustness gap identified in imitation learning while leveraging connectivity structure to shape dynamics, yielding an interpretable and parameter-efficient recurrent design that is well-suited for deployment.",
  "target_paper": {
    "title": "Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control",
    "authors": "Neehal Tumma, Mathias Lechner, Noel Loo, Ramin Hasani, Daniela Rus",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Low-rank, sparsity, closed-loop, recurrent neural networks",
    "abstract": "Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback loop within the environment. In this work, we explore the application of recurrent neural networks to tasks of this nature and understand how a parameterization of their recurrent connectivity influences robustness in closed-loop settings. Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can ou",
    "openreview_id": "EriR6Ec69a",
    "forum_id": "EriR6Ec69a"
  },
  "analysis_timestamp": "2026-01-06T11:47:54.823987"
}
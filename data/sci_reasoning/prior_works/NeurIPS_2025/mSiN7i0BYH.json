{
  "prior_works": [
    {
      "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
      "authors": "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer",
      "year": 2015,
      "role": "Exposure-bias mitigation via mixed teacher-forced and self-generated context during training",
      "relationship_sentence": "Self Forcing generalizes the core idea of on-policy conditioning from Scheduled Sampling by fully rolling out model-generated video frames during training rather than intermittently replacing ground-truth inputs."
    },
    {
      "title": "Professor Forcing: A New Algorithm for Training Recurrent Networks",
      "authors": "Alex Lamb, Anirudh Goyal, Saizheng Zhang, Ying Zhang, Aaron Courville, Yoshua Bengio",
      "year": 2016,
      "role": "Aligning train-time and test-time dynamics using sequence-level signals",
      "relationship_sentence": "The paper\u2019s holistic video-level supervision echoes Professor Forcing\u2019s principle of matching training and inference behaviors at the sequence level, but implements it by conditioning on self-rollouts instead of adversarial dynamics matching."
    },
    {
      "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning (DAgger)",
      "authors": "St\u00e9phane Ross, Geoffrey J. Gordon, J. Andrew Bagnell",
      "year": 2011,
      "role": "On-policy rollouts to mitigate covariate shift (exposure bias) in sequential decision making",
      "relationship_sentence": "Self Forcing adopts DAgger\u2019s central insight\u2014training on states visited by the current policy\u2014by supervising diffusion predictions on sequences produced by the model\u2019s own autoregressive rollouts."
    },
    {
      "title": "Sequence Level Training with Recurrent Neural Networks",
      "authors": "Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba",
      "year": 2015,
      "role": "Optimizing sequence-level objectives (MIXER) to better reflect end-task quality",
      "relationship_sentence": "The paper\u2019s video-level loss directly evaluating full rollouts parallels MIXER\u2019s shift from token/frame-wise losses to sequence-level objectives to reduce train\u2013test mismatch."
    },
    {
      "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
      "authors": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",
      "year": 2019,
      "role": "Segment-level recurrence and memory (rolling cache) enabling long-context autoregression with truncated backprop",
      "relationship_sentence": "Self Forcing\u2019s rolling KV cache and stochastic gradient truncation draw directly on Transformer-XL\u2019s idea of reusing past keys/values as memory to efficiently train on long autoregressive sequences."
    },
    {
      "title": "Progressive Distillation for Fast Sampling of Diffusion Models",
      "authors": "Tim Salimans, Jonathan Ho",
      "year": 2022,
      "role": "Technique to reduce diffusion sampling to a few steps while retaining quality",
      "relationship_sentence": "The use of a few-step diffusion model to make on-policy rollouts computationally feasible is enabled by progressive distillation-style acceleration of diffusion sampling."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
      "year": 2020,
      "role": "Foundational diffusion modeling framework and training objective",
      "relationship_sentence": "Self Forcing builds on DDPM\u2019s denoising-based training but departs from frame-wise teacher forcing by conditioning each predicted frame on previously self-generated outputs and supervising with a video-level loss."
    }
  ],
  "synthesis_narrative": "Self Forcing targets the core weakness of teacher-forced video diffusion\u2014exposure bias\u2014by training under the same autoregressive conditions faced at inference. This on-policy orientation directly traces to three seminal strands: Scheduled Sampling introduced mixing model outputs into training inputs; DAgger formalized training on states induced by the model\u2019s own policy to counter covariate shift; and Professor Forcing advocated sequence-level alignment of train and test dynamics. Building on these, Self Forcing fully rolls out self-generated context during training and evaluates with a video-level objective, echoing the sequence-level emphasis of Ranzato et al. (MIXER) but in a diffusion setting.\n\nRealizing on-policy training for video diffusion demands efficiency mechanisms. Transformer-XL\u2019s memory-based segment recurrence provides the conceptual and practical foundation for the paper\u2019s rolling KV cache and stochastic gradient truncation, enabling long-horizon rollouts with manageable backpropagation. To keep rollouts affordable within diffusion, the work leverages advances in few-step diffusion, with progressive distillation furnishing a concrete pathway to reduce sampling steps while preserving fidelity. All of this builds upon the DDPM framework, whose denoising objective underlies the model\u2019s per-step predictions while Self Forcing reshapes the conditioning regime and supervision granularity from frame-wise to video-level. Together, these prior works directly enable Self Forcing\u2019s central contribution: efficient, on-policy, video-level training for autoregressive video diffusion that bridges the train\u2013test gap.",
  "analysis_timestamp": "2026-01-07T00:21:32.354099"
}
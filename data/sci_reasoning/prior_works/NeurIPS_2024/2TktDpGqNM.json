{
  "prior_works": [
    {
      "title": "On optimum recognition error and reject trade-off",
      "authors": "C. K. Chow",
      "year": 1970,
      "role": "Foundational theory of classification with a reject option, formalizing the core risk\u2013reject trade-off that underlies selective classification evaluation.",
      "relationship_sentence": "The proposed AUGRC metric directly builds on Chow\u2019s formulation of abstention as a principled way to reduce risk, reframing evaluation as integrating risk over varying rejection levels."
    },
    {
      "title": "Classification with reject option",
      "authors": "Radu Herbei, Marten H. Wegkamp",
      "year": 2006,
      "role": "Modern statistical foundations for reject-option classification, clarifying optimal decision rules and the mathematics of coverage and risk.",
      "relationship_sentence": "AUGRC\u2019s requirements on task alignment and interpretability follow the Herbei\u2013Wegkamp perspective that evaluation must reflect the optimality conditions of reject decisions induced by posterior thresholds."
    },
    {
      "title": "On the Foundations of Selective Classification",
      "authors": "Ran El-Yaniv, Yoav Wiener",
      "year": 2010,
      "role": "Formalized selective classification as a learning paradigm with definitions of coverage, selective risk, and consistency guarantees.",
      "relationship_sentence": "The paper\u2019s notion of averaging \u2018risk of undetected failures\u2019 across coverages is a direct operationalization of the selective risk\u2013coverage framework laid out by El\u2011Yaniv and Wiener."
    },
    {
      "title": "Selective Classification for Deep Neural Networks",
      "authors": "Yonatan Geifman, Ran El-Yaniv",
      "year": 2017,
      "role": "Introduced the risk\u2013coverage (RC) curve for deep nets and popularized AURC as a multi-threshold metric for selective classification.",
      "relationship_sentence": "AUGRC explicitly addresses shortcomings of AURC/RC highlighted in practice, generalizing the RC-based evaluation while preserving its alignment with selective decision-making."
    },
    {
      "title": "SelectiveNet: A Deep Neural Network with an Integrated Reject Option",
      "authors": "Yonatan Geifman, Ran El-Yaniv",
      "year": 2019,
      "role": "Methodologically integrated selection and prediction, evaluated primarily via RC curves and AURC, cementing these as de facto metrics.",
      "relationship_sentence": "By showing where SelectiveNet-era metrics fail the proposed requirements, the paper motivates AUGRC as a drop-in, better-aligned alternative for benchmarking selective systems."
    },
    {
      "title": "To Trust or Not To Trust A Classifier",
      "authors": "Heinrich Jiang, Been Kim, Melody Guan, Maya Gupta",
      "year": 2018,
      "role": "Framed misclassification detection and confidence scoring, evaluated with AUROC/AUPR across thresholds.",
      "relationship_sentence": "The new metric is positioned as the selective\u2011classification analogue of AUROC, correcting the mismatch when AUROC is applied to failure detection without accounting for abstention costs."
    },
    {
      "title": "Addressing Failure Prediction by Learning Model Confidence",
      "authors": "Charles Corbi\u00e8re, Nicolas Thome, Alain Bar-Hen, Matthieu Cord, Patrick P\u00e9rez",
      "year": 2019,
      "role": "Advanced failure prediction with confidence learning and evaluated with AUROC/AUPR and RC-based measures.",
      "relationship_sentence": "AUGRC responds to the evaluation inconsistencies exposed by failure-prediction work, unifying multi-threshold assessment around risk-of-undeclared errors rather than proxy metrics."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014AUGRC as an interpretable, multi\u2011threshold metric for selective classification\u2014traces a direct lineage from the classical reject\u2011option literature through modern deep learning practice. Chow\u2019s seminal formulation of the error\u2013reject trade\u2011off defined abstention as a principled risk control mechanism, later refined statistically by Herbei and Wegkamp, who formalized optimal rejection in terms of posterior thresholds and thus the notions of coverage and risk that evaluation should honor. El\u2011Yaniv and Wiener then crystallized selective classification as a learning paradigm, providing the risk\u2013coverage vocabulary and consistency goals that the present work explicitly adopts in its requirements for task alignment and interpretability.\nIn the deep learning era, Geifman and El\u2011Yaniv operationalized these ideas with risk\u2013coverage (RC) curves and AURC, which became standard for benchmarking selective systems, especially with SelectiveNet integrating prediction and selection. However, concurrent lines in failure prediction and confidence estimation\u2014exemplified by Jiang et al. and Corbi\u00e8re et al.\u2014popularized AUROC/AUPR for error detection, introducing evaluation practices that can be misaligned with abstention decisions and costs. The present paper synthesizes these streams, diagnosing where AURC and AUROC-based assessments violate desiderata for multi\u2011threshold evaluation. AUGRC generalizes RC-based metrics and yields a clear interpretation as average risk of undetected failures over coverage, thereby meeting the stated requirements while remaining comparable across models, datasets, and confidence mechanisms.",
  "analysis_timestamp": "2026-01-06T23:33:36.254569"
}
{
  "prior_works": [
    {
      "title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
      "authors": "Antoine Miech et al.",
      "year": 2019,
      "arxiv_id": "1906.03327",
      "role": "Foundation",
      "relationship_sentence": "HowTo100M established the web-scale video\u2013text pretraining paradigm using weak ASR transcripts, whose noisy and weakly grounded supervision InternVid directly addresses by replacing metadata/transcripts with LLM-generated, temporally granular descriptions."
    },
    {
      "title": "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
      "authors": "Max Bain et al.",
      "year": 2021,
      "arxiv_id": "2104.00650",
      "role": "Baseline",
      "relationship_sentence": "Frozen in Time introduced the WebVid scraping recipe showing that short web video titles/descriptions suffice for contrastive pretraining, and InternVid explicitly builds on this scalable harvesting pipeline while overcoming its sparse captions via LLM-written multi-scale descriptions and much larger coverage."
    },
    {
      "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
      "authors": "Christoph Schuhmann et al.",
      "year": 2022,
      "arxiv_id": "2210.08402",
      "role": "Inspiration",
      "relationship_sentence": "LAION-5B demonstrated autonomous web-scale curation with automatic filtering, deduplication, and quality control, which InternVid extends to the video domain and augments by synthesizing richer text using LLMs rather than relying solely on raw web metadata."
    },
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Inspiration",
      "relationship_sentence": "LLaVA showed that prompting LLMs with visual inputs can transform sparse captions into rich, instruction-like descriptions, directly motivating InternVid\u2019s use of LLM prompting to generate detailed video descriptions at multiple temporal scales."
    },
    {
      "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
      "authors": "Hassan Maaz et al.",
      "year": 2023,
      "arxiv_id": "2306.05424",
      "role": "Related Problem",
      "relationship_sentence": "Video-ChatGPT pioneered leveraging LLMs to synthesize dense supervision for videos (QA/instructions) from frames/transcripts, which InternVid adapts by producing descriptive, multi-granularity captions instead of QA to enable large-scale video\u2013text training."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "arxiv_id": "2103.00020",
      "role": "Foundation",
      "relationship_sentence": "CLIP introduced the contrastive image\u2013text objective and zero-shot evaluation protocol that InternVid\u2019s ViCLIP directly adopts and extends to video, aligning the dataset design with contrastive video\u2013text representation learning."
    }
  ],
  "synthesis_narrative": "Large-scale video\u2013text learning emerged with HowTo100M, which paired web instructional videos with ASR transcripts to train video\u2013language embeddings despite noisy, weak alignments. Frozen in Time operationalized a scalable web scraping recipe (WebVid) showing that short titles/descriptions paired with videos suffice for strong retrieval when used with contrastive pretraining. Meanwhile, LAION-5B proved that autonomous web-scale curation\u2014crawling, deduplication, and automatic quality filters\u2014can yield high-quality multimodal data without manual labeling. On the modeling side, CLIP established the contrastive language\u2013vision objective and zero-shot evaluation paradigm that many video\u2013text methods inherit. More recently, LLaVA demonstrated that prompting LLMs with visual cues can synthesize rich, instruction-like descriptions from sparse web captions, and Video-ChatGPT extended this LLM-in-the-loop annotation idea to videos by generating detailed QA-style supervision from frames/transcripts.\nTogether, these works suggested both the opportunity and the bottlenecks: web scraping achieves massive scale but produces sparse or noisy text, while LLM prompting can densify and structure supervision. Building on the web-scale harvesting and filtering practices of WebVid/LAION and the CLIP contrastive training recipe, the logical next step was to replace weak metadata with LLM-authored, temporally aware text. InternVid synthesizes these insights by prompting LLMs at multiple temporal granularities to generate high-quality video descriptions, aligning the data with CLIP-style objectives and enabling ViCLIP to achieve strong zero-shot understanding and retrieval at unprecedented scale.",
  "target_paper": {
    "title": "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation",
    "authors": "Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "video-language dataset, video understanding, video generation, multimodal understanding, action recognition, video retrieval",
    "abstract": "This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. InternVid contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset a",
    "openreview_id": "MLBdiWu4Fw",
    "forum_id": "MLBdiWu4Fw"
  },
  "analysis_timestamp": "2026-01-06T12:31:58.537910"
}
{
  "prior_works": [
    {
      "title": "Unsupervised Image-to-Image Translation Networks",
      "authors": "Ming-Yu Liu et al.",
      "year": 2017,
      "arxiv_id": "1703.00848",
      "role": "Inspiration",
      "relationship_sentence": "The shared-latent-space assumption in UNIT directly inspires the paper\u2019s idea of aligning heterogeneous daytime RGB and event domains with nighttime via a common latent representation to bridge domain gaps."
    },
    {
      "title": "Multimodal Unsupervised Image-to-Image Translation",
      "authors": "Xun Huang et al.",
      "year": 2018,
      "arxiv_id": "1804.04732",
      "role": "Inspiration",
      "relationship_sentence": "MUNIT\u2019s decomposition into a domain-invariant content space and domain-specific style provides the conceptual template for separating illumination-invariant appearance and domain-specific factors when building the paper\u2019s common appearance space."
    },
    {
      "title": "Learning Intrinsic Image Decomposition from Watching the World",
      "authors": "Zhengqi Li et al.",
      "year": 2018,
      "arxiv_id": "1804.10648",
      "role": "Foundation",
      "relationship_sentence": "This work supplies a practical intrinsic image decomposition framework (reflectance/shading) that the paper leverages to embed auxiliary domains into an illumination-invariant appearance representation for adaptation to nighttime."
    },
    {
      "title": "EV-FlowNet: Self-Supervised Learning of Optical Flow from Event Cameras",
      "authors": "Alex Zhu et al.",
      "year": 2018,
      "arxiv_id": "1802.06898",
      "role": "Foundation",
      "relationship_sentence": "EV-FlowNet established that event streams capture sharp motion edges in low light, which the paper exploits by treating events as an auxiliary boundary-rich domain for guiding nighttime flow."
    },
    {
      "title": "E-RAFT: Dense Optical Flow from Event Cameras",
      "authors": "Dominik Gehrig et al.",
      "year": 2021,
      "arxiv_id": "unknown",
      "role": "Related Problem",
      "relationship_sentence": "E-RAFT shows how RAFT-style correlational matching can be adapted to event data, enabling the paper to import event-domain flow priors and boundary cues compatible with a RAFT-based pipeline."
    },
    {
      "title": "EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow",
      "authors": "J\u00e9r\u00f4me Revaud et al.",
      "year": 2015,
      "arxiv_id": "1501.02565",
      "role": "Inspiration",
      "relationship_sentence": "EpicFlow\u2019s central insight that accurate image boundaries are critical guidance for dense flow directly motivates the paper\u2019s boundary-centric adaptation branch as a domain-invariant cue."
    },
    {
      "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
      "authors": "Zachary Teed et al.",
      "year": 2020,
      "arxiv_id": "2003.12039",
      "role": "Baseline",
      "relationship_sentence": "The paper builds its adaptation framework on a RAFT-style flow backbone and positions improvements against RAFT as the principal baseline for nighttime performance."
    }
  ],
  "synthesis_narrative": "A shared-latent representation across heterogeneous domains was crystallized by UNIT, which posited that disparate inputs can be mapped to a common space to mitigate domain shifts, later refined by MUNIT\u2019s split into domain-invariant content and domain-specific style. Practical routes to illumination invariance came from intrinsic image decomposition, where Li and Snavely demonstrated that reflectance and shading can be disentangled from single images and used as stable, illumination-agnostic appearance cues. On the motion side, EV-FlowNet established that event streams remain reliable under low light and densely encode motion edges, while E-RAFT showed how RAFT\u2019s correlation volumes and updates can be adapted to event inputs, making event-derived priors architecturally compatible with modern dense flow. Long before deep models, EpicFlow highlighted the primacy of precise boundaries for accurate flow interpolation, elevating edges as a robust guidance signal when appearance degrades. RAFT then provided a strong, modular flow backbone onto which such cues can be injected and evaluated. Together, these works revealed that direct adaptation in pixel or flow output space struggles with severe illumination shifts, while two robust anchors exist: illumination-invariant appearance via intrinsic decomposition and domain-stable motion boundaries from events. The natural next step was to synthesize these anchors in a common latent space atop a RAFT-style estimator\u2014aligning daytime RGB and event cues into an appearance-boundary representation that transfers reliably to nighttime and directly addresses the heterogeneous-domain gap.",
  "target_paper": {
    "title": "Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow",
    "authors": "Hanyu Zhou, Yi Chang, Haoyue Liu, YAN WENDING, Yuxing Duan, Zhiwei Shi, Luxin Yan",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "nighttime optical flow, event camera, domain adaptation, common space",
    "abstract": "We investigate a challenging task of nighttime optical flow, which suffers from weakened texture and amplified noise. These degradations weaken discriminative visual features, thus causing invalid motion feature matching. Typically, existing methods employ domain adaptation to transfer knowledge from auxiliary domain to nighttime domain in either input visual space or output motion space. However, this direct adaptation is ineffective, since there exists a large domain gap due to the intrinsic heterogeneous nature of the feature representations between auxiliary and nighttime domains. To overcome this issue, we explore a common-latent space as the intermediate bridge to reinforce the feature alignment between auxiliary and nighttime domains. In this work, we exploit two auxiliary daytime and event domains, and propose a novel common appearance-boundary adaptation framework for nighttime optical flow. In appearance adaptation, we employ the intrinsic image decomposition to embed the aux",
    "openreview_id": "776lhoaulC",
    "forum_id": "776lhoaulC"
  },
  "analysis_timestamp": "2026-01-06T13:02:31.393437"
}
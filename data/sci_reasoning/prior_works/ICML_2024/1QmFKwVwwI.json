{
  "prior_works": [
    {
      "title": "Adaptive Treatment Assignment in Experiments for Policy Learning and Causal Inference",
      "authors": "Kasy et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "This paper formally frames adaptive experimental assignment as a joint welfare\u2013inference design problem; the ICML\u201924 paper builds directly on this formulation and extends it to contextual (CATE-focused) settings with tight Pareto characterizations and privacy constraints."
    },
    {
      "title": "Balanced Linear Contextual Bandits",
      "authors": "Dimakopoulou et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "By showing how adaptive assignments bias effect estimation and proposing propensity-weighted estimation within contextual bandits, this work directly motivates the ICML\u201924 paper\u2019s explicit treatment of the regret\u2013CATE-accuracy tradeoff in adaptive experiment design."
    },
    {
      "title": "Confidence intervals for policy evaluation in adaptive experiments",
      "authors": "Hadad et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "This paper highlights validity and power challenges caused by adaptivity, identifying a gap the ICML\u201924 paper addresses by quantifying optimal regret\u2013statistical-power tradeoffs and designing allocation mechanisms that target CATE accuracy."
    },
    {
      "title": "Online Decision-Making with High-Dimensional Covariates",
      "authors": "Bastani et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Greedy and contextual bandit policies from this work serve as practical welfare-focused baselines that sacrifice estimation quality; the ICML\u201924 paper benchmarks against and theoretically improves upon such baselines via a Pareto-optimal design for regret vs. CATE estimation."
    },
    {
      "title": "The Algorithmic Foundations of Differential Privacy",
      "authors": "Dwork et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "This monograph provides the formal differential privacy framework and composition tools that the ICML\u201924 paper adopts to design and analyze privacy-preserving adaptive allocation rules."
    },
    {
      "title": "Algorithms for Differentially Private Multi-Armed Bandits",
      "authors": "Tossou et al.",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "As an early study of DP in bandit learning, this work\u2019s noise-calibration approach to preserve privacy while controlling regret directly informs the ICML\u201924 paper\u2019s integration of DP into adaptive experimental design."
    },
    {
      "title": "Differentially Private Contextual Linear Bandits",
      "authors": "Shariff et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "The ICML\u201924 paper extends the DP contextual-bandit machinery developed here\u2014private regression/UCB style mechanisms\u2014from single-objective regret minimization to a two-objective design that also targets CATE estimation power with matched upper and lower bounds."
    }
  ],
  "synthesis_narrative": "The core innovation in Privacy Preserving Adaptive Experiment Design fuses two lines of work: adaptive assignment for statistical learning of treatment effects and rigorous differential privacy in online learning. On the experimental design side, Kasy and Sautmann crystallized adaptive treatment assignment as a welfare\u2013inference tradeoff, directly framing the problem the authors pursue but in a non-private, largely average-effect setting. Dimakopoulou et al. showed that contextual bandit adaptivity distorts effect estimation and proposed propensity-weighted estimation to combat bias\u2014insight that motivates explicitly targeting CATE accuracy rather than only regret. Hadad et al. further identified validity and power deficits in adaptive experiments, sharpening the gap the present work addresses by characterizing a Pareto frontier and proving matched upper/lower bounds for regret versus CATE-estimation power. As practical welfare-first baselines, Bastani and Bayati\u2019s greedy/contextual policies highlight how low-regret assignment can harm learning, providing concrete comparators the new design improves upon. On the privacy side, the work rests on Dwork and Roth\u2019s differential privacy foundations to formalize protection guarantees under sequential adaptivity. Early DP bandit methods by Tossou and Dimitrakakis demonstrate how to inject calibrated noise while controlling regret, and Shariff and Sheffet\u2019s DP contextual linear bandits supply the technical toolkit for privatizing contextual learning. The ICML\u201924 paper integrates these strands, extending DP contextual bandit mechanisms to a bi-objective setting and delivering tight Pareto characterizations that quantify the price of privacy and welfare for CATE estimation.",
  "analysis_timestamp": "2026-01-06T23:09:26.443537"
}
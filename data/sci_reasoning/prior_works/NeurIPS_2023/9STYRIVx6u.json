{
  "prior_works": [
    {
      "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport",
      "authors": "L\u00e9na\u00efc Chizat, Francis Bach",
      "year": 2018,
      "role": "Wasserstein gradient-flow formulation of mean-field training",
      "relationship_sentence": "Established the measure-valued (Wasserstein) gradient-flow view for two-layer networks and displacement-convexity conditions, providing the variational framework that MFLD leverages to interpret noisy training as optimizing a convex functional over probability measures."
    },
    {
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "authors": "Song Mei, Andrea Montanari, Phan-Minh Nguyen",
      "year": 2018,
      "role": "Mean-field limit and PDE/SDE descriptions for neural network training",
      "relationship_sentence": "Derived the McKean\u2013Vlasov (mean-field) dynamics for two-layer networks, linking parameter gradient flows (and with noise, MFLD) to population-level PDE/SDE limits that underpin the paper\u2019s finite-particle approximation analysis."
    },
    {
      "title": "Convergence to equilibrium for granular media equations and their particle approximations",
      "authors": "Cl\u00e9ment Malrieu",
      "year": 2003,
      "role": "Uniform-in-time contraction and particle approximation for McKean\u2013Vlasov dynamics",
      "relationship_sentence": "Provided foundational uniform-in-time stability and propagation-of-chaos tools for McKean\u2013Vlasov diffusions with convexity/contractivity, which the paper extends to distribution-dependent MFLD with optimization structure."
    },
    {
      "title": "Coupling and Convergence for McKean\u2013Vlasov SDEs",
      "authors": "Andreas Eberle, Arnaud Guillin, Raphael Zimmer",
      "year": 2019,
      "role": "Quantitative coupling for uniform-in-time bounds in mean-field diffusions",
      "relationship_sentence": "Developed coupling techniques yielding quantitative, time-uniform convergence/contraction for McKean\u2013Vlasov SDEs, informing the paper\u2019s uniform-in-time propagation-of-chaos rates across particle, time-discretization, and noise errors."
    },
    {
      "title": "Bayesian Learning via Stochastic Gradient Langevin Dynamics",
      "authors": "Max Welling, Yee Whye Teh",
      "year": 2011,
      "role": "Stochastic gradient Langevin dynamics (SGLD) with minibatch noise",
      "relationship_sentence": "Introduced SGLD, the canonical stochastic-gradient-driven Langevin scheme; the present paper builds on this paradigm to handle stochastic gradient updates within mean-field Langevin dynamics and quantify their impact."
    },
    {
      "title": "Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis",
      "authors": "Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky",
      "year": 2017,
      "role": "Nonasymptotic convergence and discretization analysis for (S)GLD",
      "relationship_sentence": "Provided Lyapunov and functional-inequality tools to control discretization bias and gradient noise in Langevin methods, techniques adapted to MFLD to obtain quantitative, time-uniform bounds under stochastic gradients."
    },
    {
      "title": "Accelerating Stochastic Gradient Descent using Predictive Variance Reduction (SVRG)",
      "authors": "Rie Johnson, Tong Zhang",
      "year": 2013,
      "role": "Variance-reduced stochastic gradient methodology",
      "relationship_sentence": "Supplied the variance-reduction blueprint (SVRG) that the paper instantiates in the mean-field setting to reduce gradient noise and sharpen convergence rates for discretized MFLD."
    }
  ],
  "synthesis_narrative": "Suzuki, Wu, and Nitanda target a central gap: prior analyses of mean-field Langevin dynamics (MFLD) largely operated in idealized infinite-particle and continuous-time regimes, leaving open quantitative, uniform-in-time guarantees that simultaneously account for finite-particle approximation, time discretization, and stochastic (minibatch) gradients. Two strands of work directly set the stage. First, the mean-field optimization viewpoint for two-layer networks\u2014formalized by Chizat and Bach via Wasserstein gradient flows on probability measures and by Mei\u2013Montanari\u2013Nguyen via McKean\u2013Vlasov limits\u2014established that noisy training corresponds to gradient flows of a risk functional augmented by entropy, yielding convexity/displacement-convexity structures exploitable for global convergence. Second, uniform-in-time control tools for McKean\u2013Vlasov SDEs\u2014originating with Malrieu\u2019s analysis of granular media and extended via coupling methods by Eberle\u2013Guillin\u2013Zimmer\u2014provided the quantitative contraction and propagation-of-chaos machinery necessary to pass from the infinite-population dynamics to finite-particle systems with controlled, time-uniform errors.\nOn the algorithmic side, the stochastic-gradient lens introduced by Welling and Teh\u2019s SGLD and the nonasymptotic discretization/gradient-noise analyses of Raginsky\u2013Rakhlin\u2013Telgarsky inform how minibatch noise perturbs Langevin-type dynamics, guiding bias\u2013variance tradeoffs in discrete time. Finally, variance-reduction methods exemplified by SVRG (Johnson\u2013Zhang) supply a concrete mechanism to tame gradient stochasticity; the paper adapts these to the mean-field setting, yielding sharper, uniform-in-time convergence guarantees to the entropy-regularized global optimum across learning tasks such as mean-field neural networks and MMD minimization.",
  "analysis_timestamp": "2026-01-06T23:42:49.058247"
}
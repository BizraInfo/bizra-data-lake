{
  "prior_works": [
    {
      "title": "Learning to Summarize from Human Feedback",
      "authors": "Nisan Stiennon et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "This work established the modern pairwise-preference RLHF formulation that our paper both evaluates for logical preference consistency and improves via REPAIR by refining the very comparison data it relies on."
    },
    {
      "title": "Training Language Models to Follow Instructions with Human Feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "By scaling instruction-following with RLHF, this paper cemented preference alignment as the central training paradigm, within which we identify and formalize logical preference consistency (transitivity, commutativity, negation invariance) as a missing alignment criterion."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model Is Secretly a Reward Model",
      "authors": "Alexander M. Rafailov et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "DPO is a primary baseline we improve upon; our REPAIR procedure is applied to DPO\u2019s pairwise data to mitigate its sensitivity to inconsistent and intransitive labels, directly boosting logical preference consistency without changing the DPO objective."
    },
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "This work popularized large-scale preference datasets and the helpful/harmless alignment objective that our method preserves, while we explicitly target and reduce logical inconsistencies that arise within such RLHF-trained systems."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "The finding that enforcing and aggregating consistency improves reasoning directly inspired our central hypothesis and experiments that higher logical preference consistency correlates with and yields better downstream decision-making performance."
    },
    {
      "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "By revealing order/position bias and instability in LLM-based judgments under swapped candidate order, this paper directly motivated our commutativity criterion and the order-balancing augmentation in REPAIR."
    },
    {
      "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly?",
      "authors": "Nora Kassner and Hinrich Sch\u00fctze",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Their demonstration that LMs are brittle under negation directly motivated our negation invariance property and the negation-based augmentation component of REPAIR for preference judgments."
    }
  ],
  "synthesis_narrative": "The core innovation of this paper is to make preference alignment logically consistent by measuring and enforcing three axioms\u2014transitivity, commutativity, and negation invariance\u2014while preserving alignment with human preferences. This trajectory begins with RLHF as the dominant paradigm for aligning LMs with human judgments (Stiennon et al., 2020; Ouyang et al., 2022) and the emergence of widely used preference datasets and objectives (Bai et al., 2022). Building atop this foundation, DPO (Rafailov et al., 2023) reframed preference optimization into a simpler supervised objective, but left a practical vulnerability: sensitivity to noisy, inconsistent, and intransitive pairwise labels. Concurrently, evaluation research exposed systemic order and position biases when LLMs act as judges (Zheng et al., 2023), directly motivating our commutativity criterion and order-balancing refinements. From the robustness literature, negation brittleness (Kassner & Sch\u00fctze, 2020) inspired our negation invariance property and negation-based augmentation. Finally, the success of self-consistency in reasoning (Wang et al., 2023) informed our central claim that consistency is not merely a nicety but a predictor of reliability and performance, guiding both our measurement framework and REPAIR\u2019s design. Together, these works define the preference-learning problem, expose concrete failure modes (intransitivity, order bias, negation brittleness), and provide the methodological baselines (especially DPO) that our evaluation framework and REPAIR directly improve.",
  "analysis_timestamp": "2026-01-06T23:07:19.604290"
}
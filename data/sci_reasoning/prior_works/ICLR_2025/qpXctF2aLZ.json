{
  "prior_works": [
    {
      "title": "Proximal Policy Optimization Algorithms",
      "authors": "John Schulman et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "SYMPOL plugs an axis-aligned, differentiable decision-tree policy into PPO\u2019s clipped surrogate objective, directly leveraging PPO\u2019s on-policy policy-gradient framework to optimize symbolic tree parameters end-to-end."
    },
    {
      "title": "Reinforced Decision Trees",
      "authors": "Mohammad Norouzi et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "SYMPOL adopts the core idea of treating internal tree routing as stochastic and training splits with policy gradients, extending this principle from supervised decision-tree training to sequential RL and integrating it with on-policy optimization."
    },
    {
      "title": "Deep Neural Decision Forests",
      "authors": "Peter Kontschieder et al.",
      "year": 2015,
      "role": "Extension",
      "relationship_sentence": "SYMPOL extends soft, differentiable split functions from neural decision forests to an RL setting, adapting them to interpretable, axis-aligned trees trained by policy gradients rather than supervised losses."
    },
    {
      "title": "Distilling a Neural Network Into a Soft Decision Tree",
      "authors": "Nicholas Frosst et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "SYMPOL explicitly avoids the two-stage distillation paradigm exemplified by soft decision trees, addressing the information loss from imitating a black-box teacher by directly optimizing the tree with RL returns."
    },
    {
      "title": "Verifiable Reinforcement Learning via Policy Extraction (VIPER)",
      "authors": "Osbert Bastani et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "VIPER is a primary tree-policy baseline that extracts decision trees from neural policies via imitation, and SYMPOL improves on it by removing the extract-then-imitate step and optimizing the symbolic policy directly on-policy to mitigate information loss."
    },
    {
      "title": "Programmatically Interpretable Reinforcement Learning",
      "authors": "Abhinav Verma et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "PIRL shares the goal of interpretable policies but relies on non-differentiable program synthesis, a limitation SYMPOL sidesteps by providing a differentiable, gradient-optimized symbolic (tree) policy within standard on-policy RL."
    },
    {
      "title": "Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data",
      "authors": "Sergei Popov et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "SYMPOL draws on the effectiveness of end-to-end gradient training of tree ensembles with soft gating from NODE, adapting the idea to axis-aligned, interpretable trees and optimizing them with policy-gradient signals in RL."
    }
  ],
  "synthesis_narrative": "SYMPOL\u2019s core innovation\u2014end-to-end, on-policy optimization of interpretable, axis-aligned decision-tree policies\u2014emerges at the intersection of policy-gradient RL and differentiable decision-tree learning. PPO provides the foundational on-policy learning scaffold and clipped surrogate objective into which SYMPOL embeds a symbolic tree, enabling stable gradient-based updates of split thresholds and leaf action distributions. From the decision-tree side, soft, differentiable routing popularized by Deep Neural Decision Forests and later refined in NODE demonstrates how to parameterize splits for gradient descent; SYMPOL repurposes these ideas for RL objectives while maintaining strict axis alignment for interpretability. Reinforced Decision Trees contributes the crucial insight that tree routing can be treated as a stochastic policy and trained with policy gradients, an idea SYMPOL generalizes from supervised prediction to sequential decision-making with PPO. The paper directly responds to a prominent limitation in interpretable RL: methods like Frosst & Hinton\u2019s soft-tree distillation and VIPER\u2019s policy extraction achieve interpretability but incur information loss by imitating a separate neural teacher. By training the tree policy directly with returns rather than mimicking a black box, SYMPOL eliminates this bottleneck. Finally, compared to program-synthesis approaches such as PIRL\u2014which achieve interpretability via non-differentiable search\u2014SYMPOL preserves interpretability while remaining fully differentiable and compatible with standard on-policy RL pipelines.",
  "analysis_timestamp": "2026-01-06T23:08:23.927755"
}
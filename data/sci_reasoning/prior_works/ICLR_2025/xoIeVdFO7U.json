{
  "prior_works": [
    {
      "title": "Diversity is All You Need: Learning Skills without a Reward Function (DIAYN)",
      "authors": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine",
      "year": 2019,
      "role": "Foundational MISL method",
      "relationship_sentence": "DIAYN formalized mutual-information\u2013based skill discovery (maximizing I(s; z) via a discriminator), providing the MISL template that the paper analyzes and upgrades with contrastive successor features."
    },
    {
      "title": "Variational Intrinsic Control",
      "authors": "Karol Gregor, Danilo J. Rezende, Daan Wierstra",
      "year": 2016,
      "role": "Early MI skill-learning formulation",
      "relationship_sentence": "VIC introduced the variational MI objective linking latent skills and controllable state changes, a conceptual precursor to MISL that underpins the paper\u2019s MI-centric reinterpretation."
    },
    {
      "title": "Variational Option Discovery Algorithms (VALOR)",
      "authors": "Joshua Achiam, Harrison Edwards, Pieter Abbeel",
      "year": 2018,
      "role": "MISL refinement via variational inference",
      "relationship_sentence": "VALOR operationalized MI-driven option discovery with explicit priors and posteriors, informing the paper\u2019s analysis of design choices (priors, decoders) within the MISL framework."
    },
    {
      "title": "Contrastive Predictive Coding (CPC): Representation Learning with Mutual Information Estimation",
      "authors": "Aaron van den Oord, Yazhe Li, Oriol Vinyals",
      "year": 2018,
      "role": "Contrastive MI estimator (InfoNCE)",
      "relationship_sentence": "CPC\u2019s InfoNCE objective provides the contrastive machinery the paper leverages to replace classifier-based MI estimation, enabling the proposed contrastive successor features objective."
    },
    {
      "title": "Improving Transfer in Reinforcement Learning with Successor Features",
      "authors": "Andr\u00e9 Barreto, R\u00e9mi Munos, Tom Schaul, David Silver",
      "year": 2017,
      "role": "Successor features for value decomposition and transfer",
      "relationship_sentence": "This work\u2019s successor-features formulation (\u03c8 for expected feature occupancies) directly informs the paper\u2019s contrastive successor features, linking skill discovery to value-predictive structure."
    },
    {
      "title": "Improving Generalization for Temporal Difference Learning: The Successor Representation",
      "authors": "Peter Dayan",
      "year": 1993,
      "role": "Foundational successor representation",
      "relationship_sentence": "Dayan\u2019s successor representation provides the theoretical backbone for using multi-step occupancy predictions, a core ingredient in the paper\u2019s CSF objective and analysis."
    },
    {
      "title": "METRA: Skill Discovery by Maximizing a Wasserstein Distance (ICLR 2024)",
      "authors": "Anonymous (ICLR 2024)",
      "year": 2024,
      "role": "Alternative to MI using Wasserstein distance",
      "relationship_sentence": "METRA\u2019s claim that Wasserstein is superior to MI is the focal comparator; the paper shows METRA\u2019s benefits can be recovered within MISL and motivates the CSF design and ablations."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014showing that mutual-information skill learning (MISL) can match the performance of Wasserstein-based METRA and proposing contrastive successor features (CSF)\u2014stands on three intertwined lines of prior work. First, VIC and DIAYN established the MISL paradigm: maximize I(s; z) between states and latent skills using variational bounds and discriminative decoding. VALOR refined this view with explicit priors/posteriors and clarified practical design choices, giving the authors a principled MISL lens through which to reinterpret METRA\u2019s gains. Second, contrastive learning via CPC (InfoNCE) provided a high-signal, stable MI estimator that replaces classifier-style decoders. This directly enables the paper\u2019s move from likelihood-based skill discriminators to contrastive objectives that better separate skills in representation space. Third, successor representations\u2014originating with Dayan and extended by Barreto et al. to successor features for transfer\u2014offered a value-predictive structure that captures multi-step state occupancies. By marrying InfoNCE-style contrastive estimation with successor features, CSF aligns skill discovery with dynamics-aware, temporally extended predictions, yielding MISL with fewer moving parts yet strong empirical performance. METRA serves as the catalyst and benchmark: by dissecting which ingredients (contrastive estimation, representation choice via SF, priors/regularization) actually drive performance, the paper demonstrates these can be realized within MISL, thereby unifying skill discovery, contrastive representation learning, and successor features under a simpler, analytically grounded framework.",
  "analysis_timestamp": "2026-01-06T23:42:48.083578"
}
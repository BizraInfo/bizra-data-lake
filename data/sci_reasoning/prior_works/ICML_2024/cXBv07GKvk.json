{
  "prior_works": [
    {
      "title": "Variational Online Newton",
      "authors": "Gian Maria Marconi et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "IVON is a direct improvement over VON, keeping the same variational online-Newton formulation but introducing refinements that make it as fast as Adam while remaining stable and effective on very large networks."
    },
    {
      "title": "VOGN: Variational Online Gauss\u2013Newton",
      "authors": "Wu Lin et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "VOGN introduced the core idea of online variational learning with diagonal Gaussian posteriors using Gauss\u2013Newton/Fisher curvature, which IVON extends toward Adam-level efficiency and modern large-scale settings."
    },
    {
      "title": "The Bayesian Learning Rule",
      "authors": "Mohammad Emtiyaz Khan et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "The Bayesian Learning Rule provides the theoretical framework that casts optimizer updates as variational inference steps, a foundation used to derive VON and thereby the improved IVON updates."
    },
    {
      "title": "Weight Uncertainty in Neural Networks",
      "authors": "Charles Blundell et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "This work established mean-field variational learning for neural networks, defining the posterior parameterization (factorized Gaussian over weights) that IVON employs and scales to modern deep architectures."
    },
    {
      "title": "Practical Deep Learning with Bayesian Principles",
      "authors": "Kazuki Osawa et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating scalable Bayesian training via noisy natural gradients with KFAC but at notable extra cost, this work highlighted the need for Adam-cost variational methods that IVON delivers."
    },
    {
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": "Diederik P. Kingma et al.",
      "year": 2015,
      "role": "Baseline",
      "relationship_sentence": "Adam is the primary training baseline that IVON is designed to match or outperform while providing calibrated predictive uncertainty."
    }
  ],
  "synthesis_narrative": "IVON\u2019s core contribution\u2014showing that variational learning can train very large networks at Adam-like cost with superior uncertainty\u2014arises from a clear lineage within variational second-order methods. The immediate precursor is Variational Online Newton (VON), which formulated variational learning as an online Newton-style update with diagonal posterior parameters; IVON directly improves this method to be robust and performant on modern large models. Earlier, VOGN (Variational Online Gauss\u2013Newton) introduced the practical recipe for online variational updates using Gauss\u2013Newton/Fisher curvature, seeding the idea that one can maintain a Gaussian posterior over weights with per-parameter second-order statistics at near\u2013first-order cost. These algorithmic developments rest on the Bayesian Learning Rule, which unifies optimization and variational inference, providing the theoretical footing for deriving such updates and connecting adaptive optimizers with approximate Bayesian learning.\n\nFoundationally, Weight Uncertainty in Neural Networks established mean-field variational posteriors for deep nets, but its perceived limitations on large models helped crystallize the widespread belief that IVON decisively challenges. In parallel, Practical Deep Learning with Bayesian Principles showed that scalable Bayesian training is possible via noisy natural gradients (e.g., KFAC), but with significant additional curvature-computation overhead\u2014precisely the gap IVON closes by achieving Bayesian-quality uncertainty at Adam-level cost. Finally, Adam serves as the practical baseline whose compute profile and performance IVON targets and surpasses, anchoring the paper\u2019s empirical claim that variational learning is effective for today\u2019s large deep networks.",
  "analysis_timestamp": "2026-01-06T23:09:26.495431"
}
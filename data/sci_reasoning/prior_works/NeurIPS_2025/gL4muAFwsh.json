{
  "prior_works": [
    {
      "title": "Reinforcement Learning: An Introduction (2nd ed.), Section on Gradient Bandit Algorithms",
      "authors": "Richard S. Sutton, Andrew G. Barto",
      "year": 2018,
      "role": "Origin of the SGB policy",
      "relationship_sentence": "This book introduced the gradient bandit algorithm\u2014softmax over preference parameters updated by a stochastic gradient\u2014which is the exact policy class whose regret\u2013stepsize behavior the present paper characterizes."
    },
    {
      "title": "Asymptotic convergence of stochastic-gradient bandits with constant step size",
      "authors": "Mei et al.",
      "year": 2023,
      "role": "Recent theory on SGB (constant stepsize, asymptotics)",
      "relationship_sentence": "Established that SGB with a constant learning rate is asymptotically attracted to the optimal policy, framing the modern analysis of SGB and motivating the present work to go beyond asymptotics to finite-time regret regimes."
    },
    {
      "title": "Logarithmic regret for stochastic-gradient bandits with sufficiently small learning rates",
      "authors": "Mei et al.",
      "year": 2024,
      "role": "Recent theory on SGB (finite-time, small stepsizes)",
      "relationship_sentence": "Showed that sufficiently small constant learning rates yield O(log T) regret, directly prompting this paper\u2019s central question of what happens at larger stepsizes and whether a sharp threshold exists."
    },
    {
      "title": "Asymptotically Efficient Adaptive Allocation Rules",
      "authors": "Tze Leung Lai, Herbert Robbins",
      "year": 1985,
      "role": "Foundational gap-dependent lower bounds",
      "relationship_sentence": "Provided the canonical log(T)/\u0394 benchmarks and gap-dependent perspective that the present paper uses to define and interpret the \u0394-scaled threshold separating logarithmic from polynomial regret."
    },
    {
      "title": "Dynamics of Stochastic Approximation Algorithms",
      "authors": "Michel Bena\u00efm",
      "year": 1999,
      "role": "Analytical toolkit for constant-stepsize dynamics",
      "relationship_sentence": "Supplied ODE and dynamical-systems techniques for analyzing constant-stepsize stochastic approximation, which underpin understanding of stability/instability that manifests as a regret phase transition with stepsize."
    },
    {
      "title": "Stochastic Approximation: A Dynamical Systems Viewpoint",
      "authors": "Vivek S. Borkar",
      "year": 2008,
      "role": "Analytical toolkit for stepsize and stability thresholds",
      "relationship_sentence": "Offers general tools to connect stepsize scales to limiting behavior and attractors, informing the paper\u2019s identification of a sharp learning-rate threshold and its scaling with problem parameters."
    },
    {
      "title": "Bandit Algorithms",
      "authors": "Tor Lattimore, Csaba Szepesv\u00e1ri",
      "year": 2020,
      "role": "Canonical reference for K-armed stochastic bandits",
      "relationship_sentence": "Provides standard gap- and arm-count\u2013dependent regret baselines that contextualize the paper\u2019s result that the learning rate must scale inversely with K to avoid polynomial regret."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014pinning down regret regimes of the stochastic gradient bandit (SGB) policy as a function of its constant learning rate and identifying a sharp, gap-scaled threshold\u2014builds directly on two strands of work. First, Sutton and Barto\u2019s formulation of gradient bandits defined the softmax-over-preferences policy updated by a stochastic gradient; this is the algorithmic object under scrutiny. Second, recent analyses by Mei et al. established the modern theoretical baseline for SGB: asymptotic convergence to the optimal arm with constant stepsizes (2023) and logarithmic regret when the constant stepsize is sufficiently small (2024). Together, these works posed the open question addressed here: does logarithmic regret persist beyond very small stepsizes, and if not, where is the boundary?\nTo answer this, the authors leverage classical bandit theory and stochastic approximation. Lai and Robbins\u2019 gap-dependent lower bounds formalize logarithmic regret as the gold standard and highlight the central role of the suboptimality gap \u0394, guiding the search for a \u0394-scaled threshold. The dynamical-systems view of stochastic approximation (Bena\u00efm; Borkar) provides the technical lens to connect constant stepsizes with stability properties of the induced stochastic dynamics, enabling the identification of parameter regimes where the process drifts toward or away from optimal behavior, translating into logarithmic versus polynomial regret. Finally, standard K-armed bandit baselines (Lattimore\u2013Szepesv\u00e1ri) contextualize the multi-arm extension, in which the paper shows the stepsize must shrink inversely with K to avoid polynomial regret, completing the regime map.",
  "analysis_timestamp": "2026-01-07T00:21:33.171323"
}
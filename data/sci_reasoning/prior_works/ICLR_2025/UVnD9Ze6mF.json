{
  "prior_works": [
    {
      "title": "AI Risks (AIR) 2024: A Regulation- and Policy-Grounded Taxonomy of AI Risks",
      "authors": "Yi Zeng et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "AIR 2024 provides the four-tier, regulation- and policy-derived safety taxonomy that AIR-BENCH 2024 directly adopts as its categorical backbone and organizing principle."
    },
    {
      "title": "Holistic Evaluation of Language Models",
      "authors": "Percy Liang et al.",
      "year": 2022,
      "arxiv_id": "2211.09110",
      "role": "Gap Identification",
      "relationship_sentence": "HELM\u2019s influential but literature- and intuition-driven safety dimensions exposed the lack of regulation-aligned categories, a shortcoming AIR-BENCH 2024 explicitly remedies by mapping evaluation to policy-grounded risk classes."
    },
    {
      "title": "Ethical and social risks of harm from language models",
      "authors": "Laura Weidinger et al.",
      "year": 2021,
      "arxiv_id": "2108.07258",
      "role": "Gap Identification",
      "relationship_sentence": "This work crystallized a literature-based harms taxonomy that many benchmarks inherit, and AIR-BENCH 2024 directly addresses its policy-misalignment by replacing it with a taxonomy decomposed from concrete regulations and company policies."
    },
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "arxiv_id": "2204.05862",
      "role": "Related Problem",
      "relationship_sentence": "By popularizing \u2018harmlessness\u2019 with a set of broad prohibited content types, this work highlighted how de facto safety categories emerge from practice rather than policy, motivating AIR-BENCH 2024\u2019s shift to regulation-specified risk categories."
    },
    {
      "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
      "authors": "Samuel Gehman et al.",
      "year": 2020,
      "arxiv_id": "2009.11462",
      "role": "Gap Identification",
      "relationship_sentence": "As a single-axis toxicity benchmark widely used to proxy \u2018safety,\u2019 it exemplifies fragmented, category-specific evaluation that AIR-BENCH 2024 unifies under a comprehensive, regulation-derived risk taxonomy."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "This jailbreak work operationalized harmful-intent categories for stress-testing alignment, a scope AIR-BENCH 2024 retains but systematically re-anchors to explicit regulatory and policy risk classes for consistent coverage and comparison."
    }
  ],
  "synthesis_narrative": "A regulation- and policy-grounded view of AI risks was crystallized in AIR 2024, which decomposed governmental regulations and company policies into a four-tier taxonomy of granular risk categories. HELM codified a holistic but largely literature- and intuition-driven set of safety dimensions to evaluate language models across, establishing influential practices for benchmarking that nevertheless lacked explicit ties to regulatory risk formulations. Weidinger and colleagues cataloged ethical and social harms for language models in a taxonomy that many subsequent evaluations implicitly inherited, further entrenching literature-based categories. Bai et al. operationalized \u2018harmlessness\u2019 through RLHF, popularizing broad prohibited-content types that shaped practical safety evaluation without grounding them in policy. RealToxicityPrompts offered a focused toxicity axis\u2014useful but emblematic of fragmented, single-harm benchmarks. Zou et al. showed how adversarial prompts expose unsafe behaviors, organizing harmful intents for stress tests but again outside any regulatory taxonomy. Together, these works established how to evaluate safety, what harms to look for, and how models can fail\u2014but they diverged in category systems and were not anchored to laws or corporate acceptable-use policies. The natural next step was to retain the evaluation rigor and harmful-intent stress testing while remapping categories to a policy-derived taxonomy. Building on AIR 2024\u2019s decomposition of regulations and policies, the current work synthesizes this lineage into a benchmark whose prompts and labels directly reflect regulation-specified risks, enabling consistent, comparable safety evaluation across diverse harm types.",
  "target_paper": {
    "title": "AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories",
    "authors": "Yi Zeng, Yu Yang, Andy Zhou, Jeffrey Ziwei Tan, Yuheng Tu, Yifan Mai, Kevin Klyman, Minzhou Pan, Ruoxi Jia, Dawn Song, Percy Liang, Bo Li",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "AI Safety, Regulation, Policy, Safety Alignment, Foundation Models",
    "abstract": "Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-BENCH 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in the AI Risks taxonomy, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-BENCH 2024 contains 5,694 diverse prompts spanning these categories, with manual cur",
    "openreview_id": "UVnD9Ze6mF",
    "forum_id": "UVnD9Ze6mF"
  },
  "analysis_timestamp": "2026-01-06T12:59:35.369532"
}
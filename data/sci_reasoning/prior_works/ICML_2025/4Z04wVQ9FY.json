{
  "prior_works": [
    {
      "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
      "authors": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew M. Stuart, Anima Anandkumar",
      "year": 2021,
      "role": "Foundational neural-operator architecture for learning function-to-function maps (PDE solution operators).",
      "relationship_sentence": "LUNO targets uncertainty quantification for trained neural operators in practice, with FNO as the canonical architecture whose learned operator is linearized and endowed with a function-valued GP belief."
    },
    {
      "title": "Learning Nonlinear Operators via DeepONet",
      "authors": "Lu Lu, Pengzhan Jin, George Em Karniadakis",
      "year": 2021,
      "role": "Foundational operator-learning framework establishing function-to-function neural mappings.",
      "relationship_sentence": "By framing neural operators broadly (including DeepONet) as function-space maps, LUNO\u2019s linearization-and-pushforward construction applies generically across operator-learning models."
    },
    {
      "title": "The Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Theory linking network linearization around parameters to Gaussian processes via the NTK.",
      "relationship_sentence": "LUNO\u2019s core step\u2014linearizing a trained neural operator and pushing Gaussian weight uncertainty forward\u2014rests on the NTK-style insight that linearized networks induce GP structure in function space."
    },
    {
      "title": "Deep Neural Networks as Gaussian Processes",
      "authors": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein",
      "year": 2018,
      "role": "Function-space view of neural networks as Gaussian processes in wide or linearized limits.",
      "relationship_sentence": "LUNO interprets the linearized operator as a function-valued Gaussian random process, directly extending the DNN-as-GP perspective from finite-dimensional outputs to operator outputs."
    },
    {
      "title": "Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks",
      "authors": "Adrian Kristiadi, Matthias Hein, Philipp Hennig",
      "year": 2020,
      "role": "Practical Laplace approximation at a trained network\u2019s MAP point for calibrated predictive uncertainty.",
      "relationship_sentence": "LUNO operationalizes the Laplace-at-MAP strategy for neural operators, using linearized predictions to obtain calibrated, tractable function-valued uncertainty."
    },
    {
      "title": "A Scalable Laplace Approximation for Neural Networks",
      "authors": "Hippolyt Ritter, Aleksandar Botev, David Barber",
      "year": 2018,
      "role": "Scalable second-order (Kronecker-factored) Laplace methods to form Gaussian weight posteriors in deep nets.",
      "relationship_sentence": "LUNO relies on scalable Gaussian approximations over weights; such Laplace machinery provides the weight-space Gaussian that is pushed forward through the linearized neural operator."
    },
    {
      "title": "Kernels for Vector-Valued Functions: A Review",
      "authors": "Mauricio A. \u00c1lvarez, Lorenzo Rosasco, Neil D. Lawrence",
      "year": 2012,
      "role": "Theory of operator-/matrix-valued kernels and multi-output Gaussian processes.",
      "relationship_sentence": "LUNO\u2019s \u2018probabilistic currying\u2019 yields a function-valued GP; operator- and vector-valued kernel theory underpins treating outputs as functions and clarifies the induced covariance structure."
    }
  ],
  "synthesis_narrative": "LUNO\u2019s core idea\u2014endowing trained neural operators with principled uncertainty via linearization\u2014sits at the intersection of operator learning and Bayesian deep learning. Foundational operator-learning works such as the Fourier Neural Operator and DeepONet established neural mappings between function spaces for PDE solution operators, defining the practical models to which LUNO attaches. On the Bayesian side, the insight that linearized neural networks behave like Gaussian processes (via the Neural Tangent Kernel and the broader DNN-as-GP perspective) directly motivates LUNO\u2019s pushforward: linearize a trained operator and propagate Gaussian weight uncertainty to obtain a Gaussian belief over outputs. Practical Bayesianization is enabled by Laplace-at-MAP methods for neural networks, which provide a Gaussian approximation in weight space; scalable Laplace techniques further make this feasible for modern architectures. LUNO then extends these ingredients from finite-dimensional outputs to function-valued predictions: by viewing an operator as a curried map, fixing an input function yields a function-valued GP over the output domain. Here, operator-/vector-valued kernel theory informs the structure of the induced covariances and legitimizes treating outputs as elements of function spaces. Together, these prior works supply (i) the operator-learning substrates (FNO/DeepONet), (ii) the linearization-to-GP bridge (NTK and DNN-as-GP), and (iii) the practical Bayesian mechanism (Laplace) that LUNO unifies to deliver function-valued Gaussian process uncertainty for neural operators.",
  "analysis_timestamp": "2026-01-07T00:04:09.154040"
}
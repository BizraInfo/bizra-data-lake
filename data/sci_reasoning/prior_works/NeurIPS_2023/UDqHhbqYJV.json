{
  "prior_works": [
    {
      "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks",
      "authors": [
        "Jason Weston",
        "Antoine Bordes",
        "Sumit Chopra"
      ],
      "year": 2015,
      "role": "Benchmark/dataset for synthetic textual reasoning (incl. path-finding)",
      "relationship_sentence": "Provided an early template for evaluating reasoning over synthetic natural language with graph-like path-finding; NLGraph generalizes this paradigm to a comprehensive suite of explicit graph algorithms and modern LLMs."
    },
    {
      "title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text",
      "authors": [
        "Koustuv Sinha",
        "Shagun Sodhani",
        "Joelle Pineau",
        "William L. Hamilton"
      ],
      "year": 2019,
      "role": "Benchmark/dataset for relational (graph) reasoning from text",
      "relationship_sentence": "Demonstrated that models can (and often cannot) infer multi-hop relations over implicit family graphs described in text; NLGraph extends this idea to explicit graph structures and diverse algorithmic operations (e.g., shortest path, max flow) with controlled difficulty."
    },
    {
      "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
      "authors": [
        "Dheeru Dua",
        "Yizhong Wang",
        "Pradeep Dasigi",
        "Gabriel Stanovsky",
        "Sameer Singh",
        "Matt Gardner"
      ],
      "year": 2019,
      "role": "Benchmark/dataset for text-to-discrete-operations reasoning",
      "relationship_sentence": "Showed the importance and challenge of mapping natural language to precise discrete computations; NLGraph adopts this lens but targets graph-theoretic computations specified in language."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Quoc V. Le",
        "Denny Zhou"
      ],
      "year": 2022,
      "role": "Prompting method for step-by-step reasoning",
      "relationship_sentence": "Motivated evaluating whether CoT improves LLMs\u2019 ability to carry out graph operations from text; NLGraph systematically measures CoT\u2019s efficacy and limitations on graph problems."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": [
        "Xuezhi Wang",
        "Jason Wei",
        "Dale Schuurmans",
        "Quoc V. Le",
        "Denny Zhou"
      ],
      "year": 2022,
      "role": "Inference-time technique to enhance CoT via sampling",
      "relationship_sentence": "Inspired NLGraph\u2019s evaluation of inference-time strategies beyond single-pass CoT to see if sampling-based reasoning can overcome graph reasoning failures."
    },
    {
      "title": "PAL: Program-Aided Language Models",
      "authors": [
        "Luyu Gao",
        "Aman Madaan",
        "Shuyan Zhou",
        "Uri Alon",
        "Graham Neubig"
      ],
      "year": 2023,
      "role": "Tool/use-of-code method for reasoning by delegating computation",
      "relationship_sentence": "Directly informed NLGraph\u2019s inclusion of program-execution baselines (e.g., code generation to run graph algorithms) to test whether delegating computation overcomes LLMs\u2019 graph-reasoning deficits."
    }
  ],
  "synthesis_narrative": "NLGraph\u2019s core contribution is a focused benchmark that expresses canonical graph problems in natural language and probes whether LLMs can ground those descriptions into structured graph representations and execute algorithmic operations. This contribution sits at the intersection of two lines of prior work. First, synthetic and relational reasoning benchmarks like bAbI and CLUTRR established the viability of probing multi-step and graph-like reasoning from text, but were limited to simple path-finding or relation inference under narrow schemas. DROP broadened the agenda by emphasizing discrete computations triggered by language, yet did not target the algorithmic breadth of graph theory. These works motivated NLGraph to systematize a diverse set of explicit graph tasks (e.g., connectivity, shortest path, maximum flow, GNN simulation) in controlled, text-only settings with scalable difficulty. Second, advances in prompting and tool use\u2014Chain-of-Thought and Self-Consistency\u2014suggested that eliciting intermediate reasoning might unlock latent algorithmic ability, while program-execution approaches like PAL posited that LLMs should delegate computation to external tools. NLGraph integrates these ideas experimentally: it evaluates CoT and sampling-based inference on graph tasks, and contrasts them with code-execution baselines to quantify when external computation is necessary. Together, these prior works directly shaped NLGraph\u2019s design goals, task suite, and evaluation protocol, enabling a clear diagnosis of LLMs\u2019 limitations and the conditions under which graph problem solving in natural language becomes tractable.",
  "analysis_timestamp": "2026-01-07T00:02:04.792649"
}
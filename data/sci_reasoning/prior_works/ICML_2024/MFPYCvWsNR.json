{
  "prior_works": [
    {
      "title": "Transformer Memory as a Differentiable Search Index (DSI)",
      "authors": "Yi Tay, Mostafa Dehghani, Donald Metzler, and colleagues",
      "year": 2022,
      "role": "Foundational generative retrieval paradigm",
      "relationship_sentence": "This work established the core GDR formulation\u2014training an autoregressive model to map queries to document identifiers\u2014which the present paper reinterprets through an information-theoretic bottleneck and rate\u2013distortion lens and aims to improve via bottleneck-minimal indexing."
    },
    {
      "title": "Autoregressive Entity Retrieval (GENRE)",
      "authors": "Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni",
      "year": 2021,
      "role": "Early demonstration of retrieval-as-generation",
      "relationship_sentence": "By showing that identifiers (entity names) can be generated directly by a seq2seq model, GENRE motivated viewing index design as a learnable target space\u2014precisely the space this paper optimizes using a mutual-information bottleneck perspective."
    },
    {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",
      "year": 2020,
      "role": "Modeling backbone enabling GDR",
      "relationship_sentence": "T5 popularized text-to-text autoregressive modeling that underpins DSI-style systems; the present paper\u2019s indexing and analysis presume such seq2seq backbones that generate index tokens from queries."
    },
    {
      "title": "Coding Theorems for a Discrete Source with a Fidelity Criterion (Rate\u2013Distortion Theory)",
      "authors": "Claude E. Shannon",
      "year": 1959,
      "role": "Foundational information-theoretic framework",
      "relationship_sentence": "The paper\u2019s central contribution\u2014casting GDR indexing as a rate\u2013distortion problem and quantifying optimality via mutual information\u2014directly builds on Shannon\u2019s rate\u2013distortion theory."
    },
    {
      "title": "The Information Bottleneck Method",
      "authors": "Naftali Tishby, Fernando C. Pereira, William Bialek",
      "year": 1999,
      "role": "Core theoretical principle for bottlenecked representations",
      "relationship_sentence": "The authors\u2019 formulation of indexes as a bottleneck that minimally transmits information from documents to queries draws explicitly from the Information Bottleneck principle."
    },
    {
      "title": "Deep Variational Information Bottleneck",
      "authors": "Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, Kevin Murphy",
      "year": 2017,
      "role": "Practical MI-based learning framework",
      "relationship_sentence": "Techniques for estimating and optimizing information bottlenecks in neural systems inform the paper\u2019s empirical quantification of the GDR bottleneck and guide the design of bottleneck-minimal indexing."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution is to reframe generative document retrieval (GDR) as an information transmission problem, where learned indexes constitute a bottleneck through which document information must flow to answer queries. This builds directly on the GDR paradigm introduced by DSI, which replaced external indices with an autoregressive model that generates document identifiers from queries, and earlier demonstrations like GENRE that proved identifiers can be produced as text strings. These works raised the crucial design question: what should the generated identifiers be? The present paper answers that by adopting formal tools from information theory. Shannon\u2019s rate\u2013distortion theory provides the mathematical foundation for quantifying the trade-off between the number of bits transmitted via indexes and the fidelity of retrieval, while the Information Bottleneck framework specifies how to compress representations to preserve task-relevant information. Practical advances in optimizing mutual-information objectives in neural networks, exemplified by the Deep Variational Information Bottleneck, inform the paper\u2019s empirical estimation strategies and objective design. T5\u2019s text-to-text modeling furnishes the backbone architecture that operationalizes these ideas, enabling the mapping from queries to learned index tokens. By synthesizing these strands, the paper proposes bottleneck-minimal indexing: index designs that explicitly minimize redundant information under a rate\u2013distortion criterion, yielding improved retrieval effectiveness on NQ320K and MS MARCO compared with prior GDR index choices.",
  "analysis_timestamp": "2026-01-06T23:42:48.069664"
}
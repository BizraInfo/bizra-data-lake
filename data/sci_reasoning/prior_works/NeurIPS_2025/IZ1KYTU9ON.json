{
  "prior_works": [
    {
      "title": "Random synaptic feedback weights support learning in deep neural networks",
      "authors": "Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, Colin J. Akerman",
      "year": 2016,
      "role": "Foundational alternative to weight transport (feedback alignment)",
      "relationship_sentence": "EBD builds on the idea that hidden layers can learn from broadcasted error signals without symmetric weights, but replaces feedback alignment\u2019s heuristic random feedback with a principled decorrelation objective derived from MMSE orthogonality."
    },
    {
      "title": "Direct Feedback Alignment provides learning in deep neural networks",
      "authors": "Arild N\u00f8kland",
      "year": 2016,
      "role": "Direct error broadcast to layers",
      "relationship_sentence": "DFA\u2019s strategy of sending output errors directly to each hidden layer is the operational template that EBD adopts, with EBD contributing a theoretically grounded layer-wise loss that penalizes activation\u2013error correlations."
    },
    {
      "title": "Decoupled Neural Interfaces using Synthetic Gradients",
      "authors": "Max Jaderberg, Wojciech M. Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, Koray Kavukcuoglu",
      "year": 2017,
      "role": "Local learning via broadcasted/locally predicted error signals",
      "relationship_sentence": "EBD aligns with the synthetic gradients paradigm of providing local learning signals to intermediate layers, but specifies that the local objective should enforce orthogonality (decorrelation) between activations and output errors."
    },
    {
      "title": "Fundamentals of Statistical Signal Processing, Volume I: Estimation Theory",
      "authors": "Steven M. Kay",
      "year": 1993,
      "role": "Theoretical basis: MMSE orthogonality principle",
      "relationship_sentence": "EBD\u2019s core insight\u2014that optimal estimator errors are orthogonal to functions of the inputs\u2014directly instantiates the MMSE orthogonality principle into a trainable layer-wise decorrelation loss."
    },
    {
      "title": "Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor Learning Rules",
      "authors": "Nicolas Fr\u00e9maux, Wulfram Gerstner",
      "year": 2016,
      "role": "Biological foundation for three-factor learning",
      "relationship_sentence": "EBD\u2019s updates naturally take a three-factor form (pre-activity, post-activity, and a broadcast error/modulatory term), matching the theoretical framework of neuromodulated three-factor plasticity articulated in this work."
    },
    {
      "title": "Learning by the dendritic prediction of somatic spiking",
      "authors": "Reimund Urbanczik, Walter Senn",
      "year": 2014,
      "role": "Mechanistic substrate for local error-modulated plasticity",
      "relationship_sentence": "By showing how distal feedback can modulate local synaptic changes via a third factor, this work provides a biological mechanism through which EBD\u2019s broadcast error and local decorrelation rule can be implemented."
    },
    {
      "title": "Towards deep learning with segregated dendrites",
      "authors": "Jordan Guerguiev, Timothy P. Lillicrap, Blake A. Richards",
      "year": 2017,
      "role": "Architectural support for error broadcasting in cortex-like models",
      "relationship_sentence": "Segregated dendritic compartments offer a pathway to route broadcast error signals separately from feedforward drive, a structural prerequisite that makes EBD\u2019s layer-wise error\u2013activity decorrelation rule biologically plausible."
    }
  ],
  "synthesis_narrative": "EBD\u2019s key contribution is to recast error broadcasting as a principled learning objective rooted in the MMSE orthogonality principle: the residual of an optimal estimator is orthogonal to functions of the inputs. This insight provides a concrete local objective\u2014penalizing correlations between layer activations and the global output error\u2014that unifies and strengthens earlier error-broadcast methods. Lillicrap et al. and N\u00f8kland established that deep networks can learn without exact weight transport by broadcasting errors directly to hidden layers (feedback and direct feedback alignment). EBD preserves this operational scheme but replaces heuristic feedback with a decorrelation criterion that specifies what the broadcasted signal should achieve at each layer. Jaderberg et al.\u2019s synthetic gradients further legitimized decoupling layers via locally supplied error signals; EBD clarifies that a statistically grounded target for such local signals is orthogonality between activations and output errors.\nThe theoretical backbone comes from estimation theory (Kay), where the orthogonality principle motivates EBD\u2019s layer-wise losses and yields a rigorous link between global prediction optimality and local decorrelation constraints. On the biological side, EBD\u2019s updates naturally manifest as three-factor rules\u2014pre/post activity modulated by a broadcast error\u2014consistent with the neuromodulation framework reviewed by Fr\u00e9maux and Gerstner and with mechanistic models like Urbanczik and Senn\u2019s dendritic prediction. Finally, architectures with segregated dendrites (Guerguiev et al.) provide a plausible substrate to route broadcast error signals separately from feedforward inputs, making EBD\u2019s principled broadcast-and-decorrelate mechanism both effective and biologically credible.",
  "analysis_timestamp": "2026-01-07T00:02:04.958357"
}
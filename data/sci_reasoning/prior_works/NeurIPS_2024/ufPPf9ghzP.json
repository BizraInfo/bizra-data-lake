{
  "prior_works": [
    {
      "title": "Complexity Results and Approximation Strategies for MAP Explanations",
      "authors": "James D. Park, Adnan Darwiche",
      "year": 2004,
      "role": "Problem framing and hardness of MPE/MAP inference in graphical models",
      "relationship_sentence": "Established the NP-hardness and practical difficulty of MAP/MPE in Bayesian networks, motivating approaches that avoid per-query exact inference\u2014such as amortizing MPE over a model via a learned network."
    },
    {
      "title": "Sum-Product Networks: A New Deep Architecture",
      "authors": "Hoifung Poon, Pedro Domingos",
      "year": 2011,
      "role": "Probabilistic circuits enabling structured, tractable inference (including MPE under selectivity)",
      "relationship_sentence": "Introduced probabilistic circuits where MPE can be efficient under determinism/selectivity, informing this work\u2019s focus on model-agnostic MPE (including circuits) and highlighting when direct MPE is tractable vs. when learning-based surrogates are valuable."
    },
    {
      "title": "Auto-Encoding Variational Bayes",
      "authors": "Diederik P. Kingma, Max Welling",
      "year": 2014,
      "role": "Foundation of amortized inference via inference networks",
      "relationship_sentence": "Pioneered amortizing inference across queries with an inference network; the present paper adapts this amortization principle from posterior estimation to argmax/MPE prediction conditioned on arbitrary evidence."
    },
    {
      "title": "Inference Compilation and Universal Probabilistic Programming",
      "authors": "Tuan Anh Le, Atilim Gunes Baydin, Frank Wood",
      "year": 2017,
      "role": "Compiling model-specific inference into neural networks",
      "relationship_sentence": "Directly inspires the idea of compiling all queries for a fixed generative model into a neural network; this paper transfers that paradigm from sampling/marginals to efficiently answering arbitrary MPE queries."
    },
    {
      "title": "Structured Prediction Energy Networks",
      "authors": "Alexander G. Belanger, Andrew McCallum",
      "year": 2016,
      "role": "Inference-time optimization and iterative refinement for structured outputs",
      "relationship_sentence": "Demonstrated combining a predictor with test-time optimization under an energy function; the current work analogously performs inference-time optimization with a self-supervised loss to iteratively improve MPE assignments."
    },
    {
      "title": "Distilling the Knowledge in a Neural Network",
      "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean",
      "year": 2015,
      "role": "Teacher\u2013student distillation for better initializers",
      "relationship_sentence": "Provides the teacher\u2013student framework used here to produce a stronger initial network, reducing the number of inference-time optimization steps needed for accurate MPE."
    },
    {
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "authors": "Dequan Wang, Evan Shelhamer, Shaojie Jin, Trevor Darrell",
      "year": 2021,
      "role": "Self-supervised/adaptation losses applied at inference time",
      "relationship_sentence": "Shows that self-supervised/unsupervised objectives can adapt models at test time; this paper adopts the principle by designing a self-supervised loss to refine MPE solutions during inference."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central advance\u2014distilling all Most Probable Explanation (MPE) queries of a fixed probabilistic model into a neural network and refining answers with inference-time self-supervision\u2014sits at the intersection of amortized inference, model-specific compilation, and test-time optimization. Park and Darwiche\u2019s hardness results for MAP/MPE established the need to avoid per-query exact inference, motivating amortized strategies. Poon and Domingos\u2019 sum\u2013product networks highlighted cases where MPE is tractable in probabilistic circuits and, by contrast, where a learned surrogate is attractive for broader model classes. Building on the amortization paradigm introduced by VAEs, the work reframes inference networks from posterior estimation to argmax inference over arbitrary evidence sets. Le, Baydin, and Wood\u2019s inference compilation directly informs the idea of compiling a model into a neural network that answers queries efficiently; the present paper extends this from sampling/marginals to MPE. To further boost accuracy, the method borrows from structured prediction energy networks by performing iterative inference-time optimization, here driven by a self-supervised objective tailored to MPE quality. Finally, adopting a teacher\u2013student distillation setup (Hinton et al.) yields a stronger initializer, which reduces the computational burden of refinement, while recent test-time adaptation ideas (e.g., Tent) justify optimizing a self-supervised loss at inference. Together, these strands yield a unified, model-agnostic approach that amortizes and then incrementally improves MPE solutions across Bayesian/Markov networks, probabilistic circuits, and neural autoregressive models.",
  "analysis_timestamp": "2026-01-06T23:33:35.546348"
}
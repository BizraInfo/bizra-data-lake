{
  "prior_works": [
    {
      "title": "Semi-Implicit Variational Inference",
      "authors": "Mingzhang Yin, Mingyuan Zhou",
      "year": 2018,
      "role": "Origin of SIVI; hierarchical variational family with implicit mixing and ELBO surrogates",
      "relationship_sentence": "Defines the semi-implicit variational family and motivates the problem that PVI solves\u2014avoiding intractable marginal densities and surrogate bounds by directly optimizing the ELBO via a particleized mixing distribution."
    },
    {
      "title": "Hierarchical Variational Models",
      "authors": "Rajesh Ranganath, Dustin Tran, David M. Blei",
      "year": 2016,
      "role": "Precursor to SIVI via hierarchical/mixture variational families",
      "relationship_sentence": "Establishes hierarchical variational constructions that SIVI refines with implicit mixing, providing the structural template that PVI retains while changing the optimization mechanism to a particle Wasserstein-flow approach."
    },
    {
      "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and GANs",
      "authors": "Sebastian Nowozin, Botond Cseke, Ryota Tomioka (often cited via Mescheder, Nowozin, Geiger 2017)",
      "year": 2017,
      "role": "Minimax approach for implicit variational distributions",
      "relationship_sentence": "Represents the minimax route used when q has no tractable density; PVI is positioned as an alternative that avoids adversarial density-ratio estimation by optimizing the ELBO through particle measures and gradient flows."
    },
    {
      "title": "Amortised MCMC",
      "authors": "Yingzhen Li, Richard E. Turner",
      "year": 2018,
      "role": "Inner-loop MCMC within amortized variational inference",
      "relationship_sentence": "Illustrates the costly inner-loop MCMC strategy to handle implicit or complex approximations; PVI contrasts by removing MCMC inner loops, instead evolving particles via a Wasserstein gradient flow of a free-energy functional."
    },
    {
      "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm",
      "authors": "Qiang Liu, Dilin Wang",
      "year": 2016,
      "role": "Particle-based variational inference via deterministic transport",
      "relationship_sentence": "Demonstrates that empirical particle measures can directly optimize divergence-based objectives over distributions, inspiring PVI\u2019s use of particles to optimize the ELBO without parametric assumptions on the mixing distribution."
    },
    {
      "title": "The Variational Formulation of the Fokker\u2013Planck Equation",
      "authors": "Richard Jordan, David Kinderlehrer, Felix Otto",
      "year": 1998,
      "role": "Foundational Wasserstein gradient flow (JKO) formulation of free-energy descent",
      "relationship_sentence": "Provides the variational and Wasserstein metric framework for viewing distributional evolution as gradient flow of a free energy, which PVI leverages to characterize and approximate the optimal mixing distribution."
    },
    {
      "title": "Gradient Flows: In Metric Spaces and in the Space of Probability Measures",
      "authors": "Luigi Ambrosio, Nicola Gigli, Giuseppe Savar\u00e9",
      "year": 2008,
      "role": "Rigorous theory of gradient flows in Wasserstein spaces",
      "relationship_sentence": "Supplies the mathematical underpinnings (existence, stability, discretizations) of Euclidean\u2013Wasserstein gradient flows that justify PVI\u2019s particle approximation to the free-energy minimizing mixing distribution."
    }
  ],
  "synthesis_narrative": "The core innovation of Particle Semi-Implicit Variational Inference (PVI) is to directly optimize the ELBO for semi-implicit variational families by representing the mixing distribution with empirical particle measures that follow a Euclidean\u2013Wasserstein gradient flow of a free-energy functional. This builds squarely on the SIVI formulation of Yin and Zhou, which introduced hierarchical variational families with implicit mixing and relied on ELBO surrogates due to intractable densities. Earlier hierarchical/mixture variational designs (Ranganath, Tran, Blei) provided the structural template that SIVI refined and that PVI retains. To address the intractability in implicit q, prior lines took two main routes that PVI explicitly avoids: minimax training with discriminators (Adversarial Variational Bayes) and inner-loop MCMC within VI (Amortised MCMC), both of which can be unstable or costly. Conceptually, PVI\u2019s move to empirical measures is motivated by particle-based VI such as SVGD, which showed that sets of particles can deterministically minimize divergence-based objectives over distributions without restrictive parametric forms. The theoretical backbone for PVI\u2019s optimization view comes from the Wasserstein gradient-flow literature: Jordan\u2013Kinderlehrer\u2013Otto\u2019s variational formulation of free-energy descent and the comprehensive metric-space theory of Ambrosio\u2013Gigli\u2013Savar\u00e9. By casting the optimal mixing distribution as the minimizer of a free energy and evolving an empirical measure along its Wasserstein gradient flow, PVI circumvents density intractability, eliminates inner MCMC or adversarial loops, and directly tightens the ELBO while making no parametric assumptions about the mixing distribution.",
  "analysis_timestamp": "2026-01-06T23:33:35.578650"
}
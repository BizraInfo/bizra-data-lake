{
  "prior_works": [
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Fabio Petroni et al.",
      "year": 2019,
      "arxiv_id": "1909.01066",
      "role": "Extension",
      "relationship_sentence": "Their LAMA probing paradigm for eliciting high-precision parametric facts directly underpins this paper\u2019s systematic procedure for extracting a model\u2019s internal (\u2018parametric\u2019) memory before introducing conflicts."
    },
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "authors": "Patrick Lewis et al.",
      "year": 2020,
      "arxiv_id": "2005.11401",
      "role": "Foundation",
      "relationship_sentence": "RAG formalized injecting retrieved evidence into generation to overcome static parametric memory, providing the core tool-augmentation setting whose conflict dynamics this paper isolates and scrutinizes."
    },
    {
      "title": "Improving Language Models by Retrieving from Trillions of Tokens",
      "authors": "Sebastian Borgeaud et al.",
      "year": 2022,
      "arxiv_id": "2112.04426",
      "role": "Foundation",
      "relationship_sentence": "RETRO explicitly frames parametric versus non-parametric memory and demonstrates large gains from retrieval, motivating this work\u2019s controlled tests of how LLMs arbitrate between internal knowledge and external evidence when they disagree."
    },
    {
      "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "authors": "Timo Schick et al.",
      "year": 2023,
      "arxiv_id": "2302.04761",
      "role": "Foundation",
      "relationship_sentence": "By establishing the broader tool-augmentation paradigm where LMs condition on tool outputs as external context, Toolformer provides the operational backdrop for analyzing LMs\u2019 receptivity to conflicting external evidence."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "The construction of targeted counterfactuals (CounterFact) to probe and manipulate model beliefs directly inspires this paper\u2019s \u2018counter-memory\u2019 design for generating controlled conflicts against elicited parametric facts."
    },
    {
      "title": "Lost in the Middle: How Language Models Use Long Context",
      "authors": "Nelson F. Liu et al.",
      "year": 2023,
      "arxiv_id": "2307.03172",
      "role": "Gap Identification",
      "relationship_sentence": "Their finding that LLMs often underutilize provided context motivates this work\u2019s controlled analysis of when coherent and convincing external evidence can override entrenched parametric memory."
    }
  ],
  "synthesis_narrative": "Retrieval-augmented generation established a practical mechanism for injecting retrieved documents into language model decoding to remedy the limits of static parametric memory, and follow-on work like RETRO sharpened the conceptual split between parametric and non-parametric memory by showing that large external corpora can be decisive for factual accuracy. Toolformer broadened this notion into a general tool-augmentation paradigm in which models consume tool outputs as context, reinforcing the idea that external evidence should guide generation. In parallel, LAMA introduced precise probing templates to extract factual associations stored inside models, demonstrating that LMs encode a rich but static reservoir of parametric knowledge. Building on this, research on model editing such as ROME created CounterFact\u2014a methodology for crafting targeted counterfactuals to stress-test and modify internal beliefs\u2014thereby providing a blueprint for constructing controlled, opposition-aligned facts. Meanwhile, work on long-context utilization, exemplified by Lost in the Middle, revealed that LMs may ignore or misweight provided evidence depending on how it is presented, raising doubts about unqualified trust in external context.\nTogether, these strands exposed a clear opportunity: despite the promise of retrieval/tool augmentation, there was no controlled, belief-aware framework to test how LLMs arbitrate conflicts between internal memory and external evidence. By combining LAMA-style belief elicitation with CounterFact-inspired counter-memory construction and situating the tests within the RAG/Toolformer paradigm, the current paper systematically probes when and why coherent, convincing external evidence can override\u2014or fail to override\u2014parametric memory, thereby resolving a central uncertainty in augmentation-based LLM use.",
  "target_paper": {
    "title": "Adaptive Chameleon  or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts",
    "authors": "Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Large Langugage Model, Knowledge Conflict, Tool Augmentation",
    "abstract": "By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory.\nHowever, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? \nWe present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts.\nWe propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments.\nOur investigation reveals seemingly contradicting behaviors of LLMs.\nOn the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing.\nOn the other hand, LLMs also ",
    "openreview_id": "auKAUJZMO6",
    "forum_id": "auKAUJZMO6"
  },
  "analysis_timestamp": "2026-01-06T18:57:23.508211"
}
{
  "prior_works": [
    {
      "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
      "authors": "Ruiz et al.",
      "year": 2022,
      "arxiv_id": "2208.12242",
      "role": "Foundation",
      "relationship_sentence": "DreamBooth establishes the subject-driven fine-tuning protocol that constitutes the unauthorized customization threat model our protection targets and is a primary setting for our attack design and evaluation."
    },
    {
      "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",
      "authors": "Gal et al.",
      "year": 2022,
      "arxiv_id": "2208.01618",
      "role": "Foundation",
      "relationship_sentence": "Textual Inversion defines a mainstream personalization mechanism via learned token embeddings that our protection seeks to degrade, shaping the problem setup and core evaluation."
    },
    {
      "title": "Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models",
      "authors": "Shan et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Glaze provides the primary untargeted cloaking baseline for safeguarding against diffusion fine-tuning, whose limited efficacy we directly improve upon by replacing untargeted perturbations with targeted attacks."
    },
    {
      "title": "Nightshade: Prompt-Specific Poisoning of Text-to-Image Models",
      "authors": "Shan et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Nightshade shows that prompt-specific targeted poisoning can more effectively steer generative models than untargeted noise, directly motivating our targeted objective and careful target selection for diffusion customization."
    },
    {
      "title": "PhotoGuard: Robust Tools for Securing Images Against AI-Mediated Manipulation",
      "authors": "Salman et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "PhotoGuard introduces target-conditioned adversarial losses within diffusion pipelines for edit prevention, informing our formulation of targeted perturbations even though it addresses inference-time rather than fine-tuning threats."
    },
    {
      "title": "Unlearnable Examples: Making Personal Data Unexploitable",
      "authors": "Huang et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Unlearnable Examples established the training-time protection paradigm of adding imperceptible perturbations to poison learning, which we adapt to diffusion customization while shifting from untargeted to targeted objectives."
    }
  ],
  "synthesis_narrative": "Subject-driven customization for diffusion models was crystallized by DreamBooth, which fine-tunes a generative model on a small set of images to bind a unique subject identity to a text token, and by Textual Inversion, which personalizes generation by optimizing a token embedding in the text encoder. To counter unauthorized customization, Glaze introduced adversarial cloaks on images published online that disrupt style mimicry during downstream fine-tuning, largely relying on untargeted or heuristic perturbations that push features away from artists\u2019 styles. In parallel, PhotoGuard demonstrated that target-conditioned adversarial objectives within diffusion pipelines can reliably prevent specific edits at inference time, providing concrete loss formulations for steering diffusion behavior. More broadly, Unlearnable Examples established the principle of training-time protective perturbations that poison learning while remaining imperceptible, sparking a line of defenses predicated on small input modifications that persist through training. Crucially, Nightshade showed in the T2I pretraining setting that prompt-specific, targeted poisoning can redirect model behavior much more effectively than untargeted noise, highlighting the power of explicit target selection. Together these works expose a gap: defenses against unauthorized diffusion customization largely rely on untargeted cloaks, despite evidence that targeted poisoning is more potent and controllable. Building on the customization protocols (DreamBooth, Textual Inversion), adopting the training-time cloaking paradigm (Unlearnable Examples, Glaze), and leveraging targeted objective design insights (PhotoGuard, Nightshade), this paper advances a targeted poisoning strategy with careful target selection to markedly degrade customization quality.",
  "target_paper": {
    "title": "Targeted Attack Improves Protection against Unauthorized Diffusion Customization",
    "authors": "Boyang Zheng, Chumeng Liang, Xiaoyu Wu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Protection, Unauthorized Diffusion Customization, Adversarial Attack, Diffusion Model, Privacy",
    "abstract": "Diffusion models build a new milestone for image generation yet raising public concerns, for they can be fine-tuned on unauthorized images for customization. Protection based on adversarial attacks rises to encounter this unauthorized diffusion customization, by adding protective watermarks to images and poisoning diffusion models. However, current protection, leveraging untargeted attacks, does not appear to be effective enough. In this paper, we propose a simple yet effective improvement for the protection against unauthorized diffusion customization by introducing targeted attacks. We show that by carefully selecting the target, targeted attacks significantly outperform untargeted attacks in poisoning diffusion models and degrading the customization image quality. Extensive experiments validate the superiority of our method on two mainstream customization methods of diffusion models, compared to existing protections. To explain the surprising success of targeted attacks, we delve in",
    "openreview_id": "agHddsQhsL",
    "forum_id": "agHddsQhsL"
  },
  "analysis_timestamp": "2026-01-06T14:57:23.376381"
}
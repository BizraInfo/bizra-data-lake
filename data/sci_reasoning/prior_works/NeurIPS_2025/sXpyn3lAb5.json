{
  "prior_works": [
    {
      "title": "Data-Driven Algorithm Design",
      "authors": "Maria-Florina Balcan, Tuomas Sandholm, Ellen Vitercik",
      "year": 2017,
      "role": "Foundational framework for learning/choosing algorithms from data with statistical guarantees",
      "relationship_sentence": "The paper\u2019s core problem\u2014selecting an algorithm via empirical evaluation on training instances\u2014builds directly on this framework, and the new contribution addresses its key bottleneck by replacing full-instance evaluations with provably valid size-based proxy evaluations."
    },
    {
      "title": "A PAC Approach to Application-Specific Algorithm Selection",
      "authors": "Anupam Gupta, Tim Roughgarden",
      "year": 2017,
      "role": "Generalization theory for data-driven algorithm selection across instance distributions",
      "relationship_sentence": "This work provides the learning-theoretic backbone for algorithm selection; the present paper extends this line by introducing and analyzing size generalization\u2014showing that performance estimated on subsampled instances predicts performance on the original larger instances."
    },
    {
      "title": "k-means++: The Advantages of Careful Seeding",
      "authors": "David Arthur, Sergei Vassilvitskii",
      "year": 2007,
      "role": "Canonical clustering algorithm with analyzable performance guarantees",
      "relationship_sentence": "Because the new theory gives size generalization guarantees specifically for k-means++, it relies on and leverages the structural and probabilistic guarantees of k-means++ seeding to relate behavior on a subsample to behavior on the full dataset."
    },
    {
      "title": "A Unified Framework for Constructing Coresets for k-Means, k-Median, and Related Problems",
      "authors": "Dan Feldman, Michael Langberg",
      "year": 2011,
      "role": "Coreset/sampling theory showing small weighted subsets can approximate clustering objectives",
      "relationship_sentence": "The paper\u2019s notion of small, representative proxy instances is theoretically rooted in coreset ideas; this prior work underpins why objective values (and thus algorithm performance) on a small sample can predict those on the full instance."
    },
    {
      "title": "Scalable k-means++",
      "authors": "Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, Sergei Vassilvitskii",
      "year": 2012,
      "role": "Sampling-based and distributed seeding to scale k-means++",
      "relationship_sentence": "This work demonstrates in practice that careful subsampling can preserve the effectiveness of k-means++ at scale; the new paper provides formal size-generalization guarantees that theoretically justify such proxy-based evaluations for selection."
    },
    {
      "title": "A Cost Function for Similarity-Based Hierarchical Clustering",
      "authors": "Sanjoy Dasgupta",
      "year": 2016,
      "role": "Objective framework for hierarchical clustering performance evaluation",
      "relationship_sentence": "By formalizing hierarchical clustering quality, this work enables rigorous analysis of algorithms like single-linkage; the new paper\u2019s guarantees for single-linkage rely on such objective frameworks to compare performance across subsampled and full instances."
    },
    {
      "title": "Approximation Guarantees for Hierarchical Clustering: Objectives and Algorithms",
      "authors": "Moses Charikar, Vaggos Chatziafratis",
      "year": 2018,
      "role": "Approximation/stability analysis for hierarchical clustering algorithms including single-linkage",
      "relationship_sentence": "Techniques and bounds from this line of work inform how single-linkage behaves under perturbations and structural conditions, supporting the present paper\u2019s analysis that performance is preserved when moving from full instances to subsampled proxies."
    }
  ],
  "synthesis_narrative": "The paper advances data-driven algorithm selection by formalizing size generalization: predicting an algorithm\u2019s large-instance performance from evaluations on subsampled proxies. Two strands of prior work converge to enable this contribution. First, the learning-theoretic foundations of algorithm selection\u2014Data-Driven Algorithm Design (Balcan\u2013Sandholm\u2013Vitercik) and the PAC framework of Gupta\u2013Roughgarden\u2014establish how to select algorithms from empirical evaluations with provable generalization across instance distributions. The present work squarely addresses a central limitation identified in that literature: the computational cost of evaluating every algorithm on every full instance, by replacing full-instance evaluations with principled proxy evaluations.\nSecond, subsampling and coreset theory for clustering (Feldman\u2013Langberg) provides the structural reason small representative subsets can approximate global objectives. This perspective is operationalized for concrete algorithms analyzed here. For k-means, the seeding guarantees of k-means++ (Arthur\u2013Vassilvitskii) and its scalable sampling-based variant k-means|| (Bahmani et al.) show how performance depends on sample coverage of cluster structure, directly motivating and informing size-generalization bounds. For hierarchical clustering, objective-based analyses (Dasgupta; Charikar\u2013Chatziafratis) supply performance metrics and approximation/stability tools for single-linkage, clarifying when subsamples preserve the algorithm\u2019s outputs and costs. Together, these works furnish both the statistical selection lens and the subsampling approximations needed to rigorously justify evaluating algorithms on smaller, derived instances while preserving their comparative ranking on the original problems.",
  "analysis_timestamp": "2026-01-06T23:42:48.127270"
}
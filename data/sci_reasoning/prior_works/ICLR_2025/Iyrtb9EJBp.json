{
  "prior_works": [
    {
      "title": "Self-RAG: Learning to Retrieve, Generate, and Critique for Reliable Question Answering",
      "authors": "Akari Asai et al.",
      "year": 2023,
      "arxiv_id": "2310.11511",
      "role": "Inspiration",
      "relationship_sentence": "Self-RAG\u2019s use of critique/control tokens to assess evidence and regulate generation directly inspires the paper\u2019s learning-to-refuse and grounded-attribution alignment for RAG."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "arxiv_id": "2212.08073",
      "role": "Inspiration",
      "relationship_sentence": "Constitutional AI establishes refusal as an explicit alignment target, which is adapted here to evidence-grounded refusals when retrieved support is insufficient."
    },
    {
      "title": "FActScore: Fine-grained Evaluation of Factual Consistency in Text Generation",
      "authors": "Philippe Laban et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "FActScore\u2019s claim-level decomposition and evidence verification paradigm underpins the paper\u2019s grounded-attribution component within the Trust-Score metric."
    },
    {
      "title": "Attributable to Identified Sources? Measuring Source Attribution in Language Modeling",
      "authors": "Hannah Rashkin et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work formalizes the notion of source attribution, directly informing the paper\u2019s definition and measurement of citation quality and grounding in RAG outputs."
    },
    {
      "title": "ASQA: Factoid Answers for Ambiguous Questions",
      "authors": "Sewon Min et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "ASQA\u2019s ambiguous long-form QA setting motivates evaluating both grounded multi-facet answers and appropriate abstention, which the paper targets with Trust-Score and Trust-Align."
    },
    {
      "title": "QAMPARI: A Benchmark for Open-domain Questions with Many Answers",
      "authors": "Mor Geva et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "QAMPARI\u2019s many-answer formulation stresses coverage with supported evidence, shaping the metric\u2019s emphasis on attribution quality and refusal when coverage is inadequate."
    },
    {
      "title": "ELI5: Long Form Question Answering",
      "authors": "Angela Fan et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "ELI5\u2019s long-form QA emphasizes justification and verifiability, directly motivating the metric\u2019s groundedness and the alignment objective to produce citations or refuse."
    }
  ],
  "synthesis_narrative": "Self-RAG introduced control and critique signals that let a model reflect on retrieved evidence and regulate generation, offering a concrete mechanism for reliability-by-design in open-domain QA. Constitutional AI showed that refusals can be trained as an explicit alignment target, providing a general template for shaping when models should decline to answer. FActScore advanced claim-level evaluation by decomposing outputs into atomic facts and verifying them against evidence, spotlighting the need for granular, evidence-aware scoring of long-form outputs. Work on source attribution by Rashkin et al. formalized what it means for generated content to be attributable to identified sources, laying conceptual groundwork for measuring citation quality and grounding. ASQA posed ambiguous questions requiring multi-faceted, consolidated answers with supporting evidence, while QAMPARI stressed many-answer coverage with support, and ELI5 emphasized long-form, verifiable justifications\u2014collectively defining challenging evaluation regimes where hallucination, poor attribution, and overconfident answering are prevalent. Together, these strands revealed a gap: reliable RAG demands a holistic metric that jointly assesses grounded attribution and correct abstention, and a training paradigm that aligns models to those behaviors. Building on critique/reflection and refusal as alignment targets, and on claim-level and attribution-aware evaluation, the paper synthesizes these insights into Trust-Score for measuring trustworthiness and Trust-Align for evidence-grounded refusal and citation, making a natural next step beyond prior RAG prompting and tuning approaches.",
  "target_paper": {
    "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse",
    "authors": "Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Large Language Models, Trustworthiness, Hallucinations, Retrieval Augmented Generation",
    "abstract": "LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effectively adapt LLMs to the RAG task as measured by Trust-Score. Consequently, we propose Trust-Align, a method to align LLMs for improved Trust-Score performance. 26 out of 27 models aligned using Trust-Align substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (\u219112.56), QAMPARI (\u219136.04), and ELI5 (\u219117.69). Trust-Align also significantly enhances models\u2019 ability to correctly refuse and provide quality citations. We also demonstrate the effectiv",
    "openreview_id": "Iyrtb9EJBp",
    "forum_id": "Iyrtb9EJBp"
  },
  "analysis_timestamp": "2026-01-06T06:25:20.317083"
}
{
  "prior_works": [
    {
      "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
      "authors": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew M. Stuart, Anima Anandkumar",
      "year": 2021,
      "role": "Foundational architecture/baseline",
      "relationship_sentence": "DINOZAUR directly targets the FNO\u2019s dense Fourier-mode multiplier by replacing it with a heat-kernel-inspired diffusion multiplier, addressing FNO\u2019s overparameterization and enabling native UQ."
    },
    {
      "title": "Diffusion Kernels on Graphs and Other Discrete Input Spaces",
      "authors": "Risi Kondor, John Lafferty",
      "year": 2002,
      "role": "Core mathematical inspiration for diffusion-based spectral filters",
      "relationship_sentence": "Established diffusion kernels e^{-\u03b2L} as positive-definite, dimension-agnostic propagators, motivating DINOZAUR\u2019s exponential spectral multiplier (e^{-t|k|^2}) with a single \u2018time\u2019 parameter per channel."
    },
    {
      "title": "Scale-Space Theory in Computer Vision",
      "authors": "Tony Lindeberg",
      "year": 1994,
      "role": "Scale-space/heat-kernel foundation",
      "relationship_sentence": "The scale-space view of Gaussian (heat-kernel) smoothing with a single scale parameter directly informs DINOZAUR\u2019s dimensionality-independent diffusion multiplier design."
    },
    {
      "title": "Wavelets on Graphs via Spectral Graph Theory",
      "authors": "David K. Hammond, Pierre Vandergheynst, R\u00e9mi Gribonval",
      "year": 2011,
      "role": "Spectral filtering with heat-kernel-like parametrizations",
      "relationship_sentence": "Showed that smooth, Laplacian-eigenvalue\u2013based filters (including heat kernels) yield localized, stable convolutions, underpinning DINOZAUR\u2019s choice of an exponential spectral filter governed by time."
    },
    {
      "title": "Spectral Networks and Locally Connected Networks on Graphs",
      "authors": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun",
      "year": 2014,
      "role": "Precedent for learning spectral multipliers",
      "relationship_sentence": "Introduced learned spectral multipliers; DINOZAUR can be viewed as constraining this multiplier family to the heat-kernel manifold, drastically reducing parameters while preserving spectral bias."
    },
    {
      "title": "Weight Uncertainty in Neural Networks",
      "authors": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra",
      "year": 2015,
      "role": "Variational Bayesian methodology for neural parameters",
      "relationship_sentence": "Provides the practical variational inference (Bayes-by-Backprop) machinery that DINOZAUR uses to place priors over per-channel diffusion times and obtain calibrated, spatially aware UQ."
    },
    {
      "title": "DeepONet: Learning Nonlinear Operators for Differential Equations",
      "authors": "Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, George Em Karniadakis",
      "year": 2021,
      "role": "Operator learning paradigm and bridge to probabilistic operator UQ variants",
      "relationship_sentence": "Established operator-learning as mapping between function spaces and spawned probabilistic DeepONet variants, informing DINOZAUR\u2019s framing of Bayesian neural operators for UQ."
    }
  ],
  "synthesis_narrative": "DINOZAUR\u2019s core innovation\u2014replacing the Fourier Neural Operator\u2019s dense spectral multiplier with a heat-kernel diffusion multiplier and endowing it with Bayesian uncertainty\u2014builds on two converging lines of work. From operator learning, the Fourier Neural Operator established a powerful spectral architecture for PDE surrogates but relied on large, dense mode-mixing tensors without native UQ. DeepONet further cemented the operator-learning paradigm and inspired probabilistic operator variants, motivating DINOZAUR\u2019s goal of intrinsic, operator-level uncertainty rather than post hoc heuristics.\nFrom spectral and diffusion theory, scale-space and diffusion-kernel foundations (Lindeberg; Kondor & Lafferty) show that Gaussian/heat-kernel propagators offer dimension-agnostic, stable smoothing controlled by a single time/scale parameter. Spectral signal-processing works (Hammond et al.) formalized how smooth, eigenvalue-dependent filters\u2014especially heat kernels\u2014yield localized, well-conditioned operations, while early spectral networks (Bruna et al.) demonstrated the learnability of spectral multipliers themselves. DINOZAUR synthesizes these insights by constraining the FNO\u2019s learned multiplier to the heat-kernel manifold, yielding a dimensionality-independent diffusion multiplier with one learnable time per channel, drastically reducing parameters and memory.\nFinally, for uncertainty, variational Bayesian neural network methodology (Blundell et al.) provides the practical inference machinery to place priors on these per-channel diffusion times, producing calibrated, spatially structured UQ that respects the operator\u2019s geometry. The result is a parameter-efficient, diffusion-grounded neural operator with native Bayesian uncertainty\u2014directly traceable to FNO, diffusion kernels/scale-space, spectral filtering theory, and modern variational Bayes.",
  "analysis_timestamp": "2026-01-07T00:02:04.940672"
}
{
  "prior_works": [
    {
      "title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers (PatchTST)",
      "authors": [
        "... et al."
      ],
      "year": 2023,
      "role": "Established patch-based modeling for time series forecasting",
      "relationship_sentence": "This work popularized adjacent, fixed-length patch partitioning and patch embeddings for LTSF, providing the direct baseline and motivating the paper\u2019s claim that fixed adjacency yields a rigid representation space that SRS overcomes via selective patching and reassembly."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "et al."
      ],
      "year": 2020,
      "role": "Originated patchification as a representation strategy",
      "relationship_sentence": "ViT\u2019s core idea of tokenizing inputs into patches underlies the paper\u2019s patch perspective; SRS extends this principle by learning which patches to keep and how to reorganize them to form a more expressive representation space for forecasting."
    },
    {
      "title": "TokenLearner: Adaptive Space-Tokenization for Vision Transformers",
      "authors": [
        "Michael S. Ryoo",
        "A. Piergiovanni",
        "Anurag Arnab",
        "Mostafa Dehghani",
        "et al."
      ],
      "year": 2021,
      "role": "Introduced learnable selection of informative tokens",
      "relationship_sentence": "TokenLearner\u2019s adaptive token selection directly informs the SRS module\u2019s Selective Patching, where the model learns to pick the most informative temporal patches instead of relying on dense, fixed partitions."
    },
    {
      "title": "Token Merging: Your ViT But Faster (ToMe)",
      "authors": [
        "Chris Bolya",
        "et al."
      ],
      "year": 2023,
      "role": "Demonstrated dynamic token re-aggregation for better efficiency/structure",
      "relationship_sentence": "ToMe\u2019s idea of merging tokens to restructure representations parallels SRS\u2019s Dynamic Reassembly, which shuffles and recombines selected patches to build a flexible, task-aligned representation space."
    },
    {
      "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": 2021,
      "role": "Showed that shifting local partitions improves cross-region interactions",
      "relationship_sentence": "Swin\u2019s shifted windows motivate breaking rigid locality; SRS generalizes this by learning to select and rearrange non-adjacent temporal patches, enabling interactions beyond fixed neighboring chunks."
    },
    {
      "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
      "authors": [
        "Haoyi Zhou",
        "Shanghang Zhang",
        "Jieqi Peng",
        "et al."
      ],
      "year": 2021,
      "role": "Pioneered sparsity via ProbSparse attention for salient information",
      "relationship_sentence": "Informer\u2019s focus on salient query-key interactions supports the SRS premise that forecasting benefits from concentrating capacity on informative context; SRS operationalizes this at the patch level via selective inclusion."
    },
    {
      "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
      "authors": [
        "Juho Lee",
        "Yoonho Lee",
        "Jungtaek Kim",
        "Adam R. Kosiorek",
        "Seungjin Choi",
        "Yee Whye Teh"
      ],
      "year": 2019,
      "role": "Provided tools for modeling sets without fixed order",
      "relationship_sentence": "Set Transformer\u2019s permutation-invariant processing informs SRS\u2019s view of selected patches as a set whose order can be dynamically reassembled, supporting the shuffle-and-recombine design for selective representation spaces."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014Selective Representation Space (SRS) with learnable Selective Patching and Dynamic Reassembly\u2014builds on the emergence of patch-based modeling for time series and advances in adaptive tokenization. PatchTST established that chunking time series into adjacent patches helps long-term forecasting, but its fixed, contiguous partitioning constrains expressivity. ViT laid the conceptual groundwork by showing that patchification can serve as a general tokenization strategy, later adapted to temporal data. To lift the rigidity of fixed partitions, the authors borrow from vision works on adaptive token selection: TokenLearner demonstrates that models can learn which tokens are informative, motivating SRS\u2019s Selective Patching to pick salient temporal patches rather than densely using all adjacent segments. Complementarily, ToMe\u2019s token merging highlights the value of re-aggregating tokens to form better-structured representations, an idea echoed in SRS\u2019s Dynamic Reassembly that shuffles and recombines chosen patches to maximize contextual utility. Swin Transformer\u2019s shifted windows inspire breaking strict locality to enable cross-window interactions, which SRS generalizes through learned, non-adjacent patch relationships. Finally, Informer\u2019s ProbSparse attention and Set Transformer\u2019s permutation-invariant set processing validate the principle of focusing computation on salient context and treating selected elements as a re-orderable set. Together, these works directly shape SRS\u2019s selective, flexible patch space for stronger time series forecasting.",
  "analysis_timestamp": "2026-01-07T00:05:12.557801"
}
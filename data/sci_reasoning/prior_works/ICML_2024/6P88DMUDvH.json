{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Established the core formulation of using external evaluators to define rewards for RL; VLM-CaR adopts this paradigm but replaces repeated, expensive feedback with a compiled, executable reward function."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated that AI feedback can substitute for human feedback; VLM-CaR leverages this idea by using a VLM as the feedback provider and then distilling that feedback into code to eliminate inference-time querying."
    },
    {
      "title": "SayCan: Grounding Language Models with Affordances for Robotic Control",
      "authors": "Michael Ahn et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "Showed that language models can decompose tasks and align plans with grounded signals; VLM-CaR builds on this capability by having VLMs decompose and formalize task success criteria into dense, executable reward code."
    },
    {
      "title": "Visual Programming: Compositional Visual Reasoning without Training",
      "authors": "Tanmay Gupta et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "Introduced the paradigm of LLMs generating executable Python that orchestrates vision tools; VLM-CaR directly extends this visual-programming idea to compile VLM understanding into runnable reward functions for RL."
    },
    {
      "title": "Code as Policies: Language Model Programs for Embodied Control",
      "authors": "Jacky Liang et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Showed that generating interpretable, verifiable code with LLMs can control embodied agents; VLM-CaR transfers this code-generation insight to the reward channel, producing code that evaluates task progress instead of actions."
    },
    {
      "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "authors": "Timo Schick et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated LLMs can learn to invoke external tools/APIs; VLM-CaR relies on this tool-use paradigm by having VLMs generate reward code that calls perception and geometric primitives to compute dense visual rewards."
    }
  ],
  "synthesis_narrative": "VLM-CaR\u2019s core contribution\u2014compiling vision-language model judgments into executable reward code\u2014sits at the intersection of reward learning from evaluators and programmatic composition of perception. The intellectual starting point is Christiano et al. (2017), which formalized training policies from external feedback. That formulation also exposed a practical bottleneck: feedback is expensive. Bai et al. (2022) showed AI feedback can replace humans, suggesting that a VLM can act as an automated judge; yet directly querying such models during RL is still costly. VLM-CaR resolves this by distilling the judge into code, thereby preserving alignment while eliminating repeated inference. This \u2018compilation\u2019 move is enabled by the visual-programming line. Gupta et al. (2023) established that LLMs can write Python to orchestrate perception modules, providing the template for turning high-level visual descriptions into runnable logic. Liang et al. (2023) reinforced that code generated by language models affords interpretability and reliability in control; VLM-CaR applies the same principle to the reward channel, yielding dense and fast reward evaluators. Finally, Ahn et al. (2022) demonstrated that language models can decompose tasks and ground them in affordances, a capability VLM-CaR leverages when translating task descriptions and visual concepts into stepwise, dense reward computations. Together, these works directly motivate and enable VLM-CaR\u2019s key idea: use a VLM once to produce code that codifies visual success criteria, thereby delivering accurate dense rewards without incurring the prohibitive cost of ongoing VLM queries.",
  "analysis_timestamp": "2026-01-06T23:09:26.453602"
}
{
  "prior_works": [
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "The paper\u2019s adaptive schedules are explicitly designed to beat the static compute-optimal frontier defined by Hoffmann et al., which formalized how to allocate compute between model size and data under fixed (non-adaptive) architectures."
    },
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Kaplan et al. established predictive compute\u2013performance scaling and a compute-optimal regime but assumed a static model shape; this static assumption is the explicit limitation the present work removes by allowing the model to change shape during training."
    },
    {
      "title": "Deep Learning Scaling is Predictable, Empirically",
      "authors": "Joel Hestness et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Hestness et al. provided the cross-domain empirical scaling-law framework that underpins the paper\u2019s premise of navigating between scaling regimes across modalities."
    },
    {
      "title": "Net2Net: Accelerating Learning via Knowledge Transfer",
      "authors": "Tianqi Chen et al.",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "Net2Net introduced function-preserving depth/width expansions, demonstrating that changing a model\u2019s shape mid-training can retain learned capabilities\u2014an operational enabler for the paper\u2019s adaptive model growth along a compute-optimal trajectory."
    },
    {
      "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "authors": "Tero Karras et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "Karras et al. showed that progressively increasing model capacity during training can improve efficiency and outcomes, directly motivating the paper\u2019s idea of traversing training with capacity growth rather than fixing architecture."
    },
    {
      "title": "Once-for-All: Train One Network and Specialize it for Efficient Deployment",
      "authors": "Han Cai et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Once-for-All demonstrates a single training process supporting multiple model shapes (width/depth/resolution), informing the paper\u2019s assertion that one can move across architectures during training; the present work reframes this adaptivity through compute-optimal scaling laws."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014compute-optimal training with an architecture that adapts its shape during learning\u2014sits at the intersection of two lines of work: scaling-law based compute optimality and techniques that enable mid-training architecture changes. Foundational scaling-law studies by Hestness et al. established predictable power-law behavior across modalities, while Kaplan et al. formalized compute-driven performance prediction and a compute-optimal regime under the key assumption of a static model. Hoffmann et al. refined this into the widely used Chinchilla-style static compute-optimal baseline that balances parameters and data at fixed architecture. The present paper directly targets the central limitation in these baselines\u2014the static-shape assumption\u2014by proposing adaptive models that traverse between scaling regimes during training to reduce the compute required for a target performance.\n\nOperationally, prior methods have shown that models can change shape without losing learned knowledge: Net2Net introduced function-preserving depth/width growth, and progressive growing of GANs demonstrated that staged capacity increases can improve training efficiency and quality. Complementing these, Once-for-All validated that a single training run can support many architectural instantiations, reinforcing the feasibility of moving across model shapes. Building on these ideas, the paper provides a principled framework that marries scaling-law compute optimality with adaptive capacity, deriving trajectories that strategically reallocate compute over training. In doing so, it establishes adaptive training as a superior path to the static compute-optimal frontier across modalities.",
  "analysis_timestamp": "2026-01-06T23:09:26.418392"
}
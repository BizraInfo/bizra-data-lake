{
  "prior_works": [
    {
      "title": "MLE-bench: Evaluating Machine Learning Engineering Agents on Real-World Competitions",
      "authors": "Minqi Jiang et al.",
      "year": 2024,
      "role": "Benchmark foundation",
      "relationship_sentence": "This work established the Kaggle-competition-based evaluation setting and MLE-bench lite protocol that the present paper directly targets and improves upon, providing the tasks, metrics, and environment for their SOTA results."
    },
    {
      "title": "TPOT: A Tree-Based Pipeline Optimization Tool for Automating Machine Learning",
      "authors": "Randal S. Olson et al.",
      "year": 2016,
      "role": "Evolutionary pipeline search",
      "relationship_sentence": "TPOT demonstrated that mutation/crossover operators over ML pipelines paired with evolutionary selection can effectively navigate complex solution spaces, directly informing the paper\u2019s operator-centric design and its evolutionary search baseline."
    },
    {
      "title": "Auto-sklearn 2.0: Hands-free AutoML via Meta-Learning",
      "authors": "Matthias Feurer et al.",
      "year": 2020,
      "role": "Structured pipeline space and search",
      "relationship_sentence": "Auto-sklearn\u2019s decomposition of ML solutions into modular components and hyperparameters influenced the paper\u2019s formalization of candidate solutions and operator sets that edit components step-by-step under a search policy."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "LLM deliberation as explicit search",
      "relationship_sentence": "ToT framed LLM reasoning as tree search over intermediate states, motivating the paper\u2019s treatment of agent behavior as a search policy over iterative modifications and enabling comparisons between greedy and tree-search strategies like MCTS."
    },
    {
      "title": "PromptBreeder: Self-Referential Self-Improvement in Large Language Models",
      "authors": "Chrisantha Fernando et al.",
      "year": 2023,
      "role": "Evolutionary operators for LLM-driven improvement",
      "relationship_sentence": "PromptBreeder showed that LLMs can generate and refine candidates via mutation and selection, directly inspiring the paper\u2019s design of operator sets and evolutionary policies for iteratively improving ML solutions."
    },
    {
      "title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch",
      "authors": "Esteban Real, Chen Liang, David R. So, Quoc V. Le",
      "year": 2020,
      "role": "Operators over ML primitives with evolutionary search",
      "relationship_sentence": "AutoML-Zero\u2019s use of primitive-level mutations to evolve end-to-end ML algorithms influenced the paper\u2019s view of operators as fundamental edits to ML artifacts and the evaluation of evolutionary search in this space."
    },
    {
      "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (AlphaZero)",
      "authors": "David Silver et al.",
      "year": 2017,
      "role": "MCTS as a general-purpose search policy",
      "relationship_sentence": "AlphaZero popularized MCTS as a powerful policy for exploring vast combinatorial spaces, underpinning the paper\u2019s adaptation of MCTS to guide exploration of ML candidate solutions and its comparison to greedy/evolutionary search."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014formalizing AI research agents as search policies operating over a space of ML candidate solutions and systematically studying the interplay between search strategy and operators\u2014sits at the intersection of AutoML, LLM-based deliberative search, and MLE-bench evaluation. MLE-bench provided the target environment and metrics, making its Kaggle-based tasks the natural proving ground and enabling the paper\u2019s state-of-the-art results on MLE-bench lite. From AutoML, TPOT contributed the concrete notion that operator-centric edits (mutation/crossover) over pipelines, coupled with evolutionary selection, are an effective way to traverse large ML design spaces; Auto-sklearn reinforced a modular view of solutions (components and hyperparameters) that can be incrementally edited, clarifying the object over which operators act. On the agentic side, Tree of Thoughts introduced explicit tree search over intermediate reasoning states for LLMs, motivating the paper\u2019s framing of agents as search policies and the inclusion of MCTS alongside greedy baselines. PromptBreeder showed that LLMs can themselves generate and evolve candidates via operator-driven self-improvement, directly shaping the design and evaluation of operator sets. Finally, AutoML-Zero and AlphaZero offered complementary evidence that primitive-level operators and MCTS can scale search in complex spaces, respectively, grounding the paper\u2019s comparative study of evolutionary and MCTS policies. Together, these works directly informed the paper\u2019s operator design, policy choices, and evaluation methodology.",
  "analysis_timestamp": "2026-01-07T00:21:33.134753"
}
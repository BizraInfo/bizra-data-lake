{
  "prior_works": [
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Rafael Rafailov et al.",
      "year": 2023,
      "role": "Preference learning framework",
      "relationship_sentence": "Established a scalable, label-efficient paradigm for training from pairwise preferences that directly motivates learning fine-grained reward models like MJ-VIDEO to guide generative models."
    },
    {
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparse Mixture-of-Experts",
      "authors": "William Fedus, Barret Zoph, Noam Shazeer",
      "year": 2021,
      "role": "MoE architecture foundation",
      "relationship_sentence": "Provided the core MoE design principles\u2014sparse expert routing and conditional computation\u2014that MJ-VIDEO adopts to select specialized experts for different fine-grained video evaluation criteria."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Vision\u2013language representation backbone",
      "relationship_sentence": "Supplied robust text\u2013video/image alignment features that underpin many alignment and preference signals; MJ-VIDEO builds on such vision\u2013language foundations to judge text\u2013video consistency."
    },
    {
      "title": "PickScore: Learning a Human-aligned Image Reward from Pairwise Preferences (built on Pick-a-Pic)",
      "authors": "Yuval Kirstain et al.",
      "year": 2023,
      "role": "Image preference reward modeling",
      "relationship_sentence": "Demonstrated that training a reward model on large-scale pairwise human preferences yields strong alignment with user judgments, directly inspiring MJ-VIDEO\u2019s preference-trained reward for videos."
    },
    {
      "title": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation",
      "authors": "Jiayi Weng et al.",
      "year": 2023,
      "role": "Multi-aspect reward modeling for generative media",
      "relationship_sentence": "Showed that a learned reward can capture fine-grained attributes (aesthetics, faithfulness, etc.), informing MJ-VIDEO\u2019s design to cover diverse aspects like alignment, safety, and coherence."
    },
    {
      "title": "VBench: Comprehensive Benchmark Suite for Video Generative Models",
      "authors": "Wang et al.",
      "year": 2023,
      "role": "Video generation benchmarking",
      "relationship_sentence": "Provided a structured, multi-faceted evaluation suite for text-to-video models, motivating MJ-BENCH-VIDEO\u2019s broader and finer-grained criteria and preference-based evaluation protocol."
    },
    {
      "title": "EvalCrafter: Benchmarking and Evaluating Text-to-Video Generation",
      "authors": "Ma et al.",
      "year": 2023,
      "role": "Evaluation methodology for T2V",
      "relationship_sentence": "Introduced systematic human/automatic evaluations for T2V, influencing MJ-BENCH-VIDEO\u2019s dataset curation and its emphasis on comprehensive, aspect-driven preference judgments."
    }
  ],
  "synthesis_narrative": "MJ-VIDEO\u2019s key contribution\u2014fine-grained preference benchmarking and a Mixture-of-Experts video reward model\u2014sits at the intersection of preference learning, video evaluation, and sparse expert architectures. On the learning side, Direct Preference Optimization (Rafailov et al.) catalyzed the shift toward directly leveraging pairwise preferences to align generative models without explicit supervised targets, laying conceptual groundwork for training reliable reward models. In image generation, PickScore and ImageReward validated that large-scale human preference data can be distilled into reward models capturing nuanced qualities such as faithfulness, aesthetics, and detail; MJ-VIDEO extends this paradigm from images to videos while expanding the attribute space to include coherence, consistency, safety, and bias. For architectural choices, Switch Transformers established effective sparse Mixture-of-Experts routing, enabling conditional selection of specialized experts. MJ-VIDEO adapts this to reward modeling so different experts specialize in distinct evaluation criteria, yielding more accurate, aspect-aware judgments. In evaluation and benchmarking, VBench and EvalCrafter defined early multi-dimensional standards for assessing text-to-video models. Their taxonomies and protocols informed MJ-BENCH-VIDEO\u2019s broader, finer-grained criteria and preference collection methodology. Finally, vision\u2013language representation advances like CLIP provide robust alignment features that underpin the assessment of text\u2013video consistency. Together, these works directly shaped MJ-VIDEO\u2019s design: a comprehensive preference benchmark and a MoE reward model that dynamically routes to fine-grained experts for precise, adaptable video preference judgments.",
  "analysis_timestamp": "2026-01-07T00:21:32.293967"
}
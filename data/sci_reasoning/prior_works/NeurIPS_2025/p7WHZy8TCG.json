{
  "prior_works": [
    {
      "title": "Neural Combinatorial Optimization with Reinforcement Learning",
      "authors": "Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, Samy Bengio",
      "year": 2016,
      "role": "Pioneered RL-based neural policies (pointer networks) for routing and introduced test-time active search.",
      "relationship_sentence": "MEMENTO addresses the same goal of instance-specific improvement at inference as active search, but replaces parameter fine-tuning with lightweight memory-driven adaptation of the action distribution."
    },
    {
      "title": "Attention, Learn to Solve Routing Problems!",
      "authors": "Wouter Kool, Herke van Hoof, Max Welling",
      "year": 2019,
      "role": "Established attention-based policies trained with REINFORCE and popularized inference-time sampling for routing.",
      "relationship_sentence": "MEMENTO augments the Kool et al. sampling regime by maintaining a memory across rollouts to update action probabilities based on observed outcomes instead of sampling independently."
    },
    {
      "title": "POMO: Policy Optimization with Multiple Optima for Reinforcement Learning",
      "authors": "Yeong-Dae Kwon, Jinkyoo Park, et al.",
      "year": 2020,
      "role": "Introduced multi-start rollouts exploiting permutation symmetries to improve routing solutions.",
      "relationship_sentence": "MEMENTO operates in a similar multi-rollout setting but adds an adaptive memory that biases subsequent rollouts using feedback from earlier ones to better exploit the compute budget."
    },
    {
      "title": "Ant Colony System: A Cooperative Learning Approach to the Traveling Salesman Problem",
      "authors": "Marco Dorigo, Luca M. Gambardella",
      "year": 1997,
      "role": "Classical pheromone-based metaheuristic that adapts edge selection probabilities from prior solutions.",
      "relationship_sentence": "MEMENTO\u2019s dynamic, outcome-driven updates of the action distribution are a neural analogue of pheromone updates guiding constructive search in ACS."
    },
    {
      "title": "The Adaptive Large Neighborhood Search Heuristic for the Pickup and Delivery Problem with Time Windows",
      "authors": "Stefan Ropke, David Pisinger",
      "year": 2006,
      "role": "Established ALNS with adaptive operator selection based on historical performance.",
      "relationship_sentence": "MEMENTO imports the ALNS principle of performance-driven adaptation into neural policies by updating action biases online during inference."
    },
    {
      "title": "Neural Large Neighborhood Search for the Capacitated Vehicle Routing Problem",
      "authors": "Andreas Hottung, Kevin Tierney",
      "year": 2020,
      "role": "Hybrid learned destroy/repair that iteratively improves solutions using feedback within a fixed budget.",
      "relationship_sentence": "MEMENTO similarly leverages search-time feedback but applies it to a generative policy\u2019s action distribution rather than operator selection."
    },
    {
      "title": "NeuroLKH: Combining Deep Learning with Lin\u2013Kernighan\u2013Helsgaun for the Traveling Salesman Problem",
      "authors": "Shuai Xin, Wen Song, Zhiguang Cao, Jie Zhang, Andrew Lim",
      "year": 2021,
      "role": "Guides a powerful heuristic with neural predictions to improve inference-time search.",
      "relationship_sentence": "MEMENTO shares the idea of using learned signals to steer search during inference, but does so via a memory that adapts action probabilities from rollout outcomes."
    }
  ],
  "synthesis_narrative": "MEMENTO\u2019s key contribution\u2014dynamically updating a neural policy\u2019s action distribution at inference using memory\u2014sits at the intersection of neural routing policies and classical adaptive metaheuristics. On the neural side, Bello et al. introduced RL-trained constructive policies and the notion of instance-specific improvement via active search; Kool et al. and POMO then made sampling-based inference and multi-start rollouts standard tools for exploiting a compute budget. These works established that much of the performance of neural solvers comes from how they search at inference, yet their rollouts are typically independent or rely on expensive parameter fine-tuning. On the classical side, Ant Colony System and Adaptive LNS provided the blueprint for outcome-driven adaptation: pheromone trails and adaptive operator weights encode a memory of what has worked, steering future decisions without changing the underlying algorithmic parameters. Modern hybrids like Neural LNS and NeuroLKH further demonstrated that learned signals can guide powerful search procedures at test time. MEMENTO fuses these lines: within the sampling/multi-start paradigm of attention-based neural solvers, it introduces a lightweight, online memory that updates action biases based on the quality of previous partial or complete solutions. This retains the flexibility and speed of neural policies, avoids costly fine-tuning, and bridges classical adaptive-memory heuristics with learned constructive decoding to better utilize the available computational budget.",
  "analysis_timestamp": "2026-01-07T00:02:04.951959"
}
{
  "prior_works": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "VoT explicitly inherits the Chain-of-Thought principle of decomposing complex problems into sequential sub-steps, providing the core reasoning template that VoT adapts from language to video."
    },
    {
      "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
      "authors": "Denny Zhou et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "The least-to-most strategy of solving easier perceptual subproblems before higher-level reasoning directly motivates VoT\u2019s staged pipeline from pixel-level perception to cognitive interpretation."
    },
    {
      "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "authors": "Ranjay Krishna et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Visual Genome introduced scene graphs as a structured representation of objects, attributes, and relations, which MotionEpic generalizes to the video domain via spatio-temporal scene graphs for fine-grained grounding."
    },
    {
      "title": "Action Genome: Actions as Compositions of Spatio-Temporal Scene Graphs",
      "authors": "Ji et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Action Genome operationalized spatio-temporal scene graphs (STSG) for video, directly enabling MotionEpic\u2019s choice of STSG as the backbone representation for pixel-level spatio-temporal grounding."
    },
    {
      "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision-Language Models",
      "authors": "Maaz et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "As a representative Video-LLM, Video-ChatGPT relies on coarse clip/frame features and lacks fine-grained grounding, a limitation VoT/MotionEpic addresses by integrating STSG and step-wise reasoning."
    },
    {
      "title": "TVQA: Localized, Compositional Video Question Answering",
      "authors": "Jie Lei et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "TVQA formalized temporally localized, multi-step video QA, shaping the problem setup that VoT targets and motivating its need for temporal grounding plus compositional reasoning."
    },
    {
      "title": "CLEVRER: CoLlision Events for Video REasoning",
      "authors": "Kexin Yi et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "CLEVRER\u2019s emphasis on causal, counterfactual video reasoning exposed the gap between low-level perception and high-level cognition, directly motivating VoT\u2019s perception-to-cognition reasoning pipeline."
    }
  ],
  "synthesis_narrative": "Video-of-Thought (VoT) fuses two threads of prior work: step-wise reasoning from language models and structured spatio-temporal perception from video understanding. Chain-of-Thought (Wei et al., 2022) provides the core template of decomposing complex tasks into intermediate steps, while Least-to-Most Prompting (Zhou et al., 2022) specifically inspires VoT\u2019s curriculum-like flow from easier perceptual subproblems to harder cognitive inference\u2014mirrored in VoT\u2019s progression from pixel-grounded observations to semantic/causal conclusions. On the perception side, Visual Genome (Krishna et al., 2017) established scene graphs as a structured interface between vision and language, and Action Genome (2020) extended this to videos as spatio-temporal scene graphs (STSG). MotionEpic builds directly on this lineage by embedding STSG into an MLLM to achieve fine-grained, pixel-level spatial-temporal grounding that prior Video-LLMs lacked. Representative video MLLMs such as Video-ChatGPT (Maaz et al., 2023) crystallized the gap: strong language priors but coarse visual grounding, motivating VoT\u2019s integration of STSG with step-by-step reasoning. Finally, problem formulations from TVQA (Lei et al., 2018)\u2014localized, compositional, temporally grounded QA\u2014and the causal demands highlighted by CLEVRER (Yi et al., 2020) define the reasoning challenges VoT targets. Together, these works directly shape VoT\u2019s core innovation: a video-native, STSG-grounded, chain-of-thought framework that bridges low-level perception and high-level cognition.",
  "analysis_timestamp": "2026-01-06T23:09:26.496372"
}
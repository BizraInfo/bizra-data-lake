{
  "prior_works": [
    {
      "title": "Interpretability Beyond Feature Attribution: Testing with Concept Activation Vectors (TCAV)",
      "authors": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Vi\u00e9gas, Rory Sayres",
      "year": 2018,
      "role": "Conceptual precursor for concept vectors in internal activations",
      "relationship_sentence": "CDC generalizes TCAV\u2019s idea of linear concept directions from supervised, label-defined concepts to unsupervised, behavior-differentiating directions that are explicitly required to flip a model\u2019s behavior."
    },
    {
      "title": "Towards Automatic Concept-Based Explanations (ACE)",
      "authors": "Amirata Ghorbani, Abubakar Abid, James Zou",
      "year": 2019,
      "role": "Algorithmic inspiration for unsupervised concept discovery",
      "relationship_sentence": "While ACE clusters input-space segments to discover concepts and then probes with TCAV, CDC shifts discovery into LM activation space and replaces external labels/segmentations with a behavior-driven objective, addressing ACE\u2019s reliance on predefined features."
    },
    {
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": "Aaron van den Oord, Yazhe Li, Oriol Vinyals",
      "year": 2018,
      "role": "Foundational technique for contrastive objectives (InfoNCE)",
      "relationship_sentence": "CDC builds on the contrastive paradigm by introducing a constrained contrastive objective tailored to isolate latent directions that minimally and causally differentiate target behaviors, rather than maximizing general mutual-information proxies."
    },
    {
      "title": "Contrastive Explanations Method (CEM): A Pertinent Positive and Negative Approach",
      "authors": "Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chih-Chun Hung, Paishun Tu, Karthikeyan Shanmugam, Payel Das, et al.",
      "year": 2018,
      "role": "Counterfactual/contrastive explanation with sparsity constraints",
      "relationship_sentence": "CDC adapts CEM\u2019s core insight\u2014identify minimal, sparse changes sufficient/necessary to alter a decision\u2014by moving the optimization into latent activation space and enforcing sparsity over concepts that must change to elicit a new LM behavior."
    },
    {
      "title": "ROME: Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",
      "year": 2022,
      "role": "Causal intervention on internal representations to change behavior",
      "relationship_sentence": "CDC\u2019s notion of causal differentiating concepts aligns with ROME\u2019s demonstration that targeted internal edits can reliably change outputs, but CDC discovers such directions unsupervised and favors sparse, concept-level changes rather than parameter updates."
    },
    {
      "title": "Interchange Interventions for Causal Abstraction in Neural Networks",
      "authors": "Atticus Geiger, et al.",
      "year": 2021,
      "role": "Methodological basis for testing causal roles of internal variables",
      "relationship_sentence": "CDC operationalizes a similar causal abstraction idea\u2014alter specific internal factors and observe behavior change\u2014by learning interpretable latent directions that mediate behaviors and can be perturbed to produce targeted output differences."
    },
    {
      "title": "Toward Causal Representation Learning",
      "authors": "Bernhard Sch\u00f6lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Ignavier Ng, Ilya Sutskever, Yoshua Bengio",
      "year": 2021,
      "role": "Theoretical grounding for latent causal variables and sparse interventions",
      "relationship_sentence": "CDC\u2019s assumption that behavior changes arise from sparse interventions on underlying concepts is grounded in the causal representation learning literature\u2019s independent causal mechanisms and sparse mechanism shift principles."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014unsupervised discovery of causal differentiating concepts as latent directions in language model activations that must be changed to elicit different behaviors\u2014sits at the intersection of concept-based interpretability, causal representation learning, and contrastive/counterfactual optimization. TCAV established that linear directions in internal representations can encode human-meaningful concepts, while ACE showed concepts can be discovered automatically, albeit with reliance on input-space heuristics and labels. CDC retains the interpretability of concept directions but removes supervision by tying concept discovery directly to behavior change in the model.\n\nMethodologically, the work adapts contrastive learning (CPC/InfoNCE) to a constrained setting where positives/negatives are defined by behaviors, not labels, and incorporates sparsity to enforce minimal concept changes\u2014an idea borrowed from counterfactual explanation methods like CEM. The causal framing is influenced by ROME and interchange intervention studies, which demonstrate that targeted manipulations of internal representations can reliably flip specific outputs; CDC learns such steering directions without predefined targets or parameter edits. Finally, the paper\u2019s identifiability intuition\u2014that only a sparse subset of latent causal factors needs to change across behaviors\u2014builds on principles from causal representation learning (independent causal mechanisms and sparse mechanism shifts). Together, these lines of work directly motivate CDC\u2019s constrained contrastive objective, its sparsity prior over concept changes, and its causal validation protocol, resulting in an interpretable, label-free approach to uncovering the minimal internal factors that mediate language model behaviors.",
  "analysis_timestamp": "2026-01-07T00:02:04.933592"
}
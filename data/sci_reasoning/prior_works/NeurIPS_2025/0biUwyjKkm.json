{
  "prior_works": [
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Ahn et al.",
      "year": 2022,
      "role": "Conceptual foundation for language-to-affordance grounding and task decomposition",
      "relationship_sentence": "OpenHOI\u2019s MLLM jointly grounds affordances and decomposes free-form instructions into sub-tasks, directly echoing SayCan\u2019s principle of coupling LLM semantics with environment/affordance signals for executable long-horizon plans."
    },
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Methodological precedent for multimodal LLMs via visual instruction tuning",
      "relationship_sentence": "OpenHOI builds on the LLaVA-style visual instruction tuning recipe to align language with visual inputs, adapting the paradigm to 3D inputs for open-vocabulary HOI guidance and instruction following."
    },
    {
      "title": "3D-LLM: Injecting the 3D World into Large Language Models",
      "authors": "Chen et al.",
      "year": 2023,
      "role": "Model component precedent for 3D-aware multimodal LLMs",
      "relationship_sentence": "The paper\u2019s 3D MLLM for affordance grounding/localization is a natural extension of 3D-LLM\u2019s approach to infusing 3D scene representations into LLMs, enabling object-part reasoning (e.g., handles, buttons) in 3D."
    },
    {
      "title": "Where2Act: From Pixels to Actions for Articulated Object Manipulation",
      "authors": "Wang et al.",
      "year": 2021,
      "role": "Algorithmic foundation for actionable region/affordance localization on objects",
      "relationship_sentence": "OpenHOI\u2019s precise localization of interaction regions inherits the idea of predicting actionable object parts from Where2Act, extending it to open-vocabulary language grounding and long-horizon HOI sequences."
    },
    {
      "title": "Human Motion Diffusion Model (MDM)",
      "authors": "Tevet et al.",
      "year": 2022,
      "role": "Methodological precedent for diffusion-based motion synthesis",
      "relationship_sentence": "OpenHOI\u2019s affordance-driven diffusion generator for hand-object interactions is built on the MDM paradigm, using diffusion to synthesize temporally coherent, conditioned motion sequences."
    },
    {
      "title": "PhysDiff: Physics-Guided Human Motion Diffusion",
      "authors": "Yuan et al.",
      "year": 2023,
      "role": "Algorithmic precedent for physics guidance/refinement in motion generation",
      "relationship_sentence": "OpenHOI\u2019s training-free physics refinement echoes PhysDiff\u2019s strategy of injecting physics constraints into diffusion sampling to enforce contact and feasibility, adapted here to hand\u2013object contact and manipulation."
    },
    {
      "title": "ArtiGrasp: Learning to Interact with Articulated Objects in 3D",
      "authors": "Huang et al.",
      "year": 2023,
      "role": "Methodological precedent for articulation-aware hand\u2013object interaction synthesis",
      "relationship_sentence": "OpenHOI\u2019s focus on object-part affordances (handles/buttons) and physically plausible hand interactions draws directly from ArtiGrasp\u2019s articulation- and contact-aware modeling of 3D hand\u2013object manipulation."
    }
  ],
  "synthesis_narrative": "OpenHOI\u2019s core contribution\u2014open-world HOI synthesis that follows free-form language while generalizing to novel objects\u2014emerges from the convergence of three research lines. First, SayCan established that LLMs become actionable planners when grounded in affordances and cost-to-go estimates, which OpenHOI adapts to hand\u2013object manipulation by jointly learning semantic task decomposition and affordance grounding in a single 3D MLLM. Second, visual-instruction-tuned MLLMs (LLaVA) and their 3D counterparts (3D-LLM) provide the training recipe and representational interface to align language with geometric context; OpenHOI extends this to localize interaction-relevant parts (e.g., handles, buttons) and to parse complex, long-horizon commands into executable sub-tasks in 3D. Third, for physically plausible synthesis, diffusion-based motion generation (MDM) gives the backbone for producing long, coherent interaction trajectories. This is complemented by physics-aware guidance exemplified by PhysDiff, inspiring OpenHOI\u2019s training-free refinement to enforce contact and feasibility without extra learning. Finally, HOI- and articulation-focused work such as Where2Act and ArtiGrasp directly inform OpenHOI\u2019s affordance-driven conditioning: predicting actionable regions on articulated objects and modeling contact/part interactions. Together, these works concretely enable OpenHOI\u2019s key innovation: an affordance-conditioned diffusion pipeline steered by a 3D MLLM that both grounds open-vocabulary semantics in object parts and decomposes tasks, yielding long-horizon, physically consistent hand\u2013object interactions for unseen objects.",
  "analysis_timestamp": "2026-01-07T00:21:32.300538"
}
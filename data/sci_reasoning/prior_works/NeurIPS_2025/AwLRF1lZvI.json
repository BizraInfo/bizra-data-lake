{
  "prior_works": [
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Michael Ahn, Anthony Brohan, et al.",
      "year": 2022,
      "role": "Language-guided control with affordance grounding",
      "relationship_sentence": "Established the use of language as an explicit intermediate representation to guide action selection in embodied agents, motivating MIMIC\u2019s use of inner speech as a structured, steerable internal plan."
    },
    {
      "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "authors": "Anthony Brohan, Alex X. Lee, et al.",
      "year": 2023,
      "role": "Language-conditioned visuomotor policies via VLMs",
      "relationship_sentence": "Demonstrated that vision-language-action models can map observations and language to actions, informing MIMIC\u2019s strategy of conditioning a policy on textual intent to enable steerability at inference."
    },
    {
      "title": "InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations",
      "authors": "Yunzhu Li, Jiaming Song, Stefano Ermon",
      "year": 2017,
      "role": "Latent-variable imitation for multimodal behaviors",
      "relationship_sentence": "Showed that introducing latent codes can capture diverse expert behaviors, directly inspiring MIMIC\u2019s replacement of unstructured latent variables with language (inner speech) to model behavioral diversity."
    },
    {
      "title": "Diffuser: Diffusion Models for Planning",
      "authors": "Michael Janner, Yilun Du, Joshua T. Goodman, Sergey Levine, Chelsea Finn",
      "year": 2022,
      "role": "Diffusion-based sequence generation for control",
      "relationship_sentence": "Introduced diffusion models for generating action trajectories, underpinning MIMIC\u2019s diffusion-based behavior cloning policy that samples diverse actions conditioned on inner speech."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, et al.",
      "year": 2022,
      "role": "Intermediate natural-language reasoning as latent computation",
      "relationship_sentence": "Demonstrated that explicit textual rationales improve problem solving, supporting MIMIC\u2019s premise that language (inner speech) can serve as an internal reasoning substrate for sequential decision-making."
    },
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee",
      "year": 2023,
      "role": "Vision-language modeling for visual description and reasoning",
      "relationship_sentence": "Provided a practical VLM capable of generating faithful image-grounded text, enabling MIMIC\u2019s linguistic scaffolding step that produces inner-speech supervision from observations."
    },
    {
      "title": "Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data",
      "authors": "Boris Ivanovic, Karen Leung, Edward Schmerling, Marco Pavone",
      "year": 2020,
      "role": "CVAE-based modeling of multimodal, non-Markovian human trajectories",
      "relationship_sentence": "Validated conditional VAEs for capturing diverse, history-dependent human behaviors, directly informing MIMIC\u2019s CVAE that maps observations to inner-speech intents."
    }
  ],
  "synthesis_narrative": "MIMIC\u2019s core idea\u2014using inner speech as a steerable internal representation to capture diverse, non-Markovian human behaviors\u2014sits at the intersection of language-conditioned control, latent-variable imitation, and diffusion-based action generation. SayCan established language as an actionable intermediate plan tethered to affordances, motivating MIMIC\u2019s use of textual intent to guide low-level control. RT-2 showed that vision-language-action models can map visual context and language to actions in a unified policy, reinforcing the feasibility and benefits of language-conditioned, steerable behavior. To model behavioral diversity, InfoGAIL demonstrated that latent variables can faithfully capture multiple expert modes; MIMIC advances this by replacing opaque latents with interpretable language, enabling both expressivity and user steering. The diffusion backbone draws from Diffuser, which introduced diffusion models as powerful generators of action sequences; MIMIC adapts this to behavior cloning, conditioning the sampler on inner speech to produce diverse yet aligned actions. Chain-of-Thought provided the conceptual bridge that explicit textual rationales can serve as useful intermediate computations, bolstering the hypothesis that inner speech can function as an internal policy state. Practically, LLaVA furnishes a capable VLM to scaffold training by producing observation-grounded linguistic descriptions and rationales that supervise the inner-speech generator. Finally, Trajectron++ validated CVAEs for multi-modal, history-aware human behavior modeling, shaping MIMIC\u2019s CVAE that maps observations to language intents. Together, these works enabled MIMIC\u2019s key contribution: a language-latent, VLM-scaffolded, diffusion-policy framework that is both diverse and steerable.",
  "analysis_timestamp": "2026-01-06T23:42:48.115714"
}
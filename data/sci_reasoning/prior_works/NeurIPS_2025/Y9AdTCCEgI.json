{
  "prior_works": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "authors": "Bernhard Kerbl, Thomas Leimk\u00fchler, Georgios Kopanas, George Drettakis",
      "year": 2023,
      "role": "Foundational representation and training heuristics",
      "relationship_sentence": "OnlineSplatter builds directly on the 3D Gaussian primitive, SH-based view dependence, and densification/pruning strategies of 3DGS, extending them to an online, pose-free, object-centric reconstruction pipeline."
    },
    {
      "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
      "authors": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng",
      "year": 2020,
      "role": "View-direction modeling and volumetric rendering paradigm",
      "relationship_sentence": "The paper\u2019s explicit directional keys echo NeRF\u2019s core idea of conditioning appearance on viewing direction, but implements it as a feed-forward key in a memory-readout over 3D Gaussians rather than through optimization of a volumetric MLP."
    },
    {
      "title": "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections",
      "authors": "Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth",
      "year": 2021,
      "role": "Appearance embeddings to disentangle illumination/appearance from geometry",
      "relationship_sentence": "OnlineSplatter\u2019s latent appearance-geometry keys are conceptually aligned with NeRF-W\u2019s per-image appearance codes, enabling robust fusion across frames with varying appearance without relying on poses."
    },
    {
      "title": "Nerfies: Deformable Neural Radiance Fields",
      "authors": "Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B. Goldman, Steven M. Seitz, Ricardo Martin-Brualla",
      "year": 2021,
      "role": "Canonicalization and handling non-rigid object motion",
      "relationship_sentence": "The idea of maintaining an object-centric canonical space under free motion informs OnlineSplatter\u2019s first-frame anchoring and progressive refinement of a canonical 3D representation without explicit pose/deformation optimization."
    },
    {
      "title": "Space-Time Memory Networks for Video Object Segmentation",
      "authors": "Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim",
      "year": 2019,
      "role": "Key\u2013value memory with spatially guided readout for temporally consistent fusion",
      "relationship_sentence": "OnlineSplatter\u2019s dual-key memory (latent appearance-geometry keys plus explicit directional keys) and spatial-guided readout directly adopt the key\u2013value memory paradigm to fuse current-frame features with an aggregated object state."
    },
    {
      "title": "DynamicFusion: Reconstruction and Tracking of Non-rigid Scenes in Real-Time",
      "authors": "Richard A. Newcombe, Dieter Fox, Steven M. Seitz",
      "year": 2015,
      "role": "First-frame canonical anchoring and online fusion paradigm",
      "relationship_sentence": "OnlineSplatter generalizes DynamicFusion\u2019s canonical-first-frame anchoring and incremental integration to monocular RGB and 3D Gaussian primitives, avoiding depth sensors and explicit tracking/pose optimization."
    },
    {
      "title": "DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras",
      "authors": "Zachary Teed, Jia Deng",
      "year": 2021,
      "role": "Learned correspondence and online, constant-cost updates without classical bundle adjustment",
      "relationship_sentence": "The method\u2019s pose-free, feed-forward aggregation leverages the insight that learned, robust correspondences can drive online reconstruction; OnlineSplatter adopts a memory-based alternative to BA to keep cost constant over long sequences."
    }
  ],
  "synthesis_narrative": "OnlineSplatter\u2019s core innovation\u2014pose-free, online reconstruction of a moving object as a compact field of 3D Gaussians powered by a dual-key memory\u2014sits at the intersection of three lines of work. First, 3D Gaussian Splatting provides the computational and representational backbone: efficient Gaussian primitives with SH-based view dependence, together with densification and pruning strategies that OnlineSplatter repurposes for continuous online updates at constant cost. Second, ideas from neural rendering guide how appearance and viewpoint are handled without poses. NeRF established conditioning on viewing direction, while NeRF in the Wild introduced appearance embeddings to decouple lighting/appearance from geometry. OnlineSplatter translates these into a dual-key memory: latent appearance-geometry keys maintain a stable object state across frames, and explicit directional keys enable correct view-dependent readout during fusion. Handling free motion draws from dynamic/canonicalization works: Nerfies shows how to maintain an object-centric canonical space under deformation, and DynamicFusion pioneered anchoring to the first frame with incremental fusion\u2014both principles that OnlineSplatter adapts to monocular RGB and Gaussian primitives without optimization over deformations or poses. Finally, online correspondence/tracking insights from DROID-SLAM motivate eschewing bundle adjustment in favor of feed-forward, memory-based aggregation, aligning with the paper\u2019s spatially guided readout and sparsification mechanism. Collectively, these works directly enable OnlineSplatter\u2019s design: a memory-augmented, direction-aware Gaussian field that fuses per-frame features into a canonical, compact object reconstruction without camera poses or depth priors.",
  "analysis_timestamp": "2026-01-07T00:21:32.346456"
}
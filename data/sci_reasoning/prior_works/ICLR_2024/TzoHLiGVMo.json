{
  "prior_works": [
    {
      "title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
      "authors": "Steven L. Brunton et al.",
      "year": 2016,
      "arxiv_id": "1509.03580",
      "role": "Foundation",
      "relationship_sentence": "This work formalized the problem of recovering an ODE\u2019s right-hand side from observed trajectories (SINDy), which ODEFormer tackles while removing SINDy\u2019s reliance on hand-crafted libraries and numerical differentiation."
    },
    {
      "title": "Inferring biological networks by sparse identification of nonlinear dynamics",
      "authors": "N. M. Mangan et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Implicit-SINDy extended SINDy to rational forms via implicit sparse regression, a capability ODEFormer generalizes by learning symbolic forms without pre-specifying function libraries."
    },
    {
      "title": "AI Feynman: A Physics-Inspired Method for Symbolic Regression",
      "authors": "Silviu-Marian Udrescu et al.",
      "year": 2020,
      "arxiv_id": "1905.11481",
      "role": "Baseline",
      "relationship_sentence": "AI Feynman is a primary symbolic-regression baseline that requires supervised (x, y) pairs\u2014applied to ODEs via estimated derivatives\u2014whose noise sensitivity and derivative dependence ODEFormer explicitly addresses."
    },
    {
      "title": "Deep Learning for Symbolic Mathematics",
      "authors": "Guillaume Lample et al.",
      "year": 2020,
      "arxiv_id": "1912.01412",
      "role": "Inspiration",
      "relationship_sentence": "This paper demonstrated that sequence-to-sequence Transformers trained on synthetic corpora can manipulate and generate formal mathematical expressions, directly inspiring ODEFormer\u2019s autoregressive generation of symbolic ODEs."
    },
    {
      "title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression",
      "authors": "Shayan G. Valipour et al.",
      "year": 2021,
      "arxiv_id": "2106.14131",
      "role": "Inspiration",
      "relationship_sentence": "SymbolicGPT showed that conditioning Transformers on sampled input\u2013output data enables end-to-end symbolic regression, an idea ODEFormer adapts to condition on time-series trajectories to recover vector-field expressions."
    },
    {
      "title": "Discovering symbolic models from deep learning with inductive biases",
      "authors": "Miles Cranmer et al.",
      "year": 2020,
      "arxiv_id": "2006.11287",
      "role": "Foundation",
      "relationship_sentence": "This work popularized symbolic discovery for dynamical systems and curated the 2D \u2018Strogatz\u2019 ODE benchmark, which defines the evaluation setting ODEFormer targets and extends."
    },
    {
      "title": "PySR: Fast & Parallelized Symbolic Regression in Python/Julia",
      "authors": "Miles Cranmer",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "PySR is a state-of-the-art evolutionary symbolic regression baseline against which ODEFormer is directly compared, highlighting gains in robustness when derivatives are unavailable or noisy."
    }
  ],
  "synthesis_narrative": "Sparse Identification of Nonlinear Dynamics (SINDy) established the blueprint for discovering ODE right-hand sides from trajectories by selecting a few terms from a hand-crafted function library, and Implicit-SINDy broadened that formulation to rational dynamics via implicit sparse regression. AI Feynman advanced general-purpose symbolic regression with physics-inspired heuristics and noise-aware procedures, typically applied to ODEs by first estimating derivatives from data. In parallel, Deep Learning for Symbolic Mathematics showed that Transformers trained on synthetic corpora can learn to parse and generate formal mathematical expressions with strong generalization, while SymbolicGPT demonstrated conditioning generative Transformers on input\u2013output samples to recover concise closed-form expressions end-to-end. Complementing these methodological advances, work on discovering symbolic models with inductive biases curated the widely used two-dimensional \u2018Strogatz\u2019 ODE benchmark, crystallizing evaluation practices for symbolic modeling of dynamical systems. PySR then provided a fast evolutionary baseline for symbolic regression that is competitive on noiseless supervised settings but typically operates on (state, derivative) pairs derived from numeric differentiation. Together, these works highlight a gap: library- or derivative-dependent methods struggle with noise and irregular sampling, while Transformer-based symbolic regression had not targeted vector-field discovery from a single trajectory. ODEFormer naturally synthesizes these strands by conditioning a Transformer on raw trajectories to autoregressively generate symbolic ODE systems, thereby bypassing derivative estimation and hand-crafted libraries while aligning with established dynamical-system benchmarks.",
  "target_paper": {
    "title": "ODEFormer: Symbolic Regression of Dynamical Systems with Transformers",
    "authors": "St\u00e9phane d'Ascoli, S\u00f6ren Becker, Philippe Schwaller, Alexander Mathis, Niki Kilbertus",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "symbolic regression, dynamical systems, differential equations, transformer",
    "abstract": "We introduce ODEFormer, the first transformer able to infer multidimensional ordinary differential equation (ODE) systems in symbolic form from the observation of a single solution trajectory. We perform extensive evaluations on two datasets: (i) the existing \u2018Strogatz\u2019 dataset featuring two-dimensional systems; (ii) ODEBench, a collection of one- to four-dimensional systems that we carefully curated from the literature to provide a more holistic benchmark. ODEFormer consistently outperforms existing methods while displaying substantially improved robustness to noisy and irregularly sampled observations, as well as faster inference. We release our code, model and benchmark at https://github.com/sdascoli/odeformer.",
    "openreview_id": "TzoHLiGVMo",
    "forum_id": "TzoHLiGVMo"
  },
  "analysis_timestamp": "2026-01-06T17:53:45.133921"
}
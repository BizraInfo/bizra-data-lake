{
  "prior_works": [
    {
      "title": "Spectral Networks and Deep Locally Connected Networks on Graphs",
      "authors": "Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun",
      "year": 2013,
      "role": "Foundational spectral GNN formulation using Laplacian eigen-decomposition",
      "relationship_sentence": "Established the spectral paradigm (filters in the Laplacian eigenbasis) that underlies the class of spectrally-enhanced GNNs whose expressivity this paper analyzes and seeks to improve."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka",
      "year": 2019,
      "role": "Expressivity benchmark linking MPNNs to 1-WL",
      "relationship_sentence": "Provides the standard WL-based expressivity yardstick that the paper argues is misaligned with spectral information and against which the new multiplicity-based hierarchy is motivated."
    },
    {
      "title": "Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks",
      "authors": "Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, Martin Grohe",
      "year": 2019,
      "role": "k-WL hierarchy for stronger GNN expressivity",
      "relationship_sentence": "Introduces the k-WL hierarchy used widely to assess GNN expressivity; the present work critiques its suitability for spectral methods and proposes a spectrum-aligned alternative based on eigenvalue multiplicities."
    },
    {
      "title": "Universal Invariant and Equivariant Graph Neural Networks",
      "authors": "Mathieu Keriven, Gabriel Peyr\u00e9",
      "year": 2019,
      "role": "Expressivity via invariant/equivariant architectures and homomorphism counts",
      "relationship_sentence": "Connects GNN expressivity to homomorphism counting, a framework the paper cites as offering limited insight for spectral features, motivating their spectrum-aware expressivity hierarchy."
    },
    {
      "title": "Graph Neural Networks with Learnable Structural and Positional Representations",
      "authors": "Vijay Prakash Dwivedi, Xavier Bresson",
      "year": 2021,
      "role": "Popularized Laplacian eigenvector positional encodings (LapPE) in GNNs/Transformers",
      "relationship_sentence": "Directly motivates the paper\u2019s focus: many modern SGNNs inject Laplacian eigenvectors as positional encodings, and the paper proves such SGNNs can be incomplete even on simple-spectrum graphs."
    },
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco S. Cohen, Max Welling",
      "year": 2016,
      "role": "General framework for building neural networks equivariant to group actions",
      "relationship_sentence": "Provides the core equivariance design principle adapted here to the orthogonal transformations inherent in eigenspaces, enabling the proposed equiEPNN to respect spectral gauge symmetries."
    },
    {
      "title": "SE(3)-Transformers: 3D Roto-Translation Equivariant Attention",
      "authors": "Fabian B. Fuchs, Daniel E. Worrall, Volker Fischer, Max Welling",
      "year": 2020,
      "role": "Rotation-equivariant attention architecture in Euclidean space",
      "relationship_sentence": "Inspires the paper\u2019s adaptation of rotation-equivariant mechanisms to the spectral domain, yielding an SGNN (equiEPNN) that is equivariant to O(m) basis changes within eigenspaces and improves expressivity."
    }
  ],
  "synthesis_narrative": "The paper targets the expressive limits of spectrally-enhanced GNNs (SGNNs)\u2014models that exploit Laplacian eigenvectors and spectra\u2014now common through Laplacian positional encodings in MPNNs and Graph Transformers. This trend was catalyzed by the spectral GNN foundations of Bruna et al. and operationalized in practice by Dwivedi and Bresson, whose LapPE popularized injecting eigenvectors as positional signals. Yet, the dominant lenses for expressivity\u20141-WL and its higher-order variants (Xu et al.; Morris et al.) and the homomorphism-count viewpoint (Keriven & Peyr\u00e9)\u2014do not align with the algebraic structure of graph spectra. These frameworks, while central benchmarks, overlook the symmetries and degeneracies created by eigenvalue multiplicities. The present work fills this gap by proposing an expressivity hierarchy keyed to the multiplicity of the largest eigenvalue and proving that many SGNNs remain incomplete even on graphs with simple spectrum, revealing a fundamental shortfall of current spectral positional encodings. To address the root cause\u2014orthogonal gauge freedom within eigenspaces\u2014the authors adapt the group-equivariant design principle of Cohen & Welling and rotation-equivariant attention ideas from SE(3)-Transformers to the spectral setting. This yields equiEPNN, an SGNN that is equivariant to O(m) basis rotations in eigenspaces, thereby respecting the intrinsic symmetries of spectral features and overcoming documented completeness gaps. Together, these prior works shape the paper\u2019s critique, theoretical framework, and architectural remedy.",
  "analysis_timestamp": "2026-01-07T00:21:33.154614"
}
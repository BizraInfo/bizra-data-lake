{
  "prior_works": [
    {
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "authors": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu",
      "year": 2018,
      "role": "Foundational formalization of adversarial risk via a min\u2013max objective and threat models",
      "relationship_sentence": "The paper\u2019s definitions of adversarial risk and its comparisons between deterministic and probabilistic classifiers are grounded in the Madry et al. min\u2013max framework, which it uses to state and prove when randomization or determinism can reduce robust error."
    },
    {
      "title": "Certified Adversarial Robustness via Randomized Smoothing",
      "authors": "Jeremy M. Cohen, Elan Rosenfeld, J. Zico Kolter",
      "year": 2019,
      "role": "Canonical probabilistic defense based on randomization",
      "relationship_sentence": "As a leading example of probabilistic classifiers for robustness, randomized smoothing is a primary target of the authors\u2019 analysis, informing their explicit characterization of the deterministic hypothesis set that can match or outperform such probabilistic methods."
    },
    {
      "title": "Mitigating Adversarial Effects by Randomization",
      "authors": "Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan L. Yuille",
      "year": 2018,
      "role": "Empirical evidence and methodology for randomization-based defenses",
      "relationship_sentence": "This work motivated the central question addressed here by showing practical gains from randomized defenses, which the present paper formalizes by proving precise conditions under which randomized ensembles can outperform a base hypothesis set in adversarial risk."
    },
    {
      "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples",
      "authors": "Anish Athalye, Nicholas Carlini, David Wagner",
      "year": 2018,
      "role": "Critical analysis showing many randomized defenses fail under adaptive attacks",
      "relationship_sentence": "The critique of randomization as superficial robustness motivates the authors\u2019 rigorous results, including the theorem that for any probabilistic binary classifier there exists a deterministic classifier that strictly outperforms it in adversarial risk."
    },
    {
      "title": "Robustness May Be at Odds with Accuracy",
      "authors": "Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry",
      "year": 2019,
      "role": "Characterization of robust Bayes decision rules and trade-offs",
      "relationship_sentence": "The paper builds on robust Bayes decision rule insights to construct dominating deterministic classifiers and to reason about when aggregating predictions (via randomization) can change robust decision regions compared to the base hypothesis class."
    },
    {
      "title": "VC Classes are Adversarially Robustly Learnable, but Only Properly",
      "authors": "Omar Montasser, Steve Hanneke, Nathan Srebro",
      "year": 2019,
      "role": "Theoretical framework for adversarially robust learning and hypothesis classes",
      "relationship_sentence": "Their formal treatment of robust losses and hypothesis class structure underpins this paper\u2019s analysis of ensembles versus base classes, and the explicit identification of deterministic hypothesis sets capturing the performance of common probabilistic classifiers."
    },
    {
      "title": "Ensemble Adversarial Training: Attacks and Defenses",
      "authors": "Florian Tram\u00e8r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel",
      "year": 2018,
      "role": "Use of ensembles to improve robustness and analyze transferability",
      "relationship_sentence": "This work catalyzed the study of ensembles for robustness; the present paper extends the concept by giving theoretical conditions under which randomized ensembles can surpass the base hypothesis set in adversarial risk."
    }
  ],
  "synthesis_narrative": "This paper clarifies when randomization truly helps in adversarially robust classification and when it does not. It rests on Madry et al.\u2019s min\u2013max formalization of adversarial risk, adopting that framework to compare deterministic classifiers, ensembles, and fully probabilistic predictors. Empirical defenses that leverage randomness\u2014such as Xie et al.\u2019s randomization-based transformations\u2014and the certification line inaugurated by Cohen et al.\u2019s randomized smoothing motivated the authors\u2019 focus on probabilistic classifiers as a pathway to robustness. However, the skepticism raised by Athalye et al. about obfuscated gradients underscores the necessity of principled guarantees, which this work provides by proving that in binary classification any probabilistic classifier is strictly dominated in adversarial risk by some deterministic classifier.\nBuilding on robust Bayes decision-rule insights and accuracy\u2013robustness trade-offs from Tsipras et al., the authors identify explicit deterministic rules that match or exceed the performance of popular probabilistic defenses and provide a constructive description of the deterministic hypothesis set containing such rules. Their results also extend ensemble-based robustness insights (e.g., Tram\u00e8r et al.) by specifying conditions under which randomized ensembles can genuinely outperform a given base hypothesis class under adversarial risk\u2014resolving conflicting empirical observations. Finally, the learnability and hypothesis-class perspective of Montasser et al. informs the paper\u2019s structural arguments about when moving from a base class to (randomized) mixtures expands achievable robust decisions. Together, these works directly shape the paper\u2019s central contributions: precise conditions for randomization to help, and a general dominance result favoring deterministic classifiers in binary robust classification.",
  "analysis_timestamp": "2026-01-06T23:33:35.585013"
}
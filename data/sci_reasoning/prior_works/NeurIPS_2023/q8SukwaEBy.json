{
  "prior_works": [
    {
      "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning (DAgger)",
      "authors": "St\u00e9phane Ross, Geoffrey J. Gordon, J. Andrew Bagnell",
      "year": 2011,
      "role": "Interactive imitation learning framework collecting expert interventions/corrections on the learner\u2019s state distribution.",
      "relationship_sentence": "PVP builds on DAgger\u2019s principle of active human involvement\u2014using demonstrations and real-time interventions\u2014but replaces supervised action cloning with proxy value labels that are later propagated via TD learning."
    },
    {
      "title": "Deep Q-learning from Demonstrations (DQfD)",
      "authors": "Andrew Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, et al.",
      "year": 2018,
      "role": "Integrates demonstration transitions into off-policy TD learning to bootstrap and propagate value estimates.",
      "relationship_sentence": "PVP generalizes DQfD\u2019s idea of propagating information from demonstrations with TD by using human demos as high-value labels and human interventions as low-value labels in a reward-free setting."
    },
    {
      "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics (DDPGfD)",
      "authors": "Matej Ve\u010der\u00edk, Todd Hester, Jonathan Scholz, Fabio Ramos, Olivier Pietquin, Bilal Piot, Nadine Heess, Martin Riedmiller",
      "year": 2017,
      "role": "Uses demonstration data in actor-critic off-policy TD updates to accelerate learning.",
      "relationship_sentence": "PVP adopts the core mechanism of TD-based value bootstrapping from limited expert data, but reframes it as proxy value propagation from human-provided labels without requiring task rewards."
    },
    {
      "title": "Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces",
      "authors": "Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, Peter Stone",
      "year": 2018,
      "role": "Learns from human scalar feedback to shape a value function in absence of environment rewards.",
      "relationship_sentence": "PVP echoes Deep TAMER\u2019s reward-free, human-in-the-loop supervision but simplifies supervision to binary-like proxy value labels and leverages TD to propagate these labels across unlabeled experience."
    },
    {
      "title": "Interactive Learning from Policy-Dependent Human Feedback (COACH)",
      "authors": "Joseph MacGlashan, Mark K. Ho, Robert Loftin, Bei Peng, David Roberts, Matthew E. Taylor, Michael L. Littman, W. Bradley Knox",
      "year": 2017,
      "role": "Models corrective human feedback as advantage-like signals for policy learning.",
      "relationship_sentence": "PVP draws on COACH\u2019s insight that human corrective signals can directly shape policy optimization, but operationalizes them as explicit high/low proxy values that TD-propagate through the replay data."
    },
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei",
      "year": 2017,
      "role": "Uses human preferences to learn a reward model that guides policy optimization.",
      "relationship_sentence": "PVP targets the same alignment goal as preference-based RL but bypasses reward learning by directly supervising a proxy value function whose labels (from demos and interventions) are propagated via TD."
    },
    {
      "title": "Inverse Reinforcement Learning from Failure",
      "authors": "Vasileios (V.) Shiarlis, Jo\u00e3o Messias, Shimon Whiteson",
      "year": 2016,
      "role": "Incorporates negative examples (failures) to infer what behaviors should be avoided.",
      "relationship_sentence": "PVP similarly leverages negative human signal\u2014interventions\u2014as low proxy values to explicitly suppress unsafe actions while TD propagation spreads this constraint to nearby unlabeled states."
    }
  ],
  "synthesis_narrative": "Proxy Value Propagation (PVP) sits at the intersection of interactive imitation learning and human-feedback-driven reinforcement learning. From DAgger, it inherits the central operational insight that the expert should actively intervene on the learner\u2019s induced state distribution, yielding targeted supervision where it matters most. Unlike action cloning in DAgger, however, PVP encodes these interactions as proxy value labels\u2014high for demonstrated actions, low when the human intervenes.\n\nThe mechanism that makes those sparse labels broadly useful is temporal-difference propagation, an idea crystallized in DQfD and DDPGfD: demonstrations seeded into off-policy TD updates can bootstrap and spread value information beyond the labeled transitions. PVP extends this to a reward-free regime, using TD to propagate both positive (demo) and negative (intervention) labels across the agent\u2019s exploratory data.\n\nHuman feedback methods such as Deep TAMER and COACH demonstrated that direct, policy-dependent human signals can effectively shape value/policy without environment rewards. PVP adopts this reward-free supervision perspective but simplifies the target by assigning explicit proxy values, which TD then disseminates to unlabeled states, increasing sample efficiency and stability relative to training a separate reward model.\n\nFinally, the emphasis on using negative signals aligns with IRL from Failure, which exploits suboptimal data to learn what not to do. By treating interventions as low-value labels, PVP explicitly suppresses unsafe or undesired actions while propagating this constraint. Together, these strands yield a simple, general, and efficient pipeline: collect demos and interventions, assign proxy values, and rely on TD to propagate human intent throughout the state-action space.",
  "analysis_timestamp": "2026-01-06T23:42:49.088038"
}
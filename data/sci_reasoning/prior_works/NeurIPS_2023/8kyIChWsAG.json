{
  "prior_works": [
    {
      "title": "Strictly Proper Scoring Rules, Prediction, and Estimation",
      "authors": "Tilmann Gneiting, Adrian E. Raftery",
      "year": 2007,
      "role": "Foundational theory of proper losses",
      "relationship_sentence": "Established that proper scoring rules are uniquely minimized by the true conditional probabilities, providing the global-optimality intuition that this paper localizes to restricted hypothesis classes."
    },
    {
      "title": "Composite Binary Losses",
      "authors": "Mark D. Reid, Robert C. Williamson",
      "year": 2010,
      "role": "Structure of proper composite losses and link functions",
      "relationship_sentence": "Characterized proper composite losses and link functions, informing this paper\u2019s view of post-processing (via links) and how loss can change under transformations of predictions."
    },
    {
      "title": "Deterministic Calibration and Smooth Calibration",
      "authors": "Sham M. Kakade, Dean P. Foster",
      "year": 2008,
      "role": "Definition and framework for smooth calibration",
      "relationship_sentence": "Introduced smooth calibration via tests against Lipschitz function families\u2014the exact calibration notion this paper guarantees under a local optimality condition."
    },
    {
      "title": "The Sample Complexity of Smooth Calibration",
      "authors": "Jaros\u0142aw B\u0142asiok, Parikshit Gopalan, Lunjia Hu, Preetum Nakkiran",
      "year": 2023,
      "role": "Quantitative development of smooth calibration",
      "relationship_sentence": "Provided quantitative bounds and operational definitions for smooth calibration that this work leverages to translate local loss-optimality into formal smooth calibration guarantees."
    },
    {
      "title": "Multicalibration: Calibration for the (Computationally-Identifiable) Subgroups",
      "authors": "Guy N. Hebert-Johnson, Michael P. Kim, Omer Reingold, Aaron Roth",
      "year": 2018,
      "role": "Calibration against rich function classes",
      "relationship_sentence": "Showed that calibration can be defined against rich families of tests, conceptually aligning with this paper\u2019s calibration-against-Lipschitz-functions perspective induced by post-processing."
    },
    {
      "title": "Transforming Classifier Scores into Accurate Multiclass Probability Estimates",
      "authors": "Bianca Zadrozny, Charles Elkan",
      "year": 2002,
      "role": "Post-hoc recalibration via post-processing",
      "relationship_sentence": "Demonstrated that suitable post-processing (e.g., isotonic/Platt scaling) can improve calibration, directly motivating this paper\u2019s criterion: if no Lipschitz post-processing lowers proper loss, predictions are already smoothly calibrated."
    },
    {
      "title": "Mixability is Bayes Risk Curvature",
      "authors": "Tim van Erven, Mark D. Reid, Robert C. Williamson",
      "year": 2012,
      "role": "Local geometry of proper losses via Bayes risk curvature",
      "relationship_sentence": "Connected properties of proper losses to curvature of the Bayes risk, underpinning the paper\u2019s local optimality analysis that links small post-processing perturbations to potential loss improvements."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution is to replace the usual global-optimality intuition for proper losses with a local optimality condition\u2014insisting that predictions cannot be improved in proper loss by any Lipschitz post-processing\u2014and to show this implies smooth calibration. This builds directly on the foundational theory of proper scoring rules (Gneiting and Raftery), which guarantees that the true probabilities globally minimize any proper loss. Reid and Williamson\u2019s theory of proper composite losses and link functions clarifies how transformations of predictions interact with proper losses, aligning with the paper\u2019s focus on post-processing families. The target guarantee, smooth calibration, is taken from Kakade and Foster\u2019s formulation using Lipschitz test functions; the authors\u2019 own recent work on the sample complexity of smooth calibration provides quantitative control and a modern operational lens for this notion. The idea of calibrating against rich families of tests, central to multicalibration (Hebert-Johnson et al.), informs the paper\u2019s calibration-against-functions viewpoint and strengthens the connection between loss landscapes and calibration properties. Empirical and algorithmic traditions in post-hoc calibration (Zadrozny and Elkan) motivate the specific choice to consider post-processing as a route to improving proper loss: if such improvements are unavailable, the model must already be calibrated in a smooth sense. Finally, van Erven, Reid, and Williamson\u2019s link between proper losses and Bayes risk curvature supports the paper\u2019s local analysis, tying small perturbations from Lipschitz post-processing to potential loss reductions and thus to calibration guarantees.",
  "analysis_timestamp": "2026-01-06T23:42:49.073563"
}
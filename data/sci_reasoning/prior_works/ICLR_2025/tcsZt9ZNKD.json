{
  "prior_works": [
    {
      "title": "k-Sparse Autoencoders",
      "authors": "Alireza Makhzani et al.",
      "year": 2013,
      "arxiv_id": "1312.5663",
      "role": "Extension",
      "relationship_sentence": "The paper directly adopts and modifies the top-k sparsity projection from k-sparse autoencoders to control latent sparsity, which simplifies tuning and improves the reconstruction\u2013sparsity frontier relative to L1-penalized SAEs."
    },
    {
      "title": "Winner-Take-All Autoencoders",
      "authors": "Alireza Makhzani et al.",
      "year": 2014,
      "arxiv_id": "1409.2752",
      "role": "Inspiration",
      "relationship_sentence": "The winner-take-all/lifetime sparsity mechanisms motivated design choices for preventing dead latents under fixed-sparsity activations, informing the modifications that yield few dead latents at large scale."
    },
    {
      "title": "Toy Models of Superposition",
      "authors": "Nicholas Elhage et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "By formalizing superposition and showing that sparse coding can disentangle superposed features, it provided the core motivation to use sparse autoencoders to recover interpretable features from language model activations."
    },
    {
      "title": "Towards Monosemanticity: Decomposing Language Models with Sparse Autoencoders",
      "authors": "Ethan Bricken et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This work established L1-regularized SAEs as the main approach for extracting interpretable features from LMs and highlighted practical issues\u2014balancing sparsity vs. reconstruction and pervasive dead latents\u2014that the current paper explicitly addresses."
    },
    {
      "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
      "authors": "Tom Lieberum et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Scaling SAEs in production LMs exposed worsening dead-latent rates and hyperparameter fragility, motivating the need for methods (like k-sparsity) that maintain feature quality and stability at large scales."
    },
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan et al.",
      "year": 2020,
      "arxiv_id": "2001.08361",
      "role": "Related Problem",
      "relationship_sentence": "Their methodology for measuring and fitting clean scaling relationships informed the analysis framework for deriving scaling laws with respect to autoencoder size and sparsity."
    },
    {
      "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
      "authors": "David Bau et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Its concept-level alignment metrics inspired creating quantitative feature-quality evaluations (e.g., hypothesized feature recovery and explainability of activation patterns) tailored to SAEs on LM activations."
    }
  ],
  "synthesis_narrative": "k-sparse autoencoders introduced enforcing a fixed top\u2011k activation pattern to directly control sparsity, showing that projection-based sparsity can avoid the tuning instability of penalty-based methods while preserving reconstruction quality. Winner\u2011take\u2011all autoencoders extended this idea by emphasizing k\u2011winners and lifetime sparsity mechanisms that mitigate unit collapse, offering concrete practices for reducing dead units under strict sparsity constraints. Toy Models of Superposition framed why sparse codes matter in neural representations, arguing that features are superposed and that sparse dictionary recovery can disentangle them\u2014pointing directly to SAEs as a tool for interpretability. Towards Monosemanticity operationalized this in language models with L1\u2011regularized SAEs, demonstrating interpretable features but documenting hard tradeoffs between sparsity and reconstruction and the prevalence of dead latents. Scaling Monosemanticity pushed SAEs to modern models and layers, revealing that dead-latent rates and hyperparameter fragility worsen with scale, and motivating more robust sparsity control. Network Dissection provided a template for quantitative, concept-based evaluations of interpretability that could be adapted from vision to LM feature dictionaries. Finally, scaling laws for LMs offered a methodology for fitting and interpreting clean power-law trends.\nThese threads jointly suggested an approach: use projection-based k\u2011sparsity to directly set activity levels, incorporate design choices from winner\u2011take\u2011all to keep latents alive at scale, and evaluate with concept-aligned metrics while analyzing performance through scaling-law methodology. The resulting synthesis naturally targets the core gaps identified by L1\u2011SAE work\u2014unstable tuning and dead latents\u2014enabling large\u2011scale, well\u2011behaved SAEs and revealing clean scaling laws over autoencoder size and sparsity.",
  "target_paper": {
    "title": "Scaling and evaluating sparse autoencoders",
    "authors": "Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, Jeffrey Wu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "interpretability, sparse autoencoders, superposition, scaling laws",
    "abstract": "Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and t",
    "openreview_id": "tcsZt9ZNKD",
    "forum_id": "tcsZt9ZNKD"
  },
  "analysis_timestamp": "2026-01-06T12:57:30.014480"
}
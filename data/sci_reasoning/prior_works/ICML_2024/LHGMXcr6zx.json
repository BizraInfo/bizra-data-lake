{
  "prior_works": [
    {
      "title": "MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
      "authors": "Julian Schrittwieser et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "EfficientZero V2 retains MuZero\u2019s core idea of learning a latent dynamics model and using search to produce training targets, and builds its discrete-action component directly on this planning-with-learned-model framework."
    },
    {
      "title": "EfficientZero: Mastering Atari with Limited Data",
      "authors": "Weirui Ye et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "EfficientZero V2 is a direct extension of EfficientZero\u2014keeping its short-horizon value expansion and consistency regularization while redesigning the framework to operate reliably across discrete and continuous control and both visual and low-dimensional inputs."
    },
    {
      "title": "DreamerV3: Mastering Diverse Domains via World Models",
      "authors": "Danijar Hafner et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "DreamerV3 set the prevailing general-purpose world-model baseline across discrete and continuous domains; EfficientZero V2 explicitly targets and overcomes DreamerV3\u2019s inconsistency across tasks, using its Proprio and Vision Control benchmarks as the proving ground."
    },
    {
      "title": "Dream to Control: Learning Behaviors by Latent Imagination (Dreamer)",
      "authors": "Danijar Hafner et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "EfficientZero V2 extends Dreamer\u2019s latent imagination and actor-critic training paradigm to handle continuous actions within a MuZero/EfficientZero-style value-expansion-and-search framework, enabling a unified algorithm across action types."
    },
    {
      "title": "Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning",
      "authors": "Evan Feinberg et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "EfficientZero V2 relies on short model rollouts to construct multi-step bootstrapped targets, directly extending the MVE principle to search-guided latent rollouts under limited data."
    },
    {
      "title": "Model-Based Reinforcement Learning for Atari (SimPLe)",
      "authors": "\u0141ukasz Kaiser et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "SimPLe introduced the Atari 100k low-data evaluation regime that EfficientZero V2 adopts and aims to advance, anchoring the paper\u2019s core problem formulation of extreme sample efficiency from pixels."
    },
    {
      "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations (SPR)",
      "authors": "Michael Schwarzer et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "EfficientZero V2\u2019s use of representation/dynamics consistency to stabilize learning with scarce data is inspired by SPR\u2019s self-predictive latent objectives, which it integrates within a MuZero-style world model."
    }
  ],
  "synthesis_narrative": "EfficientZero V2\u2019s core innovation\u2014a single, sample-efficient framework that spans discrete and continuous control from both pixels and low-dimensional inputs\u2014emerges from unifying two historically strong but separate lines of work: MuZero-style planning and Dreamer-style latent imagination. MuZero provides the foundational blueprint of learning a latent dynamics model and using search-generated targets to train value and policy; EfficientZero subsequently adapted this recipe to the low-data regime via short value expansion and consistency regularization. EfficientZero V2 directly extends these mechanisms while generalizing beyond discrete Atari to continuous control. Dreamer introduced actor-critic learning over imagined trajectories, enabling strong continuous-control performance; DreamerV3 further positioned itself as a general-purpose world-model agent across diverse domains. EfficientZero V2 explicitly takes DreamerV3 as the generalist baseline whose limitations\u2014inconsistent superiority across tasks\u2014motivate a MuZero/EfficientZero style upgrade that reintroduces search-guided targets and value expansion into a broader, unified framework. This extension is grounded in the principle of Model-based Value Expansion, allowing short-horizon rollouts to produce low-variance multi-step targets under limited data. To stabilize representation learning in this regime, EfficientZero V2 leverages self-predictive/consistency objectives akin to SPR. The Atari 100k setting introduced by SimPLe anchors the paper\u2019s sample-efficiency problem formulation and remains a primary proving ground. Together, these works directly shape EfficientZero V2\u2019s design: search-based target generation from MuZero/EfficientZero, latent imagination from Dreamer, MVE-style bootstrapping, and SPR-inspired consistency\u2014combined into a single generalist, data-efficient agent.",
  "analysis_timestamp": "2026-01-06T23:09:26.454079"
}
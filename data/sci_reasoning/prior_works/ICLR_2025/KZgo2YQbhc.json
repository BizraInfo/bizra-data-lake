{
  "prior_works": [
    {
      "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
      "authors": "Ruiz et al.",
      "year": 2023,
      "arxiv_id": "2208.12242",
      "role": "Baseline",
      "relationship_sentence": "PaRa targets the same few-shot personalization setting but addresses DreamBooth\u2019s tendency to overfit and erode text editability by replacing full-model fine-tuning with explicit rank-controlled parameterization."
    },
    {
      "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",
      "authors": "Gal et al.",
      "year": 2022,
      "arxiv_id": "2208.01618",
      "role": "Foundation",
      "relationship_sentence": "Textual Inversion formalized the few-shot concept personalization problem and evaluation protocol that PaRa builds upon while seeking stronger identity/style fidelity without sacrificing editability."
    },
    {
      "title": "Custom Diffusion: Multi-Concept Customization of Text-to-Image Diffusion",
      "authors": "Kumari et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing that restricting training to specific modules (e.g., cross-attention K/V) preserves editability but can underfit concepts, Custom Diffusion motivates PaRa\u2019s more principled capacity control via explicit parameter-rank reduction across layers."
    },
    {
      "title": "Key-Locked Rank One Editing for Text-to-Image Personalization (Perfusion)",
      "authors": "Tewel et al.",
      "year": 2023,
      "arxiv_id": "2305.01644",
      "role": "Inspiration",
      "relationship_sentence": "Perfusion\u2019s use of rank-1 updates to tightly constrain concept drift directly inspires PaRa\u2019s core insight that low-rank constraints can delimit the generation space to balance fidelity and editability."
    },
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Hu et al.",
      "year": 2022,
      "arxiv_id": "2106.09685",
      "role": "Extension",
      "relationship_sentence": "LoRA introduced low-rank parameterization for efficient fine-tuning, which PaRa extends by reducing the intrinsic rank of diffusion model weights themselves rather than adding low-rank update matrices."
    },
    {
      "title": "SVDiff: Compact Parameter Space for Diffusion Fine-Tuning",
      "authors": "Chen et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "SVDiff showed that operating in an SVD-based low-capacity parameter space stabilizes diffusion fine-tuning, and PaRa advances this line by explicitly controlling/truncating parameter rank to bound denoising trajectory space."
    }
  ],
  "synthesis_narrative": "DreamBooth established that full-model fine-tuning of text-to-image diffusion models can capture a subject or style from a handful of images, but it also revealed that such capacity often causes overfitting and prompt drift despite prior-preservation losses. Textual Inversion reframed personalization as learning a compact token embedding, preserving editability but at the cost of weaker fidelity for rich styles or identities, highlighting the need for more capacity than a single embedding affords. Custom Diffusion demonstrated that limiting which modules are tuned\u2014particularly cross-attention K/V and token embeddings\u2014curbs drift and improves compositionality, yet its coarse module-level selection can underfit or overconstrain certain concepts. Perfusion crystallized the importance of low-rank structure by using rank-1 key-locking to tightly anchor concept behavior, offering a precise capacity bottleneck that maintains editability. LoRA introduced low-rank parameterization as an efficient fine-tuning mechanism broadly adopted in diffusion, making rank a practical knob for controlling adaptation strength. SVDiff further showed that constraining updates in an SVD-structured, compact parameter space stabilizes diffusion fine-tuning and reduces overfitting. Together, these works reveal a consistent opportunity: personalization needs a principled, fine-grained capacity control that is stronger than token-only editing and more nuanced than module selection or additive low-rank deltas. The natural next step is to directly regulate the intrinsic rank of diffusion weights during fine-tuning, shrinking the model\u2019s effective generation space\u2014and thus its denoising trajectories\u2014so personalization stays faithful while text editability is preserved. PaRa synthesizes these insights by enforcing explicit parameter rank reduction as the core mechanism to balance fidelity and flexibility.",
  "target_paper": {
    "title": "PaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction",
    "authors": "Shangyu Chen, Zizheng Pan, Jianfei Cai, Dinh Phung",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Text-to-Image diffusion model, Diffusion model fine-tuning",
    "abstract": "Personalizing a large-scale pretrained Text-to-Image (T2I) diffusion model is chal-\nlenging as it typically struggles to make an appropriate trade-off between its training\ndata distribution and the target distribution, i.e., learning a novel concept with only a\nfew target images to achieve personalization (aligning with the personalized target)\nwhile preserving text editability (aligning with diverse text prompts). In this paper,\nwe propose PaRa, an effective and efficient Parameter Rank Reduction approach\nfor T2I model personalization by explicitly controlling the rank of the diffusion\nmodel parameters to restrict its initial diverse generation space into a small and\nwell-balanced target space. Our design is motivated by the fact that taming a T2I\nmodel toward a novel concept such as a specific art style implies a small generation\nspace. To this end, by reducing the rank of model parameters during finetuning, we\ncan effectively constrain the space of the denoising sampling trajectorie",
    "openreview_id": "KZgo2YQbhc",
    "forum_id": "KZgo2YQbhc"
  },
  "analysis_timestamp": "2026-01-06T16:25:30.098374"
}
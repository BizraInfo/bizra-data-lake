{
  "prior_works": [
    {
      "title": "A Path Towards Autonomous Machine Intelligence",
      "authors": "Yann LeCun et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "3D-JEPA is a concrete instantiation of the JEPA principle proposed by LeCun, adopting masked prediction in latent space rather than pixel/point reconstruction as the core self-supervised objective."
    },
    {
      "title": "Self-Supervised Learning from Video with a Joint-Embedding Predictive Architecture",
      "authors": "Mido Assran et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "V-JEPA demonstrated that JEPA-style masked latent prediction scales to spatiotemporal sensory streams, directly informing LOCATE 3D\u2019s masked-latent pretraining on posed RGB-D sequences."
    },
    {
      "title": "Point-MAE: Masked Autoencoders for Point Cloud Self-Supervised Learning",
      "authors": "Pang et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "Point-MAE is the dominant masked modeling baseline for point clouds that LOCATE 3D improves upon by replacing reconstructive objectives with JEPA-style latent prediction and by leveraging 2D foundation features lifted into 3D."
    },
    {
      "title": "Point-BERT: Pre-Training 3D Point Cloud Transformers with Masked Point Modeling",
      "authors": "Yu et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Point-BERT\u2019s masked token reconstruction highlights the limitations of generative point-level pretext tasks for semantic reasoning; 3D-JEPA addresses this gap by predicting semantic latents instead of coordinates."
    },
    {
      "title": "ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language",
      "authors": "Chen et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "ScanRefer introduced the modern 3D referring expression grounding task and dataset\u2014exactly the problem formulation on which LOCATE 3D is evaluated and improved."
    },
    {
      "title": "ReferIt3D: Neural listeners for fine-grained 3D referential grounding in real-world scenes",
      "authors": "Achlioptas et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "ReferIt3D defined fine-grained 3D language grounding benchmarks and neural listeners, providing the core evaluation setting and motivating stronger 3D language-conditioned localization."
    },
    {
      "title": "LERF: Language Embedded Radiance Fields",
      "authors": "Kerr et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "LERF showed that lifting CLIP features into 3D enables language-driven localization; LOCATE 3D adopts this idea for posed RGB-D point clouds (rather than NeRFs) and integrates it with JEPA pretraining for robust real-world deployment."
    }
  ],
  "synthesis_narrative": "LOCATE 3D\u2019s core innovation\u20143D-JEPA\u2014sits squarely in the JEPA lineage. LeCun\u2019s JEPA framework established the principle of masked prediction in latent space as a superior pretext to reconstruction, a concept operationalized for sensory streams by V-JEPA, which validated JEPA\u2019s efficacy on spatiotemporal inputs. LOCATE 3D adapts this masked latent prediction to 3D point clouds derived from posed RGB-D, learning contextualized 3D features tailored for downstream grounding.\nIn 3D self-supervision, Point-MAE and Point-BERT defined the prevailing masked point modeling paradigm, but their reconstructive objectives often over-emphasize geometry at the expense of semantics. LOCATE 3D directly addresses this gap by predicting semantic latents, not raw points, while injecting 2D foundation features into the 3D representation to better capture object and relational cues essential for language grounding.\nThe problem setting itself traces to ScanRefer and ReferIt3D, which introduced 3D referring expression grounding benchmarks and established the target outputs (3D boxes/masks) and evaluation protocols. Finally, LERF demonstrated that lifting image-language representations into 3D can enable natural-language localization, but required NeRFs and lacked a sensor-stream pathway. LOCATE 3D extends this idea to real-world RGB-D streams with a JEPA-trained 3D encoder and a language-conditioned decoder that jointly predicts 3D masks and boxes, yielding state-of-the-art grounding with strong generalization.",
  "analysis_timestamp": "2026-01-06T23:07:19.599361"
}
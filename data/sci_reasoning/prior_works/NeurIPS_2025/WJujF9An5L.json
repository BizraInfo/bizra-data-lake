{
  "prior_works": [
    {
      "title": "Pangu-Weather: A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast",
      "authors": "Kaifeng Bi et al.",
      "year": 2023,
      "role": "Architecture and sub-daily autoregressive forecasting blueprint",
      "relationship_sentence": "FuXi-Ocean extends Pangu-Weather\u2019s stacked attention and 6-hour-step autoregressive training paradigm from atmosphere to the 3D ocean, adapting it to eddy-resolving 1/12\u00b0 grids and multi-variable ocean states."
    },
    {
      "title": "GraphCast: Learning skillful medium-range global weather forecasting",
      "authors": "R\u00e9mi Lam et al.",
      "year": 2023,
      "role": "Context-aware encoding and stable iterative rollout at sub-daily cadence",
      "relationship_sentence": "GraphCast\u2019s use of rich temporal context and iterative 6-hour rollouts informs FuXi-Ocean\u2019s context-aware feature extractor and training for stable sub-daily forecasts in a geophysical setting."
    },
    {
      "title": "FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators",
      "authors": "Rohit Pathak et al.",
      "year": 2022,
      "role": "Proof of feasibility for global, high-resolution data-driven geofluid forecasting",
      "relationship_sentence": "FuXi-Ocean builds on FourCastNet\u2019s demonstration that data-driven models can match numerical baselines while being computationally efficient, pushing to eddy-resolving ocean scales and six-hourly cadence with attention-based networks."
    },
    {
      "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
      "authors": "Zongyi Li et al.",
      "year": 2021,
      "role": "Scalable operator-learning for high-resolution PDE dynamics",
      "relationship_sentence": "While FuXi-Ocean employs attention rather than operators, FNO\u2019s paradigm of efficient, resolution-agnostic learning for complex PDE-governed fields underpins design choices for scalable high-resolution training and inference in geophysical modeling."
    },
    {
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "authors": "Noam Shazeer et al.",
      "year": 2017,
      "role": "Gating-based mixture principle",
      "relationship_sentence": "FuXi-Ocean\u2019s Mixture-of-Time module is conceptually rooted in MoE gating, replacing experts with predictors at different temporal horizons and learning adaptive gates to combine their outputs for reduced error accumulation."
    },
    {
      "title": "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting",
      "authors": "Bryan Lim and Sercan O. Arik",
      "year": 2021,
      "role": "Multi-horizon temporal fusion and gating",
      "relationship_sentence": "The MoT\u2019s adaptive integration of signals across temporal contexts echoes TFT\u2019s gated temporal fusion, which FuXi-Ocean repurposes for spatiotemporal ocean fields and sub-daily horizons."
    },
    {
      "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
      "authors": "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer",
      "year": 2015,
      "role": "Mitigating exposure bias and rollout error accumulation",
      "relationship_sentence": "FuXi-Ocean addresses sub-daily error accumulation in autoregressive rollouts in a spirit similar to scheduled sampling, complementing this with MoT\u2019s horizon-ensemble to enhance stability over long sequences."
    }
  ],
  "synthesis_narrative": "FuXi-Ocean\u2019s core advance\u2014stable six-hourly global ocean forecasts at eddy-resolving 1/12\u00b0 with depth\u2014stands on three converging lines of prior work. First, recent breakthroughs in data-driven weather models established that autoregressive, sub-daily forecasting at global scale is feasible and efficient. Pangu-Weather and GraphCast demonstrated that stacking attention blocks (or learned graph operators), conditioning on recent history, and iterating 6-hour steps can rival numerical models. FuXi-Ocean imports this recipe into the oceanic domain, where multivariate, 3D dynamics and eddy-resolving resolution amplify stability challenges. FourCastNet and the Fourier Neural Operator line further legitimized operator-learning and spectral efficiency for high-resolution geophysical dynamics, shaping FuXi-Ocean\u2019s emphasis on scalable training and inference.\nSecond, the Mixture-of-Time (MoT) module adapts mixture-of-experts principles to the temporal axis. Inspired by sparsely gated MoE, FuXi-Ocean treats predictors at different temporal horizons as the \u201cexperts,\u201d learning gates that adaptively combine their outputs based on context. This is complemented by ideas from Temporal Fusion Transformers, whose gated fusion of multiple temporal signals directly motivates MoT\u2019s aggregation of multi-horizon predictions.\nThird, the model tackles autoregressive error accumulation\u2014a key barrier for sub-daily forecasts\u2014by drawing on sequence learning strategies akin to scheduled sampling and stable iterative rollout practices from modern weather models. Collectively, these strands yield a context-aware, attention-based architecture whose MoT module fuses temporal scales to deliver robust, sub-daily, eddy-resolving global ocean forecasts.",
  "analysis_timestamp": "2026-01-07T00:21:32.283455"
}
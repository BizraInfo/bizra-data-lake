{
  "prior_works": [
    {
      "title": "Semantic reconstruction of continuous language from non-invasive brain recordings",
      "authors": "Jerry Tang, Alexander G. Huth, et al.",
      "year": 2023,
      "role": "LLM-based fMRI-to-text foundation",
      "relationship_sentence": "Established open-vocabulary decoding from fMRI by coupling brain-to-semantic mappings with an autoregressive language model, which the present work extends with a sequential, segment-wise decoding regime to combat long-context drift."
    },
    {
      "title": "Toward a universal decoder of linguistic meaning from brain activation",
      "authors": "Francisco Pereira et al.",
      "year": 2018,
      "role": "Semantic embedding alignment between fMRI and text",
      "relationship_sentence": "Provided the core paradigm of mapping fMRI signals into distributed semantic spaces, a step reused here at the segment level before language generation."
    },
    {
      "title": "Topographic mapping of a hierarchy of temporal receptive windows using a narrated story (Temporal receptive windows in human cortex)",
      "authors": "Yulia Lerner, Christopher J. Honey, Lauren J. Silbert, Uri Hasson",
      "year": 2011,
      "role": "Neuroscience evidence for hierarchical timescales",
      "relationship_sentence": "Demonstrated cortical processing at increasing temporal integration windows, motivating the paper\u2019s segmentation of long fMRI sequences into cognitively appropriate comprehension lengths."
    },
    {
      "title": "STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Wait-k Policy",
      "authors": "Mingbo Ma, Liang Huang, et al.",
      "year": 2019,
      "role": "Incremental decoding paradigm",
      "relationship_sentence": "Inspired the incremental, partial-input-to-output strategy; the paper adopts a similar decode-as-you-go approach for fMRI segments with controlled accumulation of context."
    },
    {
      "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
      "authors": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",
      "year": 2019,
      "role": "Long-context recurrence and memory",
      "relationship_sentence": "Introduced segment-level recurrence to preserve information across chunks; the proposed wrap-up mechanism analogously summarizes each fMRI segment and carries it forward as priors for subsequent decoding."
    },
    {
      "title": "Monotonic Chunkwise Attention",
      "authors": "Chung-Cheng Chiu, Colin Raffel",
      "year": 2018,
      "role": "Streaming/chunkwise sequence processing",
      "relationship_sentence": "Provided a practical template for chunkwise attention and online processing, informing the paper\u2019s design to process and integrate consecutive fMRI segments without full-sequence reprocessing."
    },
    {
      "title": "Punctuation and intonation effects on sentence wrap-up: Evidence from eye movements and ERPs",
      "authors": "Hirotani, Frazier, Rayner",
      "year": 2006,
      "role": "Psycholinguistic wrap-up inspiration",
      "relationship_sentence": "Documented wrap-up effects at clause/sentence boundaries in human reading, directly inspiring the paper\u2019s \u2018wrap-up\u2019 step that summarizes and consolidates semantics at segment boundaries."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014an incremental, segment-wise fMRI-to-text decoder with an explicit wrap-up memory\u2014sits at the intersection of three lines of prior work. First, LLM-driven brain decoding (Tang et al., 2023) and earlier fMRI-to-semantics paradigms (Pereira et al., 2018) showed that mapping brain activity into distributed semantic spaces and leveraging autoregressive LMs can enable open-vocabulary text reconstruction. However, these systems typically operate over long sequences as monolithic inputs, which invites memory saturation and semantic drift. Second, neuroscience and psycholinguistics provide the brain-inspired blueprint: the cortex integrates information over hierarchical temporal windows (Lerner et al., 2011), and human readers exhibit wrap-up effects at clause/sentence boundaries (Hirotani et al., 2006), suggesting natural segmentation and consolidation points. Third, incremental NLP architectures demonstrate how to operationalize these ideas computationally. Simultaneous translation with wait-k (Ma et al., 2019) shows that high-quality outputs can be produced from partial, growing inputs, while Transformer-XL (Dai et al., 2019) introduces segment recurrence to carry forward compressed history. Monotonic chunkwise attention (Chiu & Raffel, 2018) offers a streaming mechanism for chunked processing.\nBy unifying these strands, the authors segment long fMRI time series into cognitively plausible chunks, decode each segment incrementally with an LLM, and perform a wrap-up summarization that becomes the prior for the next step\u2014an explicit, brain-inspired memory that counters long-context degradation while preserving open-vocabulary generation.",
  "analysis_timestamp": "2026-01-07T00:29:42.059535"
}
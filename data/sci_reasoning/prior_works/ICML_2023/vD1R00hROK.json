{
  "prior_works": [
    {
      "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
      "authors": "McMahan et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Defines the federated learning setting with multi-step local updates (FedAvg) that induce client drift under non-iid data\u2014the core optimization inconsistency FedSMOO seeks to correct."
    },
    {
      "title": "Federated Optimization in Heterogeneous Networks",
      "authors": "Li et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Introduces a proximal regularizer (FedProx) to limit local drift toward client-specific optima; FedSMOO replaces this static proximal term with a dynamic, globally informed and SAM-revised regularizer."
    },
    {
      "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning",
      "authors": "Karimireddy et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Mitigates client drift via control variates to better match local and global objectives; FedSMOO targets the same inconsistency but through a regularization route that is further enhanced by sharpness-aware signals."
    },
    {
      "title": "Federated Learning with Dynamic Regularization",
      "authors": "Acar et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "Proposes a dynamic regularizer to counter objective inconsistency across rounds; FedSMOO directly extends this idea by revising the dynamic term using global sharpness (SAM) to jointly enforce global consistency and flatness."
    },
    {
      "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization",
      "authors": "Foret et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Introduces SAM to seek flat minima for better generalization; FedSMOO integrates a global SAM signal to shape the dynamic regularizer and smooth the global landscape in FL."
    },
    {
      "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
      "authors": "Keskar et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Establishes the link between sharp minima and poor generalization, motivating FedSMOO\u2019s explicit pursuit of a smooth (flat) global loss landscape via SAM-guided regularization."
    }
  ],
  "synthesis_narrative": "FedSMOO\u2019s core innovation\u2014combining a dynamic regularizer that aligns local and global objectives with a global sharpness-aware mechanism\u2014rests on two intellectual pillars: client-drift correction in federated optimization and flat-minima\u2013driven generalization. FedAvg formalized the FL setting with multiple local steps that under non-iid data cause clients to overfit to divergent optima, laying the optimization problem FedSMOO addresses. FedProx and SCAFFOLD became the main practical baselines for reducing this drift, respectively via a static proximal penalty and control variates; their degradation under severe heterogeneity highlights the gap FedSMOO targets. Crucially, FedDyn introduced a dynamic regularizer that adapts across rounds to better reconcile local and global objectives\u2014this mechanism directly inspires FedSMOO\u2019s regularization scaffold. FedSMOO extends it by revising the regularizer through a global sharpness signal so that local updates move toward both the global objective and flatter regions. The sharpness component traces to SAM, which operationalizes the pursuit of flat minima, itself motivated by foundational evidence that sharp minima generalize poorly (Keskar et al.). By fusing FedDyn-style dynamic alignment with SAM\u2019s flatness criterion at the global level, FedSMOO simultaneously improves optimization consistency and generalization under high heterogeneity\u2014precisely where prior drift-focused methods falter.",
  "analysis_timestamp": "2026-01-06T23:09:26.562501"
}
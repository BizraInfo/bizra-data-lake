{
  "prior_works": [
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "Provides the parameter-efficient low\u2011rank update mechanism that this paper reparameterizes on a hyperbolic manifold, serving as the primary baseline the proposed hyperbolic fine\u2011tuning improves upon."
    },
    {
      "title": "Poincar\u00e9 Embeddings for Learning Hierarchical Representations",
      "authors": "Maximilian Nickel et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Established hyperbolic geometry as a natural representation space for hierarchical and power\u2011law structured data, directly motivating the paper\u2019s choice to place token embeddings in hyperbolic space."
    },
    {
      "title": "Hyperbolic Neural Networks",
      "authors": "Octavian Ganea et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Introduced exp/log maps and gyrovector operations for learning on hyperbolic manifolds but presupposed hyperbolic parameters throughout; the present work explicitly addresses the unmet need of fine\u2011tuning when pretrained LLM weights remain Euclidean and na\u00efve exp/log mappings fail."
    },
    {
      "title": "Poincar\u00e9 GloVe: Hyperbolic Word Embeddings",
      "authors": "Adrian Tifrea et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Showed that word embeddings in hyperbolic space capture lexical hierarchies with radial positions correlating with frequency, directly inspiring the paper\u2019s empirical finding of token hyperbolicity and frequency\u2013radius structure in LLM embeddings."
    },
    {
      "title": "Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry",
      "authors": "Maximilian Nickel et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Provided a numerically stable Lorentz-model parameterization and mappings that the paper extends conceptually to design manifold-aware updates compatible with Euclidean weight matrices during hyperbolic fine\u2011tuning."
    },
    {
      "title": "Hyperbolic Graph Convolutional Neural Networks",
      "authors": "Ines Chami et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated practical recipes for projecting between Euclidean inputs and hyperbolic representations in deep networks, informing this work\u2019s boundary handling between Euclidean LLM parameters and hyperbolic adaptation."
    },
    {
      "title": "Riemannian Adaptive Optimization Methods",
      "authors": "Micka\u00ebl B\u00e9cigneul et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Supplied Riemannian Adam-style optimizers for manifold-valued parameters, which underpin the paper\u2019s stable optimization of hyperbolic fine\u2011tuning updates."
    }
  ],
  "synthesis_narrative": "This work\u2019s core idea\u2014parameter\u2011efficiently fine\u2011tuning LLMs in hyperbolic space while base embeddings and weights remain Euclidean\u2014emerges from two converging lines of prior research. First, hyperbolic representation learning established both the motivation and the mathematical tools. Poincar\u00e9 Embeddings (Nickel & Kiela, 2017) showed that hyperbolic geometry naturally models hierarchical, power\u2011law phenomena, directly motivating the authors\u2019 analysis of token frequency and latent tree structure. Poincar\u00e9 GloVe (Tifrea et al., 2019) brought that insight to words, linking radial position with frequency and hierarchy, which inspired the paper\u2019s empirical observations on LLM token embeddings. Methodologically, Hyperbolic Neural Networks (Ganea et al., 2018) and the Lorentz model (Nickel et al., 2018) provided exp/log maps, gyrovector operations, and numerically stable parameterizations; however, these methods largely assume hyperbolic parameters throughout, leaving a gap when interfacing with pretrained Euclidean LLMs. Hyperbolic GCNs (Chami et al., 2019) further informed practical projection between Euclidean features and hyperbolic layers, shaping the boundary design needed here. Second, parameter\u2011efficient fine\u2011tuning in LLMs provided the operational template: LoRA (Hu et al., 2022) is the baseline update mechanism the authors reparameterize on a hyperbolic manifold to retain efficiency while exploiting non\u2011Euclidean structure. Finally, Riemannian adaptive optimizers (B\u00e9cigneul & Ganea, 2019) enable stable training on curved spaces. Together, these works directly lead to the paper\u2019s key contribution: a manifold\u2011aware, low\u2011rank hyperbolic fine\u2011tuning scheme that resolves the incompatibility of na\u00efve exp/log mappings with Euclidean pretrained weights.",
  "analysis_timestamp": "2026-01-06T23:08:23.951647"
}
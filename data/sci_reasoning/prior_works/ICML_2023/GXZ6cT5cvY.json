{
  "prior_works": [
    {
      "title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent",
      "authors": "Martin Zinkevich",
      "year": 2003,
      "role": "Foundation",
      "relationship_sentence": "Zinkevich formalized online/convex Lipschitz optimization with subgradient methods whose optimal rates depend on the unknown diameter/distance D, directly setting the problem formulation D-Adaptation solves by removing the need to know D."
    },
    {
      "title": "Robust Stochastic Approximation Approach to Stochastic Programming",
      "authors": "Arkadi Nemirovski et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "Nemirovski et al. established optimal O(DG/\u221aT) rates for stochastic mirror/subgradient methods on convex Lipschitz problems (with step sizes depending on D), providing the theoretical benchmark that D-Adaptation matches without requiring D."
    },
    {
      "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
      "authors": "John Duchi et al.",
      "year": 2011,
      "role": "Related Problem",
      "relationship_sentence": "AdaGrad introduced adaptive stepsizes to reduce tuning but still relies on a base learning-rate choice and does not adapt to the unknown distance D, motivating D-Adaptation\u2019s global, parameter-free scaling that can wrap SGD/Adam while preserving optimal convex-Lipschitz rates."
    },
    {
      "title": "Coin Betting and Parameter-Free Online Learning",
      "authors": "Francesco Orabona et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "Coin-betting gives parameter-free algorithms but typically incurs extra logarithmic factors in regret/convergence, a clear limitation that D-Adaptation explicitly removes by achieving the optimal rate without multiplicative log terms in a single-loop method."
    },
    {
      "title": "Black-Box Reductions for Parameter-Free Online Learning in Banach Spaces",
      "authors": "Ashok Cutkosky et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Cutkosky and Orabona provide parameter-free reductions yet still suffer overheads (e.g., logarithmic factors or more complex constructions), directly motivating D-Adaptation\u2019s simpler single-loop scheme that attains optimal rates without such overhead."
    },
    {
      "title": "Adagrad Stepsizes: Sharp Convergence Over Nonconvex Landscapes",
      "authors": "Rachel Ward et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "AdaGrad-Norm shows the power of single-scalar normalization for robustness, and D-Adaptation builds on this insight by designing a global scalar that specifically tracks the unknown D to guarantee optimal convex-Lipschitz convergence without tuning."
    },
    {
      "title": "Probabilistic Line Searches for Stochastic Optimization",
      "authors": "Onur Mahsereci et al.",
      "year": 2015,
      "role": "Gap Identification",
      "relationship_sentence": "Probabilistic (and backtracking) line-search methods automate stepsizes but require additional function/gradient evaluations, a practical cost that D-Adaptation avoids by providing automatic, learning-rate-free updates with no extra evaluations."
    }
  ],
  "synthesis_narrative": "The core innovation of D-Adaptation targets the classical convex Lipschitz optimization setting where optimal subgradient convergence rates depend on the unknown distance D to the solution set. This setting was crystallized by Zinkevich\u2019s OGD and the robust stochastic approximation framework of Nemirovski et al., which together established both the O(DG/\u221aT) benchmark and the need to know D for rate-optimal tuning. While AdaGrad pioneered adaptive stepsizes, it still requires a base learning rate and does not resolve dependence on D, and subsequent scalar-normalization insights (AdaGrad-Norm) highlighted the promise of global, low-parameter scaling but without guaranteeing the D-optimal regime.\n\nParallel advances in parameter-free online learning\u2014most notably coin-betting (Orabona & P\u00e1l) and black-box reductions (Cutkosky & Orabona)\u2014demonstrated learning-rate-free guarantees yet typically incurred multiplicative logarithmic factors or algorithmic complexity. D-Adaptation is explicitly designed to close this gap: it retains a single-loop, practical update while eliminating extra log factors, thus asymptotically reaching the optimal rate for convex Lipschitz objectives without knowing D. Finally, unlike line-search approaches such as probabilistic line searches that automate step-size at the expense of additional evaluations, D-Adaptation achieves its guarantees with no extra function or gradient calls. Collectively, these works define the theoretical target, expose the shortcomings of existing parameter-free and adaptive methods, and provide the conceptual footholds (global scalar adaptation) that D-Adaptation unifies into a learning-rate-free, optimal, and practical optimizer.",
  "analysis_timestamp": "2026-01-06T23:09:26.586759"
}
{
  "prior_works": [
    {
      "title": "A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects",
      "authors": "Truccolo et al.",
      "year": 2005,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper established the GLM/point-process formulation in which coupling filters encode directed effective connectivity from population spike histories, the formal modeling setup NetFormer generalizes to state- and time-dependent interactions via attention."
    },
    {
      "title": "Spatio-temporal correlations and visual signalling in a complete neuronal population",
      "authors": "Pillow et al.",
      "year": 2008,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Their coupled spike-train GLM is a canonical baseline for inferring directed but stationary synaptic influences, whose static-coupling limitation NetFormer directly addresses by making weights a function of instantaneous neural state."
    },
    {
      "title": "Discovering Latent Network Structure in Point Process Data",
      "authors": "Linderman and Adams",
      "year": 2014,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Network Hawkes models infer sparse directed interaction matrices from event data under stationary kernels, and NetFormer replaces this assumption with a state-conditioned attention matrix to capture nonstationary, nonlinear interactions."
    },
    {
      "title": "Neural Relational Inference for Interacting Systems",
      "authors": "Kipf et al.",
      "year": 2018,
      "arxiv_id": "1802.04687",
      "role": "Inspiration",
      "relationship_sentence": "NRI introduced learning interaction graphs from trajectories via learned pairwise messages, an idea NetFormer instantiates with query\u2013key attention to yield an interpretable, time-varying connectivity matrix."
    },
    {
      "title": "Neural Granger Causality for Nonlinear Time Series",
      "authors": "Tank et al.",
      "year": 2018,
      "arxiv_id": "1802.05842",
      "role": "Extension",
      "relationship_sentence": "By showing neural networks can recover directed dependencies via lag-structured inputs and sparsity, this work is extended in NetFormer by tying directional influence to query\u2013key similarities, enabling nonlinear, state-dependent Granger effects."
    },
    {
      "title": "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting",
      "authors": "Lim et al.",
      "year": 2021,
      "arxiv_id": "1912.09363",
      "role": "Inspiration",
      "relationship_sentence": "TFT demonstrated how attention mechanisms can be structured for interpretability over variables and time, a design principle NetFormer leverages so attention weights correspond directly to dynamic neuron\u2013neuron connectivity."
    },
    {
      "title": "Recurrent Switching Linear Dynamical Systems",
      "authors": "Linderman et al.",
      "year": 2017,
      "arxiv_id": "1610.08466",
      "role": "Gap Identification",
      "relationship_sentence": "rSLDS captured neural nonstationarities via switching latent modes rather than changing pairwise couplings, highlighting a gap that NetFormer fills by letting directed connections vary continuously with state through attention."
    }
  ],
  "synthesis_narrative": "A point-process GLM view of neural population activity established that directed effective connectivity can be encoded by coupling filters on spike histories, providing a rigorous bridge between statistical models and synaptic influence (Truccolo et al.). This formulation became a workhorse baseline, exemplified by coupled GLMs applied to complete retinal populations, but these couplings were effectively stationary once fit (Pillow et al.). Parallel work with network Hawkes processes inferred sparse directed interaction matrices from events, again assuming stationary kernels and linear superposition (Linderman and Adams). To address nonstationarity, recurrent switching linear dynamical systems modeled neural dynamics with discrete mode switches, capturing changes in regime but not continuous, state-contingent changes in pairwise influence (Linderman et al.). In a different vein, Neural Relational Inference showed how interaction graphs could be learned from trajectories via learned pairwise messages, placing graph discovery at the heart of dynamical modeling (Kipf et al.). Neural Granger causality demonstrated that neural networks can recover directed dependencies from lagged inputs under sparsity constraints, but typically with time-invariant structures (Tank et al.). Finally, Temporal Fusion Transformers illustrated how attention can be architected for interpretability over variables and time (Lim et al.). Together, these works exposed a gap: interpretable, directed network inference that is both nonlinear and continuously state- (thus time-) dependent. NetFormer synthesizes the GLM/Hawkes interpretability of directed influence with NRI-style graph-from-dynamics and TFT\u2019s interpretable attention by treating each neuron\u2019s recent history as a token and using query\u2013key mappings to produce a state-conditioned attention matrix, yielding an analytically grounded, nonstationary effective connectivity model.",
  "target_paper": {
    "title": "NetFormer: An interpretable model for recovering dynamical connectivity in neuronal population dynamics",
    "authors": "Ziyu Lu, Wuwei Zhang, Trung Le, Hao Wang, Uygar S\u00fcmb\u00fcl, Eric Todd SheaBrown, Lu Mi",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "neuronal dynamics, dynamical connectivity, interpretability, attention mechanism, transformer",
    "abstract": "Neuronal dynamics are highly nonlinear and nonstationary. Traditional methods for extracting the underlying network structure from neuronal activity recordings mainly concentrate on modeling static connectivity, without accounting for key nonstationary aspects of biological neural systems, such as ongoing synaptic plasticity and neuronal modulation. To bridge this gap, we introduce the NetFormer model, an interpretable approach applicable to such systems. In NetFormer, the activity of each neuron across a series of historical time steps is defined as a token. These tokens are then linearly mapped through a query and key mechanism to generate a state- (and hence time-) dependent attention matrix that directly encodes nonstationary connectivity structures. We analyze our formulation from the perspective of nonstationary and nonlinear networked dynamical systems, and show both via an analytical expansion and targeted simulations how it can approximate the underlying  ground truth.  Next, ",
    "openreview_id": "bcTjW5kS4W",
    "forum_id": "bcTjW5kS4W"
  },
  "analysis_timestamp": "2026-01-06T13:21:46.674641"
}
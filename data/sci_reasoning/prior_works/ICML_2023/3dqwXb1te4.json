{
  "prior_works": [
    {
      "title": "Analysis of Boolean Functions",
      "authors": "Ryan O'Donnell",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s notion of degree and its Fourier/low-degree lens on Boolean functions is taken directly from O\u2019Donnell\u2019s framework, which underpins the definition of the min-degree interpolator used to analyze GOTU."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "The authors leverage the NTK/linearized regime to argue that overparameterized networks (including Transformer instances) trained by (S)GD converge to minimum-norm interpolants, a key step they connect to selecting low-degree solutions under GOTU."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "The kernel (lazy) versus mean-field distinction in this work directly motivates the paper\u2019s empirical/theoretical split between small-LR runs that yield min-degree solutions and mean-field or large-LR runs that produce \u2018leaky\u2019 min-degree behavior."
    },
    {
      "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks",
      "authors": "Suriya Gunasekar et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Building on this characterization of GD\u2019s spectral/path-norm bias in linear networks, the paper extends the idea to diagonal/feature models on Boolean inputs, showing SGD prefers low-complexity (here, low-degree) interpolants on the unseen."
    },
    {
      "title": "Surprises in High-Dimensional Ridgeless Least Squares Interpolation",
      "authors": "Trevor Hastie et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "The result that gradient descent converges to minimum-norm interpolants in overparameterized linear models underlies the paper\u2019s RF/NTK analysis, which it refines by identifying that the induced norm favours low-degree coefficients under GOTU."
    },
    {
      "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks (SCAN)",
      "authors": "Brenden M. Lake et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "SCAN crystallized failures of systematic/OOD generalization on unseen compositional combinations; the present paper formalizes GOTU to capture this setting and explains successes/failures via a degree-based inductive bias."
    },
    {
      "title": "Curriculum Learning",
      "authors": "Yoshua Bengio et al.",
      "year": 2009,
      "role": "Inspiration",
      "relationship_sentence": "The proposed \u2018degree curriculum\u2019 is a targeted curriculum design inspired by Bengio et al.\u2019s paradigm, using degree as the organizing principle to drive the learner toward low-degree solutions that extrapolate under GOTU."
    }
  ],
  "synthesis_narrative": "Abbe, Bengio, Lotfi, and Rizk ground their study in the Fourier-analytic view of Boolean functions from O\u2019Donnell, which supplies the precise notion of degree and the low-degree lens essential to defining a min-degree interpolator on the Boolean cube. To connect modern training dynamics to this combinatorial structure, they build on the NTK framework and ridgeless regression results (Jacot et al.; Hastie et al.), which show that overparameterized models trained with (S)GD converge to minimum-norm interpolants. The key step is to identify how the induced norms in RF/NTK-like regimes penalize higher-degree coefficients, yielding an implicit bias toward low-degree solutions\u2014precisely the min-degree interpolator they observe on unseen inputs in GOTU.\nChizat and Bach\u2019s lazy-versus-mean-field dichotomy directly shapes the paper\u2019s second core finding: small-learning-rate or kernel-like runs produce clean min-degree behavior, whereas mean-field or larger learning rates yield \u2018leaky\u2019 min-degree solutions. Gunasekar et al.\u2019s analysis of implicit bias in linear networks provides the methodological bridge, showing how gradient dynamics encode simplicity biases in spectral/feature coordinates, which this paper adapts to Boolean monomial degree.\nFinally, the work is motivated by systematic/OOD generalization gaps highlighted by SCAN, and it operationalizes a remedy through curriculum design. Inspired by Bengio et al.\u2019s Curriculum Learning, the authors propose a \u2018degree curriculum\u2019 that sequences training by algebraic degree, aligning optimization\u2019s implicit bias with the combinatorial structure needed for GOTU and explaining length generalization phenomena in Transformer-like models.",
  "analysis_timestamp": "2026-01-06T23:09:26.556901"
}
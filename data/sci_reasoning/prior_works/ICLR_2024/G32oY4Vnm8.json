{
  "prior_works": [
    {
      "title": "Prototypical Networks for Few-shot Learning",
      "authors": "Jake Snell et al.",
      "year": 2017,
      "arxiv_id": "1703.05175",
      "role": "Foundation",
      "relationship_sentence": "It formalized representing data by distances to learned class prototypes in an embedding space, a formulation PTaRL adopts to define its prototype-based projection space (P-Space) for tabular prediction."
    },
    {
      "title": "This Looks Like That: Deep Learning for Interpretable Image Recognition",
      "authors": "Chaofan Chen et al.",
      "year": 2019,
      "arxiv_id": "1806.10574",
      "role": "Inspiration",
      "relationship_sentence": "By projecting inputs onto learned prototypes and using prototype similarities as features for decisions, it inspired PTaRL\u2019s idea of expressing tabular samples in a prototype-similarity space to disentangle representations."
    },
    {
      "title": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments (SwAV)",
      "authors": "Mathilde Caron et al.",
      "year": 2020,
      "arxiv_id": "2006.09882",
      "role": "Inspiration",
      "relationship_sentence": "SwAV showed that learning prototypes jointly with the encoder and pulling embeddings toward prototype assignments yields globally structured spaces, informing PTaRL\u2019s use of prototypes as anchors to calibrate tabular representations."
    },
    {
      "title": "A Discriminative Feature Learning Approach for Deep Face Recognition",
      "authors": "Yandong Wen et al.",
      "year": 2016,
      "arxiv_id": "1604.01345",
      "role": "Extension",
      "relationship_sentence": "PTaRL extends center-based regularization by replacing single class centers with learned global prototypes and explicitly optimizing distances in P-Space to reduce intra-prototype variance and inter-prototype confusion."
    },
    {
      "title": "Revisiting Deep Learning Models for Tabular Data",
      "authors": "Yury Gorishniy et al.",
      "year": 2021,
      "arxiv_id": "2106.11959",
      "role": "Baseline",
      "relationship_sentence": "As a main deep tabular baseline (FT-Transformer/ResNet), it provides the embedding-space paradigm that PTaRL replaces with a prototype-based projection and highlights performance inconsistency PTaRL\u2019s space calibration targets."
    },
    {
      "title": "Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data",
      "authors": "Sergey Popov et al.",
      "year": 2019,
      "arxiv_id": "1909.06312",
      "role": "Baseline",
      "relationship_sentence": "As a strong deep tabular competitor, NODE underscores the lack of explicit global structure in standard embeddings that PTaRL addresses via prototype-anchored representations."
    },
    {
      "title": "Why do tree-based models still outperform deep learning on tabular data?",
      "authors": "L\u00e9o Grinsztajn et al.",
      "year": 2022,
      "arxiv_id": "2207.08815",
      "role": "Gap Identification",
      "relationship_sentence": "By documenting deep models\u2019 instability and inconsistent generalization on tabular data, it crystallizes the gap\u2014representation entanglement and localization\u2014that PTaRL\u2019s prototype-based space calibration explicitly targets."
    }
  ],
  "synthesis_narrative": "Prototypical Networks established a simple but powerful metric-learning view where each class is represented by a prototype and decisions are made via distances in an embedding space, directly suggesting that similarity to global anchors can structure features. ProtoPNet operationalized this idea for interpretability by learning prototypes and projecting inputs onto prototype similarities, demonstrating that prototype-similarity features can be both discriminative and disentangling. SwAV extended prototype usage to representation learning, showing that jointly learning prototypes with the encoder and aligning representations to prototype assignments yields globally organized, less entangled feature spaces. Center loss introduced center-based regularization that reduces intra-class variance by pulling embeddings toward class centers, highlighting how distance-based objectives can calibrate representation geometry. On tabular data, FT-Transformer/ResNet from \u201cRevisiting Deep Learning Models for Tabular Data\u201d became standard deep baselines but exhibited dataset-dependent performance and unstable representation quality, while NODE offered strong accuracy without explicit global geometric structure. Complementing these, Grinsztajn et al. systematically documented deep nets\u2019 instability and inconsistency on tabular tasks, framing the need for robust, globally structured representations. Together, these works suggested an opportunity: replace raw embedding spaces in tabular models with a prototype-anchored projection that encodes global structure explicitly. The current paper synthesizes prototype similarity projection (Prototypical Networks, ProtoPNet), prototype-guided space organization (SwAV), and center-style geometric regularization into a supervised, tabular-specific P-Space, addressing the instability and entanglement identified in leading tabular baselines while retaining strong predictive power.",
  "target_paper": {
    "title": "PTaRL: Prototype-based Tabular Representation Learning via Space Calibration",
    "authors": "Hangting Ye, Wei Fan, Xiaozhuang Song, Shun Zheng, He Zhao, Dan dan Guo, Yi Chang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Tabular data, Deep neural networks, Tabular representation learning, Prototype learning",
    "abstract": "Tabular data have been playing a mostly important role in diverse real-world fields, such as healthcare, engineering, finance, etc.\nWith the recent success of deep learning, many tabular machine learning (ML) methods based on deep networks (e.g., Transformer, ResNet) have achieved competitive performance on tabular benchmarks. However, existing deep tabular ML methods suffer from the representation entanglement and localization, which largely hinders their prediction performance and leads to  performance inconsistency on tabular tasks.\nTo overcome these problems, we explore a novel direction of applying prototype learning for tabular ML and propose a prototype-based tabular representation learning framework, PTaRL, for tabular prediction tasks. The core idea of PTaRL is to construct prototype-based projection space (P-Space) and learn the disentangled representation around global data prototypes. Specifically, PTaRL mainly involves two stages: (i) Prototype Generating, that constructs ",
    "openreview_id": "G32oY4Vnm8",
    "forum_id": "G32oY4Vnm8"
  },
  "analysis_timestamp": "2026-01-06T18:27:46.805685"
}
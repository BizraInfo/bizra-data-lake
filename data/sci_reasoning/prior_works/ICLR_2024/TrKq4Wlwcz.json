{
  "prior_works": [
    {
      "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
      "authors": "Lili Chen et al.",
      "year": 2021,
      "arxiv_id": "2106.01345",
      "role": "Extension",
      "relationship_sentence": "By conditioning autoregressive policies on return-to-go tokens, this work directly inspired representing engagement outcomes (e.g., likes, clicks) as explicit \"behavior tokens\" that condition and predict content\u2013behavior trajectories in a single sequence model."
    },
    {
      "title": "A Generalist Agent",
      "authors": "Scott Reed et al.",
      "year": 2022,
      "arxiv_id": "2205.06175",
      "role": "Inspiration",
      "relationship_sentence": "Demonstrating that one transformer can jointly model heterogeneous modalities and actions via tokenization, this paper motivated fusing content text and downstream receiver behaviors within one unified autoregressive token space."
    },
    {
      "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation",
      "authors": "Nitish Shirish Keskar et al.",
      "year": 2019,
      "arxiv_id": "1909.05858",
      "role": "Inspiration",
      "relationship_sentence": "Introducing discrete control codes to steer language generation, CTRL provided the template that is adapted here by using learned behavior tokens as control signals to target effectiveness (engagement) rather than static stylistic attributes."
    },
    {
      "title": "Self-Attentive Sequential Recommendation",
      "authors": "Wang-Cheng Kang et al.",
      "year": 2018,
      "arxiv_id": "1808.09781",
      "role": "Baseline",
      "relationship_sentence": "As a canonical transformer-based next-behavior predictor over user interaction sequences, SASRec serves as the primary baseline that the current model subsumes when behaviors are tokenized and modeled jointly with content."
    },
    {
      "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformers",
      "authors": "Fei Sun et al.",
      "year": 2019,
      "arxiv_id": "1904.06690",
      "role": "Foundation",
      "relationship_sentence": "By framing user interactions as token sequences and leveraging transformers for next-item prediction, BERT4Rec establishes the behavior-sequence modeling paradigm that is generalized here to open-domain content with multiple behavior types as tokens."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "arxiv_id": "2203.02155",
      "role": "Gap Identification",
      "relationship_sentence": "While RLHF optimizes LMs toward human preferences via pairwise feedback, it does not use measured downstream behaviors, motivating the direct incorporation of logged engagement as behavior tokens to predict and optimize real effectiveness."
    }
  ],
  "synthesis_narrative": "Decision Transformer showed that desired outcomes can be inserted as tokens into an autoregressive sequence, conditioning behavior generation on return-to-go and reframing control as sequence modeling. Gato extended this idea across modalities, proving that a single transformer can operate over text, perception, and actions by tokenizing everything into one sequence space. CTRL established that prepending discrete control codes can steer language generation toward target attributes without architectural changes. In recommender systems, SASRec introduced self-attention for user interaction histories, modeling next-behavior prediction as autoregression over behavior tokens, while BERT4Rec framed the same problem bidirectionally, cementing the notion that user behaviors can be treated as tokens in transformer objectives. In parallel, RLHF (InstructGPT) demonstrated that language models can be optimized for human preferences but largely through subjective comparisons rather than logged behavioral outcomes.\nBuilding on these threads, a natural opportunity emerged: unify content tokens and receiver behaviors as a single autoregressive space, using discrete behavior tokens akin to control codes but grounded in measured engagement, and condition generation on desired outcomes as in Decision Transformer, all within a generalist modeling setup exemplified by Gato. The result synthesizes sequence-conditioned control with behavior-sequence foundations from SASRec/BERT4Rec, while addressing RLHF\u2019s gap by directly learning from real engagement. This makes it possible to both predict receiver responses and optimize content toward specified behavioral targets within one scalable large language model.",
  "target_paper": {
    "title": "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior",
    "authors": "Ashmit Khandelwal, Aditya Agrawal, Aanisha Bhattacharyya, Yaman Kumar, Somesh Singh, Uttaran Bhattacharya, Ishita Dasgupta, Stefano Petrangeli, Rajiv Ratn Shah, Changyou Chen, Balaji Krishnamurthy",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "large language models, behavior simulation, large content and behavior models, behavior understanding, behavior in the wild, computational marketing, computational behavior science",
    "abstract": "Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' \"behavior tokens,\" such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained model",
    "openreview_id": "TrKq4Wlwcz",
    "forum_id": "TrKq4Wlwcz"
  },
  "analysis_timestamp": "2026-01-06T16:22:22.549843"
}
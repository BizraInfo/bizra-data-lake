{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Introduced the linearized training dynamics governed by an architecture-dependent kernel, directly enabling this paper\u2019s \"average geometry\" notion and its result that early geometry evolution is the data covariance projected onto an architecture-defined kernel."
    },
    {
      "title": "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
      "authors": "Jaehoon Lee et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Generalized NTK dynamics to deep architectures (including ResNets), which this work leverages to analyze architecture-dependent geometry evolution and to explain why MLPs vs. ResNets differ with respect to plane orientation."
    },
    {
      "title": "Spectral Bias and Task-Model Alignment in Kernel Regression",
      "authors": "Mehmet Canatar et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "Showed that learning dynamics and generalization are governed by alignment between data/task spectra and the kernel\u2019s eigenfunctions; this paper recasts that alignment as data-covariance projections onto an architecture-driven \"average geometry\" and uses it to predict where geometry can or cannot change."
    },
    {
      "title": "On the Inductive Bias of Neural Tangent Kernels",
      "authors": "Adrien Bietti et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Analyzed how architectural choices imprint anisotropic spectral/geometric biases in NTKs; this directly motivates the paper\u2019s architecture-dependent invariance directions and its orientation-dependent generalization phenomena."
    },
    {
      "title": "Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes",
      "authors": "Roman Novak et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Established architecture-specific function-space priors (NNGP/NTK) for CNNs and related architectures, grounding the paper\u2019s use of an architecture-dependent, initialization-time \"average geometry\" to summarize input\u2013output geometry."
    },
    {
      "title": "Universal Adversarial Perturbations",
      "authors": "Seyed-Mohsen Moosavi-Dezfooli et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Revealed shared, low-dimensional input directions that consistently alter network outputs, inspiring this paper\u2019s geometric invariance hypothesis and its claim that architecture determines subspaces where input-space curvature is invariant or evolves."
    },
    {
      "title": "Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks",
      "authors": "Andrew M. Saxe et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "Provided closed-form training dynamics showing evolution along data covariance modes, a precursor to this paper\u2019s result that early geometry changes follow data covariance projected onto an architecture-defined geometry."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014that a network\u2019s input-space geometry evolves only along architecture-determined directions and, at initialization, according to data covariance projected onto an architecture-defined average geometry\u2014emerges from the kernel-view of training dynamics. Jacot et al. introduced the Neural Tangent Kernel (NTK), establishing that early training follows linearized dynamics determined by an architecture-dependent kernel. Lee et al. extended this to deep networks, including ResNets, enabling a head-to-head architectural comparison central to the paper\u2019s orientation-dependent generalization findings. Canatar et al. showed that learning and generalization depend on alignment between data spectra and kernel eigenfunctions; the present work reframes this as data-covariance projections onto an average geometry, predicting where geometry can change (and where it remains invariant). Bietti and Mairal analyzed how architectural choices induce anisotropic spectral biases in NTKs, directly motivating the paper\u2019s architecture-specific invariance directions and explaining why MLPs (near rotationally invariant kernels) and ResNets (anisotropic kernels) behave differently. Novak et al. grounded these ideas by deriving architecture-specific Gaussian process/NTK priors, legitimizing an architecture-only summary of input\u2013output geometry at initialization. Beyond the kernel lens, Saxe et al. showed in deep linear nets that training progresses along data covariance modes, foreshadowing the paper\u2019s projection result. Finally, Moosavi-Dezfooli et al.\u2019s universal adversarial perturbations highlighted low-dimensional, shared input directions with outsized effect, inspiring the geometric invariance hypothesis that certain architecture-dependent directions preserve curvature while others drive its evolution.",
  "analysis_timestamp": "2026-01-06T23:08:23.926176"
}
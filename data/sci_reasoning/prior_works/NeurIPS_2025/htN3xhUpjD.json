{
  "prior_works": [
    {
      "title": "Root-N-consistent semiparametric regression",
      "authors": "Peter M. Robinson",
      "year": 1988,
      "role": "Foundational residualization identity underpinning ECC estimators",
      "relationship_sentence": "The paper\u2019s debiased ECC estimators rely on the Robinson residual-on-residual moment E[(Y\u2212E[Y|X])(A\u2212E[A|X])]\u2014the core representation that turns ECC estimation into a problem of learning two nuisance regressions and taking their residual covariance."
    },
    {
      "title": "The Hardness of Conditional Independence Testing and the Generalized Covariance Measure",
      "authors": "Rajen D. Shah, Jonas Peters",
      "year": 2018,
      "role": "Target functional and estimators for CI via residual covariance (ECC/GCM)",
      "relationship_sentence": "This work formalized testing via generalized residual covariance (closely aligned with ECC), providing concrete estimators that the present paper evaluates and then debiases and tunes under proportional asymptotics."
    },
    {
      "title": "Double/Debiased Machine Learning for Treatment and Structural Parameters",
      "authors": "Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins",
      "year": 2018,
      "role": "Orthogonal scores and cross-fitting for bias control with learned nuisance functions",
      "relationship_sentence": "The authors\u2019 orthogonal moment and cross-fitting framework motivates the paper\u2019s bias-correction strategy and sample-splitting designs to secure valid ECC estimation when nuisance regressions are high-dimensional and potentially poorly estimated."
    },
    {
      "title": "High-dimensional asymptotics of prediction: Ridge regression and classification",
      "authors": "Edgar Dobriban, Stefan Wager",
      "year": 2018,
      "role": "Proportional-asymptotic risk characterizations and optimal ridge tuning",
      "relationship_sentence": "Their random-matrix-based formulas for ridge risk when p/n\u2192c directly inform this paper\u2019s derivation of asymptotically optimal ridge regularization for nuisance regression under proportional asymptotics."
    },
    {
      "title": "Random Design Analysis of Ridge Regression",
      "authors": "Daniel J. Hsu, Sham M. Kakade, Tong Zhang",
      "year": 2014,
      "role": "Risk analysis for ridge under random design",
      "relationship_sentence": "This work provides nonasymptotic and asymptotic risk characterizations for ridge with random covariates, supplying technical tools and intuition the paper leverages to calibrate ridge tuning for nuisance estimation rather than prediction."
    },
    {
      "title": "Doubly robust estimation in missing data and causal inference",
      "authors": "Heejung Bang, James M. Robins",
      "year": 2005,
      "role": "Core doubly robust principle underlying bias correction",
      "relationship_sentence": "The paper\u2019s \u2018doubly robust functional\u2019 perspective and associated bias-correction draw on the Bang\u2013Robins idea that orthogonal scores can deliver valid estimation when at least one nuisance component is well specified, even if others are inconsistent."
    },
    {
      "title": "Locally Robust Semiparametric Estimation",
      "authors": "Victor Chernozhukov, Whitney K. Newey, Rahul Singh, Vasilis Syrgkanis",
      "year": 2022,
      "role": "Orthogonal/Riesz representer framework for linear functionals",
      "relationship_sentence": "This framework for constructing locally robust, Neyman-orthogonal scores for linear functionals guides the paper\u2019s derivation of debiased ECC estimators whose leading bias is insensitive to first-order nuisance estimation error."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014deriving asymptotically optimal ridge tuning for nuisance-function estimation and bias-corrected estimators of the Expected Conditional Covariance (ECC) under proportional asymptotics\u2014rests on three intertwined lines of prior work. First, Robinson\u2019s residual-on-residual identity provides the fundamental representation of ECC as the covariance of regression residuals, making the problem one of estimating two nuisance regressions and then combining them via an orthogonal moment. Second, recent conditional independence literature, especially Shah and Peters\u2019 Generalized Covariance Measure, elevated residual-covariance functionals (ECC/GCM) as practical targets for CI and causal problems; their concrete estimators are precisely the ones this paper evaluates and improves via debiasing. Third, the debiasing and sample-splitting blueprint comes from the doubly robust and orthogonal-score tradition: Bang and Robins introduced the DR principle, while Chernozhukov et al. developed double/debiased machine learning and cross-fitting, and later formalized locally robust estimation using Riesz representers. These tools justify bias correction that remains valid when nuisance estimates are high-dimensional and potentially inconsistent. Finally, the proportional-asymptotics and ridge-tuning aspects draw directly on random matrix analyses of ridge regression (Dobriban and Wager; Hsu, Kakade, and Zhang), which provide precise risk characterizations in the p/n\u2192c regime. The present work fuses these strands to show how to optimally tune ridge for nuisance learning tailored to ECC and to quantify the impact of sample splitting on the resulting debiased, doubly robust functional estimators.",
  "analysis_timestamp": "2026-01-07T00:02:04.920358"
}
{
  "prior_works": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "authors": "Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra",
      "year": 2022,
      "role": "Empirical foundation on modular arithmetic and delayed generalization",
      "relationship_sentence": "Introduced grokking on modular arithmetic tasks and linked early-stopping/regularization to a transition from memorization to algorithmic generalization, directly motivating this paper\u2019s focus on training dynamics and OOD generalization in modular linear functions."
    },
    {
      "title": "In-context Learning and Induction Heads",
      "authors": "Catherine Olsson, Nelson Elhage, Neel Nanda, Tom Henighan, Nicholas Joseph, et al.",
      "year": 2022,
      "role": "Mechanistic account of in-context learning in transformers",
      "relationship_sentence": "Identified induction-head circuits and their depth requirements, informing the paper\u2019s finding that at least two transformer blocks are needed for out-of-distribution generalization via in-context learning."
    },
    {
      "title": "Transformers Learn In-Context by Gradient Descent",
      "authors": "Ezgi Aky\u00fcrek, Jacob Andreas, et al.",
      "year": 2022,
      "role": "Theoretical framing of ICL as learned optimization",
      "relationship_sentence": "Showed that transformers can implement gradient-descent-like procedures in-context on linear tasks, directly inspiring the design of a family of linear modular functions and the expectation of cross-task generalization from pretraining over task families."
    },
    {
      "title": "Emergent Abilities of Large Language Models",
      "authors": "Jason Wei, Yi Tay, Rishi Bommasani, Denny Zhou, et al.",
      "year": 2022,
      "role": "Scaling/phase-transition perspective",
      "relationship_sentence": "Documented sharp, threshold-like emergence of new capabilities with scale, grounding this paper\u2019s investigation of a phase transition in OOD generalization as the number of pretraining tasks increases."
    },
    {
      "title": "Progress Measures for Grokking via Mechanistic Interpretability",
      "authors": "Neel Nanda et al.",
      "year": 2023,
      "role": "Mechanistic interpretability template for modular arithmetic circuits",
      "relationship_sentence": "Provided techniques and circuit-level insights for modular addition/multiplication grokking, which this paper extends to analyze the mechanisms underpinning in-context learning and skill composition in linear mod-p tasks."
    },
    {
      "title": "The Mathematics Dataset",
      "authors": "David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, et al.",
      "year": 2019,
      "role": "Benchmarking arithmetic reasoning in neural networks",
      "relationship_sentence": "Established arithmetic tasks (including modular-style operations) as a probe for algorithmic generalization, supporting this paper\u2019s choice of modular arithmetic as a controlled, interpretable testbed for ICL and composition."
    },
    {
      "title": "Transformers Learn to Learn In-Context",
      "authors": "Johannes von Oswald, Christian Henning, Jo\u00e3o Sacramento, et al.",
      "year": 2022,
      "role": "Meta-learning view of ICL across task families",
      "relationship_sentence": "Showed that transformers meta-learn across task distributions and perform task-specific adaptation in-context, directly aligning with this paper\u2019s pretrain-on-some-(a,b) and test-on-novel-(a,b) protocol and its analysis of skill composition."
    }
  ],
  "synthesis_narrative": "This work builds on three converging threads: grokking in algorithmic tasks, mechanistic accounts of in-context learning (ICL), and scaling-induced phase transitions. Power et al. introduced the grokking phenomenon on modular arithmetic, revealing delayed generalization and sensitivity to training dynamics\u2014core motifs this paper revisits while expanding to a structured family of linear mod-p functions and out-of-distribution (OOD) testing over unseen (a, b). Nanda et al. provided circuit-level progress measures for grokking in similar settings, seeding the interpretability methods the authors apply when probing the learned mechanisms for ICL and skill composition.\n\nOlsson et al.\u2019s induction heads work gave a concrete mechanism and depth requirements for ICL, anticipating the paper\u2019s empirical result that two transformer blocks are the minimal architecture for achieving OOD generalization. Complementarily, Aky\u00fcrek et al. and von Oswald et al. framed ICL as learned optimization/meta-learning over task families, directly motivating the paper\u2019s pretrain/test protocol across linear modular functions and its interpretation of cross-task transfer as in-context adaptation and composition of skills.\n\nFinally, Wei et al.\u2019s observations of emergent abilities with scale inform the paper\u2019s central finding: a phase transition from in-distribution to OOD generalization as the number of pretraining tasks increases, including the transient nature of OOD generalization in deeper models that necessitates early stopping. Saxton et al.\u2019s mathematics dataset legitimizes modular arithmetic as a controlled, interpretable benchmark, enabling precise measurement of ICL emergence and compositional structure in transformers.",
  "analysis_timestamp": "2026-01-06T23:33:35.520785"
}
{
  "prior_works": [
    {
      "title": "Spectral Experts: A Spectral Learning Approach to Mixture of Linear Regressions",
      "authors": "Arjun K. Chaganty; Percy Liang",
      "year": 2013,
      "role": "Foundational provable framework for learning mixtures of linear regressions via low-order moments/tensors.",
      "relationship_sentence": "Established identifiability and consistent estimation of multiple regression vectors behind latent subpopulations, providing the conceptual target the present paper pursues in a batched small-sample setting."
    },
    {
      "title": "Provable Tensor Methods for Learning Mixtures of Linear Regressions",
      "authors": "Hanie Sedghi; Mohammad Janzamin; Anima Anandkumar",
      "year": 2016,
      "role": "Extended mixture-of-linear-regressions learning beyond Gaussian designs using tensor/moment methods and score functions.",
      "relationship_sentence": "Demonstrated that removing isotropic Gaussian assumptions is possible with principled moment methods; the new paper similarly removes such assumptions but with a gradient-based estimator tailored to heterogeneous batches."
    },
    {
      "title": "Statistical Guarantees for the EM Algorithm: From Population to Sample-Based Analysis",
      "authors": "Sivaraman Balakrishnan; Martin J. Wainwright; Bin Yu",
      "year": 2017,
      "role": "Non-asymptotic analysis of EM for latent-variable models including mixtures (e.g., mixtures of regressions) under regularity such as Gaussian designs.",
      "relationship_sentence": "Provides a benchmark iterative approach that typically relies on isotropy/regular design conditions, underscoring the limitations the present work overcomes by handling general, heterogeneous covariate structures."
    },
    {
      "title": "Clustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization Under Privacy Constraints",
      "authors": "Felix Sattler; Klaus-Robert M\u00fcller; Wojciech Samek",
      "year": 2020,
      "role": "Introduced the federated/batched viewpoint with client clusters, motivating learning multiple models from many small local datasets.",
      "relationship_sentence": "Frames the practical regime of abundant small batches belonging to few latent groups; the current paper gives sharper, linear-regression-specific guarantees without restrictive design assumptions."
    },
    {
      "title": "IFCA: An Iterative Federated Clustering Algorithm for Personalized Federated Learning",
      "authors": "Avishek Ghosh; Jichan Hong; Dong Yin; Kannan Ramchandran",
      "year": 2020,
      "role": "Gradient-based clustered federated learning algorithm with theoretical analysis under (often) isotropic feature assumptions.",
      "relationship_sentence": "Directly relevant algorithmically\u2014learns cluster-specific linear models from small client batches\u2014but relies on spherical/isotropic designs that the present paper explicitly removes via a new gradient construction."
    },
    {
      "title": "Score Function Features for Discriminative Learning: Matrix and Tensor Framework",
      "authors": "Mohammad Janzamin; Hanie Sedghi; Anima Anandkumar",
      "year": 2014,
      "role": "Introduced score-function features to handle non-Gaussian inputs in latent-variable learning via moments.",
      "relationship_sentence": "Conceptually informs how to decouple unknown input distributions from parameter recovery; the new work achieves analogous decoupling for linear regression via gradient identities rather than explicit score features."
    }
  ],
  "synthesis_narrative": "The paper tackles a batched mixture-of-linear-regressions problem: many small batches, each generated by one of k latent linear models with heterogeneous (non-isotropic, subgroup-specific) covariate distributions. Early provable methods for mixtures of linear regressions\u2014Spectral Experts (Chaganty & Liang, 2013) and subsequent tensor approaches (Sedghi\u2013Janzamin\u2013Anandkumar, 2016)\u2014established identifiability and sample-efficient recovery, and importantly showed that moving beyond Gaussian designs is possible via moment-based techniques (often requiring access to score functions). Parallel iterative approaches, such as EM\u2019s statistical analysis (Balakrishnan\u2013Wainwright\u2013Yu, 2017), provided guarantees under Gaussian or well-conditioned designs, highlighting the fragility of likelihood-based iterations in heterogeneous settings.\n\nIn practical multi-source regimes, clustered/personalized federated learning (Sattler\u2013M\u00fcller\u2013Samek, 2020) and IFCA (Ghosh\u2013Hong\u2013Yin\u2013Ramchandran, 2020) crystallized the batched, small-sample paradigm: many clients with few examples per client, belonging to a handful of clusters, and gradient-based procedures to learn per-cluster models. However, their analyses typically assume isotropic or spherical covariates, which the present paper identifies as the key bottleneck.\n\nSynthesizing these lines, the core contribution here is a novel gradient-based estimator that inherits the practicality and scalability of federated-style clustering while achieving the distributional robustness envisioned by tensor/moment methods\u2014removing isotropic Gaussian assumptions and accommodating heterogeneous subgroup-specific designs. It leverages gradient identities that neutralize unknown covariate structure across batches, yielding improved sample\u2013batch complexity in the small-batch regime.",
  "analysis_timestamp": "2026-01-06T23:33:35.530783"
}
{
  "prior_works": [
    {
      "title": "Individual differences in reasoning: Implications for the rationality debate?",
      "authors": "Keith E. Stanovich; Richard F. West",
      "year": 2000,
      "role": "Cognitive foundation (dual\u2011process theory)",
      "relationship_sentence": "SwiftSage\u2019s separation into a fast, intuitive Swift module and a slow, deliberative Sage module is a direct computational instantiation of dual\u2011process theory articulated by Stanovich & West."
    },
    {
      "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "authors": "Richard S. Sutton; Doina Precup; Satinder P. Singh",
      "year": 1999,
      "role": "Hierarchical control and subgoal abstraction",
      "relationship_sentence": "The Sage\u2013Swift split mirrors the options framework\u2019s division of high-level (subgoal) decisions from low-level action execution, with Sage proposing subgoals and Swift handling reactive control."
    },
    {
      "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning (DAgger)",
      "authors": "St\u00e9phane Ross; Geoffrey J. Gordon; J. Andrew Bagnell",
      "year": 2011,
      "role": "Imitation learning for fast policies",
      "relationship_sentence": "SwiftSage\u2019s Swift module is a small encoder\u2013decoder policy trained via behavior cloning on oracle trajectories, directly drawing on imitation learning principles formalized by DAgger/BC."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Michael Ahn et al.",
      "year": 2022,
      "role": "LLM planning grounded by executable skills/values",
      "relationship_sentence": "SayCan\u2019s pipeline of LLM high-level planning grounded by affordances informs SwiftSage\u2019s Sage module for subgoal planning and its heuristic integration with a learned action executor."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2022,
      "role": "Reasoning\u2013acting interleaving in interactive tasks",
      "relationship_sentence": "ReAct showed gains from interleaving thoughts and actions; SwiftSage extends this by decoupling into two specialized systems and arbitrating when to rely on fast actions versus deliberate subgoal reasoning."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "role": "Deliberate reasoning via prompting",
      "relationship_sentence": "SwiftSage\u2019s Sage module leverages CoT-style deliberate decomposition to generate and refine subgoals before grounding them, embodying the paper\u2019s insight that explicit reasoning boosts performance."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "authors": "Noah Shinn et al.",
      "year": 2023,
      "role": "Self-reflection for improved interactive performance",
      "relationship_sentence": "Reflexion\u2019s meta-cognitive feedback motivates SwiftSage\u2019s deliberate, LLM-driven subgoal revision and grounding, complementing the fast policy with corrective deliberation."
    }
  ],
  "synthesis_narrative": "SwiftSage\u2019s core innovation\u2014an agent that marries fast, reactive control with slow, deliberate reasoning\u2014arises from converging lines of work in cognitive science, hierarchical control, imitation learning, and LLM-based planning. Dual-process theory (Stanovich & West) provides the conceptual blueprint: a System 1\u2013like module handles rapid action selection, while a System 2\u2013like module performs reflective planning. This split is operationalized with insights from hierarchical reinforcement learning, particularly the options framework (Sutton et al.), which separates subgoal decisions from low-level execution. The Swift module is trained via behavior cloning from oracle trajectories, following the imitation learning paradigm crystallized by DAgger/BC (Ross et al.), enabling robust, low-latency action proposals. On the deliberate side, Chain-of-Thought prompting (Wei et al.) supplies the mechanism for explicit subgoal reasoning, while ReAct (Yao et al.) demonstrates the benefits of interleaving reasoning and actions in interactive environments. SayCan (Ahn et al.) directly inspires SwiftSage\u2019s grounding of language plans in executable behaviors and informs its arbitration mechanism between high-level intent and low-level feasibility. Finally, Reflexion (Shinn et al.) highlights the value of meta-cognitive feedback for correcting and improving plan execution over time, complementing SwiftSage\u2019s Sage module as it revises and grounds subgoals. Together, these works shape SwiftSage\u2019s dual-module architecture and its heuristic integration strategy, yielding a practical, efficient agent that outperforms prior LLM-acting baselines on complex interactive tasks.",
  "analysis_timestamp": "2026-01-07T00:02:04.807246"
}
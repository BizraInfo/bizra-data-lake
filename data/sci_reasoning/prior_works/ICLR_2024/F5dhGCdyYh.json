{
  "prior_works": [
    {
      "title": "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents",
      "authors": "Yen-Chen Lin et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This seminal observation-space RL attack proposed strategically-timed and enchanting perturbations without any statistical detectability constraint, a limitation the current work explicitly formalizes and addresses."
    },
    {
      "title": "Robust Deep Reinforcement Learning with Adversarial Attacks",
      "authors": "Ameya Pattanaik et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Their white-box perturbation framework for degrading RL policies serves as a primary baseline that optimizes for effectiveness under Lp bounds but ignores information-theoretic detectability, which the current method constrains."
    },
    {
      "title": "Towards Evaluating the Robustness of Neural Networks",
      "authors": "Nicholas Carlini and David Wagner",
      "year": 2017,
      "arxiv_id": "1608.04644",
      "role": "Inspiration",
      "relationship_sentence": "By framing attacks as an explicit optimization with a Lagrangian trade-off between task loss and perturbation size, this work directly inspires the present paper\u2019s dual-ascent formulation that replaces norm-based imperceptibility with an information-theoretic detectability constraint."
    },
    {
      "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization",
      "authors": "Sebastian Nowozin et al.",
      "year": 2016,
      "arxiv_id": "1606.00709",
      "role": "Foundation",
      "relationship_sentence": "The variational estimation of f-divergences introduced here provides the practical machinery to measure and differentiate an information-theoretic detectability signal, enabling end-to-end learning of undetectable attacks."
    },
    {
      "title": "Sequential Tests of Statistical Hypotheses",
      "authors": "Abraham Wald",
      "year": 1945,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Wald\u2019s likelihood-ratio view of optimal sequential detection underpins the paper\u2019s notion of epsilon-bounded statistical detectability, linking divergence-based constraints to the power of any detector over trajectories."
    },
    {
      "title": "Calibrating Noise to Sensitivity in Private Data Analysis",
      "authors": "Cynthia Dwork et al.",
      "year": 2006,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The differential privacy notion of bounding log-likelihood ratios (epsilon-indistinguishability) directly motivates the paper\u2019s epsilon-bounded detectability criterion and its composition over sequential observations."
    }
  ],
  "synthesis_narrative": "Early attacks on deep reinforcement learning policies demonstrated that observation perturbations can reliably degrade agent performance; for example, strategically-timed and enchanting attacks showed how to manipulate trajectories but optimized only for effectiveness under simple Lp budgets. Subsequent white-box attacks provided stronger optimization-based procedures for perturbing observations, again centering on norm-bounded or visually imperceptible changes rather than statistical stealth. In parallel, adversarial example research cast attack generation as an explicit optimization with a Lagrangian balance between task loss and perturbation magnitude, establishing a template for constrained attack design. From information theory and sequential analysis, likelihood-ratio\u2013based optimal tests formalized the detectability of distributional shifts across time, tying error exponents to divergences. Differential privacy contributed the epsilon-indistinguishability lens\u2014bounding log-likelihood ratios and composing guarantees across steps\u2014clarifying how to restrict an adversary\u2019s detectability budget over sequences. Finally, variational f-divergence estimation offered practical, differentiable surrogates for information-theoretic quantities, making it feasible to train models to satisfy divergence constraints.\nTaken together, these works exposed a gap: effective RL attacks did not control statistical detectability, even though hypothesis testing and DP provide precisely the tools to quantify and limit it, and variational divergence estimators enable gradient-based enforcement. The present paper synthesizes these ideas by replacing norm-based imperceptibility with an epsilon-bounded detectability constraint grounded in likelihood ratios/divergences, and by optimizing attacks via a dual-ascent procedure directly over this information-theoretic budget. This unifies effectiveness and stealth in a principled, sequentially composable framework that prior RL attacks lacked.",
  "target_paper": {
    "title": "Illusory Attacks: Information-theoretic detectability matters in adversarial attacks",
    "authors": "Tim Franzmeyer, Stephen Marcus McAleer, Joao F. Henriques, Jakob Nicolaus Foerster, Philip Torr, Adel Bibi, Christian Schroeder de Witt",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "sequential decision making, adversarial attacks, robust human-AI systems, robust mixed-autonomy systems",
    "abstract": "Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. \nRobustifying agent policies requires anticipating the strongest attacks possible.\nWe demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of information-theoretic detectability constraints makes them \\textit{detectable} using automated means or human inspection. \nDetectability is undesirable to adversaries as it may trigger security escalations.\nWe introduce \\textit{\\eattacks{}}, a novel form of adversarial attack on sequential decision-makers that is both effective and of $\\epsilon-$bounded statistical detectability. \nWe propose a novel dual ascent algorithm to learn such attacks end-to-end.\nCompared to existing attacks, we empirically find \\eattacks{} to be significantly harder to detect with automated methods, and a small study with human participants\\footnote{IRB approval under reference R8",
    "openreview_id": "F5dhGCdyYh",
    "forum_id": "F5dhGCdyYh"
  },
  "analysis_timestamp": "2026-01-06T15:53:15.505587"
}
{
  "prior_works": [
    {
      "title": "LaVIT: Unified Language-Vision Pre-Training with Decoupled Visual Tokenization",
      "authors": "Jin et al.",
      "year": 2024,
      "role": "Baseline",
      "relationship_sentence": "Video-LaVIT directly extends LaVIT\u2019s LLM-centric, discrete visual tokenization framework from images to videos, keeping the unified generative pre-training recipe while adding a motion tokenizer and keyframe\u2013motion decomposition."
    },
    {
      "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
      "authors": "Kondratyuk et al.",
      "year": 2024,
      "role": "Gap Identification",
      "relationship_sentence": "VideoPoet showed that an LLM can autoregress over discrete video tokens for generation, but relied on a single monolithic video tokenizer that entangles appearance and motion\u2014precisely the limitation Video-LaVIT addresses by decoupling visual and motional token streams."
    },
    {
      "title": "MAGVIT: Masked Generative Video Transformer",
      "authors": "Huang et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "MAGVIT provided the practical discrete video tokenization and masked generative training paradigm that Video-LaVIT modifies\u2014replacing MAGVIT-style unified video codes with separate keyframe (visual) and motion tokenizers to improve efficiency and controllability."
    },
    {
      "title": "Zero-Shot Text-to-Image Generation",
      "authors": "Ramesh et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "DALL\u00b7E established the formulation of training an autoregressive language model over discrete visual code sequences; Video-LaVIT adopts this LM-over-codes paradigm for both images and videos within a unified LLM."
    },
    {
      "title": "DVC: An End-to-end Deep Video Compression Framework",
      "authors": "Lu et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "DVC\u2019s codec-style decomposition into I-frames and motion-compensated residuals directly inspires Video-LaVIT\u2019s core idea of representing videos as keyframes plus temporal motions, enabling separate tokenizers for appearance and motion."
    },
    {
      "title": "VideoGPT: Video Generation using VQ-VAE and Transformers",
      "authors": "Yaniv et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "VideoGPT generalized the VQ-token plus autoregressive modeling pipeline from images to videos, a foundational step that Video-LaVIT builds upon while redesigning the tokenization to decouple motion from appearance."
    }
  ],
  "synthesis_narrative": "Video-LaVIT\u2019s core innovation\u2014decoupled visual\u2013motional tokenization for unified video\u2013language pre-training\u2014emerges from a clear lineage of discrete token + language-modeling works and a codec-inspired decomposition of video. DALL\u00b7E crystallized the idea of training an autoregressive language model over discrete visual codes, and VideoGPT carried this paradigm into the video domain. MAGVIT then provided a scalable, VQ-based video tokenization and masked generative training recipe that became the de facto backbone for discrete video modeling. VideoPoet demonstrated that an LLM can operate directly over such video tokens for zero-shot video generation, but its single, monolithic tokenizer entangled appearance and motion, creating inefficiencies and limiting controllability and transfer across images and videos. In parallel, the LaVIT framework showed that an LLM can be a unified generative learner over text and discretized visual tokens, with decoupled visual tokenization for images. Video-LaVIT fuses these threads: it inherits LaVIT\u2019s unified LLM training over discrete tokens and extends it to video by explicitly decomposing a video into keyframes and temporal motions\u2014an idea inspired by deep video compression (DVC). By introducing separate tokenizers for appearance and motion and training the LLM to reason and generate over both streams, Video-LaVIT resolves the entanglement and scalability limitations of prior monolithic tokenizers, enabling efficient, unified comprehension and generation across images, videos, and text.",
  "analysis_timestamp": "2026-01-06T23:09:26.475349"
}
{
  "prior_works": [
    {
      "title": "In-context Learning and Induction Heads",
      "authors": "Catherine Olsson et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This work identified the induction head circuit as the concrete mechanism behind in-context learning, which the current paper directly extends by building a two-parameter mechanistic model of the induction head and tracking its abrupt emergence and competition with in-weights learning."
    },
    {
      "title": "Progress Measures for Grokking via Mechanistic Interpretability",
      "authors": "Neel Nanda et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The paper\u2019s methodology of defining circuit-level progress measures that rise before an abrupt phase transition directly inspired the current work\u2019s pre-ICL progress metrics that anticipate the emergence of the induction head."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "authors": "Alethea Power et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "By establishing the phenomenon of abrupt, late generalization, this work motivated analyzing the sharp, grokking-like onset of in-context classification and provided a target behavioral signature for the mechanistic account developed here."
    },
    {
      "title": "Transformers Learn In-Context by Gradient Descent",
      "authors": "Ekin Aky\u00fcrek et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By proposing an alternative account of ICL as implicit gradient descent, this work highlighted a gap in mechanistic, circuit-level explanations that the current paper addresses by showing data-dependent, induction-head-driven ICL and its competition with in-weights learning."
    },
    {
      "title": "Meta-Learning with Memory-Augmented Neural Networks",
      "authors": "Adam Santoro et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper introduced the in-sequence, few-shot classification formulation (learn-from-support, answer-query within a single episode) that underlies the simplified in-context classification task analyzed mechanistically here."
    },
    {
      "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Learning",
      "authors": "Shivam Garg et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "By demonstrating when small transformers succeed at in-context learning on synthetic tasks and how data/task structure affects meta-learning versus memorization, this work informed the simplified dataset design and the focus on distributional factors that the current paper mechanizes."
    }
  ],
  "synthesis_narrative": "Meta-learning with Memory-Augmented Neural Networks established the episodic, in-sequence few-shot classification setup where a model must infer a label mapping from a handful of examples and answer a query within the same sequence. In-context Learning and Induction Heads then revealed a concrete transformer circuit\u2014the induction head\u2014that implements the copying-and-matching behavior enabling such in-context adaptation, and documented its sudden emergence during training. Progress Measures for Grokking via Mechanistic Interpretability introduced a way to track the buildup of specific circuits before a sharp transition, providing tools to quantify pre-transition dynamics. Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets showcased abrupt, late generalization, anchoring the behavioral pattern of phase transitions that mechanistic analyses should explain. Transformers Learn In-Context by Gradient Descent put forward an alternative algorithmic account of ICL as implicit optimization, clarifying the need for a circuit-level, data-dependent explanation of when in-context strategies prevail over in-weights learning. Complementing this, What Can Transformers Learn In-Context? showed that success on synthetic in-context tasks is highly sensitive to task and training distribution. Taken together, these works foregrounded a tension between induction-head-based in-context mechanisms and in-weights memorization, suggested measurable precursors to abrupt capability onset, and emphasized data distribution as a key driver. The present work synthesizes these by recapitulating distributional effects in a minimal attention-only model, defining progress measures that predict the abrupt rise of an induction head, and proposing a two-parameter mechanistic model that explains the data-dependent competition between in-context and in-weights learning.",
  "target_paper": {
    "title": "The mechanistic basis of data dependence and abrupt learning in an in-context classification task",
    "authors": "Gautam Reddy",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "in-context learning, mechanistic interpretability, language models, induction heads",
    "abstract": "Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence, which contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction h",
    "openreview_id": "aN4Jf6Cx69",
    "forum_id": "aN4Jf6Cx69"
  },
  "analysis_timestamp": "2026-01-06T06:47:22.075386"
}
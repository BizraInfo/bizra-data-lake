{
  "prior_works": [
    {
      "title": "Neural Ordinary Differential Equations",
      "authors": "Ricky T. Q. Chen et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "The paper adopts the continuous-time view of deep networks introduced by Neural ODEs to recast weight-parameterized layers as trajectories of a dynamical system, which is the starting point for their dynamics-based neuromorphic architecture."
    },
    {
      "title": "A Proposal on Machine Learning via Dynamical Systems",
      "authors": "Weinan E",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This work established the interpretation of deep networks (e.g., ResNets) as discretizations of differential equations, directly motivating the conversion of weight-based structures into explicit dynamics that the current paper formalizes and extends."
    },
    {
      "title": "Hamiltonian Neural Networks",
      "authors": "Sam Greydanus et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "HNNs demonstrated embedding Hamilton\u2019s principle in neural models but remained focused on conservative physical dynamics; the present paper explicitly addresses this gap by generalizing the Hamiltonian formalism to representation learning and by replacing static weights with path-integral interactions."
    },
    {
      "title": "Deep Lagrangian Networks: Using Physics as Model Prior for Deep Learning",
      "authors": "Michael Lutter et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "The paper\u2019s entropy-reduction training derived from Euler\u2013Lagrange equations directly extends the Lagrangian framework of Deep Lagrangian Networks from system identification to generating feedback (stress forces) among sub-models for visual representation learning."
    },
    {
      "title": "Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation",
      "authors": "Benjamin Scellier et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Equilibrium Propagation\u2019s principle of learning by relaxing neural dynamics to an energy minimum inspires the paper\u2019s neuromorphic update rule, where feedback forces drive an entropy-reduction process instead of relying solely on backpropagation through static weights."
    },
    {
      "title": "Space-Time Approach to Quantum Mechanics",
      "authors": "Richard P. Feynman",
      "year": 1948,
      "role": "Foundation",
      "relationship_sentence": "The core mechanism\u2014measuring relations between sub-models by computing path integrals over their dynamical states\u2014directly uses Feynman\u2019s path-integral formalism as the mathematical backbone for replacing weights."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "authors": "Kaiming He et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "The authors convert pre-trained ResNet models into their dynamics-based form and fine-tune them via entropy reduction, demonstrating consistent improvements over this primary baseline on ImageNet and WebVision."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014recasting weight-based networks as neuromorphic dynamical systems governed by Hamilton\u2019s principle and Euler\u2013Lagrange equations\u2014stands on a tight lineage that unifies continuous-depth learning and physics-informed modeling. Neural ODEs and Weinan E\u2019s dynamical-systems view lay the foundation by showing how deep networks can be interpreted as trajectories of differential equations. Building on this, Hamiltonian Neural Networks reveal how Hamilton\u2019s principle can be embedded in learnable models, but their focus on conservative physical dynamics highlights a gap for high-level learning tasks. The present work addresses that gap by extending the Hamiltonian/Lagrangian paradigm to visual representation learning, introducing an explicit entropy-reduction process derived from Euler\u2013Lagrange equations that generates feedback \u2018stress forces\u2019 driving sub-model interactions. Deep Lagrangian Networks provide the methodological scaffolding for using Lagrangian mechanics as a learning prior; this paper extends that approach from system identification to neuromorphic training dynamics. Crucially, the replacement of static weights with computed relations via path integrals directly borrows its mathematical formalism from Feynman\u2019s path-integral framework. Finally, the training-as-relaxation idea resonates with Equilibrium Propagation, which informs how dynamic feedback can minimize an energy/entropy objective. Empirically, the approach is validated by transforming and improving standard baselines such as ResNets on large-scale vision datasets, demonstrating that the dynamics-inspired neuromorphic formulation is both principled and practically advantageous.",
  "analysis_timestamp": "2026-01-06T23:09:26.577039"
}
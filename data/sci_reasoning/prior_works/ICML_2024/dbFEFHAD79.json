{
  "prior_works": [
    {
      "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "This work established the LLM-as-a-judge paradigm using pairwise comparisons and multi-turn prompts, providing the core evaluation protocol that MLLM-as-a-Judge explicitly extends from text-only to multimodal settings."
    },
    {
      "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "G-Eval introduced rubric-guided scoring with LLM judges, directly informing the paper\u2019s Scoring Evaluation task design and prompting strategy for assessing alignment with human preferences."
    },
    {
      "title": "RankGPT: Instructing Large Language Models to Rank",
      "authors": "Sun et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "RankGPT formalized listwise/batch ranking with LLMs, which the paper generalizes to the multimodal regime via its Batch Ranking task to probe judge consistency beyond pairwise comparison."
    },
    {
      "title": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
      "authors": "Yushi Hu et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "TIFA demonstrated that VLMs can act as evaluators via question answering for image\u2013text faithfulness, motivating the paper\u2019s broader use of MLLMs as judges across modalities and tasks."
    },
    {
      "title": "Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation",
      "authors": "Guy Kirstain et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "By framing evaluation as human preference-aligned pairwise comparisons and yielding PickScore, this work inspired the paper\u2019s Pair Comparison setup and its focus on preference alignment in multimodal judging."
    },
    {
      "title": "MMMU: A Massive Multi-discipline Multimodal Understanding Benchmark for MLLMs",
      "authors": "Haotian Yin et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "MMMU typifies existing MLLM benchmarks that measure task accuracy rather than judge reliability or preference alignment, a gap the paper explicitly addresses with a judge-centric multimodal benchmark."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a multimodal benchmark for MLLMs-as-judges across scoring, pairwise, and batch ranking\u2014directly builds on the text-only LLM-as-a-judge paradigm and adapts it to vision-language settings. MT-Bench/Chatbot Arena (Zheng et al.) provided the foundational protocol of using LLMs as judges with pairwise comparisons, serving as the primary baseline that this work extends to multimodal data. For scalar assessment, G-Eval operationalized rubric-guided scoring with LLMs, which the authors generalize to visual contexts in their Scoring Evaluation task to examine alignment with human preferences. To probe listwise consistency beyond pairwise judgments, the paper draws on RankGPT\u2019s formulation of LLM-driven batch ranking, adapting it to multimodal inputs in their Batch Ranking task. Prior VLM-as-a-judge efforts in vision, such as TIFA, showed that VLMs can evaluate image\u2013text faithfulness via QA, and Pick-a-Pic framed evaluation as human preference-aligned pairwise comparison for text-to-image generation\u2014both informing the paper\u2019s multimodal judge framing and its emphasis on preference alignment. Finally, large multimodal benchmarks like MMMU highlight a key gap: they evaluate task performance, not the reliability and biases of MLLMs acting as judges. The present work addresses this gap by systematically benchmarking MLLMs\u2019 judging behavior, uncovering divergences from human preferences and persistent issues like bias, hallucination, and inconsistency across modalities.",
  "analysis_timestamp": "2026-01-06T23:09:26.421771"
}
{
  "prior_works": [
    {
      "title": "Partial Information Decomposition: A Unified Approach to the Multivariate Information Decomposition",
      "authors": "Paul L. Williams, Randall D. Beer",
      "year": 2010,
      "role": "Information-theoretic foundation for synergy",
      "relationship_sentence": "Provided the formal notion of synergistic information (distinct from redundancy and uniqueness), motivating InfMasking\u2019s objective to explicitly extract cross-modal interactions that no single modality contains alone."
    },
    {
      "title": "ModDrop: Adaptive Multi-Modal Learning with Dropout",
      "authors": "Natalia Neverova, Christian Wolf, Graham W. Taylor, Florian Nebout",
      "year": 2016,
      "role": "Modality-level stochastic occlusion inspiration",
      "relationship_sentence": "Introduced modality dropout to improve robustness to missing inputs; InfMasking generalizes this idea by stochastically masking most feature dimensions within each modality during fusion to elicit diverse synergistic interaction patterns rather than merely handling missing modalities."
    },
    {
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": "Aaron van den Oord, Yazhe Li, Oriol Vinyals",
      "year": 2018,
      "role": "Contrastive objective (InfoNCE) template",
      "relationship_sentence": "Established the InfoNCE contrastive framework that InfMasking uses to align masked and unmasked fused representations, turning agreement across complementary views into a learning signal."
    },
    {
      "title": "Contrastive Multiview Coding",
      "authors": "Yonglong Tian, Dilip Krishnan, Phillip Isola",
      "year": 2020,
      "role": "Multiview contrastive learning for complementary information",
      "relationship_sentence": "Showed how contrasting different views encourages capturing shared and complementary information; InfMasking creates masked multimodal \u2018views\u2019 at fusion time to emphasize cross-modal complementarity and synergy."
    },
    {
      "title": "ALBEF: Align Before Fuse \u2014 Vision and Language Representation Learning with Momentum Distillation",
      "authors": "Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, Steven C. H. Hoi",
      "year": 2021,
      "role": "Multimodal alignment-and-fusion paradigm",
      "relationship_sentence": "Demonstrated that aligning modalities before fusion and using a momentum teacher improves multimodal representations; InfMasking similarly leverages alignment but does so between masked and unmasked fused representations to target synergistic patterns."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "authors": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick",
      "year": 2022,
      "role": "High-ratio masking as an effective learning signal",
      "relationship_sentence": "Showed that aggressive random masking can yield strong representations; InfMasking adopts severe stochastic occlusion to produce many partial cross-modal views that surface synergy during fusion rather than reconstruction."
    },
    {
      "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
      "authors": "Alexei Baevski et al.",
      "year": 2022,
      "role": "Masked-to-unmasked latent alignment via teacher-student",
      "relationship_sentence": "Proposed aligning masked-input student representations to unmasked teacher targets; InfMasking adapts this principle to multimodal fusion by contrastively aligning masked fused representations to their unmasked counterparts to distill synergistic information."
    }
  ],
  "synthesis_narrative": "InfMasking targets the unique value of multimodal learning\u2014synergistic information\u2014by combining ideas from information theory, masking-based self-supervision, and multiview contrastive learning. The formal notion of synergy from Partial Information Decomposition (Williams & Beer) motivates designing objectives that isolate information present only in cross-modal interactions, not in any single modality. Building on stochastic occlusion in multimodal models (ModDrop), InfMasking moves beyond dropping entire modalities to aggressively masking most feature dimensions within each modality at fusion time, producing diverse partial views that require cross-modal cooperation to succeed.\nContrastive learning provides the optimization backbone. InfoNCE (CPC) and subsequent multiview contrastive work (CMC) show how agreement across views extracts shared and complementary information. InfMasking creates masked fused views and aligns them with the unmasked fused representation, ensuring the model preserves interaction-dependent content. Insights from masked modeling\u2014particularly MAE\u2019s effectiveness at high masking ratios\u2014justify the Infinite Masking strategy, which yields a practically unbounded set of synergistic interaction patterns. Finally, recent teacher-student latent alignment approaches such as data2vec inspire aligning masked to unmasked targets, which InfMasking repurposes in a multimodal fusion setting via contrastive rather than regression losses. Within the multimodal landscape, ALBEF\u2019s align-before-fuse paradigm informs the interplay between alignment and fusion, while InfMasking\u2019s key novelty is to align across masked/unmasked fused states to explicitly amplify synergy rather than mere cross-modal correspondence.",
  "analysis_timestamp": "2026-01-07T00:05:12.524060"
}
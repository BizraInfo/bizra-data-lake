{
  "prior_works": [
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "NExT-GPT generalizes BLIP-2\u2019s frozen-LLM + lightweight bridging paradigm\u2014reusing strong modality encoders while training only small projection layers\u2014to multiple modalities and to generation via paired decoders."
    },
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Flamingo established the core recipe of connecting frozen perception backbones to an LLM via learned adapters, which NExT-GPT extends from input-side vision to a unified, multi-modality adaptor design coupled with modality decoders."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "LLaVA\u2019s simple projection from visual features to the LLM token space and instruction tuning serves as NExT-GPT\u2019s baseline for perception, which NExT-GPT expands into an any-to-any framework with additional modality adapters and generative decoders."
    },
    {
      "title": "MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models",
      "authors": "Deyao Zhu et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "MiniGPT-4 demonstrates strong multimodal understanding but outputs only text; NExT-GPT explicitly targets this limitation by enabling LLM-controlled generation in image, video, and audio through diffusion decoders."
    },
    {
      "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
      "authors": "Chenfei Wu et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Visual ChatGPT showed that an LLM can orchestrate perception and generation models (e.g., Stable Diffusion) via tool calling; NExT-GPT transforms this idea into a unified end-to-end model with learned multimodal adapters and decoders rather than ad-hoc tool prompts."
    },
    {
      "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
      "authors": "Yongliang Shen et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "HuggingGPT motivated LLM-centric orchestration of expert models across modalities; NExT-GPT internalizes this orchestration by learning projection layers that directly condition modality-specific encoders/decoders for any-to-any I/O."
    }
  ],
  "synthesis_narrative": "NExT-GPT\u2019s core innovation\u2014an end-to-end, any-to-any multimodal LLM that both perceives and generates across text, image, video, and audio\u2014emerges from two converging lines of work. First, BLIP-2 and Flamingo established the foundational architectural principle of leveraging frozen, high-performing perception backbones with a frozen LLM, bridged by lightweight learned adapters. NExT-GPT adopts and generalizes this principle: it introduces modality-specific adapters for multiple inputs, and, crucially, maintains the parameter-efficiency ethos by training only small projection layers (~1%). LLaVA operationalized a particularly simple and effective visual-to-LLM projection plus visual instruction tuning pipeline; NExT-GPT uses this as the starting point for perception and extends it to a broader multi-modality setting.\nA second line of influence comes from LLM-orchestrated generation. Visual ChatGPT and HuggingGPT demonstrated that an LLM can coordinate perception and generation tools (e.g., diffusion models), but their reliance on external tool calling and prompt engineering limits unification and end-to-end learning. NExT-GPT converts that insight into a parametric system: the LLM, via learned adapters, directly conditions modality-specific diffusion decoders, achieving coherent any-to-any generation. Finally, works like MiniGPT-4 crystallized the field\u2019s limitation to input-side understanding with text-only output; NExT-GPT explicitly targets and resolves this gap by enabling controlled output synthesis in multiple modalities while keeping training cost low through adapter-only tuning.",
  "analysis_timestamp": "2026-01-06T23:09:26.492165"
}
{
  "prior_works": [
    {
      "title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples",
      "authors": "M. Belkin et al.",
      "year": 2006,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Introduced graph/Laplacian-based regularization within RKHS to leverage unlabeled data, which STKR generalizes by replacing a fixed Laplacian penalty with broad spectral transforms and by formalizing target smoothness beyond the manifold-regularization assumption."
    },
    {
      "title": "Learning with Local and Global Consistency",
      "authors": "D. Zhou et al.",
      "year": 2004,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "Serves as the canonical label-propagation baseline (harmonic extension over a graph), which STKR unifies as a specific spectral filter and surpasses by providing inductive, scalable estimators rather than a purely transductive solution."
    },
    {
      "title": "Diffusion Kernels on Graphs and Other Discrete Input Spaces",
      "authors": "R. Kondor et al.",
      "year": 2002,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Proposed constructing kernels via spectral functions of the graph Laplacian (e.g., heat diffusion), directly inspiring STKR\u2019s core idea of learning with spectrally transformed similarity operators to exploit unlabeled-data geometry."
    },
    {
      "title": "Kernels and Regularization on Graphs",
      "authors": "A. Smola et al.",
      "year": 2003,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Established the operator-theoretic view that graph regularization corresponds to spectral multipliers defining kernels, a viewpoint STKR adopts to design and analyze general spectral transformations for regression."
    },
    {
      "title": "Optimal Rates for the Regularized Least-Squares Algorithm",
      "authors": "A. Caponnetto et al.",
      "year": 2007,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Provided the spectral-source-condition framework and learning-rate analysis for kernel ridge regression that STKR extends to characterize universal target smoothness and consistency under unlabeled-data-driven spectral filters."
    },
    {
      "title": "Using the Nystr\u00f6m Method to Speed Up Kernel Machines",
      "authors": "C. Williams et al.",
      "year": 2001,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "Introduced Nystr\u00f6m low-rank approximations that STKR leverages on unlabeled data to implement general spectral transformations efficiently and inductively at scale."
    }
  ],
  "synthesis_narrative": "Graph-based semi-supervised learning established that unlabeled examples can be exploited by enforcing smoothness over a data graph or manifold. Manifold Regularization formalized this by coupling empirical loss with an RKHS norm and a graph-Laplacian penalty, making the regularizer explicitly data-distribution\u2013dependent. Label Propagation instantiated this idea as a harmonic energy minimization whose closed-form solution is fully transductive, revealing the strength and the limitation of graph smoothness when generalization to new points is needed. Diffusion Kernels introduced constructing kernels via spectral functions of the Laplacian\u2014such as the heat kernel\u2014demonstrating that geometry can be injected by transforming eigenvalues to shape similarity. Complementing this, Kernels and Regularization on Graphs provided an operator view in which regularization corresponds to spectral multipliers, clarifying how different filters encode different smoothness priors. On the statistical side, Optimal Rates for Regularized Least Squares developed spectral source conditions and learning-rate analyses for kernel regression, linking eigen-decay, target smoothness, and generalization. Finally, the Nystr\u00f6m method enabled scalable low-rank approximations of kernel operators, a practical route to apply spectral machinery on large unlabeled datasets.\nTogether, these works exposed a gap: powerful spectral smoothness priors existed but were often tied to specific filters or transductive settings, and lacked a unifying theory connecting unlabeled-data\u2013induced geometry to learnability with flexible transforms. The current paper synthesizes these insights by formulating a general spectrally transformed kernel regression framework, extending source-condition theory to a universal target smoothness class, and operationalizing it at scale via Nystr\u00f6m-based inductive implementations.",
  "target_paper": {
    "title": "Spectrally Transformed Kernel Regression",
    "authors": "Runtian Zhai, Rattana Pukdee, Roger Jin, Maria Florina Balcan, Pradeep Kumar Ravikumar",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Learning Theory, Unlabeled Data, Kernel Methods, Semi-supervised Learning, Representation Learning, Label Propagation",
    "abstract": "Unlabeled data is a key component of modern machine learning. In general, the role\nof unlabeled data is to impose a form of smoothness, usually from the similarity\ninformation encoded in a base kernel, such as the \u03f5-neighbor kernel or the adjacency\nmatrix of a graph. This work revisits the classical idea of spectrally transformed\nkernel regression (STKR), and provides a new class of general and scalable STKR\nestimators able to leverage unlabeled data. Intuitively, via spectral transformation,\nSTKR exploits the data distribution for which unlabeled data can provide additional\ninformation. First, we show that STKR is a principled and general approach,\nby characterizing a universal type of \u201ctarget smoothness\u201d, and proving that any\nsufficiently smooth function can be learned by STKR. Second, we provide scalable\nSTKR implementations for the inductive setting and a general transformation\nfunction, while prior work is mostly limited to the transductive setting. Third, we\nderive statistical gu",
    "openreview_id": "OeQE9zsztS",
    "forum_id": "OeQE9zsztS"
  },
  "analysis_timestamp": "2026-01-06T09:30:38.091739"
}
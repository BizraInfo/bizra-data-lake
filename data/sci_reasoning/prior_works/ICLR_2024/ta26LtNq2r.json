{
  "prior_works": [
    {
      "title": "On optimum recognition error and reject trade-off",
      "authors": "C. K. Chow",
      "year": 1970,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Chow\u2019s Bayes rule for 0\u20131 loss with abstention is the conceptual starting point that this paper generalizes to balanced error and other non-decomposable metrics by deriving the coupled optimal classifier\u2013rejector conditions."
    },
    {
      "title": "SelectiveNet: A Deep Neural Network with an Integrated Reject Option",
      "authors": "Yarin Geifman et al.",
      "year": 2019,
      "arxiv_id": "1901.09192",
      "role": "Baseline",
      "relationship_sentence": "SelectiveNet is a primary selective-classification baseline that optimizes standard misclassification risk, which this paper shows can be grossly suboptimal under balanced error and thereby directly improves upon."
    },
    {
      "title": "Classification with a Reject Option using a Hinge Loss",
      "authors": "Peter L. Bartlett et al.",
      "year": 2008,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This surrogate-based abstention method targets 0\u20131 error, and its limitation\u2014ignoring class- or group-dependent costs\u2014motivates the present work\u2019s derivation of Bayes-optimal reject rules tailored to balanced error."
    },
    {
      "title": "Long-tail Learning via Logit-Adjusted Softmax Loss",
      "authors": "Aditya Krishna Menon et al.",
      "year": 2021,
      "arxiv_id": "2007.07314",
      "role": "Inspiration",
      "relationship_sentence": "The logit-adjustment insight\u2014embedding class-prior\u2013dependent costs to optimize balanced error\u2014directly informs this paper\u2019s plug-in estimator for the classifier and its coupling with the rejector under long-tail metrics."
    },
    {
      "title": "Consistent Binary Classification with Generalized Performance Metrics",
      "authors": "Oluwasanmi O. Koyejo et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Building on the confusion-matrix\u2013based Bayes analysis and plug-in strategies for non-decomposable metrics, this paper extends the framework to the selective setting where reject decisions interact with label costs."
    },
    {
      "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization",
      "authors": "Shiori Sagawa et al.",
      "year": 2020,
      "arxiv_id": "1911.08731",
      "role": "Foundation",
      "relationship_sentence": "By formalizing worst-group error as a target objective, this work provides the metric that the present paper extends its Bayes-optimal selective-classification theory and plug-in method to handle."
    }
  ],
  "synthesis_narrative": "Chow established the Bayes-optimal decision rule for classification with a reject option under 0\u20131 loss, tying abstention to posterior thresholds and class-dependent costs. Bartlett and Wegkamp introduced consistent surrogate losses for abstention, but still optimized standard misclassification risk, decoupled from class or group asymmetries. In deep learning, SelectiveNet integrated a rejector into a classifier to shape the risk\u2013coverage curve, again targeting overall error rather than equity-aware metrics. Separately, Menon and colleagues showed that balanced error in long-tail regimes can be optimized by embedding class priors as costs via logit adjustment, giving a practical plug-in route to Bayes-optimal classification under imbalance. Koyejo and co-authors provided a general confusion-matrix perspective, deriving Bayes rules and plug-in strategies for non-decomposable metrics by linking decisions to metric-induced costs. Sagawa et al. formalized worst-group error through GroupDRO, elevating robustness to the worst subpopulation as an explicit metric to optimize.\nTogether, these works expose a gap: selective-classification methods focus on average 0\u20131 error, while long-tail and robustness metrics require decision rules that couple class/group-specific costs with rejection. The natural next step is to derive the Bayes-optimal classifier\u2013rejector for equity-focused metrics (balanced and worst-group error) and to operationalize it via a plug-in approach. By marrying Chow\u2019s abstention principle with logit-adjusted costs and confusion-matrix\u2013based plug-in analysis, the current work produces a coupled decision rule and estimator that align rejection with class/group costs, overcoming the suboptimality of prior 0\u20131\u2013centric selective methods.",
  "target_paper": {
    "title": "Learning to Reject Meets Long-tail Learning",
    "authors": "Harikrishna Narasimhan, Aditya Krishna Menon, Wittawat Jitkrittum, Neha Gupta, Sanjiv Kumar",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Learning to reject, balanced error, evaluation metrics, selective classification, plug-in approach, long-tail learning, class imbalance, non-decomposable metrics",
    "abstract": "Learning to reject (L2R) is a classical problem where one seeks a classifier capable of abstaining on low-confidence samples. Most prior work on L2R has focused on minimizing the standard misclassification error. However, in many real-world applications, the label distribution is highly imbalanced,  necessitating alternate evaluation metrics such as the balanced error or the worst-group error that enforce equitable performance across both the head and tail classes. In this paper, we establish that traditional L2R methods can be grossly sub-optimal for such metrics, and show that this is due to an intricate  dependence in the objective between the label costs and the rejector. We then derive the form of the Bayes-optimal classifier and rejector for the balanced error, propose a novel plug-in approach to mimic this solution, and extend our results to general evaluation metrics. Through experiments on benchmark  image classification tasks, we show that our approach yields better trade-off",
    "openreview_id": "ta26LtNq2r",
    "forum_id": "ta26LtNq2r"
  },
  "analysis_timestamp": "2026-01-06T10:18:05.646582"
}
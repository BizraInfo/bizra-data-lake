{
  "prior_works": [
    {
      "title": "Reformer: The Efficient Transformer",
      "authors": "Nikita Kitaev et al.",
      "year": 2020,
      "arxiv_id": "2001.04451",
      "role": "Extension",
      "relationship_sentence": "MagicPIG repurposes Reformer\u2019s LSH-based bucketing of queries/keys, but instead of restricting attention within buckets, it uses the buckets to importance-sample KV pairs during decoding, yielding an unbiased estimator of attention with far lower runtime."
    },
    {
      "title": "SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems",
      "authors": "Beidi Chen et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "MagicPIG adopts SLIDE\u2019s heterogeneous design\u2014maintaining LSH hash tables on CPU to cheaply select a small candidate set that is then processed on GPU\u2014adapting it from sparse MLP training to LLM attention inference."
    },
    {
      "title": "Similarity Estimation Techniques from Rounding Algorithms",
      "authors": "Moses Charikar",
      "year": 2002,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "MagicPIG relies on Charikar\u2019s sign random projection LSH for angular similarity to construct hash tables whose collision probabilities track query\u2013key cosine similarity, enabling provably good sampling of high-contribution keys."
    },
    {
      "title": "Asymmetric LSH (ALSH) for Maximum Inner Product Search (MIPS)",
      "authors": "Anshumali Shrivastava et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Because attention is governed by dot products, MagicPIG draws on ALSH\u2019s transformation for MIPS so that LSH buckets preferentially retrieve large inner-product keys, aligning the sampler with the attention scoring function."
    },
    {
      "title": "Rethinking Attention with Performers",
      "authors": "Krzysztof Choromanski et al.",
      "year": 2021,
      "arxiv_id": "2009.14794",
      "role": "Related Problem",
      "relationship_sentence": "Performer\u2019s unbiased, theoretically grounded approximation of softmax attention motivates MagicPIG\u2019s shift from deterministic TopK selection to a provably justified stochastic estimator, though MagicPIG uses data-dependent LSH sampling rather than random features."
    },
    {
      "title": "Big Bird: Transformers for Longer Sequences",
      "authors": "Manzil Zaheer et al.",
      "year": 2020,
      "arxiv_id": "2007.14062",
      "role": "Gap Identification",
      "relationship_sentence": "BigBird shows fixed sparse patterns can approximate attention but implicitly assume sparsity, a limitation that MagicPIG addresses by demonstrating quality drops when attention is dense and replacing hard sparsity with unbiased LSH-based sampling."
    }
  ],
  "synthesis_narrative": "Charikar introduced sign random projection LSH for angular similarity, establishing that hash collisions probabilistically track cosine similarity; this makes it possible to retrieve vectors with high dot products using compact binary hashes. Shrivastava and Li extended this line to maximum inner product search via ALSH, mapping inner-product neighbors to LSH-retrievable space so that large dot-product items are found efficiently. Reformer carried LSH into the Transformer itself, hashing queries and keys so attention is computed within buckets, reducing complexity by exploiting locality in representation space. Performer reframed approximate attention as a statistically principled estimation problem, using unbiased random feature expansions to approximate softmax attention with theoretical guarantees. BigBird demonstrated that sparse patterns can preserve long-range modeling with theoretical coverage, but these patterns still hinge on attention being sparse. SLIDE showed that hashing can power a heterogeneous system: maintain cheap hash tables on CPU to shortlist high-value candidates and push only those to accelerators for heavy compute. These threads collectively reveal that (1) attention relevance aligns with dot-product neighbors recoverable by LSH, (2) unbiased estimators can replace brittle TopK heuristics, and (3) hashing-driven candidate selection maps naturally to CPU\u2013GPU pipelines. The natural next step is to replace deterministic TopK or fixed sparsity with LSH-guided, theoretically grounded sampling of keys and values during decoding, and to operationalize it in a heterogeneous system that builds and queries hash tables efficiently while preserving attention quality when sparsity assumptions fail.",
  "target_paper": {
    "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
    "authors": "Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "locality sensitive hashing, randomized algorithms, llm inference, kv cache",
    "abstract": "Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation o",
    "openreview_id": "ALzTQUgW8a",
    "forum_id": "ALzTQUgW8a"
  },
  "analysis_timestamp": "2026-01-06T17:37:05.387461"
}
{
  "prior_works": [
    {
      "title": "Neural Ordinary Differential Equations",
      "authors": "Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud",
      "year": 2018,
      "role": "Conceptual foundation for continuous-depth neural models",
      "relationship_sentence": "DRAGON builds on the Neural ODE paradigm by casting graph propagation as a continuous-time dynamical system, but replaces integer-order dynamics with a learnable distributed-order fractional operator."
    },
    {
      "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering",
      "authors": "Micha\u00ebl Defferrard, Xavier Bresson, Pierre Vandergheynst",
      "year": 2016,
      "role": "Spectral filtering with polynomial powers of the Laplacian",
      "relationship_sentence": "ChebNet\u2019s learnable polynomial filters mix multiple Laplacian powers (discrete orders of propagation), a stepping stone that DRAGON generalizes to a continuous mixture over derivative orders via distributed-order fractional calculus."
    },
    {
      "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank (APPNP)",
      "authors": "Johannes Klicpera, Aleksandar Bojchevski, Stephan G\u00fcnnemann",
      "year": 2019,
      "role": "Propagation as a learnable mixture of walk lengths",
      "relationship_sentence": "APPNP demonstrates that weighting a distribution over diffusion steps (walk lengths) improves graph learning; DRAGON extends this idea by learning a probability distribution over fractional derivative orders, yielding a principled continuous-time, non-Markovian counterpart."
    },
    {
      "title": "The random walk\u2019s guide to anomalous diffusion: a fractional dynamics approach",
      "authors": "Ralf Metzler, Joseph Klafter",
      "year": 2000,
      "role": "Core theory linking fractional derivatives, memory kernels, and anomalous diffusion",
      "relationship_sentence": "DRAGON\u2019s interpretation as non-Markovian random walks with anomalous diffusion directly leverages the fractional kinetics framework established by Metzler and Klafter to justify memory effects induced by fractional-order operators."
    },
    {
      "title": "Distributed-order fractional kinetics",
      "authors": "A. V. Chechkin, Rudolf Gorenflo, Igor M. Sokolov",
      "year": 2002,
      "role": "Foundational formulation of distributed-order fractional derivatives",
      "relationship_sentence": "The central idea in DRAGON\u2014learning a distribution over derivative orders\u2014echoes distributed-order fractional models introduced by Chechkin et al., which capture multi-scale, heterogeneous anomalous diffusion via superpositions of fractional orders."
    },
    {
      "title": "L\u00e9vy random walks on networks: long-range navigation",
      "authors": "R. Felipe Riascos, Jos\u00e9 L. Mateos",
      "year": 2014,
      "role": "Anomalous diffusion and nonlocal random walks on graphs",
      "relationship_sentence": "By connecting node feature evolution to anomalous diffusion/random walks on networks, DRAGON draws on results showing how fractional dynamics induce nonlocal, memoryful propagation over graphs."
    },
    {
      "title": "Wavelets on graphs via spectral graph theory",
      "authors": "David K. Hammond, Pierre Vandergheynst, R\u00e9mi Gribonval",
      "year": 2011,
      "role": "Spectral operators and multiscale diffusion on graphs",
      "relationship_sentence": "Spectral graph wavelets formalize flexible spectral filters g(L) (including heat-kernel-style diffusion), providing the operator-theoretic backdrop that DRAGON generalizes with learned distributed-order fractional operators."
    }
  ],
  "synthesis_narrative": "DRAGON\u2019s core contribution\u2014learning a probability distribution over fractional derivative orders to govern continuous-time graph propagation\u2014sits at the intersection of continuous-depth neural modeling, graph diffusion, and fractional kinetics. Neural ODEs established how to parameterize and train continuous-time dynamics end-to-end, which DRAGON adopts on graphs. On the graph side, spectral filtering and diffusion-based propagation (via ChebNet and spectral wavelets) showed that mixing powers of the Laplacian effectively blends multi-hop information, while APPNP made this idea explicit by weighting a distribution over walk lengths to improve long-range propagation. Fractional dynamics supply the missing physical lens: classic results by Metzler and Klafter link fractional derivatives to non-Markovian dynamics and anomalous diffusion with power-law memory kernels, providing the theoretical mechanism DRAGON exploits to encode history dependence in feature updates. Crucially, distributed-order fractional calculus (Chechkin\u2013Gorenflo\u2013Sokolov) generalizes single fractional orders to mixtures over orders, capturing heterogeneous, multi-scale diffusion\u2014precisely the flexibility DRAGON renders learnable. Finally, anomalous diffusion on networks and L\u00e9vy-style random walks (Riascos\u2013Mateos) connect fractional kinetics to graph domains, grounding DRAGON\u2019s interpretation as a non-Markovian random walk-driven update process. Together, these strands motivate and enable DRAGON\u2019s learnable distributed-order operator, which unifies and extends diffusion-based GNNs and continuous-time models to capture richer, multi-scale, memoryful dynamics on graphs.",
  "analysis_timestamp": "2026-01-06T23:33:35.541873"
}
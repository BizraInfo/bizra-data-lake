{
  "prior_works": [
    {
      "title": "Long Range Arena: A Benchmark for Efficient Transformers",
      "authors": "Yi Tay et al.",
      "year": 2020,
      "arxiv_id": "2011.04006",
      "role": "Foundation",
      "relationship_sentence": "LRA established the long-sequence benchmark and the default train-from-scratch evaluation protocol (including tasks like PathX) that this work argues biases architectural comparisons and replaces with in-domain denoising pretraining."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "authors": "Albert Gu et al.",
      "year": 2022,
      "arxiv_id": "2111.00396",
      "role": "Baseline",
      "relationship_sentence": "S4 is the primary SSM baseline whose reported superiority on LRA/PathX under scratch training is re-evaluated here by introducing data-driven denoising pretraining that shrinks the Transformer\u2013SSM gap and boosts SSM results."
    },
    {
      "title": "Hyena Hierarchy: Towards Larger Context with Recursive Gating",
      "authors": "Francesco Poli et al.",
      "year": 2023,
      "arxiv_id": "2302.10866",
      "role": "Related Problem",
      "relationship_sentence": "Hyena reported state-of-the-art long-range results on LRA under the same scratch-training regime, motivating the need to reassess such claims using identical data-driven priors across architectures."
    },
    {
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": "Jacob Devlin et al.",
      "year": 2019,
      "arxiv_id": "1810.04805",
      "role": "Inspiration",
      "relationship_sentence": "BERT introduced masked-language-model denoising pretraining, which this work repurposes on the downstream task\u2019s unlabeled inputs to inject data-driven priors before supervised fine-tuning across architectures."
    },
    {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)",
      "authors": "Colin Raffel et al.",
      "year": 2020,
      "arxiv_id": "1910.10683",
      "role": "Extension",
      "relationship_sentence": "T5\u2019s span-corruption denoising objective directly informs the pretraining objective used here for long-sequence text, adapted to operate solely on the downstream task data to ensure fair initialization."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "authors": "Kaiming He et al.",
      "year": 2022,
      "arxiv_id": "2111.06377",
      "role": "Inspiration",
      "relationship_sentence": "MAE demonstrated that masked reconstruction on image patches yields strong in-domain representations, which this work adapts to image-like long-range tasks (e.g., Pathfinder/PathX) for fair, modality-appropriate pretraining."
    }
  ],
  "synthesis_narrative": "Long Range Arena codified the evaluation of long-sequence modeling with a fixed suite of tasks and a train-from-scratch protocol that became the de facto standard, and under this setup large performance gaps were repeatedly observed across architectures on tasks like PathX. Building on that regime, Structured State Spaces (S4) showed strong advantages over Transformers and introduced PathX as a particularly challenging long-context task, with results reported under scratch training. Hyena further advanced non-attention long-range modeling, again establishing gains on LRA using the same training-from-scratch convention. In parallel, self-supervised denoising emerged as a way to encode data-driven priors without labels: BERT\u2019s masked language modeling learned in-domain statistics from raw text; T5\u2019s span corruption extended denoising to longer contiguous spans that better capture long-range dependencies; and MAE showed that masked patch reconstruction on images can produce powerful representations from the same data distribution.\nTaken together, these strands suggested an opportunity: the scratch-training protocol in long-range benchmarks may inflate architectural differences by omitting simple, modality-appropriate denoising priors that are already known to be effective. The present work synthesizes this insight by applying standard denoising pretraining using only the downstream task data\u2014span corruption for text and masked reconstruction for image-like inputs\u2014across competing architectures, then re-evaluating on LRA. This data-driven initialization dramatically lifts all models, collapses reported gaps between Transformers and SSMs, and yields new state-of-the-art results on challenging PathX variants, establishing a fairer comparison paradigm for long-sequence modeling.",
  "target_paper": {
    "title": "Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors",
    "authors": "Ido Amos, Jonathan Berant, Ankit Gupta",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Pre Training, Transformers, State Space Models, Long Range Models, Fair Evaluation",
    "abstract": "Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, *using only the downstream task data*, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute ",
    "openreview_id": "PdaPky8MUn",
    "forum_id": "PdaPky8MUn"
  },
  "analysis_timestamp": "2026-01-06T07:56:52.502489"
}
{
  "prior_works": [
    {
      "title": "A Faster Combinatorial Algorithm for Discrepancy Minimization",
      "authors": "Kasper Green Larsen",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "This work is the immediate baseline the paper accelerates and matches in approximation: the new algorithms adopt Larsen\u2019s combinatorial framework for minimizing disc(A,x) and improve its O~(mn^2) runtime to input-sparsity time O~(nnz(A)+n^3) and further to O~(nnz(A)+n^{2.53})."
    },
    {
      "title": "Constructive Algorithms for Discrepancy Minimization",
      "authors": "Nikhil Bansal",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "Bansal\u2019s SDP-based algorithm established the modern constructive framework and herdisc-based guarantees that both Larsen and the present paper aim to match while dramatically reducing runtime."
    },
    {
      "title": "Efficient algorithms for discrepancy minimization in convex sets",
      "authors": "Ronen Eldan and Mohit Singh",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "LP/convex-programming approaches exemplified by Eldan\u2013Singh incur per-iteration costs that prevent subcubic runtimes on dense instances; the new paper explicitly bypasses these limitations and achieves O~(nnz(A)+n^{2.53}) for square matrices."
    },
    {
      "title": "Factorization Norms and Hereditary Discrepancy",
      "authors": "Ji\u0159\u00ed Matou\u0161ek, Aleksandar Nikolov, and Kunal Talwar",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized the tight connection between hereditary discrepancy and \u03b32 factorization norms, providing the theoretical lens and approximation target that Larsen\u2019s method and the present input-sparsity-time algorithms explicitly preserve."
    },
    {
      "title": "Constructive Discrepancy Minimization by Walking on the Edges",
      "authors": "Shachar Lovett and Raghu Meka",
      "year": 2012,
      "role": "Inspiration",
      "relationship_sentence": "Their combinatorial partial-coloring/edge-walk paradigm directly inspired the line of combinatorial (non-SDP) algorithms for discrepancy minimization that Larsen advanced and the present work further accelerates."
    },
    {
      "title": "Low-Rank Approximation and Regression in Input Sparsity Time",
      "authors": "Kenneth L. Clarkson and David P. Woodruff",
      "year": 2013,
      "role": "Extension",
      "relationship_sentence": "The paper\u2019s input-sparsity-time results rest on applying oblivious subspace embedding/sketching techniques pioneered by Clarkson\u2013Woodruff to reduce dependence on m, enabling the O~(nnz(A)+poly(n)) runtimes while maintaining the guarantees needed by the combinatorial coloring procedure."
    }
  ],
  "synthesis_narrative": "The paper stands squarely in the combinatorial lineage of constructive discrepancy minimization. Bansal (2010) crystallized the algorithmic objective\u2014minimizing discrepancy with guarantees in terms of hereditary discrepancy\u2014and delivered an SDP-based method with strong bounds but heavy runtime. Lovett and Meka (2012) then showed that combinatorial, partial-coloring walks could achieve powerful constructive bounds without SDPs, catalyzing a non-convex-programming approach to discrepancy. Building on this trajectory, Larsen (SODA 2023) provided a faster purely combinatorial algorithm with O~(mn^2) time and herdisc-based guarantees, establishing the immediate baseline that the present paper targets. \nA key enabler of the new contribution is sketching: techniques from Clarkson and Woodruff (2013) allow reducing the dependence on the number of rows m to input-sparsity time while preserving the linear-algebraic structure the combinatorial walk relies on. This yields an O~(nnz(A)+n^3) algorithm and, with additional algebraic speedups, O~(nnz(A)+n^{2.53}). The guarantees align with the \u03b32/hereditary-discrepancy framework of Matou\u0161ek, Nikolov, and Talwar (2015), ensuring the approximation targets remain intact under sketching and combinatorial updates. Finally, the work explicitly surpasses the practical and theoretical limitations of LP-based approaches represented by Eldan and Singh (2018), which face inherent bottlenecks preventing subcubic performance on dense, square instances. Together, these works directly form the conceptual and technical backbone of the paper\u2019s input-sparsity-time discrepancy minimization algorithms.",
  "analysis_timestamp": "2026-01-06T23:07:19.635803"
}
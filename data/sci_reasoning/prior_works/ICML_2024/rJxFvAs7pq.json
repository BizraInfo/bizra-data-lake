{
  "prior_works": [
    {
      "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation",
      "authors": "Richard S. Sutton et al.",
      "year": 1999,
      "role": "Foundation",
      "relationship_sentence": "This work introduced the policy-gradient formulation with function approximation that underlies the actor update; the present paper adopts this framework in continuous state-action spaces with neural network policies to analyze global, last-iterate convergence."
    },
    {
      "title": "Actor-Critic Algorithms",
      "authors": "Vijay R. Konda et al.",
      "year": 2000,
      "role": "Foundation",
      "relationship_sentence": "Konda and Tsitsiklis established the two-time-scale actor\u2013critic scheme and its stochastic approximation under Markovian sampling, which the current paper directly analyzes but now with multi-layer neural parametrizations and last-iterate global guarantees."
    },
    {
      "title": "The ODE method for convergence of stochastic approximation and reinforcement learning",
      "authors": "V. S. Borkar et al.",
      "year": 2000,
      "role": "Foundation",
      "relationship_sentence": "The ODE/SA framework for handling Markovian noise is a core technical tool that the paper leverages to control actor\u2013critic updates under Markovian sampling and derive finite-sample, last-iterate convergence."
    },
    {
      "title": "Natural Actor-Critic Algorithms",
      "authors": "Shalabh Bhatnagar et al.",
      "year": 2009,
      "role": "Baseline",
      "relationship_sentence": "As a canonical actor\u2013critic baseline with convergence under Markovian sampling (for linear function approximation), this work is generalized here to multi-layer neural actors/critics with global optimality and last-iterate guarantees."
    },
    {
      "title": "Finite-Sample Analysis of Actor-Critic for Discounted MDPs",
      "authors": "Shaofeng Zou et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "This paper provided finite-sample AC analysis but relied on linear approximation, i.i.d./MDS noise and often averaged-iterate guarantees; the present work explicitly closes these gaps by addressing Markovian sampling, neural networks, continuous spaces, and last-iterate global convergence."
    },
    {
      "title": "Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator",
      "authors": "Maryam Fazel et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "By revealing a PL/gradient-dominance-type landscape enabling global convergence of policy gradient, this work inspired the present paper\u2019s use of weak-PL-style arguments to obtain global, last-iterate convergence for actor\u2013critic."
    },
    {
      "title": "Gradient Descent Finds Global Minima of Over-parameterized Deep Neural Networks",
      "authors": "Simon S. Du et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Techniques for controlling optimization dynamics of over-parameterized neural networks inform the paper\u2019s analysis of multi-layer actor/critic networks, which is adapted to the RL/Markovian setting to achieve global last-iterate guarantees."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014global, last-iterate convergence of actor\u2013critic under Markovian sampling with multi-layer neural parameterization in continuous spaces\u2014rests on a direct lineage that brings together classic RL foundations, stochastic approximation under Markovian noise, and modern global convergence insights. Sutton et al. (1999) provide the policy-gradient formulation with function approximation that defines the actor component the paper studies, while Konda and Tsitsiklis (2000) introduce the two-time-scale actor\u2013critic architecture and its analysis under Markovian sampling\u2014the operational template the new results extend. Borkar and Meyn (2000) furnish the ODE/SA toolkit for Markovian noise, which the authors adapt to control coupled actor\u2013critic recursions without resorting to i.i.d. assumptions. On the performance side, existing finite-sample AC analyses such as Zou et al. (2019) expose key gaps\u2014linear approximation, i.i.d. noise, non-global or averaged-iterate guarantees\u2014that the present work explicitly closes via its MMCLG criteria. To move from local to global guarantees and to the last-iterate, the paper draws on the global-optimization perspective of policy gradient landscapes exemplified by Fazel et al. (2018), importing weak-PL/graduent-dominance style reasoning into an AC setting. Finally, to accommodate multi-layer neural parameterizations, the analysis leverages ideas from over-parameterized neural network optimization (Du et al., 2019), adapting them to RL with Markovian sampling. These strands jointly enable the paper\u2019s O~(\u03b5^-3) last-iterate global convergence bounds for neural actor\u2013critic in continuous domains.",
  "analysis_timestamp": "2026-01-06T23:09:26.467454"
}
{
  "prior_works": [
    {
      "title": "Language Models are Few-Shot Learners",
      "authors": "Tom B. Brown et al.",
      "year": 2020,
      "arxiv_id": "2005.14165",
      "role": "Inspiration",
      "relationship_sentence": "This work established in-context learning via prompting with demonstrations, directly inspiring OFA\u2019s design of a graph prompting paradigm that enables a single model to solve diverse graph tasks without task-specific fine-tuning."
    },
    {
      "title": "Finetuned Language Models are Zero-Shot Learners",
      "authors": "Jason Wei et al.",
      "year": 2021,
      "arxiv_id": "2109.01652",
      "role": "Inspiration",
      "relationship_sentence": "The instruction-tuning insight\u2014unifying diverse tasks under textual instructions\u2014motivated OFA\u2019s instruction-style templates for node, link, and graph classification that standardize task interfaces for in-context learning on graphs."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "arxiv_id": "2103.00020",
      "role": "Inspiration",
      "relationship_sentence": "By showing text can serve as a universal interface to align heterogeneous data, CLIP directly motivated OFA\u2019s use of text-attributed graphs to place diverse graph domains into a shared representation space."
    },
    {
      "title": "Generative Pre-Training of Graph Neural Networks",
      "authors": "Weihua Hu et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "GPT-GNN demonstrated graph pre-training across datasets but required task-specific heads and fine-tuning, a limitation OFA explicitly addresses by enabling a single graph model to handle multiple task types via prompting and in-context learning."
    },
    {
      "title": "Link Prediction Based on Graph Neural Networks",
      "authors": "Muhan Zhang and Yixin Chen",
      "year": 2018,
      "arxiv_id": "1802.09691",
      "role": "Extension",
      "relationship_sentence": "SEAL\u2019s idea of casting a candidate edge as an enclosing subgraph classification is directly extended in OFA to encode link tasks in the same unified prompting/representation interface used for node and graph classification."
    },
    {
      "title": "Open Graph Benchmark: Datasets for Machine Learning on Graphs",
      "authors": "Weihua Hu et al.",
      "year": 2020,
      "arxiv_id": "2005.00687",
      "role": "Foundation",
      "relationship_sentence": "OGB formalized standardized node-, link-, and graph-level tasks that OFA targets, providing the task formulations and diverse domains that the unified, single-model design sets out to encompass."
    }
  ],
  "synthesis_narrative": "In-context learning in language models showed that a single sequence model could perform diverse tasks by conditioning on a few labeled demonstrations, with GPT-3 pioneering the format and later instruction-tuning research demonstrating that natural-language instructions could standardize heterogeneous task interfaces. Concurrently, CLIP revealed that natural language can function as a universal modality to align disparate data sources, suggesting text as a powerful unifying representation layer. In the graph domain, GPT-GNN established the feasibility of pretraining graph neural networks across datasets, but its reliance on task-specific heads and fine-tuning left open the question of a truly task-agnostic graph solver. For link prediction, SEAL reframed edges as subgraph classification via enclosing subgraphs, providing a concrete mechanism to cast edges in a classification template comparable to nodes and graphs. Finally, the Open Graph Benchmark codified standardized node, link, and graph tasks across diverse domains, clarifying the multi-task landscape a single model would need to address.\nThese threads collectively indicated a path: use natural language to unify heterogeneous graph attributes, adopt instruction/prompt formats to standardize task interfaces, and encode links via subgraphs so all tasks look like classification under a single model. Building on these insights, a one-for-all graph learner becomes natural: represent nodes and edges with textual descriptions, format node/link/graph tasks via instruction-like prompts and in-context exemplars, and process all with one graph model that can generalize across datasets and task types without per-task heads or fine-tuning.",
  "target_paper": {
    "title": "One For All: Towards Training One Graph Model For All Classification Tasks",
    "authors": "Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Graph Neural Network, Large Language Model, In-context Learning",
    "abstract": "Designing a single model to address multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in solving different tasks within the language domain. However, a unified model for various graph tasks remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. We propose **One for All (OFA)**, the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-attributed graphs to unify different graph data by describing nodes and edge",
    "openreview_id": "4IT2pgc9v6",
    "forum_id": "4IT2pgc9v6"
  },
  "analysis_timestamp": "2026-01-06T17:00:56.914552"
}
{
  "prior_works": [
    {
      "title": "Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming",
      "authors": "Michel X. Goemans, David P. Williamson",
      "year": 1995,
      "role": "Foundational SDP relaxation and randomized rounding for Max-Cut (and related CSPs)",
      "relationship_sentence": "OptGNN directly leverages the Goemans\u2013Williamson SDP and hyperplane rounding paradigm as the canonical template for encoding SDP-based approximation procedures within a message-passing GNN."
    },
    {
      "title": "Optimal Algorithms and Inapproximability Results for Every CSP",
      "authors": "Prasad Raghavendra",
      "year": 2008,
      "role": "UGC-calibrated optimality of a single SDP relaxation and rounding for all Max-CSPs",
      "relationship_sentence": "The paper\u2019s central claim\u2014that polynomial-sized message passing can represent the most powerful polynomial-time algorithms for Max-CSPs under UGC\u2014builds directly on Raghavendra\u2019s optimal SDP framework and rounding guarantees."
    },
    {
      "title": "On the Power of Unique 2-Prover 1-Round Games",
      "authors": "Subhash Khot",
      "year": 2002,
      "role": "Unique Games Conjecture (UGC) foundation",
      "relationship_sentence": "All optimality statements in OptGNN hinge on the Unique Games Conjecture introduced by Khot, which underpins the equivalence between achievable approximation ratios and SDP-based algorithms."
    },
    {
      "title": "Optimal Inapproximability Results for MAX-CUT and Other 2-Variable CSPs?",
      "authors": "Subhash Khot, Guy Kindler, Elchanan Mossel, Ryan O'Donnell",
      "year": 2007,
      "role": "UGC-based hardness matching SDP rounding performance for 2-CSPs",
      "relationship_sentence": "KKMO\u2019s results calibrate the Goemans\u2013Williamson ratio as optimal under UGC, guiding OptGNN\u2019s design to target SDP-achievable approximation factors via learnable rounding implemented by message passing."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka",
      "year": 2019,
      "role": "Theoretical expressivity and limitations of message-passing GNNs",
      "relationship_sentence": "OptGNN\u2019s representational claims build on the expressivity analyses of MPNNs, extending this line by constructing polynomial-size message-passing schemes that simulate SDP relaxations and rounding."
    },
    {
      "title": "Learning Fast Approximations of Sparse Coding",
      "authors": "Karol Gregor, Yann LeCun",
      "year": 2010,
      "role": "Algorithm unrolling as network design principle",
      "relationship_sentence": "The architecture mirrors the unrolling paradigm by instantiating layers that emulate steps of SDP-based relaxations and rounding, a direct conceptual inheritance from algorithm unrolling."
    },
    {
      "title": "Differentiable Convex Optimization Layers",
      "authors": "Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, J. Zico Kolter",
      "year": 2019,
      "role": "Bridging convex optimization and deep networks via differentiable layers",
      "relationship_sentence": "By showing how convex relaxations can be integrated into end-to-end learning, this work motivates OptGNN\u2019s use of convex-relaxation structure and the extraction of bounds from learned embeddings."
    }
  ],
  "synthesis_narrative": "OptGNN\u2019s core insight is that message-passing graph neural networks can be architected to emulate the best-known polynomial-time approximation algorithms for Max-CSPs\u2014namely, semidefinite-programming (SDP) relaxations with problem-specific randomized rounding\u2014thereby achieving optimal approximation factors under the Unique Games Conjecture (UGC). This draws a direct algorithmic lineage from Goemans\u2013Williamson\u2019s seminal SDP and hyperplane rounding for Max-Cut, and from Raghavendra\u2019s sweeping result that a single SDP template and rounding scheme achieves the optimal ratio for every Max-CSP assuming UGC. The UGC itself (Khot) and its calibration of Max-Cut hardness matching the GW ratio (KKMO) provide the complexity-theoretic foundation that equates \u201coptimal polynomial-time\u201d with \u201cSDP-based,\u201d setting the performance target OptGNN aims to realize.\nOn the neural side, OptGNN\u2019s design leverages the algorithm-unrolling paradigm (Gregor & LeCun), translating iterative optimization and rounding steps into learnable message-passing modules. Theoretical work on GNN expressivity (Xu et al.) anchors the feasibility of simulating structured computations on graphs with polynomial-sized MPNNs, while guiding architectural choices to encode constraint aggregation and rounding operations. Finally, the broader movement integrating convex optimization into deep learning (Agrawal et al.) motivates incorporating convex-relaxation structure and enables principled extraction of bounds from network embeddings. Together, these threads yield a GNN architecture that captures SDP relaxations and their optimal rounding, unifying approximation algorithms and message passing into a single, scalable framework.",
  "analysis_timestamp": "2026-01-06T23:39:42.957026"
}
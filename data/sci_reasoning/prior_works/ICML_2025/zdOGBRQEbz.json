{
  "prior_works": [
    {
      "title": "Toy Models of Superposition",
      "authors": "Nelson Elhage et al. (Anthropic)",
      "year": 2022,
      "role": "Conceptual foundation",
      "relationship_sentence": "Introduced the superposition hypothesis and argued for sparse feature decompositions, motivating the use of sparse autoencoders to disentangle overlapping features in transformer residual streams as done here for protein LMs."
    },
    {
      "title": "Towards Monosemanticity: Decomposing Language Models with Sparse Autoencoders",
      "authors": "Bricken et al. (Anthropic)",
      "year": 2023,
      "role": "Methodological template",
      "relationship_sentence": "Established training SAEs on the residual stream of LMs to recover interpretable, monosemantic features and provided practical design choices that this paper adapts to ESM-2."
    },
    {
      "title": "Understanding intermediate layers using linear classifier probes",
      "authors": "Guillaume Alain, Yoshua Bengio",
      "year": 2016,
      "role": "Evaluation methodology",
      "relationship_sentence": "Provided the linear probing framework adopted here to assess which biological properties are linearly decodable from SAE-derived features."
    },
    {
      "title": "Evaluating Protein Transfer Learning with TAPE",
      "authors": "Rao et al.",
      "year": 2019,
      "role": "Protein LM probing benchmark",
      "relationship_sentence": "Popularized linear evaluation on protein LM representations for structure/function tasks, informing this work\u2019s probing of SAE features for properties like thermostability and localization."
    },
    {
      "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
      "authors": "Alexander Rives et al.",
      "year": 2021,
      "role": "Precursor protein LM (ESM-1b)",
      "relationship_sentence": "Showed that transformer pLMs encode structural and functional signals and introduced the ESM line, providing the precedent for interpreting residual-stream features in pLMs."
    },
    {
      "title": "Language models of protein sequences at the scale of evolution enable accurate structure prediction (ESMFold/ESM-2)",
      "authors": "Zeming Lin et al.",
      "year": 2022,
      "role": "Base model and context",
      "relationship_sentence": "Described ESM-2, the pLM whose residual stream this paper decomposes with SAEs, and established its rich internal representations relevant to structural biology."
    },
    {
      "title": "Transformer protein language models are unsupervised structure learners",
      "authors": "Roshan Rao et al.",
      "year": 2021,
      "role": "Evidence of interpretable protein features",
      "relationship_sentence": "Demonstrated that transformer internals (e.g., attention patterns) capture contacts and motifs, motivating mechanistic interpretability applied to protein models beyond attention\u2014here via SAEs on the residual stream."
    }
  ],
  "synthesis_narrative": "This work\u2019s core contribution\u2014training sparse autoencoders (SAEs) on the residual stream of a protein language model (ESM-2) to extract interpretable features that bridge model internals and protein biology\u2014directly builds on mechanistic interpretability advances and protein LM foundations. The superposition framework of Elhage et al. articulated why high-dimensional transformer features overlap and why sparsity-based decompositions are needed, setting the conceptual basis for disentangling polysemantic representations. Bricken et al. operationalized this with SAEs trained on language model residual streams to yield monosemantic features, providing the exact methodological blueprint the present work adapts to the protein domain, including choices around sparsity, normalization, and feature interpretability analyses.\nOn the protein side, the ESM series (Rives et al.; Lin et al.) established that pLM residual streams contain rich structural and functional information, and supplied the specific model (ESM-2) used here. Earlier evidence that model internals encode biologically meaningful signals (Rao et al.) motivated going beyond attention visualization toward a more systematic feature decomposition. To evaluate the biological relevance of discovered features, the paper relies on linear probing\u2014formalized by Alain and Bengio\u2014and on the protein community\u2019s practice of probing pLM embeddings for downstream properties exemplified by TAPE. Together, these works converge: SAEs from mechanistic interpretability address superposition in transformer representations; ESM provides biologically meaningful activations; and linear probes validate that the learned sparse features align with known determinants (e.g., thermostability, localization) while surfacing novel, testable hypotheses about protein mechanisms.",
  "analysis_timestamp": "2026-01-07T00:21:32.374951"
}
{
  "prior_works": [
    {
      "title": "Embedding Watermarks into Deep Neural Networks",
      "authors": "Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, Shin'ichi Satoh",
      "year": 2017,
      "role": "Foundational model watermarking (white-box)",
      "relationship_sentence": "Established parameter-embedded watermarks for DNN IP protection, whose need for weight access motivates ErrorTrace\u2019s black-box, parameter-agnostic traceability."
    },
    {
      "title": "Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring",
      "authors": "Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, Joseph Keshet",
      "year": 2018,
      "role": "Black-box trigger-based watermarking baseline",
      "relationship_sentence": "Demonstrated remote verification via backdoor triggers but also highlighted brittleness to fine-tuning and pruning, directly motivating ErrorTrace\u2019s trigger-free, robust behavioral signature."
    },
    {
      "title": "Adversarial Frontier Stitching for Remote Neural Network Watermarking",
      "authors": "Erwan Le Merrer, Patrick P\u00e9rez, Gilles Tr\u00e9dan",
      "year": 2020,
      "role": "Remote black-box watermarking using crafted queries",
      "relationship_sentence": "Showed black-box ownership checks via tailored adversarial probes, which ErrorTrace supersedes by leveraging intrinsic error-space patterns rather than special query responses."
    },
    {
      "title": "A Watermark for Large Language Models",
      "authors": "Johannes Kirchenbauer, Saeed Mahloujifar, Mohammad Mahmoody, Ian Miers, Florian Tram\u00e8r, etc.",
      "year": 2023,
      "role": "Text-generation watermarking for LLM outputs",
      "relationship_sentence": "Illustrated output-level watermarks that can be weakened by paraphrasing or fine-tuning, reinforcing ErrorTrace\u2019s design to avoid content watermarks and instead use family-specific error profiles."
    },
    {
      "title": "Radioactive Data: Tracing through Training",
      "authors": "Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Herv\u00e9 J\u00e9gou",
      "year": 2020,
      "role": "Black-box traceability via data-side marking",
      "relationship_sentence": "Pioneered black-box provenance without weight access by modifying training data; ErrorTrace adopts the traceability goal but achieves it without data marking by exploiting natural error-space signatures."
    },
    {
      "title": "Transferability in Machine Learning: from Phenomena to Black-Box Attacks",
      "authors": "Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, et al.",
      "year": 2016,
      "role": "Behavioral commonalities and error-boundary insights",
      "relationship_sentence": "Revealed structured relationships in errors and decision boundaries across models, underpinning ErrorTrace\u2019s premise that families exhibit characteristic error patterns usable for attribution."
    },
    {
      "title": "Dataset Cartography: Mapping and Diagnosing Datasets by Training Dynamics",
      "authors": "Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, Yejin Choi",
      "year": 2020,
      "role": "Methodological inspiration for error-space mapping",
      "relationship_sentence": "Introduced mapping examples via error/confidence dynamics, inspiring ErrorTrace\u2019s construction and analysis of a model-family error space as a compact behavioral representation."
    }
  ],
  "synthesis_narrative": "ErrorTrace advances LLM IP protection by shifting from explicit watermarks and triggers to intrinsic, black-box behavioral fingerprints captured in a model-family error space. Early watermarking, such as Uchida et al., embedded ownership information in model parameters, while Adi et al. and Le Merrer et al. enabled remote checks using backdoor or adversarial triggers. These approaches, however, are often fragile under fine-tuning, pruning, or model editing and typically rely on special query-response patterns or weight access. In contrast, Kirchenbauer et al.\u2019s text watermarks target generated content but can degrade under paraphrasing and adaptation, highlighting the need for model-centric, content-agnostic attribution. Sablayrolles et al.\u2019s radioactive data demonstrated that black-box traceability is possible without parameter access by altering the training data; ErrorTrace preserves the black-box assurance but removes the requirement to control training by leveraging behavior that naturally arises from model families.\nCrucially, ErrorTrace is grounded in insights from transferability (Papernot et al.) showing structured, family-related similarities and differences in decision-boundary errors, suggesting that families leave distinctive \u201cerror fingerprints.\u201d Methodologically, Dataset Cartography (Swayamdipta et al.) informs the idea of representing models via the geometry of their error and confidence dynamics, enabling a robust embedding of model behavior. By mapping and classifying models in this error space, ErrorTrace achieves robust attribution across base, fine-tuned, pruned, and merged LLMs without relying on parameters or bespoke triggers, directly addressing the key weaknesses of prior watermarking-based IP protection.",
  "analysis_timestamp": "2026-01-06T23:42:48.112579"
}
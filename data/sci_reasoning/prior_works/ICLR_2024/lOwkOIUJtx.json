{
  "prior_works": [
    {
      "title": "Recurrent Models of Visual Attention",
      "authors": "Volodymyr Mnih et al.",
      "year": 2014,
      "arxiv_id": "arXiv:1406.6247",
      "role": "Baseline",
      "relationship_sentence": "This work established the RL-trained saccadic glimpse policy and retina-like multi-resolution crops that the current paper directly adopts and extends to bandwidth-limited, foveated sampling with full-scene accumulation."
    },
    {
      "title": "Spatial Transformer Networks",
      "authors": "Max Jaderberg et al.",
      "year": 2015,
      "arxiv_id": "arXiv:1506.02025",
      "role": "Extension",
      "relationship_sentence": "The differentiable sampling/cropping mechanism from STNs underpins the paper\u2019s retina-like foveated sampler, which is adapted to extract multi-scale foveal/peripheral views at learned fixation locations."
    },
    {
      "title": "DRAW: A Recurrent Neural Network For Image Generation",
      "authors": "Karol Gregor et al.",
      "year": 2015,
      "arxiv_id": "arXiv:1502.04623",
      "role": "Inspiration",
      "relationship_sentence": "DRAW\u2019s recurrent read\u2013write attention and canvas accumulation directly inspire the paper\u2019s continuous scene reconstruction module that incrementally integrates successive foveal glimpses."
    },
    {
      "title": "Context Encoders: Feature Learning by Inpainting",
      "authors": "Deepak Pathak et al.",
      "year": 2016,
      "arxiv_id": "arXiv:1604.07379",
      "role": "Related Problem",
      "relationship_sentence": "This paper\u2019s demonstration that missing regions can be \u2018filled in\u2019 from context motivates the paper\u2019s use of reconstruction losses to hallucinate peripheral content between saccades."
    },
    {
      "title": "Neural Scene Representation and Rendering (GQN)",
      "authors": "S. M. Ali Eslami et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "GQN\u2019s key idea of building a latent scene representation from multiple partial observations informs the paper\u2019s approach to stitching multi-glance foveal samples into a coherent global scene."
    },
    {
      "title": "Active Object Localization with Deep Reinforcement Learning",
      "authors": "Juan C. Caicedo et al.",
      "year": 2015,
      "arxiv_id": "arXiv:1511.06464",
      "role": "Related Problem",
      "relationship_sentence": "By showing that RL can learn sequential, information-seeking actions for object search, this work motivates the paper\u2019s learned saccade policy to navigate to informative regions under strict pixel budgets."
    }
  ],
  "synthesis_narrative": "Recurrent Models of Visual Attention introduced the notion of learning fixation policies with REINFORCE over discrete glimpse locations, pairing a retina-like, multi-resolution crop with a recurrent state that aggregates information over time. Spatial Transformer Networks provided the differentiable sampling operator that enables learnable cropping and warping, a building block for implementing retina-inspired foveated readouts. DRAW advanced sequential attention by coupling recurrent \u201creads\u201d with a write-on-canvas mechanism, showing how a scene can be progressively reconstructed from partial observations. Context Encoders established that plausible image content can be inferred in missing regions from surrounding context via learned reconstruction objectives. Neural Scene Representation and Rendering (GQN) demonstrated how multiple partial views can be fused into a latent representation capable of rendering unobserved aspects of a scene, formalizing multi-observation scene integration. Active Object Localization with Deep RL confirmed that reinforcement learning can drive sequential, information-seeking visual actions for efficient object search in large images.\n\nTogether, these works exposed an opportunity: combine RL-driven saccadic selection (RAM, active localization) with differentiable foveated sensing (STN) and a recurrent canvas capable of contextual \u201cfill-in\u201d (DRAW, Context Encoders), all organized around a multi-observation scene representation (GQN). The current paper realizes this synthesis by learning where to fixate under tight pixel budgets, reading retina-like glimpses, and continuously reconstructing a global scene state that preserves task performance while drastically reducing sensing and compute.",
  "target_paper": {
    "title": "Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling",
    "authors": "Jiayang Liu, Yiming Bu, Daniel Tso, Qinru Qiu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Biological inspired high performance energy efficient vision system, data efficient training, energy saving sensoring, learned saccade, reinforcement learning, foveated visual sampling, continuous scene reconstruction.",
    "abstract": "High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\\% per frame while maintaining the same level of performance in i",
    "openreview_id": "lOwkOIUJtx",
    "forum_id": "lOwkOIUJtx"
  },
  "analysis_timestamp": "2026-01-06T06:55:15.537973"
}
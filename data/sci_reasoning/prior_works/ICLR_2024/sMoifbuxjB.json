{
  "prior_works": [
    {
      "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries",
      "authors": "Ainsworth et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "By showing that neuron alignment via an assignment problem enables effective weight-space fusion, this work provides the matching machinery that the paper generalizes to many-to-one couplings for intra-layer neuron fusion during pruning."
    },
    {
      "title": "Probabilistic Federated Neural Matching",
      "authors": "Yurochkin et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "This paper frames model aggregation as neuron matching with transport-style couplings, directly inspiring the use of transport plans to merge neurons rather than delete them in the pruning setting."
    },
    {
      "title": "Federated Learning with Matched Averaging (FedMA)",
      "authors": "Wang et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "FedMA\u2019s layer-wise neuron matching and averaging across models informs the paper\u2019s strategy of matching low-importance units to survivors and fusing their weights instead of discarding them."
    },
    {
      "title": "SNIP: Single-shot Network Pruning Based on Connection Sensitivity",
      "authors": "Lee et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "SNIP highlights that one-shot structural pruning based on importance scores incurs accuracy drops without fine-tuning, a limitation the paper addresses by using fusion to recover accuracy without costly retraining."
    },
    {
      "title": "Learning Efficient Convolutional Networks through Network Slimming",
      "authors": "Liu et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This influential channel-pruning method uses BN-scale magnitudes as importance scores, which the paper treats as agnostic inputs and then replaces hard removal with optimal-transport-based fusion into retained channels."
    },
    {
      "title": "Computational Optimal Transport",
      "authors": "Peyr\u00e9 and Cuturi",
      "year": 2019,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Provides the optimal transport formulations and regularized solvers that underpin the paper\u2019s construction of many-to-one neuron transport plans for fusion-based pruning."
    },
    {
      "title": "Model Soup: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy Without Increasing Inference Time",
      "authors": "Wortsman et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By demonstrating that weight-space averaging can recover or improve accuracy, this work motivates using fusion/averaging as a post-pruning recovery mechanism rather than heavy fine-tuning."
    }
  ],
  "synthesis_narrative": "Permutation symmetries in neural networks make naive weight averaging ineffective, and Git Re-Basin showed that solving a neuron alignment problem yields well-behaved weight-space fusion through assignment-based matching. In federated settings, Probabilistic Federated Neural Matching cast aggregation as constructing transport-like couplings between client neurons, while FedMA operationalized layer-wise neuron matching and averaging to form a coherent global model. Model Soup further established that weight-space averaging can recover or improve accuracy without changing inference cost when models are suitably aligned. In contrast, mainstream structural pruning approaches like Network Slimming rely on importance scores (e.g., BN scales) to delete channels outright, and SNIP\u2019s single-shot pruning with sensitivity scores revealed that such deletion typically causes accuracy drops unless followed by expensive fine-tuning. The mathematical toolkit for these matching and fusion procedures is grounded in Computational Optimal Transport, which provides objectives and solvers to compute transport plans and their regularized variants. Bringing these threads together reveals a clear opportunity: instead of deleting low-importance neurons and relying on heavy fine-tuning to repair damage, match them to surviving neurons and fuse their parameters to preserve function. The paper synthesizes alignment from model-fusion work with importance scores from pruning, formulating pruning as many-to-one optimal transport that maps pruned units onto keepers. This OT-driven fusion recovers accuracy in a single shot and can be integrated into the pruning process during training to reduce training time while retaining competitive performance.",
  "target_paper": {
    "title": "Towards Meta-Pruning via Optimal Transport",
    "authors": "Alexander Theus, Olin Geimer, Friedrich Wicke, Thomas Hofmann, Sotiris Anagnostidis, Sidak Pal Singh",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Pruning, Fusion",
    "abstract": "Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm.\nUnlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure.\nThrough utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation.\nNotably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.\nAdditionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We be",
    "openreview_id": "sMoifbuxjB",
    "forum_id": "sMoifbuxjB"
  },
  "analysis_timestamp": "2026-01-06T12:29:00.694945"
}
{
  "prior_works": [
    {
      "title": "Bayesian Learning via Stochastic Gradient Langevin Dynamics",
      "authors": "Max Welling, Yee Whye Teh",
      "year": 2011,
      "role": "Foundational algorithm (SGLD) for large-scale, gradient-based sampling",
      "relationship_sentence": "Introduced the core discrete-time Langevin paradigm the paper generalizes beyond, motivating a framework that gives last-iterate guarantees for a broad family of Langevin-based samplers."
    },
    {
      "title": "Nonasymptotic convergence of Langevin Monte Carlo algorithms",
      "authors": "Alain Durmus, Eric Moulines",
      "year": 2017,
      "role": "Discrete-time Wasserstein convergence theory for basic LMC/ULA",
      "relationship_sentence": "Provided nonasymptotic Wasserstein convergence bounds for unadjusted Langevin schemes, a benchmark the paper extends by reducing last-iterate analysis of many advanced samplers to their continuous-time counterparts."
    },
    {
      "title": "Reflection couplings and contraction rates for diffusions",
      "authors": "Andreas Eberle",
      "year": 2016,
      "role": "Continuous-time contraction (Wasserstein) for Langevin SDEs under nonconvexity",
      "relationship_sentence": "Supplies the continuous-time Wasserstein contractivity results that the paper leverages via its reduction: last-iterate convergence of discrete samplers follows from contraction of the associated diffusion."
    },
    {
      "title": "Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis",
      "authors": "Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky",
      "year": 2017,
      "role": "Nonconvex SGLD analysis via dissipativity and Lyapunov tools",
      "relationship_sentence": "Established standard nonconvex assumptions (e.g., dissipativity) and Lyapunov frameworks the paper inherits, enabling its last-iterate guarantees beyond log-concave settings by tying to well-understood diffusions."
    },
    {
      "title": "Proximal Markov chain Monte Carlo algorithms",
      "authors": "Charles-Emmanuel Pereyra",
      "year": 2016,
      "role": "Proximal/Moreau\u2013Yosida-based Langevin samplers",
      "relationship_sentence": "Developed proximal Langevin-type methods that the paper explicitly covers; the reduction to continuous-time dynamics yields last-iterate Wasserstein convergence for these proximal schemes."
    },
    {
      "title": "Riemann manifold Langevin and Hamiltonian Monte Carlo methods",
      "authors": "Mark Girolami, Ben Calderhead",
      "year": 2011,
      "role": "Geometry-aware Langevin dynamics (precursor to mirror-type samplers)",
      "relationship_sentence": "Introduced geometry-adapted Langevin dynamics underpinning mirror/preconditioned samplers; the paper\u2019s framework extends last-iterate guarantees to such geometry-aware methods via their continuous-time analogues."
    },
    {
      "title": "Stochastic Approximation: A Dynamical Systems Viewpoint",
      "authors": "Vivek S. Borkar",
      "year": 2008,
      "role": "Dynamical-systems/ODE method for discrete-to-continuous analysis",
      "relationship_sentence": "Provides the conceptual toolkit to relate discrete stochastic recursions to limiting continuous-time dynamics, directly inspiring the paper\u2019s reduction of last-iterate convergence to properties of the associated diffusion."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation is a general reduction: last-iterate Wasserstein convergence for a broad class of discrete-time Langevin-based samplers can be inferred from the better-understood continuous-time dynamics. Foundationally, Welling and Teh (2011) launched the stochastic-gradient Langevin paradigm the authors aim to go beyond. Nonasymptotic, last-iterate Wasserstein guarantees for basic unadjusted Langevin algorithms were developed by Durmus and Moulines (2017), setting the discrete-time benchmark the present work seeks to generalize to more advanced samplers and nonconvex regimes. The reduction itself leans on dynamical-systems and stochastic-approximation ideas in the spirit of Borkar (2008), which justify approximating discrete recursions by their limiting diffusions. Crucially, Eberle\u2019s reflection coupling (2016) provides continuous-time Wasserstein contraction for overdamped Langevin even under certain nonconvexities; by transferring these contraction properties through the reduction, the authors obtain last-iterate convergence for many samplers. The nonconvex setting and standard MCMC assumptions (e.g., dissipativity and Lyapunov drift) are aligned with the framework of Raginsky, Rakhlin, and Telgarsky (2017), ensuring that the continuous-time targets possess the requisite stability. The breadth of covered algorithms includes proximal and geometry-aware methods: Pereyra (2016) introduced proximal/MYULA-type Langevin schemes, and Girolami and Calderhead (2011) established geometry-adapted (Riemann-manifold) Langevin dynamics underlying mirror/preconditioned variants. Together, these works provide the algorithmic targets and analytical tools that directly enable the paper\u2019s last-iterate Wasserstein guarantees via a discrete-to-continuous reduction.",
  "analysis_timestamp": "2026-01-06T23:42:49.101522"
}
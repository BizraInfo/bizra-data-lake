{
  "prior_works": [
    {
      "title": "Audio to Body Dynamics",
      "authors": "Eli Shlizerman et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing that musical audio can drive performer body motion (including instrument playing) via learned audio-to-motion mappings, this work directly motivates the paper\u2019s audio-conditioned formulation for generating piano hand movements."
    },
    {
      "title": "Learning Individual Styles of Conversational Gesture",
      "authors": "Shiry Ginosar et al.",
      "year": 2019,
      "arxiv_id": "1906.04160",
      "role": "Inspiration",
      "relationship_sentence": "This paper established the now-standard audio-to-pose generation paradigm that the baseline extends to fine-grained pianist hand keypoints via a two-stage position-guided pipeline."
    },
    {
      "title": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++",
      "authors": "Ruilong Li et al.",
      "year": 2021,
      "arxiv_id": "2008.08171",
      "role": "Related Problem",
      "relationship_sentence": "By coupling a large, curated music-conditioned motion dataset with hierarchical conditioning on musical structure, it provided a template for building domain-specific benchmarks and inspired the position-first, detail-later strategy for music-driven motion."
    },
    {
      "title": "Human Motion Diffusion Model",
      "authors": "Guy Tevet et al.",
      "year": 2022,
      "arxiv_id": "2209.14916",
      "role": "Inspiration",
      "relationship_sentence": "This work demonstrated effective trajectory/position conditioning to control motion synthesis, directly informing the paper\u2019s design of a position predictor that guides a subsequent gesture generator."
    },
    {
      "title": "The GENEA Challenge 2022: A Large-Scale, Comparative Evaluation of Speech-Driven Gesture Generation",
      "authors": "Taras Kucherenko et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Its standardized objective metrics and protocols (e.g., kinematic smoothness, distributional realism) underpin the benchmark\u2019s motion similarity and smoothness evaluation for generated hand motions."
    },
    {
      "title": "MAESTRO: A Dataset for Model-Based Music Generation and Transcription",
      "authors": "Curtis Hawthorne et al.",
      "year": 2019,
      "arxiv_id": "1810.12247",
      "role": "Gap Identification",
      "relationship_sentence": "While establishing large-scale, aligned piano performance data for notes and timing, it lacks visual hand motion and fingering, highlighting the specific data gap this paper fills."
    },
    {
      "title": "Automatic Piano Fingering from Music Score via Conditional Random Fields",
      "authors": "Eita Nakamura et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work formalized piano fingering as structured prediction from musical content, which the benchmark extends by supervising and evaluating finger assignments together with continuous hand trajectories."
    }
  ],
  "synthesis_narrative": "Audio-driven motion generation matured with the demonstration that musical audio signals can predict performers\u2019 body dynamics, revealing a direct path from sound to movement without explicit symbolic control. Subsequent advances in audio-to-pose synthesis framed the problem as generating pose sequences conditioned on acoustic features, introducing architectures and training setups that reliably map continuous audio to temporally coherent motion. In music-conditioned human motion, large-scale domain-specific datasets paired with hierarchical conditioning on musical structure showed that benchmarked corpora catalyze progress and that separating coarse movement planning from fine articulation improves fidelity. More recently, trajectory- and position-conditioned motion synthesis proved that providing explicit spatial guidance dramatically increases controllability and realism, highlighting a practical two-stage recipe: plan key spatial anchors first, then synthesize detailed kinematics. In parallel, community challenges for gesture generation established robust, quantitative metrics\u2014covering motion similarity, smoothness, and distributional realism\u2014that enable fair comparisons across systems. In piano specifically, large audio\u2013MIDI corpora standardized timing and content but offered no visual hand data, while fingering research formalized finger assignment as a structured task, yet did not capture continuous hand trajectories. Together, these works expose a clear opportunity: build a piano-specific, large-scale, video-grounded benchmark with explicit hand pose and fingering, and pair it with a controllable audio-to-motion baseline. The paper naturally synthesizes trajectory-guided generation with audio conditioning and adopts proven evaluation protocols to assess motion quality, smoothness, and positional accuracy of both hands.",
  "target_paper": {
    "title": "PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance",
    "authors": "Qijun Gan, Song Wang, Shengtao Wu, Jianke Zhu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Hand pose estimation, piano music, motion generation",
    "abstract": "Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. In this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. To this end, we collect an annotated dataset, PianoMotion10M, consisting of 116 hours of piano playing videos from a bird's-eye view with 10 million annotated hand poses. We also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. Furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smoothness, positional accuracy of left and ",
    "openreview_id": "rxVvRBgqmS",
    "forum_id": "rxVvRBgqmS"
  },
  "analysis_timestamp": "2026-01-06T16:51:28.667456"
}
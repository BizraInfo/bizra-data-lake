{
  "prior_works": [
    {
      "title": "Multiaccuracy: Black-Box Post-Processing to Improve Fairness in AI",
      "authors": "Michael P. Kim et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "The paper adapts Multiaccuracy\u2019s core auditing idea\u2014training a single auxiliary model to predict residual errors\u2014to a new \"fair use\" test that checks whether using a group attribute yields nonnegative group-level performance gains."
    },
    {
      "title": "Multicalibration: Calibration for the (Computationally Identifiable) Multitudes",
      "authors": "Aviad Hebert-Johnson et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "The notion of certifying performance guarantees across many identifiable subpopulations directly inspires the paper\u2019s requirement that the use of a group attribute be justified by demonstrable, group-level benefit."
    },
    {
      "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness",
      "authors": "Michael Kearns et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "This work established the audit-as-learning paradigm for subgroup guarantees, which the paper leverages to formalize and operationalize its subgroup-level \"fair use\" conditions."
    },
    {
      "title": "Equality of Opportunity in Supervised Learning",
      "authors": "Moritz Hardt et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "Per-group thresholding uses sensitive attributes at prediction time\u2014an archetypal form of group-based personalization that the paper scrutinizes and constrains via its fair-use conditions."
    },
    {
      "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Group Accuracy",
      "authors": "Shiori Sagawa et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "By showing standard ERM can hurt worst-group performance under group shifts, this work highlights a key failure mode the paper explains and guards against when group attributes are used for personalization."
    },
    {
      "title": "Hidden in Plain Sight \u2014 Reconsidering the Use of Race Correction in Clinical Algorithms",
      "authors": "Darshali A. Vyas et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "This critique of race-based personalization in clinical tools motivates the need for principled criteria; the paper answers with formal fair-use conditions and a practical test for when group attributes should be used."
    },
    {
      "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
      "authors": "Ziad Obermeyer et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Evidence that a widely used clinical risk algorithm disadvantaged Black patients provides concrete impetus for the paper\u2019s central claim that using group attributes can systematically harm groups absent fair-use checks."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014formal fair-use conditions for group attributes and a simple test that requires training one additional model\u2014draws directly from the auditing paradigm developed in subgroup-fairness research. Multicalibration and Fairness Gerrymandering established that one can certify guarantees across many subpopulations by training auxiliary predictors to uncover systematic residual structure; Multiaccuracy operationalized this with a black-box auditor trained on residuals. The present work extends this idea by reframing the auditing target: rather than seeking calibration or parity, it audits whether using a group attribute yields a Pareto improvement at the group level, thereby defining when personalization is justified. \nConcurrently, the paper interrogates a widely used baseline\u2014group-aware prediction at decision time, exemplified by per-group thresholding from Equality of Opportunity\u2014and shows that such personalization can fail to benefit each group. Insights from Group DRO on worst-group degradation under standard ERM connect to the mechanisms the paper formalizes, explaining how common development practices (e.g., data imbalance, overfitting, and model mis-specification) can induce fair-use violations. Finally, critiques of race-based personalization in clinical algorithms (Vyas et al.; Obermeyer et al.) supply the domain motivation: sensitive attributes are often used with the intent to personalize, yet can harm the very groups they aim to help. Synthesizing these threads, the paper contributes a precise, auditable criterion for when group attributes should be used in prediction.",
  "analysis_timestamp": "2026-01-06T23:09:26.551460"
}
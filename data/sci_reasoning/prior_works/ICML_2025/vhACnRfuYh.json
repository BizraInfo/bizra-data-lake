{
  "prior_works": [
    {
      "title": "Diffuser: Diffusion Models for Planning",
      "authors": "Michael Janner et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "LDP adopts Diffuser\u2019s core idea of using denoising diffusion to generate full trajectories for planning and extends it by operating in a learned latent state space and decoupling action prediction from planning."
    },
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": "Robin Rombach et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "LDP directly borrows the latent-diffusion paradigm\u2014first learning a VAE and then running diffusion in the compact latent\u2014to make trajectory generation from high-dimensional images efficient and stable."
    },
    {
      "title": "Behavioral Cloning from Observation",
      "authors": "Arash Torabi et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "LDP\u2019s design of a separate inverse dynamics module is grounded in BCO\u2019s insight that state-only demonstrations can be leveraged by learning inverse dynamics, enabling action-free learning."
    },
    {
      "title": "PlaNet: Learning Latent Dynamics for Planning from Pixels",
      "authors": "Danijar Hafner et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "LDP follows PlaNet\u2019s principle of planning in a learned latent state space from pixels, but replaces explicit dynamics+CEM with a diffusion-based planner and a learned inverse-dynamics stage."
    },
    {
      "title": "Learning Latent Plans from Play",
      "authors": "Corey Lynch et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "LDP\u2019s ability to leverage broad, suboptimal interaction data for control echoes Play-LMP\u2019s finding that diverse play can train control primitives; LDP extends this by restricting suboptimality to inverse dynamics while planning from action-free data."
    },
    {
      "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
      "authors": "Chi et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "LDP targets Diffusion Policy\u2019s limitation of requiring large expert, action-labeled datasets by separating planning (trainable from action-free demos) from action prediction, yielding stronger performance on visual manipulation."
    }
  ],
  "synthesis_narrative": "Latent Diffusion Planning (LDP) sits at the intersection of three lines of work that directly enabled its core innovation: diffusion-based planning, latent-space planning from pixels, and learning from action-free or suboptimal data. Diffuser established that denoising diffusion can synthesize expert-like trajectories for decision making; LDP builds on this by moving planning into a compact latent space and by decoupling plan generation from action prediction. The latent-space move is inspired by Latent Diffusion Models, which showed that VAE-backed latent diffusion delivers efficient and scalable generation from high-dimensional inputs\u2014critical for image-based robotics. On the planning side, PlaNet provided the blueprint for learning from pixels by planning in a learned latent state space; LDP follows this decomposition but substitutes classical dynamics+CEM with a diffusion trajectory generator and executes plans via a learned inverse-dynamics head. The inverse-dynamics component and the use of action-free demonstrations are grounded in Behavioral Cloning from Observation, which formalized how to leverage state-only demos via inverse dynamics. LDP refines this by training the planner purely from action-free data while using abundant, possibly suboptimal interaction data to learn inverse dynamics. Finally, Diffusion Policy serves as the primary baseline whose limitations\u2014dependence on large action-labeled expert datasets and entangling action generation with planning\u2014are explicitly addressed. Together with Play-LMP\u2019s insight that diverse, suboptimal play data suffices for control learning, these works directly shaped LDP\u2019s modular latent-diffusion planning framework.",
  "analysis_timestamp": "2026-01-06T23:07:19.563902"
}
{
  "prior_works": [
    {
      "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
      "authors": "Alex Kendall and Yarin Gal",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This paper\u2019s aleatoric\u2013epistemic decomposition directly motivates the paper\u2019s formal notion of regression unreliability (intrinsic variability vs. model error) and shapes the problem formulation the new score is designed to capture."
    },
    {
      "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "authors": "Balaji Lakshminarayanan et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "Deep ensembles\u2019 predictive variance is a primary baseline for error detection in regression; the new discrepancy-density score is proposed to supersede variance-based proxies and empirically outperforms ensembles on detecting errors beyond a threshold."
    },
    {
      "title": "Accurate Uncertainties for Deep Learning Using Calibrated Regression",
      "authors": "Alex Kuleshov et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "While this work calibrates regression uncertainties, its guarantees are marginal and not tailored to detecting when the loss exceeds a specified threshold; the paper explicitly addresses this gap by modeling the conditional discrepancy density and deriving a targeted unreliability score."
    },
    {
      "title": "Conformalized Quantile Regression",
      "authors": "Yaniv Romano et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "CQR provides prediction intervals whose width is widely used as a reliability proxy; the paper replaces this indirect proxy with a discrepancy-density\u2013based score that directly targets error exceedance events and shows improved detection over CQR-width baselines."
    },
    {
      "title": "Conformal Risk Control",
      "authors": "Anastasios N. Angelopoulos et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "CRC formalizes controlling the probability that a loss exceeds a user-specified threshold via calibrated scores; the paper extends this paradigm by introducing a learned score derived from the conditional discrepancy density and a new statistical dissimilarity, yielding stronger error detection."
    },
    {
      "title": "Deep Evidential Regression",
      "authors": "Alexander Amini et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Evidential regression yields decomposed (aleatoric/epistemic) uncertainty used as a regression error-detector; the proposed method is designed to surpass such distribution-parameter\u2013based scores by exploiting the full discrepancy density and its diversity."
    },
    {
      "title": "Strictly Proper Scoring Rules, Prediction, and Estimation",
      "authors": "Tilmann Gneiting and Adrian E. Raftery",
      "year": 2007,
      "role": "Inspiration",
      "relationship_sentence": "The theory of proper scoring rules and divergence-based comparison of predictive distributions directly inspires the paper\u2019s new statistical dissimilarity metric for quantifying the diversity of estimated discrepancy densities."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014learning a discrepancy-density\u2013based score and quantifying its diversity via a new statistical dissimilarity\u2014emerges from a confluence of uncertainty quantification, selective reliability, and conformal risk control. Kendall and Gal\u2019s foundational distinction between aleatoric and epistemic uncertainty anchors the paper\u2019s precise definition of regression unreliability as loss exceedance driven by intrinsic variability or model error. Standard UQ mechanisms such as Deep Ensembles and Deep Evidential Regression provide strong baselines but rely largely on variance or parameterized distribution surrogates; these proxies often misalign with the concrete event of exceeding a user-defined error threshold. Calibrated Regression and Conformalized Quantile Regression improve marginal calibration and coverage, and interval width is commonly used as a reliability proxy, yet these methods still do not directly target instance-wise exceedance detection. Conformal Risk Control reframed the problem by aiming to control loss-threshold risk via calibrated scores, but its effectiveness hinges on the quality of the underlying score. The present work extends this paradigm by estimating the conditional distribution of the discrepancy itself and introducing a principled dissimilarity measure\u2014drawing on proper scoring rule theory\u2014to summarize its statistical diversity into a powerful, data-driven unreliability score. This lineage explains both the methodological choice (model the loss distribution) and the evaluation focus (error-threshold detection), culminating in consistent empirical gains over ensembles, evidential approaches, and conformal interval\u2013based baselines.",
  "analysis_timestamp": "2026-01-06T23:09:26.405939"
}
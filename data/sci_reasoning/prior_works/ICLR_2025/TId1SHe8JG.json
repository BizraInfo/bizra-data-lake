{
  "prior_works": [
    {
      "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
      "authors": "Alex Kendall et al.",
      "year": 2017,
      "arxiv_id": "1703.04977",
      "role": "Foundation",
      "relationship_sentence": "This work introduces the aleatoric vs. epistemic decomposition that is taken as the target semantic split here, while its Bayesian heuristics lacked the distribution-free guarantees this paper provides."
    },
    {
      "title": "On Calibration of Modern Neural Networks",
      "authors": "Chuan Guo et al.",
      "year": 2017,
      "arxiv_id": "1706.04599",
      "role": "Foundation",
      "relationship_sentence": "The definition and measurement of calibration from this paper are generalized here from first-order probability vectors to higher-order predictors over mixtures of label distributions, preserving the average-correctness semantics."
    },
    {
      "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "authors": "Balaji Lakshminarayanan et al.",
      "year": 2017,
      "arxiv_id": "1612.01474",
      "role": "Baseline",
      "relationship_sentence": "Deep ensembles constitute the primary practical baseline for decomposing predictive uncertainty into aleatoric and epistemic parts, which this work replaces with a provably calibrated higher-order estimator using k-snapshots."
    },
    {
      "title": "Predictive Uncertainty Estimation via Prior Networks",
      "authors": "Andrey Malinin et al.",
      "year": 2018,
      "arxiv_id": "1802.10501",
      "role": "Inspiration",
      "relationship_sentence": "Prior Networks\u2019 idea of predicting a distribution over class-probability vectors directly motivates modeling higher-order uncertainty objects that this paper calibrates and evaluates with formal guarantees."
    },
    {
      "title": "Human Uncertainty Makes Classification More Robust",
      "authors": "Joshua C. Peterson et al.",
      "year": 2019,
      "arxiv_id": "1908.07086",
      "role": "Foundation",
      "relationship_sentence": "CIFAR-10H\u2019s multiple human labels per image instantiate the k-snapshot paradigm used here to both measure and achieve higher-order calibration tied to the true conditional label distribution."
    },
    {
      "title": "Multicalibration: Calibration for the (Computationally-Identifiable) Masses",
      "authors": "Hussein Hebert-Johnson et al.",
      "year": 2018,
      "arxiv_id": "1801.03204",
      "role": "Extension",
      "relationship_sentence": "The audit-and-correct framework and distribution-free guarantees of multicalibration inform the algorithmic and proof approach that this paper extends to calibrate higher-order predictors."
    },
    {
      "title": "Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration",
      "authors": "Meelis Kull et al.",
      "year": 2019,
      "arxiv_id": "1910.12656",
      "role": "Related Problem",
      "relationship_sentence": "Dirichlet calibration shows how to calibrate probability vectors via a simplex-valued mapping, a perspective this work conceptually lifts to calibrate distributions over those vectors using k-snapshots."
    }
  ],
  "synthesis_narrative": "A central thread in uncertainty quantification begins with the distinction between aleatoric and epistemic uncertainty, articulated by Kendall and Gal, who decomposed predictive uncertainty within Bayesian deep learning but left the semantics tied to model assumptions. In parallel, Guo et al. formalized modern calibration for probabilistic classifiers, defining average-correctness properties and practical estimators that shape how calibrated predictions are assessed. Deep ensembles emerged as the leading practical approach to separating epistemic and aleatoric components, yet their variance-based interpretation lacks distribution-grounded guarantees. Malinin and Gales advanced the idea of predicting a distribution over class-probability vectors (e.g., Dirichlet prior networks), elevating the object of prediction beyond a single probability vector and suggesting a natural higher-order target for uncertainty representation. Peterson et al. collected multiple human labels per input (CIFAR-10H), enabling empirical access to p(y|x) and making it possible to assess and learn from k independent label draws per point. From a theory standpoint, multicalibration introduced an audit-and-correct paradigm that yields distribution-free calibration guarantees across rich families of subsets, while Dirichlet calibration showed how to calibrate multiclass probability vectors via structured mappings on the simplex.\nBringing these strands together exposes a gap: existing decompositions and calibrators operate on first-order outputs or heuristic model uncertainties, without guarantees that the estimated aleatoric component matches the real-world conditional uncertainty. With repeated-label datasets providing k-snapshots of p(y|x), and with higher-order predictors that output distributions over label distributions, it is natural to define and achieve a higher-order calibration property. Extending audit-style, distribution-free calibration techniques to this setting yields provable semantics for the aleatoric/epistemic split and a principled path to measure and attain it.",
  "target_paper": {
    "title": "Provable Uncertainty Decomposition via Higher-Order Calibration",
    "authors": "Gustaf Ahdritz, Aravind Gollakota, Parikshit Gopalan, Charlotte Peale, Udi Wieder",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "uncertainty quantification, calibration, trustworthy ML, mixture learning",
    "abstract": "We give a principled method for decomposing the predictive uncertainty of a model into aleatoric and epistemic components with explicit semantics relating them to the real-world data distribution. While many works in the literature have proposed such decompositions, they lack the type of formal guarantees we provide. Our method is based on the new notion of higher-order calibration, which generalizes ordinary calibration to the setting of higher-order predictors that predict _mixtures_ over label distributions at every point. We show how to measure as well as achieve higher-order calibration using access to $k$-snapshots, namely examples where each point has $k$ independent conditional labels. Under higher-order calibration, the estimated aleatoric uncertainty at a point is guaranteed to match the real-world aleatoric uncertainty averaged over all points where the prediction is made. To our knowledge, this is the first formal guarantee of this type that places no assumptions whatsoever",
    "openreview_id": "TId1SHe8JG",
    "forum_id": "TId1SHe8JG"
  },
  "analysis_timestamp": "2026-01-06T07:31:10.985733"
}
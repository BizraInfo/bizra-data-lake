{
  "prior_works": [
    {
      "title": "Human memory: A proposed system and its control processes",
      "authors": "Atkinson and Shiffrin",
      "year": 1968,
      "role": "Foundation",
      "relationship_sentence": "Memoria\u2019s multi-store design\u2014with separate fast/short-term and slow/long-term stores and control processes like rehearsal/consolidation\u2014is a direct computational instantiation of the Atkinson\u2013Shiffrin model."
    },
    {
      "title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models",
      "authors": "McClelland et al.",
      "year": 1995,
      "role": "Inspiration",
      "relationship_sentence": "Memoria\u2019s consolidation mechanism and division between rapid, episodic-like memory and gradual, stable storage are explicitly motivated by the Complementary Learning Systems theory to resolve stability\u2013plasticity (fateful forgetting)."
    },
    {
      "title": "A distributed representation of temporal context",
      "authors": "Howard and Kahana",
      "year": 2002,
      "role": "Foundation",
      "relationship_sentence": "Memoria\u2019s context-based retrieval that reproduces temporal contiguity and serial position effects draws directly on the Temporal Context Model\u2019s notion of drifting context and context reinstatement."
    },
    {
      "title": "Neural Turing Machines",
      "authors": "Graves et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "Memoria builds on NTMs\u2019 core idea of differentiable external memory with learned read/write addressing, but replaces their purely short-term retention policy with human-inspired consolidation and decay."
    },
    {
      "title": "Hybrid computing using a neural network with dynamic external memory",
      "authors": "Graves et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "Differentiable Neural Computers are a primary baseline for algorithmic tasks (e.g., sorting), and Memoria directly improves on DNC by preventing long-horizon information loss through selective consolidation and rehearsal."
    },
    {
      "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
      "authors": "Dai et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Memoria targets Transformer-XL\u2019s recency-biased segment-level recurrence by introducing a long-term store that retains older but important information rather than letting it fade with rolling caches."
    },
    {
      "title": "Memorizing Transformers",
      "authors": "Wu et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "Memoria extends the notion of persistent key\u2013value memories by adding human-inspired consolidation, decay, and context-driven retrieval that produce primacy/recency and temporal contiguity effects absent in Memorizing Transformers."
    }
  ],
  "synthesis_narrative": "Memoria\u2019s core innovation\u2014combining an external memory with human-like consolidation and retrieval dynamics to prevent long-term loss\u2014sits at the intersection of classic cognitive theory and modern memory-augmented neural networks. The multi-store structure and explicit control processes closely follow Atkinson\u2013Shiffrin, while the consolidation pipeline and separation of rapid versus stable memory are directly inspired by the Complementary Learning Systems view of hippocampal\u2013neocortical interplay. To reproduce human serial position and temporal contiguity effects, Memoria imports the Temporal Context Model\u2019s drifting context and reinstatement mechanisms, making these psychological regularities emergent properties of its retrieval policy.\n\nOn the machine learning side, Neural Turing Machines provided the crucial template of differentiable read\u2013write external memory, and Differentiable Neural Computers served as the main algorithmic-task baseline whose short-horizon retention Memoria surpasses via selective consolidation and rehearsal. In language modeling, Transformer-XL\u2019s segment recurrence highlighted a central gap: memory extended in length but still dominated by recency, allowing older information to be fatefully forgotten. Finally, Memorizing Transformers showed that persistent memories can aid long-range recall, but lacked the human-inspired consolidation/decay and context-driven retrieval needed to yield primacy, recency, and temporal contiguity. By integrating these strands, Memoria transforms external memory from a short-term cache into a human-aligned long-term system that resists fateful forgetting.",
  "analysis_timestamp": "2026-01-06T23:09:26.502372"
}
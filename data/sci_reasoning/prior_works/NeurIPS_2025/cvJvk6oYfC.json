{
  "prior_works": [
    {
      "title": "k-Sparse Autoencoders",
      "authors": [
        "Alireza Makhzani",
        "Brendan J. Frey"
      ],
      "year": 2013,
      "role": "Sparsity-controlled representation learning in autoencoders via winner-take-all activations",
      "relationship_sentence": "By making sparsity an explicit, controllable property of encoder activations, this work underpins SparseMVC\u2019s idea of probing per-view sparsity and motivates its entropy-based mechanism to harmonize code sparsity across views."
    },
    {
      "title": "Deep Variational Information Bottleneck",
      "authors": [
        "Alexander A. Alemi",
        "Ian Fischer",
        "Joshua V. Dillon",
        "Kevin Murphy"
      ],
      "year": 2017,
      "role": "Controlling representation entropy/capacity via an information bottleneck (KL) objective",
      "relationship_sentence": "The principle of shaping the information content of latent codes inspires SparseMVC\u2019s entropy-matching loss, which equalizes latent entropy across views to mitigate sparsity-induced encoding discrepancies."
    },
    {
      "title": "Deep Embedded Clustering (DEC): Learning Feature Representations for Clustering",
      "authors": [
        "Junyuan Xie",
        "Ross Girshick",
        "Ali Farhadi"
      ],
      "year": 2016,
      "role": "KL-based distribution sharpening/alignment for deep clustering",
      "relationship_sentence": "SparseMVC\u2019s cross-view distribution alignment echoes DEC\u2019s strategy of refining soft assignments via KL objectives, extending it to reconcile view-specific and globally fused clustering distributions."
    },
    {
      "title": "Co-regularized Spectral Clustering with Multiple Views",
      "authors": [
        "Abhishek Kumar",
        "Piyush Rai",
        "Hal Daum\u00e9 III"
      ],
      "year": 2011,
      "role": "Cross-view co-regularization to enforce agreement among view-specific clusterings",
      "relationship_sentence": "This foundational co-regularization idea directly informs SparseMVC\u2019s aim of cross-view consistency, which it achieves through distribution alignment that is robust to heterogeneous sparsity across views."
    },
    {
      "title": "Deep Canonical Correlation Analysis",
      "authors": [
        "Galen Andrew",
        "Raman Arora",
        "Jeff Bilmes",
        "Karen Livescu"
      ],
      "year": 2013,
      "role": "Learning representations by maximizing cross-view correlations",
      "relationship_sentence": "DCCA\u2019s focus on inter-view correlation motivates SparseMVC\u2019s correlation-informed sample reweighting, where attention estimates the agreement between global and view-specific features to set per-sample weights."
    },
    {
      "title": "Self-Paced Learning for Latent Variable Models",
      "authors": [
        "M. Pawan Kumar",
        "Benjamin Packer",
        "Daphne Koller"
      ],
      "year": 2010,
      "role": "Sample-level weighting based on model confidence/learned curriculum",
      "relationship_sentence": "The concept of learning sample weights to handle heterogeneity inspires SparseMVC\u2019s sample-level reweighting; it replaces curriculum heuristics with attention derived from cross-view correlations."
    },
    {
      "title": "Attention Is All You Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakub Uszkoreit",
        "Llion Jones",
        "Aidan N. Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": 2017,
      "role": "Self-attention mechanism for data-dependent weighting and correlation modeling",
      "relationship_sentence": "SparseMVC leverages attention to compute correlation-informed weights between global and view-specific features, operationalizing its sample reweighting module."
    }
  ],
  "synthesis_narrative": "SparseMVC\u2019s central insight\u2014that cross-view sparsity variations distort encodings and render coarse view-level weighting unreliable\u2014emerges at the intersection of sparse representation learning, cross-view consistency, and modern attention-based fusion. On the representation side, k-Sparse Autoencoders demonstrated how to explicitly control activation sparsity, while the Deep Variational Information Bottleneck formalized controlling latent entropy/capacity. These ideas directly shape SparseMVC\u2019s adaptive sparse autoencoder and its entropy-matching loss, which probes each view\u2019s sparsity and equalizes latent entropy to harmonize encoding formats. For clustering, DEC introduced KL-based distribution sharpening of soft assignments, and co-regularized multi-view spectral clustering established the principle that view-specific clusterings should agree. SparseMVC synthesizes these by aligning distributions between view-specific and globally fused assignments, but crucially does so after correcting sparsity-induced heterogeneity. To balance contributions at a finer granularity, DCCA\u2019s emphasis on maximizing cross-view correlation motivates estimating agreement between representations, while self-paced learning contributes the paradigm of sample-level weighting to handle heterogeneity. Operationally, attention (per Vaswani et al.) provides the mechanism to compute correlation-informed per-sample weights between global and view-specific features, replacing brittle global view weights with adaptive, data-dependent weighting. Together, these threads yield SparseMVC\u2019s three modules\u2014entropy-matched sparse encoding, correlation-informed sample reweighting via attention, and cross-view distribution alignment\u2014that directly address cross-view sparsity variation in multi-view clustering.",
  "analysis_timestamp": "2026-01-07T00:21:32.292911"
}
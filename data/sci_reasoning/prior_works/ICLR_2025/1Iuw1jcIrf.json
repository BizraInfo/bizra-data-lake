{
  "prior_works": [
    {
      "title": "Minerva: Solving Quantitative Reasoning Problems with Language Models",
      "authors": "Aitor Lewkowycz et al.",
      "year": 2022,
      "role": "Foundational evidence for math-specific continued pretraining on LaTeX-rich corpora",
      "relationship_sentence": "MathCoder2 extends Minerva\u2019s insight that continued pretraining on math-heavy, LaTeX-centric data boosts quantitative reasoning, but goes further by translating math into executable code plus reasoning steps as pretraining targets."
    },
    {
      "title": "Program-Aided Language Models (PAL)",
      "authors": "Luyu Gao, Aman Madaan, et al.",
      "year": 2023,
      "role": "Methodological precedent: using Python programs as intermediate reasoning",
      "relationship_sentence": "PAL demonstrated that delegating computation to code markedly improves problem solving; MathCoder2 internalizes this idea by pretraining on model-translated mathematical code so that the code-style reasoning becomes native to the model."
    },
    {
      "title": "Program of Thoughts Prompting: Disentangle Reasoning from Language Models through Program Execution",
      "authors": "Xinyun Chen et al.",
      "year": 2023,
      "role": "Methodological precedent: code-based intermediate reasoning steps (executable scratchpads)",
      "relationship_sentence": "PoT showed that representing reasoning as executable programs is effective; MathCoder2 operationalizes this at the pretraining stage by generating paired math reasoning steps and executable code derived from LaTeX expressions and conditions."
    },
    {
      "title": "MathCoder: Enhancing Mathematical Reasoning via Code Generation and Execution",
      "authors": "Zimu Lu et al.",
      "year": 2024,
      "role": "Immediate predecessor/baseline for code-centric math reasoning",
      "relationship_sentence": "MathCoder established the viability of using code for math reasoning; MathCoder2 directly builds on it by constructing a higher-quality, math-focused pretraining corpus with model-translated mathematical code and explicit reasoning steps."
    },
    {
      "title": "Galactica: A Large Language Model for Science",
      "authors": "Ross Taylor, Aitor Kardas, et al.",
      "year": 2022,
      "role": "Systems/data precedent: pretraining on science/LaTeX to improve technical reasoning",
      "relationship_sentence": "Galactica evidenced that LaTeX-heavy scientific corpora aid symbolic/technical competence; MathCoder2 narrows this to mathematics and augments it with executable code derived from LaTeX to make the symbolic reasoning verifiable."
    },
    {
      "title": "MetaMath: Bootstrapping High-quality Mathematical Reasoning Data with Verification",
      "authors": "Yu et al.",
      "year": 2023,
      "role": "Data synthesis/verification precedent for math reasoning corpora",
      "relationship_sentence": "MetaMath\u2019s verified synthetic data pipeline motivates MathCoder2\u2019s construction of reliable reasoning steps, here by extracting expressions/conditions from math text and producing consistent, executable code as supervision."
    }
  ],
  "synthesis_narrative": "MathCoder2\u2019s core idea\u2014continued pretraining on model-translated mathematical code paired with explicit reasoning steps\u2014sits at the intersection of two threads: math-specific pretraining and code-as-reasoning. Minerva showed that continued pretraining on LaTeX-rich mathematical text can significantly enhance quantitative reasoning, establishing the value of domain-focused corpora. Galactica further reinforced that scientific/LaTeX content cultivates symbolic competence at scale. In parallel, PAL and Program-of-Thoughts revealed that rendering reasoning as executable Python programs (often leveraging libraries like SymPy) markedly improves solution accuracy by separating high-level planning from precise computation. Building directly on these insights, the original MathCoder demonstrated that code generation and execution can be harnessed for math problem solving. MathCoder2 advances this trajectory by shifting code from an inference-time crutch to a pretraining signal: it systematically extracts LaTeX expressions and their conditions/results from math sources and translates them into executable mathematical code accompanied by reasoning steps. This both biases the model toward code-structured reasoning and enables verification via execution. Complementing this, MetaMath\u2019s verified data synthesis informs MathCoder2\u2019s emphasis on high-quality, reliable supervision. Together, these works motivate MathCoder2\u2019s data pipeline and training recipe: curate math-centric text, transform symbolic content into executable programs with aligned reasoning traces, and continue pretraining so models internalize precise, checkable mathematical reasoning.",
  "analysis_timestamp": "2026-01-06T23:42:48.089356"
}
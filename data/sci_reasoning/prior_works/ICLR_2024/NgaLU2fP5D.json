{
  "prior_works": [
    {
      "title": "Knowledge Tracing: Modeling the Acquisition of Procedural Knowledge",
      "authors": "Albert T. Corbett et al.",
      "year": 1995,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "PSI-KT generalizes BKT\u2019s interpretable latent skill-mastery dynamics by embedding them in a hierarchical generative model that couples mastery with person-specific traits and prerequisite structure."
    },
    {
      "title": "Probabilistic Models for Some Intelligence and Attainment Tests",
      "authors": "Georg Rasch",
      "year": 1960,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "PSI-KT adopts the IRT-style ability\u2013difficulty parameterization to model response likelihoods, providing psychologically interpretable learner and item parameters within its generative framework."
    },
    {
      "title": "DINA model and Q-matrix validation",
      "authors": "Jimmy de la Torre",
      "year": 2009,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "PSI-KT builds on the Q-matrix idea from cognitive diagnosis to tie items to concepts for interpretability, while relaxing DINA\u2019s binary mastery and independence assumptions via continuous traits and explicit prerequisite structure."
    },
    {
      "title": "Graph-based Knowledge Tracing",
      "authors": "Hiroto Nakagawa et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "PSI-KT takes the insight of leveraging an explicit concept prerequisite graph from GKT but replaces GNN black boxes with a transparent probabilistic mechanism that propagates effects along the graph."
    },
    {
      "title": "Deep Knowledge Tracing",
      "authors": "Chris Piech et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "PSI-KT targets DKT\u2019s strong predictive performance yet opaque dynamics by introducing an interpretable generative model that supports multi-step forecasting without sacrificing accuracy."
    },
    {
      "title": "A Trainable Spaced Repetition Model for Language Learning",
      "authors": "Burr Settles et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "PSI-KT incorporates timing- and forgetting-sensitive dynamics inspired by half-life regression, encoding learner-specific retention parameters within a scalable Bayesian hierarchy."
    }
  ],
  "synthesis_narrative": "Bayesian Knowledge Tracing framed student learning as a probabilistic process of latent skill mastery with interpretable transitions, offering psychological clarity but limited structural expressivity. Item Response Theory introduced the ability\u2013difficulty parameterization that grounds responses on a shared latent scale, enabling transparent learner and item factors. Cognitive diagnosis work on the DINA model established the Q-matrix link from items to skills, formalizing concept-level interpretability but assuming binary mastery and largely independent skills. Graph-based Knowledge Tracing showed that injecting an explicit prerequisite graph can improve prediction by propagating concept influences, though it relies on opaque graph neural computations. Deep Knowledge Tracing demonstrated that recurrent deep models can deliver high predictive accuracy, but at the cost of interpretability and principled multi-step forecasting. Finally, half-life regression in spaced repetition highlighted the importance of time and forgetting, capturing retention with simple, learner-specific parameters.\nTogether these ideas revealed a gap: a model that unifies interpretable person\u2013item parameters, explicit concept structure, and time-sensitive learning dynamics, while retaining the predictive strength of modern KT and scaling to many learners. The present work synthesizes IRT-like response modeling with BKT-like latent dynamics, relaxes DINA\u2019s binary mastery using continuous traits, operationalizes prerequisite graphs in a transparent generative mechanism as in GKT, and incorporates half-life style retention effects. With scalable Bayesian inference, this yields interpretable, structured, and predictive multi-step knowledge tracing.",
  "target_paper": {
    "title": "Predictive, scalable and interpretable knowledge tracing on structured domains",
    "authors": "Hanqi Zhou, Robert Bamler, Charley M Wu, \u00c1lvaro Tejero-Cantero",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "knowledge tracing, interpretable representations, knowledge graphs, probabilistic models, variational inference, continual learning",
    "abstract": "Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention. This requires estimates of both the learner's progress (\"knowledge tracing\"; KT), and the prerequisite structure of the learning domain (\"knowledge mapping\"). While recent deep learning models achieve high KT accuracy, they do so at the expense of the interpretability of psychologically-inspired models. In this work, we present a solution to this trade-off. PSI-KT is a hierarchical generative approach that explicitly models how both individual cognitive traits and the prerequisite structure of knowledge influence learning dynamics, thus achieving interpretability by design. Moreover, by using scalable Bayesian inference, PSI-KT targets the real-world need for efficient personalization even with a growing body of learners and interaction data. Evaluated on three datasets from online learning platforms, PSI-KT achieves superior multi-step **p**redictive a",
    "openreview_id": "NgaLU2fP5D",
    "forum_id": "NgaLU2fP5D"
  },
  "analysis_timestamp": "2026-01-06T07:15:55.693494"
}
{
  "prior_works": [
    {
      "title": "Toy Models of Superposition",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "This work formalized polysemanticity/superposition in neural representations, directly motivating Sparse Feature Circuits\u2019 shift from neurons/heads to sparse, monosemantic features as the basic causal units."
    },
    {
      "title": "Towards Monosemanticity: Decomposing Language Models with Dictionary Learning",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "The paper provides the dictionary-learning/sparse autoencoder methodology that Sparse Feature Circuits use to obtain human-interpretable features that become the nodes in their causal graphs."
    },
    {
      "title": "Towards Automated Circuit Discovery in Transformers",
      "authors": "Samson Conmy et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "This is the primary circuit-discovery baseline\u2014operating at attention-head/neuron granularity\u2014that Sparse Feature Circuits explicitly improve upon by discovering circuits over fine-grained, interpretable features."
    },
    {
      "title": "ROME: Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "ROME introduced causal tracing/activation patching to locate and intervene on internal mediators; Sparse Feature Circuits extend this intervention logic from coarse components to SAE-discovered features and their edges."
    },
    {
      "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks",
      "authors": "David Bau et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "GAN Dissection established the causal-intervention paradigm (ablation/activation edits) to link internal units to human concepts, a methodological foundation that SFC adapts to feature-level nodes and causal graphs in LMs."
    },
    {
      "title": "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations",
      "authors": "Andrew Ross et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Ross et al. showed that removing model reliance on human-identified spurious signals improves generalization; SHIFT operationalizes this idea by ablating task-irrelevant internal features rather than constraining input gradients."
    }
  ],
  "synthesis_narrative": "Sparse Feature Circuits (SFC) sit at the intersection of two lines of work: causal circuit analysis in transformers and the move from polysemantic neurons to monosemantic features. Elhage et al.\u2019s Toy Models of Superposition crystallized the core limitation of neuron/head-level analyses\u2014superposition\u2014prompting a search for finer units. Anthropic\u2019s dictionary learning work (Towards Monosemanticity) provided exactly those units, showing that sparse autoencoders can recover human-interpretable features that serve as suitable nodes for mechanistic analysis. On the circuit side, Conmy et al.\u2019s Towards Automated Circuit Discovery in Transformers established the baseline for discovering causally implicated subnetworks but remained confined to polysemantic heads/neurons, limiting interpretability and downstream use\u2014precisely the gap SFC targets. Methodologically, SFC\u2019s discovery and verification pipeline inherits the causal-intervention toolkit: Meng et al.\u2019s ROME popularized causal tracing/patching to localize and edit mediators in language models, while Bau et al.\u2019s GAN Dissection earlier grounded the idea that ablation/activation edits can causally link internal units to human concepts. SFC extends this intervention logic to feature-level nodes and edges, constructing sparse causal graphs over SAE features. Finally, the SHIFT application directly channels Ross et al.\u2019s Right for the Right Reasons principle: by identifying and ablating features a human deems spurious, SFC improves out-of-distribution generalization, but crucially performs the intervention inside the model\u2019s learned feature space rather than via input-level constraints. Together, these works directly enable SFC\u2019s core innovation: interpretable, causally validated circuits over sparse features and their practical editing.",
  "analysis_timestamp": "2026-01-06T23:09:26.631507"
}
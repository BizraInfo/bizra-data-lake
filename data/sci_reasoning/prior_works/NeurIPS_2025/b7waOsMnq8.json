{
  "prior_works": [
    {
      "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
      "authors": "H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Aguera y Arcas",
      "year": 2017,
      "role": "Foundational algorithm for federated learning (local updates with periodic aggregation)",
      "relationship_sentence": "Established the local-update/periodic-averaging paradigm (FedAvg) that local SGD formalizes; the present paper\u2019s Gaussian approximations target precisely the distribution of such locally updated iterates."
    },
    {
      "title": "Local SGD Converges Fast and Communicates Little",
      "authors": "Sebastian U. Stich",
      "year": 2019,
      "role": "Convergence theory for local SGD in decentralized/distributed settings",
      "relationship_sentence": "Provided sharp non-asymptotic convergence guarantees for local SGD, which this paper extends by supplying distributional (Berry\u2013Esseen and trajectory-level) Gaussian approximations beyond mere convergence rates."
    },
    {
      "title": "Acceleration of Stochastic Approximation by Averaging",
      "authors": "Boris T. Polyak, Anatoli B. Juditsky",
      "year": 1992,
      "role": "Stochastic approximation asymptotics (CLT and covariance characterization for averaged SGD)",
      "relationship_sentence": "Gave classical asymptotic normality for (averaged) SGD iterates; the new results build on this stochastic approximation lineage to deliver sharper, nonasymptotic Gaussian approximations for (non-averaged) local SGD and entire trajectories."
    },
    {
      "title": "Rate of convergence in the central limit theorem for martingales with discrete time",
      "authors": "Eberhard Haeusler",
      "year": 1988,
      "role": "Berry\u2013Esseen bounds for martingale difference arrays",
      "relationship_sentence": "Supplies the martingale Berry\u2013Esseen machinery that directly underpins the sharp normal approximation rates derived for the final local SGD iterate."
    },
    {
      "title": "Gaussian approximation for high-dimensional time series",
      "authors": "Xianchao Zhang, Wei Biao Wu",
      "year": 2017,
      "role": "Gaussian approximation and bootstrap for dependent processes via functional dependence",
      "relationship_sentence": "Provides Gaussian approximation tools under dependence and a framework (functional dependence) well-suited to SGD noise, informing the paper\u2019s time-uniform trajectory approximations and validity of Gaussian bootstraps."
    },
    {
      "title": "Gaussian approximation and multiplier bootstrap for maxima of sums of high-dimensional random vectors",
      "authors": "Victor Chernozhukov, Denis Chetverikov, Kengo Kato",
      "year": 2014,
      "role": "Multiplier (Gaussian) bootstrap and uniform Gaussian approximation theory",
      "relationship_sentence": "Offers the theoretical foundation for Gaussian/multiplier bootstrap procedures and uniform approximations over index sets, which the paper adapts to bootstrap inference along the local SGD trajectory."
    },
    {
      "title": "Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates",
      "authors": "Dong Yin, Yudong Chen, Kannan Ramchandran, Peter L. Bartlett",
      "year": 2018,
      "role": "Adversarial robustness in distributed/federated learning",
      "relationship_sentence": "Motivates the need for principled statistical tests to detect adversarial behavior; the paper\u2019s time-uniform Gaussian approximations enable such Gaussian bootstrap-based attack detection."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014sharp Gaussian approximations for decentralized federated learning with local SGD, including a Berry\u2013Esseen theorem for the terminal iterate and time-uniform approximations for the full trajectory\u2014rests on two pillars: the local-SGD/federated optimization paradigm and modern probabilistic approximation/bootstrapping tools for dependent processes.\n\nOn the algorithmic side, McMahan et al.\u2019s FedAvg introduced local updating with intermittent aggregation, the very structure formalized as local SGD and analyzed by Stich, whose results established fast convergence with limited communication. These works identify the object of inference (local SGD iterates) and baseline convergence properties that this paper advances to distributional guarantees.\n\nOn the statistical side, Polyak and Juditsky\u2019s stochastic-approximation CLT for averaged SGD seeded the idea that SGD iterates admit Gaussian limits. To obtain sharp, nonasymptotic guarantees for non-averaged local SGD, the authors draw on martingale Berry\u2013Esseen theory (Haeusler), enabling explicit rates for the final iterate. For trajectory-wide, time-uniform approximations and valid Gaussian multiplier bootstraps, they leverage Gaussian approximation techniques for dependent sequences (Zhang & Wu), naturally modeling SGD\u2019s dependence structure, together with multiplier bootstrap theory and uniform Gaussian approximations over index sets (Chernozhukov\u2013Chetverikov\u2013Kato).\n\nFinally, the application to adversarial attack detection in federated settings is motivated by Byzantine-robust distributed learning (Yin et al.), where detecting malicious behavior is critical; the proposed time-uniform Gaussian approximations justify bootstrap-based tests along the training trajectory, turning robustness goals into principled statistical procedures.",
  "analysis_timestamp": "2026-01-07T00:21:33.145234"
}
{
  "prior_works": [
    {
      "title": "A Theory of the Learnable",
      "authors": "Leslie G. Valiant",
      "year": 1984,
      "role": "Foundational learning theory (PAC) and sample complexity",
      "relationship_sentence": "The paper\u2019s sample-complexity guarantees for learnable NeSy tasks build on PAC learnability foundations introduced by Valiant, translating NeSy-specific identifiability (unique DCSP solution) into standard generalization bounds."
    },
    {
      "title": "Theory of Disagreement-Based Active Learning",
      "authors": "Steve Hanneke",
      "year": 2014,
      "role": "Disagreement-based characterization of learning difficulty",
      "relationship_sentence": "The result that asymptotic expected concept error is controlled by the degree of disagreement among DCSP solutions parallels Hanneke\u2019s disagreement-region analysis, motivating the paper\u2019s error control via solution-set disagreement."
    },
    {
      "title": "Posterior Regularization for Structured Latent Variable Models",
      "authors": "Kuzman Ganchev, Jo\u00e3o Gra\u00e7a, Jennifer Gillenwater, Ben Taskar",
      "year": 2010,
      "role": "Learning with constraints via projected/regularized objectives",
      "relationship_sentence": "Casting NeSy tasks as derived constraint satisfaction problems directly echoes posterior regularization\u2019s use of constraint sets to shape learning, informing the DCSP formulation and its role in identifiability."
    },
    {
      "title": "Hinge-Loss Markov Random Fields and Probabilistic Soft Logic",
      "authors": "Stephen H. Bach, Matthias Broecheler, Bert Huang, Lise Getoor",
      "year": 2017,
      "role": "Logical constraints as convex potentials; solution structure and uniqueness",
      "relationship_sentence": "PSL\u2019s relaxation of logical constraints into convex programs highlights how the structure and uniqueness of solutions govern inference, underpinning the paper\u2019s learnability-iff-unique-DCSP-solution characterization."
    },
    {
      "title": "DeepProbLog: Neural Probabilistic Logic Programming",
      "authors": "Robin Manhaeve, Sebastijan Duman\u010di\u0107, Angelika Kimmig, Thomas Demeester, Luc De Raedt",
      "year": 2018,
      "role": "Canonical neuro-symbolic framework integrating neural perception with logical inference",
      "relationship_sentence": "By operationalizing NeSy tasks as logic programs with (soft) constraints, DeepProbLog provides the archetype from which the paper\u2019s DCSP abstraction is derived and analyzed for learnability."
    },
    {
      "title": "Underspecification Presents Challenges for Credibility in Modern Machine Learning",
      "authors": "Alexander D'Amour et al.",
      "year": 2020,
      "role": "Multiple compatible solutions (underspecification) and their generalization risks",
      "relationship_sentence": "The link between non-unique DCSP solutions and degraded generalization mirrors underspecification, motivating the paper\u2019s necessity of uniqueness for learnability and its error\u2013disagreement tradeoff."
    },
    {
      "title": "Shortcut Learning in Deep Neural Networks",
      "authors": "Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann",
      "year": 2020,
      "role": "Phenomenology of shortcut reasoning and spurious solutions",
      "relationship_sentence": "The characterization of \u2018reasoning shortcuts\u2019 as arising from spurious but consistent solutions aligns with the paper\u2019s formalization of error being governed by disagreement among DCSP solutions."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014a learnability theory for neuro-symbolic tasks via derived constraint satisfaction problems (DCSPs)\u2014stands on three pillars unified from prior work. First, classical PAC learning (Valiant) anchors the sample-complexity analysis, enabling the authors to translate NeSy-specific identifiability into standard generalization guarantees once the task is shown to be learnable. Second, the constraint-centric lineage in machine learning (Posterior Regularization; Probabilistic Soft Logic; DeepProbLog) provides the formal bridge from hybrid NeSy specifications to explicit constraint sets and solution spaces. These works establish that symbolic knowledge can be operationalized as constraints and that the structure\u2014and especially uniqueness\u2014of solutions governs downstream inference and training behavior, directly anticipating the paper\u2019s \u201clearnable iff unique DCSP solution\u201d result. Third, the paper\u2019s control of asymptotic concept error by the \u201cdegree of disagreement\u201d among DCSP solutions refines ideas from disagreement-based learning (Hanneke), adapting the disagreement-region lens to NeSy solution sets. Finally, empirical phenomena that motivate theory\u2014underspecification (D\u2019Amour et al.) and shortcut learning (Geirhos et al.)\u2014map naturally onto the paper\u2019s analysis: multiple DCSP solutions encode alternative, shortcut-prone explanations that are indistinguishable from finite data, thereby inflating error in proportion to solution-set disagreement. Together these strands yield a principled identifiability criterion, sample-complexity bounds under mild assumptions, and actionable prescriptions for designing NeSy systems that avoid shortcut-prone ambiguity by enforcing or encouraging DCSP uniqueness.",
  "analysis_timestamp": "2026-01-07T00:21:32.304265"
}
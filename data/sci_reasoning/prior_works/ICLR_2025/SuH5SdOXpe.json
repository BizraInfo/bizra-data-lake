{
  "prior_works": [
    {
      "title": "Adversarial Reprogramming of Neural Networks",
      "authors": "Gamaleldin F. Elsayed et al.",
      "year": 2019,
      "arxiv_id": "1806.11146",
      "role": "Foundation",
      "relationship_sentence": "This work established the core premise that a frozen network can be steered by learning an external input-space program, directly motivating the paper\u2019s weight-frozen robustness reprogramming formulation."
    },
    {
      "title": "Co-Op: Learning to Prompt for Vision-Language Models",
      "authors": "Kaiyang Zhou et al.",
      "year": 2022,
      "arxiv_id": "2109.01134",
      "role": "Inspiration",
      "relationship_sentence": "Co-Op showed that learnable prompts attached to a frozen backbone can controllably shift model behavior, inspiring the use of external programmable components to modulate robustness without modifying backbone parameters."
    },
    {
      "title": "Visual Prompt Tuning",
      "authors": "Menglin Jia et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "By introducing shallow vs. deep visual prompts to trade off efficiency and control, this work directly informs the paper\u2019s three reprogramming paradigms that place robust modules at different insertion points."
    },
    {
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "authors": "Aleksander Madry et al.",
      "year": 2018,
      "arxiv_id": "1706.06083",
      "role": "Baseline",
      "relationship_sentence": "As the de facto adversarial training baseline that requires updating model weights, this work provides the primary point of comparison that the paper aims to surpass by achieving robustness without retraining."
    },
    {
      "title": "Theoretically Principled Trade-off Between Robustness and Accuracy",
      "authors": "Hongyang Zhang et al.",
      "year": 2019,
      "arxiv_id": "1901.08573",
      "role": "Gap Identification",
      "relationship_sentence": "TRADES formalized the robustness\u2013accuracy trade-off under weight-updating regimes, motivating the paper\u2019s design for externally controllable robustness that avoids degrading clean accuracy via retraining."
    },
    {
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "authors": "Dequan Wang et al.",
      "year": 2021,
      "arxiv_id": "2006.10726",
      "role": "Gap Identification",
      "relationship_sentence": "Tent adapts models at test time by updating BN parameters, highlighting the need for adaptation to perturbations without changing any internal weights, which the paper addresses via reprogramming."
    },
    {
      "title": "Feature Denoising for Improving Adversarial Robustness",
      "authors": "Cihang Xie et al.",
      "year": 2019,
      "arxiv_id": "1812.03411",
      "role": "Related Problem",
      "relationship_sentence": "This work inserts denoising blocks to alter feature transformations for robustness, directly motivating the paper\u2019s shift to a non-linear robust pattern matching operator as an alternative feature mechanism."
    }
  ],
  "synthesis_narrative": "Adversarial reprogramming demonstrated that a frozen model can be repurposed by learning an input-space program, establishing the principle that external, trainable interfaces can steer a network\u2019s behavior without touching its parameters. Prompt-based methods extended this paradigm: Co-Op showed learnable prompts can systematically bias a frozen backbone\u2019s behavior, while Visual Prompt Tuning introduced shallow and deep prompt placements to balance efficiency and control by choosing where to intervene in the processing pipeline. In robustness, adversarial training became the standard, with Madry et al. defining strong PGD-based training but at the cost of heavy retraining and model parameter updates. TRADES further formalized the robustness\u2013accuracy tension under such retraining paradigms, clarifying the need for more flexible control over robustness. Test-time adaptation with Tent partially alleviated distribution shift but still required modifying internal parameters (e.g., BN), contrasting with truly weight-frozen approaches. Concurrently, feature denoising inserted explicit transformations to suppress adversarial noise, highlighting that altering the feature mechanism can materially impact robustness. Together, these works exposed a gap: steering robustness without modifying backbone weights. The reprogramming and prompt literature provided the external control interface and placement strategies, while robustness studies framed the retraining and trade-off limitations to avoid. Synthesizing these insights naturally led to a weight-frozen robustness reprogramming approach that replaces standard feature transformations with a non-linear, robust pattern matching operator and instantiates multiple placement paradigms to flexibly control robustness\u2013efficiency trade-offs across architectures.",
  "target_paper": {
    "title": "Robustness Reprogramming for Representation Learning",
    "authors": "Zhichao Hou, MohamadAli Torkamani, Hamid Krim, Xiaorui Liu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Adversarial Robustness, Robustness Reprogramming, Robust Representation Learning",
    "abstract": "This work tackles an intriguing and fundamental open challenge in representation learning: Given a well-trained deep learning model, can it be reprogrammed to enhance its robustness against adversarial or noisy input perturbations without altering its parameters?\nTo explore this, we revisit the core feature transformation mechanism in representation learning and propose a novel non-linear robust pattern matching technique as a robust alternative. Furthermore, we introduce three model reprogramming paradigms to offer flexible control of robustness under different efficiency requirements. Comprehensive experiments and ablation studies across diverse learning models ranging from basic linear model and MLPs to shallow and modern deep ConvNets demonstrate the effectiveness \nof our approaches.\nThis work not only opens a promising and orthogonal direction for improving adversarial defenses in deep learning beyond existing methods but also provides new insights into designing more resilient AI",
    "openreview_id": "SuH5SdOXpe",
    "forum_id": "SuH5SdOXpe"
  },
  "analysis_timestamp": "2026-01-06T06:52:37.100529"
}
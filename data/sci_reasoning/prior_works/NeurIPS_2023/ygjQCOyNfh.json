{
  "prior_works": [
    {
      "title": "Algorithmic Learning in a Random World",
      "authors": "Vladimir Vovk, Alexander Gammerman, Glenn Shafer",
      "year": 2005,
      "role": "Foundational theory of conformal prediction (CP) with exchangeability-based finite-sample validity and nonconformity scores.",
      "relationship_sentence": "CF-GNN directly extends the CP framework\u2019s exchangeability-based guarantees by formulating a permutation invariance condition tailored to graph data so that finite-sample coverage holds for GNN outputs."
    },
    {
      "title": "Distribution-Free Predictive Inference for Regression",
      "authors": "Jing Lei, Max G\u2019Sell, Alessandro Rinaldo, Ryan Tibshirani, Larry Wasserman",
      "year": 2018,
      "role": "Introduces split conformal prediction for practical, model-agnostic calibration with exact finite-sample coverage.",
      "relationship_sentence": "CF-GNN adopts the split conformal calibration paradigm to turn GNN point predictions into valid prediction sets/intervals and bases its exact test-time coverage characterization on this inductive CP machinery."
    },
    {
      "title": "Classification with Valid and Adaptive Coverage",
      "authors": "Yaniv Romano, Evan Patterson, Emmanuel J. Cand\u00e8s",
      "year": 2020,
      "role": "Conformal prediction for multi-class classification with adaptive nonconformity scores to control set size while maintaining coverage (e.g., APS).",
      "relationship_sentence": "CF-GNN builds on classification CP ideas for constructing label sets and explicitly targets smaller, more informative sets by adapting nonconformity scores, now informed by graph structure."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka",
      "year": 2019,
      "role": "Theoretical foundations of message-passing GNNs, emphasizing permutation invariance/equivariance and expressive power.",
      "relationship_sentence": "CF-GNN\u2019s validity hinges on a permutation invariance condition aligned with standard GNN invariance, and this work provides the formal backdrop for leveraging such invariance in graph learning."
    },
    {
      "title": "Correct and Smooth: A Simple Approach for Node Classification",
      "authors": "Johannes Klicpera, Stefan Wei\u00dfenberger, Stephan G\u00fcnnemann",
      "year": 2021,
      "role": "Topology-aware post-processing that corrects and smooths model predictions via graph propagation.",
      "relationship_sentence": "CF-GNN\u2019s topology-aware output correction that adjusts nonconformity scores is conceptually inspired by C&S-style graph-based refinement to exploit network structure for tighter (smaller) prediction sets."
    },
    {
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "authors": "Thomas N. Kipf, Max Welling",
      "year": 2017,
      "role": "Canonical transductive GNN architecture for node classification that aggregates features over graph neighborhoods.",
      "relationship_sentence": "CF-GNN is designed to wrap standard node-classification GNNs like GCNs, calibrating their outputs on a fixed graph for distribution-free coverage while respecting transductive learning specifics."
    },
    {
      "title": "Conformalized Quantile Regression",
      "authors": "Yaniv Romano, Evan Patterson, Emmanuel J. Cand\u00e8s",
      "year": 2019,
      "role": "Learns instance-conditional uncertainty via quantile regression, then uses conformalization to retain exact coverage with improved efficiency.",
      "relationship_sentence": "CF-GNN analogously learns a topology-aware adjustment to nonconformity scores before conformalization, mirroring CQR\u2019s strategy of learning local variability to shrink intervals while keeping guarantees."
    }
  ],
  "synthesis_narrative": "CF-GNN fuses conformal prediction\u2019s distribution-free guarantees with graph learning\u2019s permutation structure. The core validity engine is classical conformal prediction (Vovk, Gammerman, Shafer), operationalized via split conformal calibration (Lei et al.), which supplies exact finite-sample coverage without distributional assumptions. For the multi-class node-labeling setting, CF-GNN relies on advances in classification-oriented conformal methods (Romano, Patterson, Cand\u00e8s) that design nonconformity scores to yield prediction sets with guaranteed coverage and a focus on minimizing set size. On the graph learning side, the framework leverages the permutation invariance/equivariance properties that underpin message-passing GNNs (Xu et al.), aligning CP\u2019s exchangeability requirement with a graph-specific permutation invariance condition to re-establish validity under network dependence. CF-GNN is intended to wrap standard node-classification backbones such as GCNs (Kipf & Welling), situating the method squarely in the widely used transductive regime. Beyond validity, CF-GNN addresses efficiency by reducing prediction set sizes through a topology-aware output correction model. This design echoes two key ideas: (i) graph-based post-processing to refine predictions via network structure, as in Correct-and-Smooth (Klicpera et al.), and (ii) learning instance-conditional adjustments prior to conformalization to shrink intervals/sets while preserving guarantees, in the spirit of Conformalized Quantile Regression (Romano et al.). Together, these strands directly inform CF-GNN\u2019s main contribution: provably valid, topology-aware uncertainty quantification for GNNs with exact coverage characterization and practical set-size efficiency.",
  "analysis_timestamp": "2026-01-07T00:02:04.827325"
}
{
  "prior_works": [
    {
      "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
      "authors": [
        "Ben Mildenhall",
        "Pratul P. Srinivasan",
        "Matthew Tancik",
        "Jonathan T. Barron",
        "Ravi Ramamoorthi",
        "Ren Ng"
      ],
      "year": 2020,
      "role": "Foundational neural volumetric scene representation and differentiable rendering",
      "relationship_sentence": "ROGR builds its relightable object representation and training/rendering pipeline on the NeRF formulation of radiance fields and volumetric rendering."
    },
    {
      "title": "Ref-NeRF: Structured View-Dependent Appearance",
      "authors": [
        "Dor Verbin",
        "Peter Hedman",
        "Ben Mildenhall",
        "Jonathan T. Barron",
        "Pratul P. Srinivasan"
      ],
      "year": 2022,
      "role": "Architectural separation of diffuse and specular effects in radiance fields",
      "relationship_sentence": "ROGR\u2019s dual-branch design that separately models general lighting effects and specularities is directly inspired by Ref-NeRF\u2019s decomposition of view-dependent (specular) and diffuse components."
    },
    {
      "title": "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections",
      "authors": [
        "Ricardo Martin-Brualla",
        "Noha Radwan",
        "Mehdi S. M. Sajjadi",
        "Jonathan T. Barron",
        "Alexey Dosovitskiy",
        "Daniel Duckworth"
      ],
      "year": 2021,
      "role": "Modeling variable illumination/appearance within NeRF",
      "relationship_sentence": "ROGR conditions appearance on environment lighting; NeRF-W demonstrated how to factor illumination and appearance variability within a NeRF, informing ROGR\u2019s lighting-conditioned formulation."
    },
    {
      "title": "NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis",
      "authors": [
        "Pratul P. Srinivasan",
        "et al."
      ],
      "year": 2021,
      "role": "Relightable neural fields driven by environment maps",
      "relationship_sentence": "ROGR targets feed-forward relighting under arbitrary environment maps; NeRV established the feasibility of environment-map\u2013driven relighting with neural fields, shaping ROGR\u2019s objective and conditioning strategy."
    },
    {
      "title": "Neural-PIL: Neural Pre-Integrated Lighting for Real-Time Relighting of Reflectance Fields",
      "authors": [
        "Mark Boss",
        "Varun Jampani",
        "Kihwan Kim",
        "Hendrik P. A. Lensch",
        "Jan Kautz"
      ],
      "year": 2021,
      "role": "Efficient feed-forward relighting via pre-integrated lighting and reflectance decomposition",
      "relationship_sentence": "ROGR\u2019s efficient feed-forward relighting and separation of lighting effects align with Neural-PIL\u2019s pre-integrated lighting paradigm, motivating ROGR\u2019s design for fast relighting without per-illumination optimization."
    },
    {
      "title": "DreamFusion: Text-to-3D using 2D Diffusion",
      "authors": [
        "Ben Poole",
        "Ajay Jain",
        "Jonathan T. Barron",
        "Ben Mildenhall"
      ],
      "year": 2022,
      "role": "Using powerful 2D generative models to supervise/train 3D neural fields",
      "relationship_sentence": "ROGR\u2019s core idea of using a generative relighting model to synthesize illumination-conditioned training data follows DreamFusion\u2019s paradigm of leveraging 2D generative priors to drive the optimization of 3D representations."
    }
  ],
  "synthesis_narrative": "ROGR\u2019s key contribution\u2014training a lighting-conditioned NeRF for feed-forward relighting using a generative relighting model\u2014sits at the intersection of neural radiance fields, structured reflectance modeling, and generative supervision. NeRF provided the fundamental radiance field parameterization and differentiable volume rendering ROGR builds upon for multi-view reconstruction. Ref-NeRF directly motivates ROGR\u2019s dual-branch architecture by showing that separating diffuse and specular/view-dependent components improves the modeling of complex reflectance, which is essential for accurate relighting. NeRF in the Wild demonstrated that illumination and appearance variability can be disentangled within a NeRF, informing ROGR\u2019s strategy to explicitly condition appearance on environment lighting rather than treating lighting as nuisance factors. NeRV established a blueprint for environment-map\u2013driven relighting with neural fields, aligning closely with ROGR\u2019s goal of fast, feed-forward relighting under arbitrary HDR environment maps without per-illumination optimization. Complementing this, Neural-PIL showed how pre-integrated lighting and reflectance factorization can yield efficient relighting, reinforcing ROGR\u2019s emphasis on practical inference speed and separation of lighting effects. Finally, DreamFusion introduced the powerful idea of using 2D generative models to supervise 3D neural fields; ROGR adapts this paradigm from text/image synthesis to relighting by sampling appearances under diverse environment maps via a generative relighting model, creating supervision that trains its lighting-conditioned NeRF. Together, these works directly inform ROGR\u2019s representation, conditioning, architectural decomposition, and the novel use of generative relighting for scalable, feed-forward, physically-plausible object relighting.",
  "analysis_timestamp": "2026-01-07T00:21:32.274686"
}
{
  "prior_works": [
    {
      "title": "Shikra: Unleashing Multimodal LLMs\u2019 Referential Dialogue Skills",
      "authors": "B. Chen et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Ferret explicitly augments Shikra\u2019s coordinate-token interface for region referring by adding continuous region features, overcoming Shikra\u2019s box/point-only and purely discrete representation limits."
    },
    {
      "title": "KOSMOS-2: Grounding Multimodal Large Language Models to the World",
      "authors": "X. Wang et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Ferret targets Kosmos-2\u2019s coordinate-only phrase grounding with boxes by unifying referring and grounding through a hybrid region token that supports arbitrary shapes and finer granularity."
    },
    {
      "title": "GPT4RoI: Instruction Tuning Large Language Models on Region-of-Interest",
      "authors": "H. Li et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Ferret generalizes GPT4RoI\u2019s idea of extracting continuous visual features from specified boxes by introducing a spatial-aware sampler that pools features for regions of any shape and sparsity and pairing them with discrete coordinates."
    },
    {
      "title": "Segment Anything",
      "authors": "Alexander Kirillov et al.",
      "year": 2023,
      "arxiv_id": "2304.02643",
      "role": "Inspiration",
      "relationship_sentence": "Ferret adopts SAM\u2019s promptable region notion (points, boxes, masks) to accept free-form shapes and leverages this paradigm in its spatial-aware sampler and GRIT data construction."
    },
    {
      "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
      "authors": "Shilong Liu et al.",
      "year": 2023,
      "arxiv_id": "2303.05499",
      "role": "Foundation",
      "relationship_sentence": "Ferret builds on Grounding DINO\u2019s open-vocabulary grounding formulation and uses it to mine region\u2013text pairs and hard negatives for GRIT, integrating grounding into the MLLM setting."
    },
    {
      "title": "Generation and Comprehension of Unambiguous Object Descriptions",
      "authors": "Junhua Mao et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Ferret inherits the referring-expression task setup from RefCOCOg and extends it beyond box-based comprehension to arbitrary-granularity regions via its hybrid region representation."
    },
    {
      "title": "Visual Instruction Tuning (LLaVA): Large Language-and-Vision Assistant",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Baseline",
      "relationship_sentence": "Ferret follows LLaVA\u2019s multimodal instruction-tuning framework but replaces image-only inputs with a hybrid region representation to enable precise refer-and-ground capabilities."
    }
  ],
  "synthesis_narrative": "Shikra introduced a simple but influential interface for multimodal LLMs to refer to regions using discrete coordinate tokens, enabling box/point-level referencing while revealing the brittleness of purely discrete spatial tokens for fine-grained shapes. KOSMOS-2 similarly grounded phrases to bounding boxes via coordinate serialization, establishing a coordinate-only grounding paradigm and strong baseline performance on box-level grounding tasks. GPT4RoI moved beyond coordinates by pooling continuous visual features from specified boxes (via RoI operations), demonstrating that coupling language with localized features improves region-aware instruction following while still being limited to rectangular RoIs. Segment Anything popularized promptable region interaction\u2014points, boxes, and free-form masks\u2014showing that diverse region prompts can capture arbitrary shapes and granularity. Grounding DINO provided an open-vocabulary grounding framework that aligns text with spatial regions and offered a practical route to mine large-scale region\u2013text pairs and hard negatives. RefCOCOg formalized the referring-expression problem with natural language descriptions and region targets, seeding benchmarks and data conventions widely used for refer/ground tasks. LLaVA established the multimodal instruction-tuning pipeline that many subsequent MLLMs adopt for aligning vision encoders with LLMs. Together, these works exposed a clear opportunity: coordinate-only MLLMs lacked fine-grained spatial fidelity, and RoI-only methods were constrained to boxes, while instruction-tuned MLLMs lacked a unified region interface and data at scale. Ferret synthesizes these insights by combining discrete coordinates with continuous region features in a single hybrid region token and introducing a spatial-aware sampler to handle arbitrary shapes; leveraging SAM and Grounding DINO, it curates GRIT with hierarchical spatial knowledge and hard negatives, integrating precise refer-and-ground skills into the LLM paradigm.",
  "target_paper": {
    "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
    "authors": "Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Ferret, Multimodal Large Language Model, Referring, Grounding",
    "abstract": "We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions,  we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with an additional 130K hard negative data to promote model robustness. The resulting model not only ach",
    "openreview_id": "2msbbX3ydD",
    "forum_id": "2msbbX3ydD"
  },
  "analysis_timestamp": "2026-01-06T19:37:51.178415"
}
{
  "prior_works": [
    {
      "title": "Acceleration of Stochastic Approximation by Averaging",
      "authors": "Boris T. Polyak, Anatoli B. Juditsky",
      "year": 1992,
      "role": "Baseline optimal SGD theory",
      "relationship_sentence": "Provides the canonical optimally tuned SGD/averaging rates (and their dependence on problem parameters) that this paper aims to match with tuning-free methods up to polylogarithmic factors."
    },
    {
      "title": "Robust Stochastic Approximation Approach to Stochastic Programming",
      "authors": "Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, Alexander Shapiro",
      "year": 2009,
      "role": "Foundational stochastic (mirror) descent on bounded domains",
      "relationship_sentence": "Establishes parameter-dependent step-size prescriptions and optimal rates for convex stochastic optimization over bounded domains, furnishing the benchmark this paper shows can be matched by tuning-free algorithms."
    },
    {
      "title": "Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization",
      "authors": "Alekh Agarwal, Peter L. Bartlett, Pradeep K. Ravikumar, Martin J. Wainwright",
      "year": 2012,
      "role": "Lower bounds clarifying parameter dependence",
      "relationship_sentence": "Demonstrates fundamental dependence on quantities like domain diameter and noise, underpinning this paper\u2019s impossibility result for tuning-free optimization over unbounded domains."
    },
    {
      "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
      "authors": "John Duchi, Elad Hazan, Yoram Singer",
      "year": 2011,
      "role": "Adaptive parameter-free algorithm on bounded domains",
      "relationship_sentence": "Introduces AdaGrad, an adaptive method that removes manual step-size tuning and attains near-optimal rates with data-dependent steps, which this paper identifies as tuning-free (up to logs) on bounded domains."
    },
    {
      "title": "Coin Betting and Parameter-free Online Learning",
      "authors": "Francesco Orabona, D\u00e1vid P\u00e1l",
      "year": 2016,
      "role": "Parameter-free OCO framework with logarithmic overhead",
      "relationship_sentence": "Provides a generic parameter-free design (coin-betting) that achieves oracle-matching guarantees up to logarithmic factors and, via online-to-batch conversion, directly supports the paper\u2019s bounded-domain positive results."
    },
    {
      "title": "MetaGrad: Multiple Learning Rates in Online Learning",
      "authors": "Tim van Erven, Wouter M. Koolen",
      "year": 2016,
      "role": "Meta-learning of learning rates (tuning-free)",
      "relationship_sentence": "Develops a multi-eta scheme that adaptively matches the best fixed learning rate in hindsight up to logs, exemplifying algorithms the paper shows can match optimally tuned SGD without prior parameter knowledge on bounded domains."
    },
    {
      "title": "Universal Gradient Methods for Convex Optimization Problems",
      "authors": "Yurii Nesterov",
      "year": 2015,
      "role": "Adaptivity to unknown smoothness/geometry",
      "relationship_sentence": "Establishes the paradigm of \u2018universal\u2019 methods that adapt to unknown problem constants, conceptually motivating the paper\u2019s formal definition of tuning-free and its separation between bounded and unbounded domains."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution is to formalize tuning-free stochastic optimization\u2014algorithms that, given only coarse hints, match the performance of optimally tuned SGD up to polylogarithmic factors\u2014and to delineate precisely when this is possible. Classical SGD theory (Polyak\u2013Juditsky) and stochastic mirror descent (Nemirovski\u2013Juditsky\u2013Lan\u2013Shapiro) provide the baseline: optimal rates and their explicit dependence on smoothness, Lipschitz constants, noise, and domain diameter. These works define the oracle performance that tuning-free procedures must match. On the positive side, a rich line of adaptive and parameter-free methodology shows how to eliminate manual step-size choice on bounded domains. AdaGrad (Duchi\u2013Hazan\u2013Singer) yields data-dependent steps that achieve near-optimal rates without prior parameter knowledge. Parameter-free online learning frameworks\u2014coin-betting (Orabona\u2013P\u00e1l) and multi-eta aggregation (MetaGrad)\u2014provably compete with the best fixed learning rate in hindsight with only logarithmic overhead; via online-to-batch conversion, they furnish stochastic optimizers that meet the paper\u2019s tuning-free criterion on bounded sets. On the negative side, information-theoretic lower bounds (Agarwal\u2013Bartlett\u2013Ravikumar\u2013Wainwright) expose unavoidable dependence on the domain diameter and noise, which the paper sharpens into an impossibility of tuning-free optimization over unbounded domains for convex smooth or Lipschitz objectives. Finally, universal gradient methods (Nesterov) conceptually motivate adaptivity to unknown problem parameters, while the paper clarifies the exact boundary\u2014bounded versus unbounded\u2014where such universal/tuning-free guarantees are achievable and validates specific recent algorithms (e.g., DoG/DoWG) under appropriate noise assumptions.",
  "analysis_timestamp": "2026-01-07T00:02:04.881978"
}
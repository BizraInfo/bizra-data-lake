{
  "prior_works": [
    {
      "title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback",
      "authors": "Ruining He and Julian McAuley",
      "year": 2016,
      "arxiv_id": "1510.01784",
      "role": "Extension",
      "relationship_sentence": "VBPR\u2019s linear projection of frozen, pre-trained content features (images) into a collaborative embedding space directly foreshadows our approach of using a simple linear map to turn frozen language model representations into effective item factors for recommendation."
    },
    {
      "title": "DeepCoNN: Deep Cooperative Neural Networks for Personalized Recommendation",
      "authors": "Lei Zheng et al.",
      "year": 2017,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "DeepCoNN established that textual signals can serve as primary carriers of user and item preference representations, motivating our use of LM-derived text representations as the substrate from which to derive recommendation embeddings."
    },
    {
      "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer",
      "authors": "Fei Sun et al.",
      "year": 2019,
      "arxiv_id": "1904.06690",
      "role": "Baseline",
      "relationship_sentence": "BERT4Rec is a primary sequential recommendation baseline built on Transformer objectives over item IDs, against which we contrast our finding that a direct linear readout from language representations can yield superior recommendation performance."
    },
    {
      "title": "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
      "authors": "Xiangnan He et al.",
      "year": 2020,
      "arxiv_id": "2002.02126",
      "role": "Baseline",
      "relationship_sentence": "LightGCN is the canonical collaborative filtering baseline whose learned item space we effectively replace by linearly mapping advanced language representations, demonstrating that collaborative signals can be read out from LMs."
    },
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Fabio Petroni et al.",
      "year": 2019,
      "arxiv_id": "1909.01066",
      "role": "Inspiration",
      "relationship_sentence": "By showing that LMs implicitly store factual knowledge that can be accessed without task-specific fine-tuning, this work motivates our hypothesis that preference-relevant collaborative signals are implicitly encoded and can be extracted for recommendation."
    },
    {
      "title": "A Structural Probe for Finding Syntax in Word Representations",
      "authors": "John Hewitt and Christopher D. Manning",
      "year": 2019,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "This paper demonstrates that complex structure is linearly recoverable from LM embeddings, directly informing our decision to use a simple linear mapping to recover an effective recommendation space from language representations."
    }
  ],
  "synthesis_narrative": "Linear alignment of powerful, frozen content encoders with collaborative filtering has precedent in VBPR, which used a simple linear projection to inject pre-trained image features into the recommendation embedding space. DeepCoNN further established that language signals alone can serve as effective user and item representations by learning from reviews, indicating that textual semantics capture preference-relevant structure. In parallel, BERT4Rec adopted Transformer objectives for sequential recommendation but trained on item IDs, reinforcing a perceived divide between linguistic modeling and behavioral embeddings. LightGCN distilled collaborative filtering to pure graph-based item\u2013user embeddings, becoming the de facto standard space learned directly from interactions. Outside recommendation, Petroni et al. showed that language models store rich factual knowledge retrievable without task-specific fine-tuning, while Hewitt and Manning proved that such knowledge can often be linearly read out via simple probes.\nTogether, these works suggest a gap and an opportunity: content features can be linearly aligned to collaborative spaces (VBPR), LMs encode rich world knowledge (Petroni), and much of that structure is linearly accessible (Hewitt & Manning), yet mainstream recommenders still learn separate behavior-specific spaces (LightGCN, BERT4Rec). The current paper synthesizes these insights by directly linearly mapping advanced LM representations into an item embedding space, revealing a near-homomorphic relationship between language and recommendation spaces and demonstrating superior recommendation performance without heavy task-specific retraining.",
  "target_paper": {
    "title": "Language Representations Can be What Recommenders Need: Findings and Potentials",
    "authors": "Leheng Sheng, An Zhang, Yi Zhang, Yuxin Chen, Xiang Wang, Tat-Seng Chua",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Collaborative filtering, Language-representation-based recommendation, Language models, Language model representations",
    "abstract": "Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields.\nHowever, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to prevailing understanding that LMs and traditional recommenders learn two distinct representation spaces due to the huge gap in language and behavior modeling objectives, this work re-examines such understanding and explores extracting a recommendation space directly from the language representation space.\nSurprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance.\nThis outcome suggests the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded with",
    "openreview_id": "eIJfOIMN9z",
    "forum_id": "eIJfOIMN9z"
  },
  "analysis_timestamp": "2026-01-06T18:14:07.621559"
}
{
  "prior_works": [
    {
      "title": "How Deep Networks Learn Hierarchical Data: the Random Hierarchy Model",
      "authors": "Umberto Maria Tomasini et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "SRHM is a direct modification of the Random Hierarchy Model, adding sparsity to the latent hierarchy to resolve the original model\u2019s inability to induce insensitivity to spatial transformations."
    },
    {
      "title": "Group invariant scattering",
      "authors": "St\u00e9phane Mallat",
      "year": 2012,
      "role": "Foundation",
      "relationship_sentence": "Mallat\u2019s scattering theory established that hierarchical, sparse wavelet representations yield stability to diffeomorphic (smooth) deformations; SRHM transposes this principle to a generative data model, showing sparsity in a hierarchy confers invariance to discrete analogues of smooth transforms."
    },
    {
      "title": "Invariance and stability to deformations in deep convolutional networks",
      "authors": "Alberto Bietti et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "This work formalized and empirically linked invariance/stability to CNN performance but did not explain why tasks possess such invariances; SRHM fills this gap by deriving invariance from sparsity within a hierarchical generative process."
    },
    {
      "title": "Deep vs. shallow networks: An approximation theory perspective",
      "authors": "Hrushikesh N. Mhaskar et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "By showing depth\u2019s advantage for compositional (hierarchical) functions, this work provides the theoretical problem formulation that SRHM instantiates as a random hierarchical generative model learned progressively with depth."
    },
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco S. Cohen et al.",
      "year": 2016,
      "role": "Related Problem",
      "relationship_sentence": "G-CNNs enforce equivariance/invariance architecturally via group symmetries; SRHM instead explains how similar insensitivities arise from the structure of the data itself (sparse hierarchy), independent of architectural constraints."
    },
    {
      "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
      "authors": "Bruno A. Olshausen et al.",
      "year": 1996,
      "role": "Inspiration",
      "relationship_sentence": "The finding that natural images exhibit sparse latent causes motivated SRHM\u2019s key step of injecting sparsity into the hierarchy to capture image-like invariances and localized feature emergence."
    },
    {
      "title": "Inductive Bias of Deep Convolutional Networks through Pooling Geometry",
      "authors": "Nadav Cohen et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "By analyzing how hierarchical pooling geometries induce invariances and compositional biases, this work informed SRHM\u2019s use of hierarchical structure, while SRHM\u2019s novelty is to show that sparsity in the data hierarchy itself yields those invariances."
    }
  ],
  "synthesis_narrative": "The Sparse Random Hierarchy Model (SRHM) crystallizes two major lines of theory: why depth benefits hierarchical data and why invariances matter for generalization. The earlier Random Hierarchy Model by Tomasini and Wyart provided a concrete generative setting where depth progressively uncovers hierarchical features, but it lacked a mechanism for the invariances empirically tied to performance. Mallat\u2019s scattering framework showed that hierarchical, sparse representations are provably stable to smooth deformations, a principle extended to CNNs by Bietti and Mairal, who also documented the tight link between invariance/stability and accuracy\u2014yet without a generative account of where such invariances originate. In parallel, approximation-theoretic work by Mhaskar and Poggio, and compositional analyses by Cohen and collaborators, established the foundational advantage and inductive biases of hierarchical architectures, including how pooling geometries yield invariances. Olshausen and Field\u2019s seminal discovery that natural images possess sparse latent structure further suggested that sparsity is the missing ingredient. SRHM integrates these threads by injecting sparsity into a hierarchical generative model, thereby deriving task insensitivity to discrete versions of smooth spatial transformations and unifying hierarchical feature learning with invariance acquisition. In doing so, it upgrades the Random Hierarchy Model into a framework that both explains depth\u2019s progressive representation building and rationalizes why invariances should emerge from the data\u2019s sparse hierarchical organization.",
  "analysis_timestamp": "2026-01-06T23:09:26.411299"
}
{
  "prior_works": [
    {
      "title": "Robust Dynamic Programming",
      "authors": "Iyengar",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "Established the minimax robust MDP framework and robust Bellman operators, providing the theoretical backdrop that this paper adapts to state-adversarial perturbations when proving the existence and structure of an optimal robust policy (ORP)."
    },
    {
      "title": "Robust control of Markov decision processes with uncertain transition matrices",
      "authors": "Nilim et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "Formalized distributionally robust MDPs and existence of robust optimal stationary policies under uncertainty sets, which the present work leverages conceptually to align ORP with the Bellman optimal policy under the proposed CAP assumption."
    },
    {
      "title": "Adversarial Attacks on Neural Network Policies",
      "authors": "Huang et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduced the state-adversarial perturbation setting for RL policies, directly motivating this paper\u2019s focus on state-adversarial robustness and the search for an ORP in that regime."
    },
    {
      "title": "Robust Deep Reinforcement Learning with Adversarial Attacks",
      "authors": "Pattanaik et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Proposed adversarial training of DQN with state perturbations as a practical defense; the current paper\u2019s CA-DQN is designed to remedy its vulnerability by enforcing CAP-consistent learning and minimizing Bellman error in L\u221e."
    },
    {
      "title": "Action Robust Reinforcement Learning",
      "authors": "Tessler et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Cast robustness as a zero-sum adversary-agent formulation (but in action space), informing this paper\u2019s minimax view and its existence results for robust optimal policies under state adversarial perturbations."
    },
    {
      "title": "Finite-time bounds for fitted value iteration",
      "authors": "Munos et al.",
      "year": 2008,
      "role": "Inspiration",
      "relationship_sentence": "Showed that the Bellman operator is a contraction in the sup-norm and that controlling L\u221e Bellman error yields uniform performance guarantees, directly motivating this paper\u2019s central claim that L\u221e-norm residuals are necessary to attain ORP."
    },
    {
      "title": "Learning Near-Optimal Policies with Bellman-Residual Minimization Based on Rollouts",
      "authors": "Antos et al.",
      "year": 2008,
      "role": "Inspiration",
      "relationship_sentence": "Analyzed Bellman-residual minimization objectives and the effect of the chosen norm, underpinning this paper\u2019s critique of L1-residual\u2013based targets and its shift to Bellman infinity-error for robust optimality."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014formalizing a Consistency Assumption of Policy (CAP), proving the existence of a deterministic, stationary optimal robust policy (ORP) that aligns with the Bellman optimal policy, and establishing the necessity of minimizing Bellman error in L\u221e\u2014emerges at the intersection of robust MDP theory and state-adversarial RL. Foundational robust MDP works by Iyengar (2005) and Nilim & El Ghaoui (2005) provided the minimax framework and robust Bellman operators that justify seeking robust optimality through Bellman-based reasoning; this paper adapts that lens to the state-perturbation setting, using CAP to reconcile existence questions.\n\nThe modern problem formulation\u2014adversarial perturbations to observed states\u2014was crystallized by Huang et al. (2017), and subsequent adversarial training defenses such as Pattanaik et al. (2018) supplied the practical baselines that the proposed CA-DQN aims to supersede. While Tessler et al. (2019) demonstrated how adversarial robustness can be cast in a zero-sum setting (albeit for action perturbations), their formulation informed this work\u2019s structural view of robust policies and minimax optimality.\n\nCrucially, the paper\u2019s insistence on the L\u221e Bellman residual draws directly from approximate dynamic programming theory: Munos & Szepesv\u00e1ri (2008) highlighted the sup-norm contraction of the Bellman operator and uniform error propagation, and Antos, Szepesv\u00e1ri & Munos (2008) analyzed Bellman-residual minimization objectives and norm choice. Building on these insights, the paper argues that only L\u221e Bellman error control guarantees the CAP-consistent ORP, clarifying why prior L1-focused targets are brittle under adversarial state perturbations.",
  "analysis_timestamp": "2026-01-06T23:09:26.420335"
}
{
  "summary": {
    "model": "gpt-5.2",
    "k": 10,
    "crawl_method": "exa_ai_improved",
    "total_papers": 65,
    "hits": 27,
    "hit_rate_percent": 41.54,
    "average_crawl_rate": 100.0,
    "average_quality_rate": 100.0,
    "runtime_minutes": 20.62,
    "cost": {
      "input_tokens": 668934,
      "output_tokens": 66550,
      "input_cost_usd": 1.34,
      "output_cost_usd": 0.93,
      "total_cost_usd": 2.27
    },
    "timestamp": "2026-01-05T00:30:34.387491"
  },
  "results": [
    {
      "paper_idx": 0,
      "paper_title": "Generalized Linear Mode Connectivity for Transformers",
      "contribution": "They develop a unified, symmetry\u2011aware reparameterization framework (permutations, semi\u2011permutations, orthogonal transforms, and general invertible maps) that uncovers low\u2011 and zero\u2011barrier linear interpolation paths between independently trained Transformers (including Vision Transformers and GPT\u20112) and across architectures of differing widths.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11280,
      "output_tokens": 1107,
      "predecessor_details": [
        {
          "success": true,
          "title": "Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers",
          "url": "https://arxiv.org/abs/2511.04514",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2511.04514** (cs)\n\n\\[Submitted on 6 Nov 2025\\]\n\n# Title:Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers\n\nAuthors: [C. Hepburn](https://arxiv.org/search/cs?searchtype=author&query=Hepburn,+C), [T. Zielke](https://arxiv.org/search/cs?searchtype=author&query=Zielke,+T), [A.P. Raulf](https://arxiv.org/search/cs?searchtype=author&query=Raulf,+A)\n\nView a PDF of the paper titled Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers, by C. Hepburn and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2511.04514) [HTML (experimental)](https://arxiv.org/html/2511.04514v1)\n\n> Abstract:The phenomenon of linear mode connectivity (LMC) links several aspects of deep learning, including training stability under noisy stochastic gradients, the smoothness and generalization of local minima (basins), the similarity and functional diversity of sampled models, and architectural effects on data processing. In this work, we experimentally study LMC under data shifts and identify conditions that mitigate their impact. We interpret data shifts as an additional source of stochastic gradient noise, which can be reduced through small learning rates and large batch sizes. These parameters influence whether models converge to the same local minimum or to regions of the loss landscape with varying smoothness and generalization. Although models sampled via LMC tend to make similar errors more frequently than those converging to different basins, the benefit of LMC lies in balancing training efficiency against the gains achieved from larger, more diverse ensembles. Code and supplementary materials will be made publicly available at [this https URL](https://github.com/DLR-KI/LMC) in due course.\n\n| | |\n| --- | --- |\n| Comments: | 16 pages, 22 figures |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2511.04514](https://arxiv.org/abs/2511.04514) \\[cs.LG\\] |\n| (or [arXiv:2511.04514v1](https://arxiv.org/abs/2511.04514v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2511.04514](https://doi.org/10.48550/arXiv.2511.04514) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Tobias Zielke \\[ [view email](https://arxiv.org/show-email/84061443/2511.04514)\\] **\\[v1\\]**\nThu, 6 Nov 2025 16:30:56 UTC (1,802 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers, by C. Hepburn and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2511.04514)\n- [HTML (experimental)](https://arxiv.org/html/2511.04514v1)\n- [TeX Source](https://arxiv.org/src/2511.04514)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2511.04514&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2511.04514&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-11](https://arxiv.org/list/cs.LG/2025-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2511.04514?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2511.04514)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2511.04514)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2511.04514)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2511.04514) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of Deep Neural Networks",
          "cleaned_query": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of Deep Neural Networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Disentangling Linear Mode Connectivity - arXiv",
          "url": "https://arxiv.org/html/2312.09832v1",
          "content": "Disentangling Linear Mode Connectivity\nLicense: arXiv.org perpetual non-exclusive license\narXiv:2312.09832v1 [cs.LG] 15 Dec 2023\n# Disentangling Linear Mode Connectivity\nG\u00fcl Sena Alt\u0131nta\u015f \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Gregor Bachmann \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Lorenzo Noci \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Thomas Hofmann\nETH Z\u00fcrich\n{galtintas, gregorb, lnoci}@ethz.ch\n###### Abstract\nLinear mode-connectivity (LMC) (or lack thereof) is one of the intriguing characteristics of neural network loss landscapes. While empirically well established, it unfortunately still lacks a proper theoretical understanding. Even worse, although empirical data points are abound, a systematic study of when networks exhibit LMC is largely missing in the literature. In this work we aim to close this gap. We explore how LMC is affected by three factors: (1) architecture (sparsity, weight-sharing), (2) training strategy (optimization setup) as well as (3) the underlying dataset. We place particular emphasis on minimal but non-trivial settings, removing as much unnecessary complexity as possible. We believe that our insights can guide future theoretical works on uncovering the inner workings of LMC.\n## 1Introduction\nIn recent years, there has been a growing interest in understanding the geometry of loss landscapes, how modern stochastic first-order gradient-based algorithms navigate them and the relationship between different optima. There is a large body of work on the mode-connectivity (MC)> Garipov et\u00a0al. (\n[> 2018\n](#bib.bib10)> ); Draxler et\u00a0al. (\n[> 2018\n](#bib.bib7)> ); Benton et\u00a0al. (\n[> 2021\n](#bib.bib2)> )\n,linearmode-connectivity (LMC)> Frankle et\u00a0al. (\n[> 2020\n](#bib.bib9)> )\n, permutation invariance> Entezari et\u00a0al. (\n[> 2022\n](#bib.bib8)> ); Ainsworth et\u00a0al. (\n[> 2023\n](#bib.bib1)> ); Benzing et\u00a0al. (\n[> 2022\n](#bib.bib3)> ); Simsek et\u00a0al. (\n[> 2021\n](#bib.bib16)> )\nand a broader range of symmetries> Zhao et\u00a0al. (\n[> 2023\n](#bib.bib18)> )\nof neural networks, showing that loss landscapes are not solely characterized by high non-convexity and isolated minima but can often contain flat connected regions. A more detailed account of these works are included in[Appendix\u00a0B](#A2).\nCrucial to this work, LMC (and the lack of it) has been observed in disparate settings, however its root causes have not been epistemically investigated. For instance, when it comes to architectural components, it is well-known that convolution-based architectures lack LMC> Frankle et\u00a0al. (\n[> 2020\n](#bib.bib9)> )\ncompared to fully-connected models even after accounting for permutation invariance> Entezari et\u00a0al. (\n[> 2022\n](#bib.bib8)> ); Ainsworth et\u00a0al. (\n[> 2023\n](#bib.bib1)> )\n. The varying factors distinguishing these two architectures, locality, weight-sharing, pooling layers, etc., make it hard to pinpoint the source of disruption for LMC. Furthermore, these architectures are often trained under different optimization schemes and with different datasets, which are likely to be confounding factors.\nIn this paper, we systemically isolate some of the causes of LMC. We start from the simplest connected setting of logistic regression, i.e. linear model with no hidden layers, and gradually incorporate architectural changes, training techniques, and datasets typically used in modern deep learning pipelines. We identify the minimal non-linear setting, namely an MLP with one hidden layer and ReLU activation, where LMC can be robustly observed over different optimization schemes and datasets. We then analyze which components break LMC, in particular we study:\n* \u2022The effect of the model architecture by introducing locality, weight-sharing, and sparsity to the hidden layer. This way, we recover locally connected, convolutional, and attention-based models that have a correspondence with the minimal model. Our experiments suggest that while locality preserves LMC, weight-sharing breaks it.\n* \u2022Optimization algorithm and training strategy.\nWe show that ADAM breaks connectivity more than SGD, while it can be recovered by modifying learning rate and adding warm-up.\n* \u2022How dataset complexity affects LMC\nby training MLPs with increasing dataset complexity, namely MNIST, CiFAR-10, CiFAR-100 and TinyImageNet. We observe that LMC can be more easily broken under more complex datasets.\n## 2Background\nWe consider the classification problem for a generalL\ud835\udc3fLitalic\\_L-layer model with\u03c3\ud835\udf0e\\\\sigmaitalic\\_\u03c3activation trained with the cross entropy loss, whose intermediate output at layerl&lt;L\ud835\udc59\ud835\udc3fl&lt;Litalic\\_l &lt;&lt; italic\\_Lis given by:\n|\ud835\udc33l\u2254fl\u2062(\ud835\udc31;\u03b8l)=\u03c3\u2062(Wl\u2062\ud835\udc33l\u22121+\ud835\udc1bl)\u2254subscript\ud835\udc33\ud835\udc59subscript\ud835\udc53\ud835\udc59\ud835\udc31subscript\ud835\udf03\ud835\udc59\ud835\udf0esubscript\ud835\udc4a\ud835\udc59subscript\ud835\udc33\ud835\udc591subscript\ud835\udc1b\ud835\udc59\\\\mathbf{z}\\_{l}\\\\coloneqq f\\_{l}(\\\\mathbf{x};\\\\mathbf{\\\\theta}\\_{l})=\\\\sigma(W\\_{l}%\n\\\\mathbf{z}\\_{l-1}+\\\\mathbf{b}\\_{l})bold\\_z start\\_POSTSUBSCRIPT italic\\_l end\\_POSTSUBSCRIPT \u2254italic\\_f start\\_POSTSUBSCRIPT italic\\_l end\\_POSTSUBSCRIPT ( bold\\_x ; italic\\_\u03b8 start\\_POSTSUBSCRIPT italic\\_l end\\_POSTSUBSCRIPT ) = italic\\_\u03c3 ( italic\\_W start\\_POSTSUBSCRIPT italic\\_l end\\_POSTSUBSCRIPT bold\\_z start\\_POSTSUBSCRIPT italic\\_l - 1 end\\_POSTSUBSCRIPT + bold\\_b start\\_POSTSUBSCRIPT italic\\_l end\\_POSTSUBSCRIPT )||\nand the final output isy^\u2254WL\u2062\ud835\udc33L\u22121+\ud835\udc1bL\u2254^\ud835\udc66subscript\ud835\udc4a\ud835\udc3fsubscript\ud835\udc33\ud835\udc3f1subscript\ud835\udc1b\ud835\udc3f\\\\hat{y}\\\\coloneqq W\\_{L}\\\\mathbf{z}\\_{L-1}+\\\\mathbf{b}\\_{L}over^ start\\_ARG italic\\_y end\\_ARG \u2254italic\\_W start\\_POSTSUBSCRIPT italic\\_L end\\_POSTSUBSCRIPT bold\\_z start\\_POSTSUBSCRIPT italic\\_L - 1 end\\_POSTSUBSCRIPT + bold\\_b start\\_POSTSUBSCRIPT italic\\_L end\\_POSTSUBSCRIPT. WhenL=1\ud835\udc3f1L=1italic\\_L = 1we recover logistic regression, and if\u03c3\ud835\udf0e\\\\sigmaitalic\\_\u03c3is the identity function we have anL\ud835\udc3fLitalic\\_L-layer linear model. All models in the rest of the text are trained for 200 epochs and non-linear models reach\u223c0similar-toabsent0\\\\sim 0\u223c 0training loss.\n#### Linear Interpolation:\nFor two networksA\ud835\udc34Aitalic\\_AandB\ud835\udc35Bitalic\\_Bwith parameters\u0398Asubscript\u0398\ud835\udc34\\\\Theta\\_{A}roman\\_\u0398 start\\_POSTSUBSCRIPT italic\\_A end\\_POSTSUBSCRIPTand\u0398Bsubscript\u0398\ud835\udc35\\\\Theta\\_{B}roman\\_\u0398 start\\_POSTSUBSCRIPT italic\\_B end\\_POSTSUBSCRIPT, their linear interpolation is defined with respect to the convex combination of the parameters at each layer, i.e.\u0398\u2062(\u03b1)\u2254{(1\u2212\u03b1)\u2062WAi+\u03b1\u2062WBi,(1\u2212\u03b1)\u2062\ud835\udc1bAi+\u03b1\u2062\ud835\udc1bBi}i:1\u2192L\u2254\u0398\ud835\udefcsubscript1\ud835\udefcsubscript\ud835\udc4asubscript\ud835\udc34\ud835\udc56\ud835\udefcsubscript\ud835\udc4asubscript\ud835\udc35\ud835\udc561\ud835\udefcsubscript\ud835\udc1bsubscript\ud835\udc34\ud835\udc56\ud835\udefcsubscript\ud835\udc1bsubscript\ud835\udc35\ud835\udc56:\ud835\udc56\u21921\ud835\udc3f\\\\Theta(\\\\alpha)\\\\coloneqq\\\\{(1-\\\\alpha)W\\_{A\\_{i}}+\\\\alpha W\\_{B\\_{i}},(1-\\\\alpha)%\n\\\\mathbf{b}\\_{A\\_{i}}+\\\\alpha\\\\mathbf{b}\\_{B\\_{i}}\\\\}\\_{i:1\\\\to L}roman\\_\u0398 ( italic\\_\u03b1 ) \u2254{ ( 1 - italic\\_\u03b1 ) italic\\_W start\\_POSTSUBSCRIPT italic\\_A start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT end\\_POSTSUBSCRIPT + italic\\_\u03b1 italic\\_W start\\_POSTSUBSCRIPT italic\\_B start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT end\\_POSTSUBSCRIPT , ( 1 - italic\\_\u03b1 ) bold\\_b start\\_POSTSUBSCRIPT italic\\_A start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT end\\_POSTSUBSCRIPT + italic\\_\u03b1 bold\\_b start\\_POSTSUBSCRIPT italic\\_B start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT end\\_POSTSUBSCRIPT } start\\_POSTSUBSCRIPT italic\\_i : 1 \u2192italic\\_L end\\_POSTSUBSCRIPT.\n#### Error Barrier:\nWe are interested in how the error evolves along the linear path between two modelsA,B\ud835\udc34\ud835\udc35A,Bitalic\\_A , italic\\_B, where\u0398B\u2254\u0398A\u2254subscript\u0398\ud835\udc35subscript\u0398\ud835\udc34\\\\Theta\\_{B}\\\\coloneqq\\\\Theta\\_{A}roman\\_\u0398 start\\_POSTSUBSCRIPT italic\\_B end\\_POSTSUBSCRIPT \u2254roman\\_\u0398 start\\_POSTSUBSCRIPT italic\\_A end\\_POSTSUBSCRIPT, during training. Do they stay linearly mode-connected even though they are trained separately, i.e. with different SGD noise (data orderings and augmentation).\nWe base our measure of connectivity on> Frankle et\u00a0al. (\n[> 2020\n](#bib.bib9)> )\n\u2019s definition of the error barrier ([Equation\u00a01](#S2.E1)).\n|\u212c\u212c\\\\displaystyle\\\\mathcal{B}caligraphic\\_B|=sup\u03b1\u2130\u2062(f\u2062(\u22c5;\u0398\u2062(\u03b1)))\u221212\u2062(\u2130\u2062(f\u2062(\u22c5;\u0398A))+\u2130\u2062(f\u2062(\u22c5;\u0398B)))absentsubscriptsupremum\ud835\udefc\u2130\ud835\udc53\u22c5\u0398\ud835\udefc12\u2130\ud835\udc53\u22c5subscript\u0398\ud835\udc34\u2130\ud835\udc53\u22c5subscript\u0398\ud835\udc35\\\\displaystyle=\\\\sup\\_{\\\\alpha}\\\\mathcal{E}(f(\\\\cdot;\\\\Theta(\\\\alpha)))-\\\\frac{1}{2}(%\n\\\\mathcal{E}(f(\\\\cdot;\\\\Theta\\_{A}))+\\\\mathcal{E}(f(\\\\cdot;\\\\Theta\\_{B})))= roman\\_sup start\\_POSTSUBSCRIPT italic\\_\u03b1 end\\_POSTSUBSCRIPT caligraphic\\_E ( italic\\_f ( \u22c5; roman\\_\u0398 ( italic\\_\u03b1 ) ) ) - divide start\\_ARG 1 end\\_ARG start\\_ARG 2 end\\_ARG ( caligraphic\\_E ( italic\\_f ( \u22c5; roman\\_\u0398 start\\_POSTSUBSCRIPT italic\\_A end\\_POSTSUBSCRIPT ) ) + caligraphic\\_E ( italic\\_f ( \u22c5; roman\\_\u0398 start\\_POSTSUBSCRIPT italic\\_B end\\_POSTSUBSCRIPT ) ) )||(1)|\nThe error is quantified as the ratio of incorrect predictions, represented as\u2130\u2062(\u22c5)\u2254(1\u2212Acc\u2062(\u22c5)),Acc\u2062(\u22c5)\u2208[0,1]formulae-sequence\u2254\u2130\u22c51Acc\u22c5Acc\u22c501\\\\mathcal{E}(\\\\cdot)\\\\coloneqq(1-\\\\text{Acc}(\\\\cdot)),\\\\;\\\\text{Acc}(\\\\cdot)\\\\in[0,1]caligraphic\\_E ( \u22c5) \u2254( 1 - Acc ( \u22c5) ) , Acc ( \u22c5) \u2208[ 0 , 1 ].\nWhile the current barrier definition offers an absolute measure, it doesn\u2019t differentiate the extent of performance loss, which is the primary focus of LMC research, across various levels of task complexity. Hence, we propose to use a normalized version that accounts for test accuracy when comparing the same architecture on different datasets.\n|\u212c\u00af=\u212c12\u2062(Acct\u2062e\u2062(f\u2062(\u22c5;\u0398A))+Acct\u2062e\u2062(f\u2062(\u22c5;\u0398B)))\u00af\u212c\u212c12subscriptAcc\ud835\udc61\ud835\udc52\ud835\udc53\u22c5subscript\u0398\ud835\udc34subscriptAcc\ud835\udc61\ud835\udc52\ud835\udc53\u22c5subscript\u0398\ud835\udc35\\\\bar{\\\\mathcal{B}}=\\\\frac{\\\\mathcal{B}}{\\\\frac{1}{2}(\\\\text{Acc}\\_{te}(f(\\\\cdot;%\n\\\\Theta\\_{A}))+\\\\text{Acc}\\_{te}(f(\\\\cdot;\\\\Theta\\_{B})))}over\u00af start\\_ARG caligraphic\\_B end\\_ARG = divide start\\_ARG caligraphic\\_B end\\_ARG start\\_ARG divide start\\_ARG 1 end\\_ARG start\\_ARG 2 end\\_ARG ( Acc start\\_POSTSUBSCRIPT italic\\_t italic\\_e end\\_POSTSUBSCRIPT ( italic\\_f ( \u22c5; roman\\_\u0398 start\\_POSTSUBSCRIPT italic\\_A end\\_POSTSUBSCRIPT ) ) + Acc start\\_POSTSUBSCRIPT italic\\_t italic\\_e end\\_POSTSUBSCRIPT ( italic\\_f ( \u22c5; roman\\_\u0398 start\\_POSTSUBSCRIPT italic\\_B end\\_POSTSUBSCRIPT ) ) ) end\\_ARG||(2)|\nWe follow the convention in the literature and evaluate\u0398\u2062(\u03b1)\u0398\ud835\udefc\\\\Theta(\\\\alpha)roman\\_\u0398 ( italic\\_\u03b1 )atT\ud835\udc47Titalic\\_Tequidistant values of\u03b1\ud835\udefc\\\\alphaitalic\\_\u03b1between 0 and 1.\nT is set to 11, i.e. the model is evaluated at\u03b1\u2208{0,0.1,\u2026,1}\ud835\udefc00.1\u20261\\\\alpha\\\\in\\\\{0,0.1,\\\\dots,1\\\\}italic\\_\u03b1 \u2208",
          "original_query": "Linear Mode Connectivity and its implications for neural networks",
          "cleaned_query": "Linear Mode Connectivity and its implications for neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "New study blames diet, not physical inactivity, for obesity ...",
          "url": "https://www.science.org/content/article/new-study-blames-diet-not-physical-inactivity-obesity-crisis",
          "content": "[Skip to main content](https://www.science.org/content/article/new-study-blames-diet-not-physical-inactivity-obesity-crisis#main-content-focus)\n\nAdvertisement\n\nMain content starts here\n\n- [News](https://www.science.org/news/all-news)\n- [Health](https://www.science.org/topic/category/health)\n\n# New study blames diet, not physical inactivity, for obesity crisis\n\n## But some scientists warn the research, which compared energy burned across populations, can\u2019t reveal the epidemic\u2019s causes\n\n- 14 Jul 2025\n- 3:00 PM ET\n- By [Catherine Offord](https://www.science.org/content/author/catherine-offord)\n\nA new study concludes diet is by far the most important driver of obesity in more economically developed countries.thebigland88/iStock\n\nLISTEN TO THIS ARTICLE\n\nPlayPause\n\n_Skip backwards_ Go ten seconds backward_Skip forwards_ Go ten seconds forward\n\nProgress\n\n1.0x\n\nMuteUnmute\n\nVolume is at 50%\n\n00:00\n\n5:45\n\n1.0x\n\nAudio is AI-generated. [Report an issue](https://airtable.com/appfONKW7RlmwsGdI/shrS3naRuSbE541qz) \\| [Give feedback](https://airtable.com/appfONKW7RlmwsGdI/shrqAHbeq0Kiip9Dh)\n\nMore than 1 billion people worldwide live with obesity, a global epidemic that health authorities have blamed on both increased consumption of calories and decreased physical activity. But which factor contributes more? After measuring the calories burned by people from different economic backgrounds and lifestyles, research published today in the _Proceedings of the National Academy of Sciences_ concludes diet plays [a far bigger role than physical inactivity in driving the epidemic](https://www.pnas.org/cgi/doi/10.1073/pnas.2420902122)\u2014though not everyone agrees with that interpretation.\n\nThe new study found that, when adjusted for body size, people in more economically developed societies expend relatively less energy. However, the differences are too small to explain the higher rates of obesity in these societies. Worldwide weight gains instead seem to be associated mainly with how much we consume, not how much we burn off.\n\nThe findings align with the current conventional wisdom that increased energy intake is the primary driver of obesity and highlight the need for policies to reduce that, says Vanessa Oddo, an epidemiologist at the University of Illinois Chicago who was not involved in the work. However, she and others caution the study\u2014which relied on computer modeling, one-off observations of study participants, and indirect measures of physical activity and diet\u2014isn\u2019t set up to pinpoint the epidemic\u2019s causes.\n\n#### SIGN UP FOR THE AWARD-WINNING _SCIENCE_ ADVISER NEWSLETTER\n\nThe latest news, commentary, and research, free to your inbox daily\n\n[Sign up](https://www.science.org/action/clickThrough?id=501060&url=%2Fcontent%2Fpage%2Fscienceadviser%3Fintcmp%3Dnewsint-adviser%26utm_id%3DrecyGhsQrqyNFKTmt&loc=%2Fcontent%2Farticle%2Fnew-study-blames-diet-not-physical-inactivity-obesity-crisis&pubId=42142176&placeholderId=501022&productId=501030)\n\nPrevious research had shown that people in more industrialized societies tend to have a higher average body mass index and body fat percentage, though the trends aren\u2019t clear-cut. To explore the relative roles of diet and physical activity in shaping this pattern, a team led by Duke University evolutionary anthropologist Herman Pontzer analyzed a database that includes thousands of participants across six continents living in diverse societies and lifestyles.\n\nThe database contains energy expenditure measurements from those people\u2019s urine after they had drunk water molecularly tagged with heavy versions of oxygen and hydrogen that scientists can measure once they\u2019ve passed through the body. As we burn calories, some of the oxygen atoms in the water we drink are used to make the carbon dioxide we exhale. By measuring the excess of heavy hydrogen in a person\u2019s urine a few days after drinking the labeled water, scientists can estimate how much oxygen was turned into carbon dioxide\u2014and, thus, how much energy they burned.\n\nThe researchers also measured or estimated the fraction of a person\u2019s energy that was expended on basal functions such as breathing and regulating body temperature. They then subtracted this energy, along with an estimate of the energy needed for digestion, from a person\u2019s total energy expenditure to calculate calories burned during physical activity.\n\n#### Advertisement\n\nWhen the researchers calculated these measures across 34 populations around the world and adjusted for factors including age and body size, they found that people in more economically developed areas expended less total energy. Rural societies such as the Tuvan community in Siberia, for example, burned relatively more calories than people in the United States.\n\nHowever, the differences between populations were small and varied considerably, says study co-author Amanda McGrosky, an evolutionary anthropologist at Elon University. The researchers\u2019 model suggests this decrease was driven mainly by lower basal expenditure, not physical activity. McGrosky notes that previous work by Pontzer\u2019s team suggests people in less industrialized societies might have [higher metabolic rates](https://onlinelibrary.wiley.com/doi/10.1002/ajpa.23040) because they\u2019re exposed to more germs and thus have elevated immune activity.\n\nOverall, the researchers found that differences in total energy expenditure explained just 10% of the relationship between economic development and obesity measures. Their conclusion: The remaining 90% of the trend must be driven by excess energy intake.\n\nResearchers not involved in the work praised how the team used objective measures of energy expenditure and body fat to study differences among populations. The findings support a strong association between energy intake and obesity, says Jeff Goldsmith, a biostatistician and public health data scientist at Columbia University.\n\nHowever, the study\u2019s underlying database is globally uneven, containing data from thousands of people in economically developed societies but only from a few dozen people from hunter-gatherer or horticulturist communities. \u201cI do worry about trying to make broad claims,\u201d Goldsmith says, \u201cwhen the sample is skewed.\u201d\n\nIn addition, the work assumes that the energy used for digestion and basic metabolism can be extrapolated across populations, even though that might not be the case. The study\u2019s methods, though interesting, rely on \u201ca lot of assumptions stacked on assumptions,\u201d says Andrew Brown, a biostatistician at the University of Arkansas for Medical Sciences. \u201cIt very well could be \\[that\\] small changes in physical activity have outsized effects on energy intake itself.\u201d\n\nBecause the researchers don\u2019t have data on what individual participants ate, they can\u2019t tease out exactly what it is about the diets in more economically developed societies that could explain their higher obesity rates. The team did observe more body fat in people from populations that are known to eat more calorie-dense, ultraprocessed foods on average. But McGrosky stresses that this potential mechanism, among others, needs further study\u2014and adds that the team\u2019s results don\u2019t contradict the importance of exercise.\n\n\u201cPhysical activity is absolutely good for all-round health,\u201d she says. \u201cIt\u2019s just that it may not have the effects on your overall energy expenditure \\[as\\] previously thought.\u201d\n\ndoi: 10.1126/science.z9i2b0l\n\n#### Relevant tags:\n\n[Health](https://www.science.org/topic/category/health)\n\n## About the author\n\n#### [Catherine Offord](https://www.science.org/content/author/catherine-offord)\n\n##### Author\n\nCatherine Offord is a science journalist based in Barcelona, Spain.\n\n## More from news\n\n18 Jul 2025\n\n[Fearful of AI-generated grant proposals, NIH limits scientists to six applications per year](https://www.science.org/content/article/fearful-ai-generated-grant-proposals-nih-limits-scientists-six-applications-year)\n\nBy\n\n- Phie Jacobs\n\n18 Jul 2025\n\n[U.S. research community says new indirect cost model is still too complicated](https://www.science.org/content/article/u-s-research-community-says-new-indirect-cost-model-still-too-complicated)\n\nBy\n\n- Jeffrey Mervis\n\n18 Jul 2025\n\n[Why do some moms have more boys than girls\u2014or vice versa? New study provides clues](https://www.science.org/content/article/why-do-some-moms-have-more-boys-girls-or-vice-versa-new-study-provides-clues)\n\nBy\n\n- Nazeefa Ahmed\n\n[View more](https://www.science.org/news/all-news)\n\nGet _Science_\u2019s award-winning newsletter with the latest news, commentary, and research, free to your inbox daily.\n\n[Subscribe](https://www.science.org/content/page/scienceadviser?intcmp=popup-adviser&utm_id=recbEndseGq5WulpU) [Not Now](https://www.science.org/content/article/new-study-blames-diet-not-physical-inactivity-obesity-crisis)\n\n\u2713\n\nThanks for sharing!\n\n[AddToAny](https://www.addtoany.com)\n\n[More\u2026](https://www.science.org/content/article/new-study-blames-diet-not-physical-inactivity-obesity-crisis#addtoany)",
          "original_query": "Weight/Neuron Matching (permutation) approaches for revealing connectivity between trained networks",
          "cleaned_query": "Weight",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "GitHub - snimu/rebasin: Apply methods described in \"Git Re-basin\"-paper [1] to arbitrary models  ---   [1] Ainsworth et al. (https://arxiv.org/abs/2209.04836)",
          "url": "https://github.com/snimu/rebasin",
          "content": "GitHub - snimu/rebasin: Apply methods described in &quot;Git Re-basin&quot;-paper [1] to arbitrary models --- [1] Ainsworth et al. (https://arxiv.org/abs/2209.04836)\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/snimu/rebasin)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n \nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n \nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/snimu/rebasin)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=snimu/rebasin)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[snimu](https://github.com/snimu)/**[rebasin](https://github.com/snimu/rebasin)**Public\n* [Notifications](https://github.com/login?return_to=/snimu/rebasin)You must be signed in to change notification settings\n* [Fork1](https://github.com/login?return_to=/snimu/rebasin)\n* [Star15](https://github.com/login?return_to=/snimu/rebasin)\nApply methods described in \"Git Re-basin\"-paper [1] to arbitrary models --- [1] Ainsworth et al. ([https://arxiv.org/abs/2209.04836](https://arxiv.org/abs/2209.04836))\n### License\n[MIT license](https://github.com/snimu/rebasin/blob/main/LICENSE)\n[15stars](https://github.com/snimu/rebasin/stargazers)[1fork](https://github.com/snimu/rebasin/forks)[Branches](https://github.com/snimu/rebasin/branches)[Tags](https://github.com/snimu/rebasin/tags)[Activity](https://github.com/snimu/rebasin/activity)\n[Star](https://github.com/login?return_to=/snimu/rebasin)\n[Notifications](https://github.com/login?return_to=/snimu/rebasin)You must be signed in to change notification settings\n# snimu/rebasin\nmain\n[Branches](https://github.com/snimu/rebasin/branches)[Tags](https://github.com/snimu/rebasin/tags)\n[](https://github.com/snimu/rebasin/branches)[](https://github.com/snimu/rebasin/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[461 Commits](https://github.com/snimu/rebasin/commits/main/)\n[](https://github.com/snimu/rebasin/commits/main/)\n|\n[rebasin](https://github.com/snimu/rebasin/tree/main/rebasin)\n|\n[rebasin](https://github.com/snimu/rebasin/tree/main/rebasin)\n|\n|\n|\n[tests](https://github.com/snimu/rebasin/tree/main/tests)\n|\n[tests](https://github.com/snimu/rebasin/tree/main/tests)\n|\n|\n|\n[.gitignore](https://github.com/snimu/rebasin/blob/main/.gitignore)\n|\n[.gitignore](https://github.com/snimu/rebasin/blob/main/.gitignore)\n|\n|\n|\n[.pre-commit-config.yaml](https://github.com/snimu/rebasin/blob/main/.pre-commit-config.yaml)\n|\n[.pre-commit-config.yaml](https://github.com/snimu/rebasin/blob/main/.pre-commit-config.yaml)\n|\n|\n|\n[LICENSE](https://github.com/snimu/rebasin/blob/main/LICENSE)\n|\n[LICENSE](https://github.com/snimu/rebasin/blob/main/LICENSE)\n|\n|\n|\n[README.md](https://github.com/snimu/rebasin/blob/main/README.md)\n|\n[README.md](https://github.com/snimu/rebasin/blob/main/README.md)\n|\n|\n|\n[plans.md](https://github.com/snimu/rebasin/blob/main/plans.md)\n|\n[plans.md](https://github.com/snimu/rebasin/blob/main/plans.md)\n|\n|\n|\n[requirements-dev.txt](https://github.com/snimu/rebasin/blob/main/requirements-dev.txt)\n|\n[requirements-dev.txt](https://github.com/snimu/rebasin/blob/main/requirements-dev.txt)\n|\n|\n|\n[requirements.txt](https://github.com/snimu/rebasin/blob/main/requirements.txt)\n|\n[requirements.txt](https://github.com/snimu/rebasin/blob/main/requirements.txt)\n|\n|\n|\n[ruff.toml](https://github.com/snimu/rebasin/blob/main/ruff.toml)\n|\n[ruff.toml](https://github.com/snimu/rebasin/blob/main/ruff.toml)\n|\n|\n|\n[setup.cfg](https://github.com/snimu/rebasin/blob/main/setup.cfg)\n|\n[setup.cfg](https://github.com/snimu/rebasin/blob/main/setup.cfg)\n|\n|\n|\n[setup.py](https://github.com/snimu/rebasin/blob/main/setup.py)\n|\n[setup.py](https://github.com/snimu/rebasin/blob/main/setup.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# rebasin\n[](#rebasin)\n[![PyPI Version](https://camo.githubusercontent.com/2af74ac6d5c8f7f34aa23c09f34c1f96ecdfbd0c48470d3427c879e6a82a7e6e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7265626173696e)](https://camo.githubusercontent.com/2af74ac6d5c8f7f34aa23c09f34c1f96ecdfbd0c48470d3427c879e6a82a7e6e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7265626173696e)[![Wheel](https://camo.githubusercontent.com/c15d656bca8c4432c8c2d3cfd0611ed3bd56cb5585f9be5b0a9b718294712726/68747470733a2f2f696d672e736869656c64732e696f2f707970692f776865656c2f7265626173696e)](https://camo.githubusercontent.com/c15d656bca8c4432c8c2d3cfd0611ed3bd56cb5585f9be5b0a9b718294712726/68747470733a2f2f696d672e736869656c64732e696f2f707970692f776865656c2f7265626173696e)[![Python 3.8+](https://camo.githubusercontent.com/5d93683edb8c07343416dd43d001407612a076960545ee5518241b30bda34a29/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e382b2d626c75652e737667)](https://www.python.org/downloads/release/python-370/)[![License](https://camo.githubusercontent.com/14f41dd1cd429dee39c272c8d2a3a838debc1f76a8213ff540135f00d5a299ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f736e696d752f7265626173696e)](https://camo.githubusercontent.com/14f41dd1cd429dee39c272c8d2a3a838debc1f76a8213ff540135f00d5a299ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f736e696d752f7265626173696e)\nAn implementation of methods described in[\"Git Re-basin\"-paper by Ainsworth et al.](https://arxiv.org/abs/2209.04836)\nCan be applied to**arbitrary models**, without modification.\n(Well,*almost*arbitrary models, see[Limitations](#limitations)).\n**Table of Contents**\n* [Installation](#installation)\n* [Usage](#usage)\n* [Limitations](#limitations)\n* [Results](#results)\n* [Acknowledgements](#acknowledgements)\n## Installation\n[](#installation)\nRequirements should be automatically installed, but one of them is graphviz,\nwhich you might have to install per apt / brew / ... on your device.\nThe following install instructions are taken directly from[torchview's installation instructions](https://github.com/mert-kurttutan/torchview#installation).\nDebian-based Linux distro (e.g. Ubuntu):\n```\napt-get install graphviz\n```\nWindows:\n```\nchoco install graphviz\n```\nmacOS\n```\nbrew install graphviz\n```\nsee more details[here](https://graphviz.readthedocs.io/en/stable/manual.html).\nThen, install rebasin via pip:\n```\npip install rebasin\n```\n## Usage\n[](#usage)\nCurrently, only weight-matching is implemented as a method for rebasing,\nand only a simplified form of linear interpolation is implemented.\nThe following is a minimal example. For now, the documentation lives in the docstrings,\nthough I intend to create a proper one.`PermutationCoordinateDescent`and`interpolation.LerpSimple`are the main classes, beside`MergeMany`(see below).\n```\nfromrebasinimportPermutationCoordinateDescentfromrebasinimportinterpolationmodel\\_a,model\\_b,train\\_dl=...input\\_data=next(iter(train\\_dl))[0]# Rebasinpcd=PermutationCoordinateDescent(model\\_a,model\\_b,input\\_data)# weight-matchingpcd.rebasin()# Rebasin model\\_b towards model\\_a. Automatically updates model\\_b# Interpolatelerp=interpolation.LerpSimple(models=[model\\_a,model\\_b],devices=[\"cuda:0\",\"cuda:1\"],# Optional, defaults to cpudevice\\_interp=\"cuda:2\",# Optional, defaults to cpusavedir=\"/path/to/save/interpolation\"# Optional, save all interpolated models)lerp.interpolate(steps=99)# Interpolate 99 models between model\\_a and model\\_b\n```\nThe`MergeMany`-algorithm is also implemented\n(though there will be interface-changes regarding the devices in the future):\n```\nfromrebasinimportMergeManyfromtorchimportnnclassExampleModel(nn.Module):\n...model\\_a,model\\_b,model\\_c=ExampleModel(),ExampleModel(),ExampleModel()train\\_dl=...# Mergemerge=MergeMany(models=[model\\_a,model\\_b,model\\_c],working\\_model=ExampleModel(),input\\_data=next(iter(train\\_dl))[0],\n)merged\\_model=merge.run()# The merged model is also accessible through merge.working\\_model,# but only after merge.run() has been called.\n```\n## Terminology\n[](#terminology)\nIn this document, I will use the following terminology:\n* **To rebasin**: To apply one of the methods described in the paper to a model,\npermuting the rows and columns of its weights (and biases)\n* `model\\_a`: The model that stays unchanged\n* `model\\_b`: The model that is changed by rebasin it towards`model\\_a`\n* `model\\_b (original)`for the unchanged, original`model\\_b`\n* `model\\_b (rebasin)`for the changed, rebasined`model\\_b`\n* **Path**: A sequence of modules in a model\n## Limitations\n[](#limitations)\n### Only some methods are implemented\n[](#only-some-methods-are-implemented)\nFor rebasin, only weight-matching is implemented via`rebasin.PermutationCoordinateDescent`.\nFor interpolation, only a simplified method of linear interpolation is implemented\nvia`rebasin.interpolation.LerpSimple`.\n### Limitations of the`PermutationCoordinateDescent`-class\n[](#limitations-of-the-permutationcoordinatedescent-class)\nThe`PermutationCoordinateDescent`-class only permutes some Modules.\nMost modules should work",
          "original_query": "Symmetry\u2011aware rebasin / neuron alignment for deeper architectures (Ainsworth et al., 2022 and related)",
          "cleaned_query": "Symmetry\u2011aware rebasin",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Backward-Compatible Aligned Representations via an ...",
          "url": "https://www.arxiv.org/abs/2408.08793",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2408.08793** (cs)\n\n\\[Submitted on 16 Aug 2024\\]\n\n# Title:Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer\n\nAuthors: [Simone Ricci](https://arxiv.org/search/cs?searchtype=author&query=Ricci,+S), [Niccol\u00f2 Biondi](https://arxiv.org/search/cs?searchtype=author&query=Biondi,+N), [Federico Pernici](https://arxiv.org/search/cs?searchtype=author&query=Pernici,+F), [Alberto Del Bimbo](https://arxiv.org/search/cs?searchtype=author&query=Del+Bimbo,+A)\n\nView a PDF of the paper titled Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer, by Simone Ricci and 3 other authors\n\n[View PDF](https://www.arxiv.org/pdf/2408.08793) [HTML (experimental)](https://arxiv.org/html/2408.08793v1)\n\n> Abstract:Visual retrieval systems face significant challenges when updating models with improved representations due to misalignment between the old and new representations. The costly and resource-intensive backfilling process involves recalculating feature vectors for images in the gallery set whenever a new model is introduced. To address this, prior research has explored backward-compatible training methods that enable direct comparisons between new and old representations without backfilling. Despite these advancements, achieving a balance between backward compatibility and the performance of independently trained models remains an open problem. In this paper, we address it by expanding the representation space with additional dimensions and learning an orthogonal transformation to achieve compatibility with old models and, at the same time, integrate new information. This transformation preserves the original feature space's geometry, ensuring that our model aligns with previous versions while also learning new data. Our Orthogonal Compatible Aligned (OCA) approach eliminates the need for re-indexing during model updates and ensures that features can be compared directly across different model updates without additional mapping functions. Experimental results on CIFAR-100 and ImageNet-1k demonstrate that our method not only maintains compatibility with previous models but also achieves state-of-the-art accuracy, outperforming several existing methods.\n\n| | |\n| --- | --- |\n| Comments: | Accepted at BEW2024 Workshop at ECCV2024 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2408.08793](https://arxiv.org/abs/2408.08793) \\[cs.CV\\] |\n| | (or [arXiv:2408.08793v1](https://arxiv.org/abs/2408.08793v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2408.08793](https://doi.org/10.48550/arXiv.2408.08793) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Simone Ricci \\[ [view email](https://www.arxiv.org/show-email/6dda7ace/2408.08793)\\]\n\n**\\[v1\\]**\nFri, 16 Aug 2024 15:05:28 UTC (262 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer, by Simone Ricci and 3 other authors\n\n- [View PDF](https://www.arxiv.org/pdf/2408.08793)\n- [HTML (experimental)](https://arxiv.org/html/2408.08793v1)\n- [TeX Source](https://www.arxiv.org/src/2408.08793)\n- [Other Formats](https://www.arxiv.org/format/2408.08793)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://www.arxiv.org/prevnext?id=2408.08793&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://www.arxiv.org/prevnext?id=2408.08793&function=next&context=cs.CV)\n\n[new](https://www.arxiv.org/list/cs.CV/new) \\| [recent](https://www.arxiv.org/list/cs.CV/recent) \\| [2024-08](https://www.arxiv.org/list/cs.CV/2024-08)\n\nChange to browse by:\n\n[cs](https://www.arxiv.org/abs/2408.08793?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2408.08793)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2408.08793)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2408.08793)\n\n[a](https://www.arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://www.arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2408.08793&description=Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer) [![Reddit logo](https://www.arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2408.08793&title=Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://www.arxiv.org/auth/show-endorsers/2408.08793) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Representational alignment via orthogonal transformations (e.g., SVCCA / CKA / Orthogonal Procrustes literature)",
          "cleaned_query": "Representational alignment via orthogonal transformations (e.g., SVCCA",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "1. **Shift-Aware Linear Mode Connectivity via Explicit Gradient-Noise Control**  \n   Build on Paper 1\u2019s view of data shift as added stochastic gradient noise by designing an optimizer that estimates \u201cshift-induced noise\u201d online (e.g., via gradient variance decomposition across augmentations/domains) and adaptively schedules learning rate and batch size to preserve LMC. Evaluate whether maintaining LMC under controlled noise improves calibration and robustness compared to standard large-batch/low-LR recipes on synthetic and real shifts (e.g., ImageNet \u2192 ImageNet-C/R).",
        "**Weight-Sharing \u201cSoft Re-basing\u201d to Restore LMC in Convolutional Networks**  \n   Paper 2 indicates weight-sharing disrupts LMC; combine this with re-basin-style alignment (Paper 4) by introducing a learnable, structured permutation/assignment over channels and filter groups that approximately reindexes shared kernels across models. Test whether this \u201csoft re-basing\u201d yields low-loss linear paths between independently trained CNNs and whether it increases ensemble diversity without retraining.",
        "**Connectivity-Preserving Adam: Warmup-and-Noise Matched Adaptive Optimization**  \n   Paper 2 finds Adam breaks connectivity more than SGD but can be mitigated with warmup; extend this by proposing an Adam variant that matches SGD\u2019s effective noise scale (through controlled gradient preconditioning or injected noise) while keeping adaptivity. Benchmark LMC frequency, barrier height along interpolation, and OOD performance under data shifts (Paper 1) across MLPs/CNNs/ViTs.",
        "**OCA-Connected Retrieval Models: Linear Paths That Maintain Backward Compatibility**  \n   Combine OCA\u2019s orthogonal transformation layer for backward-compatible embeddings (Paper 5) with LMC by explicitly training two successive retrieval models such that both (a) the new model aligns to the old via an orthogonal layer and (b) the linear interpolation between their parameters stays low-loss. This yields a practical update mechanism where not only features are cross-version comparable, but intermediate checkpoints along the update trajectory remain deployable without backfilling.",
        "**Multi-Version \u201cFeature Git Re-basin\u201d for Incremental Indexes Without Backfilling**  \n   Extend re-basin tooling (Paper 4) from parameter alignment to *representation alignment across many model versions* by chaining orthogonal alignment modules (Paper 5) and periodically re-basing internal layers to keep a stable \u201ccanonical\u201d embedding coordinate system. Evaluate on long-horizon update sequences (10\u201350 versions) measuring retrieval accuracy, compatibility, and drift accumulation versus single-step OCA.",
        "**Connectivity as a Diagnostic for Dataset Complexity and Shift Severity**  \n   Paper 2 shows LMC breaks with dataset complexity; Paper 1 studies LMC under shifts\u2014turn this into a diagnostic tool: measure the probability and quality of LMC between multiple seeds as a scalar \u201clandscape connectivity index.\u201d Test whether this index predicts (i) downstream OOD generalization, (ii) calibration degradation under shift, and (iii) when larger ensembles are worth the compute.",
        "**Diversity-Optimized LMC Ensembles Under Shift (Error-Overlap Regularization on the Path)**  \n   Paper 1 notes LMC-sampled models tend to share errors; propose training/connectivity procedures that explicitly penalize error overlap among checkpoints sampled along an LMC path (e.g., disagreement regularization under shifted augmentations). The actionable output is an ensemble method that keeps LMC\u2019s training efficiency but achieves diversity comparable to multi-basin ensembles, improving shift robustness.",
        "**Localized vs Shared Connectivity Maps: Layerwise Interpolation to Pinpoint Where LMC Breaks**  \n   Based on Paper 2\u2019s architectural factorization (locality vs weight-sharing), perform *layerwise* and *blockwise* interpolation experiments (only interpolate specific layers while keeping others fixed) to identify which layers create interpolation barriers in CNNs/ViTs. Use findings to propose targeted architectural tweaks (e.g., partial unsharing, grouped sharing, or low-rank shared bases) that restore near-linear connectivity without sacrificing inductive bias.",
        "**Orthogonal Subspace Expansion for Shift-Robustness Without Sacrificing Compatibility**  \n   Paper 5 expands representation dimensions and applies an orthogonal transform; extend this to domain shift by reserving extra dimensions as \u201cshift-adaptation subspaces\u201d trained with robustness objectives (e.g., invariance penalties or distributionally robust losses), while the original subspace remains backward-compatible. Measure whether this reduces the need for costly full re-embedding in retrieval systems when the data distribution drifts.",
        "**Connectivity-Guided Checkpoint Selection for Efficient Deployment Under Distribution Drift**  \n   Use Paper 1\u2019s connectivity-efficiency trade-off to create an algorithm that, during training, selects a small set of checkpoints that are (a) mutually LMC-connected (for cheap interpolation/sampling) and (b) maximally cover different basins when connectivity breaks (Paper 2). The deliverable is a deployment recipe that automatically chooses when to rely on LMC sampling versus when to branch into true multi-basin ensembles to optimize robustness-per-FLOP under anticipated shifts."
      ],
      "generated_ideas_raw": "1. **Shift-Aware Linear Mode Connectivity via Explicit Gradient-Noise Control**  \n   Build on Paper 1\u2019s view of data shift as added stochastic gradient noise by designing an optimizer that estimates \u201cshift-induced noise\u201d online (e.g., via gradient variance decomposition across augmentations/domains) and adaptively schedules learning rate and batch size to preserve LMC. Evaluate whether maintaining LMC under controlled noise improves calibration and robustness compared to standard large-batch/low-LR recipes on synthetic and real shifts (e.g., ImageNet \u2192 ImageNet-C/R).\n\n2. **Weight-Sharing \u201cSoft Re-basing\u201d to Restore LMC in Convolutional Networks**  \n   Paper 2 indicates weight-sharing disrupts LMC; combine this with re-basin-style alignment (Paper 4) by introducing a learnable, structured permutation/assignment over channels and filter groups that approximately reindexes shared kernels across models. Test whether this \u201csoft re-basing\u201d yields low-loss linear paths between independently trained CNNs and whether it increases ensemble diversity without retraining.\n\n3. **Connectivity-Preserving Adam: Warmup-and-Noise Matched Adaptive Optimization**  \n   Paper 2 finds Adam breaks connectivity more than SGD but can be mitigated with warmup; extend this by proposing an Adam variant that matches SGD\u2019s effective noise scale (through controlled gradient preconditioning or injected noise) while keeping adaptivity. Benchmark LMC frequency, barrier height along interpolation, and OOD performance under data shifts (Paper 1) across MLPs/CNNs/ViTs.\n\n4. **OCA-Connected Retrieval Models: Linear Paths That Maintain Backward Compatibility**  \n   Combine OCA\u2019s orthogonal transformation layer for backward-compatible embeddings (Paper 5) with LMC by explicitly training two successive retrieval models such that both (a) the new model aligns to the old via an orthogonal layer and (b) the linear interpolation between their parameters stays low-loss. This yields a practical update mechanism where not only features are cross-version comparable, but intermediate checkpoints along the update trajectory remain deployable without backfilling.\n\n5. **Multi-Version \u201cFeature Git Re-basin\u201d for Incremental Indexes Without Backfilling**  \n   Extend re-basin tooling (Paper 4) from parameter alignment to *representation alignment across many model versions* by chaining orthogonal alignment modules (Paper 5) and periodically re-basing internal layers to keep a stable \u201ccanonical\u201d embedding coordinate system. Evaluate on long-horizon update sequences (10\u201350 versions) measuring retrieval accuracy, compatibility, and drift accumulation versus single-step OCA.\n\n6. **Connectivity as a Diagnostic for Dataset Complexity and Shift Severity**  \n   Paper 2 shows LMC breaks with dataset complexity; Paper 1 studies LMC under shifts\u2014turn this into a diagnostic tool: measure the probability and quality of LMC between multiple seeds as a scalar \u201clandscape connectivity index.\u201d Test whether this index predicts (i) downstream OOD generalization, (ii) calibration degradation under shift, and (iii) when larger ensembles are worth the compute.\n\n7. **Diversity-Optimized LMC Ensembles Under Shift (Error-Overlap Regularization on the Path)**  \n   Paper 1 notes LMC-sampled models tend to share errors; propose training/connectivity procedures that explicitly penalize error overlap among checkpoints sampled along an LMC path (e.g., disagreement regularization under shifted augmentations). The actionable output is an ensemble method that keeps LMC\u2019s training efficiency but achieves diversity comparable to multi-basin ensembles, improving shift robustness.\n\n8. **Localized vs Shared Connectivity Maps: Layerwise Interpolation to Pinpoint Where LMC Breaks**  \n   Based on Paper 2\u2019s architectural factorization (locality vs weight-sharing), perform *layerwise* and *blockwise* interpolation experiments (only interpolate specific layers while keeping others fixed) to identify which layers create interpolation barriers in CNNs/ViTs. Use findings to propose targeted architectural tweaks (e.g., partial unsharing, grouped sharing, or low-rank shared bases) that restore near-linear connectivity without sacrificing inductive bias.\n\n9. **Orthogonal Subspace Expansion for Shift-Robustness Without Sacrificing Compatibility**  \n   Paper 5 expands representation dimensions and applies an orthogonal transform; extend this to domain shift by reserving extra dimensions as \u201cshift-adaptation subspaces\u201d trained with robustness objectives (e.g., invariance penalties or distributionally robust losses), while the original subspace remains backward-compatible. Measure whether this reduces the need for costly full re-embedding in retrieval systems when the data distribution drifts.\n\n10. **Connectivity-Guided Checkpoint Selection for Efficient Deployment Under Distribution Drift**  \n   Use Paper 1\u2019s connectivity-efficiency trade-off to create an algorithm that, during training, selects a small set of checkpoints that are (a) mutually LMC-connected (for cheap interpolation/sampling) and (b) maximally cover different basins when connectivity breaks (Paper 2). The deliverable is a deployment recipe that automatically chooses when to rely on LMC sampling versus when to branch into true multi-basin ensembles to optimize robustness-per-FLOP under anticipated shifts.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Shift-Aware Linear Mode Connectivity via Explicit Gradient-Noise Control**  \n   Build on Paper 1\u2019s view of data shift as added stochastic gradient noise by designing an optimizer that estimates \u201c",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Weight-Sharing \u201cSoft Re-basing\u201d to Restore LMC in Convolutional Networks**  \n   Paper 2 indicates weight-sharing disrupts LMC; combine this with re-basin-style alignment (Paper 4) by introducing a l",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Connectivity-Preserving Adam: Warmup-and-Noise Matched Adaptive Optimization**  \n   Paper 2 finds Adam breaks connectivity more than SGD but can be mitigated with warmup; extend this by proposing an",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**OCA-Connected Retrieval Models: Linear Paths That Maintain Backward Compatibility**  \n   Combine OCA\u2019s orthogonal transformation layer for backward-compatible embeddings (Paper 5) with LMC by explic",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Multi-Version \u201cFeature Git Re-basin\u201d for Incremental Indexes Without Backfilling**  \n   Extend re-basin tooling (Paper 4) from parameter alignment to *representation alignment across many model vers",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Connectivity as a Diagnostic for Dataset Complexity and Shift Severity**  \n   Paper 2 shows LMC breaks with dataset complexity; Paper 1 studies LMC under shifts\u2014turn this into a diagnostic tool: mea",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Diversity-Optimized LMC Ensembles Under Shift (Error-Overlap Regularization on the Path)**  \n   Paper 1 notes LMC-sampled models tend to share errors; propose training/connectivity procedures that e",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Localized vs Shared Connectivity Maps: Layerwise Interpolation to Pinpoint Where LMC Breaks**  \n   Based on Paper 2\u2019s architectural factorization (locality vs weight-sharing), perform *layerwise* an",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Orthogonal Subspace Expansion for Shift-Robustness Without Sacrificing Compatibility**  \n   Paper 5 expands representation dimensions and applies an orthogonal transform; extend this to domain shift",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Connectivity-Guided Checkpoint Selection for Efficient Deployment Under Distribution Drift**  \n   Use Paper 1\u2019s connectivity-efficiency trade-off to create an algorithm that, during training, select",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 1,
      "paper_title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
      "contribution": "Introduce a compositional phase-domain diffusion framework (with ACT-PAE, SPDM and TPDM) that denoises semantic and transition-aware phase latents so long multi-segment motion sequences are both semantically aligned and smoothly transitioned.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 11020,
      "output_tokens": 904,
      "predecessor_details": [
        {
          "success": true,
          "title": "DeepPhase: periodic autoencoders for learning motion phase ...",
          "url": "https://dl.acm.org/doi/10.1145/3528223.3530178",
          "content": "DeepPhase: periodic autoencoders for learning motion phase manifolds: ACM Transactions on Graphics: Vol 41, No 4[skip to main content](#skip-to-main-content)\n[](#global-menu)\nSearch ACM Digital Library\nSearchSearch\n[Advanced Search](https://dl.acm.org/search/advanced)\n[ACM Transactions on Graphics](#)\n**## Export Citations\nSelect Citation formatBibTeXEndNoteACM Ref**\n* Please download or close your previous search result export first before starting a new bulk export.\nPreview is not available.\nBy clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.\n* ```\n```\n* [Download citation**](javascript:void(0))\n* [Copy citation**](javascript:void(0))\nresearch-article\nShare on\n* **\n* **\n* **\n* **\n* **\n# DeepPhase:periodic autoencoders for learning motion phase manifolds\nAuthors:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)SebastianStarke](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)IanMason](#artseq-00002),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)TakuKomura](#artseq-00003)[Authors Info &amp; Claims](#tab-contributors)\n[ACM Transactions on Graphics (TOG),Volume41,Issue4](https://dl.acm.org/toc/tog/2022/41/4)\nArticle No.: 136, Pages1-13\n[https://doi.org/10.1145/3528223.3530178](https://doi.org/10.1145/3528223.3530178)\nPublished:22 July 2022[Publication History](#core-history)[](#)\n**120citation**6,643Downloads\nMetrics\n[\nTotal Citations120\n](#tab-citations)[\nTotal Downloads6,643\n](#tab-metrics-inner)\nLast 12 Months1,235\nLast 6 weeks125\n**Get Citation Alerts\n**## New Citation Alert added!\nThis alert has been successfully added and will be sent to:\nYou will be notified whenever a record that you have chosen has been cited.\nTo manage your alert preferences, click on the button below.\n[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)\n**## New Citation Alert!\nPlease[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.1145/3528223.3530178)\n**\n**\n[**Get Access](#core-collateral-purchase-access)\n**Contents\n## Abstract\nLearning the spatial-temporal structure of body movements is a fundamental problem for character motion synthesis. In this work, we propose a novel neural network architecture called the Periodic Autoencoder that can learn periodic features from large unstructured motion datasets in an unsupervised manner. The character movements are decomposed into multiple latent channels that capture the non-linear periodicity of different body segments while progressing forward in time. Our method extracts a multi-dimensional phase space from full-body motion data, which effectively clusters animations and produces a manifold in which computed feature distances provide a better similarity measure than in the original motion space to achieve better temporal and spatial alignment. We demonstrate that the learned periodic embedding can significantly help to improve neural motion synthesis in a number of tasks, including diverse locomotion skills, style-based movements, dance motion synthesis from music, synthesis of dribbling motions in football, and motion query for matching poses within large animation databases.\n## Supplemental Material\nMP4 File\nsupplemental material\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3528223.3530178/suppl_file/136-732-supp-video.mp4)\n* 483.02 MB\nMP4 File\npresentation\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3528223.3530178/suppl_file/3528223.3530178.mp4)\n* 1671.05 MB\nSRT File\npresentation\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3528223.3530178/suppl_file/3528223.3530178.srt)\n* 25.40 KB\nZIP File\nsupplemental material\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3528223.3530178/suppl_file/136-732-supp-mtl.zip)\n* 343.08 MB\n## References\n[1]\nOkan Arikan and David A Forsyth. 2002. Interactive motion generation from examples.*ACM Trans on Graph*21, 3 (2002), 483--490.\n[Digital Library](https://dl.acm.org/doi/10.1145/566654.566606)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/566654.566606)\n[2]\nPhilippe Beaudoin, Pierre Poulin, and Michiel van de Panne. 2007. Adapting wavelet compression to human motion capture clips. In*Proceedings of Graphics Interface 2007.*313--318.\n[Google Scholar](https://scholar.google.com/scholar?q=Philippe+Beaudoin,+Pierre+Poulin,+and+Michiel+van+de+Panne.+2007.+Adapting+wavelet+compression+to+human+motion+capture+clips.+In+Proceedings+of+Graphics+Interface+2007.+313--318.)\n[3]\nKevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes. 2019. DReCon: data-driven responsive control of physics-based characters.*ACM Transactions on Graphics (TOG)*38, 6 (2019), 1--11.\n[Digital Library](https://dl.acm.org/doi/10.1145/3355089.3356536)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3355089.3356536)\n[4]\nArmin Bruderlin and Lance Williams. 1995. Motion signal processing. In*Proceedings of the 22nd annual conference on Computer graphics and interactive techniques.*97--104.\n[Digital Library](https://dl.acm.org/doi/10.1145/218380.218421)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/218380.218421)\n[5]\nKyungmin Cho, Chaelin Kim, Jungjin Park, Joonkyu Park, and Junyong Noh. 2021. Motion recommendation for online character control.*ACM Transactions on Graphics (TOG)*40, 6 (2021), 1--16.\n[Digital Library](https://dl.acm.org/doi/10.1145/3478513.3480512)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3478513.3480512)\n[6]\nSimon Clavet. 2016. Motion matching and the road to next-gen animation. In*Proc. of GDC.*\n[Google Scholar](https://scholar.google.com/scholar?q=Simon+Clavet.+2016.+Motion+matching+and+the+road+to+next-gen+animation.+In+Proc.+of+GDC.)\n[7]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).*4171--4186.\n[Crossref](https://doi.org/10.18653/v1/N19-1423)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.18653/v1/N19-1423)\n[8]\nMilan R Dimitrijevic, Yuri Gerasimenko, and Michaela M Pinter. 1998. Evidence for a spinal central pattern generator in humans.*Annals of the New York Academy of Sciences*860, 1 (1998), 360--376.\n[Crossref](https://doi.org/10.1111/j.1749-6632.1998.tb09062.x)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1111/j.1749-6632.1998.tb09062.x)\n[9]\nLevi Fussell, Kevin Bergamin, and Daniel Holden. 2021. SuperTrack: motion tracking for physically simulated characters using supervised learning.*ACM Transactions on Graphics (TOG)*40, 6 (2021), 1--13.\n[Digital Library](https://dl.acm.org/doi/10.1145/3478513.3480527)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3478513.3480527)\n[10]\nF\u00e9lix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. 2020. Robust motion in-betweening.*ACM Transactions on Graphics (TOG)*39, 4 (2020), 60--1.\n[Digital Library](https://dl.acm.org/doi/10.1145/3386569.3392480)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3386569.3392480)\n[11]\nRachel Heck and Michael Gleicher. 2007. Parametric motion graphs. In*Proc. I3D.*129--136.\n[Digital Library](https://dl.acm.org/doi/10.1145/1230100.1230123)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/1230100.1230123)\n[12]\nGustav Eje Henter, Simon Alexanderson, and Jonas Beskow. 2020. Moglow: Probabilistic and controllable motion synthesis using normalising flows.*ACM Transactions on Graphics (TOG)*39, 6 (2020), 1--14.\n[Digital Library](https://dl.acm.org/doi/10.1145/3414685.3417836)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3414685.3417836)\n[13]\nJonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning.*Advances in neural information processing systems*29 (2016), 4565--4573.\n[Google Scholar](https://scholar.google.com/scholar?q=Jonathan+Ho+and+Stefano+Ermon.+2016.+Generative+adversarial+imitation+learning.+Advances+in+neural+information+processing+systems+29+(2016),+4565--4573.)\n[14]\nDaniel Holden, Taku Komura, and Jun Saito. 2017. Phase-functioned neural networks for character control.*ACM Trans on Graph*36, 4 (2017), 42.\n[Digital Library](https://dl.acm.org/doi/10.1145/3072959.3073663)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3072959.3073663)\n[15]\nDaniel Holden, Jun Saito, and Taku Komura. 2016. A deep learning framework for character motion synthesis and editing.*ACM Trans on Graph*35, 4 (2016), 138.\n[Digital Library](https://dl.acm.org/doi/10.1145/2897824.2925975)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/2897824.2925975)\n[16]\nDaniel Holden, Jun Saito, Taku Komura, and Thomas Joyce. 2015. Learning motion manifolds with convolutional autoencoders. In*SIGGRAPH Asia 2015 Technical Briefs.*ACM, 18.\n[Google Scholar](https://scholar.google.com/scholar?q=Daniel+Holden,+Jun+Saito,+Taku+Komura,+and+Thomas+Joyce.+2015.+Learning+motion+manifolds+with+convolutional+autoencoders.+In+SIGGRAPH+Asia+2015+Technical+Briefs.+ACM,+18.)\n[17]\nLucas Kovar and Michael Gleicher. 2004. Automated Extraction and Parameterization of Motions in Large Data Sets.*ACM Trans on Graph*23, 3 (2004), 559--568.\n[Digital Library](https://dl.acm.org/doi/10.1145/1015706.1015760)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/1015706.1015760)\n[18]\nLucas Kovar, Michael Gleicher, and Fr\u00e9d\u00e9ric Pighin. 2008. Motion graphs. In*ACM SIGGRAPH 2008 classes.*1--10.\n[Digital Library](https://dl.acm.org/doi/10.1145/1401132.1401202)\n[Goog",
          "original_query": "Deepphase: Periodic autoencoders for learning motion phase manifolds",
          "cleaned_query": "Deepphase: Periodic autoencoders for learning motion phase manifolds",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "DiffusionPhase: Motion Diffusion in Frequency Domain",
          "url": "https://phaselanguagemotion.weilinwl.com/",
          "content": "DiffusionPhase: Motion Diffusion in Frequency Domain\nDiffusionPhase: Motion Diffusion in Frequency Domain\n[Weilin Wan](https://weilinwl.com)1, 2\\* ,[Yiming Huang](https://www.grasp.upenn.edu/people/yiming-huang/)2\\*,[Shutong Wu](https://www.linkedin.com/in/shutong-wu-214043172)2,[Taku Komura](https://www.cs.hku.hk/index.php/people/academic-staff/taku)1,[Wenping Wang](https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html)3,[Dinesh Jayaraman](https://www.seas.upenn.edu/~dineshj/)2,[Lingjie Liu](https://lingjie0206.github.io)2\n1The University of Hong Kong2University of Pennsylvania3Texas A&M University\n[**Paper](https://arxiv.org/abs/2312.04036)[**Video](https://youtu.be/C-GosksuLMU)[**Github](https://github.com/HiWilliamWWL/PhaseLanguageMotion)\n# Introduction\nIn this study, we introduce a learning-based method for generating high-quality human motion sequences from text descriptions (e.g., ``A person walks forward\"). Existing techniques struggle with motion diversity and smooth transitions due to limited text-to-motion datasets and reliance on full-body skeletal pose representations. To address this, we develop a network encoder that converts motion sequences into periodic signals, capturing the local periodicity of motions in time and space. We also propose a conditional diffusion model for predicting periodic motion parameters based on text descriptions and the starting pose. Our approach outperforms current methods, generating a broader variety of high-quality motions with natural transitions, especially in longer sequences.|\nHTML5 video not supportedHTML5 video not supported\nHTML5 video not supported\n# Method Overview\n![](./resources/pipeline_new.png)| \n Method Overview: (a) First, we conduct a preprocessing stage to isolate the periodic and non-periodic segments in the motion sequences. (b)-(c) Using the preprocessed movements, we then learn a network encoder to transform the motion space into a learned periodic parameterized phase space by minimizing the reconstruction errors between the original motions and the motions formed by decoding periodic parameters via Inverse FFT. (d) Next, we train a conditional diffusion model to predict the periodic parameters with a text prompt and a starting pose as inputs. During inference time, given a text prompt and a starting pose, we apply diffusion to predict the periodic parameters and then decode the motion from the signal.|\n# Video\n[![Embedded YouTube video](https://img.youtube.com/vi/C-GosksuLMU/0.jpg)](https://www.youtube.com/watch?v=C-GosksuLMU)\n# Citation\n```\n@article{wan2023diffusionphase,\ntitle={DiffusionPhase: Motion Diffusion in Frequency Domain},\nauthor={Wan, Weilin and Huang, Yiming and Wu, Shutong and Komura, Taku and Wang, Wenping and Jayaraman, Dinesh and Liu, Lingjie},\njournal={arXiv preprint arXiv:2312.04036},\nyear={2023}\n}\n```\n|\nThanks[Richard Zhang](http://richzhang.github.io/)for the website[template](https://github.com/richzhang/webpage-template).",
          "original_query": "Diffusionphase: Motion diffusion in frequency domain",
          "cleaned_query": "Diffusionphase: Motion diffusion in frequency domain",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "MDM: Human Motion Diffusion Model",
          "url": "https://guytevet.github.io/mdm-page/",
          "content": "# MDM: Human Motion Diffusion Model\n\nICLR2023 (Top-25%)\n\n[Guy Tevet](https://guytevet.github.io/),[Sigal Raab](https://sigal-raab.github.io/),[Brian Gordon](https://scholar.google.com/citations?user=VNDhTycAAAAJ&hl=en),[Yonatan Shafir](https://www.linkedin.com/in/yonatan-shafir-2811a3148/),[Daniel Cohen-Or](https://danielcohenor.com/),\n[Amit H. Bermano](https://www.cs.tau.ac.il/~amberman/)\n\nTel Aviv University, Israel\n\n[arXiv](https://arxiv.org/abs/2209.14916)[Code](https://github.com/GuyTevet/motion-diffusion-model)[Demo](https://replicate.com/arielreplicate/motion_diffusion_model)\n\n## Abstract\n\nNatural and expressive human motion generation is the holy grail of computer animation.\nIt is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness.\nDiffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control.\nIn this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain.\nMDM is transformer-based, combining insights from motion generation literature.\nA notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion.\n\nThe MDM framework has a generic design enabling different forms of conditioning.\nWe showcase three tasks: text-to-motion, action-to-motion, and unconditioned generation.\nWe train the model in a classifier-free manner, which enables trading-off diversity to fidelity, and sampling both conditionally and unconditionally from the same model.\nIn the text-to-motion task, our model generates coherent motions that achieve state-of-the-art results on the HumanML3D and KIT benchmarks.\nMoreover, our user study shows that human evaluators\nprefer our generated motions over real motions 42% of the time.\nIn action-to-motion, MDM outperforms the state-of-the-art, even though they were specifically designed for this task,\non the common HumanAct12 and UESTC benchmarks.\n\n**\u201cA person walks forward, bends down to pick something up off the ground.\u201d**\n\n**\u201ca person turns to his right and paces back and forth.\u201d**\n\n**\u201cA person punches in a manner consistent with martial arts.\u201d**\n\n## Text-to-Motion\n\nText-to-motion is the task of generating motion given an input text prompt.\nThe output motion is expected to be both implementing the textual description, and a valid sample from the data distribution (i.e. adhering to general human abilities and the rules of physics).\nIn addition, for each text prompt, we also expect a distribution of motions matching it, rather than just a single result.\n\n**\u201cA person kicks.\u201d**\n\n**\u201cA person kicks.\u201d**\n\n**\u201cA person kicks.\u201d**\n\n**\u201cA person kicks.\u201d**\n\n**\u201cA person kicks.\u201d**\n\n**\u201cA person kicks.\u201d**\n\n**\u201cA person kicks.\u201d**\n\n**\u201cA person kicks.\u201d**\n\n**\u201cA person kicks.\u201d**\n\n**\u201cA person is skipping rope.\u201d**\n\n**\u201cA person is skipping rope.\u201d**\n\n**\u201cA person is skipping rope.\u201d**\n\n**\u201cA person is skipping rope.\u201d**\n\n**\u201cA person is skipping rope.\u201d**\n\n**\u201cA person is skipping rope.\u201d**\n\n**\u201cA person is skipping rope.\u201d**\n\n**\u201cA person is skipping rope.\u201d**\n\n## Action-to-Motion\n\nAction-to-motion is the task of generating motion given an input action class, represented by a scalar.\nThe output motion should faithfully animate the input action, and at the same time be natural and reflect the distribution of the dataset on which the model is trained.\n\n**(Class) Run**\n\n**(Class) Warm up**\n\n**(Class) Sit**\n\n**(Class) Jump**\n\n## Motion Editing\n\nWe also demonstrate completion and editing. By adapting diffusion image-inpainting, we set a motion prefix and suffix, and use our model to fill in the gap.\nDoing so under a textual condition guides MDM to fill the gap with a specific motion that still maintains the semantics of the original input.\nBy performing inpainting in the joints space rather than temporally, we also demonstrate the semantic editing of specific body parts, without changing the others.\n\n### Upper body editing (lower body is fixed)\n\n(Blue=Input, Gold=Synthesis)\n\n### Motion In-Betweening\n\n(Blue=Input, Gold=Synthesis)\n\n## BibTeX\n\n```\n\n@inproceedings{\ntevet2023human,\ntitle={Human Motion Diffusion Model},\nauthor={Guy Tevet and Sigal Raab and Brian Gordon and Yoni Shafir and Daniel Cohen-or and Amit Haim Bermano},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=SJ1kSyO2jwu}\n}\n```",
          "original_query": "Human motion diffusion model (MDM)",
          "cleaned_query": "Human motion diffusion model (MDM)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2303.01418] Human Motion Diffusion as a Generative Prior - arXiv",
          "url": "https://arxiv.org/abs/2303.01418",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2303.01418** (cs)\n\n\\[Submitted on 2 Mar 2023 ( [v1](https://arxiv.org/abs/2303.01418v1)), last revised 30 Aug 2023 (this version, v3)\\]\n\n# Title:Human Motion Diffusion as a Generative Prior\n\nAuthors: [Yonatan Shafir](https://arxiv.org/search/cs?searchtype=author&query=Shafir,+Y), [Guy Tevet](https://arxiv.org/search/cs?searchtype=author&query=Tevet,+G), [Roy Kapon](https://arxiv.org/search/cs?searchtype=author&query=Kapon,+R), [Amit H. Bermano](https://arxiv.org/search/cs?searchtype=author&query=Bermano,+A+H)\n\nView a PDF of the paper titled Human Motion Diffusion as a Generative Prior, by Yonatan Shafir and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2303.01418)\n\n> Abstract:Recent work has demonstrated the significant potential of denoising diffusion models for generating human motion, including text-to-motion capabilities. However, these methods are restricted by the paucity of annotated motion data, a focus on single-person motions, and a lack of detailed control. In this paper, we introduce three forms of composition based on diffusion priors: sequential, parallel, and model composition. Using sequential composition, we tackle the challenge of long sequence generation. We introduce DoubleTake, an inference-time method with which we generate long animations consisting of sequences of prompted intervals and their transitions, using a prior trained only for short clips. Using parallel composition, we show promising steps toward two-person generation. Beginning with two fixed priors as well as a few two-person training examples, we learn a slim communication block, ComMDM, to coordinate interaction between the two resulting motions. Lastly, using model composition, we first train individual priors to complete motions that realize a prescribed motion for a given joint. We then introduce DiffusionBlending, an interpolation mechanism to effectively blend several such models to enable flexible and efficient fine-grained joint and trajectory-level control and editing. We evaluate the composition methods using an off-the-shelf motion diffusion model, and further compare the results to dedicated models trained for these specific tasks.\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR) |\n| Cite as: | [arXiv:2303.01418](https://arxiv.org/abs/2303.01418) \\[cs.CV\\] |\n| | (or [arXiv:2303.01418v3](https://arxiv.org/abs/2303.01418v3) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2303.01418](https://doi.org/10.48550/arXiv.2303.01418) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Guy Tevet \\[ [view email](https://arxiv.org/show-email/a2d593de/2303.01418)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2303.01418v1)**\nThu, 2 Mar 2023 17:09:27 UTC (19,793 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2303.01418v2)**\nThu, 17 Aug 2023 00:24:41 UTC (19,761 KB)\n\n**\\[v3\\]**\nWed, 30 Aug 2023 04:41:10 UTC (1,801 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Human Motion Diffusion as a Generative Prior, by Yonatan Shafir and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2303.01418)\n- [TeX Source](https://arxiv.org/src/2303.01418)\n- [Other Formats](https://arxiv.org/format/2303.01418)\n\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2303.01418&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2303.01418&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2023-03](https://arxiv.org/list/cs.CV/2023-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2303.01418?context=cs)\n\n[cs.GR](https://arxiv.org/abs/2303.01418?context=cs.GR)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2303.01418)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2303.01418)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2303.01418)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2303.01418&description=Human Motion Diffusion as a Generative Prior) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2303.01418&title=Human Motion Diffusion as a Generative Prior)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2303.01418) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Human motion diffusion as a generative prior (priorMDM)",
          "cleaned_query": "Human motion diffusion as a generative prior (priorMDM)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Temporal Action Compositions for 3D Humans\" [3DV 2022] - GitHub",
          "url": "https://github.com/atnikos/teach",
          "content": "[Skip to content](https://github.com/atnikos/teach#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n{{ message }}\n\n[atnikos](https://github.com/atnikos)/ **[teach](https://github.com/atnikos/teach)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fatnikos%2Fteach) You must be signed in to change notification settings\n- [Fork\\\n41](https://github.com/login?return_to=%2Fatnikos%2Fteach)\n- [Star\\\n393](https://github.com/login?return_to=%2Fatnikos%2Fteach)\n\n\nOfficial PyTorch implementation of the paper \"TEACH: Temporal Action Compositions for 3D Humans\" \\[3DV 2022\\]\n\n[teach.is.tue.mpg.de](https://teach.is.tue.mpg.de)\n\n### License\n\n[View license](https://github.com/atnikos/teach/blob/main/LICENSE)\n\n[393\\\nstars](https://github.com/atnikos/teach/stargazers) [41\\\nforks](https://github.com/atnikos/teach/forks) [Branches](https://github.com/atnikos/teach/branches) [Tags](https://github.com/atnikos/teach/tags) [Activity](https://github.com/atnikos/teach/activity)\n\n[Star](https://github.com/login?return_to=%2Fatnikos%2Fteach)\n\n[Notifications](https://github.com/login?return_to=%2Fatnikos%2Fteach) You must be signed in to change notification settings\n\n# atnikos/teach\n\nmain\n\n[Branches](https://github.com/atnikos/teach/branches) [Tags](https://github.com/atnikos/teach/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit ## History [55 Commits](https://github.com/atnikos/teach/commits/main/) |\n| [assets](https://github.com/atnikos/teach/tree/main/assets) | [assets](https://github.com/atnikos/teach/tree/main/assets) | | |\n| [configs](https://github.com/atnikos/teach/tree/main/configs) | [configs](https://github.com/atnikos/teach/tree/main/configs) | | |\n| [deps](https://github.com/atnikos/teach/tree/main/deps) | [deps](https://github.com/atnikos/teach/tree/main/deps) | | |\n| [nlp\\_actions](https://github.com/atnikos/teach/tree/main/nlp_actions) | [nlp\\_actions](https://github.com/atnikos/teach/tree/main/nlp_actions) | | |\n| [scripts](https://github.com/atnikos/teach/tree/main/scripts) | [scripts](https://github.com/atnikos/teach/tree/main/scripts) | | |\n| [teach](https://github.com/atnikos/teach/tree/main/teach) | [teach](https://github.com/atnikos/teach/tree/main/teach) | | |\n| [.gitignore](https://github.com/atnikos/teach/blob/main/.gitignore) | [.gitignore](https://github.com/atnikos/teach/blob/main/.gitignore) | | |\n| [LICENSE](https://github.com/atnikos/teach/blob/main/LICENSE) | [LICENSE](https://github.com/atnikos/teach/blob/main/LICENSE) | | |\n| [README.md](https://github.com/atnikos/teach/blob/main/README.md) | [README.md](https://github.com/atnikos/teach/blob/main/README.md) | | |\n| [canonicalize\\_motion.py](https://github.com/atnikos/teach/blob/main/canonicalize_motion.py) | [canonicalize\\_motion.py](https://github.com/atnikos/teach/blob/main/canonicalize_motion.py) | | |\n| [compute\\_stats.py](https://github.com/atnikos/teach/blob/main/compute_stats.py) | [compute\\_stats.py](https://github.com/atnikos/teach/blob/main/compute_stats.py) | | |\n| [compute\\_td.py](https://github.com/atnikos/teach/blob/main/compute_td.py) | [compute\\_td.py](https://github.com/atnikos/teach/blob/main/compute_td.py) | | |\n| [eval.py](https://github.com/atnikos/teach/blob/main/eval.py) | [eval.py](https://github.com/atnikos/teach/blob/main/eval.py) | | |\n| [interact\\_teach.py](https://github.com/atnikos/teach/blob/main/interact_teach.py) | [interact\\_teach.py](https://github.com/atnikos/teach/blob/main/interact_teach.py) | | |\n| [render.py](https://github.com/atnikos/teach/blob/main/render.py) | [render.py](https://github.com/atnikos/teach/blob/main/render.py) | | |\n| [requirements.txt](https://github.com/atnikos/teach/blob/main/requirements.txt) | [requirements.txt](https://github.com/atnikos/teach/blob/main/requirements.txt) | | |\n| [sample\\_seq.py](https://github.com/atnikos/teach/blob/main/sample_seq.py) | [sample\\_seq.py](https://github.com/atnikos/teach/blob/main/sample_seq.py) | | |\n| [train.py](https://github.com/atnikos/teach/blob/main/train.py) | [train.py](https://github.com/atnikos/teach/blob/main/train.py) | | |\n| View all files |\n\n## Repository files navigation\n\n# TEACH: Temporal Action Compositions for 3D Humans [![ArXiv PDF](https://camo.githubusercontent.com/1643c0acf6341364baa48b06a2463d7fdb3ebf9457b806d3401b3de798b24b47/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61727869762d7265706f72742d726564)](https://arxiv.org/abs/2209.04066)[![Project Page](https://camo.githubusercontent.com/38be6316abc8fc9e8ff833ac1c8b63fb8cccb66e9ff95869aaf6ee1c2bb8d8d4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50726f6a6563742d506167652d626c75653f7374796c653d666c6174266c6f676f3d476f6f676c652532306368726f6d65266c6f676f436f6c6f723d626c7565)](https://teach.is.tue.mpg.de/)\n\n[**Nikos Athanasiou**](https://ps.is.mpg.de/person/nathanasiou)\n\u00b7\n[**Mathis Petrovich**](https://mathis.petrovich.fr)\n\u00b7\n[**Michael J. Black**](https://ps.is.tuebingen.mpg.de/person/black)\n\u00b7\n[**G\u00fcl Varol**](https://imagine.enpc.fr/~varolg)\n\n## 3DV 2022\n\n[![](https://github.com/atnikos/teach/raw/main/assets/action2.gif)](https://github.com/atnikos/teach/blob/main/assets/action2.gif)[![](https://github.com/atnikos/teach/raw/main/assets/action3.gif)](https://github.com/atnikos/teach/blob/main/assets/action3.gif)\n\nCheck our upcoming YouTube video for a quick overview and our paper for more details.\n\n### Video\n\n## Features\n\nThis implementation:\n\n- Instruction on how to prepare the datasets used in the experiments.\n- The training code:\n - For both baselines\n - For TEACH method\n- A simple interacting demo that given some prompts with texts and durations returns back:\n - a `npy` file containing the vertices of the body generated by TEACH.\n - a video that demonstrates the result.\n\n## Updates\n\nTo be uploaded:\n\n- Instructions about the baselines and how to run them.\n- Instructions for sampling and evaluating with the code all of the models.\n- The rendering code for the blender renderings used in the paper.\n\n## Getting Started\n\nTEACH has been implemented and tested on Ubuntu 20.04 with python >= 3.9.\n\nClone the repo:\n\n```\ngit clone https://github.com/athn-nik/teach.git\n```\n\nAfter it do this to install DistillBERT:\n\n```\ncd deps/\ngit lfs install\ngit clone https://huggingface.co/distilbert-base-uncased\ncd ..\n```\n\nInstall the requirements using `virtualenv` :\n\n```\n# pip\nsource scripts/install.sh\n```\n\nYou can do something equivalent with `conda` as well.\n\n## Running the Demo\n\nWe have prepared a nice demo code to run TEACH on arbitrary videos.\nFirst, you need download the required data(i.e our trained model from our [website](https://teach.is.tue.mpg.de)).\nThe `path/to/experiment` directory should look like:\n\n```\nexperiment\n\u2502\n\u2514\u2500\u2500\u2500.hydra\n\u2502 | config.yaml\n| | overrides.yaml\n| | hydra.yaml\n|\n\u2514\u2500\u2500\u2500checkpoints\n \u2502 last.ckpt\n\n```\n\nThen, running the demo is as simple as:\n\n```\npython interact_teach.py folder=/path/to/experiment output=/path/to/yourfname texts='[text prompt1, text prompt2, text prompt3,]' durs='[dur1, dur2, dur3, ...]'\n\n```\n\n## Data\n\n### \u203c\ufe0f\u26a0\ufe0f You can directly download the data from [this link](https://drive.google.com/drive/folders/1gKwLYP8TrbyY1YjsKz1A04s-rCwM9CfX?usp=sharing) and use them!\n\nDownload the data from [AMASS website](https://amass.is.tue.mpg.de). Then, run this command to extract the amass sequences that are annotated in babel:\n\n```\npython scripts/process_amass.py --input-path /path/to/data --output-path path/of/choice/default_is_/babel/babel-smplh-30fps-male --use-betas --gender male\n```\n\nDownload the data from [TEACH website](https://teach.is.tue.mpg.de), after signing in. The data TEACH was trained was a processed version of BABEL. Hence, we provide them directly to your via our website, where you will also find more relevant details.\nFinally, download the male SMPLH male body model from the [SMPLX website](https://smpl-x.is.tue.mpg.de/). Specifically the AMASS version of the SMPLH model. Then, follow the instructions [here](https://github.com/vchoutas/smplx/blob/main/tools/README.md#smpl-h-version-used-in-amass) to extract the smplh model in pickle format.\n\nThe run this script and change your paths accordingly inside it extract the different babel splits from amass:\n\n```\npython scripts/amass_splits_babel.py\n```\n\nThen create a directory named `data` and put the babel data and the processed amass data in.\nYou should end up with a data folder with the structure like this:\n\n```\ndata\n|-- amass\n| `-- your-processed-amass-data\n|\n|-- babel\n| `-- babel-teach\n| `...\n| `-- babel-smplh-30fps-male\n| `...\n|\n|-- smpl_models\n| `-- smplh\n| `--SMPLH_MALE.pkl\n\n```\n\nBe careful not to push any data!\nThen you should softlink inside this repo. To softlink your data, do:\n\n`ln -s /path/to/data`\n\n## Training\n\nTo start training after activating your environment. Do:\n\n```\npython train.py experiment=baseline logger=none\n```\n\nExplore `configs/train.yaml` to change some basic things like where you want\nyour output stored, which data you want to choose if you want to do a small\nexperiment on a subset of the data etc.\n\\[TODO\\]: More on this coming soon.\n\n### Sampling & Evaluation\n\nHere are some commands if you want to sample from the validaiton set and evaluate on the metrics reported\nin the paper:\n\n```\npython sample_seq.py folder=/path/to/experiment align=full slerp_ws=8\n```\n\nIn general the folder is: `folder_our/ / / / `\nThis folder should contain a `checkpoints` directory with a `last.ckpt` file inside and a `.hydra` directory from which the configuration\nwill be pulled and the relevant checkpoint. This folder is created during training in the output directory and is provided in our website\nfor the experiments in the paper.\n\n- `align=trans`: chooses if translation",
          "original_query": "TEACH: Temporal action composition for 3d humans (BABEL-TEACH)",
          "cleaned_query": "TEACH: Temporal action composition for 3d humans (BABEL-TEACH)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Motion In-Betweening for Densely Interacting Characters - arXiv",
          "url": "https://arxiv.org/html/2510.00314v1",
          "content": "Motion In-Betweening for Densely Interacting Characters\n# Motion In-Betweening for Densely Interacting Characters\nXiaotang Zhang[xiaotang.zhang@durham.ac.uk](mailto:xiaotang.zhang@durham.ac.uk)[0000-0003-0822-9064](https://orcid.org/0000-0003-0822-9064)Durham UniversityDurhamUnited Kingdom,Ziyi Chang[ziyi.chang@durham.ac.uk](mailto:ziyi.chang@durham.ac.uk)[0000-0003-0746-6826](https://orcid.org/0000-0003-0746-6826)Durham UniversityDurhamUnited Kingdom,Qianhui Men[qianhui.men@bristol.ac.uk](mailto:qianhui.men@bristol.ac.uk)[0000-0002-0059-5484](https://orcid.org/0000-0002-0059-5484)University of BristolBristolUnited KingdomandHubert P. H. Shum[hubert.shum@durham.ac.uk](mailto:hubert.shum@durham.ac.uk)[0000-0001-5651-6039](https://orcid.org/0000-0001-5651-6039)Durham UniversityDurhamUnited Kingdom\n(2025)\n###### Abstract.\nMotion in-betweening is the problem to synthesize movement between keyposes. Traditional research focused primarily on single characters. Extending them to densely interacting characters is highly challenging, as it demands precise spatial-temporal correspondence between the characters to maintain the interaction, while creating natural transitions towards predefined keyposes. In this research, we present a method for long-horizon interaction in-betweening that enables two characters to engage and respond to one another naturally.\nTo effectively represent and synthesize interactions, we propose a novel solution called Cross-Space In-Betweening, which models the interactions of each character across different conditioning representation spaces.\nWe further observe that the significantly increased constraints in interacting characters heavily limit the solution space, leading to degraded motion quality and diminished interaction over time.\nTo enable long-horizon synthesis, we present two solutions to maintain long-term interaction and motion quality, thereby keeping synthesis in the stable region of the solution space.\nWe first sustain interaction quality by identifying periodic interaction patterns through adversarial learning.\nWe further maintain the motion quality by learning to refine the drifted latent space and prevent pose error accumulation.\nWe demonstrate that our approach produces realistic, controllable, and long-horizon in-between motions of two characters with dynamic boxing and dancing actions across multiple keyposes, supported by extensive quantitative evaluations and user studies.\nAnimation, Motion Synthesis, Motion In-betweening, Deep Learning\n\u2020\u2020journalyear:2025\u2020\u2020conference:SIGGRAPH Asia 2025 Conference Papers; December 15\u201318, 2025; Hong Kong, Hong Kong\u2020\u2020booktitle:SIGGRAPH Asia 2025 Conference Papers (SA Conference Papers \u201925), December 15\u201318, 2025, Hong Kong, Hong Kong\u2020\u2020doi:10.1145/3757377.3763950\u2020\u2020isbn:979-8-4007-2137-3/2025/12\u2020\u2020ccs:Computing methodologies\u00a0Motion capture\u2020\u2020ccs:Computing methodologies\u00a0Neural networks![Refer to caption](figures/Teaser.png)Figure 1.Our approach is capable of producing interactive in-between motions (light blue and pink characters) for multiple keyposes (blue and red characters) while maintaining high quality over long transitions.\n## 1.Introduction\nMotion in-betweening involves synthesizing realistic character movement between predefined keyposes.\nIt enables animators to efficiently create controllable motions by specifying only keyposes.\nPrior studies> (Studer et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.00314v1#bib.bib42)> ; Hong et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.00314v1#bib.bib16)> ; Chu and Yang, [> 2024\n](https://arxiv.org/html/2510.00314v1#bib.bib7)> ; Chai and Qin, [> 2024\n](https://arxiv.org/html/2510.00314v1#bib.bib5)> ; Oreshkin et\u00a0al., [> 2023\n](https://arxiv.org/html/2510.00314v1#bib.bib29)> ; Tang et\u00a0al., [> 2022\n](https://arxiv.org/html/2510.00314v1#bib.bib45)> ; Qin et\u00a0al., [> 2022\n](https://arxiv.org/html/2510.00314v1#bib.bib33)> ; Harvey and Pal, [> 2018\n](https://arxiv.org/html/2510.00314v1#bib.bib13)> ; Harvey et\u00a0al., [> 2020\n](https://arxiv.org/html/2510.00314v1#bib.bib14)> )\nhave explored various architectures for the motion in-betweening task, under diverse conditioning schemes such as text> (Pinyoanuntapong et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.00314v1#bib.bib32)> ; Cohan et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.00314v1#bib.bib8)> )\n, style> (Tang et\u00a0al., [> 2023\n](https://arxiv.org/html/2510.00314v1#bib.bib46)> )\n, skeletal topology> (Yun et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.00314v1#bib.bib53)> ; Gat et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.00314v1#bib.bib9)> )\nand keyframe timing> (Starke et\u00a0al., [> 2023\n](https://arxiv.org/html/2510.00314v1#bib.bib39)> ; Goel et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.00314v1#bib.bib11)> )\n.\nDespite extensive research, existing methods primarily focus on motion in-betweening for a single character, and it is non-trivial to extend these methods to multiple densely interacting characters.\nDense interactions such as boxing or dancing are characterized by precise movements as well as precise timing.\nExtending interaction synthesis to in-betweening requires generation to fulfill three constraints:\n(1) The two-character motion should be spatio-temporally aligned such that it\u2019s semantically interactive.\n(2) The two-character motion should end at the predefined keypose at the same time.\n(3) In-betweening should generalize to user-customized keypose which typically lies outside the spatio-temporal distribution learned from dataset.\nThese precise requirements introduce significantly more constraints to the solution space than single-character in-betweening.\nFundamentally, the core challenge of interaction in-betweening is two-fold.\nFirst, it requires explicitly modeling interactions to capture precise movement and timing that define the interaction, while dynamically satisfying keyposes for each character.\nSecond, enforcing dense interaction introduces a substantial number of constraints. To fulfill them would easily lead to unnatural motion and cause keyposes unreachable. This degradation compounds over time, ultimately making long-horizon interaction synthesis infeasible.\nIn this paper, we present a novel solution for long-horizon, densely interacting in-betweening that enables two characters to engage and respond to one another naturally.\nTo represent interaction effectively, we introduceCross-Space In-Betweeningto model and synthesize reactive in-between motions for two characters.\nOur approach first represents motions as spatial offsets relative to keyposes and synthesizes transitions for each character individually.\nThe transitions are then transformed relative to the other character, with interaction conditions integrated via an affine transformation learned by Feature-wise Linear Modulation (FiLM)> (Perez et\u00a0al., [> 2018\n](https://arxiv.org/html/2510.00314v1#bib.bib31)> )\n.\nThis two-stage synthesis ensures stable, responsive motion transitions conditioned on the relative positions to both the keyposes and the counterpart character.\nTo address the challenge of overly restrictive constraints in this task, we present two solutions that help to preserve interaction consistency and motion quality over time, enabling long-horizon interaction in-betweening.\nWe first sustain interaction quality by identifying periodic interaction patterns through adversarial learning, which distinguishes real temporal structures from synthetic, inconsistent interactions.\nThis design is inspired by the observation that dense interactions like boxing and dancing often follow periodic and repetitive spatial distance.\nSecond, we maintain individual motion quality by sampling from a refined latent space during inference\u2014a simple yet effective strategy to mitigate error accumulation and distribution drift common in auto-regressive methods.\nTogether, these two strategies foster a robust latent space informed by interaction periodicity and individual motion correction, supporting our goal of high-quality long-horizon synthesis.\nWe showcase long-horizon interaction in-betweening across the Boxing> (Shum et\u00a0al., [> 2010\n](https://arxiv.org/html/2510.00314v1#bib.bib38)> )\n, ReMoCap> (Ghosh et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.00314v1#bib.bib10)> )\nand InterHuman> (Liang et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.00314v1#bib.bib24)> )\ndatasets.\nOur system enables users to interactively select, translate, and rotate keyposes for two characters, with valid interactions automatically generated in response (see Fig.[10](https://arxiv.org/html/2510.00314v1#S6.F10)).\nAblation studies, quantitative evaluations, and user studies demonstrate that our method outperforms prior work on interaction in-betweening.\nThe main contributions of this work can be summarized as:\n* \u2022We propose Cross-Space In-betweening that enables stable and responsive interaction modeling for two-character motion in-betweening.\n* \u2022We maintain long-term interaction quality by identifying periodic interaction patterns through adversarial learning.\n* \u2022We preserve long-term motion quality by learning to refine the drifted latent space and prevent pose error accumulation.\n* \u2022We demonstrate that our system is robust to produce responsive in-between interactions for user-defined keyposes.\n![Refer to caption](figures/Architecture.png)Figure 2.An overview of our framework. The system first generates an initial prediction for individual character which minimizes the distance to keypose. Then, it extracts relative pose representations as conditions to refine the initial prediction and generates interactive motions. Pairwise joint distances and the outcomes of main network are fed into an interaction discriminator and a motion refiner to model interaction periodicity and to reduce pose error, respectively.\n## 2.Related Works\n### 2.1.Multi-Character Interaction Synthesis\nModeling interactions between virtual characters has been widely explored in computer graphics and vision. Early works relied on handcrafted patches or pro",
          "original_query": "Motion in-betweening with phase manifolds (PhaseBetweener / RSMT)",
          "cleaned_query": "Motion in-betweening with phase manifolds (PhaseBetweener",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Phase-Aligned DoubleTake for Ultra-Long Text-to-Motion\nBuild on DoubleTake\u2019s sequential composition by inserting DeepPhase-style periodic embeddings to explicitly align cycle boundaries (heel strikes, arm swings) across prompted intervals. Implement an inference-time \u201cphase snapping\u201d step that optimizes boundary frames to match target phase and velocity, reducing visible seams when stitching dozens of clips.",
        "Joint-Wise Frequency-Domain Diffusion Blending\nExtend DiffusionBlending by training per-joint (or per-limb) diffusion \u201cexperts\u201d that operate in DiffusionPhase\u2019s frequency/phase parameterization rather than joint positions. Blend experts via learnable frequency masks (e.g., low-frequency torso from one expert, high-frequency hands/feet from another) to enable fine-grained editing like \u201ckeep gait but make hands clap on beats.\u201d",
        "Multi-Rate Periodic Autoencoders for Hierarchical Motion Rhythms\nModify DeepPhase to learn multiple coupled oscillators per body (e.g., gait cycle, arm swing, breathing/torso bob) with explicit integer or rational frequency ratios. Use these multi-rate phases as conditioning channels for an MDM-style transformer to improve generation of motions that mix rhythms (e.g., run + wave, dance with syncopation).",
        "Two-Person Frequency-Domain ComMDM with Interaction Spectra\nUpgrade ComMDM\u2019s slim communication block by exchanging frequency-domain descriptors (relative-distance harmonics, synchronized contact phases) instead of raw pose features. This makes interaction timing explicit and could stabilize long-horizon coordination for periodic interactions (boxing feints, partner dance) while requiring only limited paired data.",
        "Phase-Consistent Motion Inpainting for Contact-Critical Edits\nExtend MDM\u2019s joint-space inpainting to enforce phase constraints from DeepPhase (e.g., maintain left-foot stance phase while editing upper body). Implement a constrained diffusion sampling procedure that projects intermediate samples onto a target phase manifold, yielding edits that preserve contacts and periodic timing instead of drifting into foot-slide.",
        "Adaptive Periodic/Non-Periodic Decomposition for Hybrid Motions\nImprove DiffusionPhase\u2019s preprocessing by learning the periodic vs. aperiodic split end-to-end with a differentiable gating network supervised by reconstruction and contact losses. This enables robust modeling of hybrid sequences (walk \u2192 sudden stop \u2192 kick \u2192 resume walk) without manual heuristics and improves transitions in long text-generated motions.",
        "Phase-Based Retrieval-Augmented Diffusion for Low-Data Styles\nUse DeepPhase embeddings as an index for retrieving nearest phase-consistent motion snippets from a large unlabelled mocap bank, then condition an MDM/DiffusionPhase generator on retrieved phase trajectories. This retrieval-augmented setup targets scarce text-labeled styles (e.g., specific dance genres), improving fidelity and diversity while minimizing additional annotation.",
        "Beat-Synchronous Frequency Diffusion with Explicit Music-to-Phase Coupling\nBuild on DeepPhase\u2019s success in dance-from-music by training DiffusionPhase with an auxiliary loss that aligns predicted phase velocity to detected beat/tempo curves. The actionable contribution is a conditioning interface that accepts beat grids and \u201caccent strength,\u201d generating dances that stay synchronized over long horizons without accumulating timing drift."
      ],
      "generated_ideas_raw": "1. **Phase-Aligned DoubleTake for Ultra-Long Text-to-Motion**\n   Build on DoubleTake\u2019s sequential composition by inserting DeepPhase-style periodic embeddings to explicitly align cycle boundaries (heel strikes, arm swings) across prompted intervals. Implement an inference-time \u201cphase snapping\u201d step that optimizes boundary frames to match target phase and velocity, reducing visible seams when stitching dozens of clips.\n\n2. **Joint-Wise Frequency-Domain Diffusion Blending**\n   Extend DiffusionBlending by training per-joint (or per-limb) diffusion \u201cexperts\u201d that operate in DiffusionPhase\u2019s frequency/phase parameterization rather than joint positions. Blend experts via learnable frequency masks (e.g., low-frequency torso from one expert, high-frequency hands/feet from another) to enable fine-grained editing like \u201ckeep gait but make hands clap on beats.\u201d\n\n3. **Multi-Rate Periodic Autoencoders for Hierarchical Motion Rhythms**\n   Modify DeepPhase to learn multiple coupled oscillators per body (e.g., gait cycle, arm swing, breathing/torso bob) with explicit integer or rational frequency ratios. Use these multi-rate phases as conditioning channels for an MDM-style transformer to improve generation of motions that mix rhythms (e.g., run + wave, dance with syncopation).\n\n4. **Phase-Language as a Structured Intermediate for Compositional Prompts**\n   Combine TEACH\u2019s temporal action compositions with a learned \u201cphase-language\u201d tokenization: discretize DeepPhase/DiffusionPhase latent phases into a small vocabulary of phase-events (contact, lift, apex). Train a text\u2192(phase-event sequence + durations) planner, then a diffusion decoder to realize it, improving controllability for prompts like \u201cwalk 2 steps, pivot, then jog 3 seconds.\u201d\n\n5. **Two-Person Frequency-Domain ComMDM with Interaction Spectra**\n   Upgrade ComMDM\u2019s slim communication block by exchanging frequency-domain descriptors (relative-distance harmonics, synchronized contact phases) instead of raw pose features. This makes interaction timing explicit and could stabilize long-horizon coordination for periodic interactions (boxing feints, partner dance) while requiring only limited paired data.\n\n6. **Phase-Consistent Motion Inpainting for Contact-Critical Edits**\n   Extend MDM\u2019s joint-space inpainting to enforce phase constraints from DeepPhase (e.g., maintain left-foot stance phase while editing upper body). Implement a constrained diffusion sampling procedure that projects intermediate samples onto a target phase manifold, yielding edits that preserve contacts and periodic timing instead of drifting into foot-slide.\n\n7. **Adaptive Periodic/Non-Periodic Decomposition for Hybrid Motions**\n   Improve DiffusionPhase\u2019s preprocessing by learning the periodic vs. aperiodic split end-to-end with a differentiable gating network supervised by reconstruction and contact losses. This enables robust modeling of hybrid sequences (walk \u2192 sudden stop \u2192 kick \u2192 resume walk) without manual heuristics and improves transitions in long text-generated motions.\n\n8. **Phase-Based Retrieval-Augmented Diffusion for Low-Data Styles**\n   Use DeepPhase embeddings as an index for retrieving nearest phase-consistent motion snippets from a large unlabelled mocap bank, then condition an MDM/DiffusionPhase generator on retrieved phase trajectories. This retrieval-augmented setup targets scarce text-labeled styles (e.g., specific dance genres), improving fidelity and diversity while minimizing additional annotation.\n\n9. **Beat-Synchronous Frequency Diffusion with Explicit Music-to-Phase Coupling**\n   Build on DeepPhase\u2019s success in dance-from-music by training DiffusionPhase with an auxiliary loss that aligns predicted phase velocity to detected beat/tempo curves. The actionable contribution is a conditioning interface that accepts beat grids and \u201caccent strength,\u201d generating dances that stay synchronized over long horizons without accumulating timing drift.\n\n10. **Cross-Space Interaction In-Betweening with Phase-Manifold Drift Correction**\n   Fuse the SIGGRAPH Asia 2025 Cross-Space In-Betweening idea with DeepPhase-style phase manifolds to create a drift-correction module that periodically reprojects each character back onto a stable interaction-phase torus. Implement this as a lightweight post-step during iterative generation (or diffusion sampling) to prevent long-horizon degradation of both interaction tightness and individual motion quality.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Phase-Aligned DoubleTake for Ultra-Long Text-to-Motion\nBuild on DoubleTake\u2019s sequential composition by inserting DeepPhase-style periodic embeddings to explicitly align cycle boundaries (heel strikes,",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Joint-Wise Frequency-Domain Diffusion Blending\nExtend DiffusionBlending by training per-joint (or per-limb) diffusion \u201cexperts\u201d that operate in DiffusionPhase\u2019s frequency/phase parameterization rather",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Multi-Rate Periodic Autoencoders for Hierarchical Motion Rhythms\nModify DeepPhase to learn multiple coupled oscillators per body (e.g., gait cycle, arm swing, breathing/torso bob) with explicit intege",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Two-Person Frequency-Domain ComMDM with Interaction Spectra\nUpgrade ComMDM\u2019s slim communication block by exchanging frequency-domain descriptors (relative-distance harmonics, synchronized contact phas",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Phase-Consistent Motion Inpainting for Contact-Critical Edits\nExtend MDM\u2019s joint-space inpainting to enforce phase constraints from DeepPhase (e.g., maintain left-foot stance phase while editing upper",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Adaptive Periodic/Non-Periodic Decomposition for Hybrid Motions\nImprove DiffusionPhase\u2019s preprocessing by learning the periodic vs. aperiodic split end-to-end with a differentiable gating network supe",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Phase-Based Retrieval-Augmented Diffusion for Low-Data Styles\nUse DeepPhase embeddings as an index for retrieving nearest phase-consistent motion snippets from a large unlabelled mocap bank, then cond",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Beat-Synchronous Frequency Diffusion with Explicit Music-to-Phase Coupling\nBuild on DeepPhase\u2019s success in dance-from-music by training DiffusionPhase with an auxiliary loss that aligns predicted phas",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 2,
      "paper_title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
      "contribution": "Introduce an exemplar-based global GNN explainer that selects representative nodes in embedding space via a coverage-maximization over reverse k-nearest neighbors and converts their neighborhoods into concise natural-language rules using an LLM self-refinement prompting strategy, yielding scalable, high-fidelity, and human-interpretable class-level explanations.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 11466,
      "output_tokens": 1096,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Context Theory of Classification Learning - Psychology - Northwestern",
          "url": "https://groups.psych.northwestern.edu/medin/documents/MedinSchaffer1978PsychRev.pdf",
          "content": "Psychological Review\n1978, Vol. 85, No. 3, 207-238\nContext Theory of Classification Learning\nDouglas L. Medin\nRockefeller University\nMarguerite M. Schaffer\nBarnard College\nMost theories dealing with ill-defined concepts assume that performance is based\non category level information or a mixture of category level and specific item\ninformation. A context theory of classification is described in which judgments\nare assumed to derive exclusively from stored exemplar information. The main\nidea is that a probe item acts as a retrieval cue to access information associated\nwith stimuli similar to the probe. The predictions of the context theory are con\u0002trasted with those of a class of theories (including prototype theory) that as\u0002sume that the information entering into judgments can be derived from an\nadditive combination of information from component cue dimensions. Across\nfour experiments using both geometric forms and schematic faces as stimuli, the\ncontext theory consistently gave a better account of the data. The relation of\nthe context theory to other theories and phenomena associated with ill-defined\nconcepts is discussed in detail.\nOne of the major components of cognitive\nbehavior concerns abstracting rules and form\u0002ing concepts. Our entire system of naming ob\u0002jects and events, talking about them, and inter\u0002acting with them presupposes the ability to\ngroup experiences into appropriate classes.\nYoung children learn to tell the difference be\u0002tween dogs and cats, between clocks and fans,\nand between stars and street lights. Since few\nconcepts are formally taught, the evolution of\nconcepts from experience with exemplars must\nbe a fundamental learning phenomenon. The\nfocus of the present article is to explore how\nsuch conceptual achievements emerge from\nindividual instances.\nStructure of Concepts\nAn early step in analyzing task demands\ninvolved in conceptual behavior is to ask how\nThis research was supported by United States Public\nHealth Grant MH16100 and by Grant MH 23878 from\nthe National Institute of Mental Health.\nWe wish to acknowledge the support of Edith Skaar,\nwho assisted in all phases of this research, and the\npatience of Mark Altom, Lee Brooks, Donald Robbins,\nand Edward E. Smith, who read earlier drafts of this\narticle and provided helpful suggestions.\nRequests for reprints should be sent to Douglas L.\nMedin, Box 298, Rockefeller University, New York,\nNew York 10021.\nindividual instances or exemplars are related\nto the superordinate concept. Although there\nis general agreement that natural categories\nare structured so that exemplars within a\ncategory are more similar to one another than\nto exemplars from alternative categories, there\nis disagreement concerning the rigidity of this\nstructure. One extreme view is that all natural\nconcepts are characterized by simple sets of\ndenning features that are singly necessary and\njointly sufficient to determine category mem\u0002bership (Katz & Postal, 1964). Each exemplar\nof the concept must possess these defining\nfeatures, and therefore, each exemplar is\nequally representative of the concept. Con\u0002cepts containing singly necessary and jointly\nsufficient denning features are said to be well\u0002defined concepts.\nA contrasting point of view is that most\nnatural concepts are not well-defined but\nrather are based on relationships that are only\ngenerally true. Individual exemplars may vary\nin the number of characteristic features they\npossess, and consequently, some exemplars\nmay be more representative or more typical of\na concept than others. For example, cows may\nbe better exemplars of the concept mammal\nthan are whales. Instances are neither arbi\u0002trarily associated with categories nor strictly\nlinked by defining features, but rather in\u0002Copyright 1978 by the American Psychological Association, Inc. 0033-295X/78/8503-0207$00.75\n207\n208 DOUGLAS L. MEDIN AND MARGUERITE M. SCHAFFER\nstances reflect more nearly a \"family resem\u0002blance\" structure (Rosch & Mervis, 1975).\nThese ideas concerning the structure of\ncategories influence the way instances are set\nup in laboratory studies of artificial concepts.\nMost early work with concepts used well\u0002defined concepts and focused on such issues as\nthe relative difficulty of acquiring different\nrules, strategies for formulating and testing\nalternative hypotheses, and the transfer of\nbehavior to new stimulus sets (e.g., Bourne,\n1970; Levine, 1975; Trabasso & Bower, 1968).\nWhile this approach has accumulated con\u0002siderable information about processes under\u0002lying hypothesis selection and rule learning\nwhere rules are well-defined, little information\nexists to show how these models might be\napplied to a variety of other situations where\nrules and concepts are not so precisely defined.\nTherefore, substantial reason exists to examine\nmore closely the case in which the concepts\nand classifications acquired in everyday ex\u0002perience do not conform to well-defined rules.\nOn the basis of an extensive series of experi\u0002ments, Rosch and her associates (Rosch, 1973,\n1975a, 1975b, 1975c; Rosch & Mervis, 1975;\nRosch et al., 1976) have argued that most\nnatural categories do not have well-defined\nrules or fixed boundaries separating alternative\ncategories. Rather, members vary in the degree\nto which they are judged to be good examples\n(typical) of the category, and many natural\nconcepts cannot be defined by a single set of\ncritical features. In addition, subjects appear\nto use nonnecessary features in making cate\u0002gory judgments. Smith, Shoben, and Rips\n(1974) found that the items judged to be\ntypical of a category possess features that are\ncharacteristic of the class but not necessary for\ncategory definition. For example, robin is a\ntypical member of the category bird and has\nthe characteristic feature that it flies, but not\nall birds fly (e.g., penguins). In a reaction time\ntask, Smith et al. observed that subjects re\u0002quired less time to verify the category member\u0002ship of the more typical items in a category.\nCharacteristic features and not just defining\nfeatures appear to be involved in these category\njudgments.\nIf many natural categories have a loosely\ndefined structure, how do people acquire and\nuse this structure? Posner and Keele (1968)\nproposed that based on experience with ex\u0002emplars, people form an impression of the\ncentral tendency of a category and that cate\u0002gorical judgments come to be based on this\ncentral tendency, or prototype. While there is\nnot universal agreement that prototype forma\u0002tion underlies conceptual learning in this\ndomain, the increasing evidence that natural\ncategories and concepts are not well-defined\nhas amplified the interest in developing theories\nof conceptual behavior appropriate to rules\nwith exceptions.\nThe present article takes the perspective of\naiming to see if recent theoretical develop\u0002ments arising in the domain of discrimination\nlearning might be profitably applied to classi\u0002fication learning. In the case of well-defined\nrules for stimulus classification, there are some\nstriking parallels between paradigm and theory\nin discrimination learning and concept identi\u0002fication. For example, the simple affirmative\nconcept \"red in one pile, green in the other\npile\" corresponds to a simultaneous discrimi\u0002nation learning task, where red is correct and\ngreen is incorrect. Likewise, hypothesis-testing\ntheories for concept identification tasks (e.g.,\nTrabasso & Bower, 1968) are closely mirrored\nby theories of selective attention in discrimina\u0002tion learning (Medin, 1976; Sutherland &\nMackintosh, 1971).\nIs there any basis for expecting useful inter\u0002action between the domains of discrimination\nlearning and concept learning for ill-defined\nrules? We shall argue that there is. Not all\ndiscrimination learning problems map onto\nsimple affirmative rules. For example, in a\nsuccessive brightness discrimination problem,\nthe solution might be \"If the choice stimuli\nare white, go right; if black, go left.\" The\nvarious stimulus components, that is, black,\nwhite, left, and right, each are associated with\nreward half the time, and the problem could\nnot be solved on the basis of associations to\nthese independent stimulus components. In\u0002deed, Spence's (1936) theory of discrimination\nlearning assumed independence of compo\u0002nents, and it was unable to predict that suc\u0002cessive discrimination problems could be mas\u0002tered. Other discrimination learning theories\nhave been proposed that can account for rela\u0002tionships between simultaneous and successive\ndiscrimination learning, and the present article\nTHEORY OF CLASSIFICATION LEARNING 209\nattempts to demonstrate the applicability of\none such theory (Medin, 1975) to learning and\nclassification involving ill-defined categories.\nBefore discussing this theory, however, we\nconsider two phenomena that have been ad\u0002duced in support of prototype theory and that\ndirectly motivated our theoretical efforts.\nSome Evidence Related to Classification Involving\nIll-defined Concepts\nIn a typical study assessing the learning of\nill-defined concepts, subjects learn to sort a\nset of instances into two or more categories\nand then are given transfer tests with new\nstimuli, including a pattern representing the\ncentral tendency, or prototype. An alternative\nto the procedure of presenting two or more\ncontrasting categories is simply to present\nsubjects with instances of a single concept and\nthen to give a new-old recognition test for new\nand old instances. Stimuli range from sche\u0002matic faces, geometric forms, and dot patterns\nto letter sequences and biographical descrip\u0002tions. A major theoretical view is that as a\nfunction of experience with exemplars of a\ncategory, subjects abstract out the central\ntendency of the category. This summary\nrepresentation, or prototype, is assumed to\nprovide the basis for classification performance.\nThe closer an exemplar is to its category proto\u0002type and the farther it is from the prototypes\nassociated with alternative categories, the\ngreater the likelihood that it will be appropri\u0002ately classified.\nTw",
          "original_query": "Context theory of classification learning",
          "cleaned_query": "Context theory of classification learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Inductive representation learning on large graphs",
          "url": "https://dl.acm.org/doi/10.5555/3294771.3294869",
          "content": "- [Consent](https://dl.acm.org/dl.acm.org)\n- [Details](https://dl.acm.org/dl.acm.org)\n- [\\[#IABV2SETTINGS#\\]](https://dl.acm.org/dl.acm.org)\n- [About](https://dl.acm.org/dl.acm.org)\n\nThis website uses cookies\n\nWe occasionally run membership recruitment campaigns on social media channels and use cookies to track post-clicks. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you\u2019ve provided to them or that they\u2019ve collected from your use of their services. Use the check boxes below to choose the types of cookies you consent to have stored on your device.\n\nConsent Selection\n\n**Necessary**\n\n**Preferences**\n\n**Statistics**\n\n**Marketing**\n\n[Show details](https://dl.acm.org/dl.acm.org)\n\nDetails\n\n- Necessary 10\n\n\n\n\n\nNecessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. These cookies do not gather information about you that could be used for marketing purposes and do not remember where you have been on the internet.\n\n\n\n\n\n\n\n- ACM\n5\n[Learn more about this provider](https://www.acm.org/privacy-policy)\n\n\n**\\_\\_cf\\_bm\u00a0\\[x2\\]** This cookie is used to distinguish between humans and bots. This is beneficial for the website, in order to make valid reports on the use of their website.\n\n**Maximum Storage Duration**: 1 day**Type**: HTTP Cookie\n\n\n\n\n\n**\\_cfuvid** This cookie is a part of the services provided by Cloudflare - Including load-balancing, deliverance of website content and serving DNS connection for website operators.\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n\n\n\n\n**cf\\_chl\\_rc\\_ni** This cookie is a part of the services provided by Cloudflare - Including load-balancing, deliverance of website content and serving DNS connection for website operators.\n\n**Maximum Storage Duration**: 1 day**Type**: HTTP Cookie\n\n\n\n\n\n**JSESSIONID** Preserves users states across page requests.\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n- Cloudflare\n1\n[Learn more about this provider](https://www.cloudflare.com/privacypolicy/)\n**cf.turnstile.u** This cookie is used to distinguish between humans and bots.\n\n**Maximum Storage Duration**: Persistent**Type**: HTML Local Storage\n\n- Cookiebot\n2\n[Learn more about this provider](https://www.cookiebot.com/goto/privacy-policy/)\n\n\n**CookieConsent** Stores the user's cookie consent state for the current domain\n\n**Maximum Storage Duration**: 1 year**Type**: HTTP Cookie\n\n\n\n\n\n**1.gif** Used to count the number of sessions to the website, necessary for optimizing CMP product delivery.\n\n**Maximum Storage Duration**: Session**Type**: Pixel Tracker\n\n- c.disquscdn.com\n2\n\n\n\n**\\_\\_jid** Used to add comments to the website and remember the user's Disqus login credentials across websites that use said service.\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n\n\n\n\n**disqusauth** Registers whether the user is logged in. This allows the website owner to make parts of the website inaccessible, based on the user's log-in status.\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n\n- Preferences 5\n\n\n\n\n\nPreference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n\n\n\n\n\n\n- ACM\n1\n[Learn more about this provider](https://www.acm.org/privacy-policy)\n**MACHINE\\_LAST\\_SEEN** Pending\n\n**Maximum Storage Duration**: 300 days**Type**: HTTP Cookie\n\n- Mopinion\n1\n[Learn more about this provider](https://mopinion.com/privacy/)\n**mopDeploy** Pending\n\n**Maximum Storage Duration**: Session**Type**: HTML Local Storage\n\n- c.disquscdn.com\n3\n\n\n\n**aet-dismiss** Necessary for the functionality of the website's comment-system.\n\n**Maximum Storage Duration**: Persistent**Type**: HTML Local Storage\n\n\n\n\n\n**drafts.queue** Necessary for the functionality of the website's comment-system.\n\n**Maximum Storage Duration**: Persistent**Type**: HTML Local Storage\n\n\n\n\n\n**submitted\\_posts\\_cache** Necessary for the functionality of the website's comment-system.\n\n**Maximum Storage Duration**: Persistent**Type**: HTML Local Storage\n\n\n- Statistics 9\n\n\n\n\n\nStatistic cookies help website owners understand how visitors interact with websites by collecting and reporting information anonymously.\n\n\n\n\n\n\n\n- Google\n4\n[Learn more about this provider](https://business.safety.google/privacy/)\n\n\nSome of the data collected by this provider is for the purposes of personalization and measuring advertising effectiveness.\n\n\n\n**\\_ga** Registers a unique ID that is used to generate statistical data on how the visitor uses the website.\n\n**Maximum Storage Duration**: 2 years**Type**: HTTP Cookie\n\n\n\n\n\n**\\_ga\\_#** Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit.\n\n**Maximum Storage Duration**: 2 years**Type**: HTTP Cookie\n\n\n\n\n\n**\\_gat** Used by Google Analytics to throttle request rate\n\n**Maximum Storage Duration**: 1 day**Type**: HTTP Cookie\n\n\n\n\n\n**\\_gid** Registers a unique ID that is used to generate statistical data on how the visitor uses the website.\n\n**Maximum Storage Duration**: 1 day**Type**: HTTP Cookie\n\n- Hotjar\n4\n[Learn more about this provider](https://www.hotjar.com/legal/policies/privacy/)\n\n\n**\\_hjSession\\_#** Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read.\n\n**Maximum Storage Duration**: 1 day**Type**: HTTP Cookie\n\n\n\n\n\n**\\_hjSessionUser\\_#** Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read.\n\n**Maximum Storage Duration**: 1 year**Type**: HTTP Cookie\n\n\n\n\n\n**\\_hjTLDTest** Registers statistical data on users' behaviour on the website. Used for internal analytics by the website operator.\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n\n\n\n\n**\\_hjCookieTest** Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner.\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n- c.disquscdn.com\n1\n\n**disqus\\_unique** Collects statistics related to the user's visits to the website, such as number of visits, average time spent on the website and loaded pages.\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n\n- Marketing 18\n\n\n\n\n\nMarketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n\n\n\n\n\n\n- Google\n1\n[Learn more about this provider](https://business.safety.google/privacy/)\n\n\nSome of the data collected by this provider is for the purposes of personalization and measuring advertising effectiveness.\n\n\n\n**NID** Registers a unique ID that identifies a returning user's device. The ID is used for targeted ads.\n\n**Maximum Storage Duration**: 6 months**Type**: HTTP Cookie\n\n- YouTube\n17\n[Learn more about this provider](https://business.safety.google/privacy/)\n\n\n**\\_\\_Secure-ROLLOUT\\_TOKEN** Used to track user\u2019s interaction with embedded content.\n\n**Maximum Storage Duration**: 180 days**Type**: HTTP Cookie\n\n\n\n\n\n**\\_\\_Secure-YEC** Stores the user's video player preferences using embedded YouTube video\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n\n\n\n\n**LAST\\_RESULT\\_ENTRY\\_KEY** Used to track user\u2019s interaction with embedded content.\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n\n\n\n\n**LogsDatabaseV2:V#\\|\\|LogsRequestsStore** Used to track user\u2019s interaction with embedded content.\n\n**Maximum Storage Duration**: Persistent**Type**: IndexedDB\n\n\n\n\n\n**remote\\_sid** Necessary for the implementation and functionality of YouTube video-content on the website.\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n\n\n\n\n**TESTCOOKIESENABLED** Used to track user\u2019s interaction with embedded content.\n\n**Maximum Storage Duration**: 1 day**Type**: HTTP Cookie\n\n\n\n\n\n**VISITOR\\_INFO1\\_LIVE** Tries to estimate the users' bandwidth on pages with integrated YouTube videos.\n\n**Maximum Storage Duration**: 180 days**Type**: HTTP Cookie\n\n\n\n\n\n**YSC** Registers a unique ID to keep statistics of what videos from YouTube the user has seen.\n\n**Maximum Storage Duration**: Session**Type**: HTTP Cookie\n\n\n\n\n\n**ytidb::LAST\\_RESULT\\_ENTRY\\_KEY** Used to track user\u2019s interaction with embedded content.\n\n**Maximum Storage Duration**: Persistent**Type**: HTML Local Storage\n\n\n\n\n\n**YtIdbMeta#databases** Used to track user\u2019s interaction with embedded content.\n\n**Maximum Storage Duration**: Persistent**Type**: IndexedDB\n\n\n\n\n\n**yt-remote-cast-available** Stores the user's video player preferences using embedded YouTube video\n\n**Maximum Storage Duration**: Session**Type**: HTML Local Storage\n\n\n\n\n\n**yt-remote-cast-installed** Stores the user's video player preferences using embedded YouTube video\n\n**Maximum Storage Duration**: Session**Type**: HTML Local Storage\n\n\n\n\n\n**yt-remote-connected-devices** Stores the user's video player preferences using embedded YouTube video\n\n**Maximum Storage Duration**: Persistent**Type**: HTML Local Storage\n\n\n\n\n\n**yt-remote-device-id** Stores the user's video player preferences using embedded YouTube video\n\n**Maximum Storage Duration**: Persistent**Type**: HTML Local Storage\n\n\n\n\n\n**yt-remote-fast-check-period** Stores the user's video player preferences using embedded YouTube video\n\n**Maximum Storage Duration**: Session**Type**: HTML Local Storage\n\n\n\n\n\n**yt-remote-session-app** Stores the user's video player preferences using embedded YouTube video\n\n**Maximum Storage Duration**: Session**Type**: HTML Local Storage\n\n\n\n\n\n**yt-remote-session-name** Stores the user's video player preferences using em",
          "original_query": "Inductive representation learning on large graphs (GraphSAGE)",
          "cleaned_query": "Inductive representation learning on large graphs (GraphSAGE)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "GNNExplainer: Generating Explanations for Graph Neural ...",
          "url": "https://arxiv.org/abs/1903.03894",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1903.03894** (cs)\n\n\\[Submitted on 10 Mar 2019 ( [v1](https://arxiv.org/abs/1903.03894v1)), last revised 13 Nov 2019 (this version, v4)\\]\n\n# Title:GNNExplainer: Generating Explanations for Graph Neural Networks\n\nAuthors: [Rex Ying](https://arxiv.org/search/cs?searchtype=author&query=Ying,+R), [Dylan Bourgeois](https://arxiv.org/search/cs?searchtype=author&query=Bourgeois,+D), [Jiaxuan You](https://arxiv.org/search/cs?searchtype=author&query=You,+J), [Marinka Zitnik](https://arxiv.org/search/cs?searchtype=author&query=Zitnik,+M), [Jure Leskovec](https://arxiv.org/search/cs?searchtype=author&query=Leskovec,+J)\n\nView a PDF of the paper titled GNNExplainer: Generating Explanations for Graph Neural Networks, by Rex Ying and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/1903.03894)\n\n> Abstract:Graph Neural Networks (GNNs) are a powerful tool for machine learning on [this http URL](http://graphs.GNNs) combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models, and explaining predictions made by GNNs remains unsolved. Here we propose GNNExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GNNExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1% on average. GNNExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1903.03894](https://arxiv.org/abs/1903.03894) \\[cs.LG\\] |\n| | (or [arXiv:1903.03894v4](https://arxiv.org/abs/1903.03894v4) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1903.03894](https://doi.org/10.48550/arXiv.1903.03894) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Rex Ying \\[ [view email](https://arxiv.org/show-email/d1801065/1903.03894)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1903.03894v1)**\nSun, 10 Mar 2019 00:56:26 UTC (3,349 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1903.03894v2)**\nThu, 12 Sep 2019 22:53:52 UTC (7,761 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/1903.03894v3)**\nFri, 8 Nov 2019 19:08:14 UTC (5,468 KB)\n\n**\\[v4\\]**\nWed, 13 Nov 2019 22:36:57 UTC (5,468 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GNNExplainer: Generating Explanations for Graph Neural Networks, by Rex Ying and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/1903.03894)\n- [TeX Source](https://arxiv.org/src/1903.03894)\n- [Other Formats](https://arxiv.org/format/1903.03894)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1903.03894&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1903.03894&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2019-03](https://arxiv.org/list/cs.LG/2019-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1903.03894?context=cs)\n\n[stat](https://arxiv.org/abs/1903.03894?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1903.03894?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1903.03894)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1903.03894)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1903.03894)\n\n### [1 blog link](https://arxiv.org/tb/1903.03894)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1903.html#abs-1903-03894) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1903-03894)\n\n[Rex Ying](https://dblp.uni-trier.de/search/author?author=Rex%20Ying)\n\n[Dylan Bourgeois](https://dblp.uni-trier.de/search/author?author=Dylan%20Bourgeois)\n\n[Jiaxuan You](https://dblp.uni-trier.de/search/author?author=Jiaxuan%20You)\n\n[Marinka Zitnik](https://dblp.uni-trier.de/search/author?author=Marinka%20Zitnik)\n\n[Jure Leskovec](https://dblp.uni-trier.de/search/author?author=Jure%20Leskovec)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1903.03894&description=GNNExplainer: Generating Explanations for Graph Neural Networks) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1903.03894&title=GNNExplainer: Generating Explanations for Graph Neural Networks)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1903.03894) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "GNNExplainer: Generating explanations for graph neural networks",
          "cleaned_query": "GNNExplainer: Generating explanations for graph neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "GraphTrail: Translating GNN Predictions into Human-Interpretable...",
          "url": "https://openreview.net/forum?id=fzlMza6dRZ&referrer=%5Bthe%20profile%20of%20Sayan%20Ranu%5D(%2Fprofile%3Fid%3D~Sayan_Ranu2)",
          "content": "GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules | OpenReview\n[![back arrow](https://openreview.net/images/arrow_left.svg)Back to**the profile of Sayan Ranu**](https://openreview.net/profile?id=~Sayan_Ranu2)\n## GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules\n[![Download PDF](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=fzlMza6dRZ)\n### [Burouj Armgaan](https://openreview.net/profile?id=~Burouj_Armgaan1),[Manthan Dalmia](https://openreview.net/profile?id=~Manthan_Dalmia1),[Sourav Medya](https://openreview.net/profile?id=~Sourav_Medya1),[Sayan Ranu](https://openreview.net/profile?id=~Sayan_Ranu2)\nPublished: 25 Sept 2024, Last Modified: 06 Nov 2024NeurIPS 2024 posterEveryone[Revisions](https://openreview.net/revisions?id=fzlMza6dRZ)[BibTeX](#)[CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)\n**Keywords:**Graph Neural Network, Explainability, Global Factual Explanation, Symbolic Regression, Computation Trees\n**TL;DR:**We generate formula based global explainations of graph neural networks using symbolic regression over computation trees identified through Shapley values.\n**Abstract:**Instance-level explanation of graph neural networks (GNNs) is a well-studied area. These explainers, however, only explain an instance (e.g., a graph) and fail to uncover the combinatorial reasoning learned by a GNN from the training data towards making its predictions. In this work, we introduce GraphTrail, the first end-to-end, global, post-hoc GNN explainer that translates the functioning of a black-box GNN model to a boolean formula over the (sub)graph level concepts without relying on local explainers. GraphTrail is unique in automatically mining the discriminative subgraph-level concepts using Shapley values. Subsequently, the GNN predictions are mapped to a human-interpretable boolean formula over these concepts through symbolic regression. Extensive experiments across diverse datasets and GNN architectures demonstrate significant improvement over existing global explainers in mapping GNN predictions to faithful logical formulae. The robust and accurate performance of GraphTrail makes it invaluable for improving GNNs and facilitates adoption in domains with strict transparency requirements.\n**Primary Area:**Graph neural networks\n**Submission Number:**12856\nLoading\n[OpenReview](https://openreview.net/about)is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the[OpenReview Sponsors](https://openreview.net/sponsors). \u00a92025OpenReview",
          "original_query": "Graphtrail: Translating gnn predictions into human-interpretable logical rules",
          "cleaned_query": "Graphtrail: Translating gnn predictions into human-interpretable logical rules",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Global Explainability of GNNs via Logic Combination of Learned ...",
          "url": "https://arxiv.org/abs/2210.07147",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2210.07147** (cs)\n\n\\[Submitted on 13 Oct 2022 ( [v1](https://arxiv.org/abs/2210.07147v1)), last revised 11 Apr 2023 (this version, v3)\\]\n\n# Title:Global Explainability of GNNs via Logic Combination of Learned Concepts\n\nAuthors: [Steve Azzolin](https://arxiv.org/search/cs?searchtype=author&query=Azzolin,+S), [Antonio Longa](https://arxiv.org/search/cs?searchtype=author&query=Longa,+A), [Pietro Barbiero](https://arxiv.org/search/cs?searchtype=author&query=Barbiero,+P), [Pietro Li\u00f2](https://arxiv.org/search/cs?searchtype=author&query=Li%C3%B2,+P), [Andrea Passerini](https://arxiv.org/search/cs?searchtype=author&query=Passerini,+A)\n\nView a PDF of the paper titled Global Explainability of GNNs via Logic Combination of Learned Concepts, by Steve Azzolin and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2210.07147)\n\n> Abstract:While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned. In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.\n\n| | |\n| --- | --- |\n| Comments: | Camera ready version for ICLR2023 publication |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO) |\n| Cite as: | [arXiv:2210.07147](https://arxiv.org/abs/2210.07147) \\[cs.LG\\] |\n| | (or [arXiv:2210.07147v3](https://arxiv.org/abs/2210.07147v3) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2210.07147](https://doi.org/10.48550/arXiv.2210.07147) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Steve Azzolin \\[ [view email](https://arxiv.org/show-email/3b65eec6/2210.07147)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2210.07147v1)**\nThu, 13 Oct 2022 16:30:03 UTC (1,292 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2210.07147v2)**\nFri, 14 Oct 2022 16:47:12 UTC (1,292 KB)\n\n**\\[v3\\]**\nTue, 11 Apr 2023 18:15:20 UTC (1,463 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Global Explainability of GNNs via Logic Combination of Learned Concepts, by Steve Azzolin and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2210.07147)\n- [TeX Source](https://arxiv.org/src/2210.07147)\n- [Other Formats](https://arxiv.org/format/2210.07147)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2210.07147&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2210.07147&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-10](https://arxiv.org/list/cs.LG/2022-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2210.07147?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2210.07147?context=cs.AI)\n\n[cs.LO](https://arxiv.org/abs/2210.07147?context=cs.LO)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2210.07147)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2210.07147)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2210.07147)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2210.07147&description=Global Explainability of GNNs via Logic Combination of Learned Concepts) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2210.07147&title=Global Explainability of GNNs via Logic Combination of Learned Concepts)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2210.07147) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Global explainability of GNNs via logic combination of learned concepts (GLGExplainer)",
          "cleaned_query": "Global explainability of GNNs via logic combination of learned concepts (GLGExplainer)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Iterative Refinement with Self-Feedback - OpenReview",
          "url": "https://openreview.net/pdf?id=S37hOerQLB",
          "content": "SELF-REFINE:\nIterative Refinement with Self-Feedback\nAman Madaan1, Niket Tandon2, Prakhar Gupta1, Skyler Hallinan3, Luyu Gao1,\nSarah Wiegreffe2, Uri Alon1\u2217, Nouha Dziri2, Shrimai Prabhumoye4, Yiming Yang1,\nShashank Gupta2, Bodhisattwa Prasad Majumder5, Katherine Hermann6,\nSean Welleck2,3, Amir Yazdanbakhsh6, Peter Clark2\n1Language Technologies Institute, Carnegie Mellon University\n2Allen Institute for Artificial Intelligence\n3University of Washington 4NVIDIA 5UC San Diego 6Google Deepmind\nselfrefine@googlegroups.com\nAbstract\nLike humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text, we\nintroduce SELF-REFINE, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an initial\noutput using an LLM; then, the same LLM provides feedback for its output and uses\nit to refine itself, iteratively. SELF-REFINE does not require any supervised training\ndata, additional training, or reinforcement learning, and instead uses a single LLM\nas the generator, refiner, and feedback provider. We evaluate SELF-REFINE across 7\ndiverse tasks, ranging from dialog response generation to mathematical reasoning,\nusing state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks,\noutputs generated with SELF-REFINE are preferred by humans and automatic\nmetrics over those generated with the same LLM using conventional one-step\ngeneration, improving by \u223c20% absolute on average in task performance. Our work\ndemonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at\ntest-time using our simple, standalone approach.2.\n1 Introduction\nAlthough large language models (LLMs) can generate coherent outputs, they often fall short in\naddressing intricate requirements. This mostly includes tasks with multifaceted objectives, such\nas dialogue response generation, or tasks with hard-to-define goals, such as enhancing program\nreadability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may\nbenefit from further iterative refinement\u2014i.e., iteratively mapping a candidate output to an improved\none\u2014to ensure that the desired quality is achieved. Iterative refinement typically involves training\na refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al.\n(2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models\nrequire large training sets or expensive human annotations (Madaan et al., 2021; Ouyang et al., 2022),\nwhich may not always be feasible to obtain. These limitations underscore the need for an effective\nrefinement approach that can be applied to various tasks without requiring extensive supervision.\nIterative self-refinement is a fundamental characteristic of human problem-solving (Simon, 1962;\nFlower and Hayes, 1981; Amabile, 1983). Iterative self-refinement is a process that involves creating\nan initial draft and subsequently refining it based on self-provided feedback. For example, when\n\u2217Now at Google DeepMind\n2Code and data at https://selfrefine.info/\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nFeedback Refine\nUse M to get feedback on its own output\nInput\nUse M to rene its previous output, given its feedba\nModel M\n1 2\n0\nFigure 1: Given an input (\u20dd0 ), SELF-REFINE starts by generating an output and passing it back to the\nsame model M to get feedback (\u20dd1 ). The feedback is passed back to M, which refines the previously\ngenerated output (\u20dd2 ). Steps (\u20dd1 ) and (\u20dd2 ) iterate until a stopping condition is met. SELF-REFINE is\ninstantiated with a language model such as GPT-3.5 and does not involve human assistance.\ndrafting an email to request a document from a colleague, an individual may initially write a direct\nrequest such as \u201cSend me the data ASAP\u201d. Upon reflection, however, the writer recognizes the\npotential impoliteness of the phrasing and revises it to \u201cHi Ashley, could you please send me the data\nat your earliest convenience?\". When writing code, a programmer may implement an initial \u201cquick\nand dirty\u201d implementation, and then, upon reflection, refactor their code to a solution that is more\nefficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement\nwithout additional training, leading to higher-quality outputs on a wide range of tasks.\nWe present SELF-REFINE: an iterative self-refinement algorithm that alternates between two gener\u0002ative steps\u2013FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs.\nGiven an initial output generated by a model M, we pass it back to the same model M to get\nfeedback. Then, the feedback is passed back to the same model to refine the previously-generated\ndraft. This process is repeated either for a specified number of iterations or until M determines that\nno further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to\nboth generate feedback and incorporate the feedback into an improved draft. Figure 1 illustrates the\nhigh-level idea, that SELF-REFINE uses the same underlying language model to generate feedback\nand refine its outputs.\nWe evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural language\nand source-code generation. We show that SELF-REFINE outperforms direct generation from strong\nLLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang et al., 2022) and\nGPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks, SELF-REFINE\nimproves the initial generation by up to absolute 13% when applied to strong code models such\nas CODEX (code-davinci-002; Chen et al., 2021). We release all of our code, which is easily\nextensible to other LLMs. In essence, our results show that even when an LLM cannot generate an\noptimal output on its first try, the LLM can often provide useful feedback and improve its own output\naccordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs from a single\nmodel without any additional training, via iterative (self-)feedback and refinement.\n2 Iterative Refinement with SELF-REFINE\nGiven an input sequence, SELF-REFINE generates an initial output, provides feedback on the output,\nand refines the output according to the feedback. SELF-REFINE iterates between feedback and\nrefinement until a desired condition is met. SELF-REFINE relies on a suitable language model\nand three prompts (for initial generation, feedback, and refinement), and does not require training.\nSELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail.\nInitial generation Given an input x, prompt pgen, and model M, SELF-REFINE generates an initial\noutput y0:\ny0 = M(pgen\u2225x). (1)\n2\n(a) Dialogue: x, yt\nUser: I am interested\nin playing Table\ntennis.\nResponse: I'm sure\nit's a great way to\nsocialize, stay active\n(b) FEEDBACK fb\nEngaging: Provides no\ninformation about table\ntennis or how to play it.\nUser understanding: Lacks\nunderstanding of user's\nneeds and state of mind.\n(c) REFINE yt+1\nResponse (refined): That's\ngreat to hear (...) ! It's\na fun sport requiring\nquick reflexes and good\nhand-eye coordination.\nHave you played before, or\nare you looking to learn?\n(d) Code optimization: x, yt\nGenerate sum of 1, ..., N\ndef sum(n):\nres = 0\nfor i in range(n+1):\nres += i\nreturn res\n(e) FEEDBACK fb\nThis code is slow as\nit uses brute force.\nA better approach is\nto use the formula\n... (n(n+1))/2.\n(f) REFINE yt+1\nCode (refined)\ndef sum_faster(n):\nreturn (n*(n+1))//2\nFigure 2: Examples of SELF-REFINE: an initial output generated by the base LLM and then passed\nback to the same LLM to receive feedback to the same LLM to refine the output . The top row\nillustrates this for dialog generation where an initial dialogue response can be transformed into a\nmore engaging one that also understands the user by applying feedback. The bottom row illustrates\nthis for code optimization where the code is made more efficient by applying feedback.\nAlgorithm 1 SELF-REFINE algorithm\nRequire: input x, model M, prompts {pgen, pfb, prefine}, stop condition stop(\u00b7)\n1: y0 = M(pgen\u2225x) \u25b7 Initial generation (Eqn. 1)\n2: for iteration t \u2208 0, 1, . . . do\n3: f bt = M(pfb\u2225x\u2225yt) \u25b7 Feedback (Eqn. 2)\n4: if stop(f bt, t) then \u25b7 Stop condition\n5: break\n6: else\n7: yt+1 = M(prefine\u2225x\u2225y0\u2225f b0\u2225...\u2225yt\u2225f bt) \u25b7 Refine (Eqn. 4)\n8: end if\n9: end for\n10: return yt\nFigure 3: The SELF-REFINE algorithm. See (\u00a72) for a discussion of each component.\nFor example, in Figure 2(d), the model generates functionally correct code for the given input.\nHere, pgen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes\nconcatenation. The few-shot prompt contains input-output pairs \u27e8x\n(k)\n, y(k)\u27e9 for the task.3\nFEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f bt on its own\noutput, given a task-specific prompt pfb for generating feedback:\nf bt = M(pfb\u2225x\u2225yt). (2)\nIntuitively, the feedback may address multiple aspects of the output. For example, in code optimiza\u0002tion, the feedback might address the efficiency, readability, and overall quality of the code.\n3\nFew-shot prompting (also referred to as \u201cin-context learning\u201d) provides a model with a prompt consisting of\nk in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9 (Brown et al., 2020).\n3\nHere, the prompt pfb provides examples of feedback in the form of input-output-feedback triples\n\u27e8x\n(k)\n, y(k), fb(k)\u27e9. We prompt the model to write feedback that is actionable and specific via f b(k).\nBy \u2018actionable\u2019, we mean the feedback should contain a concrete action that would likely improve the\noutput. By \u2018specific\u2019, we mean the feedback should identify concrete phrases in the output to change.\nFor example, the feedback in Figure 2(e) is \u201cThis code is slow as it uses a for loop which is brute\nforce. A better approach is to use the ",
          "original_query": "Self-refine: Iterative refinement with self-feedback (LLM prompting technique)",
          "cleaned_query": "Self-refine: Iterative refinement with self-feedback (LLM prompting technique)",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Exemplar-Retrieval GNNs for Context-Dependent Node Classification\nBuild a GNN variant whose prediction for a query node is computed primarily from a learned memory of training exemplars retrieved by similarity (feature + structural), mirroring Medin & Schaffer\u2019s context theory. Evaluate whether exemplar-only retrieval can match or outperform prototype-style aggregation on ill-defined graph categories (e.g., noisy social roles, ambiguous molecular functions), and analyze when retrieval yields more human-aligned decision boundaries.",
        "Exemplar-Based Global Explanations: From Retrieved Cases to Boolean Rules\nCombine an exemplar-retrieval classifier with GraphTrail/GLGExplainer to convert frequently retrieved exemplar subgraphs into reusable \u201cconcepts,\u201d then synthesize a global Boolean formula over these concepts. The contribution is a pipeline that grounds global rules in concrete training instances, enabling explanations like \u201cclass = (similar-to-exemplar-A AND motif-X) OR (similar-to-exemplar-B AND feature-Y)\u201d and allowing direct auditing of which memories drive each rule.",
        "Iterative Self-Refinement for GNN Explanations (Local\u2192Global)\nUse SELF-REFINE to repeatedly improve explanations produced by GNNExplainer (local) and GraphTrail/GLGExplainer (global) by prompting an LLM to critique explanations for faithfulness, minimality, and consistency, then revise masks/rules accordingly. Implement a closed-loop system where critique triggers additional optimization steps (e.g., adjusting mutual-information objectives or symbolic regression constraints), and quantify gains in fidelity and sparsity without additional supervision.",
        "Human-Subject Validation of \u201cTypicality\u201d for Graph Concepts\nOperationalize typicality (from context theory) for graph concepts by defining typicality scores from (i) exemplar retrieval frequency and (ii) concept-cluster centrality used in GLGExplainer/GraphTrail. Run human studies where participants judge which subgraphs are \u201cbest examples\u201d of a class, and test whether typicality-aligned concept sets improve explanation usefulness and reduce verification time (analogous to typicality RT effects in category verification).",
        "Stability-Regularized Global Rule Discovery for Inductive Graph Learning\nExtend inductive representation learning on large graphs by adding a stability objective: global logical explanations (GraphTrail/GLGExplainer) should remain invariant across neighborhood sampling, subgraph perturbations, and train/test splits. The key contribution is a training-time regularizer that jointly optimizes predictive accuracy and explanation stability, producing models whose learned \u201ccombinatorial reasoning\u201d is robust under inductive settings.",
        "Counterfactual Rule Editing: Minimal Graph Changes That Flip a Learned Formula\nGiven a global Boolean rule explanation (GraphTrail/GLGExplainer), compute minimal counterfactual edits to a graph (edge/node/feature changes) that flip the rule outcome while staying realistic (degree constraints, motif priors). This yields actionable debugging: identify whether the GNN\u2019s decision can be changed by trivial spurious cues versus meaningful structural shifts, and compare counterfactual sets to those implied by GNNExplainer masks.",
        "Shapley-to-Concept Alignment: Auditing Concept Mining in Computation Trees\nGraphTrail mines discriminative subgraph concepts using Shapley values over computation trees; propose a systematic alignment test that checks whether mined concepts correspond to stable retrieval neighborhoods in an exemplar memory model. The contribution is an audit metric and procedure that flags \u201cconcept leakage\u201d (concepts that look important via Shapley but are not consistently retrieved/used across similar instances), improving trust in global explanations.",
        "Prototype vs Exemplar Mixture Models in GNN Readouts with Explainability Hooks\nImplement a readout that learns a tunable mixture between prototype-style pooling and exemplar-style retrieval per class (or per region of the graph), then expose the learned mixture weights as part of the global explanation. Study whether ill-defined classes induce higher exemplar reliance, and whether the resulting hybrid improves both accuracy and interpretability (rules become simpler when prototypes dominate, more case-based when exemplars dominate).",
        "Concept Drift Detection via Explanation Drift on Dynamic Graphs\nFor evolving graphs, track how global logical formulas (GraphTrail/GLGExplainer) and local subgraph masks (GNNExplainer) change over time; define \u201cexplanation drift\u201d as a first-class signal for concept drift. Build detectors that trigger when the rule structure changes significantly (e.g., new motifs enter the formula), and validate on streaming benchmarks where semantics shift (fraud rings, evolving citation topics).",
        "LLM-Assisted Symbolic Regression Constraints for Domain-Safe Graph Rules\nIn GraphTrail-style symbolic regression, incorporate LLM-suggested constraints derived from domain ontologies (e.g., chemistry valence constraints, biomedical pathway constraints) to restrict the space of candidate boolean formulas. The contribution is a constrained global explainer that yields rules that are not only faithful but also domain-plausible, and a measurable reduction in \u201cnonsensical but predictive\u201d concepts discovered from purely data-driven regression."
      ],
      "generated_ideas_raw": "1. **Exemplar-Retrieval GNNs for Context-Dependent Node Classification**  \n   Build a GNN variant whose prediction for a query node is computed primarily from a learned memory of training exemplars retrieved by similarity (feature + structural), mirroring Medin & Schaffer\u2019s context theory. Evaluate whether exemplar-only retrieval can match or outperform prototype-style aggregation on ill-defined graph categories (e.g., noisy social roles, ambiguous molecular functions), and analyze when retrieval yields more human-aligned decision boundaries.\n\n2. **Exemplar-Based Global Explanations: From Retrieved Cases to Boolean Rules**  \n   Combine an exemplar-retrieval classifier with GraphTrail/GLGExplainer to convert frequently retrieved exemplar subgraphs into reusable \u201cconcepts,\u201d then synthesize a global Boolean formula over these concepts. The contribution is a pipeline that grounds global rules in concrete training instances, enabling explanations like \u201cclass = (similar-to-exemplar-A AND motif-X) OR (similar-to-exemplar-B AND feature-Y)\u201d and allowing direct auditing of which memories drive each rule.\n\n3. **Iterative Self-Refinement for GNN Explanations (Local\u2192Global)**  \n   Use SELF-REFINE to repeatedly improve explanations produced by GNNExplainer (local) and GraphTrail/GLGExplainer (global) by prompting an LLM to critique explanations for faithfulness, minimality, and consistency, then revise masks/rules accordingly. Implement a closed-loop system where critique triggers additional optimization steps (e.g., adjusting mutual-information objectives or symbolic regression constraints), and quantify gains in fidelity and sparsity without additional supervision.\n\n4. **Human-Subject Validation of \u201cTypicality\u201d for Graph Concepts**  \n   Operationalize typicality (from context theory) for graph concepts by defining typicality scores from (i) exemplar retrieval frequency and (ii) concept-cluster centrality used in GLGExplainer/GraphTrail. Run human studies where participants judge which subgraphs are \u201cbest examples\u201d of a class, and test whether typicality-aligned concept sets improve explanation usefulness and reduce verification time (analogous to typicality RT effects in category verification).\n\n5. **Stability-Regularized Global Rule Discovery for Inductive Graph Learning**  \n   Extend inductive representation learning on large graphs by adding a stability objective: global logical explanations (GraphTrail/GLGExplainer) should remain invariant across neighborhood sampling, subgraph perturbations, and train/test splits. The key contribution is a training-time regularizer that jointly optimizes predictive accuracy and explanation stability, producing models whose learned \u201ccombinatorial reasoning\u201d is robust under inductive settings.\n\n6. **Counterfactual Rule Editing: Minimal Graph Changes That Flip a Learned Formula**  \n   Given a global Boolean rule explanation (GraphTrail/GLGExplainer), compute minimal counterfactual edits to a graph (edge/node/feature changes) that flip the rule outcome while staying realistic (degree constraints, motif priors). This yields actionable debugging: identify whether the GNN\u2019s decision can be changed by trivial spurious cues versus meaningful structural shifts, and compare counterfactual sets to those implied by GNNExplainer masks.\n\n7. **Shapley-to-Concept Alignment: Auditing Concept Mining in Computation Trees**  \n   GraphTrail mines discriminative subgraph concepts using Shapley values over computation trees; propose a systematic alignment test that checks whether mined concepts correspond to stable retrieval neighborhoods in an exemplar memory model. The contribution is an audit metric and procedure that flags \u201cconcept leakage\u201d (concepts that look important via Shapley but are not consistently retrieved/used across similar instances), improving trust in global explanations.\n\n8. **Prototype vs Exemplar Mixture Models in GNN Readouts with Explainability Hooks**  \n   Implement a readout that learns a tunable mixture between prototype-style pooling and exemplar-style retrieval per class (or per region of the graph), then expose the learned mixture weights as part of the global explanation. Study whether ill-defined classes induce higher exemplar reliance, and whether the resulting hybrid improves both accuracy and interpretability (rules become simpler when prototypes dominate, more case-based when exemplars dominate).\n\n9. **Concept Drift Detection via Explanation Drift on Dynamic Graphs**  \n   For evolving graphs, track how global logical formulas (GraphTrail/GLGExplainer) and local subgraph masks (GNNExplainer) change over time; define \u201cexplanation drift\u201d as a first-class signal for concept drift. Build detectors that trigger when the rule structure changes significantly (e.g., new motifs enter the formula), and validate on streaming benchmarks where semantics shift (fraud rings, evolving citation topics).\n\n10. **LLM-Assisted Symbolic Regression Constraints for Domain-Safe Graph Rules**  \n   In GraphTrail-style symbolic regression, incorporate LLM-suggested constraints derived from domain ontologies (e.g., chemistry valence constraints, biomedical pathway constraints) to restrict the space of candidate boolean formulas. The contribution is a constrained global explainer that yields rules that are not only faithful but also domain-plausible, and a measurable reduction in \u201cnonsensical but predictive\u201d concepts discovered from purely data-driven regression.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Exemplar-Retrieval GNNs for Context-Dependent Node Classification\nBuild a GNN variant whose prediction for a query node is computed primarily from a learned memory of training exemplars retrieved by s",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Exemplar-Based Global Explanations: From Retrieved Cases to Boolean Rules\nCombine an exemplar-retrieval classifier with GraphTrail/GLGExplainer to convert frequently retrieved exemplar subgraphs into ",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Iterative Self-Refinement for GNN Explanations (Local\u2192Global)\nUse SELF-REFINE to repeatedly improve explanations produced by GNNExplainer (local) and GraphTrail/GLGExplainer (global) by prompting an L",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Human-Subject Validation of \u201cTypicality\u201d for Graph Concepts\nOperationalize typicality (from context theory) for graph concepts by defining typicality scores from (i) exemplar retrieval frequency and (",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Stability-Regularized Global Rule Discovery for Inductive Graph Learning\nExtend inductive representation learning on large graphs by adding a stability objective: global logical explanations (GraphTra",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Counterfactual Rule Editing: Minimal Graph Changes That Flip a Learned Formula\nGiven a global Boolean rule explanation (GraphTrail/GLGExplainer), compute minimal counterfactual edits to a graph (edge/",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Shapley-to-Concept Alignment: Auditing Concept Mining in Computation Trees\nGraphTrail mines discriminative subgraph concepts using Shapley values over computation trees; propose a systematic alignment",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Prototype vs Exemplar Mixture Models in GNN Readouts with Explainability Hooks\nImplement a readout that learns a tunable mixture between prototype-style pooling and exemplar-style retrieval per class ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Concept Drift Detection via Explanation Drift on Dynamic Graphs\nFor evolving graphs, track how global logical formulas (GraphTrail/GLGExplainer) and local subgraph masks (GNNExplainer) change over tim",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "LLM-Assisted Symbolic Regression Constraints for Domain-Safe Graph Rules\nIn GraphTrail-style symbolic regression, incorporate LLM-suggested constraints derived from domain ontologies (e.g., chemistry ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 3,
      "paper_title": "RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation",
      "contribution": "Introduce RAG4GFM, an end-to-end retrieval-augmented generation framework that adapts the RAG paradigm to graph corpora via hierarchical multi-level graph indexing, task-aware retrieval, and graph-fusion enhancement to enable fast knowledge updating and more faithful reasoning for Graph Foundation Models.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10225,
      "output_tokens": 957,
      "predecessor_details": [
        {
          "success": true,
          "title": "RAG Research Paper Explained: Retrieval-Augmented Generation ...",
          "url": "https://towardsai.net/p/data-science/rag-research-paper-explained-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks",
          "content": "RAG Research Paper Explained: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Towards AI\n* [**Latest](https://towardsai.net/p/)\n* [**Trends](https://medium.com/towards-artificial-intelligence/trending)\n* [**Shop](https://gumroad.com/towardsai)\n[**](https://news.google.com/publications/CAAqBwgKMNiLmgswgpayAw)[**](https://pub.towardsai.net)[**](https://linkedin.com/company/towards-artificial-intelligence)[**](https://twitter.com/towards_ai?lang=en)[**](https://www.facebook.com/towardsAl/)[**](https://instagram.com/towards_ai/)[**](https://github.com/towardsai)[**](http://feeds.feedburner.com/towards-ai)[**](mailto:pub@towardsai.net)\n[](https://towardsai.net/)\n[**](#)\n**\n[LLM Academy](https://academy.towardsai.net/?utm_source=taiwordpress&#038;utm_medium=button)[LLM Academy](https://academy.towardsai.net/?utm_source=taiwordpress&#038;utm_medium=button)\nName:Towards AILegal Name:Towards AI, Inc.Description:Towards AI is the world's leading artificial intelligence (AI) and technology publication. Read by thought-leaders and decision-makers around the world.Phone Number:+1-650-246-9381Email:pub@towardsai.net\n228 Park Avenue SouthNew York,NY10003United States\nWebsite:[https://towardsai.net/](https://towardsai.net/)Publisher:[https://towardsai.net/#publisher](https://towardsai.net/#publisher)Diversity Policy:[https://towardsai.net/about](https://towardsai.net/about)Ethics Policy:[https://towardsai.net/about](https://towardsai.net/about)Masthead:[https://towardsai.net/about](https://towardsai.net/about)\nName:Towards AILegal Name:Towards AI, Inc.Description:Towards AI is the world's leading artificial intelligence (AI) and technology publication.Founders:Roberto Iriondo,[Website](https://www.robertoiriondo.com/),\nJob Title:Co-founder and AdvisorWorks for:Towards AI, Inc.Follow Roberto:[X](https://x.com/@robiriondo),[LinkedIn](https://www.linkedin.com/in/robiriondo),[GitHub](https://github.com/robiriondo),[Google Scholar](https://scholar.google.com/citations?user=dTn7NEcAAAAJ),[Towards AI Profile](https://towardsai.net/p/author/robiriondo),[Medium](https://medium.com/@robiriondo),[ML@CMU](https://www.ml.cmu.edu/robiriondo),[FreeCodeCamp](https://www.freecodecamp.org/news/author/robiriondo/),[Crunchbase](https://www.crunchbase.com/person/roberto-iriondo),[Bloomberg](https://www.bloomberg.com/profile/person/22994840),[Roberto Iriondo, Generative AI Lab](https://roberto.generativeailab.org/),[Generative AI Lab](https://generativeailab.org/)[VeloxTrend](https://veloxtrend.com/)[Ultrarix Capital Partners](https://ultrarix.com/)Denis Piffaretti,\nJob Title:Co-founderWorks for:Towards AI, Inc.Louie Peters,\nJob Title:Co-founderWorks for:Towards AI, Inc.Louis-Fran\u00e7ois Bouchard,\nJob Title:Co-founderWorks for:Towards AI, Inc.Cover:\nLogo:\nAreas Served:WorldwideAlternate Name:Towards AI, Inc.Alternate Name:Towards AI Co.Alternate Name:towards aiAlternate Name:towardsaiAlternate Name:towards.aiAlternate Name:taiAlternate Name:toward aiAlternate Name:toward.aiAlternate Name:Towards AI, Inc.Alternate Name:towardsai.netAlternate Name:pub.towardsai.net\nFollow us on:[Facebook](https://www.facebook.com/towardsAl/)[X](https://x.com/towards_ai)[LinkedIn](https://www.linkedin.com/company/towards-artificial-intelligence)[Instagram](https://www.instagram.com/towards_ai/)[Youtube](https://www.youtube.com/channel/UCQNjFuhOJM1YqFTPY1Q_kYQ)[Github](https://github.com/towardsai)[Google My Business](< https://local.google.com/place?id=4955254490173856159&amp;use=srp&amp;ved=1t:65428&amp;_ga=2.191706990.559569912.1635283323-985655235.1633987376#fpstate=lie>)[Google Search](https://www.google.com/search?ved=1t:65428&amp;_ga=2.191706990.559569912.1635283323-985655235.1633987376&amp;q=Towards+AI&amp;ludocid=4955254490173856159&amp;lsig=AB86z5Ur3DZsSmdOFFUwd-bMHTIe#fpstate=lie)[Google News](https://news.google.com/publications/CAAqBwgKMNiLmgswgpayAw?oc=3&amp;ceid=US:en)[Google Maps](https://g.page/TowardsAI?gm)[Discord](https://discord.com/invite/V7RGX9XKAW)[Shop](https://gumroad.com/towardsai)[Towards AI, Medium Editorial](https://towardsai.medium.com)[Medium](https://medium.com/towards-artificial-intelligence)[Flipboard](https://flipboard.com/@Towards_AI)[Publication](https://pub.towardsai.net/)[Feed](https://feed.towardsai.net/)[Sponsors](https://sponsors.towardsai.net/)[Sponsors](https://members.towardsai.net/)[Contribute](https://contribute.towardsai.net/)\n5stars \u2013based on497reviews\n#### Frequently Used, Contextual References\nTODO: Remember to copy unique IDs whenever it needs used. i.e., URL: 304b2e42315e\n## Resources\nOur 15 AI experts built the most comprehensive, practical, 90+ lesson courses to master AI Engineering - we have pathways for any experience at[Towards AI Academy](https://academy.towardsai.net/?utm_source=taiwordpress&amp;utm_medium=banner). Cohorts still open - use COHORT10 for 10% off.\n## Publication\n* [Home](https://towardsai.net/)\n* [Publication](https://towardsai.net/p)\n* [Data Science](https://towardsai.net/ai/data-science)\n* RAG Research Paper Explained: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n[Data Science](https://towardsai.net/ai/data-science)[Latest](https://towardsai.net/ai/l)[Machine Learning](https://towardsai.net/ai/machine-learning)\n# RAG Research Paper Explained: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n**[Aman Agrawal](https://towardsai.net/p/author/amanagrawal)\n[**11 likes](https://towardsai.net/wp-admin/admin-ajax.php?action=process_simple_like&post_id=39785&nonce=ca1c8851ce&is_comment=0&disabled=true)\n**January 3, 2025\nLast Updated on January 3, 2025 by[Editorial Team](https://towardsai.net/p/author/editorial-team)\n#### **Author(s):[Aman Agrawal](https://pub.towardsai.net/@amannagrawall002)**\nOriginally published on[Towards AI](https://towardsai.net/).\n## **RAG Research Paper Explained:**Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n## *Step Inside the World of RAG for a Detailed Breakdown of Its Core Components and Advanced Fine-Tuning Strategies*\nRAG is one of the most hyped terms in the AI and[LLM](https://academy.towardsai.net/courses/beginner-to-advanced-llm-dev)domain, and many articles tell us about advanced techniques in Retrieval-Augmented Generation (RAG), but few explore what happens behind the scenes. If you\u2019ve never encountered an article explaining the mechanism of Retrieval and Generation, this one\u2019s for you. This article aims at everything from scratch, so it\u2019s going to be a long article, we will understand the whole mechanism in depth. I am not gonna write down another variant of RAG like adaptive, corrective, contextual, order-preserve etc , there is plenty of deep information available on it. I have just written down the mechanism of pre-training that RAG used as in the original research paper which is[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\nI have written down this article with the motivation of explaining the paper in simple language, with mathematical intuition(loss functions, back-prop) and using simple examples and also with the motivation that maybe someday someone will make some tweak in the mechanism of pre-training in RAG instead of making a wrapper on vanilla RAG or craft another variant of RAG because I believe more on engineering and coding mechanism of retriever or generator, rather than how to feed better English to generator model for better accuracy, as RAG is not that much scalable when it comes to production and usage of apps among millions of users as it comes with the cost of high token usage, some extra engineering and coding is required in the whole pipeline of RAG to bring down the cost, that can only happen when one have a basic understanding of how pre-training and fine-tuning of vanilla RAG has been done , around which variants are built.\nThis will be a detailed breakdown of the RAG research paper of the two mechanisms discussed in the paper &#8211;\n* **RAG sequence**\n* **RAG token-based**\nIn this article, the discussion is about the two main parts of RAG separately: Retriever and the Generator. We explain how each part works and how they come together to make RAG function. We\u2019ll also cover how these systems are trained and improved through fine-tuning and pre-training and how errors are managed through loss functions and backpropagation. The goal is to cover everything in depth, using simple examples (I have taken the help of ChatGPT to make examples at various instances, which will mention in the article itself whether there\u2019s use of an AI tool to generate the text and related examples) to help understand each part of the process clearly. More or less, the concepts related to fine-tuning various RAG techniques remain the same, so understanding the mechanism of vanilla RAG as discussed in the original RAG paper \u2014Retrieval-Augmented Generation for Knowledge-Intensive[NLP](https://towardsai.net/p/nlp/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0)Tasks will develop the core concepts.\nSo, let\u2019s go ahead with this. This is a long read, so have patience and try to read it through till the end, it will give you a lot of perspective on how RAG works \u2014IN and OUT.\nLet\u2019s go step by step and try to develop the intuition behind the Retrieval Augmented Generation and RAG fine-tuning and then go into details of joint pre-training and fine-tuning of RAG.\nThe traditional vanilla RAG feeds the generative model with context attached (retrieval from the external database) along with the input so that the generative model produces a better answer compared to the response generated by the generative model alone without any retrieval of context from an external database.\nSo surely, using RAG, our generative response would become better in comparison to a solely generative model without retrieval of context as we are trying to give the model the right context, the right kind of information for generating the answer ",
          "original_query": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
          "cleaned_query": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Paper page - Inductive Representation Learning on Large Graphs",
          "url": "https://huggingface.co/papers/1706.02216",
          "content": "[Papers](https://huggingface.co/papers)\n\narxiv:1706.02216\n\n# Inductive Representation Learning on Large Graphs\n\nPublished on Jun 7, 2017\n\n[Upvote\\\n\\\n2](https://huggingface.co/login?next=%2Fpapers%2F1706.02216)\n\nAuthors:\n\nWilliam L. Hamilton\n\n,\n\nRex Ying\n\n,\n\nJure Leskovec\n\n## Abstract\n\nGraphSAGE generates embeddings for unseen nodes by sampling and aggregating features from local neighborhoods, outperforming baselines on inductive node-classification tasks.\n\nAI-generated summary\n\nLow-dimensional embeddings of nodes in [large graphs](https://huggingface.co/papers?q=large%20graphs) have proved extremely\nuseful in a variety of [prediction tasks](https://huggingface.co/papers?q=prediction%20tasks), from content recommendation to\nidentifying protein functions. However, most existing approaches require that\nall nodes in the graph are present during training of the embeddings; these\nprevious approaches are inherently transductive and do not naturally generalize\nto unseen nodes. Here we present GraphSAGE, a general, [inductive framework](https://huggingface.co/papers?q=inductive%20framework) that\nleverages [node feature information](https://huggingface.co/papers?q=node%20feature%20information) (e.g., text attributes) to efficiently\ngenerate [node embeddings](https://huggingface.co/papers?q=node%20embeddings) for previously unseen data. Instead of training\nindividual embeddings for each node, we learn a function that generates\nembeddings by sampling and aggregating features from a node's local\nneighborhood. Our algorithm outperforms strong baselines on three inductive\n[node-classification benchmarks](https://huggingface.co/papers?q=node-classification%20benchmarks): we classify the category of unseen nodes in\nevolving information graphs based on citation and [Reddit post data](https://huggingface.co/papers?q=Reddit%20post%20data), and we show\nthat our algorithm generalizes to completely unseen graphs using a multi-graph\ndataset of [protein-protein interactions](https://huggingface.co/papers?q=protein-protein%20interactions).\n\n[View arXiv page](https://arxiv.org/abs/1706.02216) [View PDF](https://arxiv.org/pdf/1706.02216) [Add to collection](https://huggingface.co/login?next=%2Fpapers%2F1706.02216)\n\n### Community\n\ndeleted\n\n[Jul 8, 2024](https://huggingface.co/huggingface.co#668c0710ad3c9cc911d1a65a)\n\nThis comment has been hidden\n\nEditPreview\n\nUpload images, audio, and videos by dragging in the text input, pasting, or clicking here.\n\nTap or paste here to upload images\n\nComment\n\n\u00b7 [Sign up](https://huggingface.co/join?next=%2Fpapers%2F1706.02216) or\n[log in](https://huggingface.co/login?next=%2Fpapers%2F1706.02216) to comment\n\n[Upvote\\\n\\\n2](https://huggingface.co/login?next=%2Fpapers%2F1706.02216)\n\n## Models citing this paper 0\n\nNo model linking this paper\n\nCite arxiv.org/abs/1706.02216 in a model README.md to link it from this page.\n\n## Datasets citing this paper 0\n\nNo dataset linking this paper\n\nCite arxiv.org/abs/1706.02216 in a dataset README.md to link it from this page.\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/1706.02216 in a Space README.md to link it from this page.\n\n## Collections including this paper 1",
          "original_query": "Inductive representation learning on large graphs (GraphSAGE)",
          "cleaned_query": "Inductive representation learning on large graphs (GraphSAGE)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Graph Foundation Models: A Comprehensive Survey",
          "url": "https://arxiv.org/html/2505.15116",
          "content": "Graph Foundation Models: A Comprehensive Survey\n# Graph Foundation Models: A Comprehensive Survey\nZehong Wang\u2020,\u2217,\u229b,1Zheyuan Liu\u2217,1Tianyi Ma\u2217,1Jiazheng Li\u2217,2Zheyuan Zhang\u2217,1\nXingbo Fu\u2217,3Yiyang Li\u2217,1Zhengqing Yuan\u2217,1Wei Song1Yijun Ma1Qingkai Zeng1\nXiusi Chen4Jianan Zhao6,7Jundong Li3Meng Jiang1Pietro Li\u00f25\nNitesh Chawla1Chuxu Zhang2Yanfang Ye\u229b,1\n\u2020Project Leader\u2217Major Student Contributors\n\u229bCorrespondance: Zehong Wang &lt;zwang43@nd.edu&gt;, Yanfang Ye &lt;yye7@nd.edu&gt;\n1University of Notre Dame,2University of Connecticut,3University of Virginia,\n4University of Illinois Urbana-Champaign,5University of Cambridge,\n6Mila - Qu\u00e9bec AI Institute,7Universit\u00e9 de Montr\u00e9al\n###### Abstract\nGraph-structured data pervades domains such as social networks, biological systems, knowledge graphs, and recommender systems. While foundation models have transformed natural language processing, vision, and multimodal learning through large-scale pretraining and generalization, extending these capabilities to graphs\u2014characterized by non-Euclidean structures and complex relational semantics\u2014poses unique challenges and opens new opportunities. To this end,Graph Foundation Models(GFMs) aim to bring scalable, general-purpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms. We categorize GFMs by their generalization scope\u2014universal, task-specific, and domain-specific\u2014and review representative methods, key innovations, and theoretical insights within each category. Beyond methodology, we examine theoretical foundations including transferability and emergent capabilities, and highlight key challenges such as structural alignment, heterogeneity, scalability, and evaluation. Positioned at the intersection of graph learning and general-purpose AI, GFMs are poised to become foundational infrastructure for open-ended reasoning over structured data. This survey consolidates current progress and outlines future directions to guide research in this rapidly evolving field.Resources are available at[https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs](https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs).\n## 1Introduction\n![Refer to caption](x1.png)Figure 1:From Task-Specific Graph Models to General-Purpose Graph Foundation Models.This figure contrasts the paradigm shift from traditional Graph Neural Networks (GNNs) to Graph Foundation Models (GFMs).(a)GFMs are pretrained on large-scale graph corpora spanning multiple domains (e.g., social, web, academic, molecular) to acquire broadly transferable representations. Through various adaptation techniques\u2014such as fine-tuning, distillation, prompting, or zero-shot inference\u2014they can generalize across a wide spectrum of downstream tasks, including node classification, link prediction, graph classification, and graph-to-text generation.(b)In contrast, traditional GNNs are typically trained in an end-to-end manner on a single-domain dataset for a specific task, often lacking the scalability and generalization capabilities required for open-world settings. This shift mirrors the transition observed in language and vision domains, where foundation models have redefined the standard for general-purpose intelligence.\nThe pursuit of aone-model-fits-allparadigm stands as one of the most ambitious and transformative goals in machine learning. This vision aspires to develop highly generalizable models capable of performing a wide spectrum of tasks across diverse domains, without requiring extensive task-specific architecture design or training. Historically, machine learning has been dominated by specialized models tailored to specific data modalities and objectives> [\n[> 1\n](https://arxiv.org/html/2505.15116v1#bib.bib1)> ]\n, often requiring handcrafted features> [\n[> 2\n](https://arxiv.org/html/2505.15116v1#bib.bib2)> ]\nand domain-dependent optimization strategies> [\n[> 3\n](https://arxiv.org/html/2505.15116v1#bib.bib3)> ]\n. From early rule-based systems and linear classifiers to the rise of deep learning, the evolution of machine learning has been marked by progressive gains in representation learning, scalability, and task performance> [\n[> 4\n](https://arxiv.org/html/2505.15116v1#bib.bib4)> , [> 5\n](https://arxiv.org/html/2505.15116v1#bib.bib5)> ]\n. Classical models such as decision trees, support vector machines (SVMs), and k-nearest neighbors (KNN) demonstrated success in low-dimensional and structured settings, but faced challenges when applied to high-dimensional, unstructured, or multimodal data. The emergence of deep learning models\u2014such as convolutional neural networks (CNNs) for vision> [\n[> 6\n](https://arxiv.org/html/2505.15116v1#bib.bib6)> ]\nand recurrent neural networks (RNNs) for sequential data> [\n[> 7\n](https://arxiv.org/html/2505.15116v1#bib.bib7)> , [> 8\n](https://arxiv.org/html/2505.15116v1#bib.bib8)> ]\n\u2014significantly advanced performance in perceptual tasks. Nonetheless, these models still required task-specific tuning, architecture adjustments, and large-scale labeled datasets to achieve robust generalization. A paradigm shift occurred with the development oftransfer learning> [\n[> 9\n](https://arxiv.org/html/2505.15116v1#bib.bib9)> ]\nandself-supervised learning> [\n[> 10\n](https://arxiv.org/html/2505.15116v1#bib.bib10)> ]\n, which enabled models to learn broadly transferable representations from large-scale unlabeled data. These developments laid the groundwork for the emergence offoundation models> [\n[> 11\n](https://arxiv.org/html/2505.15116v1#bib.bib11)> ]\n, which are trained on massive datasets with the objective of acquiring universal knowledge that can be readily adapted to a wide array of downstream tasks.\nFoundation models are characterized by their scale, general-purpose nature, and pretraining across heterogeneous data sources. They are built to capture transferable inductive biases, enabling strong performance with minimal task-specific supervision. Scaling laws> [\n[> 12\n](https://arxiv.org/html/2505.15116v1#bib.bib12)> , [> 13\n](https://arxiv.org/html/2505.15116v1#bib.bib13)> ]\nand data-driven learning paradigms have driven their success across numerous domains, including natural language processing, computer vision, and robotics. For instance,Large Language Models(LLMs)> [\n[> 14\n](https://arxiv.org/html/2505.15116v1#bib.bib14)> , [> 15\n](https://arxiv.org/html/2505.15116v1#bib.bib15)> ]\nprocess text by tokenizing input sequences and formulating tasks such as translation, summarization, or reasoning as autoregressive next-token prediction problems. Similarly,Large Vision Models(LVMs)> [\n[> 16\n](https://arxiv.org/html/2505.15116v1#bib.bib16)> , [> 17\n](https://arxiv.org/html/2505.15116v1#bib.bib17)> , [> 18\n](https://arxiv.org/html/2505.15116v1#bib.bib18)> ]\ntreat visual inputs as sequences of tokens and apply Transformer-based architectures for visual question answering, captioning, or image generation. These models exhibit remarkable zero-shot and few-shot generalization capabilities, enabling rapid adaptation to novel tasks without requiring substantial fine-tuning.\nIn this context, the rise ofGraph Foundation Models(GFMs), as illustrated in Figure[1](https://arxiv.org/html/2505.15116v1#S1.F1), seeks to extend these capabilities to graph-structured data\u2014an essential yet fundamentally different modality characterized by relational dependencies, permutation invariance, and non-Euclidean geometry> [\n[> 19\n](https://arxiv.org/html/2505.15116v1#bib.bib19)> , [> 20\n](https://arxiv.org/html/2505.15116v1#bib.bib20)> , [> 21\n](https://arxiv.org/html/2505.15116v1#bib.bib21)> ]\n. GFMs aspire to offer a unified, pretrainable, and adaptable solution for a wide range of graph-based applications, spanning from molecular property prediction and knowledge graph reasoning to social network analysis and recommendation systems. For instance, OFA> [\n[> 22\n](https://arxiv.org/html/2505.15116v1#bib.bib22)> ]\noperates on eight text-attributed graphs (TAGs), spanning citation networks, Wikipedia networks, knowledge graphs, and molecular graphs, where each node is associated with a textual description. By employing a shared textual encoder, OFA projects these node descriptions into a unified embedding space, thereby aligning node features across graphs. To bridge the gap between pretraining and downstream tasks, it further introduces aprompt graphmechanism tailored to facilitate task adaptation. Similarly, GFT> [\n[> 23\n](https://arxiv.org/html/2505.15116v1#bib.bib23)> ]\nidentifies transferable patterns in graph data by modeling them as computation trees. It aligns node representations across graphs via a tree reconstruction task designed to capture cross-domain generalization. A key innovation of GFT lies in its construction of a transferable tree vocabulary, which encodes structural patterns shared across diverse graph domains. Beyond these general-purpose models, various GFMs have been proposed for specific tasks\u2014such as node classification> [\n[> 24\n](https://arxiv.org/html/2505.15116v1#bib.bib24)> , [> 25\n](https://arxiv.org/html/2505.15116v1#bib.bib25)> ]\n, anomaly detection> [\n[> 26\n](https://arxiv.org/html/2505.15116v1#bib.bib26)> ]\n, and recommendation systems> [\n[> 27\n](https://arxiv.org/html/2505.15116v1#bib.bib27)> ]\n\u2014or are specialized for particular domains, including knowledge graphs> [\n[> 28\n](https://arxiv.org/html/2505.15116v1#bib.bib28)> , [> 29\n](https://arxiv.org/html/2505.15116v1#bib.bib29)> ]\n, molecular graphs> [\n[> 30\n](https://arxiv.org/html/2505.15116v1#bib.bib30)> , [> 31\n](https://arxiv.org/html/2505.15116v1#bib.bib31)> ]\n, and computation graphs> [\n[> 32\n](https://arxiv.org/html/2505.15116v1#bib.bib32)> , [> 33\n](https://arxiv.org/html/2505.15116v1#bib.bib33)> ]\n.\nExisting Surveys.Despite the rapid p",
          "original_query": "Towards graph foundation models: A survey and beyond",
          "cleaned_query": "Towards graph foundation models: A survey and beyond",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "YaooXu/GoG: Generate-on-Graph: Treat LLM as both Agent and KG ...",
          "url": "https://github.com/yaooxu/gog",
          "content": "[Skip to content](https://github.com/YaooXu/GoG#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/YaooXu/GoG) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/YaooXu/GoG) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/YaooXu/GoG) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[YaooXu](https://github.com/YaooXu)/ **[GoG](https://github.com/YaooXu/GoG)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FYaooXu%2FGoG) You must be signed in to change notification settings\n- [Fork\\\n2](https://github.com/login?return_to=%2FYaooXu%2FGoG)\n- [Star\\\n14](https://github.com/login?return_to=%2FYaooXu%2FGoG)\n\n\n[14\\\nstars](https://github.com/YaooXu/GoG/stargazers) [2\\\nforks](https://github.com/YaooXu/GoG/forks) [Branches](https://github.com/YaooXu/GoG/branches) [Tags](https://github.com/YaooXu/GoG/tags) [Activity](https://github.com/YaooXu/GoG/activity)\n\n[Star](https://github.com/login?return_to=%2FYaooXu%2FGoG)\n\n[Notifications](https://github.com/login?return_to=%2FYaooXu%2FGoG) You must be signed in to change notification settings\n\n# YaooXu/GoG\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmain\n\n[Branches](https://github.com/YaooXu/GoG/branches) [Tags](https://github.com/YaooXu/GoG/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit ## History [32 Commits](https://github.com/YaooXu/GoG/commits/main/) |\n| [CoT](https://github.com/YaooXu/GoG/tree/main/CoT) | [CoT](https://github.com/YaooXu/GoG/tree/main/CoT) | | |\n| [Freebase](https://github.com/YaooXu/GoG/tree/main/Freebase) | [Freebase](https://github.com/YaooXu/GoG/tree/main/Freebase) | | |\n| [Wikidata](https://github.com/YaooXu/GoG/tree/main/Wikidata) | [Wikidata](https://github.com/YaooXu/GoG/tree/main/Wikidata) | | |\n| [assets](https://github.com/YaooXu/GoG/tree/main/assets) | [assets](https://github.com/YaooXu/GoG/tree/main/assets) | | |\n| [data](https://github.com/YaooXu/GoG/tree/main/data) | [data](https://github.com/YaooXu/GoG/tree/main/data) | | |\n| [prompts](https://github.com/YaooXu/GoG/tree/main/prompts) | [prompts](https://github.com/YaooXu/GoG/tree/main/prompts) | | |\n| [prompts2](https://github.com/YaooXu/GoG/tree/main/prompts2) | [prompts2](https://github.com/YaooXu/GoG/tree/main/prompts2) | | |\n| [src](https://github.com/YaooXu/GoG/tree/main/src) | [src](https://github.com/YaooXu/GoG/tree/main/src) | | |\n| [tools](https://github.com/YaooXu/GoG/tree/main/tools) | [tools](https://github.com/YaooXu/GoG/tree/main/tools) | | |\n| [.gitignore](https://github.com/YaooXu/GoG/blob/main/.gitignore) | [.gitignore](https://github.com/YaooXu/GoG/blob/main/.gitignore) | | |\n| [README.md](https://github.com/YaooXu/GoG/blob/main/README.md) | [README.md](https://github.com/YaooXu/GoG/blob/main/README.md) | | |\n| [get\\_topic\\_entity.py](https://github.com/YaooXu/GoG/blob/main/get_topic_entity.py) | [get\\_topic\\_entity.py](https://github.com/YaooXu/GoG/blob/main/get_topic_entity.py) | | |\n| [requirements.txt](https://github.com/YaooXu/GoG/blob/main/requirements.txt) | [requirements.txt](https://github.com/YaooXu/GoG/blob/main/requirements.txt) | | |\n| [test.py](https://github.com/YaooXu/GoG/blob/main/test.py) | [test.py](https://github.com/YaooXu/GoG/blob/main/test.py) | | |\n| View all files |\n\n## Repository files navigation\n\n# Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering\n\nThis repository contains official implementation for paper `Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering`.\n\nIn this documentation, we detail how to construct Incomplete KGs and run GoG.\n\n## Freebase Setup\n\nSee ./Freebase/README.md\n\n## Selecting Crucial Triples\n\nThis step can be skipped, as all processed data are contained in the /data.\n\n```\npython src/generate_samples_with_crucial_edges.py\n```\n\n## Starting name\\_to\\_id service\n\nDownloading the pickle file from [Google Drive](https://drive.google.com/file/d/1PIUDBwbiuhUHJ4FdujfXfuOUWkb40NDJ/view?usp=drive_link), and put it in Freebase/bm25.pkl.\n\nStart the service with this command, and the default port is 18891.\n\n```\npython src/bm25_name2ids.py\n```\n\n## Runing GoG\n\n```\npython src/GoG.py --n_process=4 --dataset data/cwq/data_with_ct_1000_-1_1.json\n```\n\n## Proxy\n\nIf you want to access openai and huggingface with proxy, please uncomment lines and change the proxy address in `run_llm` in `src/llms/interface.py` and `set_environment_variable` in `src/utils.py`.\n\n## About\n\nNo description, website, or topics provided.\n\n### Resources\n\n[Readme](https://github.com/YaooXu/GoG#readme-ov-file)\n\n[Activity](https://github.com/YaooXu/GoG/activity)\n\n### Stars\n\n[**14**\\\nstars](https://github.com/YaooXu/GoG/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/YaooXu/GoG/watchers)\n\n### Forks\n\n[**2**\\\nforks](https://github.com/YaooXu/GoG/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FYaooXu%2FGoG&report=YaooXu+%28user%29)\n\n## [Releases](https://github.com/YaooXu/GoG/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/YaooXu/packages?repo_name=GoG)\n\nNo packages published\n\n## Languages\n\n- [Python99.7%](https://github.com/YaooXu/GoG/search?l=python)\n- [Shell0.3%](https://github.com/YaooXu/GoG/search?l=shell)\n\nYou can\u2019t perform that action at this time.",
          "original_query": "Generate-on-graph: Treat llm as both agent and kg in incomplete knowledge graph question answering",
          "cleaned_query": "Generate-on-graph: Treat llm as both agent and kg in incomplete knowledge graph question answering",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "GraphEdit: Large Language Models for Graph Structure Learning",
          "url": "https://arxiv.org/abs/2402.15183",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2402.15183** (cs)\n\n\\[Submitted on 23 Feb 2024 ( [v1](https://arxiv.org/abs/2402.15183v1)), last revised 10 Mar 2025 (this version, v5)\\]\n\n# Title:GraphEdit: Large Language Models for Graph Structure Learning\n\nAuthors: [Zirui Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo,+Z), [Lianghao Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia,+L), [Yanhua Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+Y), [Yuling Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Y), [Kangkang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu,+K), [Zhiyong Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+Z), [Chao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+C)\n\nView a PDF of the paper titled GraphEdit: Large Language Models for Graph Structure Learning, by Zirui Guo and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2402.15183) [HTML (experimental)](https://arxiv.org/html/2402.15183v5)\n\n> Abstract:Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy connections but also identifies node-wise dependencies from a global perspective, providing a comprehensive understanding of the graph structure. We conduct extensive experiments on multiple benchmark datasets to demonstrate the effectiveness and robustness of GraphEdit across various settings. We have made our model implementation available at: [this https URL](https://github.com/HKUDS/GraphEdit).\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2402.15183](https://arxiv.org/abs/2402.15183) \\[cs.LG\\] |\n| (or [arXiv:2402.15183v5](https://arxiv.org/abs/2402.15183v5) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2402.15183](https://doi.org/10.48550/arXiv.2402.15183) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zirui Guo \\[ [view email](https://arxiv.org/show-email/d08867fa/2402.15183)\\] **[\\[v1\\]](https://arxiv.org/abs/2402.15183v1)**\nFri, 23 Feb 2024 08:29:42 UTC (959 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2402.15183v2)**\nTue, 27 Feb 2024 08:22:11 UTC (959 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2402.15183v3)**\nThu, 29 Feb 2024 04:15:44 UTC (960 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2402.15183v4)**\nTue, 5 Mar 2024 05:22:00 UTC (959 KB)\n**\\[v5\\]**\nMon, 10 Mar 2025 14:04:39 UTC (408 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GraphEdit: Large Language Models for Graph Structure Learning, by Zirui Guo and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2402.15183)\n- [HTML (experimental)](https://arxiv.org/html/2402.15183v5)\n- [TeX Source](https://arxiv.org/src/2402.15183)\n- [Other Formats](https://arxiv.org/format/2402.15183)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2402.15183&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2402.15183&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2024-02](https://arxiv.org/list/cs.LG/2024-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2402.15183?context=cs) [cs.AI](https://arxiv.org/abs/2402.15183?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.15183)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15183)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.15183)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2402.15183) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Graphedit: Large language models for graph structure learning",
          "cleaned_query": "Graphedit: Large language models for graph structure learning",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Inductive GraphRAG for Unseen Entities via GraphSAGE Neighborhood Synthesis\nExtend RAG to open-world settings by generating embeddings for newly observed entities/doc nodes using GraphSAGE-style neighborhood sampling over an evolving entity-document graph. Evaluate whether inductive node embeddings improve retrieval quality and answer faithfulness when the query mentions entities not present at training time.",
        "Uncertainty-Calibrated Retrieval from Incomplete KGs (GoG + Selective Generation)\nAugment GoG-style incomplete KG QA with an uncertainty model that decides when to (a) traverse available triples, (b) ask the LLM to \u201cgenerate-on-graph\u201d missing links, or (c) fall back to text RAG. The key contribution is a decision policy trained on abstention/verification signals to reduce hallucinated KG completions while maintaining coverage.",
        "GraphEdit-for-RAG: LLM-Driven Denoising of Document Graphs to Improve Evidence Quality\nApply GraphEdit to learn/clean edges in a document graph (doc\u2013doc, entity\u2013doc, claim\u2013source) built from a corpus used for RAG. Measure how LLM-edited structure (removing noisy edges and adding latent links) changes retrieval sets, citation accuracy, and contradiction rates in generated answers.",
        "Contrastive Pretraining Objective Aligning Text Chunks and KG Subgraphs (Cross-Modal GFM)\nPretrain a model to align embeddings of (question, supporting text passages) with embeddings of (supporting KG subgraphs) using contrastive learning, then use it as the retriever in RAG. The actionable contribution is a unified retriever that can return either passages or subgraphs depending on which better explains the query, enabling hybrid evidence for generation.",
        "Graph-Prompted Generation: Structured \u201cReasoning Paths\u201d as a Control Interface for LLMs\nInstead of providing raw retrieved triples, convert retrieved subgraphs into constrained \u201cgraph prompts\u201d (e.g., ordered relation-path templates with slots) that the LLM must follow while generating. Test whether graph-constrained prompting improves multi-hop QA consistency and reduces spurious relation jumps compared to unconstrained CoT and vanilla RAG.",
        "Verification-as-Message-Passing: Post-Generation Fact Checking with GNN Over Cited Evidence\nAfter the LLM produces an answer with citations (passages/triples), build a small evidence graph linking claims \u2194 cited nodes \u2194 entities and run a GNN to score claim support. Use these scores to trigger targeted re-retrieval or answer revision, creating an end-to-end \u201cgenerate\u2013verify\u2013repair\u201d loop grounded in graph propagation.",
        "Dynamic Retrieval Depth via Graph Topology (Adaptive k Using Structural Signals)\nDesign a retrieval controller that chooses how many items to retrieve (k) and how far to expand neighborhoods based on graph properties of the initial hits (e.g., clustering coefficient, path redundancy, degree skew). The contribution is an adaptive retrieval strategy that reduces token/cost while preserving answer accuracy, especially on multi-hop questions.",
        "Benchmark: Open-World Incomplete-KG + Text Corpus QA for Inductive GraphRAG\nCreate a dataset where questions require combining an incomplete KG (missing edges/entities) with an accompanying text corpus, and where test-time introduces unseen entities/relations to stress inductive generalization. Provide standardized splits, evaluation for both answer correctness and evidence correctness, and baselines spanning GoG, GraphEdit, GraphSAGE retrieval, and classical RAG.",
        "Safety and Robustness Study: Adversarial Edge Injection Attacks on Graph-Augmented RAG\nSystematically study how malicious or noisy edges (in KGs or document graphs) can steer retrieval and degrade generation, and test defenses using GraphEdit denoising plus GFM-based outlier detection. The deliverable is an attack/defense protocol with measurable robustness gains (e.g., reduced targeted misinformation success rate under controlled edge poisoning)."
      ],
      "generated_ideas_raw": "1. **Graph-Aware RAG with GFM-Based Retrieval Re-Ranking**  \n   Build a RAG pipeline where retrieved evidence is not just text chunks but also graph neighborhoods (entities + relations) encoded by a pretrained Graph Foundation Model. Use GFM embeddings to re-rank candidates by structural relevance (e.g., shortest-path coherence to query entities) and compare against purely vector/BM25 retrieval on knowledge-intensive QA.\n\n2. **Inductive GraphRAG for Unseen Entities via GraphSAGE Neighborhood Synthesis**  \n   Extend RAG to open-world settings by generating embeddings for newly observed entities/doc nodes using GraphSAGE-style neighborhood sampling over an evolving entity-document graph. Evaluate whether inductive node embeddings improve retrieval quality and answer faithfulness when the query mentions entities not present at training time.\n\n3. **Uncertainty-Calibrated Retrieval from Incomplete KGs (GoG + Selective Generation)**  \n   Augment GoG-style incomplete KG QA with an uncertainty model that decides when to (a) traverse available triples, (b) ask the LLM to \u201cgenerate-on-graph\u201d missing links, or (c) fall back to text RAG. The key contribution is a decision policy trained on abstention/verification signals to reduce hallucinated KG completions while maintaining coverage.\n\n4. **GraphEdit-for-RAG: LLM-Driven Denoising of Document Graphs to Improve Evidence Quality**  \n   Apply GraphEdit to learn/clean edges in a document graph (doc\u2013doc, entity\u2013doc, claim\u2013source) built from a corpus used for RAG. Measure how LLM-edited structure (removing noisy edges and adding latent links) changes retrieval sets, citation accuracy, and contradiction rates in generated answers.\n\n5. **Contrastive Pretraining Objective Aligning Text Chunks and KG Subgraphs (Cross-Modal GFM)**  \n   Pretrain a model to align embeddings of (question, supporting text passages) with embeddings of (supporting KG subgraphs) using contrastive learning, then use it as the retriever in RAG. The actionable contribution is a unified retriever that can return either passages or subgraphs depending on which better explains the query, enabling hybrid evidence for generation.\n\n6. **Graph-Prompted Generation: Structured \u201cReasoning Paths\u201d as a Control Interface for LLMs**  \n   Instead of providing raw retrieved triples, convert retrieved subgraphs into constrained \u201cgraph prompts\u201d (e.g., ordered relation-path templates with slots) that the LLM must follow while generating. Test whether graph-constrained prompting improves multi-hop QA consistency and reduces spurious relation jumps compared to unconstrained CoT and vanilla RAG.\n\n7. **Verification-as-Message-Passing: Post-Generation Fact Checking with GNN Over Cited Evidence**  \n   After the LLM produces an answer with citations (passages/triples), build a small evidence graph linking claims \u2194 cited nodes \u2194 entities and run a GNN to score claim support. Use these scores to trigger targeted re-retrieval or answer revision, creating an end-to-end \u201cgenerate\u2013verify\u2013repair\u201d loop grounded in graph propagation.\n\n8. **Dynamic Retrieval Depth via Graph Topology (Adaptive k Using Structural Signals)**  \n   Design a retrieval controller that chooses how many items to retrieve (k) and how far to expand neighborhoods based on graph properties of the initial hits (e.g., clustering coefficient, path redundancy, degree skew). The contribution is an adaptive retrieval strategy that reduces token/cost while preserving answer accuracy, especially on multi-hop questions.\n\n9. **Benchmark: Open-World Incomplete-KG + Text Corpus QA for Inductive GraphRAG**  \n   Create a dataset where questions require combining an incomplete KG (missing edges/entities) with an accompanying text corpus, and where test-time introduces unseen entities/relations to stress inductive generalization. Provide standardized splits, evaluation for both answer correctness and evidence correctness, and baselines spanning GoG, GraphEdit, GraphSAGE retrieval, and classical RAG.\n\n10. **Safety and Robustness Study: Adversarial Edge Injection Attacks on Graph-Augmented RAG**  \n   Systematically study how malicious or noisy edges (in KGs or document graphs) can steer retrieval and degrade generation, and test defenses using GraphEdit denoising plus GFM-based outlier detection. The deliverable is an attack/defense protocol with measurable robustness gains (e.g., reduced targeted misinformation success rate under controlled edge poisoning).",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Inductive GraphRAG for Unseen Entities via GraphSAGE Neighborhood Synthesis\nExtend RAG to open-world settings by generating embeddings for newly observed entities/doc nodes using GraphSAGE-style neigh",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Uncertainty-Calibrated Retrieval from Incomplete KGs (GoG + Selective Generation)\nAugment GoG-style incomplete KG QA with an uncertainty model that decides when to (a) traverse available triples, (b) ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "GraphEdit-for-RAG: LLM-Driven Denoising of Document Graphs to Improve Evidence Quality\nApply GraphEdit to learn/clean edges in a document graph (doc\u2013doc, entity\u2013doc, claim\u2013source) built from a corpus ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Contrastive Pretraining Objective Aligning Text Chunks and KG Subgraphs (Cross-Modal GFM)\nPretrain a model to align embeddings of (question, supporting text passages) with embeddings of (supporting KG",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Graph-Prompted Generation: Structured \u201cReasoning Paths\u201d as a Control Interface for LLMs\nInstead of providing raw retrieved triples, convert retrieved subgraphs into constrained \u201cgraph prompts\u201d (e.g., ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Verification-as-Message-Passing: Post-Generation Fact Checking with GNN Over Cited Evidence\nAfter the LLM produces an answer with citations (passages/triples), build a small evidence graph linking cla",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Dynamic Retrieval Depth via Graph Topology (Adaptive k Using Structural Signals)\nDesign a retrieval controller that chooses how many items to retrieve (k) and how far to expand neighborhoods based on ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Benchmark: Open-World Incomplete-KG + Text Corpus QA for Inductive GraphRAG\nCreate a dataset where questions require combining an incomplete KG (missing edges/entities) with an accompanying text corpu",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Safety and Robustness Study: Adversarial Edge Injection Attacks on Graph-Augmented RAG\nSystematically study how malicious or noisy edges (in KGs or document graphs) can steer retrieval and degrade gen",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 4,
      "paper_title": "Agnostic Active Learning Is Always Better Than Passive Learning",
      "contribution": "A new agnostic active learning algorithm and analysis that give a sharp, instance-independent first-order query complexity for all concept classes whose leading term is always strictly smaller than passive sample complexity, eliminating disagreement-coefficient-type factors from the leading term.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8511,
      "output_tokens": 962,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Agnostic Active Learning ? - CMU School of Computer Science",
          "url": "http://www.cs.cmu.edu/~ninamf/papers/a2.pdf",
          "content": "Agnostic Active Learning ?\nMaria-Florina Balcan\nCarnegie Mellon University, Pittsburgh, PA 15213\nAlina Beygelzimer\nIBM T. J. Watson Research Center, Hawthorne, NY 10532\nJohn Langford\nYahoo! Research, New York, NY 10011\nAbstract\nWe state and analyze the first active learning algorithm that finds an \u000f-optimal\nhypothesis in any hypothesis class, when the underlying distribution has arbitrary\nforms of noise. The algorithm, A2(for Agnostic Active), relies only upon the as\u0002sumption that it has access to a stream of unlabeled examples drawn i.i.d. from\na fixed distribution. We show that A2 achieves an exponential improvement (i.e.,\nrequires only O\n\nln 1\n\u000f\n\u0001\nsamples to find an \u000f-optimal classifier) over the usual sample\ncomplexity of supervised learning, for several settings considered before in the real\u0002izable case. These include learning threshold classifiers and learning homogeneous\nlinear separators with respect to an input distribution which is uniform over the\nunit sphere.\nKey words: Active Learning, Agnostic Setting, Sample Complexity, Linear\nSeparators.\n1 Introduction\nTraditionally, machine learning has focused on the problem of learning a\ntask from labeled examples only. In many applications, however, labeling\n? A preliminary version of this paper appeared in the 23rd International Conference\non Machine Learning, ICML 2006.\nEmail addresses: ninamf@cs.cmu.edu (Maria-Florina Balcan),\nbeygel@us.ibm.com (Alina Beygelzimer), jl@yahoo-inc.com (John Langford).\nPreprint submitted to Elsevier Science 13 June 2008\nis expensive while unlabeled data is usually ample. This observation moti\u0002vated substantial work on properly using unlabeled data to benefit learning\n[4,10,11,30,34,28,33], and there are many examples showing that unlabeled\ndata can significantly help [9,32].\nThere are two main frameworks for incorporating unlabeled data into the\nlearning process. The first framework is semi-supervised learning [18], where\nin addition to a set of labeled examples, the learning algorithm can also use\na (usually larger) set of unlabeled examples drawn at random from the same\nunderlying data distribution. In this setting, unlabeled data becomes useful\nunder additional assumptions and beliefs about the learning problem. For\nexample, transductive SVM learning [28] assumes that the target function cuts\nthrough low density regions of the space, while co-training [11] assumes that\nthe target should be self-consistent in some way. Unlabeled data is potentially\nuseful in this setting because it allows one to reduce the search space to a set\nwhich is a-priori reasonable with respect to the underlying distribution.\nThe second setting, which is the main focus of this paper, is active learn\u0002ing [19,22]. Here the learning algorithm is allowed to draw random unlabeled\nexamples from the underlying distribution and ask for the labels of any of these\nexamples. The hope is that a good classifier can be learned with significantly\nfewer labels by actively directing the queries to informative examples.\nAs in passive supervised learning, but unlike in semi-supervised learning, the\nonly prior belief about the learning problem here is that the target function\n(or a good approximation of it) belongs to a given concept class. For some\nconcept classes such as thresholds on the line, one can achieve an exponential\nimprovement over the usual sample complexity of supervised learning, under\nno additional assumptions about the learning problem [19,22]. In general, the\nspeedups achievable in active learning depend on the match between the data\ndistribution and the hypothesis class, and therefore on the target hypothesis\nin the class. The most noteworthy non-trivial example of improvement is the\ncase of homogeneous (i.e., through the origin) linear separators, when the data\nis linearly separable and distributed uniformly over the unit sphere [25,24,22].\nThere are also simple examples where active learning does not help at all, even\nin the realizable case [22].\nMost of the previous work on active learning has focused on the realizable\ncase. In fact, many of the existing active learning strategies are noise seeking\non natural learning problems, because the process of actively finding an opti\u0002mal separation between one class and another often involves label queries for\nexamples close to the decision boundary, and such examples often used a large\nconditional noise rate (e.g., due to a mismatch between the hypothesis class\nand the data distribution). Thus the most informative examples are also the\nones that are typically the most noise-prone.\n2\nConsider an active learning algorithm which searches for the optimal threshold\non an interval using binary search. This example is often used to demonstrate\nthe potential of active learning in the noise-free case when there is a perfect\nthreshold separating the classes [19]. Binary search needs O(ln 1\n\u000f\n) labeled ex\u0002amples to learn a threshold with error less than \u000f, while learning passively\nrequires O\n\u0010\n1\n\u000f\n\u0011\nlabels. A fundamental drawback of this algorithm is that a\nsmall amount of adversarial noise can force the algorithm to behave badly. Is\nthis extreme brittleness to small amounts of noise essential? Can an exponen\u0002tial decrease in sample complexity be achieved? Can assumptions about the\nmechanism producing noise be avoided? These are the questions addressed\nhere.\nPrevious Work on Active Learning There has been substantial work\non active learning under additional assumptions. For example, the Query by\nCommittee analysis [25] assumes realizability (i.e., existence of a perfect clas\u0002sifier in a known set), and a correct Bayesian prior on the set of hypotheses.\nDasgupta [22] has identified sufficient conditions (which are also necessary\nagainst an adversarially chosen distribution) for active learning given only\nthe additional realizability assumption. There are several other papers that\nassume only realizability [21,24]. If there exists a perfect separator amongst\nhypotheses, any informative querying strategy can direct the learning process\nwithout the need to worry about the distribution it induces\u2014any inconsis\u0002tent hypothesis can be eliminated based on a single query, regardless of which\ndistribution this query comes from. In the agnostic case, however, a hypoth\u0002esis that performs badly on the query distribution may well be the optimal\nhypothesis with respect to the input distribution. This is the main challenge\nin agnostic active learning that is not present in the non-agnostic case. Bur\u0002nashev and Zigangirov [15] allow noise, but require a correct Bayesian prior\non threshold functions. Some papers require specific noise models such as a\nconstant noise rate everywhere [17] or Tsybakov noise conditions [5,16].\nThe membership-query setting [1,2,14,27] is similar to active learning con\u0002sidered here, except that no unlabeled data is given. Instead, the learning\nalgorithm is allowed to query examples of its own choice. This is problematic\nin several applications because natural oracles, such as hired humans, have dif\u0002ficulty labeling synthetic examples [8]. Ulam\u2019s Problem (quoted in [20]), where\nthe goal is find a distinguished element in a set by asking subset membership\nqueries, is also related. The quantity of interest is the smallest number of such\nqueries required to find the element, given a bound on the number of queries\nthat can be answered incorrectly. But both types of results do not apply here\nsince an active learning strategy can only buy labels of the examples it ob\u0002serves. For example, a membership query algorithm can be used to quickly\nfind a separating hyperplane in a high-dimensional space. An active learning\nalgorithm can not do so when the data distribution does not support queries\nclose to the decision boundary.\n3\nOur Contributions This paper presents the first agnostic active learning\nalgorithm, A2. The only necessary assumption is that the algorithm has ac\u0002cess to a stream of examples drawn i.i.d. from some fixed distribution. No\nadditional assumptions are made about the mechanism producing noise (e.g.,\nclass/target misfit, fundamental randomization, adversarial situations). The\nmain contribution of this paper is to prove the feasibility of agnostic active\nlearning.\nTwo comments are in order:\n(1) We define the noise rate of a hypothesis class H with respect to a fixed\ndistribution D as the minimum error rate of any hypothesis in H on D\n(see section 2 for a formal definition). Note that for the special case of\nso called label noise (where a coin of constant bias is used to determine\nwhether any particular example is mislabeled with respect to the best\nhypothesis) these definitions coincide.\n(2) We regard unlabeled data as being free so as to focus exclusively on\nthe question of whether or not agnostic active learning is possible at\nall. Substantial follow-up work to this paper has successfully optimized\nunlabeled data usage to be on the same order as passive learning [23].\nA2is provably correct (for any 0 < \u000f < 1/2 and 0 < \u03b4 < 1/2, it outputs an\n\u000f-optimal hypothesis with probability at least 1 \u2212 \u03b4) and it is never harmful\n(it never requires significantly more labeled examples than batch learning).\nA2 provides exponential sample complexity reductions in several settings pre\u0002viously analyzed without noise or with known noise conditions. This includes\nlearning threshold functions with small noise with respect to \u000f and hypothesis\nclasses consisting of homogeneous (through the origin) linear separators with\nthe data distributed uniformly over the unit sphere in R\nd\n. The last exam\u0002ple has been the most encouraging theoretical result so far in the realizable\ncase [24].\nThe A2 analysis achieves an almost contradictory property: for some sets of\nclassifiers, an \u000f-optimal classifier can be output with fewer labeled examples\nthan are needed to estimate the error rate of the chosen classifier with precision\n\u000f from random examples only.\nLower Bounds It is important to keep in mind that the speedups achiev\u0002able with",
          "original_query": "A2: Agnostic Active Learning (Balcan, Beygelzimer, and Langford; 2005/2006/2009)",
          "cleaned_query": "A2: Agnostic Active Learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "The Label Complexity of Active Learning from Observational Data",
          "url": "https://arxiv.org/abs/1905.12791",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: Counterfactual learning from observational data involves learning a\nclassifier on an entire population based on data that is observed conditioned\non a selection policy. This work considers this problem in an active setting,\nwhere the learner additionally has access to unlabeled examples and can choose\nto get a subset of these labeled by an oracle.\n Prior work on this problem uses disagreement-based active learning, along\nwith an importance weighted loss estimator to account for counterfactuals,\nwhich leads to a high label complexity. We show how to instead incorporate a\nmore efficient counterfactual risk minimizer into the active learning\nalgorithm. This requires us to modify both the counterfactual risk to make it\namenable to active learning, as well as the active learning process to make it\namenable to the risk. We provably demonstrate that the result of this is an\nalgorithm which is statistically consistent as well as more label-efficient\nthan prior work.\n \n \n \n \n Submission history From: Songbai Yan [ view email]\n \n [v1] \n Wed, 29 May 2019 23:48:16 UTC (30 KB) [v2] \nMon, 28 Oct 2019 03:03:17 UTC (97 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > stat > arXiv:1905.12791\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Statistics > Machine Learning\n\n arXiv:1905.12791 (stat)\n [Submitted on 29 May 2019 (v1), last revised 28 Oct 2019 (this version, v2)]\n\n Title: The Label Complexity of Active Learning from Observational Data\n\n Authors: Songbai Yan, Kamalika Chaudhuri, Tara Javidi\n Download PDF\n Abstract: Counterfactual learning from observational data involves learning a classifier on an entire population based on data that is observed conditioned on a selection policy. This work considers this problem in an active setting, where the learner additionally has access to unlabeled examples and can choose to get a subset of these labeled by an oracle.\n Prior work on this problem uses disagreement-based active learning, along with an importance weighted loss estimator to account for counterfactuals, which leads to a high label complexity. We show how to instead incorporate a more efficient counterfactual risk minimizer into the active learning algorithm. This requires us to modify both the counterfactual risk to make it amenable to active learning, as well as the active learning process to make it amenable to the risk. We provably demonstrate that the result of this is an algorithm which is statistically consistent as well as more label-efficient than prior work.\n Comments: NeurIPS 2019 \n Subjects: Machine Learning (stat.ML) ; Machine Learning (cs.LG)\n Cite as: arXiv:1905.12791 [stat.ML] \n (or arXiv:1905.12791v2 [stat.ML] for this version) \n https://doi.org/10.48550/arXiv.1905.12791 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Songbai Yan [view email]\n [v1] Wed, 29 May 2019 23:48:16 UTC (30 KB)\n [v2] Mon, 28 Oct 2019 03:03:17 UTC (97 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n (license)\n Current browse context:\n stat.ML\n < prev | next >\n new | recent | 1905\n Change to browse by:\n cs\n cs.LG\n stat\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Label Complexity of Active Learning (Hanneke; 2007b)",
          "cleaned_query": "Label Complexity of Active Learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Active Learning in the Non-realizable Case - Springer Link",
          "url": "https://link.springer.com/chapter/10.1007/11894841_9",
          "content": "\n \n \n Abstract Most of the existing active learning algorithms are based on the realizability assumption: The learner\u2019s hypothesis class is assumed to contain a target function that perfectly classifies all training and test examples. This assumption can hardly ever be justified in practice. In this paper, we study how relaxing the realizability assumption affects the sample complexity of active learning. First, we extend existing results on query learning to show that any active learning algorithm for the realizable case can be transformed to tolerate random bounded rate class noise. Thus, bounded rate class noise adds little extra complications to active learning, and in particular exponential label complexity savings over passive learning are still possible. However, it is questionable whether this noise model is any more realistic in practice than assuming no noise at all. Our second result shows that if we move to the truly non-realizable model of statistical learning theory, then the label complexity of active learning has the same dependence \u03a9(1/ \u03b5 \n 2) on the accuracy parameter \u03b5 as the passive learning label complexity. More specifically, we show that under the assumption that the best classifier in the learner\u2019s hypothesis class has generalization error at most \u03b2 &gt;0, the label complexity of active learning is \u03a9( \u03b2 \n 2 / \u03b5 \n 2 log(1/ \u03b4)), where the accuracy parameter \u03b5 measures how close to optimal within the hypothesis class the active learner has to get and \u03b4 is the confidence parameter. The implication of this lower bound is that exponential savings should not be expected in realistic models of active learning, and thus the label complexity goals in active learning should be refined. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Preview \n \n Unable to display preview.\u00a0 Download preview PDF. \n \n \n \n \n \n \n \n References Freund, Y., Seung, H.S., Shamir, E., Tishby, N.: Selective sampling using the query by committee algorithm. Machine Learning\u00a028(2-3), 133\u2013168 (1997) CrossRef \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Dasgupta, S., Kalai, A.T., Monteleoni, C.: Analysis of perceptron-based active learning. In: Auer, P., Meir, R. (eds.) COLT 2005. LNCS, vol.\u00a03559, pp. 249\u2013263. Springer, Heidelberg (2005) CrossRef \u00a0\n \n Google Scholar \u00a0\n Dasgupta, S.: Coarse sample complexity bounds for active learning. In: NIPS 2005 (2005) \n Google Scholar \u00a0\n Balcan, N., Beygelzimer, A., Langford, J.: Agnostic active learning. In: ICML (accepted, 2006) \n Google Scholar \u00a0\n Tong, S., Koller, D.: Support vector machine active learning with applications to text classification. Journal of Machine Learning Research\u00a02, 45\u201366 (2002) CrossRef \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Angluin, D., Laird, P.: Learning from noisy examples. Machine Learning\u00a02(4), 343\u2013370 (1987) \n Google Scholar \u00a0\n Sakakibara, Y.: On learning from queries and counterexamples in the presence of noise. Information Processing Letters\u00a037(5), 279\u2013284 (1991) CrossRef \u00a0\n MATH \u00a0\n MathSciNet \u00a0\n \n Google Scholar \u00a0\n Vapnik, V.N.: Estimation of Dependencies Based on Empirical Data. Springer, Heidelberg (1982) \n Google Scholar \u00a0\n Gentile, C., Helmbold, D.P.: Improved lower bounds for learning from noisy examples: an information-theoretic approach. In: COLT 1998, pp. 104\u2013115. ACM Press, New York (1998) CrossRef \u00a0\n \n Google Scholar \u00a0\n Domingo, C., Gavald\u00e1, R., Watanabe, O.: Adaptive sampling methods for scaling up knowledge discovery algorithms. In: Arikawa, S., Furukawa, K. (eds.) DS 1999. LNCS, vol.\u00a01721, pp. 172\u2013183. Springer, Heidelberg (1999) CrossRef \u00a0\n \n Google Scholar \u00a0\n Castro, R.: Personal communication (March 2006) \n Google Scholar \u00a0\n Silvey, S.D.: Optimal Design. Chapman and Hall, London (1980) MATH \u00a0\n \n Google Scholar \u00a0\n Elfving, G.: Selection of nonrepeatable observations for estimation. In: Proceedings of the 3rd Berkeley Symposium on Mathematical Statistics and Probability, vol.\u00a01, pp. 69\u201375 (1956) \n Google Scholar \u00a0\n Canetti, R., Even, G., Goldreich, O.: Lower bounds for sampling algorithms for estimating the average. Information Processing Letters\u00a053(1), 17\u201325 (1995) CrossRef \u00a0\n MATH \u00a0\n MathSciNet \u00a0\n \n Google Scholar \u00a0\n Download references Author information Authors and Affiliations Department of Computer Science, University of Helsinki, \u00a0 Matti K\u00e4\u00e4ri\u00e4inen Authors Matti K\u00e4\u00e4ri\u00e4inen You can also search for this author in\n PubMed \u00a0 Google Scholar Editor information Editors and Affiliations Departament de Llenguatges i Sistemes Inform\u00e0tics Laboratori d\u2019Algor\u00edsmica Relacional, Complexitat i Aprenentatge, Universitat Polit\u00e8cnica de Catalunya, Barcelona, \u00a0 Jos\u00e9 L. Balc\u00e1zar Google, 1600 Amphitheatre Parkway, 94043, Mountain View, CA, USA Philip M. Long Department of Computer Science and Department of Mathematics, National University of Singapore, 117543, Singapore, Republic of Singapore Frank Stephan Rights and permissions Copyright information \u00a9 2006 Springer-Verlag Berlin Heidelberg About this paper Cite this paper K\u00e4\u00e4ri\u00e4inen, M. (2006). Active Learning in the Non-realizable Case.\n In: Balc\u00e1zar, J.L., Long, P.M., Stephan, F. (eds) Algorithmic Learning Theory. ALT 2006. Lecture Notes in Computer Science(), vol 4264. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11894841_9 Download citation.RIS.ENW.BIB DOI: https://doi.org/10.1007/11894841_9 \n Publisher Name: Springer, Berlin, Heidelberg \n Print ISBN: 978-3-540-46649-9 \n Online ISBN: 978-3-540-46650-5 eBook Packages: Computer Science Computer Science (R0) \n \n \n \n ||||I|||| Skip to main content\n\n Advertisement\n\n Search\n Go to cart\n * Log in\n Search SpringerLink\n Search\n\n International Conference on Algorithmic Learning Theory\n\n ALT 2006: Algorithmic Learning Theory pp 63\u201377Cite as\n\n Active Learning in the Non-realizable Case\n\n * Matti K\u00e4\u00e4ri\u00e4inen21\n * Conference paper\n\n * 826 Accesses\n\n * 26 Citations\n\n Part of the Lecture Notes in Computer Science book series (LNAI,volume 4264)\n\n Abstract\n\n Most of the existing active learning algorithms are based on the realizability assumption: The learner\u2019s hypothesis class is assumed to contain a target function that perfectly classifies all training and test examples. This assumption can hardly ever be justified in practice. In this paper, we study how relaxing the realizability assumption affects the sample complexity of active learning. First, we extend existing results on query learning to show that any active learning algorithm for the realizable case can be transformed to tolerate random bounded rate class noise. Thus, bounded rate class noise adds little extra complications to active learning, and in particular exponential label complexity savings over passive learning are still possible. However, it is questionable whether this noise model is any more realistic in practice than assuming no noise at all.\n\n Our second result shows that if we move to the truly non-realizable model of statistical learning theory, then the label complexity of active learning has the same dependence \u03a9(1/\u03b5 2) on the accuracy parameter \u03b5 as the passive learning label complexity. More specifically, we show that under the assumption that the best classifier in the learner\u2019s hypothesis class has generalization error at most \u03b2>0, the label complexity of active learning is \u03a9(\u03b2 2/\u03b5 2log(1/\u03b4)), where the accuracy parameter \u03b5 measures how close to optimal within the hypothesis class the active learner has to get and \u03b4 is the confidence parameter. The implication of this lower bound is that exponential savings should not be expected in realistic models of active learning, and thus the label complexity goals in active learning should be refined.\n\n This is a preview of subscription content, access via your institution.\n\n Buying options\n\n Chapter\n USD 29.95\n Price excludes VAT (USA)\n * DOI: 10.1007/11894841_9\n * Chapter length: 15 pages\n * Instant PDF download\n * Readable on all devices\n * Own it forever\n * Exclusive offer for individuals only\n * Tax calculation will be finalised during checkout\n Buy Chapter\n eBook USD 84.99 Price excludes VAT (USA)\n * ISBN: 978-3-540-46650-5\n * Instant PDF download\n * Readable on all devices\n * Own it forever\n * Exclusive offer for individuals only\n * Tax calculation will be finalised during checkout\n Buy eBook\n Softcover Book USD 109.99 Price excludes VAT (USA)\n * ISBN: 978-3-540-46649-9\n * Dispatched in 3 to 5 business days\n * Exclusive offer for individuals only\n * Free shipping worldwide\n Shipping restrictions may apply, check to see if you are impacted.\n * Tax calculation will be finalised during checkout\n Buy Softcover Book\n Learn about institutional subscriptions\n\n Preview\n\n Unable to display preview. Download preview PDF.\n\n Unable to display preview. Download preview PDF.\n\n References\n\n 1. Freund, Y., Seung, H.S., Shamir, E., Tishby, N.: Selective sampling using the query by committee algorithm. Machine Learning 28(2-3), 133\u2013168 (1997)\n\n CrossRef MATH Google Scholar\n\n 2. Dasgupta, S., Kalai, A.T., Monteleoni, C.: Analysis of perceptron-based active learning. In: Auer, P., Meir, R. (eds.) COLT 2005. LNCS, vol. 3559, pp. 249\u2013263. Springer, Heidelberg (2005)\n\n CrossRef Google Scholar\n\n 3. Dasgupta, S.: Coarse sample complexity bounds for active learning. In: NIPS 2005 (2005)\n\n Google Scholar\n\n 4. Balcan, N., Beygelzimer, A., Langford, J.: Agnostic active learning. In: ICML (accepted, 2006)\n\n Google Scholar\n\n 5. Tong, S., Koller, D.: Support vector machine active learning with applications to text classification. Journal of Machine Learning Research 2, 45\u201366 (2002)\n\n CrossRef MATH Google Scholar\n\n 6. Angluin, D., Laird, P.: Learning from noisy examples. Machine Learning 2(4), 343\u2013370 (1987)\n\n Google Scholar\n\n 7. Sakakibara, Y.: On learning from queries and counterexamples in the presence of noise. Information Processing Letters 37(5), 279\u2013284 (1991)\n\n CrossRef MATH MathSciNet Google Scholar\n\n 8. Vapnik, V.N.: Estimation of Dependencies Based on Empirical Data. Springer, Heidelberg (1982)\n\n Google Scholar\n\n 9. Gentile, C., Helmbold, D.P.: Improved lower bounds for learning from noisy",
          "original_query": "Active Learning in the Non-Realizable Case / Lower Bounds (K\u00e4\u00e4ri\u00e4inen; 2005/2006)",
          "cleaned_query": "Active Learning in the Non-Realizable Case",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Importance Weighted Active Learning",
          "url": "https://icml.cc/Conferences/2009/papers/392.pdf",
          "content": "Importance Weighted Active Learning\nAlina Beygelzimer beygel@us.ibm.com\nIBM Research, 19 Skyline Drive, Hawthorne, NY 10532\nSanjoy Dasgupta dasgupta@cs.ucsd.edu\nDepartment of Computer Science and Engineering, University of California, San Diego\n9500 Gilman Drive, La Jolla, CA 92093\nJohn Langford jl@yahoo-inc.com\nYahoo! Research, 111 West 40th Street, New York, NY 10018\nAbstract\nWe present a practical and statistically con\u0002sistent scheme for actively learning binary\nclassifiers under general loss functions. Our\nalgorithm uses importance weighting to cor\u0002rect sampling bias, and by controlling the\nvariance, we are able to give rigorous label\ncomplexity bounds for the learning process.\n1. Introduction\nAn active learner interactively chooses which data\npoints to label, while a passive learner obtains all the\nlabels at once. The great hope of active learning is that\ninteraction can substantially reduce the number of la\u0002bels required, making learning more practical. This\nhope is known to be valid in certain special cases,\nwhere the number of label queries has been shown to\nbe logarithmic in the usual sample complexity of pas\u0002sive learning; such cases include thresholds on a line,\nand linear separators with a spherically uniform unla\u0002beled data distribution (Dasgupta et al., 2005).\nMany earlier active learning algorithms, such as (Cohn\net al., 1994; Dasgupta et al., 2005), are not consistent\nwhen data is not perfectly separable under the given\nhypothesis class: even with an infinite labeling budget,\nthey might not converge to an optimal predictor (see\nDasgupta and Hsu (2008) for a discussion).\nThis problem has recently been addressed in two\nthreads of research. One approach (Balcan et al.,\n2006; Dasgupta et al., 2008; Hanneke, 2007) constructs\nAppearing in Proceedings of the 26 th International Confer\u0002ence on Machine Learning, Montreal, Canada, 2009. Copy\u0002right 2009 by the author(s)/owner(s).\nlearning algorithms that explicitly use sample com\u0002plexity bounds to assess which hypotheses are still \u201cin\nthe running,\u201d and thereby assess the relative value of\ndifferent unlabeled points (in terms of whether they\nhelp distinguish between the remaining hypotheses).\nThese algorithms have the usual PAC-style conver\u0002gence guarantees, but also have rigorous label com\u0002plexity bounds that are in many cases significantly bet\u0002ter than the bounds for passive learning. The second\napproach to active learning uses importance weights to\ncorrect sampling bias (Bach, 2007; Sugiyama, 2006).\nThe PAC-guarantee active learning algorithms have\nyet to see practical use for several reasons. First,\nthey are built explicitly for 0\u20131 loss and are not easily\nadapted to most other loss functions. This is problem\u0002atic because in many applications, other loss functions\nare more appropriate for describing the problem, or\nmake learning more tractable (as with convex proxy\nlosses on linear representations). Second, these algo\u0002rithms make internal use of generalization bounds that\nare often loose in practice, and they can thus end up\nrequiring far more labels than are really necessary. Fi\u0002nally, they typically require an explicit enumeration\nover the hypothesis class (or an \u000f-cover thereof), which\nis generally computationally intractable.\nImportance weighted approaches have only been ana\u0002lyzed in limited settings. For example, Bach (2007)\nconsiders linear models and provides an analysis of\nconsistency in cases where either (i) the model class\nfits the data perfectly, or (ii) the sampling strategy is\nnon-adaptive (that is, a query doesn\u2019t depend on the\nsequence of previous queries). Also, the analysis in\nthese works is asymptotic rather than yielding finite\nlabel bounds. Label complexity is of paramount im\u0002portance in active learning, because otherwise simpler\npassive learning approaches can be used. Furthermore,\nImportance Weighted Active Learning\na poor choice of importance weights can result in high\nlabel complexity.\nWe address the problems above with a new active\nlearning scheme that provably yields PAC-style label\ncomplexity guarantees. When presented with an un\u0002labeled point xt, this scheme queries its label with a\ncarefully chosen probability pt, taking into account the\nidentity of the point and the history of labels seen so\nfar. The points that end up getting labeled are then\nweighted according to the reciprocals of these prob\u0002abilities (that is, 1/pt), in order to remove sampling\nbias. We show (theorem 1) that this simple method\nguarantees statistical consistency: for any distribution\nand any hypothesis class, active learning eventually\nconverges to the optimal hypothesis in the class.\nAs in any importance sampling scenario, the biggest\nchallenge is controlling the variance of the process.\nThis depends crucially on how the sampling proba\u0002bility pt is chosen. Roughly, our strategy is to make it\nproportional to the spread of values h(xt), as h ranges\nover the remaining candidate hypotheses. For this set\u0002ting of pt, which we call loss-weighting, we have two\nresults: a fallback guarantee that the label complexity\nis never much worse than that of supervised learning\n(theorem 2), and a rigorous label complexity bound\n(theorem 11). Previously, label complexity bounds for\nactive learning were only known for 0\u20131 loss, and were\nbased on the disagreement coefficient of the learning\nproblem (Hanneke, 2007). We generalize this notion\nto general loss functions, and analyze label complex\u0002ity in terms of it. We consider settings in which these\nbounds turn out to be roughly the square root of the\nsample complexity of supervised learning.\nIn addition to these upper bounds, we show a general\nlower bound on the label complexity of active learn\u0002ing (theorem 12) that significantly improves the best\nprevious such result (K\u00a8a\u00a8ari\u00a8ainen, 2006).\nWe conduct practical experiments with two IWAL al\u0002gorithms. The first is a specialization of IWAL with\nloss-weighting to the case of linear classifiers with\nconvex loss functions; here, the algorithm becomes\ntractable via convex programming (section 7). The\nsecond uses a simple bootstrapping scheme that re\u0002duces active learning to passive learning without re\u0002quiring much additional computation (section 7.2). In\nevery case, these experiments yield substantial reduc\u0002tions in label complexity compared to passive learning,\nwithout compromising predictive performance. They\nsuggest that IWAL is a practical scheme that can re\u0002duce the label complexity of active learning without\nsacrificing the statistical guarantees (like consistency)\nwe take for granted in passive learning.\nBoosting and bagging-based algorithms of Abe and\nMamitsuka (1998) are similar in spirit to our boot\u0002strapping IWAL scheme in section 7.2, but they are\nnot consistent in the presence of noise.\n2. Preliminaries\nLet X be the input space and Y the output space. We\nconsider active learning in the streaming setting where\nat each step t, a learner observes an unlabeled point\nxt \u2208 X and has to decide whether to ask for the label\nyt \u2208 Y . The learner works with a hypothesis space\nH = {h : X \u2192 Z}, where Z is a prediction space.\nThe algorithm is evaluated with respect to a given loss\nfunction l : Z \u00d7 Y \u2192 [0, \u221e). The most common loss\nfunction is 0\u20131 loss, in which Y = Z = {\u22121, 1} and\nl(z, y) = 1(y 6= z) = 1(yz < 0). The following exam\u0002ples address the binary case Y = {\u22121, 1} with Z \u2282 R:\nl(z, y) = (1 \u2212 yz)+ (hinge loss), l(z, y) = ln(1 + e\n\u2212yz)\n(logistic loss), l(z, y) = (y \u2212 z)\n2 = (1 \u2212 yz)2\n(squared\nloss), and l(z, y) = |y \u2212 z| = |1 \u2212 yz| (absolute loss).\nNotice that all the loss functions mentioned here are\nof the form l(z, y) = \u03c6(yz) for some function \u03c6 on\nthe reals. We specifically highlight this subclass of\nloss functions when proving label complexity bounds.\nSince these functions are bounded (if Z is), we further\nassume they are normalized to output a value in [0, 1].\n3. The Importance Weighting Skeleton\nAlgorithm 1 describes the basic outline of importance\u0002weighted active learning (IWAL). Upon seeing xt, the\nlearner calls a subroutine rejection-threshold (instanti\u0002ated in later sections), which looks at xt and past his\u0002tory to return the probability pt of requesting yt. The\nalgorithm maintains a set of labeled examples seen so\nfar, each with an importance weight: if yt ends up\nbeing queried, its weight is set to 1/pt.\nAlgorithm 1 IWAL (subroutine rejection-threshold)\nSet S0 = \u2205.\nFor t from 1, 2, ... until the data stream runs out:\n1. Receive xt .\n2. Set pt = rejection-threshold(xt, {xi, yi, pi, Qi:\n1 \u2264 i < t}).\n3. Flip a coin Qt \u2208 {0, 1} with E[Qt] = pt. If Qt = 1,\nrequest yt and set St = St\u22121 \u222a {(xt, yt, 1/pt)}, else\nSt = St\u22121.\n4. Let ht = arg minh\u2208H\nP\n(x,y,c)\u2208St\nc \u00b7 l(h(x), y).\nExamples are assumed to be drawn i.i.d. from the un-\nImportance Weighted Active Learning\nderlying probability distribution D. The expected loss\nof h \u2208 H on D is given by L(h) = E(x,y)\u223cD l(h(x), y).\nThe importance weighted estimate of the loss at time\nT is\nLT (h) = 1\nT\nX\nT\nt=1\nQt\npt\nl(h(xt), yt),\nwhere Qt is as defined in the algorithm. It is not hard\nto see that E[LT (h)] = L(h), with the expectation\ntaken over all the random variables involved. The\u0002orem 2 gives large deviation bounds for LT (h), pro\u0002vided that the probabilities pt are chosen carefully.\nEven though step 4 is stated as empirical risk min\u0002imization, any passive importance-weighted learning\nalgorithm can be used to learn ht based on St.\n3.1. A safety guarantee for IWAL\nA desirable property for a learning algorithm is con\u0002sistency: Given an infinite budget of unlabeled and\nlabeled examples, does the algorithm converge to the\nbest predictor? Some early active learning algo\u0002rithms (Cohn et al., 1994; Dasgupta et al., 2005) do\nnot satisfy this baseline guarantee if the points cannot\nbe classified perfectly by the given hypothesis class.\nWe prove that IWAL algorithms are consistent, as long\nas pt is bounded away from 0.\nComparing this result to the usual sample complexity\nbounds in supervised learning (for example, corollary\n4.2 of (Langford, ",
          "original_query": "Importance-Weighted Active Learning and Strengthened Lower Bounds (Beygelzimer, Dasgupta, and Langford; 2009)",
          "cleaned_query": "Importance-Weighted Active Learning and Strengthened Lower Bounds",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Coarse sample complexity bounds for active learning",
          "url": "https://cseweb.ucsd.edu/~dasgupta/papers/sample.pdf",
          "content": "Coarse sample complexity bounds for active learning\nSanjoy Dasgupta\nUC San Diego\ndasgupta@cs.ucsd.edu\nAbstract\nWe characterize the sample complexity of active learning problems in terms of a parameter which takes\ninto account the specific target hypothesis and the distribution over the input space. For homogeneous\n(through the origin) linear separators in R\nd\n, we show that under a fairly wide range of data distributions,\nthe number of labels needed by an active learner to achieve an error rate \u2264 \u01eb is just O\u02dc(d log21/\u01eb),\nexponentially smaller than the usual \u2126(d/\u01eb) sample complexity of supervised learning.\n1 Introduction\nThe goal of active learning is to learn a classifier in a setting where data comes unlabeled, and any labels\nmust be explicitly requested and paid for. The hope is that an accurate classifier can be found by buying\njust a few labels.\nSo far the most encouraging theoretical results in this field are [6, 5], both of which apply to the hypoth\u0002esis class of homogeneous (i.e. through the origin) linear separators, in the specific case where the data is\ndistributed uniformly over the unit sphere in R\nd\n. They show that if the labels correspond perfectly to one\nof the hypotheses (i.e. the separable case), then just O(d log d/\u01eb) labels are needed to learn a classifier with\nerror less than \u01eb. This is exponentially smaller than the usual \u2126(d/\u01eb) sample complexity of learning linear\nclassifiers in a supervised setting.\nHowever, generalizing this result is non-trivial. Suppose, for instance, that the hypothesis class is ex\u0002panded to include non-homogeneous linear separators. Then even in just two dimensions, and under the\nsame benign input distribution, we will see that there are some target hypotheses for which active learning\ndoes not help much, for which \u2126(1/\u01eb) labels are needed. In fact, in this example the label complexity of\nactive learning depends heavily on the specific target hypothesis, and ranges all the way from O(log 1/\u01eb) to\n\u2126(1/\u01eb).\nIn this paper, we consider arbitrary hypothesis classes H of VC dimension d < \u221e, and learning problems\nwhich are separable. We characterize the sample complexity of active learning in terms of a parameter which\ntakes into account: (1) the distribution P over the input space X ; (2) the specific target hypothesis h\n\u2217 \u2208 H;\nand (3) the desired accuracy \u01eb.\nSpecifically, we observe that distribution P induces a natural topology on H, and we define a splitting\nindex \u03c1 which captures the relevant local geometry of H in the vicinity of h\n\u2217\n, at scale \u01eb. We show that this\nquantity coarsely describes the sample complexity of active learning: any active learning scheme requires\n\u2126(1/\u03c1) labels and there is a generic active learner which always uses at most O\u02dc((d/\u03c1) log2(1/\u01eb)) labels.1\nThis \u03c1 is always at least \u01eb; if it is \u01eb we get approximately the usual sample complexity of supervised learn\u0002ing. But sometimes \u03c1 is a constant, and in such instances active learning gives an exponential improvement\nin the number of labels needed.\nWe derive splitting indices for various hypothesis classes. For homogeneous linear separators under an\ninput distribution which is uniform over the unit sphere in R\nd\n, we easily find \u03c1 to be a constant \u2013 perhaps the\n1The O\u02dc(\u00b7) notation is used here to hide factors of the form polylog(d, 1/\u03b4, 1/\u03c1, log 1/\u01eb, log 1/\u03c4), where \u03b4 and \u03c4 will be\nintroduced later.\n1\nmost direct proof yet of the efficacy of active learning in this case. But we also show, for the first time, that\nthis O\u02dc(d log21/\u01eb) label complexity holds in substantially more general situations. Specifically, we consider\ninput distributions which are a multiplicative factor \u03bb away from uniform, and we find that \u03c1 remains a\nconstant (independent of \u03bb), provided the amount of unlabeled data is increased by a factor of O\u02dc(\u03bb\n2\n).\n2 Sample complexity bounds\n2.1 Motivating examples\nLinear separators in R\n1\nOur first example is taken from [3, 4]. Suppose the data lie on the real line, and the classifiers are simple\nthresholding functions, H = {hw : w \u2208 R}:\nhw(x) = \u001a\n1 if x \u2265 w\n0 if x < w\n\u2212 \u2212 \u2212 \u2212 \u2212 \u2212\u2212 + + + +\nw\nVC theory tells us that if the underlying distribution P is separable (can be classified perfectly by some\nhypothesis in H), then in order to achieve an error rate less than \u01eb, it is enough to draw m = O(1/\u01eb) random\nlabeled examples from P, and to return any classifier consistent with them. But suppose we instead draw m\nunlabeled samples from P. If we lay these points down on the line, their hidden labels are a sequence of 0\u2019s\nfollowed by a sequence of 1\u2019s, and the goal is to discover the point w at which the transition occurs. This\ncan be accomplished with a simple binary search which asks for just log m = O(log 1/\u01eb) labels. Thus, in this\ncase active learning gives an exponential improvement in the number of labels needed.\nCan we always achieve a label complexity proportional to log 1/\u01eb rather than 1/\u01eb? A natural next step\nis to consider linear separators in two dimensions.\nLinear separators in R\n2\nLet H be the hypothesis class of linear separators in R\n2\n, and suppose the input distribution P is some density\nsupported on the perimeter of the unit circle. It turns out that the positive results of the one-dimensional\ncase do not generalize: there are some target hypotheses in H for which \u2126(1/\u01eb) labels are needed to find a\nclassifier with error rate less than \u01eb, no matter what active learning scheme is used.\nTo see this, consider the following possible target hypotheses (Figure 1):\n\u2022 h0: all points are positive.\n\u2022 hi (1 \u2264 i \u2264 1/\u01eb): all points are positive except for a small slice Bi of probability mass \u01eb.\nThe slices Bi are explicitly chosen to be disjoint, with the result that \u2126(1/\u01eb) labels are needed to distinguish\nbetween these hypotheses. For instance, suppose nature chooses a target hypothesis at random from among\nthe hi, 1 \u2264 i \u2264 1/\u01eb. Then, to identify this target with probability at least 1/2, it is necessary to query points\nin at least (about) half the Bi\u2019s.\nThus for these particular target hypotheses, active learning offers little improvement in sample complexity\nover regular supervised learning. What about other target hypotheses in H, for instance those in which the\npositive and negative regions are more evenly balanced? Consider the following active learning scheme:\n1. Draw a pool of O(1/\u01eb) unlabeled points.\n2. From this pool, choose query points at random until at least one positive and one negative point have\nbeen found. (If all points have been queried, then halt.)\n3. Apply binary search to find the two boundaries between positive and negative on the perimeter of the\ncircle.\n2\nB2\nB1\nh0\nh3\nh2\nh1\nFigure 1: P is supported on the circumference of a circle. Each Biis an arc of probability mass \u01eb.\nFor any h \u2208 H, define\ni(h) = min{positive mass of h, negative mass of h}.\nIt is not hard to see that when the target hypothesis is h, step (2) asks for O(1/i(h)) labels (with probability\nat least 9/10, say) and step (3) asks for O(log 1/\u01eb) labels.\nThus even within this simple hypothesis class, the label complexity of active learning can run anywhere\nfrom O(log 1/\u01eb) to \u2126(1/\u01eb), depending on the specific target hypothesis.\nLinear separators in R\n3\nIn our two previous examples, the amount of unlabeled data needed was O(1/\u01eb), exactly the usual sample\ncomplexity of supervised learning. We next turn to a case where it helps to have significantly more unlabeled\ndata than this.\nConsider the distribution of the previous, two-dimensional example: for concreteness, fix P to be the\nuniform distribution on the unit circle in R\n2\n. Now lift it into three dimensions by adding to each point\nx = (x1, x2) a third coordinate x3 = 1. Let H consist of homogeneous (i.e. through the origin) linear\nseparators in R\n3\n. Clearly the bad cases of the previous example persist. And as before, the hard part is\nfinding an initial positive point and negative point; thereafter binary search is possible.\nSuppose, now, that a trace amount \u03c4 \u226a \u01eb of a second distribution P\n\u2032\nis mixed in with P (Figure 2, left),\ngiving an overall combined distribution of (1 \u2212 \u03c4 )P + \u03c4P\n\u2032\n. This P\n\u2032\nis uniform on the circle\nx\n2\n1 + x\n2\n2 = 1, x3 = 0.\nIt represents such a tiny fraction of the data that it has almost no influence on the error rate of a classifier.\nHowever, it is a source of good query points, for the following reason: the \u201cbad\u201d linear separators in H cut\noff just a small portion of P but nonetheless divide P\n\u2032 perfectly in half (Figure 2, right). This permits a\nthree-stage algorithm which uses P\n\u2032\nto help locate positive and negative points of P:\n1. By running the two-dimensional algorithm (given above) on points from P\n\u2032\n, approximately identify\n(within arc-length \u01eb\u03c0/2, say) the two places at which the target hypothesis h\n\u2217\ncuts P\n\u2032\n.\n2. Let the midpoints of these (approximate) positive and negative P\n\u2032\n-intervals be p\n\u2032 = (p1, p2, 0) and\nn\n\u2032 = (n1, n2, 0). Look at the points directly above them, p = (p1, p2, 1) and n = (n1, n2, 1). If the\npositive region of P has P-mass at least \u01eb, then it must contain p and all points within arc-length \u01eb\u03c0/2\nof it; likewise with n and the negative region. So query a point within arc-length \u01eb\u03c0/2 of each of p, n; if\nthey are both positive or both negative, stop and announce the all-positive or all-negative hypothesis.\nOtherwise continue.\n3\nx3\nP\nP\n\u2032\norigin\n\u2212\np\nn\n+\n\u2212\nn\n\u2032\np\n\u2032\n+\nFigure 2: Left: The same distribution P as in the 2-d case, now lifted to 3-d, and with trace amounts of\nanother distribution P\n\u2032 mixed in. Right: A bad target hypothesis cuts off a small portion of P but exactly\nhalf of P\n\u2032\n.\n3. Armed with this pair of positive and negative points from P, do binary search on P, as in step (3) of\nthe 2-d algorithm.\nSteps (1) and (3) each use O(log 1/\u01eb) labels.\nThis O(log 1/\u01eb) label complexity is made possible by the presence of P\n\u2032 and is therefore only achievable\nif the amount of unlabeled data is \u2126(1/\u03c4 ), which could potentially be enormous. With less unlabeled data,\nthe usual \u2126(1/\u01eb) label complexity ap",
          "original_query": "Coarse Sample Complexity Bounds for Active Learning (Dasgupta; 2005)",
          "cleaned_query": "Coarse Sample Complexity Bounds for Active Learning",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Variance-Optimal Query Policies for Importance-Weighted Active Learning (IWAL)\nDesign an IWAL-style active learner that explicitly optimizes the query probability \\(p_t(x)\\) to minimize an empirical Bernstein-style upper bound on the variance of the importance-weighted risk estimator, subject to label budget constraints. Provide finite-sample label complexity bounds that improve over loss-weighting when the loss is heavy-tailed or gradients are large (e.g., logistic loss with rare classes).",
        "Disagreement- and Splitting-Index-Guided Counterfactual Active Learning\nBuild an active learner for observational/counterfactual classification that replaces pure disagreement querying with a \u201ccounterfactual splitting index\u201d computed using a doubly robust or stabilized counterfactual risk objective. Show that, analogous to Dasgupta\u2019s splitting index, this quantity predicts when observational active learning can achieve sub-passive label complexity despite selection bias.",
        "Adaptive Noise-Tolerant Active Learning Beyond Bounded-Rate Class Noise\nDevelop an active algorithm that estimates local noise conditions (near the boundary) online and adapts its querying strategy to avoid \u201cnoise-seeking\u201d behavior highlighted in agnostic settings. Provide guarantees in Tsybakov/Massart-type noise (intermediate between realizable and fully agnostic), quantifying when exponential-to-polynomial transitions occur in label complexity.",
        "Unlabeled Sample Complexity vs. Label Complexity Trade-offs for Generalized Splitting Index\nExtend Dasgupta\u2019s analysis (which already hints unlabeled data may need inflation under non-uniform distributions) to derive explicit optimal trade-offs between unlabeled pool size and label queries for general loss functions (IWAL) and splitting-index-like parameters. Provide an algorithm that adaptively requests more unlabeled data only when needed to keep the effective splitting index stable.",
        "Fairness-Constrained Importance-Weighted Active Learning Under Selection Bias\nIncorporate group fairness constraints (e.g., equalized odds) into IWAL/counterfactual active learning by using importance-weighted, group-conditional risk estimators and variance controls. The key contribution is a label-efficient active learner that provably satisfies fairness constraints on the target population even when labels are acquired under biased observational policies.",
        "Active Learning for Non-Homogeneous Separators via Local Geometry Estimation\nAddress Dasgupta\u2019s negative example for non-homogeneous separators by designing an active learner that estimates the \u201clocal topology\u201d around the near-optimal separator (e.g., via localized disagreement regions) and selectively queries to disambiguate intercept vs. direction. The contribution is a refined geometric condition (a localized splitting index) and an algorithm that achieves strong label savings for broad subclasses where standard active learning fails."
      ],
      "generated_ideas_raw": "1. **Variance-Optimal Query Policies for Importance-Weighted Active Learning (IWAL)**\n   Design an IWAL-style active learner that explicitly optimizes the query probability \\(p_t(x)\\) to minimize an empirical Bernstein-style upper bound on the variance of the importance-weighted risk estimator, subject to label budget constraints. Provide finite-sample label complexity bounds that improve over loss-weighting when the loss is heavy-tailed or gradients are large (e.g., logistic loss with rare classes).\n\n2. **Agnostic Active Learning with Distribution Shift via Robust Importance Weighting**\n   Extend A2/IWAL to settings where unlabeled examples come from a target distribution but labeled data is acquired under a different (possibly drifting) selection mechanism, combining ideas from observational counterfactual risk minimization. The key contribution is a statistically consistent active learner with label complexity bounds that depend on a shift measure (e.g., bounded density ratio) and that degrade gracefully under mild violations.\n\n3. **Disagreement- and Splitting-Index-Guided Counterfactual Active Learning**\n   Build an active learner for observational/counterfactual classification that replaces pure disagreement querying with a \u201ccounterfactual splitting index\u201d computed using a doubly robust or stabilized counterfactual risk objective. Show that, analogous to Dasgupta\u2019s splitting index, this quantity predicts when observational active learning can achieve sub-passive label complexity despite selection bias.\n\n4. **Adaptive Noise-Tolerant Active Learning Beyond Bounded-Rate Class Noise**\n   Develop an active algorithm that estimates local noise conditions (near the boundary) online and adapts its querying strategy to avoid \u201cnoise-seeking\u201d behavior highlighted in agnostic settings. Provide guarantees in Tsybakov/Massart-type noise (intermediate between realizable and fully agnostic), quantifying when exponential-to-polynomial transitions occur in label complexity.\n\n5. **Computationally Efficient A2-Style Pruning via Convex Surrogates**\n   Replace hypothesis-set maintenance/enumeration in A2 with a tractable convex surrogate approach: maintain a shrinking feasible region in parameter space (for linear models) defined by confidence sets over surrogate risk. The contribution is a practical algorithm with proofs that the surrogate-driven pruning still yields \\(\\epsilon\\)-optimal 0\u20131 loss within the class under calibrated losses.\n\n6. **Active Learning Lower Bounds Under Observational Selection Policies**\n   Prove information-theoretic label complexity lower bounds for active learning from observational data that mirror K\u00e4\u00e4ri\u00e4inen-style \\(\\Omega(1/\\epsilon^2)\\) limits, but now include dependence on the logging/selection policy (e.g., propensities). Identify regimes where no active strategy can beat passive learning due to unidentifiability induced by selection, clarifying when Paper 2\u2019s improvements are fundamentally possible.\n\n7. **Unlabeled Sample Complexity vs. Label Complexity Trade-offs for Generalized Splitting Index**\n   Extend Dasgupta\u2019s analysis (which already hints unlabeled data may need inflation under non-uniform distributions) to derive explicit optimal trade-offs between unlabeled pool size and label queries for general loss functions (IWAL) and splitting-index-like parameters. Provide an algorithm that adaptively requests more unlabeled data only when needed to keep the effective splitting index stable.\n\n8. **Fairness-Constrained Importance-Weighted Active Learning Under Selection Bias**\n   Incorporate group fairness constraints (e.g., equalized odds) into IWAL/counterfactual active learning by using importance-weighted, group-conditional risk estimators and variance controls. The key contribution is a label-efficient active learner that provably satisfies fairness constraints on the target population even when labels are acquired under biased observational policies.\n\n9. **Batch (Parallel) Active Learning with Controlled Importance-Weight Variance**\n   Convert sequential IWAL/A2 querying into a batch selection method that chooses a set of points per round while controlling the maximum inverse propensity \\(1/p_t\\) and the covariance of the estimator. Provide theory showing near-sequential label complexity (up to log factors) and practical gains for modern annotation pipelines where labels arrive asynchronously.\n\n10. **Active Learning for Non-Homogeneous Separators via Local Geometry Estimation**\n   Address Dasgupta\u2019s negative example for non-homogeneous separators by designing an active learner that estimates the \u201clocal topology\u201d around the near-optimal separator (e.g., via localized disagreement regions) and selectively queries to disambiguate intercept vs. direction. The contribution is a refined geometric condition (a localized splitting index) and an algorithm that achieves strong label savings for broad subclasses where standard active learning fails.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Variance-Optimal Query Policies for Importance-Weighted Active Learning (IWAL)\nDesign an IWAL-style active learner that explicitly optimizes the query probability \\(p_t(x)\\) to minimize an empirical B",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Disagreement- and Splitting-Index-Guided Counterfactual Active Learning\nBuild an active learner for observational/counterfactual classification that replaces pure disagreement querying with a \u201ccounter",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Adaptive Noise-Tolerant Active Learning Beyond Bounded-Rate Class Noise\nDevelop an active algorithm that estimates local noise conditions (near the boundary) online and adapts its querying strategy to",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Unlabeled Sample Complexity vs. Label Complexity Trade-offs for Generalized Splitting Index\nExtend Dasgupta\u2019s analysis (which already hints unlabeled data may need inflation under non-uniform distribu",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Fairness-Constrained Importance-Weighted Active Learning Under Selection Bias\nIncorporate group fairness constraints (e.g., equalized odds) into IWAL/counterfactual active learning by using importance",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Active Learning for Non-Homogeneous Separators via Local Geometry Estimation\nAddress Dasgupta\u2019s negative example for non-homogeneous separators by designing an active learner that estimates the \u201clocal",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 5,
      "paper_title": "Learning Linear Attention in Polynomial Time",
      "contribution": "Shows that multi\u2011head linear attention can be learned in polynomial time by recasting the model as learning a rank\u2011H kernel predictor in an RKHS, and provides an algorithm that both finds near\u2011optimal MHLA parameters and certifies when all empirical best\u2011fits implement the same computation.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12235,
      "output_tokens": 1104,
      "predecessor_details": [
        {
          "success": true,
          "title": "Fast Autoregressive Transformers with Linear Attention",
          "url": "https://arxiv.org/abs/2006.16236",
          "content": "[2006.16236] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention[![close this message](/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](/IgnoreMe)\n[![arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/)&gt;[cs](/list/cs/recent)&gt;arXiv:2006.16236\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2006.16236**(cs)\n[Submitted on 29 Jun 2020 ([v1](https://arxiv.org/abs/2006.16236v1)), last revised 31 Aug 2020 (this version, v3)]\n# Title:Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\nAuthors:[Angelos Katharopoulos](https://arxiv.org/search/cs?searchtype=author&amp;query=Katharopoulos,+A),[Apoorv Vyas](https://arxiv.org/search/cs?searchtype=author&amp;query=Vyas,+A),[Nikolaos Pappas](https://arxiv.org/search/cs?searchtype=author&amp;query=Pappas,+N),[Fran\u00e7ois Fleuret](https://arxiv.org/search/cs?searchtype=author&amp;query=Fleuret,+F)\nView a PDF of the paper titled Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention, by Angelos Katharopoulos and 2 other authors\n[View PDF](/pdf/2006.16236)> > Abstract:\n> Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input&#39;s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences. Comments:|ICML 2020, project at[this https URL](https://linear-transformers.com/)|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2006.16236](https://arxiv.org/abs/2006.16236)[cs.LG]|\n|(or[arXiv:2006.16236v3](https://arxiv.org/abs/2006.16236v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2006.16236](https://doi.org/10.48550/arXiv.2006.16236)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Angelos Katharopoulos [[view email](/show-email/cacdb53f/2006.16236)]\n**[[v1]](/abs/2006.16236v1)**Mon, 29 Jun 2020 17:55:38 UTC (1,626 KB)\n**[[v2]](/abs/2006.16236v2)**Tue, 30 Jun 2020 11:26:55 UTC (1,626 KB)\n**[v3]**Mon, 31 Aug 2020 11:09:32 UTC (1,626 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention, by Angelos Katharopoulos and 2 other authors\n* [View PDF](/pdf/2006.16236)\n* [TeX Source](/src/2006.16236)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](/prevnext?id=2006.16236&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](/prevnext?id=2006.16236&amp;function=next&amp;context=cs.LG)\n[new](/list/cs.LG/new)|[recent](/list/cs.LG/recent)|[2020-06](/list/cs.LG/2020-06)\nChange to browse by:\n[cs](/abs/2006.16236?context=cs)\n[stat](/abs/2006.16236?context=stat)\n[stat.ML](/abs/2006.16236?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2006.16236)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2006.16236)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2006.16236)\n### [2 blog links](/tb/2006.16236)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2006.html#abs-2006-16236)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2006-16236)\n[Angelos Katharopoulos]()\n[Apoorv Vyas]()\n[Nikolaos Pappas]()\n[Fran\u00e7ois Fleuret]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](/auth/show-endorsers/2006.16236)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Katharopoulos et al., 2020)",
          "cleaned_query": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Rethinking Attention with Performers",
          "url": "https://research.google/blog/rethinking-attention-with-performers/",
          "content": "Rethinking Attention with Performers\n[Jump to Content](#page-content)\n[\nResearch](https://research.google/)\n[\nResearch](https://research.google/)\nSearch\n![](https://storage.googleapis.com/gweb-research2023-media/original_images/6ebe377c24a370b71518471f96dc1d48-image12.jpg)\n# Rethinking Attention with Performers\nOctober 23, 2020\nPosted by Krzysztof Choromanski and Lucy Colwell, Research Scientists, Google Research\n## Quick links\n* Share\n* [](https://twitter.com/intent/tweet?text=https://research.google/blog/rethinking-attention-with-performers/)\n* [](https://www.facebook.com/sharer/sharer.php?u=https://research.google/blog/rethinking-attention-with-performers/)\n* [](https://www.linkedin.com/shareArticle?url=https://research.google/blog/rethinking-attention-with-performers/&amp;mini=true)\n* []()\n* Copy link\n\u00d7[Transformer models](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)have achieved state-of-the-art results across a diverse range of domains, including[natural language](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html),[conversation](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html),[images](https://openai.com/blog/image-gpt/), and even[music](https://magenta.tensorflow.org/music-transformer). The core block of every Transformer architecture is the*[attention module](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html),*which computes similarity scores for all pairs of positions in an input sequence. This however, scales poorly with the length of the input sequence, requiring*quadratic*computation time to produce all similarity scores, as well as quadratic memory size to construct a matrix to store these scores.\nFor applications where long-range attention is needed, several fast and more space-efficient proxies have been proposed such as[memory caching techniques](https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html), but a far more common way is to rely on*[sparse attention](https://openai.com/blog/sparse-transformer/)*. Sparse attention reduces computation time and the memory requirements of the attention mechanism by computing a limited selection of similarity scores from a sequence rather than all possible pairs, resulting in a sparse matrix rather than a full matrix. These sparse entries may be manually proposed, found via optimization methods, learned, or even randomized, as demonstrated by such methods as*[Sparse Transformers](https://openai.com/blog/sparse-transformer/),[Longformers](https://arxiv.org/abs/2004.05150),[Routing Transformers](https://arxiv.org/abs/2003.05997),**[Reformers](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html),*and*[Big Bird](https://arxiv.org/abs/2007.14062)*. Since sparse matrices can also be represented by[graphs and edges](https://en.wikipedia.org/wiki/Graph_theory), sparsification methods are also motivated by the[graph neural network](https://arxiv.org/abs/1812.08434)literature, with specific relationships to attention outlined in*[Graph Attention Networks](https://arxiv.org/abs/1710.10903)*. Such sparsity-based architectures usually require additional layers to implicitly produce a full attention mechanism.\n[![](https://1.bp.blogspot.com/-hOTq9edN4Pg/X5IbV0Bb4yI/AAAAAAAAGs4/zs80R-GD_4IUBA-tUHVZwjYSF0Z5uX_rQCLcBGAsYHQ/s16000/image12.jpg)](https://1.bp.blogspot.com/-hOTq9edN4Pg/X5IbV0Bb4yI/AAAAAAAAGs4/zs80R-GD_4IUBA-tUHVZwjYSF0Z5uX_rQCLcBGAsYHQ/s1032/image12.jpg)|\nStandard sparsification techniques.**Left:**Example of a sparsity pattern, where tokens attend only to other nearby tokens.**Right**: In Graph Attention Networks, tokens attend only to their neighbors in the graph, which should have higher relevance than other nodes. See[Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)for a comprehensive categorization of various methods.|\nUnfortunately, sparse attention methods can still suffer from a number of limitations. (1) They require efficient sparse-matrix multiplication operations, which are not available on all accelerators; (2) they usually do not provide rigorous theoretical guarantees for their representation power; (3) they are optimized primarily for Transformer models and generative pre-training; and (4) they usually stack more attention layers to compensate for sparse representations, making them difficult to use with other pre-trained models, thus requiring[retraining and significant energy consumption](https://arxiv.org/abs/1906.02243). In addition to these shortcomings, sparse attention mechanisms are often still not sufficient to address the full range of problems to which regular attention methods are applied, such as[Pointer Networks](https://arxiv.org/abs/1506.03134). There are also some operations that cannot be sparsified, such as the commonly used[softmax operation](https://en.wikipedia.org/wiki/Softmax_function), which normalizes similarity scores in the attention mechanism and is used heavily in[industry-scale recommender systems](https://research.google/pubs/pub45530/).\nTo resolve these issues, we introduce the[Performer](https://arxiv.org/abs/2009.14794), a Transformer architecture with attention mechanisms that scale linearly, thus enabling faster training while allowing the model to process longer lengths, as required for certain image datasets such as[ImageNet64](https://arxiv.org/abs/1707.08819)and text datasets such as[PG-19](https://github.com/deepmind/pg19). The Performer uses an efficient (linear) generalized attention framework, which allows a broad class of attention mechanisms based on different similarity measures (kernels). The framework is implemented by our novel*[Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm](https://github.com/google-research/google-research/tree/master/performer)*, which provides scalable*low-variance*and*unbiased*estimation of attention mechanisms that can be expressed by random feature map decompositions (in particular, regular softmax-attention). We obtain strong accuracy guarantees for this method while preserving linear space and time complexity, which can also be applied to standalone softmax operations.\n**Generalized Attention**\nIn the original attention mechanism, the*query*and*key*inputs, corresponding respectively to rows and columns of a matrix, are multiplied together and passed through a softmax operation to form an attention matrix, which stores the similarity scores. Note that in this method, one cannot decompose the query-key product back into its original query and key components after passing it into the nonlinear softmax operation. However, it is possible to decompose the attention matrix back**to a product of random nonlinear*functions*of the original queries and keys, otherwise known as*random features*, which allows one to encode the similarity information in a more efficient manner.\n[![](https://1.bp.blogspot.com/--ulWTdkc74g/X5Ibm2gSpXI/AAAAAAAAGtA/bUJq-7HyGicl_PmsPICF9Fqi0lVGSHcEgCLcBGAsYHQ/s16000/image8.jpg)](https://1.bp.blogspot.com/--ulWTdkc74g/X5Ibm2gSpXI/AAAAAAAAGtA/bUJq-7HyGicl_PmsPICF9Fqi0lVGSHcEgCLcBGAsYHQ/s889/image8.jpg)|\n**LHS:**The standard attention matrix, which contains all similarity scores for every pair of entries, formed by a softmax operation on the query and keys, denoted by**q**and**k**.**RHS:**The standard attention matrix can be approximated via lower-rank randomized matrices**Q\u2032**and**K\u2032**with rows encoding potentially randomized nonlinear functions of the original queries/keys. For the regular softmax-attention, the transformation is very compact and involves an exponential function as well as random Gaussian projections.|\nRegular softmax-attention can be seen as a special case with these nonlinear functions defined by exponential functions and Gaussian projections. Note that we can also reason inversely, by implementing more general nonlinear functions*first*, implicitly defining other types of similarity measures, or*kernels*, on the query-key product. We frame this as*generalized attention,*based on earlier work in[kernel methods](https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines). Although for most kernels, closed-form formulae do not exist, our mechanism can still be applied since it does not rely on them.\nTo the best of our knowledge, we are the first to show that*any*attention matrix can be*effectively*approximated in downstream Transformer-applications using random features. The novel mechanism enabling this is the use of*positive random features*, i.e.,*positive-valued*nonlinear functions of the original queries and keys, which prove to be crucial for avoiding instabilities during training and provide more accurate approximation of the regular softmax attention mechanism.\n**Towards FAVOR+: Fast Attention via Matrix Associativity**\nThe decomposition described above allows one to store the implicit attention matrix with linear, rather than quadratic, memory complexity. One can also obtain a*linear time*attention mechanism using this decomposition. While the original attention mechanism multiplies the stored attention matrix with the*value*input to obtain the final result, after decomposing the attention matrix, one can rearrange matrix multiplications to approximate the result of the regular attention mechanism, without explicitly constructing the quadratic-sized attention matrix. This ultimately leads to*FAVOR+*.\n[![](https://1.bp.blogspot.com/-pQ8s4X2qXjI/X5Ib6nLtxWI/AAAAAAAAGtI/C7dmMqV3Gu0NGYtmi5Gqjkr_Pqun5T2MwCLcBGAsYHQ/s16000/image10.jpg)](https://1.bp.blogspot.com/-pQ8s4X2qXjI/X5Ib6nLtxWI/AAAAAAAAGtI/C7dmMqV3Gu0NGYtmi5Gqjkr_Pqun5T2MwCLcBGAsYHQ/s1428/image10.jpg)|\n**Left:**Standard attention module computation, where the final desired result is computed by performing a matrix multiplication with the attention matrix**A**and value tensor**V**.**Right:**By decoupling matrices**Q\u2032**and**K\u2032**used in lower rank decomposition of**A",
          "original_query": "Rethinking Attention with Performers (Choromanski et al., 2021)",
          "cleaned_query": "Rethinking Attention with Performers",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Learning with Kernels: Support Vector Machines, Regularization ...",
          "url": "https://direct.mit.edu/books/monograph/1821/Learning-with-KernelsSupport-Vector-Machines",
          "content": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond | Books Gateway | MIT Press[Skip to Main Content](#skipNav)\n[![MIT Press Direct, home](https://mitp.silverchair-cdn.com/UI/app/svg/umbrella/logo.svg)*Open Menu*](javascript:;)\n[*Search Dropdown Menu*](javascript:;)\nheader search\nsearch inputSearch input auto suggest\nfilter your searchAll ContentAll Books\n[Search](javascript:;)\n[Advanced Search](https://direct.mit.edu/advanced-search)\n[*User Tools Dropdown*](javascript:;)\n[Register](https://direct.mit.edu/my-account/register?siteId=5&amp;returnUrl=/books/monograph/1821/Learning-with-KernelsSupport-Vector-Machines)\n[Sign In*Open Menu*](javascript:;)\n[![Books](https://mitp.silverchair-cdn.com/data/SiteBuilderAssets/Live/Images/books/title_Books416199572.svg)](https://direct.mit.edu/books)\n[*Toggle Menu*Menu](javascript:;)\n[Skip Nav Destination](#)\n[![Book cover for Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond](https://mitp.silverchair-cdn.com/mitp/content_public/books/1821/book/4/m_9780262256933-cover.jpeg?Expires=1767703451&amp;Signature=o5gro1U0pRWKctqY6PetfTn8q9I3x6LNQQ3iJHTfCw~OQLmdWQvvd7Dr3yJPlmX8k~cuRFQYdWqY81vG-YRg6UIRyc5YUEiuCSrhwQZui1HxYKHKyGQkzqsgKB4kpFqTbkUIqyGQBYrQ0lrxIIUou63UjqQrcEDkS5tVc0Ba0ZosJT89rRL3AZIGCDgNy~IPPI1zWRaltxO6jOFCoGR0q-y8Sas8yHH0B6S1FvZ8zo6mvT~8odQweyIWxbFRwcS4R-zJVqHhIBDf09ISeqZGyjNKgqDd1v~nCFBIX6WEMsfql9tKMVbeFIsWLjJL98Xo1RVl0YEgqZ5Q~tjBh4arRQ__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)](https://direct.mit.edu/books/monograph/1821/Learning-with-KernelsSupport-Vector-Machines)\nAdaptive Computation and Machine Learning series\n# Learning with Kernels:Support Vector Machines, Regularization, Optimization, and Beyond*Unavailable*\nBy\n[Bernhard Sch\u00f6lkopf](javascript:;),\nBernhard Sch\u00f6lkopf\nBernhard Sch\u00f6lkopf is Director at the Max Planck Institute for Intelligent Systems in T\u00fcbingen, Germany. He is coauthor ofLearning with Kernels(2002) and is a coeditor ofAdvances in Kernel Methods: Support Vector Learning(1998),Advances in Large-Margin Classifiers(2000), andKernel Methods in Computational Biology(2004), all published by the MIT Press.\nSearch for other works by this author on:\n[This Site](https://direct.mit.edu/books/search-results?f_Authors=Bernhard+Sch%C3%B6lkopf)\n[Google Scholar]()\n[Alexander J. Smola](javascript:;)\nAlexander J. Smola\nAlexander J. Smola is Senior Principal Researcher and Machine Learning Program Leader at National ICT Australia/Australian National University, Canberra.\nSearch for other works by this author on:\n[This Site](https://direct.mit.edu/books/search-results?f_Authors=Alexander+J.+Smola)\n[Google Scholar]()\nThe MIT Press\nDOI:\n[https://doi.org/10.7551/mitpress/4175.001.0001](https://doi.org/10.7551/mitpress/4175.001.0001)\nISBN electronic:\n9780262256933\nPublication date:\n2001\n**A comprehensive introduction to Support Vector Machines and related kernel methods.**\nIn the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs\u2014-kernels\u2014for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.\n*Learning with Kernels*provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.\n* [*Share Icon*Share**](javascript:;)\n* [Facebook](javascript:;)\n* [X](javascript:;)\n* [LinkedIn](javascript:;)\n* [Email](javascript:;)\n* [Bluesky](javascript:;)\n* [*Tools Icon*Tools*Open Menu*](javascript:;)\n* [Permissions](https://mitpress.mit.edu/Rights-Permissions/#permissions)\n* [*Cite Icon*Cite](javascript:;)\nLearning with Kernels:Support Vector Machines, Regularization, Optimization, and Beyond\nBy:Bernhard Sch\u00f6lkopf, Alexander J. Smola\nhttps://doi.org/10.7551/mitpress/4175.001.0001\nISBN (electronic):9780262256933\nPublisher:The MIT Press\nPublished:2001\nDownload citation file:\n* [Ris (Zotero)](https://direct.mit.edu/Citation/Download?resourceId=1821&amp;resourceType=1&amp;citationFormat=0)\n* [Reference Manager](https://direct.mit.edu/Citation/Download?resourceId=1821&amp;resourceType=1&amp;citationFormat=0)\n* [EasyBib](https://direct.mit.edu/Citation/Download?resourceId=1821&amp;resourceType=1&amp;citationFormat=0)\n* [Bookends](https://direct.mit.edu/Citation/Download?resourceId=1821&amp;resourceType=1&amp;citationFormat=0)\n* [Mendeley](https://direct.mit.edu/Citation/Download?resourceId=1821&amp;resourceType=1&amp;citationFormat=0)\n* [Papers](https://direct.mit.edu/Citation/Download?resourceId=1821&amp;resourceType=1&amp;citationFormat=0)\n* [EndNote](https://direct.mit.edu/Citation/Download?resourceId=1821&amp;resourceType=1&amp;citationFormat=1)\n* [RefWorks](https://direct.mit.edu/Citation/Download?resourceId=1821&amp;resourceType=1&amp;citationFormat=3)\n* [BibTex](https://direct.mit.edu/Citation/Download?resourceId=1821&amp;resourceType=1&amp;citationFormat=2)\n## Table of Contents\n* [[ Front Matter ]](https://direct.mit.edu/books/monograph/1821/chapter/4919960/Front-Matter)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0029](https://doi.org/10.7551/mitpress/4175.003.0029)\n[Open the*PDF Link*PDFfor [ Front Matter ] in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411770/f010005_9780262256933.pdf)\n* [Series Foreword](https://direct.mit.edu/books/monograph/1821/chapter/50619/Series-Foreword)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0001](https://doi.org/10.7551/mitpress/4175.003.0001)\n[Open the*PDF Link*PDFfor Series Foreword in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411771/9780262256933_fb.pdf)\n* [Preface](https://direct.mit.edu/books/monograph/1821/chapter/50620/Preface)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0002](https://doi.org/10.7551/mitpress/4175.003.0002)\n[Open the*PDF Link*PDFfor Preface in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411772/9780262256933_fc.pdf)\n* [1: A Tutorial Introduction](https://direct.mit.edu/books/monograph/1821/chapter/50621/A-Tutorial-Introduction)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0003](https://doi.org/10.7551/mitpress/4175.003.0003)\n[Open the*PDF Link*PDFfor 1: A Tutorial Introduction in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411773/9780262256933_caa.pdf)\n* I: Concepts and Tools\n* [[Opening]](https://direct.mit.edu/books/monograph/1821/chapter/4919965/Opening)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0030](https://doi.org/10.7551/mitpress/4175.003.0030)\n[Open the*PDF Link*PDFfor [Opening] in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411774/c010002_9780262256933.pdf)\n* [2: Kernels](https://direct.mit.edu/books/monograph/1821/chapter/50623/Kernels)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0005](https://doi.org/10.7551/mitpress/4175.003.0005)\n[Open the*PDF Link*PDFfor 2: Kernels in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411775/9780262256933_cab.pdf)\n* [3: Risk and Loss Functions](https://direct.mit.edu/books/monograph/1821/chapter/50624/Risk-and-Loss-Functions)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0006](https://doi.org/10.7551/mitpress/4175.003.0006)\n[Open the*PDF Link*PDFfor 3: Risk and Loss Functions in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411776/9780262256933_cac.pdf)\n* [4: Regularization](https://direct.mit.edu/books/monograph/1821/chapter/50625/Regularization)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0007](https://doi.org/10.7551/mitpress/4175.003.0007)\n[Open the*PDF Link*PDFfor 4: Regularization in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411777/9780262256933_cad.pdf)\n* [5: Elements of Statistical Learning Theory](https://direct.mit.edu/books/monograph/1821/chapter/50626/Elements-of-Statistical-Learning-Theory)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0008](https://doi.org/10.7551/mitpress/4175.003.0008)\n[Open the*PDF Link*PDFfor 5: Elements of Statistical Learning Theory in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411778/9780262256933_cae.pdf)\n* [6: Optimization](https://direct.mit.edu/books/monograph/1821/chapter/50627/Optimization)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0009](https://doi.org/10.7551/mitpress/4175.003.0009)\n[Open the*PDF Link*PDFfor 6: Optimization in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411779/9780262256933_caf.pdf)\n* II: Support Vector Machines\n* [[Opening]](https://direct.mit.edu/books/monograph/1821/chapter/4919972/Opening)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0031](https://doi.org/10.7551/mitpress/4175.003.0031)\n[Open the*PDF Link*PDFfor [Opening] in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411780/c010003_9780262256933.pdf)\n* [7: Pattern Recognition](https://direct.mit.edu/books/monograph/1821/chapter/50629/Pattern-Recognition)*Unavailable*\nDoi:\n[https://doi.org/10.7551/mitpress/4175.003.0011](https://doi.org/10.7551/mitpress/4175.003.0011)\n[Open the*PDF Link*PDFfor 7: Pattern Recognition in another window](https://direct.mit.edu/books/monograph/chapter-pdf/2411781/9780262256933_cag.pdf)\n* [8: Sing",
          "original_query": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Sch\u00f6lkopf & Smola, 2002)",
          "cleaned_query": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "The Approximation of One Matrix by Another of Lower Rank",
          "url": "https://ccrma.stanford.edu/~dattorro/eckart&young.1936.pdf",
          "content": "PSYCHOMETRIKA--VOL. I, NO. 3\nSEPTEMBER, 193.6\nTHE APPROXIMATION OF ONE MATRIX BY\nANOTHER OF LOWER RANK\nCARL ECKART AND GALE YOUNG\nUniversity of Chicago, Chicago, Illinois\nThe mathematical problem of approximating one matrix by an\u0002other of lower rank is closely related to the fundamental postulate\nof factor-theory. When formulated as a least-squares problem, the\nnormal equations cannot be immediately written do~vn, since the ele\u0002ments of the approximate matrix are not independent of one another.\nThe solution of the problem is simplified by first expressing the mat\u0002rices in a canonic form. It is found that the problem always has a\nsolution which is usually unique. Several conclusions can be drawn\nfrom the form of this solution.\nA hypothetical interpretation of the canonic components of a\nscore matrix is discussed.\nIntroduction\nIf N individuals are each subjected to n tests, it is a fundamental\npostulate of factor theory that the resulting n X N score matrix a\ncan be adequately approximated by another matrix fl whose rank r is\nless than the smaller of n or N. Closely associated to this postulate\nis the purely mathematical problem of finding that matrix fl of rank\nr which most closely approximates a given matrix a of higher rank R.\nIt will be shown that if the least-squares criterion of approximation\nbe adopted, this problem has a general solution which is relatively\nsimple in a theoretical sense, though the amount of numerical work\ninvolved in applications may be prohibitive. Certain conclusions can\nbe drawn from the theoretical solution which may be of importance\nin practical work.\nTo formulate the problem precisely, it is convenient to define the\n\"scalar product\" of two n )~ N matrices as the following numerical\nfunction of their elements:\n(a,,8) ~ ~ ais flis (1)\ni:l ]=I\nwhere .~ is the C]-element of the matrix u. This function has all the\nproperties of the scalar product of vectors:\n(a, fl)~- (flea) (2)\n--211--\n212 PSYCHOMETRIKA\n(a _ fl, r) ~ (a, r) ___ (fi, r) ; (3)\n(za, fl) ~--- z (a, if z i s a number; (4)\n(a,a) >0 ifa=fi0 . (5)\nThe positive number l, defined by l\n2 ~ (a, a) may be called the length\nof the matrix a; the number l\n~ was called the span (Spannung) of \nby Frobenius.* The length of the matrix a -- ~ may be called the dis\u0002tance from a to ft.\nThe problem can now be formulated in a definite manner: to find\nthat matrix ~ of rank r, such that there is no other matrix of rank r\nwhose distance from a is less than the distance from ~ to a. This\namounts to requiring a least-squares solution of the approximation\nproblem, every element of the given matrix being given equal weight.\nSome preliminary theorems and remarks.\nTo simplify the following discussion, let a, b, u, ... denote n X n\nmatrices, A, B, U, ... denote N X N matrices, and a, fl, ... denote\nn ~( N matrices; the special case n --- N is not excluded. The N ~( \nmatrix which is obtained by writing the columns of a as rows is the\ntranspose of a and will be denoted by a\u2019. The products as, aA, aB\u2019,\na\u2019a, ... are defined in the usual way. It is then seen that the following\nequations are correct:\n(a, ~) -- (a\u2019, ~\u2019) ; (6)\n(act, fl) = (a, a\u2019fl) ~-- (a, fla\u2019) ; (7)\n(aA, ~) ~- (a, flA\u2019) = (A, a\u2019fi) (8)\nFrom Eq. (7) and (8) it follows that if u and U are orthogonal \ntrices (u u\u2019 --- u\u2019 u ~- ln, U U\u2019 ----- U\u2019 U ~- 1N), then\n(ua U\u2019, u fl U\u2019) = (a, fl) (9)\nAnother useful proposition is the following: if (a, b) ~ 0 for all\nsymmetric (skew-symmetric) matrices, b, then a is skew-symmetric\n(symmetric).\nThe solution of the problem is much simplified by an appeal to\ntwo theorems which are generalizations of well-known theorems on\nsquare matrices/f They will not be proven here.\n*Quoted from MacDuffee, \"Theory of Matrices\", Ergebn. d. Mathem., v.\n2, No. 5 p. 80 (19~).\ntCourant and Hilbert, \"Methoden der mathematischen Physik\" Berlin, 1924;\npp. 9 et seq., p. 25. MacDuffee, p. 78.\nCARL ECKART AND GALE YOUNG 213\nTheorem I. For any rea~ matrix a, two ortkogonal m~trices u and\nU c~. be found so that ~ ~ uaU\u2019 is a real diagonal matrix with no\nnegative ele~ents.\nA diagonal matrix ~ (square or rectangular) is one for which\ni;; ~- 0 unless i ~- .i. If a diagonal matrix is rectangular, then there\nwill be some rows or columns which consist entirely of zeros. For the\nfollowing, this remark is of some importance, as will be seen. The\nequation of the theorem may also be written\na~u\" ~ U (10)\nwhose right side may be called the canonic resolution of a. If n ~ N,\n~ will have N~n columns of zeros and a is seen to depend only on the\nfirst n rows of U. If u, ~ and the first n rows of U are given, a is\ndetermined.\nLet v be the diagonal n ~ n matrix which consists of the first n\ncolumns of ~, and ~o the n ~( N matrix composed of the first n rows\nof U; then these remarks can be summarized by the equation\na = u\n~ v ~o (10.1)\nwhere ~o o)\u2019 ----- 1,, but ~o\u2019 ~o =/= 1~.. For numerical work, Eq. (10.1) \npreferable to Eq. (10) ; for formal manipulation, Eq. (10) is \nconvenient.\nThe numerical evaluation of u and v (or ~) can be accomplished\nfrom the consideration of the matrix a ~- a a\" alone. This matrix is\nclosely related to the matrix of correlation coefficients of the tests. It\nis seen that\nand since ~ ),\u2019 ~ ~- is adiagonal matrix, it fol lows that u i s oneof t he\northogonal matrices which transform the correlational matrix to di\u0002agonal form. The rows of u are unit vectors along the .principal axes\nof ~ and the squares of the diagonal elements of v (or ~) are the\ncharacteristic values of a; this shows that the latter can never be\nnegative numbers, a result which can also be obtained more directly.*\nThe methods for determining the principal axes and characteristic\nvalues of a symmetric matrix are also known,? so that these remarks\nmay be considered as indicating the method for calculating u and ~.\nIf none of the characteristic values of a is zero (this will presumably\nbe the case in the overwhelming proportion of actual calculations)\n*Courant-Hilbert, p. 20.\n\u00a2Courant-Hilbert, pp. 13, 16.\n214 PSYCHOMETRIKA\nthe matrix v will have a reciprocal, and co can be obtained by solving\nEq. (10.1) \nco--- V-1 U \u00a3t .\nThe numerical values of the elements in the remaining rows of the\nmatrix U will not be needed, but could be found if necessary. For\nsimplicity of manipulation, it is convenient to proceed as though this\nhas been done.\nThe diagonal elements of 2 were called the \"canonical multipliers\"\nof a by Sylvester.* The multipliers and characteristic values of a cor\u0002relational matrix are identical; in the case of a symmetric matrix,\nthere may be a difference in sign; for a general square matrix, there\nis no simple relation between the two; for a rectangular matrix, the\ncharacteristic values are not defined.\nThe correlational matrices, Sylvester called the \"false squares\"\nof a. In the foregoing (and in the usual treatment of factor theory)\nonly the matrix a ~ a a\" has been considered. However, the matrix\nA ~ a\u2019a is related to the correlation coefficients of the individuals in\nthe same manner as a is related to the correlation coefficients of the\ntests. There is complete mathematical symmetry between the two\ncorrelation matrices.\nTo every multiplier, there is associated a row of u and a row of\nU; this complex of n ~ N -~- 1 numbers may be called a canonic com\u0002ponent of a.\nTheorem II. If a ~\" and ~\u2019 a are both symmetric matrices, then\nand only then can two arthogonal matrices u and U be found such\nthat ~ -~ u a U\u2019 and ~ ~ u fl U\" are both real diagonal matrices.\nEither one (but in general, not both) of the diagonal matrices\nmay be further restricted to have no negative elements. This theorem\nis a generalization of the theorem that the principal axes of two sym\u0002metric matrices coincide if and only if ab -- ha.\nSolution of the problem\nThe distance of fl from a is given by x, where\nx2~ (a,a) --2(a, fl) ~- (fl, fl) ; (11)\nx is a function of all the elements of fl, and these are to be determined\ngo that its value is a minimum. The elements of fl are not all indepen-\n.dent, however, because of the requirement that its rank be less\nthan the number of its rows or columns. Theorem I makes it possible\n*Messeng. Math., 1~ p. 45 (1889).\nCARL ECKART AND GALE YOUNG 215\nto eliminate some of the interdependence: suppose ~ to have been re\u0002solved into canonic form:\nfl~u\u2019~ V (12)\nwith ~ diagonal, and u and U orthogonal matrices. Then the rank of\nfl will be r if and only if/~ has this rank i.e., if just r of the diagonal\nelements of ~ are different from zero; the non-vanishing elements of\n~ will be independent. However, the elements of u or U will not be in\u0002dependent, since these matrices must be orthogonal. It is not neces\u0002sary to express these matrices in terms of independent parameters\nbecause of the following proposition:* if u is any orthogonal matrix\nand the independent variables that determine it are given any infini\u0002tesimal increments, the resulting increment of u is\n~u=us , (13)\nwhere s is a skew-symmetric matrix whose elements are infinitesimal,\nbut otherwise arbitrary.\nThe Eq. (11) becomes, because of Eq. (12) and \nx2~ (a,a) --2(a,u\u2019~ U).~t_ (~,~u) (14)\nSince x is to be a minimum, it follows that ~ x\n2 ~ 0 when u is given\nthe increment ~ u (Eq. (13)).\nHence\n0 ~ (a, -- s u\" ~ U) ~ -- (a, s fl) ~ -- (a ~\u2019, (15)\nSince s is an arbitrary skew-symmetric matriX, it follows that a ~t\u2019\nmust be symmetric. Discussing the increment of U in the same man\u0002ner, it will be found that ~/Ya must also be symmetric, and hence, by\nTheorem II, the orthogonal matrices can be found so that Eq. (12)\nand\na ~u\u2019 2 U (12.1)\n(with X the diagonal matrix of the multipliers of a) are both valid.\nThen Eq. (11) becomes\nx\n~ ~ ()~--~, ;t--r)\n--~]~ (2~ -- ~u~) 2 (11.1)\n2i and #~ being the diagonal elements of the corresponding matrices.\nIt remains to determine the matrix ~ so that this expression has\nits minimum value, subject to the condition that just r of the #~ shall\n*Courant-Hitbert, p. 2.7.\n216 PSYCHOMETRIKA\nbe different from zero. It may be supposed th",
          "original_query": "The approximation of one matrix by another of lower rank (Eckart & Young, 1936)",
          "cleaned_query": "The approximation of one matrix by another of lower rank",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Toward efficient agnostic learning - CIS UPenn",
          "url": "https://www.cis.upenn.edu/~mkearns/papers/agnostic-journal.pdf",
          "content": "Machine Learning, 17, 115-141 (1994) \n@ 1994 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands. \nToward Efficient Agnostic Learning \nMICHAEL J. KEARNS \nROBERT E. SCHAPIRE \nAT&T Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ 07974-0636 \nLINDA M. SELLIE \nDepartment of Computer Science, University of Chicago, Chicago, IL 60637 \nmkearns @ research.att.com \nschapire@ research.att.eom \nsellie@research.aU.com \nEditor: Lisa Hellerstein \nAbstract. In this paper we initiate an investigation of generalizations of the Probably Approximately Correct \n(PAC) learning model that attempt to significantly weaken the target function assumptions. The ultimate goal \nin this direction is informally termed agnostic learning, in which we make virtually no assumptions on the \ntarget function. The name derives from the fact that as designers of learning algorithms, we give up the belief \nthat Nature (as represented by the target function) has a simple or succinct explanation. We give a number \nof positive and negative results that provide an initial outline of the possibilities for agnostic learning. Our \nresults include hardness results for the most obvious generalization of the PAC model to an agnostic setting, \nan efficient and general agnostic learning method based on dynamic programming, relationships between loss \nfunctions for agnostic learning, and an algorithm for a learning problem that involves hidden variables. \nKeywords: machine learning, agnostic learning, PAC learning, computational learning theory \n1. Introduction \nOne of the major limitations of the Probably Approximately Correct (or PAC) learn\u0002ing model (Valiant, 1984) (and related models) is the strong assumptions placed on the \nso-called target function that the learning algorithm is attempting to approximate from \nexamples. While such restrictions have permitted a rigorous study of the computational \ncomplexity of learning as a function of the representational complexity of the target \nfunction, the PAC family of models diverges from the setting typically encountered in \npractice and in empirical machine learning research. Empirical approaches often make \nfew or no assumptions on the target function, but search a limited space of hypothe\u0002sis functions in an attempt to find the \"best\" approximation to the target function; in \ncases where the target function is too complex, even this best approximation may incur \nsignificant error. \nIn this paper we initiate an investigation of generalizations of the PAC model in an \nattempt to significantly weaken the target function assumptions whenever possible. Our \nultimate goal is informally termed agnostic learning, 1 in which we make virtually no \nassumptions on the target function. We use the word \"agnostic\" -- whose root means \nliterally \"not known\" -- to emphasize the fact that as designers of learning algorithms, we \nmay have no prior knowledge about the target function. It is important to note that in this \npaper we make no attempt to remove the assumption of statistical independence between \nthe examples seen by a learning algorithm, another worthwhile research direction that \n116 M.J. KEARNS, R.E. SCHAPIRE AND L.M. SELLIE \nhas been pursued by a number of authors (Aldous & Vazirani, 1990; Helmbold & Long, \n1994). \nThis paper describes a preliminary study of the possibilities and limitations for efficient \nagnostic learning. As such, we do not claim to have a definitive model but instead use \na rather general model (based on the work of Haussler (1992)) that allows easy con\u0002sideration of many natural modifications. Perhaps not surprisingly in light of evidence \nfrom the standard PAC model, efficient agnostic learning in its purest form (no assump\u0002tions on target function or distribution) is hard to come by, as some of our results will \ndemonstrate. Thus, we will consider several variations of these perhaps overly ambitious \ncriteria in an attempt to find positive results with target assumptions that are at least \nsignificantly weakened over the standard PAC setting. \nThere are several prior studies of weakened target assumptions for PAC learning that \nare relevant to our work. The first is due to Haussler (1992) who describes a powerful \ngeneralization of the standard PAC model based on decision theory and uniform con\u0002vergence results. Haussler's results are of central importance to much of the research \ndescribed here. Indeed, the agnostic model that we describe is quite similar to Haus\u0002sler's, differing only in the introduction of a \"touchstone\" class (see Section 2). However, \nwhile Haussler's concern is exclusively on the information-theoretic and statistical issues \nin agnostic learning, we are here concerned almost exclusively with efficient computa\u0002tion. Also relevant is the large body of research on nonparametric density estimation in \nthe field of statistics (see, for instance, Izenman's (1991) excellent survey). \nAnother relevant investigation is the work on probabilistic concepts of Kearns and \nSchapire (1990), as well as the work of Yamanishi (1992a) on stochastic rules. Here, \nthe target function is a conditional probability distribution, typically on a discrete range \nspace, such as {0, 1}. A significant portion of the research described in this paper extends \nthis work. Some of the results presented are also closely related to the work of Pitt and \nValiant on heuristic learning (Pitt & Valiant, 1988; Valiant, 1985), which can be viewed \nas a variant of our agnostic PAC model. \nThe following is a brief overview of the paper: in Section 2 we motivate and develop \nin detail the general learning fi'amework we will use. In Section 3 we consider the \nrestriction of this general model to the case of agnostic PAC learning and give strong \nevidence for the intractability of even rather simple learning problems in this model. \nIn Section 4 we discuss the empirical minimization of loss and give a general method \nfor agnostic learning of \"piecewise\" functions that is based on dynamic programming. \nSection 5 gives a useful relationship in the agnostic setting between two common loss \nfunctions, the quadratic and prediction loss, and gives applications of this relationship. \nIn Section 6 we investigate a compromise between agnostic learning and the strong target \nassumptions of the standard PAC model by providing an efficient learning algorithm in \na model for learning problems involving hidden variables. Finally, in Section 7, we list \na few of the many problems that remain open in this area. \nTOWARD EFFICIENT AONOSTIC LEARNING 117 \n2. Definitions and models \nIn this section we define our notation and the generalized framework we will use in \nour attempt to weaken the target function assumptions needed for efficient learning. \nOur approach is strongly influenced by the decision-theoretic learning model that was \nintroduced to the computational learning theory community by Haussler (1992). In giving \nour definitions, we err on the side of formality -- in order to lay the groundwork for \nfuture research on agnostic learning, we wish to give a model that is both precise and \nquite general. For most of the paper, however, we will be using various restrictions of \nthis general model that will be locally specified using less cumbersome notation. \nLet X be a set called the domain; we refer to points in X as instances, and we \nintuitively think of instances as the inputs to a \"black box\" whose behavior we wish \nto learn or to model. Let Y~ be a set called the range, and let Y be a set called the \nobserved range. We think of Y~ as the space of possible values that might be output \nby the black box; however, we introduce Y because we may not have direct access to \nthe output value, but only to some quantity derived from it. In general, we make no \nassumptions about the relationship between Y and Y~. We call a pair (x, y) E X x Y \nan observation. \n2.1. The assumption class \nThe assumption class A is a class of probability distributions on the observation space \nX x Y. We use .4 to represent our assumptions on the phenomenon we are trying to \nlearn or model, and the nature of our observations of this phenomenon. Note that in \nthis definition of A, there may be no functional relationship between x and y in an \nobservation (x, y). However, there are two special cases of this generalized definition \nthat we wish to define. \nIn the first special case, there is a functional relationship, and an arbitrary domain \ndistribution. Thus, consider the case where Y = Y~ and there is a class of functions f\" \nmapping X to Y~. Suppose .A is the class obtained by choosing any distribution D over \nX and any f E F, and letting ADd C A be the distribution generating observations \n(x, f(z)), where x is drawn randomly from D. Then we say that A is the functional \ndecomposition using F, and we have a familiar distribution-free function learning model. \nIn the second special case, we have Y' = [0, 1], Y = {0, 1} and there is again a class \nof functions F mapping X to Y~. Now, however, the functional value is not directly \nobserved. Instead, let A be the class obtained by choosing any distribution D over X \nand any f E F, and letting AD,y E A be the distribution generating observations (x, b), \nwhere x is drawn randomly from D and b = 1 with probability f(x) and b = 0 with \nprobability I - f(x). We call 5 c a class of probabilistic concepts (or p-concepts), and \nwe say that ~1. is the p-concept decomposition using F. Here we have a distribution-free \np-concept learning model. \nIn the case that .A is either the functional or p-concept decomposition using a class F, \nwe refer to .7- as the target class, and if the distribution AD,I E \u00a24 generates the obser\u0002vations we call f the target function or target p-concept and D the target distribution. \n118 M.J. KEARNS, R.E. SCHAPIRE AND L.M. SELLIE \n2.2. The hypothesis class 7-\u00a2 and the touchstone class 7\" \nWe next introduce two classes of functions from X to yl: the hypothesis class ~, ",
          "original_query": "Agnostic learning / Agnostic PAC learning formalism (Kearns et al., mid\u20111990s and related literature)",
          "cleaned_query": "Agnostic learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Characterizing the Expressivity of Fixed-Precision Transformer ...",
          "url": "https://arxiv.org/html/2505.23623v2",
          "content": "Characterizing the Expressivity of Fixed-Precision Transformer Language Models\n# Characterizing the Expressivity of Fixed-Precision Transformer Language Models\nJiaoda Li \u2004\u2004Ryan Cotterell\n{[jiaoda.li](mailto:jiaoda.li@inf.ethz.ch),[ryan.cotterell](mailto:ryan.cotterell@inf.ethz.ch)}@inf.ethz.ch\n![[Uncaptioned image]](figs/ethz.png)\n###### Abstract\nTransformer-based language models (LMs) have achieved widespread empirical success, but their theoretical expressive power remains only partially understood.\nIn this work, we analyze a restricted idealization of fixed-precision transformers with strict future masking, soft attention, and no positional encodings.\nWe establish that this class of models is exactly as expressive as a specific fragment of linear temporal logic that contains only a single temporal operator: thepastoperator.\nWe further connect this fragment to established classes in formal language theory, automata theory, and algebra, yielding a unified framework for understanding transformer expressivity under this idealization.\nFinally, we present empirical results that align closely with our theory: transformers trained on languages within their characterized expressive capacity generalize reliably across sequence lengths, while they consistently fail to generalize on languages beyond it.111Code available at[GitHub repository](https://github.com/rycolab/expressivity-of-fixed-precision-transformers).\n## 1Introduction\nTransformer-based language models (LMs) have demonstrated remarkable empirical success> [\n[> 46\n](https://arxiv.org/html/2505.23623v2#bib.bib46)> , [> 36\n](https://arxiv.org/html/2505.23623v2#bib.bib36)> , [> 12\n](https://arxiv.org/html/2505.23623v2#bib.bib12)> ]\non a wide variety of natural language tasks> [\n[> 47\n](https://arxiv.org/html/2505.23623v2#bib.bib47)> , [> 19\n](https://arxiv.org/html/2505.23623v2#bib.bib19)> , [> 41\n](https://arxiv.org/html/2505.23623v2#bib.bib41)> , > inter alia\n> ]\n.\nThis success has sparked growing interest in understanding the theoretical expressive power of transformers, i.e., what languages they can and cannot recognize, and, by extension, what tasks they can and cannot perform.\nA significant body of work approaches this question by relating transformers to well-established frameworks such as formal languages, logic, and circuit complexity> [\n[> 18\n](https://arxiv.org/html/2505.23623v2#bib.bib18)> , [> 31\n](https://arxiv.org/html/2505.23623v2#bib.bib31)> , [> 50\n](https://arxiv.org/html/2505.23623v2#bib.bib50)> , [> 42\n](https://arxiv.org/html/2505.23623v2#bib.bib42)> ]\n.\nTo facilitate their theoretical analysis, theoreticians often propose idealizations of transformers.\nFor instance, while practical implementations of transformers operate under fixed precision, e.g., single (32-bit) or half (16-bit) precision, many authors assume arbitrary> [\n[> 38\n](https://arxiv.org/html/2505.23623v2#bib.bib38)> , [> 18\n](https://arxiv.org/html/2505.23623v2#bib.bib18)> , [> 34\n](https://arxiv.org/html/2505.23623v2#bib.bib34)> ]\nor length-dependent precision> [\n[> 32\n](https://arxiv.org/html/2505.23623v2#bib.bib32)> , [> 7\n](https://arxiv.org/html/2505.23623v2#bib.bib7)> ]\n.\nAlthough such idealizations capture key aspects of transformers, they tend to overestimate their expressive power> [\n[> 38\n](https://arxiv.org/html/2505.23623v2#bib.bib38)> ]\n.\nA recent step toward a more faithful theoretical understanding of the expressive power of transformers comes from> Yang et al. [\n[> 50\n](https://arxiv.org/html/2505.23623v2#bib.bib50)> ]\n, who show that fixed-precision transformers with strict future masking and unique hard attention (UHA) are exactly as expressive as linear temporal logicLTL\u200b[P,F,S,U]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}},{\\\\mathrel{\\\\textup{{F}}}},{\\\\mathrel{\\\\textup{{S}}}},{\\\\mathrel{\\\\textup{{U}}}}]}, which includes four temporal operators:P{\\\\mathrel{\\\\textup{{P}}}}(past),F{\\\\mathrel{\\\\textup{{F}}}}(future),S{\\\\mathrel{\\\\textup{{S}}}}(since), andU{\\\\mathrel{\\\\textup{{U}}}}(until).\nHowever, UHA still deviates from the soft attention used in practice. To address this gap,> Yang and Chiang [\n[> 49\n](https://arxiv.org/html/2505.23623v2#bib.bib49)> ]\nanalyze fixed-precision transformers with strict future masking and soft attention, an idealization that most closely reflects the models deployed in real-world applications.> Yang and Chiang [\n[> 49\n](https://arxiv.org/html/2505.23623v2#bib.bib49)> ]\nshow that such models are upper bounded by C-RASP, a counting-based programming language, though a precise characterization of these models\u2019 expressivity remains open.\nIn this paper, we close this gap by providing an exact characterization of the expressive power of fixed-precision transformers with soft attention, strict masking, and no positional encodings (NoPE).\nWe show they are precisely characterized byLTL\u200b[P]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}}]}, a restricted fragment ofLTL\u200b[P,F,S,U]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}},{\\\\mathrel{\\\\textup{{F}}}},{\\\\mathrel{\\\\textup{{S}}}},{\\\\mathrel{\\\\textup{{U}}}}]}that uses only thepastoperator (P{\\\\mathrel{\\\\textup{{P}}}}). We further demonstrate thatLTL\u200b[P]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}}]}is equivalent in expressivity to partially ordered deterministic finite automata (PODFAs), which are characterized by\u211b{{{\\\\mathcal{R}}}}-trivial monoids and recognize left-deterministic polynomials.\nThese results offer a detailed and principled characterization of the expressive power of this idealization, delineating its strengths and limitations.\nCrucially, our findings imply that many simple languages, e.g., bounded Dyck languages, which have been shown to be recognizable under more permissive idealizations, are beyond the reach of the models we study.\nWe also extend our theoretical results to transformer LMs, showing that their expressivity matches that of transformer recognizers. A visual overview of the theoretical landscape is provided in[Fig.\u02dc1](https://arxiv.org/html/2505.23623v2#S1.F1).\nLTL\u200b[P]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}}]}PFO2\u200b[&lt;]{\\\\textup{{PFO}}^{2}[\\\\mathord{&lt;&lt;}]}PODFA\u211b{{{\\\\mathcal{R}}}}-trivialmonoidleft-deterministicpolynomialtransformerlanguage modeltransformer[Theorem\u02dc2.1](https://arxiv.org/html/2505.23623v2#S2.Thmtheorem1)[Theorem\u02dc5.2](https://arxiv.org/html/2505.23623v2#S5.Thmtheorem2)[Theorem\u02dc5.1](https://arxiv.org/html/2505.23623v2#S5.Thmtheorem1)[Theorem\u02dc3.3](https://arxiv.org/html/2505.23623v2#S3.Thmtheorem3)[Theorem\u02dc3.2](https://arxiv.org/html/2505.23623v2#S3.Thmtheorem2)[Lemma\u02dcC.1](https://arxiv.org/html/2505.23623v2#A3.Thmtheorem1)[Lemma\u02dcC.4](https://arxiv.org/html/2505.23623v2#A3.Thmtheorem4)> [\n[> 4\n](https://arxiv.org/html/2505.23623v2#bib.bib4)> ]\nFigure 1:Roadmap of the paper.Red arrowsindicate novel results.\nTo arrive at a compelling theory, it is essential to show that it faithfully reflects the behavior of models trained under standard machine learning paradigms. To this end, we provide empirical evidence using the length generalization framework, a widely used method for gauging neural network expressivity> [\n[> 10\n](https://arxiv.org/html/2505.23623v2#bib.bib10)> , [> 6\n](https://arxiv.org/html/2505.23623v2#bib.bib6)> , [> 22\n](https://arxiv.org/html/2505.23623v2#bib.bib22)> ]\n. We construct a suite of languages spanning a fine-grained hierarchy of formal language classes. Our results ([Tab.\u02dc1](https://arxiv.org/html/2505.23623v2#S5.T1)) exhibit strong alignment between theory and practice: for all languages that transformers are predicted to recognize, the models generalize perfectly over lengths (100%100\\\\%accuracy); for languages beyond their theoretical capacity, they consistently make generalization errors, regardless of learning rates or random seeds.\n## 2Background\nIn this section, we present the necessary background knowledge that underpins our analysis.\n### 2.1Strings and Languages\nAnalphabet, denoted as\u03a3{{\\\\Sigma}}, is a finite, non-empty set of symbols.\nAstringover\u03a3{{\\\\Sigma}}is a*finite*sequence of symbols drawn from\u03a3{{\\\\Sigma}}.\nThe set of all strings over\u03a3{{\\\\Sigma}}is denoted by its Kleene star\u03a3\u2217{{{{\\\\Sigma}}^{\\*}}}.\nA subset of\u03a3\u2217{{{{\\\\Sigma}}^{\\*}}}is called alanguage.\nAregular expressionis a declarative way to describe a language, defined recursively as follows:\n* \u2022\u2205\\\\emptysetand eacha\u2208\u03a3{\\\\texttt{a}}\\\\in{{\\\\Sigma}}are regular expressions;\n* \u2022If\u03b1\\\\alphaand\u03b2\\\\betaare regular expressions, so are the union\u03b1+\u03b2\\\\alpha+\\\\beta, concatenation\u03b1\u200b\u03b2\\\\alpha\\\\beta, the Kleene star\u03b1\u2217\\\\alpha^{\\*}, and complement\u03b1c{{\\\\alpha^{\\\\textsf{c}}}}.\nA language isregularif and only if it can be described by a regular expression> [\n[> 26\n](https://arxiv.org/html/2505.23623v2#bib.bib26)> ]\n. A regular language is said to bestar-freeif it can be described by a regular expression without the Kleene star> [\n[> 28\n](https://arxiv.org/html/2505.23623v2#bib.bib28)> ]\n. As an example,\u03a3\u2217{{{{\\\\Sigma}}^{\\*}}}, the set of all strings over\u03a3{{\\\\Sigma}}, is star-free, as it can be described by\u2205c{{\\\\emptyset^{\\\\textsf{c}}}}.\n### 2.2LTL\u200b[P]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}}]}\nLinear temporal logicLTL\u200b[P,F,S,U]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}},{\\\\mathrel{\\\\textup{{F}}}},{\\\\mathrel{\\\\textup{{S}}}},{\\\\mathrel{\\\\textup{{U}}}}]}> [\n[> 25\n](https://arxiv.org/html/2505.23623v2#bib.bib25)> ]\nis a modal logic, with modalities referring to time.\nThe full definition is given in[\u00a7\u02dcA.1](https://arxiv.org/html/2505.23623v2#A1.SS1). In this paper, we define a fragment ofLTL\u200b[P,F,S,U]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}},{\\\\mathrel{\\\\textup{{F}}}},{\\\\mathrel{\\\\textup{{S}}}},{\\\\mathrel{\\\\textup{{U}}}}]}\u2014denoted asLTL\u200b[P]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}}]}\u2014that includes only one temporal operatorP{\\\\mathrel{\\\\textup{{P}}}}(past).FormulasinLTL\u200b[P]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}}]}are composed ofatomic formulas\u03c0a{\\\\pi}\\_{\\\\texttt{a}}for everya\u2208\u03a3{\\\\texttt{a}}\\\\in{{\\\\Sigma}}, Boolean connectives\u2227,\u00ac\\\\land,\\\\lnot, and a temporal operatorP{\\\\mathrel{\\\\text",
          "original_query": "Expressivity results for Transformers (representative works: P\u00e9rez et al., 2019 and related literature)",
          "cleaned_query": "Expressivity results for Transformers",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Agnostic Learning Guarantees for Linear-Attention Transformers via Kernel Complexity\nDevelop a PAC-agnostic generalization bound for linear-attention Transformers (e.g., Linear Transformers, Performers) by explicitly treating the attention mechanism as learning in an implicit kernel space induced by feature maps. Use kernel-method tools (Rademacher complexity, margin bounds) to relate sample complexity to the random-feature dimension (Performer) or chosen kernel map (linear transformer), yielding actionable guidance for setting feature counts and regularization.",
        "Learned Feature-Map Kernels for Performer Attention with SVM-Style Regularization\nReplace fixed random features (FAVOR+) with a trainable, structured feature map whose parameters are constrained by kernel-validity and regularized similarly to kernel learning (e.g., multiple kernel learning / norm constraints). Implement a bilevel or alternating optimization that (i) updates feature-map parameters to maximize likelihood and (ii) enforces stability/unbiasedness constraints, then benchmark variance\u2013speed\u2013accuracy tradeoffs against standard Performer.",
        "Low-Rank + Random-Feature Hybrid Attention Using Eckart\u2013Young Optimality\nCreate an attention approximation that explicitly decomposes the attention matrix into a best rank\u2011r component (via streaming/online low-rank updates inspired by Eckart\u2013Young) plus a residual estimated with random features (Performer). The contribution is a controllable two-part approximation with a provable Frobenius-error decomposition and a practical recipe for allocating compute between rank and random features depending on sequence length and memory budget.",
        "Expressivity of Linear-Attention Models: Mapping Kernel Choices to Temporal-Logic Fragments\nExtend fixed-precision expressivity results by characterizing how different linear-attention kernels (exponential/softmax-approximating, polynomial, cosine) change the recognized language class, stated as inclusion relations between temporal-logic fragments (e.g., beyond LTL[P] when adding specific kernel features or lightweight positional signals). Provide constructive separations: synthetic languages that become learnable with one kernel family but not another, while keeping strict masking and fixed precision.",
        "Minimal Positional Signals that Strictly Increase Fixed-Precision Expressivity in Linear Attention\nIdentify the weakest positional mechanism (e.g., one scalar clock feature, bounded relative bias, or low-bit rotary variant) that provably lifts NoPE fixed-precision Transformers beyond the LTL[P] characterization while preserving linear-time attention. The work would offer both a theoretical theorem (strict expressivity increase) and an implementation that maintains the recurrent/streaming benefits of linear attention.",
        "Dynamic-Programming Distillation of Long-Context Dependencies into Linear Transformers\nInspired by Kearns\u2013Schapire dynamic programming for agnostic learning of \u201cpiecewise\u201d functions, design a teacher\u2013student pipeline where a DP-based oracle computes optimal long-range summaries for a structured task (e.g., segmentation, pattern counting), and a linear-attention Transformer is trained to imitate those summaries online. This yields a concrete method to inject algorithmic structure into linear attention and test whether it improves length generalization on tasks known to be hard under fixed-precision NoPE assumptions.",
        "Certified Length-Generalization Benchmarks for Linear Attention Using Formal-Language Separations\nBuild a benchmark suite where each task is paired with a proof of (un)recognizability under fixed-precision soft attention NoPE (per the LTL[P]/PODFA characterization), then systematically test Linear Transformers and Performers with controlled additions (kernel type, feature count, tiny positional cues). The key contribution is a reproducible protocol linking empirical length generalization directly to formal expressivity predictions, enabling diagnosis of when gains come from architecture vs. optimization artifacts.",
        "Adaptive Random-Feature Budgeting for Performers with Agnostic Risk Control\nDesign an algorithm that chooses the number of random features per layer/token on-the-fly based on an online estimate of approximation variance and a target excess-risk bound in an agnostic setting. Implement token-adaptive feature allocation (more features for high-entropy contexts) while keeping expected linear complexity, and report compute savings at fixed perplexity/accuracy on long-sequence tasks.",
        "Kernelized Attention as a Preconditioned Optimization Problem: Faster Training via Nystr\u00f6m/Low-Rank Preconditioners\nTreat attention computation as repeated kernel matrix-vector products and introduce Nystr\u00f6m-style preconditioners derived from a small set of landmark tokens (or pooled states) to accelerate convergence during training. Combine Eckart\u2013Young intuition (best low-rank approximation) with kernel optimization theory to propose a concrete preconditioning scheme, then measure improvements in training speed and stability for long-context autoregressive modeling with linear attention."
      ],
      "generated_ideas_raw": "1. **Agnostic Learning Guarantees for Linear-Attention Transformers via Kernel Complexity**\n   Develop a PAC-agnostic generalization bound for linear-attention Transformers (e.g., Linear Transformers, Performers) by explicitly treating the attention mechanism as learning in an implicit kernel space induced by feature maps. Use kernel-method tools (Rademacher complexity, margin bounds) to relate sample complexity to the random-feature dimension (Performer) or chosen kernel map (linear transformer), yielding actionable guidance for setting feature counts and regularization.\n\n2. **Learned Feature-Map Kernels for Performer Attention with SVM-Style Regularization**\n   Replace fixed random features (FAVOR+) with a trainable, structured feature map whose parameters are constrained by kernel-validity and regularized similarly to kernel learning (e.g., multiple kernel learning / norm constraints). Implement a bilevel or alternating optimization that (i) updates feature-map parameters to maximize likelihood and (ii) enforces stability/unbiasedness constraints, then benchmark variance\u2013speed\u2013accuracy tradeoffs against standard Performer.\n\n3. **Low-Rank + Random-Feature Hybrid Attention Using Eckart\u2013Young Optimality**\n   Create an attention approximation that explicitly decomposes the attention matrix into a best rank\u2011r component (via streaming/online low-rank updates inspired by Eckart\u2013Young) plus a residual estimated with random features (Performer). The contribution is a controllable two-part approximation with a provable Frobenius-error decomposition and a practical recipe for allocating compute between rank and random features depending on sequence length and memory budget.\n\n4. **Online Autoregressive Linear Transformers with Certified Numerical Stability (Fixed Precision)**\n   Using the \u201cTransformers are RNNs\u201d iterative formulation, design numerically stable recurrence updates for linear attention under fp16/bfloat16 (e.g., rescaling, log-domain normalization, or bounded kernel maps). Evaluate both speed and failure modes (overflow/underflow, accumulation error) and provide precision-aware theoretical error bounds connecting to fixed-precision expressivity limitations.\n\n5. **Expressivity of Linear-Attention Models: Mapping Kernel Choices to Temporal-Logic Fragments**\n   Extend fixed-precision expressivity results by characterizing how different linear-attention kernels (exponential/softmax-approximating, polynomial, cosine) change the recognized language class, stated as inclusion relations between temporal-logic fragments (e.g., beyond LTL[P] when adding specific kernel features or lightweight positional signals). Provide constructive separations: synthetic languages that become learnable with one kernel family but not another, while keeping strict masking and fixed precision.\n\n6. **Minimal Positional Signals that Strictly Increase Fixed-Precision Expressivity in Linear Attention**\n   Identify the weakest positional mechanism (e.g., one scalar clock feature, bounded relative bias, or low-bit rotary variant) that provably lifts NoPE fixed-precision Transformers beyond the LTL[P] characterization while preserving linear-time attention. The work would offer both a theoretical theorem (strict expressivity increase) and an implementation that maintains the recurrent/streaming benefits of linear attention.\n\n7. **Dynamic-Programming Distillation of Long-Context Dependencies into Linear Transformers**\n   Inspired by Kearns\u2013Schapire dynamic programming for agnostic learning of \u201cpiecewise\u201d functions, design a teacher\u2013student pipeline where a DP-based oracle computes optimal long-range summaries for a structured task (e.g., segmentation, pattern counting), and a linear-attention Transformer is trained to imitate those summaries online. This yields a concrete method to inject algorithmic structure into linear attention and test whether it improves length generalization on tasks known to be hard under fixed-precision NoPE assumptions.\n\n8. **Certified Length-Generalization Benchmarks for Linear Attention Using Formal-Language Separations**\n   Build a benchmark suite where each task is paired with a proof of (un)recognizability under fixed-precision soft attention NoPE (per the LTL[P]/PODFA characterization), then systematically test Linear Transformers and Performers with controlled additions (kernel type, feature count, tiny positional cues). The key contribution is a reproducible protocol linking empirical length generalization directly to formal expressivity predictions, enabling diagnosis of when gains come from architecture vs. optimization artifacts.\n\n9. **Adaptive Random-Feature Budgeting for Performers with Agnostic Risk Control**\n   Design an algorithm that chooses the number of random features per layer/token on-the-fly based on an online estimate of approximation variance and a target excess-risk bound in an agnostic setting. Implement token-adaptive feature allocation (more features for high-entropy contexts) while keeping expected linear complexity, and report compute savings at fixed perplexity/accuracy on long-sequence tasks.\n\n10. **Kernelized Attention as a Preconditioned Optimization Problem: Faster Training via Nystr\u00f6m/Low-Rank Preconditioners**\n   Treat attention computation as repeated kernel matrix-vector products and introduce Nystr\u00f6m-style preconditioners derived from a small set of landmark tokens (or pooled states) to accelerate convergence during training. Combine Eckart\u2013Young intuition (best low-rank approximation) with kernel optimization theory to propose a concrete preconditioning scheme, then measure improvements in training speed and stability for long-context autoregressive modeling with linear attention.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Agnostic Learning Guarantees for Linear-Attention Transformers via Kernel Complexity\nDevelop a PAC-agnostic generalization bound for linear-attention Transformers (e.g., Linear Transformers, Performer",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Learned Feature-Map Kernels for Performer Attention with SVM-Style Regularization\nReplace fixed random features (FAVOR+) with a trainable, structured feature map whose parameters are constrained by ke",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Low-Rank + Random-Feature Hybrid Attention Using Eckart\u2013Young Optimality\nCreate an attention approximation that explicitly decomposes the attention matrix into a best rank\u2011r component (via streaming/o",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Expressivity of Linear-Attention Models: Mapping Kernel Choices to Temporal-Logic Fragments\nExtend fixed-precision expressivity results by characterizing how different linear-attention kernels (expone",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Minimal Positional Signals that Strictly Increase Fixed-Precision Expressivity in Linear Attention\nIdentify the weakest positional mechanism (e.g., one scalar clock feature, bounded relative bias, or ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Dynamic-Programming Distillation of Long-Context Dependencies into Linear Transformers\nInspired by Kearns\u2013Schapire dynamic programming for agnostic learning of \u201cpiecewise\u201d functions, design a teacher\u2013",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Certified Length-Generalization Benchmarks for Linear Attention Using Formal-Language Separations\nBuild a benchmark suite where each task is paired with a proof of (un)recognizability under fixed-prec",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Adaptive Random-Feature Budgeting for Performers with Agnostic Risk Control\nDesign an algorithm that chooses the number of random features per layer/token on-the-fly based on an online estimate of app",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Kernelized Attention as a Preconditioned Optimization Problem: Faster Training via Nystr\u00f6m/Low-Rank Preconditioners\nTreat attention computation as repeated kernel matrix-vector products and introduce ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 6,
      "paper_title": "Optimal Mistake Bounds for Transductive Online Learning",
      "contribution": "Shows that the transductive online mistake bound is \u0398(\u221ad) (giving an \u2126(\u221ad) lower bound and a matching O(\u221ad) upper bound), thereby establishing a quadratic separation from the standard online bound of \u0398(d).",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "quality_content": 4,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 7531,
      "output_tokens": 939,
      "predecessor_details": [
        {
          "success": true,
          "title": "Nick Littlestone's research works | Harvard University and other places",
          "url": "https://www.researchgate.net/scientific-contributions/Nick-Littlestone-7647712",
          "content": "Nick Littlestone's research works | Harvard University and other places\n# Nick Littlestone\u2019s research while affiliated with Harvard University and other places\n## What is this page?\nThis page lists works of an author who doesn't have a ResearchGate profile or hasn't added the works to their profile yet. It is automatically generated from public (personal) data to further our legitimate goal of comprehensive and accurate scientific recordkeeping. If you are this author and want this page removed, please[let us know](https://help.researchgate.net/hc/en-us/requests/new?tf_38727008724625=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOjAsImlhdCI6MTc2NTIwOTE3NCwiaXNzIjoiaHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC8ifQ.uW5T2z8I0pO8wA5HeGjnPyF8OGU3IHDY_NSpnbTc5bA&amp;ticket_form_id=13146494812305).\n## Publications (20)\n[Relating Data Compression and Learnability](https://www.researchgate.net/publication/2808486_Relating_Data_Compression_and_Learnability)\n* [Article](publication/2808486_Relating_Data_Compression_and_Learnability)\nOctober 2000\n\u00b7443 Reads\n\u00b7269 Citations\n[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\n\u00b7[![](https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg)Manfred Warmuth](profile/Manfred-Warmuth)\nWe explore the learnability of two-valued functions from samples using the paradigm of Data Compression. A rst algorithm (compression) choses a small subset of the sample which is called the kernel. A second algorithm predicts future values of the function from the kernel, i.e. the algorithm acts as an hypothesis for the function to be learned. The second algorithm must be able to reconstruct the correct function values when given a point of the original sample. We demonstrate that the existence of a suitable data compression scheme is sucient to ensure learnability. We express the probability that the hypothesis predicts the function correctly on a random sample point as a function of the sample and kernel sizes. No assumptions are made on the probability distributions according to which the sample points are generated. This approach provides an alternative to that of [BEHW86], which uses the Vapnik-Chervonenkis dimension to classify learnable geometric concepts. Our bo...\nRead more\n[On-Line Learning with Linear Loss Constraints.](https://www.researchgate.net/publication/220246538_On-Line_Learning_with_Linear_Loss_Constraints)\n* [Article](publication/220246538_On-Line_Learning_with_Linear_Loss_Constraints)\nSeptember 2000\n\u00b757 Reads\n\u00b72 Citations\nInformation and Computation\n[![](https://i1.rgstatic.net/ii/profile.image/272706960621583-1442029814255_Q64/David-Helmbold.jpg)David P. Helmbold](profile/David-Helmbold)\n\u00b7[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\n\u00b7[![](https://i1.rgstatic.net/ii/profile.image/279224091987970-1443583619545_Q64/Philip-Long-2.jpg)Philip Long](profile/Philip-Long-2)\nWe consider a generalization of the mistake-bound model (for learning {0, 1}-valued functions) in which the learner must satisfy a general constraint on the number M+ of incorrect 1 predictions and the number M- of incorrect 0 predictions. We describe a general-purpose optimal algorithm for our formulation of this problem, We describe several applications of our general results, involving situations in which the learner wishes to satisfy linear inequalities in M+ and M-. (C) 2000 Academic Press.\nRead more\n[Apple Tasting.](https://www.researchgate.net/publication/220248281_Apple_Tasting)\n* [Article](publication/220248281_Apple_Tasting)\nSeptember 2000\n\u00b786 Reads\n\u00b724 Citations\nInformation and Computation\n[![](https://i1.rgstatic.net/ii/profile.image/272706960621583-1442029814255_Q64/David-Helmbold.jpg)David P. Helmbold](profile/David-Helmbold)\n\u00b7[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\n\u00b7[![](https://i1.rgstatic.net/ii/profile.image/279224091987970-1443583619545_Q64/Philip-Long-2.jpg)Philip Long](profile/Philip-Long-2)\nIn the standard on-line model the learning algorithm tries to minimizethe total number of mistakes made in a series of trials. On each trial the learner sees an instance, makes a prediction of its classification, then finds out the correct classification. We define a natural variant of this model (\u201capple tasting\u201d) whereu\u2022 the classes are interpreted as the good and bad instances,\u2022 the prediction is interpreted as accepting or rejecting the instance,and\u2022 the learner gets feedback only when the instance is accepted.We use two transformations to relate the apple tasting model to an enhanced standard model where false acceptances are counted separately from false rejections. We apply our results to obtain a good general-purpose apple tasting algorithm as well as nearly optimal apple tasting algorithms for a variety of standard classes, such as conjunctions and disjunctions of n boolean variables. We also present and analyze a simpler transformation useful when the instances are drawn at random rather than selected by an adversary.\nRead more\n[The Weighted Majority Algorithm](https://www.researchgate.net/publication/2803045_The_Weighted_Majority_Algorithm)\n* [Article](publication/2803045_The_Weighted_Majority_Algorithm)\nMay 2000\n\u00b7707 Reads\n\u00b71,469 Citations\nInformation and Computation\n[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\n\u00b7[![](https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg)Manfred Warmuth](profile/Manfred-Warmuth)\nWe study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case that the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust in the presence of errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log jAj + m) mi...\nRead more\n[The Robustness of the](https://www.researchgate.net/publication/221497223_The_Robustness_of_the)\n* [Conference Paper](publication/221497223_The_Robustness_of_the)\nJanuary 1999\n\u00b710 Reads\n\u00b75 Citations\n[![](https://i1.rgstatic.net/ii/profile.image/272437031993365-1441965458467_Q64/Claudio-Gentile-3.jpg)Claudio Gentile](profile/Claudio-Gentile-3)\n\u00b7[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\n[Learning in the Presence of Finitely or Infinitely Many Irrelevant](https://www.researchgate.net/publication/2858346_Learning_in_the_Presence_of_Finitely_or_Infinitely_Many_Irrelevant)\n* [Article](publication/2858346_Learning_in_the_Presence_of_Finitely_or_Infinitely_Many_Irrelevant)\nFebruary 1998\n\u00b77 Reads\n\u00b75 Citations\n[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Avrim Blum](scientific-contributions/Avrim-Blum-73372623)\n\u00b7[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Lisa Hellerstein](scientific-contributions/Lisa-Hellerstein-7030311)\n\u00b7[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\nThis paper addresses the problem of learning boolean functions in query and mistake-bound models in the presence of irrelevant attributes. In learning a concept, a learner may observe a great many more attributes than those the concept depends upon, and in some sense the presence of extra, irrelevant attributes does not change the underlying concept being learned.\nRead more\n[Comparing Several Linear-threshold Learning Algorithms on Tasks Involving Superfluous Attributes](https://www.researchgate.net/publication/221344805_Comparing_Several_Linear-threshold_Learning_Algorithms_on_Tasks_Involving_Superfluous_Attributes)\n* [Conference Paper](publication/221344805_Comparing_Several_Linear-threshold_Learning_Algorithms_on_Tasks_Involving_Superfluous_Attributes)\nDecember 1995\n\u00b733 Reads\n\u00b727 Citations\n[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\nUsing simulations, we compare several linear-threshold learning algorithms that differ greatly in the effect of superfluous attributes on their learning abilities. These include a Bayesian algorithm for conditionally independent attributes and two mistake-driven algorithms (algorithms that learn only from trials in which they predict incorrectly), Winnow and the Perceptron algorithm. We also look at a mistake-driven modification of the Bayesian algorithm. When there are many superfluous attributes, Winnow makes the fewest mistakes; in our experiments it takes a great many such attributes to make this difference marked. With the addition of what we call a checking procedure, Winnow was able to eventually get within twice the optimal loss rate in all of the experiments that we have focused ",
          "original_query": "Nick Littlestone (1987) \u2014 \"Learning Quickly When Irrelevant Attributes Abound\"",
          "cleaned_query": "Nick Littlestone",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Adaptive time-frequency analysis and data normalization - HAL",
          "url": "https://hal.science/tel-04892428v1/file/121330_LEIBER_2024_archivage.pdf",
          "content": "HAL Id: tel-04892428\nhttps://hal.science/tel-04892428v1\nSubmitted on 16 Jan 2025\nHAL is a multi-disciplinary open access\narchive for the deposit and dissemination of sci\u0002entific research documents, whether they are pub\u0002lished or not. The documents may come from\nteaching and research institutions in France or\nabroad, or from public or private research centers.\nL\u2019archive ouverte pluridisciplinaire HAL, est\ndestin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents\nscientifiques de niveau recherche, publi\u00e9s ou non,\n\u00e9manant des \u00e9tablissements d\u2019enseignement et de\nrecherche fran\u00e7ais ou \u00e9trangers, des laboratoires\npublics ou priv\u00e9s.\nAdaptive time-frequency analysis and data\nnormalization : contributions to monitoring under\nvarying conditions\nMaxime Leiber\nTo cite this version:\nMaxime Leiber. Adaptive time-frequency analysis and data normalization : contributions to monitor\u0002ing under varying conditions. Computer Science [cs]. Ecole Normale Sup\u00e9rieure (ENS), 2024. English.\nffNNT : ff. fftel-04892428ff\nPr\u00e9par\u00e9e \u00e0 l\u2019\u00c9cole Normale Sup\u00e9rieure\nAdaptive time-frequency analysis and data normalization :\ncontributions to monitoring under varying conditions\nSoutenue par\nMaxime LEIBER\nLe 26 f\u00e9vrier 2024\n\u00c9cole doctorale no386\nSciences Math\u00e9matiques\nde Paris Centre\nSp\u00e9cialit\u00e9\nMath\u00e9matiques\nPr\u00e9par\u00e9e \u00e0\nInria et Safran Tech\nComposition du jury :\n\u00c9milie CHOUZENOUX Pr\u00e9sidente du jury\nDirectrice de recherche, Inria Saclay Examinatrice\nPatrice ABRY Rapporteur\nDirecteur de recherche, CNRS ENS Lyon\nBruno TORR\u00c9SANI Rapporteur\nProfesseur, Aix-Marseille Universit\u00e9\nFlorence D\u2019ALCH\u00c9-BUC Examinatrice\nProfesseure, T\u00e9l\u00e9com Paris\nYosra MARNISSI Encadrante\nIng\u00e9nieure de recherche, Safran Tech\nAxel BARRAU Encadrant\nIng\u00e9nieur de recherche, Safran Tech\nLaurent MASSOULI\u00c9 Directeur de th\u00e8se\nDirecteur de recherche, Inria Paris\n\nR\u00e9sum\u00e9\nCette th\u00e8se vise \u00e0 am\u00e9liorer les outils de traitement du signal utilis\u00e9s chez Safran pour surveiller\nles syst\u00e8mes m\u00e9caniques dans des conditions externes variables.\nLa contribution principale consiste \u00e0 automatiser le r\u00e9glage des param\u00e8tres de la transform\u00e9e\nde Fourier \u00e0 court terme (TFCT), un algorithme essentiel pour l\u2019analyse des vibrations non sta\u0002tionnaires dans les moteurs d\u2019avion. Bien que la TFCT soit une repr\u00e9sentation temps-fr\u00e9quence\ntr\u00e8s utilis\u00e9e, son efficacit\u00e9 d\u00e9pend de param\u00e8tres tels que la longeur de fen\u00eatre, qui peuvent\n\u00eatre difficiles \u00e0 estimer au pr\u00e9alable, rendant la recherche des param\u00e8tres optimaux une t\u00e2che\nlaborieuse. Cette th\u00e8se introduit une version diff\u00e9rentiable de la TFCT, facilitant l\u2019optimisation\nautomatique de ses param\u00e8tres de mani\u00e8re plus efficace et pr\u00e9cise que les m\u00e9thodes convention\u0002nelles [Leiber, 2023a]. Cette approche ouvre \u00e9galement la voie \u00e0 des applications hybrides en\nint\u00e9grant facilement la TFCT diff\u00e9rentiable dans les r\u00e9seaux de neurones.\nLa deuxi\u00e8me contribution porte sur la normalisation des donn\u00e9es dans des conditions ex\u0002ternes variables, o\u00f9 les m\u00e9thodes traditionnelles, qui reposent sur des donn\u00e9es acquises dans\ndes conditions similaires, pr\u00e9sentent une applicabilit\u00e9 limit\u00e9e. Cette th\u00e8se propose une m\u00e9thode\nde normalisation bas\u00e9e sur les donn\u00e9es qui exploite la variabilit\u00e9 des conditions externes pour\ncorriger les variables mesur\u00e9es par rapport aux conditions ext\u00e9rieures [Leiber, 2023c]. Cette ap\u0002proche transforme les quantit\u00e9s mesur\u00e9es en quantit\u00e9s corrig\u00e9es qui capturent le comportement\ninterne du syst\u00e8me tout en \u00e9liminant l\u2019impact des variables externes.\nMots cl\u00e9s : TFCT diff\u00e9rentiable, optimisation de param\u00e8tre, traitement et apprentissage adap\u0002tatifs, analyse temps-fr\u00e9quence, non stationnaire\ni\nAbstract\nThis thesis aims to enhance the signal processing tools employed at Safran for monitoring me\u0002chanical systems under varying external conditions.\nThe primary contribution focuses on automating the parameter tuning of the short-time\nFourier transform (STFT), a pivotal algorithm for analyzing non-stationary vibrations in aircraft\nengines. Although the STFT is a widely used time-frequency representation, its effectiveness\nhinges on parameters such as window length, which can be challenging to estimate beforehand,\nthus rendering the search for optimal parameters a laborious task. This thesis introduces a\ndifferentiable version of the STFT, facilitating the automatic optimization of its parameters in\na more efficient and precise manner compared to conventional methods [Leiber, 2023a]. This\napproach also paves the way for hybrid applications by seamlessly integrating the differentiable\nSTFT into neural networks.\nThe second contribution addresses the normalization of data under variable conditions, where\ntraditional methods, reliant on date acquired under similar conditions, exhibit limited applica\u0002bility. This thesis proposes a data-driven normalization method that harnesses the variability\nof external conditions to rectify the measured variables with respect to the external conditions\n[Leiber, 2023c]. This approach transforms the measured quantities into corrected values that\ncapture the internal behavior of the system while eliminating the influence of external variables.\nKeywords : differentiable STFT, parameter optimization, adaptive processing and learning,\ntime-frequency analysis, non-stationary\nii\nTable of contents\nR\u00e9sum\u00e9 i\nAbstract ii\nTable of contents ii\nList of figures v\nList of tables vii\nList of abbreviations viii\n1 General introduction 1\n1 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3 Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2 Differentiable short-time Fourier transform: A learnable time-frequency layer 9\n1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2 Short-time Fourier transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3 Differentiable Short-time Fourier transform . . . . . . . . . . . . . . . . . . . . . 28\n4 Computation of DSTFT derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n5 Practical implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n6 Extension to other representations . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n3 Gradient-based tuning of DSTFT parameters for adaptive time-frequency\nrepresentation 46\n1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n2 Optimization for best representation . . . . . . . . . . . . . . . . . . . . . . . . . 49\n3 Optimization for task performance . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n4 Variable standardization with respect to external conditions for monitoring\nunder varying conditions 81\n1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n2 A general Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n3 A physics-inspired model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\niii\nTable of contents\n5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5 Conclusion and perspectives 102\n1 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n2 Perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\nAppendices 105\n1 Appendix A: Frequency estimation . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n2 Appendix B: Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\nBibliography / References 110\niv\nList of figures\n2.1 The Hann and rectangular windows and their frequency responses (in dB). . . . 20\n2.2 A signal and its magnitude discrete Fourier transform. . . . . . . . . . . . . . . . 20\n2.3 Spectrograms of the signal from Figure 2.2, comparing the Hann and rectangular\nwindows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.4 Spectrograms of the signal from Figure 2.2, comparing long and short windows. . 23\n2.5 An example of tapering function : the Hann window . . . . . . . . . . . . . . . . 30\n2.6 Differentiable STFT: The window support N of the subsignal on which DFT is\ncomputed is fixed, while on other hand, the temporal resolution \u03b8\u03c4\u03be of the tapering\nfunction \u03c9N (k \u2212 t\u03c4 , \u03b8\u03c4 \u03be), which actually determines time resolution is allowed to\nvary. Additionally, the position of the tapering windows can smoothly shift along\nthe time axis, while the window supports start at the integer part of the temporal\nposition of the tapering windows. The exponent j denotes the iteration in the\ngradient descent optimizer. H is the distance between two consecutive supports,\nand H\u02dc is the distance between two consecutive tapering functions. . . . . . . . . 31\n2.7 The sinusoidal frequency component of the signal. . . . . . . . . . . . . . . . . . 37\n2.8 Loss function and evolution of loss during optimization with different initialization. 37\n2.9 Spectrogram obtained at the convergence of DSTFT using a single window length. 38\n2.10 Evolution of the loss across gradient iterations. . . . . . . . . . . . . . . . . . . . 38\n2.11 Runtime of the forward and backward passes at each gradient iteration on CPU. 39\n2.12 Runtime of the forward and backward passes at each gradient iteration on GPU. 39\n2.13 Continuous window length. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n2.14 Continuous window temporal position. . . . . . . . . . . . . . . . . . . . . . . . . 41\n2.15 QR code link to GitHub code of DSTFT. . . . . . . . . . . . . . . . . . . . . . . 42\n2.16 The Hann window with the same length and with different exponent values",
          "original_query": "Ben\u2011David, Kushilevitz, and Mansour (1995, 1997) \u2014 work on transductive online learning",
          "cleaned_query": "Ben\u2011David, Kushilevitz, and Mansour",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Fine-Grained Distribution-Dependent Learning Curves",
          "url": "https://proceedings.mlr.press/v195/bousquet23a.html",
          "content": "\n [ edit] \n \n \n Proceedings of Thirty Sixth Conference on Learning Theory,\u00a0PMLR 195:5890-5924,\u00a02023.\n \n \n Abstract \n \n Learning curves plot the expected error of a learning algorithm as a function of the number of labeled samples it receives from a target distribution. They are widely used as a measure of an algorithm\u2019s performance, but classic PAC learning theory cannot explain their behavior. As observed by Antos and Lugosi (1996, 1998), the classic \u2018No Free Lunch\u2019 lower bounds only trace the upper envelope above all learning curves of specific target distributions. For a concept class with VC dimension $d$ the classic bound decays like $d/n$, yet it is possible that the learning curve for \\emph{every} specific distribution decays exponentially. In this case, for each $n$ there exists a different \u2018hard\u2019 distribution requiring $d/n$ samples. Antos and Lugosi asked which concept classes admit a \u2018strong minimax lower bound\u2019 \u2013 a lower bound of $d\u2019/n$ that holds for a fixed distribution for infinitely many $n$.We solve this problem in a principled manner, by introducing a combinatorial dimension called VCL that characterizes the best $d\u2019$ for which $d\u2019/n$ is a strong minimax lower bound. Conceptually, the VCL dimension determines the asymptotic rate of decay of the minimax learning curve, which we call the \u2018distribution-free trail\u2019 of the class. Our characterization strengthens the lower bounds of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021), and it refines their analysis of learning curves, by showing that for classes with finite VCL the learning rate can be decomposed into a linear component that depends only on the hypothesis class and a faster (e.g., exponential) component that depends also on the target distribution. As a corollary, we recover the lower bound of Antos and Lugosi (1996, 1998) for half-spaces in $\\mathbb{R}^d$.Finally, to provide another viewpoint on our work and how it compares to traditional PAC learning bounds, we also present an alternative formulation of our results in a language that is closer to the PAC setting.\n \n Cite this Paper \n \n \n \n BibTeX\n \n \n \n@InProceedings{pmlr-v195-bousquet23a,\n title = {Fine-Grained Distribution-Dependent Learning Curves},\n author = {Bousquet, Olivier and Hanneke, Steve and Moran, Shay and Shafer, Jonathan and Tolstikhin, Ilya},\n booktitle = {Proceedings of Thirty Sixth Conference on Learning Theory},\n pages = {5890--5924},\n year = {2023},\n editor = {Neu, Gergely and Rosasco, Lorenzo},\n volume = {195},\n series = {Proceedings of Machine Learning Research},\n month = {12--15 Jul},\n publisher = {PMLR},\n pdf = {https://proceedings.mlr.press/v195/bousquet23a/bousquet23a.pdf},\n url = {https://proceedings.mlr.press/v195/bousquet23a.html},\n abstract = {Learning curves plot the expected error of a learning algorithm as a function of the number of labeled samples it receives from a target distribution. They are widely used as a measure of an algorithm\u2019s performance, but classic PAC learning theory cannot explain their behavior. As observed by Antos and Lugosi (1996, 1998), the classic \u2018No Free Lunch\u2019 lower bounds only trace the upper envelope above all learning curves of specific target distributions. For a concept class with VC dimension $d$ the classic bound decays like $d/n$, yet it is possible that the learning curve for \\emph{every} specific distribution decays exponentially. In this case, for each $n$ there exists a different \u2018hard\u2019 distribution requiring $d/n$ samples. Antos and Lugosi asked which concept classes admit a \u2018strong minimax lower bound\u2019 \u2013 a lower bound of $d\u2019/n$ that holds for a fixed distribution for infinitely many $n$.We solve this problem in a principled manner, by introducing a combinatorial dimension called VCL that characterizes the best $d\u2019$ for which $d\u2019/n$ is a strong minimax lower bound. Conceptually, the VCL dimension determines the asymptotic rate of decay of the minimax learning curve, which we call the \u2018distribution-free trail\u2019 of the class. Our characterization strengthens the lower bounds of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021), and it refines their analysis of learning curves, by showing that for classes with finite VCL the learning rate can be decomposed into a linear component that depends only on the hypothesis class and a faster (e.g., exponential) component that depends also on the target distribution. As a corollary, we recover the lower bound of Antos and Lugosi (1996, 1998) for half-spaces in $\\mathbb{R}^d$.Finally, to provide another viewpoint on our work and how it compares to traditional PAC learning bounds, we also present an alternative formulation of our results in a language that is closer to the PAC setting.}\n}\n \n \n \n \n \n \n Endnote\n \n \n %0 Conference Paper\n%T Fine-Grained Distribution-Dependent Learning Curves\n%A Olivier Bousquet\n%A Steve Hanneke\n%A Shay Moran\n%A Jonathan Shafer\n%A Ilya Tolstikhin\n%B Proceedings of Thirty Sixth Conference on Learning Theory\n%C Proceedings of Machine Learning Research\n%D 2023\n%E Gergely Neu\n%E Lorenzo Rosasco \n%F pmlr-v195-bousquet23a\n%I PMLR\n%P 5890--5924\n%U https://proceedings.mlr.press/v195/bousquet23a.html\n%V 195\n%X Learning curves plot the expected error of a learning algorithm as a function of the number of labeled samples it receives from a target distribution. They are widely used as a measure of an algorithm\u2019s performance, but classic PAC learning theory cannot explain their behavior. As observed by Antos and Lugosi (1996, 1998), the classic \u2018No Free Lunch\u2019 lower bounds only trace the upper envelope above all learning curves of specific target distributions. For a concept class with VC dimension $d$ the classic bound decays like $d/n$, yet it is possible that the learning curve for \\emph{every} specific distribution decays exponentially. In this case, for each $n$ there exists a different \u2018hard\u2019 distribution requiring $d/n$ samples. Antos and Lugosi asked which concept classes admit a \u2018strong minimax lower bound\u2019 \u2013 a lower bound of $d\u2019/n$ that holds for a fixed distribution for infinitely many $n$.We solve this problem in a principled manner, by introducing a combinatorial dimension called VCL that characterizes the best $d\u2019$ for which $d\u2019/n$ is a strong minimax lower bound. Conceptually, the VCL dimension determines the asymptotic rate of decay of the minimax learning curve, which we call the \u2018distribution-free trail\u2019 of the class. Our characterization strengthens the lower bounds of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021), and it refines their analysis of learning curves, by showing that for classes with finite VCL the learning rate can be decomposed into a linear component that depends only on the hypothesis class and a faster (e.g., exponential) component that depends also on the target distribution. As a corollary, we recover the lower bound of Antos and Lugosi (1996, 1998) for half-spaces in $\\mathbb{R}^d$.Finally, to provide another viewpoint on our work and how it compares to traditional PAC learning bounds, we also present an alternative formulation of our results in a language that is closer to the PAC setting.\n \n \n \n \n \n \n APA\n \n \n \nBousquet, O., Hanneke, S., Moran, S., Shafer, J. &amp; Tolstikhin, I.. (2023). Fine-Grained Distribution-Dependent Learning Curves. Proceedings of Thirty Sixth Conference on Learning Theory, in Proceedings of Machine Learning Research 195:5890-5924 Available from https://proceedings.mlr.press/v195/bousquet23a.html.\n \n \n \n \n \n \n Related Material \n \n \n \n",
          "original_query": "Hanneke, Moran, and Shafer (2023)",
          "cleaned_query": "Hanneke, Moran, and Shafer",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Vladimir Vapnik - The Data Science Institute at Columbia University",
          "url": "https://datascience.columbia.edu/people/vladimir-vapnik/",
          "content": "[Faculty](https://datascience.columbia.edu/people-type/faculty/)\n![](https://datascience.columbia.edu/wp-content/uploads/2020/08/Vapnik_web.png)\n# Vladimir Vapnik\nThe Fu Foundation School of Engineering and Applied ScienceProfessor of Computer Science\n[vladimir.vapnik@gmail.com](mailto:vladimir.vapnik@gmail.com)\n[https://en.wikipedia.org/wiki/Vladimir\\_Vapnik](https://en.wikipedia.org/wiki/Vladimir_Vapnik)\n###### Center Affiliations\n[Foundations of Data Science](https://datascience.columbia.edu/research/centers/foundations-of-data-science/) Affiliated Member\nProfessor Vapnik gained his Masters Degree in Mathematics in 1958 at Uzbek State University, Samarkand, USSR. From 1961 to 1990 he worked at the Institute of Control Sciences, Moscow, where he became Head of the Computer Science Research Department. He then joined AT&T Bell Laboratories, Holmdel, NJ, having been appointed Professor of Computer Science and Statistics at Royal Holloway in 1995.\nProfessor Vapnik has taught and researched in computer science, theoretical and applied statistics for over 30 years. He has published 6 monographs and over a hundred research papers. His major achievements have been the development of a general theory of minimizing the expected risk using empirical data and a new type of learning machine called Support Vector machine that possesses a high level of generalization ability. These techniques have been used to solve many pattern recognition and regression estimation problems and have been applied to the problems of dependency estimation, forecasting, and constructing intelligent machines. His current research is presented in his latest books \u201cStatistical Learning Theory\u201d, Wiley, 1998, and \u201cThe Nature of Statistical Learning Theory\u201d, second edition, Springer, 2000.\nHe was one of the invited speakers at the Colloquium [\u201cThe Importance of being Learnable\u201d](http://www.clrc.rhul.ac.uk/events/compintell1998.htm) hosted by the Computer Learning Research Centre at Royal Holloway in September 1998.",
          "original_query": "Vladimir Vapnik \u2014 transductive inference (Statistical Learning Theory)",
          "cleaned_query": "Vladimir Vapnik",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "VCL-Refined Generalization Bounds for Compression-Based Learners\nDevelop a theory connecting Littlestone\u2013Warmuth sample compression schemes to the VCL dimension to yield *distribution-dependent* learning-curve bounds. Prove rates that decompose into a class-dependent \u201clinear\u201d term (via VCL) plus a distribution-dependent \u201cfast\u201d term (e.g., exponential) for compression-based ERM and stable compression algorithms.",
        "Learning Curves for Differentiable STFT Parameter Learning under Varying Conditions\nModel the differentiable STFT window/stride/bandwidth parameters as part of the hypothesis class and study how their learnability changes the learning curve under specific operating-condition distributions. Empirically and theoretically test whether the effective VCL decreases after condition-normalization, explaining faster-than-\\(d/n\\) error decay in monitoring tasks.",
        "Online Weighted Majority with Linear Mistake Constraints and Partial Feedback for Monitoring\nExtend Littlestone\u2019s \u201conline learning with linear loss constraints\u201d to the apple-tasting (accept/reject) feedback regime tailored to condition-varying mechanical monitoring. Design an algorithm that enforces explicit constraints like \u201cfalse-accepts \u2264 \u03b1, false-rejects \u2264 \u03b2\u201d while learning from selectively revealed labels, and prove regret/mistake tradeoffs.",
        "Condition-Aware Expert Pooling: Weighted Majority over Normalizers + Classifiers\nConstruct an expert set where each expert is a pipeline: (normalization model \u2192 feature map such as STFT \u2192 classifier such as SVM). Use Weighted Majority to adaptively weight these pipelines online as external conditions drift, and quantify when this achieves smaller effective complexity (and improved learning curves) than selecting a single pipeline.",
        "VCL for Margin-Based Classes in Time-Frequency Feature Spaces\nDefine and compute/upper-bound the VCL dimension for large-margin separators (SVM-style) operating on differentiable-STFT features. Use this to predict when learning curves transition from \\(d/n\\)-type behavior to fast rates under benign vibration distributions (e.g., clustered operating regimes), validating the predictions on engine-condition datasets.",
        "Distribution-Dependent Hardness under Condition Shift: Constructing Fixed \u201cHard\u201d Regimes\nUsing the \u201cstrong minimax lower bound\u201d framework, build explicit *fixed* operating-condition distributions for which monitoring/classification remains \\( \\Omega(d'/n) \\) for infinitely many \\(n\\), even after normalization. This would give actionable guidance on which external-condition variabilities cannot be \u201cnormalized away\u201d and require additional sensing or model structure.",
        "Differentiable STFT as a Compression Mechanism: Kernel Selection Guarantees\nInterpret learnable STFT parameterization plus sparsification (e.g., selecting salient time-frequency bins) as a sample compression scheme where a small \u201ckernel\u201d of informative events reconstructs predictions. Prove generalization bounds in terms of the learned kernel size and compare to classical VC/VCL predictions, yielding a principled method for choosing sparse TF representations.",
        "Apple-Tasting for Fault Detection with Risk Control via Structural Risk Minimization\nCombine Vapnik\u2019s structural risk minimization (nested hypothesis classes) with apple-tasting feedback to create a fault detector that only \u201caccepts to inspect\u201d when uncertainty warrants it. Provide guarantees that control inspection budget and false accepts while selecting model capacity (e.g., SVM margin/feature complexity) to optimize the resulting learning curve.",
        "Estimating VCL from Data to Predict Learning-Curve Regimes in Practice\nDesign a practical estimator or set of diagnostics (based on empirical shattering-like tests or stability/proxy dimensions) to approximate VCL for a given feature+model pipeline (including differentiable STFT). Use it to forecast whether additional labeled data will yield linear, sublinear, or exponential improvements for a specific condition distribution\u2014turning theory into a deployment tool.",
        "Adversarial vs Stochastic External Conditions: Unified Analysis of Online Monitoring Pipelines\nDevelop a unified framework that interpolates between adversarially chosen conditions (online learning) and randomly varying conditions (stochastic monitoring), explicitly leveraging the two transformations used in apple tasting. Prove performance bounds that degrade gracefully as the condition process becomes more adversarial, and validate with controlled experiments where operating regimes are progressively \u201cadversarialized.\u201d"
      ],
      "generated_ideas_raw": "1. **VCL-Refined Generalization Bounds for Compression-Based Learners**  \nDevelop a theory connecting Littlestone\u2013Warmuth sample compression schemes to the VCL dimension to yield *distribution-dependent* learning-curve bounds. Prove rates that decompose into a class-dependent \u201clinear\u201d term (via VCL) plus a distribution-dependent \u201cfast\u201d term (e.g., exponential) for compression-based ERM and stable compression algorithms.\n\n2. **Learning Curves for Differentiable STFT Parameter Learning under Varying Conditions**  \nModel the differentiable STFT window/stride/bandwidth parameters as part of the hypothesis class and study how their learnability changes the learning curve under specific operating-condition distributions. Empirically and theoretically test whether the effective VCL decreases after condition-normalization, explaining faster-than-\\(d/n\\) error decay in monitoring tasks.\n\n3. **Online Weighted Majority with Linear Mistake Constraints and Partial Feedback for Monitoring**  \nExtend Littlestone\u2019s \u201conline learning with linear loss constraints\u201d to the apple-tasting (accept/reject) feedback regime tailored to condition-varying mechanical monitoring. Design an algorithm that enforces explicit constraints like \u201cfalse-accepts \u2264 \u03b1, false-rejects \u2264 \u03b2\u201d while learning from selectively revealed labels, and prove regret/mistake tradeoffs.\n\n4. **Condition-Aware Expert Pooling: Weighted Majority over Normalizers + Classifiers**  \nConstruct an expert set where each expert is a pipeline: (normalization model \u2192 feature map such as STFT \u2192 classifier such as SVM). Use Weighted Majority to adaptively weight these pipelines online as external conditions drift, and quantify when this achieves smaller effective complexity (and improved learning curves) than selecting a single pipeline.\n\n5. **VCL for Margin-Based Classes in Time-Frequency Feature Spaces**  \nDefine and compute/upper-bound the VCL dimension for large-margin separators (SVM-style) operating on differentiable-STFT features. Use this to predict when learning curves transition from \\(d/n\\)-type behavior to fast rates under benign vibration distributions (e.g., clustered operating regimes), validating the predictions on engine-condition datasets.\n\n6. **Distribution-Dependent Hardness under Condition Shift: Constructing Fixed \u201cHard\u201d Regimes**  \nUsing the \u201cstrong minimax lower bound\u201d framework, build explicit *fixed* operating-condition distributions for which monitoring/classification remains \\( \\Omega(d'/n) \\) for infinitely many \\(n\\), even after normalization. This would give actionable guidance on which external-condition variabilities cannot be \u201cnormalized away\u201d and require additional sensing or model structure.\n\n7. **Differentiable STFT as a Compression Mechanism: Kernel Selection Guarantees**  \nInterpret learnable STFT parameterization plus sparsification (e.g., selecting salient time-frequency bins) as a sample compression scheme where a small \u201ckernel\u201d of informative events reconstructs predictions. Prove generalization bounds in terms of the learned kernel size and compare to classical VC/VCL predictions, yielding a principled method for choosing sparse TF representations.\n\n8. **Apple-Tasting for Fault Detection with Risk Control via Structural Risk Minimization**  \nCombine Vapnik\u2019s structural risk minimization (nested hypothesis classes) with apple-tasting feedback to create a fault detector that only \u201caccepts to inspect\u201d when uncertainty warrants it. Provide guarantees that control inspection budget and false accepts while selecting model capacity (e.g., SVM margin/feature complexity) to optimize the resulting learning curve.\n\n9. **Estimating VCL from Data to Predict Learning-Curve Regimes in Practice**  \nDesign a practical estimator or set of diagnostics (based on empirical shattering-like tests or stability/proxy dimensions) to approximate VCL for a given feature+model pipeline (including differentiable STFT). Use it to forecast whether additional labeled data will yield linear, sublinear, or exponential improvements for a specific condition distribution\u2014turning theory into a deployment tool.\n\n10. **Adversarial vs Stochastic External Conditions: Unified Analysis of Online Monitoring Pipelines**  \nDevelop a unified framework that interpolates between adversarially chosen conditions (online learning) and randomly varying conditions (stochastic monitoring), explicitly leveraging the two transformations used in apple tasting. Prove performance bounds that degrade gracefully as the condition process becomes more adversarial, and validate with controlled experiments where operating regimes are progressively \u201cadversarialized.\u201d",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "VCL-Refined Generalization Bounds for Compression-Based Learners\nDevelop a theory connecting Littlestone\u2013Warmuth sample compression schemes to the VCL dimension to yield *distribution-dependent* learn",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Learning Curves for Differentiable STFT Parameter Learning under Varying Conditions\nModel the differentiable STFT window/stride/bandwidth parameters as part of the hypothesis class and study how their",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Online Weighted Majority with Linear Mistake Constraints and Partial Feedback for Monitoring\nExtend Littlestone\u2019s \u201conline learning with linear loss constraints\u201d to the apple-tasting (accept/reject) fe",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Condition-Aware Expert Pooling: Weighted Majority over Normalizers + Classifiers\nConstruct an expert set where each expert is a pipeline: (normalization model \u2192 feature map such as STFT \u2192 classifier s",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "VCL for Margin-Based Classes in Time-Frequency Feature Spaces\nDefine and compute/upper-bound the VCL dimension for large-margin separators (SVM-style) operating on differentiable-STFT features. Use th",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Distribution-Dependent Hardness under Condition Shift: Constructing Fixed \u201cHard\u201d Regimes\nUsing the \u201cstrong minimax lower bound\u201d framework, build explicit *fixed* operating-condition distributions for ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Differentiable STFT as a Compression Mechanism: Kernel Selection Guarantees\nInterpret learnable STFT parameterization plus sparsification (e.g., selecting salient time-frequency bins) as a sample comp",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Apple-Tasting for Fault Detection with Risk Control via Structural Risk Minimization\nCombine Vapnik\u2019s structural risk minimization (nested hypothesis classes) with apple-tasting feedback to create a f",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Estimating VCL from Data to Predict Learning-Curve Regimes in Practice\nDesign a practical estimator or set of diagnostics (based on empirical shattering-like tests or stability/proxy dimensions) to ap",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Adversarial vs Stochastic External Conditions: Unified Analysis of Online Monitoring Pipelines\nDevelop a unified framework that interpolates between adversarially chosen conditions (online learning) a",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 7,
      "paper_title": "State Entropy Regularization for Robust Reinforcement Learning",
      "contribution": "Shows that regularizing the entropy of the state-visitation distribution yields provable robustness to structured and spatially correlated perturbations (under reward and transition uncertainty), contrasts these guarantees with policy-entropy regularization, and analyzes practical sensitivities such as number of rollouts.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "input_tokens": 10050,
      "output_tokens": 1031,
      "predecessor_details": [
        {
          "success": true,
          "title": "Twice regularized MDPs and the equivalence between robustness ...",
          "url": "https://arxiv.org/abs/2110.06267",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2110.06267** (cs)\n\n\\[Submitted on 12 Oct 2021\\]\n\n# Title:Twice regularized MDPs and the equivalence between robustness and regularization\n\nAuthors: [Esther Derman](https://arxiv.org/search/cs?searchtype=author&query=Derman,+E), [Matthieu Geist](https://arxiv.org/search/cs?searchtype=author&query=Geist,+M), [Shie Mannor](https://arxiv.org/search/cs?searchtype=author&query=Mannor,+S)\n\nView a PDF of the paper titled Twice regularized MDPs and the equivalence between robustness and regularization, by Esther Derman and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2110.06267)\n\n> Abstract:Robust Markov decision processes (MDPs) aim to handle changing or partially known system dynamics. To solve them, one typically resorts to robust optimization methods. However, this significantly increases computational complexity and limits scalability in both learning and planning. On the other hand, regularized MDPs show more stability in policy learning without impairing time complexity. Yet, they generally do not encompass uncertainty in the model dynamics. In this work, we aim to learn robust MDPs using regularization. We first show that regularized MDPs are a particular instance of robust MDPs with uncertain reward. We thus establish that policy iteration on reward-robust MDPs can have the same time complexity as on regularized MDPs. We further extend this relationship to MDPs with uncertain transitions: this leads to a regularization term with an additional dependence on the value function. We finally generalize regularized MDPs to twice regularized MDPs (R${}^2$ MDPs), i.e., MDPs with $\\\\textit{both}$ value and policy regularization. The corresponding Bellman operators enable developing policy iteration schemes with convergence and robustness guarantees. It also reduces planning and learning in robust MDPs to regularized MDPs.\n\n| | |\n| --- | --- |\n| Comments: | Accepted to NeurIPS 2021 |\n| Subjects: | Machine Learning (cs.LG); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:2110.06267](https://arxiv.org/abs/2110.06267) \\[cs.LG\\] |\n| (or [arXiv:2110.06267v1](https://arxiv.org/abs/2110.06267v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2110.06267](https://doi.org/10.48550/arXiv.2110.06267) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Esther Derman \\[ [view email](https://arxiv.org/show-email/bf2713e8/2110.06267)\\] **\\[v1\\]**\nTue, 12 Oct 2021 18:33:45 UTC (407 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Twice regularized MDPs and the equivalence between robustness and regularization, by Esther Derman and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2110.06267)\n- [TeX Source](https://arxiv.org/src/2110.06267)\n- [Other Formats](https://arxiv.org/format/2110.06267)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2110.06267&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2110.06267&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-10](https://arxiv.org/list/cs.LG/2021-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2110.06267?context=cs) [math](https://arxiv.org/abs/2110.06267?context=math) [math.OC](https://arxiv.org/abs/2110.06267?context=math.OC)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2110.06267)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2110.06267)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2110.06267)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2110.html#abs-2110-06267) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2110-06267)\n\n[Esther Derman](https://dblp.uni-trier.de/search/author?author=Esther%20Derman) [Matthieu Geist](https://dblp.uni-trier.de/search/author?author=Matthieu%20Geist) [Shie Mannor](https://dblp.uni-trier.de/search/author?author=Shie%20Mannor)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2110.06267) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Twice regularized MDPs and the equivalence between robustness and regularization",
          "cleaned_query": "Twice regularized MDPs and the equivalence between robustness and regularization",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems",
          "url": "http://bair.berkeley.edu/blog/2021/03/09/maxent-robust-rl/",
          "content": "# Maximum Entropy RL (Provably) Solves Some Robust RL Problems\n\nBen Eysenbach\nMar 9, 2021\n\nNearly all real-world applications of reinforcement learning involve some degree of shift between the training environment and the testing environment. However, prior work has observed that even small shifts in the environment cause most RL algorithms to perform [markedly](https://arxiv.org/abs/1703.06907) [worse](https://arxiv.org/abs/1610.01283).\nAs we aim to scale reinforcement learning algorithms and apply them in the real world, it is increasingly important to learn policies that are robust to changes in the environment.\n\n_**Robust reinforcement learning** maximizes reward on an adversarially-chosen environment._\n\nBroadly, prior approaches to handling distribution shift in RL aim to maximize performance in either the average case or the worst case. The first set of approaches, such as domain randomization, train a policy on a distribution of environments, and optimize the average performance of the policy on these environments. While these methods have been successfully applied to a number of areas\n(e.g., [self-driving cars](https://arxiv.org/abs/1804.09364), [robot locomotion](https://arxiv.org/abs/1804.10332) and [manipulation](https://arxiv.org/abs/1703.06907)),\ntheir success rests critically on the [design of the distribution of environments](https://arxiv.org/abs/1910.07113).\nMoreover, policies that do well on average are not guaranteed to get high reward on every environment. The policy that gets the highest reward on average might get very low reward on a small fraction of environments. The second set of approaches, typically referred to as **robust RL**, focus on the worst-case scenarios. The aim is to find a policy that gets high reward on every environment within some set. Robust RL can equivalently be viewed as a [two-player game](https://www.youtube.com/watch?v=xfyK03MEZ9Q&t=17093s) between the policy and an environment adversary. The policy tries to get high reward, while the environment adversary tries to tweak the dynamics and reward function of the environment so that the policy gets lower reward. One important property of the robust approach is that, unlike domain randomization, it is invariant to the ratio of easy and hard tasks. Whereas robust RL always evaluates a policy on the most challenging tasks, domain randomization will predict that the policy is better if it is evaluated on a distribution of environments with more easy tasks.\n\nPrior work has suggested a number of algorithms for solving robust RL problems. Generally, these algorithms all follow the same recipe: take an existing RL algorithm and add some additional machinery on top to make it robust.\nFor example, [robust value iteration](https://www.ri.cmu.edu/pub_files/pub3/bagnell_james_2001_1/bagnell_james_2001_1.pdf) uses Q-learning as the base RL algorithm, and modifies the Bellman update by solving a convex optimization problem in the inner loop of each Bellman backup.\nSimilarly, [Pinto \u201817](http://proceedings.mlr.press/v70/pinto17a/pinto17a.pdf) uses TRPO as the base RL algorithm and periodically updates the environment based on the behavior of the current policy. These prior approaches are often difficult to implement and, even once implemented correctly, they requiring tuning of many additional hyperparameters. Might there be a simpler approach, an approach that does not require additional hyperparameters and additional lines of code to debug?\n\nTo answer this question, we are going to focus on a type of RL algorithm known as maximum entropy RL, or **MaxEnt RL** for short ( [Todorov \u201806](https://proceedings.neurips.cc/paper/2006/file/d806ca13ca3449af72a1ea5aedbed26a-Paper.pdf), [Rawlik \u201808](http://www.roboticsproceedings.org/rss08/p45.pdf), [Ziebart \u201810](https://www.cs.uic.edu/pub/Ziebart/Publications/thesis-bziebart.pdf)).\nMaxEnt RL is a slight variant of standard RL that aims to learn a policy that gets high reward while acting as randomly as possible; formally, MaxEnt maximizes the entropy of the policy. Some [prior](https://arxiv.org/abs/1812.11103) [work](https://openreview.net/forum?id=r1xPh2VtPB) has observed empirically that MaxEnt RL algorithms appear to be robust to some disturbances the environment.\nTo the best of our knowledge, no prior work has actually proven that MaxEnt RL is robust to environmental disturbances.\n\nIn a [recent paper](https://arxiv.org/abs/2103.06257), we prove that every MaxEnt RL problem corresponds to maximizing a lower bound on a robust RL problem. Thus, when you run MaxEnt RL, you are implicitly solving a robust RL problem. Our analysis provides a theoretically-justified explanation for the empirical robustness of MaxEnt RL, and proves that _MaxEnt RL is itself a robust RL algorithm._\nIn the rest of this post, we\u2019ll provide some intuition into why MaxEnt RL should be robust and what sort of perturbations MaxEnt RL is robust to. We\u2019ll also show some experiments demonstrating the robustness of MaxEnt RL.\n\n# Intuition\n\nSo, why would we expect MaxEnt RL to be robust to disturbances in the environment? Recall that MaxEnt RL trains policies to not only maximize reward, but to do so while acting as randomly as possible. In essence, the policy itself is injecting as much noise as possible into the environment, so it gets to \u201cpractice\u201d recovering from disturbances. Thus, if the change in dynamics appears like just a disturbance in the original environment, our policy has already been trained on such data. Another way of viewing MaxEnt RL is as learning many different ways of solving the task ( [Kappen \u201805](https://www.cs.uic.edu/pub/Ziebart/Publications/thesis-bziebart.pdf)). For example, let\u2019s look at the task shown in videos below: we want the robot to push the white object to the green region. The top two videos show that standard RL always takes the shortest path to the goal, whereas MaxEnt RL takes many different paths to the goal. Now, let\u2019s imagine that we add a new obstacle (red blocks) that wasn\u2019t included during training. As shown in the videos in the bottom row, the policy learned by standard RL almost always collides with the obstacle, rarely reaching the goal. In contrast, the MaxEnt RL policy often chooses routes around the obstacle, continuing to reach the goal for a large fraction of trials.\n\n| | |\n| --- | --- |\n| Standard RL | MaxEnt RL |\n| Trained and evaluated without the obstacle: |\n| Trained without the obstacle, but evaluated with the obstacle: |\n\n# Theory\n\nWe now formally describe the technical results from the paper. The aim here is not to provide a full proof (see the paper Appendix for that), but instead to build some intuition for what the technical results say. Our main result is that, when you apply MaxEnt RL with some reward function and some dynamics, you are actually maximizing a lower bound on the robust RL objective. To explain this result, we must first define the MaxEnt RL objective:\n$J\\_{MaxEnt}(\\\\pi; p, r)$ is the entropy-regularized cumulative return of policy $\\\\pi$ when evaluated using dynamics $p(s\u2019 \\\\mid s, a)$ and reward function $r(s, a)$. While we will train the policy using one dynamics $p$, we will evaluate the policy on a different dynamics, $\\\\tilde{p}(s\u2019 \\\\mid s, a)$, chosen by the adversary. We can now formally state our main result as follows:\n\nThe left-hand-side is the robust RL objective. It says that the adversary gets\nto choose whichever dynamics function $\\\\tilde{p}(s\u2019 \\\\mid s, a)$ makes our policy perform as poorly as\npossible, subject to some constraints (as specified by the set $\\\\tilde{\\\\mathcal{P}}$). On\nthe right-hand-side we have the MaxEnt RL objective (note that $\\\\log T$ is a\nconstant, and the function $\\\\exp(\\\\cdots)$ is always increasing). Thus, this objective\nsays that a policy that has a high entropy-regularized reward (right hand-side)\nis guaranteed to also get high reward when evaluated on an adversarially-chosen\ndynamics.\n\nThe most important part of this equation is the set $\\\\tilde{\\\\mathcal{P}}$ of dynamics that\nthe adversary can choose from. Our analysis describes precisely how this set is\nconstructed and shows that, if we want a policy to be robust to a larger set of\ndisturbances, all we have to do is increase the weight on the entropy term and\ndecrease the weight on the reward term. Intuitively, the adversary must choose\ndynamics that are \u201cclose\u201d to the dynamics on which the policy was trained. For\nexample, in the special case where the dynamics are linear-Gaussian, this set\ncorresponds to all perturbations where the original expected next state and the\nperturbed expected next state have a Euclidean distance less than $\\\\epsilon$.\n\n# More Experiments\n\nOur analysis predicts that MaxEnt RL should be robust to many types of\ndisturbances. The first set of videos in this post showed that MaxEnt RL is robust to\nstatic obstacles. MaxEnt RL is also robust to dynamic perturbations introduced in the\nmiddle of an episode. To demonstrate this, we took the same robotic pushing task\nand knocked the puck out of place in the middle of the episode. The videos below\nshow that the policy learned by MaxEnt RL is more robust at handling these\nperturbations, as predicted by our analysis.\n\n| | |\n| --- | --- |\n| Standard RL | MaxEnt RL |\n\n_The policy learned by MaxEntRL is robust to dynamic perturbations of the puck (red frames)._\n\nOur theoretical results suggest that, even if we optimize the environment\nperturbations so the agent does as poorly as possible, MaxEnt RL policies will\nstill be robust. To demonstrate this capability, we trained both standard RL and\nMaxEnt RL on a peg insertion task shown below. During evaluation, we changed the\nposition of the hole to try to make each policy fail. If we only moved the hole\nposition a little bit ($\\\\le$ 1 cm), both policies always solved the task. However,\nif we moved the hole position up to 2cm, the policy learned by standard RL\nalmost never succeeded in inserting the peg, while the MaxEnt RL policy\nsucc",
          "original_query": "Maximum entropy RL (provably) solves some robust RL problems",
          "cleaned_query": "Maximum entropy RL (provably) solves some robust RL problems",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "State Entropy Maximization with Random Encoders for Efficient ...",
          "url": "https://arxiv.org/abs/2102.09430",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2102.09430** (cs)\n\n\\[Submitted on 18 Feb 2021 ( [v1](https://arxiv.org/abs/2102.09430v1)), last revised 21 Jun 2021 (this version, v4)\\]\n\n# Title:State Entropy Maximization with Random Encoders for Efficient Exploration\n\nAuthors: [Younggyo Seo](https://arxiv.org/search/cs?searchtype=author&query=Seo,+Y), [Lili Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+L), [Jinwoo Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin,+J), [Honglak Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+H), [Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&query=Abbeel,+P), [Kimin Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+K)\n\nView a PDF of the paper titled State Entropy Maximization with Random Encoders for Efficient Exploration, by Younggyo Seo and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2102.09430)\n\n> Abstract:Recent exploration methods have proven to be a recipe for improving sample-efficiency in deep reinforcement learning (RL). However, efficient exploration in high-dimensional observation spaces still remains a challenge. This paper presents Random Encoders for Efficient Exploration (RE3), an exploration method that utilizes state entropy as an intrinsic reward. In order to estimate state entropy in environments with high-dimensional observations, we utilize a k-nearest neighbor entropy estimator in the low-dimensional representation space of a convolutional encoder. In particular, we find that the state entropy can be estimated in a stable and compute-efficient manner by utilizing a randomly initialized encoder, which is fixed throughout training. Our experiments show that RE3 significantly improves the sample-efficiency of both model-free and model-based RL methods on locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3 allows learning diverse behaviors without extrinsic rewards, effectively improving sample-efficiency in downstream tasks. Source code and videos are available at [this https URL](https://sites.google.com/view/re3-rl).\n\n| | |\n| --- | --- |\n| Comments: | ICML 2021. First two authors contributed equally. Website: [this https URL](https://sites.google.com/view/re3-rl) Code: [this https URL](https://github.com/younggyoseo/RE3) |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2102.09430](https://arxiv.org/abs/2102.09430) \\[cs.LG\\] |\n| | (or [arXiv:2102.09430v4](https://arxiv.org/abs/2102.09430v4) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2102.09430](https://doi.org/10.48550/arXiv.2102.09430) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Younggyo Seo \\[ [view email](https://arxiv.org/show-email/be20dd08/2102.09430)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2102.09430v1)**\nThu, 18 Feb 2021 15:45:17 UTC (4,736 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2102.09430v2)**\nThu, 10 Jun 2021 14:01:37 UTC (8,757 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2102.09430v3)**\nFri, 11 Jun 2021 01:01:23 UTC (8,770 KB)\n\n**\\[v4\\]**\nMon, 21 Jun 2021 04:54:56 UTC (8,770 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled State Entropy Maximization with Random Encoders for Efficient Exploration, by Younggyo Seo and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2102.09430)\n- [TeX Source](https://arxiv.org/src/2102.09430)\n- [Other Formats](https://arxiv.org/format/2102.09430)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2102.09430&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2102.09430&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-02](https://arxiv.org/list/cs.LG/2021-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2102.09430?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2102.09430)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2102.09430)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2102.09430)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2102.html#abs-2102-09430) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2102-09430)\n\n[Lili Chen](https://dblp.uni-trier.de/search/author?author=Lili%20Chen)\n\n[Jinwoo Shin](https://dblp.uni-trier.de/search/author?author=Jinwoo%20Shin)\n\n[Honglak Lee](https://dblp.uni-trier.de/search/author?author=Honglak%20Lee)\n\n[Pieter Abbeel](https://dblp.uni-trier.de/search/author?author=Pieter%20Abbeel)\n\n[Kimin Lee](https://dblp.uni-trier.de/search/author?author=Kimin%20Lee)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2102.09430&description=State Entropy Maximization with Random Encoders for Efficient Exploration) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2102.09430&title=State Entropy Maximization with Random Encoders for Efficient Exploration)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2102.09430) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "State entropy maximization with random encoders for efficient exploration",
          "cleaned_query": "State entropy maximization with random encoders for efficient exploration",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[1812.02690] Provably Efficient Maximum Entropy Exploration - arXiv",
          "url": "https://arxiv.org/abs/1812.02690",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1812.02690** (cs)\n\n\\[Submitted on 6 Dec 2018 ( [v1](https://arxiv.org/abs/1812.02690v1)), last revised 26 Jan 2019 (this version, v2)\\]\n\n# Title:Provably Efficient Maximum Entropy Exploration\n\nAuthors: [Elad Hazan](https://arxiv.org/search/cs?searchtype=author&query=Hazan,+E), [Sham M. Kakade](https://arxiv.org/search/cs?searchtype=author&query=Kakade,+S+M), [Karan Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh,+K), [Abby Van Soest](https://arxiv.org/search/cs?searchtype=author&query=Van+Soest,+A)\n\nView a PDF of the paper titled Provably Efficient Maximum Entropy Exploration, by Elad Hazan and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/1812.02690)\n\n> Abstract:Suppose an agent is in a (possibly unknown) Markov Decision Process in the absence of a reward signal, what might we hope that an agent can efficiently learn to do? This work studies a broad class of objectives that are defined solely as functions of the state-visitation frequencies that are induced by how the agent behaves. For example, one natural, intrinsically defined, objective problem is for the agent to learn a policy which induces a distribution over state space that is as uniform as possible, which can be measured in an entropic sense. We provide an efficient algorithm to optimize such such intrinsically defined objectives, when given access to a black box planning oracle (which is robust to function approximation). Furthermore, when restricted to the tabular setting where we have sample based access to the MDP, our proposed algorithm is provably efficient, both in terms of its sample and computational complexities. Key to our algorithmic methodology is utilizing the conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an approximate MDP solver.\n\n| | |\n| --- | --- |\n| Comments: | Updated experiment results; minor revisions in writing |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1812.02690](https://arxiv.org/abs/1812.02690) \\[cs.LG\\] |\n| (or [arXiv:1812.02690v2](https://arxiv.org/abs/1812.02690v2) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1812.02690](https://doi.org/10.48550/arXiv.1812.02690) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Karan Singh \\[ [view email](https://arxiv.org/show-email/57268e59/1812.02690)\\] **[\\[v1\\]](https://arxiv.org/abs/1812.02690v1)**\nThu, 6 Dec 2018 18:15:44 UTC (890 KB)\n**\\[v2\\]**\nSat, 26 Jan 2019 01:54:32 UTC (1,145 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Provably Efficient Maximum Entropy Exploration, by Elad Hazan and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/1812.02690)\n- [TeX Source](https://arxiv.org/src/1812.02690)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1812.02690&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1812.02690&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2018-12](https://arxiv.org/list/cs.LG/2018-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1812.02690?context=cs) [cs.AI](https://arxiv.org/abs/1812.02690?context=cs.AI) [stat](https://arxiv.org/abs/1812.02690?context=stat) [stat.ML](https://arxiv.org/abs/1812.02690?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1812.02690)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1812.02690)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1812.02690)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1812.html#abs-1812-02690) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1812-02690)\n\n[Elad Hazan](https://dblp.uni-trier.de/search/author?author=Elad%20Hazan) [Sham M. Kakade](https://dblp.uni-trier.de/search/author?author=Sham%20M.%20Kakade) [Karan Singh](https://dblp.uni-trier.de/search/author?author=Karan%20Singh) [Abby Van Soest](https://dblp.uni-trier.de/search/author?author=Abby%20Van%20Soest)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1812.02690) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Provably efficient maximum entropy exploration",
          "cleaned_query": "Provably efficient maximum entropy exploration",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Best-Effort Policies for Robust Markov Decision Processes",
          "url": "https://arxiv.org/html/2508.07790v1",
          "content": "Best-Effort Policies for Robust Markov Decision Processes\n# Best-Effort Policies for Robust Markov Decision Processes\nAlessandro Abate1,\nThom Badings1,\nGiuseppe De Giacomo1,2,\nFrancesco Fabiano1\n###### Abstract\nWe study the common generalization of Markov decision processes (MDPs) with sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal in RMDPs is to compute a policy that maximizes the expected return under an adversarial choice of the transition probabilities. If the uncertainty in the probabilities is independent between the states, known asss-rectangularity, such optimal robust policies can be computed efficiently using robust value iteration. However, there might still be multiple optimal robust policies, which, while equivalent with respect to the worst-case, reflect different expected returns under non-adversarial choices of the transition probabilities. Hence, we propose a refined policy selection criterion for RMDPs, drawing inspiration from the notions of*dominance*and*best-effort*in game theory. Instead of seeking a policy that only maximizes the worst-case expected return, we additionally require the policy to achieve a*maximal*expected return under different (\\\\ienot fully adversarial) transition probabilities. We call such a policy an*optimal robust best-effort*(ORBE\\\\mathrm{ORBE}) policy. We prove thatORBE\\\\mathrm{ORBE}policies always exist, characterize their structure, and present an algorithm to compute them with a small overhead compared to standard robust value iteration.ORBE\\\\mathrm{ORBE}policies offer a principled tie-breaker among optimal robust policies. Numerical experiments show the feasibility of our approach.\n## 1Introduction\n*Markov decision processes*(MDPs) are the standard model for sequential decision making in stochastic environments and are ubiquitous in artificial intelligence (AI)> (Russell and Norvig [> 2010\n](https://arxiv.org/html/2508.07790v1#bib.bib30)> )\n, operations research> (Davis [> 2018\n](https://arxiv.org/html/2508.07790v1#bib.bib9)> )\n, control theory> (\u00c5str\u00f6m [> 2012\n](https://arxiv.org/html/2508.07790v1#bib.bib4)> )\n, and robotics> (Hanheide et\u00a0al. [> 2017\n](https://arxiv.org/html/2508.07790v1#bib.bib18)> )\n.\nWithin AI, MDPs are at the core of many model-based reinforcement learning methods> (Moerland et\u00a0al. [> 2023\n](https://arxiv.org/html/2508.07790v1#bib.bib24)> )\n.\nSolving an MDP amounts to computing a*policy*(or*strategy*) for the agent,\\\\iea mapping from states to actions, that maximizes a particular performance value, such as the expected (discounted) cumulative reward> (Puterman [> 1994\n](https://arxiv.org/html/2508.07790v1#bib.bib27)> )\n.\n#### Robust MDPs.\nA fundamental limitation of MDPs is the requirement to specify transition probabilities precisely.\nIn practice, accurately determining these probabilities can be challenging, especially if parameters are uncertain or if the model is learned from data> (Badings et\u00a0al. [> 2023b\n](https://arxiv.org/html/2508.07790v1#bib.bib6)> )\n.\nMoreover, optimal policies may be sensitive to small changes in the transition probabilities> (Mannor et\u00a0al. [> 2004\n](https://arxiv.org/html/2508.07790v1#bib.bib23)> )\n.\nTo address this issue,*robust MDPs*(RMDPs) generalize MDPs by allowing for*sets of transition probabilities*> (Iyengar [> 2005\n](https://arxiv.org/html/2508.07790v1#bib.bib20)> ; Nilim and Ghaoui [> 2005\n](https://arxiv.org/html/2508.07790v1#bib.bib26)> ; Wiesemann, Kuhn, and Rustem [> 2013\n](https://arxiv.org/html/2508.07790v1#bib.bib34)> )\n.\nThat is, instead of assigning precise probabilities between 0 and 1, the transitions in an RMDP are described by a set of feasible probabilities, called the*uncertainty set*of the RMDP.\nThe standard objective in an RMDP is to compute an*optimal robust policy*, defined as a policy that*maximizes*the expected return under the*minimizing*(\\\\ieworst-case) transition probabilities in the uncertainty set. Unfortunately, computing optimal robust policies under general uncertainty sets is NP-hard> (Wiesemann, Kuhn, and Rustem [> 2013\n](https://arxiv.org/html/2508.07790v1#bib.bib34)> )\n.\nTo ensure tractability, uncertainty sets are commonly assumed to be convex as well as independent between the states and/or actions of the RMDP, referred to as*rectangularity*of the uncertainty set.\nUnder these assumptions, optimal robust policies can be computed,\\\\egusing robust value iteration.\n#### The adversarial nature of RMDPs.\nWhen computing an optimal robust policy, the choice of transition probabilities is inherently adversarial.\nHowever, in many scenarios, the choice of transition probabilities is*not*actively working against the agent, making this assumption overly conservative.\nTake, for example, an autonomous drone flying through uncertain wind conditions.\nClearly, the wind conditions do not depend on the drone\u2019s control policy, so reasoning solely about the worst-case conditions might be too conservative.\nMoreover, multiple optimal robust policies may exist, even though their performance under non-adversarial conditions may differ.\nWe thus raise the vital question: can we compute a policy that is optimal in the worst case, but also \u201cis best\u201d when the environment does not act fully adversarially?\n#### Optimal robust best-effort policies.\nTo address the limitations of purely adversarial reasoning in RMDPs, we draw inspiration from advances in reactive stochastic games> (Aminof et\u00a0al. [> 2023\n](https://arxiv.org/html/2508.07790v1#bib.bib3)> ; Giacomo, Favorito, and Silo [> 2024\n](https://arxiv.org/html/2508.07790v1#bib.bib15)> )\n, where policies are evaluated based on their ability to succeed against sets of environment policies.\nIn this framework, a policy is deemed*winning*,*dominant*, or*best-effort*if it succeeds against*all*, the*maximum*subset, or a*maximal*subset of the environment policies, respectively.\nYet, these papers consider games where only the graph of the model is known and the probabilities are unconstrained, as opposed to RMDPs, where the uncertainty is captured by bounded sets of distributions.\nIn this paper, we leverage the concepts of dominance and best-effort to define a refined policy selection criterion for RMDPs, which we term*optimal robust best-effort*(ORBE\\\\mathrm{ORBE}).\nAnORBE\\\\mathrm{ORBE}policy satisfies two properties:(1)it achieves an optimal expected return under the*worst-case*transition probabilities in the uncertainty set; and(2)it is not dominated by any other policy,\\\\ieis best-effort.Here, one policy is said to dominate another if it performs at least as well across the entire uncertainty set and strictly better in at least one instance of the transition probabilities from the uncertainty set.\nThis best-effort perspective offers a principled tie-breaking criterion among optimal robust policies, favoring those that also achieve a maximal expected return under non-adversarial transition probabilities.\n#### Contributions and structure.\nWe introduce a novel class of policies for RMDPs, called*optimal robust best-effort*(ORBE\\\\mathrm{ORBE}) policies.\nThese policies combine the worst-case guarantees of standard robust policies with the refinement offered by best-effort reasoning, ensuring strong performance even when the environment is not fully adversarial.\nSpecifically, our key contributions are as follows:\n* \u2022We formalize the notions of dominant and best-effort policies within the context of RMDPs ([Sect.3](https://arxiv.org/html/2508.07790v1#S3)).\n* \u2022We provide a full characterization ofORBE\\\\mathrm{ORBE}policies and present an efficient algorithm to compute them with small overhead to standard robust value iteration ([Sect.5](https://arxiv.org/html/2508.07790v1#S5)).\n* \u2022We empirically demonstrate the feasibility of our techniques as a tie-breaker in robust value iteration ([Sect.6](https://arxiv.org/html/2508.07790v1#S6)).\nWe postpone a detailed discussion of related work to[Sect.7](https://arxiv.org/html/2508.07790v1#S7).\n## 2Preliminaries\nWe write\u27e8u,v\u27e9\u2254\u2211x\u2208Xu\u200b(x)\u200bv\u200b(x)\\\\langle u,v\\\\rangle\\\\coloneqq\\\\sum\\_{x\\\\in X}u(x)v(x)for the dot product between the functionsu,v:X\u2192\u211du,v\\\\colon X\\\\to\\\\mathbb{R}.\nThe cardinality of a setXXis written as|X||X|.\nA probability distribution over a setXXis a function\u03bc:X\u2192[0,1]\\\\mu\\\\colon X\\\\to[0,1]such that\u2211x\u2208X\u03bc\u200b(x)=1\\\\sum\\_{x\\\\in X}\\\\mu(x)=1.\nThe set of all probability distributions overXXis denoted by\u0394X\\\\Delta\\_{X}.\nThe relative interior of a convex setXXis defined asrelint\u200b(X)\u2254{x\u2208X:\u2200y\u2208X,\u2203\u03bb&gt;1.\u03bb\u200bx+(1\u2212\u03bb)\u200by\u2208X}\\\\mathrm{relint(X)}\\\\coloneqq\\\\{x\\\\in X:\\\\forall y\\\\in X,\\\\,\\\\exists\\\\lambda&gt;&gt;1.\\\\,\\\\,\\\\lambda x+(1-\\\\lambda)y\\\\in X\\\\}.\n### 2.1Markov Decision Processes\nWe consider Markov decision processes (MDPs) with discounted rewards, defined as follows> (Puterman [> 1994\n](https://arxiv.org/html/2508.07790v1#bib.bib27)> )\n.\n###### Definition 1(MDP).\nAn MDP is a tuple(S,sI,A,P,R,\u03b3)(S,s\\_{I},A,P,R,\\\\gamma), whereSSis a finite set of states,sI\u2208\u0394Ss\\_{I}\\\\in\\\\Delta\\_{S}is the initial distribution,AAis a finite set of actions,P:S\u00d7A\u2192\u0394SP\\\\colon S\\\\times A\\\\to\\\\Delta\\_{S}is the transition function,R:S\u00d7A\u2192\u211d\u22650R\\\\colon S\\\\times A\\\\to\\\\mathbb{R}\\_{\\\\geq 0}is a state-action reward function, and\u03b3\u2208(0,1)\\\\gamma\\\\in(0,1)is a discount factor.\nThe action choices in an MDP are resolved by a policy.\n###### Definition 2(Policy).\nA (stationary randomized) policy for the MDP\u2133\\\\mathcal{M}is a function\u03c0:S\u2192\u0394A\\\\pi\\\\colon S\\\\to\\\\Delta\\_{A}from states to distributions over actions.\nWe write\u03a0\\\\Pifor the set of all policies.\nFor convenience, we also write\u03c0\u200b(s)\u200b(a)\\\\pi(s)(a)as\u03c0\u200b(s,a)\\\\pi(s,a).\nLetP\u03c0:S\u2192\u0394SP^{\\\\pi}\\\\colon S\\\\to\\\\Delta\\_{S}andR\u03c0:S\u2192\u211d\u22650R^{\\\\pi}\\\\colon S\\\\to\\\\mathbb{R}\\_{\\\\geq 0}be the transition and reward function induced by the policy\u03c0\\\\pi, defined as\n|P\u03c0\u200b(s)\\\\displaystyle P^{\\\\pi}(s)|\u2254\u2211a\u2208A\u03c0\u200b(s,a)\u200bP\u200b(s,a)\u2208\u0394S,\\\\displaystyle\\\\coloneqq\\\\sum\\\\nolimits\\_{a\\\\in A}\\\\pi(s,a)P(s,a)\\\\in\\\\Delta\\_{S},||(1)|\n|R\u03c0\u200b(s)\\\\displaystyle R^{\\\\pi}(s)|\u2254\u2211a\u2208A\u03c0\u200b(s,a)\u200bR\u200b(s,a)\u2208\u211d\u22650.\\\\displaystyle\\\\coloneqq\\\\sum\\\\nolimits\\_{a\\\\in A}\\\\p",
          "original_query": "Robust Markov decision processes",
          "cleaned_query": "Robust Markov decision processes",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Learning Uncertainty Sets via Dual Regularization in Twice-Regularized MDPs\nUse the equivalence between robustness and regularization (R\u00b2 MDPs) to learn transition/reward uncertainty sets from data by parameterizing the regularizers and fitting them through a dual formulation. Concretely, propose a bilevel optimization where the inner loop solves a twice-regularized control problem and the outer loop adjusts regularizer parameters to match empirical calibration constraints (e.g., coverage of model errors on holdout rollouts).",
        "Frank\u2013Wolfe Planning Oracles for Robust and Twice-Regularized Bellman Operators\nAdapt the conditional gradient (Frank\u2013Wolfe) framework from provably efficient max-entropy exploration to solve the inner minimization in robust Bellman updates using lightweight linear optimization oracles. Provide an algorithmic instantiation that swaps expensive robust backups for iterative FW steps, and analyze convergence rates and computational savings under s-rectangular uncertainty.",
        "Distributionally-Weighted Best-Effort Robustness (Beyond Worst-Case vs Average-Case)\nGeneralize ORBE to a continuum between adversarial and nominal dynamics by introducing a risk profile over transition kernels (e.g., a mixture of worst-case and a prior). Propose an algorithm that computes policies optimizing worst-case return subject to maximizing a parameterized \u201ceffort\u201d functional over the transition-kernel distribution, yielding controllable conservatism and explicit robustness\u2013performance trade-off curves.",
        "Unified State-Visitation Objectives: Robust MaxEnt Exploration Under Model Misspecification\nCombine Hazan et al.\u2019s visitation-frequency objectives with robust MDP uncertainty sets to learn exploration policies whose induced state distribution remains high-entropy even under adversarial transition perturbations. Deliver a robust counterpart to maximum-entropy exploration with proofs (tabular) that the learned visitation distribution is near-uniform for all kernels in the uncertainty set, and test whether this produces better downstream transfer.",
        "Certification of MaxEnt-Implied Robustness for Specific Perturbation Classes\nBuild on the \u201cMaxEnt solves some robust RL problems\u201d result by identifying practically relevant perturbation sets (e.g., bounded Wasserstein shift in transition distributions, actuator noise models) for which MaxEnt objectives provide explicit lower bounds on robust return. Provide computable certificates (from learned Q-functions and entropy temperature) that predict robustness margins, and validate them empirically against adversarial environment generators.",
        "Robust Offline RL via Regularization\u2013Robustness Equivalence with ORBE Selection\nDevelop an offline RL algorithm that uses R\u00b2 regularization to implicitly solve a robust MDP under dataset-induced uncertainty (transition ambiguity from limited coverage), and then applies an ORBE-style tie-breaker to improve nominal performance. Make it actionable by defining the uncertainty set from confidence intervals or implicit behavioral model constraints, and evaluating on standard offline benchmarks under synthetic and real domain shifts."
      ],
      "generated_ideas_raw": "1. **ORBE-Soft RL: Best-Effort Tie-Breaking for Maximum-Entropy Robust Policies**  \n   Develop a soft (MaxEnt) robust RL objective that first optimizes worst-case return over an uncertainty set, then selects among worst-case-optimal policies using the ORBE \u201cbest-effort\u201d criterion (maximize expected return under a specified non-adversarial distribution over transitions). Implement a two-stage actor-critic where stage 1 uses robust Bellman backups and stage 2 adds a differentiable ORBE regularizer to resolve policy degeneracy without sacrificing worst-case guarantees.\n\n2. **Learning Uncertainty Sets via Dual Regularization in Twice-Regularized MDPs**  \n   Use the equivalence between robustness and regularization (R\u00b2 MDPs) to learn transition/reward uncertainty sets from data by parameterizing the regularizers and fitting them through a dual formulation. Concretely, propose a bilevel optimization where the inner loop solves a twice-regularized control problem and the outer loop adjusts regularizer parameters to match empirical calibration constraints (e.g., coverage of model errors on holdout rollouts).\n\n3. **RE3-Robust Exploration: Entropy Bonuses Calibrated to Transition Uncertainty**  \n   Extend RE3 by weighting its kNN state-entropy intrinsic reward with a learned estimate of local transition uncertainty, prioritizing exploration where robustness matters most. Evaluate on domain-shift benchmarks by measuring whether \u201cuncertainty-weighted RE3\u201d yields policies with improved worst-case performance compared to standard RE3 and standard MaxEnt RL.\n\n4. **Frank\u2013Wolfe Planning Oracles for Robust and Twice-Regularized Bellman Operators**  \n   Adapt the conditional gradient (Frank\u2013Wolfe) framework from provably efficient max-entropy exploration to solve the inner minimization in robust Bellman updates using lightweight linear optimization oracles. Provide an algorithmic instantiation that swaps expensive robust backups for iterative FW steps, and analyze convergence rates and computational savings under s-rectangular uncertainty.\n\n5. **Value-Dependent Regularization for Transition-Robust Deep RL Without Inner Loops**  \n   Paper 1 implies transition robustness induces a regularizer that depends on the value function; operationalize this in deep RL by learning a critic-dependent penalty term that approximates the robust backup. Implement as an additional term in the actor loss that uses critic gradients/features to approximate the value-dependent regularizer, aiming to match robust performance while avoiding per-update convex optimization.\n\n6. **Distributionally-Weighted Best-Effort Robustness (Beyond Worst-Case vs Average-Case)**  \n   Generalize ORBE to a continuum between adversarial and nominal dynamics by introducing a risk profile over transition kernels (e.g., a mixture of worst-case and a prior). Propose an algorithm that computes policies optimizing worst-case return subject to maximizing a parameterized \u201ceffort\u201d functional over the transition-kernel distribution, yielding controllable conservatism and explicit robustness\u2013performance trade-off curves.\n\n7. **Unified State-Visitation Objectives: Robust MaxEnt Exploration Under Model Misspecification**  \n   Combine Hazan et al.\u2019s visitation-frequency objectives with robust MDP uncertainty sets to learn exploration policies whose induced state distribution remains high-entropy even under adversarial transition perturbations. Deliver a robust counterpart to maximum-entropy exploration with proofs (tabular) that the learned visitation distribution is near-uniform for all kernels in the uncertainty set, and test whether this produces better downstream transfer.\n\n8. **Robust Skill Discovery with Random Encoders and Twice-Regularized Options**  \n   Use RE3\u2019s random-encoder entropy estimation to discover a library of diverse options, then train an R\u00b2-MDP policy over options that is robust to transition uncertainty. The contribution is an end-to-end pipeline: (i) unsupervised option learning via intrinsic state entropy, (ii) robust option selection via value+policy regularization, and (iii) evaluation on compositional tasks with dynamics shifts.\n\n9. **Certification of MaxEnt-Implied Robustness for Specific Perturbation Classes**  \n   Build on the \u201cMaxEnt solves some robust RL problems\u201d result by identifying practically relevant perturbation sets (e.g., bounded Wasserstein shift in transition distributions, actuator noise models) for which MaxEnt objectives provide explicit lower bounds on robust return. Provide computable certificates (from learned Q-functions and entropy temperature) that predict robustness margins, and validate them empirically against adversarial environment generators.\n\n10. **Robust Offline RL via Regularization\u2013Robustness Equivalence with ORBE Selection**  \n   Develop an offline RL algorithm that uses R\u00b2 regularization to implicitly solve a robust MDP under dataset-induced uncertainty (transition ambiguity from limited coverage), and then applies an ORBE-style tie-breaker to improve nominal performance. Make it actionable by defining the uncertainty set from confidence intervals or implicit behavioral model constraints, and evaluating on standard offline benchmarks under synthetic and real domain shifts.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Learning Uncertainty Sets via Dual Regularization in Twice-Regularized MDPs\nUse the equivalence between robustness and regularization (R\u00b2 MDPs) to learn transition/reward uncertainty sets from data by",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Frank\u2013Wolfe Planning Oracles for Robust and Twice-Regularized Bellman Operators\nAdapt the conditional gradient (Frank\u2013Wolfe) framework from provably efficient max-entropy exploration to solve the inne",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Distributionally-Weighted Best-Effort Robustness (Beyond Worst-Case vs Average-Case)\nGeneralize ORBE to a continuum between adversarial and nominal dynamics by introducing a risk profile over transiti",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Unified State-Visitation Objectives: Robust MaxEnt Exploration Under Model Misspecification\nCombine Hazan et al.\u2019s visitation-frequency objectives with robust MDP uncertainty sets to learn exploration",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Certification of MaxEnt-Implied Robustness for Specific Perturbation Classes\nBuild on the \u201cMaxEnt solves some robust RL problems\u201d result by identifying practically relevant perturbation sets (e.g., bo",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Robust Offline RL via Regularization\u2013Robustness Equivalence with ORBE Selection\nDevelop an offline RL algorithm that uses R\u00b2 regularization to implicitly solve a robust MDP under dataset-induced uncer",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 8,
      "paper_title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity",
      "contribution": "Shows that the stochasticity of conditional targets is not the primary driver of generalization in flow matching: closed-form velocity targets match (and sometimes improve) performance, and generalization instead arises from the neural network's failure to perfectly approximate the optimal closed-form velocity field in particular time intervals.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9837,
      "output_tokens": 1013,
      "predecessor_details": [
        {
          "success": true,
          "title": "Flow Matching for Generative Modeling - OpenReview",
          "url": "https://openreview.net/forum?id=PqvMRDCJT9t",
          "content": "Flow Matching for Generative Modeling | OpenReview\n[![back arrow](https://openreview.net/images/arrow_left.svg)Go to**ICLR 2023 Conference**homepage](https://openreview.net/group?id=ICLR.cc/2023/Conference)\n## Flow Matching for Generative Modeling[![Download PDF](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=PqvMRDCJT9t)\n### [Yaron Lipman](https://openreview.net/profile?id=~Yaron_Lipman1),[Ricky T. Q. Chen](https://openreview.net/profile?id=~Ricky_T._Q._Chen1),[Heli Ben-Hamu](https://openreview.net/profile?id=~Heli_Ben-Hamu1),[Maximilian Nickel](https://openreview.net/profile?id=~Maximilian_Nickel1),[Matthew Le](https://openreview.net/profile?id=~Matthew_Le2)\nPublished: 01 Feb 2023, Last Modified: 12 Oct 2025ICLR 2023 notable top 25%Readers:Everyone\n**Keywords:**continuous normalizing flows, generative models\n**Abstract:**We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples---which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.\n**Anonymous Url:**I certify that there is no URL (e.g., github page) that could be used to find authors\u2019 identity.\n**No Acknowledgement Section:**I certify that there is no acknowledgement section in this submission for double blind review.\n**Code Of Ethics:**I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics\n**Submission Guidelines:**Yes\n**Please Choose The Closest Area That Your Submission Falls Into:**Generative models\n**TL;DR:**We introduce a new simulation-free approach for training Continuous Normalizing Flows, generalizing the probability paths induced by simple diffusion processes. We obtain state-of-the-art on ImageNet in both NLL and FID among competing methods.\n**Community Implementations:**[![CatalyzeX](/images/catalyzex\\_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/flow-matching-for-generative-modeling/code)\n11 Replies\nLoading\n[OpenReview](https://openreview.net/about)is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the[OpenReview Sponsors](https://openreview.net/sponsors). \u00a92025OpenReview",
          "original_query": "Flow Matching for Generative Modeling (Lipman et al., 2023)",
          "cleaned_query": "Flow Matching for Generative Modeling",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Score-Based Generative Modeling",
          "url": "https://www.emergentmind.com/topics/score-based-generative-modeling-5333e17e-4790-4610-b5f4-004fcb798da5",
          "content": "Papers\n\nTopics\n\nAuthors\n\nRecent\n\n[View all](https://www.emergentmind.com/history)\n\nAssistant\nAI Research Assistant\n\nAI Research Assistant\n\nWell-researched responses based on relevant abstracts and paper content.\n\nCustom Instructions\nPro\n\nPreferences or requirements that you'd like Emergent Mind to consider when generating responses.\n\nGemini 2.5 Flash\n\nGemini 2.5 Flash\n172 tok/s\n\nGemini 2.5 Pro\n50 tok/s\nPro\n\nGPT-5 Medium\n29 tok/s\nPro\n\nGPT-5 High\n27 tok/s\nPro\n\nGPT-4o\n94 tok/s\nPro\n\nKimi K2\n194 tok/s\nPro\n\nGPT OSS 120B\n451 tok/s\nPro\n\nClaude Sonnet 4.5\n34 tok/s\nPro\n\nSign Up & Search\n\n2000 character limit reached\n\n# Score-Based Generative Modeling\n\nUpdated 3 October 2025\n\n- Score-based generative modeling is a framework that transforms noise into data through stochastic differential equations guided by a time-dependent score function.\n- It leverages neural networks trained via denoising score matching to approximate the score function, enabling effective reverse SDE sampling with a predictor-corrector scheme.\n- The approach also allows deterministic sample generation and exact likelihood evaluation via a probability flow ODE, achieving state-of-the-art results in image synthesis and inverse problems.\n\nScore-based generative modeling defines a unifying framework for constructing deep generative models by formulating sample synthesis as the numerical solution of a stochastic differential equation (SDE) whose drift depends on a time-dependent score function (the gradient of the log-density of the evolving data distribution). The central mechanism connects ideas from diffusion processes, Markov processes, and energy-based models, and provides a mathematically transparent route for transforming noise into data. This approach accommodates a variety of architectures, SDE designs, and sampling routines, and is distinguished by its flexibility, likelihood evaluation capabilities, and state-of-the-art empirical results in image synthesis and other domains.\n\n## 1\\. Stochastic Differential Equation Framework\n\nThe core generative modeling process begins by defining a forward SDE that progressively injects noise into data samples, transforming a complex data distribution p0(x)p\\_0(x)p0\u200b(x) into a tractable prior pT(x)p\\_T(x)pT\u200b(x) (frequently a standard multivariate Gaussian). The forward trajectory is given by the It\u00f4 SDE: dx=f(x,t)dt+g(t)dw,dx = f(x, t)dt + g(t)dw,dx=f(x,t)dt+g(t)dw,\nwhere f(x,t)f(x, t)f(x,t) is a drift function, g(t)g(t)g(t) a time-dependent diffusion coefficient, and www denotes Brownian motion. For large ttt, pt(x)p\\_t(x)pt\u200b(x) approaches pT(x)\u2248N(0,I)p\\_T(x) \\\\approx \\\\mathcal{N}(0, I)pT\u200b(x)\u2248N(0,I).\n\nThe reverse generation process follows a reverse-time SDE, which, given knowledge of the time-dependent score \u2207xlog\u2061pt(x)\\\\nabla\\_x \\\\log p\\_t(x)\u2207x\u200blogpt\u200b(x), takes the form: dx=\\[f(x,t)\u2212g(t)2\u2207xlog\u2061pt(x)\\]dt+g(t)dw\u02c9,dx = \\[f(x, t) - g(t)^2 \\\\nabla\\_x \\\\log p\\_t(x)\\]dt + g(t)d\\\\bar{w},dx=\\[f(x,t)\u2212g(t)2\u2207x\u200blogpt\u200b(x)\\]dt+g(t)dw\u02c9,\nwhere dw\u02c9d\\\\bar{w}dw\u02c9 is the time-reversed Wiener process. Hence, sample generation entails simulating this SDE from the simple prior pTp\\_TpT\u200b back to p0p\\_0p0\u200b, removing noise while being guided by the score (\u201cgradient flow\u201d) of the evolving distribution.\n\n## 2\\. Score Function Estimation via Neural Networks\n\nThe intractability of the true score function for complex pt(x)p\\_t(x)pt\u200b(x) necessitates learning an approximation s\u03b8(x,t)s\\_\\\\theta(x, t)s\u03b8\u200b(x,t) using a neural network. Training is performed using denoising score matching\u2014an instance of the generalized score-matching paradigm\u2014by minimizing the objective: min\u2061\u03b8Et{\u03bb(t)Ex0\u223cp0,x(t)\u223cp0t\\[\u2225s\u03b8(x(t),t)\u2212\u2207xlog\u2061p0t(x(t)\u2223x0)\u22252\\]}.\\\\min\\_\\\\theta \\\\mathbb{E}\\_t \\\\left\\\\{ \\\\lambda(t) \\\\mathbb{E}\\_{x\\_0 \\\\sim p\\_0,\\\\, x(t) \\\\sim p\\_{0t}} \\\\big\\[ \\\\\\|s\\_\\\\theta(x(t), t) - \\\\nabla\\_x \\\\log p\\_{0t}(x(t) \\\\mid x\\_0)\\\\\\|^2 \\\\big\\] \\\\right\\\\}.\u03b8min\u200bEt\u200b{\u03bb(t)Ex0\u200b\u223cp0\u200b,x(t)\u223cp0t\u200b\u200b\\[\u2225s\u03b8\u200b(x(t),t)\u2212\u2207x\u200blogp0t\u200b(x(t)\u2223x0\u200b)\u22252\\]}.\nHere, p0t(x(t)\u2223x0)p\\_{0t}(x(t)\\|x\\_0)p0t\u200b(x(t)\u2223x0\u200b) is the analytic perturbation kernel via the diffusion SDE, and \u03bb(t)\\\\lambda(t)\u03bb(t) is a weighting function for time-resampling.\n\nOnce s\u03b8(x,t)s\\_\\\\theta(x, t)s\u03b8\u200b(x,t) is trained, it replaces the unknown \u2207xlog\u2061pt(x)\\\\nabla\\_x \\\\log p\\_t(x)\u2207x\u200blogpt\u200b(x) in the reverse SDE, and generating data reduces to numerically integrating this SDE. Standard solvers such as Euler\u2013Maruyama or higher-order methods (e.g., Runge\u2013Kutta) may be adopted, subject to stability and efficiency constraints.\n\n## 3\\. Predictor\u2013Corrector Sampling Paradigm\n\nTo mitigate discretization artifacts and improve sample quality, the predictor\u2013corrector (PC) framework alternates between two procedures at each timestep:\n\n- **Predictor step:** Advances a sample using a deterministic discretization of the reverse SDE.\n- **Corrector step:** Executes score-based [MCMC](https://www.emergentmind.com/topics/monte-carlo-markov-chain-mcmc-analysis) (notably annealed Langevin dynamics) to move the sample closer to high-density regions, using\n\nx\u2190x+\u03f5s\u03b8(x,t)+2\u03f5z,x \\\\leftarrow x + \\\\epsilon\\\\, s\\_\\\\theta(x, t) + \\\\sqrt{2\\\\epsilon}\\\\,z,x\u2190x+\u03f5s\u03b8\u200b(x,t)+2\u03f5\u200bz,\n\nwith z\u223cN(0,I)z \\\\sim \\\\mathcal{N}(0, I)z\u223cN(0,I).\n\nThis combination effectively fuses deterministic approximation with stochastic exploration, leading to measurable improvements in sample fidelity (e.g., lower FID) at fixed computational cost.\n\n## 4\\. Probability Flow ODE and Likelihood Computation\n\nThe reverse SDE admits a deterministic counterpart via the probability flow ODE: dx=\\[f(x,t)\u221212g(t)2\u2207xlog\u2061pt(x)\\]dt.dx = \\\\left\\[f(x, t) - \\\\frac{1}{2}g(t)^2 \\\\nabla\\_x \\\\log p\\_t(x)\\\\right\\] dt.dx=\\[f(x,t)\u221221\u200bg(t)2\u2207x\u200blogpt\u200b(x)\\]dt.\nThis ODE, when integrating s\u03b8(x,t)s\\_\\\\theta(x, t)s\u03b8\u200b(x,t) for the score, allows deterministic generation of samples and, critically, enables exact likelihood evaluation using the change-of-variables formula: log\u2061p0(x0)=log\u2061pT(xT)+\u222b0T\u2207\u22c5\\[f(x(t),t)\u221212g(t)2s\u03b8(x(t),t)\\]dt.\\\\log p\\_0(x\\_0) = \\\\log p\\_T(x\\_T) + \\\\int\\_0^T \\\\nabla \\\\cdot \\\\left\\[ f(x(t), t) - \\\\frac{1}{2}g(t)^2 s\\_\\\\theta(x(t), t) \\\\right\\] dt.logp0\u200b(x0\u200b)=logpT\u200b(xT\u200b)+\u222b0T\u200b\u2207\u22c5\\[f(x(t),t)\u221221\u200bg(t)2s\u03b8\u200b(x(t),t)\\]dt.\nLikelihood computation is efficient via trace estimators (such as the Skilling\u2013Hutchinson estimator) and distinguishes the framework from standard diffusion or GAN-based models, which either lack tractable likelihoods or require restrictive invertibility constraints.\n\n## 5\\. Applications and Empirical Performance\n\nScore-based generative modeling has been adapted to a variety of practical settings:\n\n- **Class-conditional image generation**: Conditioning the diffusion process on labels (by integrating a time-dependent classifier on noisy data) enables precise targeted synthesis.\n- **Inverse problems**: Extensions enable recovery in image inpainting, colorization, and other ill-posed reconstructions, by modifying the reverse SDE or ODE to respect observed data constraints.\n- **Architectural advances**: Employing modern network designs\u2014residual blocks, progressive growing, skip connections\u2014further empirically improves performance.\n- **Metric results**: On CIFAR-10, Inception score (IS) of 9.89 and FID of 2.20 are reported, with likelihoods of 2.99 bits/dim on dequantized data, establishing state-of-the-art sample quality and log-likelihood.\n\n## 6\\. Mathematical Structure and Generalization\n\nThe framework can be summarized by key mathematical components:\n\n| Component | Formulation | Purpose |\n| --- | --- | --- |\n| Forward SDE | dx=f(x,t)dt+g(t)dwdx = f(x, t)dt + g(t)dwdx=f(x,t)dt+g(t)dw | Diffuse data to tractable prior |\n| Reverse SDE | dx=\\[f(x,t)\u2212g(t)2\u2207xlog\u2061pt(x)\\]dt+g(t)dw\u02c9dx = \\[f(x, t) - g(t)^2 \\\\nabla\\_x \\\\log p\\_t(x)\\]dt + g(t)d\\\\bar{w}dx=\\[f(x,t)\u2212g(t)2\u2207x\u200blogpt\u200b(x)\\]dt+g(t)dw\u02c9 | Denoise/reconstruct data from noise |\n| Training Objective (Score Net) | min\u2061\u03b8Et{\u2026}\\\\min\\_\\\\theta \\\\mathbb{E}\\_t \\\\{\\\\dots\\\\}min\u03b8\u200bEt\u200b{\u2026} (see above) | Learn time-dependent score |\n| Probability Flow ODE | dx=\\[f(x,t)\u2212(1/2)g(t)2\u2207xlog\u2061pt(x)\\]dtdx = \\[f(x, t) - (1/2)g(t)^2\\\\nabla\\_x \\\\log p\\_t(x)\\]dtdx=\\[f(x,t)\u2212(1/2)g(t)2\u2207x\u200blogpt\u200b(x)\\]dt | Deterministic mapping, enables exact likelihood |\n| Likelihood Formula | see above\u2217^{\\\\ast}\u2217 | Evaluate log\u2061p0(x0)\\\\log p\\_0(x\\_0)logp0\u200b(x0\u200b) for samples |\n\n\u2217^{\\\\ast}\u2217log\u2061p0(x0)=log\u2061pT(xT)+\u222b0T\u2207\u22c5\\[\u22c5\\]dt\\\\log p\\_0(x\\_0) = \\\\log p\\_T(x\\_T) + \\\\int\\_0^T \\\\nabla \\\\cdot \\[\\\\,\\\\cdot\\\\,\\] dtlogp0\u200b(x0\u200b)=logpT\u200b(xT\u200b)+\u222b0T\u200b\u2207\u22c5\\[\u22c5\\]dt\n\n## 7\\. Unification, Extensions, and Implications\n\nThis framework generalizes and subsumes prior approaches, including denoising score matching (Vincent, 2011), noise conditional score networks, and [diffusion probabilistic models](https://www.emergentmind.com/topics/diffusion-probabilistic-models) (DDPM, SMLD). The central insight is the continuous transformation of densities in probability space via score-driven flows; the time-dependent neural network instantiation for score estimation enables learning flexible, high-dimensional data distributions.\n\nThe predictor-corrector structure and probability flow ODE can be extended to alternative SDEs/ODEs and hybrid Sampler-ODE routines. The approach naturally handles high-resolution and high-dimensional data, supports flexible conditioning, and is applicable to domains such as image, audio synthesis, and inverse problems.\n\n## References\n\n- \"Score-Based Generative Modeling through Stochastic Differential Equations\" ( [Song et al., 2020](https://www.emergentmind.com/papers/2011.13456))\n- For specific performance metrics and methods details, see ( [Song et al., 2020](https://www.emergentmind.com/papers/2011.13456)).\n\nThis amalgamation of diffusion processes, score learning, and SDE/ODE-based sampling sets a foundation for current and emerging lines of research in generative modeling and its theoretical guarantees.\n\n[File Document Download Save Streamline Icon: https://streamlinehq.com\\\nPDF](https://www.emergentmind.com/users/sign_up?redirect_to=https%3A%2F%2Fwww.emergentmind.com%2Farticles%2Fscore-based-generative-modeling-5333e17e-4790-4",
          "original_query": "Score-Based Generative Modeling / SDE View (Song et al., 2021)",
          "cleaned_query": "Score-Based Generative Modeling",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Foundational Leadership and Organizational Wellness (FLOW) Model",
          "url": "https://pubmed.ncbi.nlm.nih.gov/38742584/",
          "content": "\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n. 2024 Jun;2024(182):93-106. \n \n \n \n \n \n doi: 10.1002/yd.20605.\n \n \n \n \n \n Epub 2024 May 14.\n \n \n \n \n \n \n \n \n \n \n \n \n Affiliations\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n PMID:\n \n \n \n 38742584 \n \n \n \n \n \n \n \n \n \n DOI:\n \n \n \n \n 10.1002/yd.20605\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Foundational Leadership and Organizational Wellness (FLOW) Model: Designing leadership learning for individuals and organizations\n \n \n \n \n \n \n \n \n Freddy Juarez \u00a0et al. \n \n \n \n \n \n \n New Dir Stud Leadersh. \n \n \n \n 2024 Jun. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Abstract\n \n \n \n \n \n \n \n This article shares the foundational leadership and organizational wellness (FLOW) model, which is a leadership development model that seeks to better understand the relationship between individual leadership development and organizational development and wellness. The model is presented as a whole, followed by deep exploration by each piece of the model undergirded in existing scholarship and practical discussion throughout. Implications for practice and future research are shared to conclude the article.\n \n \n \n \n \n \n \n \n \n \n \n \n \u00a9 2024 Wiley Periodicals, LLC.\n \n \n PubMed Disclaimer \n \n \n \n \n \n \n \n \n \n Similar articles\n \n \n \n \n \n \n \n \n \n \n \n \n \n Students encouraging other students' learning: Leadership shared metacognition in practice.\n \n Hassell-Goodman S, Yamanaka A, Athanasiou J, Arminio J. \n \n \n \n Hassell-Goodman S, et al. \n New Dir Stud Leadersh. 2024 Sep;2024(183):131-143. doi: 10.1002/yd.20634. Epub 2024 Sep 1. \n New Dir Stud Leadersh. 2024. \n \n \n PMID: 39217627 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Guided by a \"Gentle Luminary\": The role of others in women's leadership efficacy development.\n \n Devies B. \n \n \n \n Devies B. \n New Dir Stud Leadersh. 2024 Jun;2024(182):47-58. doi: 10.1002/yd.20601. Epub 2024 May 14. \n New Dir Stud Leadersh. 2024. \n \n \n PMID: 38742586 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Leadership development research and scholarship.\n \n Pontes M, Weng J. \n \n \n \n Pontes M, et al. \n New Dir Stud Leadersh. 2024 Sep;2024(183):43-49. doi: 10.1002/yd.20622. Epub 2024 Aug 20. \n New Dir Stud Leadersh. 2024. \n \n \n PMID: 39163579 \n \n \n \n \n Review. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n (Re)designing leadership engagement to center learners.\n \n Allen JL Jr, Juarez F. \n \n \n \n Allen JL Jr, et al. \n New Dir Stud Leadersh. 2024 Sep;2024(183):111-120. doi: 10.1002/yd.20632. Epub 2024 Sep 5. \n New Dir Stud Leadersh. 2024. \n \n \n PMID: 39235382 \n \n \n \n \n Review. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n References\n \n \n \n \n REFERENCES \n \n \n \n \n \n \n \n \n \n \n \n Acaroglu, L. (2017, September 7). Tools for systems thinkers: The 6 fundamental concepts of systems thinking. Medium. https://medium.com/disruptive\u2010design/tools\u2010for\u2010systems\u2010thinkers\u2010the\u20106\u2010fu... \n \n \n \n \n \n \n \n \n \n \n \n Bandura, A. (1997). Self\u2010efficacy: The exercise of control. Harper Collins.\n \n \n \n \n \n \n \n \n \n \n \n Beatty, C. C., &amp; Guthrie, K. L. (2021). Operationalizing the culturally relevant leadership learning. Information Age Publishing.\n \n \n \n \n \n \n \n \n \n \n \n Bertrand Jones, T., Guthrie, K. L., &amp; Osteen, L. (2016). Critical domains of culturally relevant leadership learning: A call to transform leadership programs. New Directions for Student Leadership, 2016(152), 9\u201321. https://doi.org/10.1002/yd.20205 \n \n \n \n \n \n \n \n \n \n \n \n Biddix, J. P. (2010). Fraternities and sororities support leadership development! How do we know? Journal of Sorority and Fraternity Life Research and Practice, 5(2), v\u2013ix. https://doi.org/10.25774/mjr2\u2010pt34 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MeSH terms\n \n \n \n \n \n \n \n \n \n \n \n \n \n LinkOut - more resources\n \n Full Text Sources \n Wiley\n \n \n \n \n \n \n \n",
          "original_query": "Foundational flow/transport theory for conditional velocity fields (Albergo & Vanden-Eijnden, 2023)",
          "cleaned_query": "Foundational flow",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "On Memorization in Diffusion Models",
          "url": "https://smcnus.comp.nus.edu.sg/archive/pdf/2025/2025_on_memorization.pdf",
          "content": "Preprint\nON MEMORIZATION IN DIFFUSION MODELS\nXiangming Gu\u22171, Chao Du\u20202, Tianyu Pang\u20202, Chongxuan Li3, Min Lin2, Ye Wang\u20201\n1School of Computing, National University of Singapore\n2Sea AI Lab, Singapore\n3Gaoling School of Artificial Intelligence, Renmin University of China\nxiangming@u.nus.edu; {duchao,tianyupang,linmin}@sea.com;\nchongxuanli@ruc.edu.cn; wangye@comp.nus.edu.sg\nABSTRACT\nDue to their capacity to generate novel and high-quality samples, diffusion models\nhave attracted significant research interest in recent years. Notably, the typical train\u0002ing objective of diffusion models, i.e., denoising score matching, has a closed-form\noptimal solution that can only generate training data replicating samples. This indi\u0002cates that a memorization behavior is theoretically expected, which contradicts the\ncommon generalization ability of state-of-the-art diffusion models, and thus calls\nfor a deeper understanding. Looking into this, we first observe that memorization\nbehaviors tend to occur on smaller-sized datasets, which motivates our definition\nof effective model memorization (EMM), a metric measuring the maximum size\nof training data at which a learned diffusion model approximates its theoretical\noptimum. Then, we quantify the impact of the influential factors on these memo\u0002rization behaviors in terms of EMM, focusing primarily on data distribution, model\nconfiguration, and training procedure. Besides comprehensive empirical results\nidentifying the influential factors, we surprisingly find that conditioning training\ndata on uninformative random labels can significantly trigger the memorization\nin diffusion models. Our study holds practical significance for diffusion model\nusers and offers clues to theoretical research in deep generative models. Code is\navailable at https://github.com/sail-sg/DiffMemorize.\n1 INTRODUCTION\nIn the last few years, diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al.,\n2020; Song et al., 2021) have achieved significant success across diverse domains of generative\nmodeling, including image generation (Dhariwal & Nichol, 2021; Karras et al., 2022), text-to\u0002image synthesis (Rombach et al., 2022; Ramesh et al., 2022), audio/speech synthesis (Kim et al.,\n2022; Huang et al., 2023), graph generation (Xu et al., 2022; Vignac et al., 2022), and 3D content\ngeneration (Poole et al., 2023; Lin et al., 2023). Substantial empirical evidence attests to the ability\nof diffusion models to generate diverse and novel high-quality samples (Dhariwal & Nichol, 2021;\nNichol & Dhariwal, 2021; Nichol et al., 2021), underscoring their powerful capability of abstracting\nand comprehending the characteristics of the training data.\nDiffusion models posit a forward diffusion process {zt}t\u2208[0,T]that gradually introduces Gaussian\nnoise to a data point x, resulting in a transition distribution qt(zt|x) = N (zt|\u03b1tx, \u03c32\nt\nI). The coef\u0002ficients \u03b1t and \u03c3t are chosen such that the initial distribution q0(z0) aligns with the data distribution\nP(x) while steering it towards an approximately Gaussian distribution qT (zT ). Sampling from the\ndata distribution P can then be achieved by reversing this process, for which a critical unknown term is\nthe data score \u2207ztlog qt(zt) (Song et al., 2021). Diffusion models approximate the data scores with a\nscore model s\u03b8(zt, t), which is typically learned via denoising score matching (DSM) (Vincent, 2011):\nJDSM(\u03b8) \u225c\n1\n2N\nX\nN\nn=1\nEt,\u03f5\u223cN(0,I) \u2225s\u03b8(\u03b1txn + \u03c3t\u03f5, t) + \u03f5/\u03c3t\u2225\n2\n2\n, (1)\n\u2217Work done during an internship at Sea AI Lab. \u2020Corresponding authors.\n1\narXiv:2310.02664v1 [cs.LG] 4 Oct 2023\nPreprint\n(a) (b)\n0.8k 1.6k 2.4k 3.2k 4.0k\nTraining Epochs\n0\n2\n4\n6\n8\n10\n12\n14\n16\nRatio (%) of Memorization\n| | = 2k\n| | = 5k\n| | = 10k\n| | = 20k\n| | = 50k\n(c)\n8k 16k 24k 32k 40k\nTraining Epochs\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nRatio (%) of Memorization\n| | = 1k\n| | = 2k\n| | = 3k\n| | = 4k\n| | = 5k\n(d)\nFigure 1: The overall motivation for this work. The top two figures illustrate the generated CIFAR-10\nimages (first row) and their \u21132-nearest training samples in dataset D (second row) by two distinct\nmodels: (a) the theoretical optimum defined in Eq. (2), and (b) EDM (Karras et al., 2022). A clear\ngap is observed as EDM generates novel samples, while the optimum does not. The bottom two\nfigures show that (c) reducing the dataset size |D| and (d) extending the number of training epochs\ntrigger memorization behavior in EDM.\ngiven a dataset of N training samples D \u225c {xn|xn \u223c P(x)}\nN\nn=1. Interestingly, it is not difficult to\nidentify the optimal solution of Eq. (1) (assuming sufficient capacity of \u03b8, see proof in Appendix A.1):\ns\n\u2217\n\u03b8\n(zt, t) = X\nN\nn\u2032=1\nexp \u0010\u2212\n\u2225\u03b1txn\u2032 \u2212 zt\u2225\n2\n2\n2\u03c3\n2\nt\n\u0011\n!\u22121\n\u00b7\nX\nN\nn=1\nexp \u2212\n\u2225\u03b1txn \u2212 zt\u2225\n2\n2\n2\u03c3\n2\nt\n!\n\u03b1txn \u2212 zt\n\u03c3\n2\nt\n, (2)\nwhich, however, leads the reverse process towards the empirical data distribution, defined as Pb(x) =\n1\nN\nPN\nn=1 \u03b4(x \u2212 xn). Consequently, the optimal score model in Eq. (2) can only produce samples\nthat replicate the training data, as shown in Fig. 1a, suggesting a memorization behavior (van den\nBurg & Williams, 2021).1 This evidently contradicts the typical generalization capability exhibited\nby state-of-the-art diffusion models such as EDM (Karras et al., 2022), as illustrated in Fig. 1b.\nSuch intriguing gap prompts inquiries into (i) the conditions under which the learned diffusion\nmodels can faithfully approximate the optimum s\n\u2217\n\u03b8\n(essentially showing memorization) and (ii) the\ninfluential factors governing memorization behaviors in diffusion models. Besides a clear issue of\npotential adverse generalization performance (Yoon et al., 2023), it further raises a crucial concern\nthat diffusion models trained with Eq. (1) might imperceptibly memorize the training data, exposing\nseveral risks such as privacy leakage (Somepalli et al., 2023b) and copyright infringement (Somepalli\net al., 2023a; Zhao et al., 2023). For example, Carlini et al. (2023) show that it is possible to extract a\nfew training images from Stable Diffusion (Rombach et al., 2022), substantiating a tangible hazard.\nIn response to these inquiries and concerns, this paper presents a comprehensive empirical study on\nmemorization behavior in diffusion models. We start with an analysis of EDM (Karras et al., 2022) on\nCIFAR-10, noting that memorization tends to occur when trained on smaller-sized datasets, while re\u0002maining undetectable on larger datasets. This motivates our definition of effective model memorization\n(EMM), a metric quantifying the maximum number of training data points (sampled from distribution\nP) at which a diffusion model M demonstrates the similar memorization behavior as the theoretical\noptimum after the training procedure T . We then quantify the impact of critical factors on memoriza\u0002tion in terms of EMM, considering the three facets of P, M, and T . Among all illuminating results,\nwe surprisingly observe that the memorization can be triggered by conditioning training data on com\u0002pletely random and uninformative labels. Specifically, using such conditioning design, we show that\nmore than 65% of samples generated by diffusion models trained on the 50k CIFAR-10 images are\nreplicas of training data, an obvious contrast to the original 0%. Our study holds practical significance\nfor diffusion model users and offers clues to theoretical research in deep generative models.\n1We also provide a theoretical analysis from the lens of backward process in Appendix A.2.\n2\nPreprint\n2 MEMORIZATION IN DIFFUSION MODELS\nWe start by examining the memorization in the widely-adopted EDM (Karras et al., 2022), which is\none of the state-of-the-art diffusion models for image generation. To determine whether a generated\nimage x is a memorized replica from the training data D, we adopt the criteria introduced in Yoon\net al. (2023), which considers x as memorized if \u2225x \u2212 NN1(x, D)\u22252 <\n1\n3\n\u00b7 \u2225x \u2212 NN2(x, D)\u22252, where\nNNj (x, D) indicates the j-th \u21132-nearest neighbor of x in D and the factor 1\n3\nis an empirical threshold.\nWe train an EDM model on the CIFAR-10 dataset without applying data augmentation (to avoid any\nambiguity regarding memorization) and evaluate the ratio of memorization among 10k generated\nimages. Remarkably, we observe a memorization ratio of zero throughout the entire training process,\nas illustrated by the bottom curve in Fig. 1c.\nIntuitively, we hypothesize that the default configuration of EDM lacks the essential capacity to\nmemorize the 50k training images in CIFAR-10, which motivates our exploration into whether\nexpected memorization behavior will manifest when reducing the training dataset size. In particular,\nwe generate a sequence of training datasets with different sizes of {20k, 10k, 5k, 2k}, by sampling\nsubsets from the original set of 50k CIFAR-10 training images. We follow the default EDM training\nprocedure (Karras et al., 2022) with consistent training epochs on these smaller datasets. As shown in\nFig. 1c, when |D| = 20k or 10k, the memorization ratio remains close to zero. However, upon further\nreducing the training dataset size to |D| = 5k or 2k, the EDM models exhibit noticeable memorization.\nThis observation indicates the substantial impact of training dataset size on memorization. Addition\u0002ally, we notice that the memorization ratio increases with more training epochs. To observe this, we\nextend the training duration to 40k epochs, ten times longer than that in Karras et al. (2022). In Fig. 1d,\nwhen |D| = 1k, the model achieves over 90% memorization. However, even with 40k training epochs,\nthe diffusion model still struggle to replicate a large portion of training samples when |D| = 5k.\nBased on our findings, we seek to quantify the maximum dataset size at which diffusion mod\u0002els demonstrate behavior similar to the theoretical optimum, which is crucial in understanding\nmemorization behavior. To formalize this notion, considering a data distribution P, a diffusion\nmodel configuration M, and a training procedure T , we introduce the concept of effective mode",
          "original_query": "Empirical study of memorization vs generalization in diffusion models (Kadkhodaie et al., 2024)",
          "cleaned_query": "Empirical study of memorization vs generalization in diffusion models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Enhancing Noise-Robust Losses for Large-Scale Noisy Data Learning",
          "url": "https://arxiv.org/html/2306.05497v3",
          "content": "# Enhancing Noise-Robust Losses for Large-Scale Noisy Data Learning\n\nMax Staats 1,2 ,\nMatthias Thamm 2,\nBernd Rosenow 2Staats is the corresponding author.\n\n###### Abstract\n\nLarge annotated datasets inevitably contain noisy labels, which poses a major challenge for training deep neural networks as they easily memorize the labels.\nNoise-robust loss functions have emerged as a notable strategy to counteract this issue, but it remains challenging to create a robust loss function which is not susceptible to underfitting.\nThrough a quantitative approach, this paper explores the limited overlap between the network output at initialization and regions of non-vanishing gradients of bounded loss functions in the initial learning phase.\nUsing these insights, we address underfitting of several noise robust losses with a novel method denoted as _logit bias_,\nwhich adds a real number \u03f5italic-\u03f5\\\\epsilonitalic\\_\u03f5 to the logit at the position of the correct class.\nThe _logit bias_ enables these losses to achieve state-of-the-art results, even on datasets like WebVision, consisting of over a million images from 1000 classes.\nIn addition, we demonstrate that our method can be used to determine\noptimal parameters for several loss functions\n\u2013 without having to train networks.\nRemarkably, our method determines\nthe hyperparameters based on the number of classes, resulting in loss functions which require zero dataset or noise-dependent parameters.\n\n## 1 Introduction\n\nSupervised deep learning depends on high-quality labeled data for effective pattern recognition and accurate predictions (Goodfellow, Bengio, and Courville [2016](https://arxiv.org/html/2306.05497v3#bib.bib9)).\nIn real-world datasets, however, there is often label noise - erroneous or unclear labels due to human error or incomplete annotations (Liang, Liu, and Yao [2022](https://arxiv.org/html/2306.05497v3#bib.bib20)). Such noise can drastically impair the effectiveness of deep learning models, which often operate under the assumption of pristine labels (Song et\u00a0al. [2022](https://arxiv.org/html/2306.05497v3#bib.bib27)). Therefore, it is important to develop robust deep-learning algorithms that can efficiently learn from noisy datasets.\n\nOne effective approach to navigate label noise lies in employing noise-robust loss functions. These loss functions, notable for their model-agnostic nature, seamlessly integrate with any deep learning paradigm.\nThe existing literature highlights their ability to improve the robustness and generalization ability of deep learning models under noisy conditions\n(Ghosh, Kumar, and Sastry [2017](https://arxiv.org/html/2306.05497v3#bib.bib6); Zhang and Sabuncu [2018](https://arxiv.org/html/2306.05497v3#bib.bib41); Wang et\u00a0al. [2019](https://arxiv.org/html/2306.05497v3#bib.bib32); Amid et\u00a0al. [2019](https://arxiv.org/html/2306.05497v3#bib.bib1); Ma et\u00a0al. [2020](https://arxiv.org/html/2306.05497v3#bib.bib22); Zhou et\u00a0al. [2021](https://arxiv.org/html/2306.05497v3#bib.bib43); Englesson and Azizpour [2021](https://arxiv.org/html/2306.05497v3#bib.bib4)).\n\nA majority of these loss functions are bounded to prevent the learning of mislabeled examples. From a theoretical point of view, bounded losses have a higher robustness to noise if they belong to the class of symmetric losses (Ghosh, Kumar, and Sastry [2017](https://arxiv.org/html/2306.05497v3#bib.bib6)). Nonetheless, it has been suggested that such symmetry could be overly constraining (Zhou et\u00a0al. [2021](https://arxiv.org/html/2306.05497v3#bib.bib43)), with functions like the Mean Absolute Error (MAE) leaning towards underfitting.\nReflecting this, many contemporary loss functions do not satisfy this symmetry condition (Zhou et\u00a0al. [2021](https://arxiv.org/html/2306.05497v3#bib.bib43); Englesson and Azizpour [2021](https://arxiv.org/html/2306.05497v3#bib.bib4)).\n\nIn this paper, we quantitatively explore how the vanishing derivatives of bounded loss functions impact their learning behavior.\nAccording to our findings, the cause of underfitting is the limited overlap between the output values of an initialized network and the region where the derivative of a particular bounded loss function is nonzero.\nTo counteract this, we add a real number, \u03f5italic-\u03f5\\\\epsilonitalic\\_\u03f5, to the logit corresponding to the correct class label. This subtle adjustment restores the overlap between network outputs and the region of sufficiently large derivatives of the loss, enabling e.g. the MAE loss to surpass the Cross Entropy loss on datasets like Cifar-100, even in the absence of label noise.\nImpressively, this approach requires only a single constant \u03f5italic-\u03f5\\\\epsilonitalic\\_\u03f5, which is determined by the number of classes, providing an effectively parameter-free method. Other loss functions like the generalized cross entropy (genCE) (Zhang and Sabuncu [2018](https://arxiv.org/html/2306.05497v3#bib.bib41)) and NF-MAE (a combination of normalized focal loss (NF) with MAE) (Ma et\u00a0al. [2020](https://arxiv.org/html/2306.05497v3#bib.bib22)) are also able to learn the WebVision dataset with the help of the logit bias.\n\nFurthermore, our description of the early learning phase enables us to calculate suitable hyperparameters for other loss functions like genCE and NCE-AGCE (a combination of Normalized Cross Entropy and Asymmetric Generalized Cross Entropy) (Zhou et\u00a0al. [2021](https://arxiv.org/html/2306.05497v3#bib.bib43)), allowing them to show their potential for an arbitrary number of classes\nwithout the need of tuning their parameters first, e.g., by an expensive grid search. The method we propose\nis intended as a first step towards a universal framework that is capable of noise robust learning across varied numbers of classes\nwithout requiring hyperparameter fine-tuning. The need for such a method is underscored by our observation that none of the proposed loss functions that are noise resistant on the Cifar-10 dataset are capable of learning the WebVision dataset.\n\nIn summary, our paper\n(i) quantitatively describes how the initial learning phase of a newly initialized network is contingent upon the dataset\u2019s class count; (ii) explores the limitations of bounded losses in multi-class datasets and introduces the _logit bias_ technique, enabling MAE to consistently deliver competitive or even superior results across benchmarks like Fashion-MNIST, Cifar-10, Cifar-100, and WebVision \u2013 without hyperparameters; (iii) enables other noise-robust loss functions to learn the WebVision dataset using either the logit bias or by determining their hyperparameters without training a single network.\n\nAll code for reproducing the data and creating the figures in this paper is open source and available under (Author [s](https://arxiv.org/html/2306.05497v3#bib.bib2)).\n\n## 2 Related Work\n\nLabel noise in training data is a pervasive challenge that has attracted much attention in recent years (Liang, Liu, and Yao [2022](https://arxiv.org/html/2306.05497v3#bib.bib20); Song et\u00a0al. [2022](https://arxiv.org/html/2306.05497v3#bib.bib27)). One strategy for addressing it is data cleaning, aiming to filter out mislabeled samples from the training dataset.\nTo identify noisy instances, (Xiao et\u00a0al. [2015](https://arxiv.org/html/2306.05497v3#bib.bib36)) employs a probabilistic model to capture the relationship between images, labels, and noise. Other approaches utilize an auxiliary neural network, trained on curated data, to clean the main dataset (Veit et\u00a0al. [2017](https://arxiv.org/html/2306.05497v3#bib.bib31); Lee et\u00a0al. [2018](https://arxiv.org/html/2306.05497v3#bib.bib18)). Yet, an overzealous curation can sometimes be counterproductive, as eliminating too many samples might degrade model performance (Khetan, Lipton, and Anandkumar [2017](https://arxiv.org/html/2306.05497v3#bib.bib15)), compared to retaining some corrupted instances.\n\nAnother approach is estimating the noise transition matrix, which depicts the likelihood of mislabeling across classes. This matrix can be incorporated directly into the loss function (Han et\u00a0al. [2018](https://arxiv.org/html/2306.05497v3#bib.bib10)) or inferred throughout training (Goldberger and Ben-Reuven [2017](https://arxiv.org/html/2306.05497v3#bib.bib8); Sukhbaatar and Fergus [2014](https://arxiv.org/html/2306.05497v3#bib.bib28)), mitigating the consequences of mislabeling. A variation of this strategy involves dynamically adjusting the weights of samples during training. For example, (Reed et\u00a0al. [2014](https://arxiv.org/html/2306.05497v3#bib.bib25)) adjust labels based on the network\u2019s current predictions, and (Ren et\u00a0al. [2018](https://arxiv.org/html/2306.05497v3#bib.bib26)) evaluate label trustworthiness based on the gradient induced by a given example.\nFurthermore, (Thulasidasan et\u00a0al. [2019](https://arxiv.org/html/2306.05497v3#bib.bib29)) prompt the network to predict the likelihood of the example being correct, enabling fine-tuned loss adjustments.\n\nAnother avenue entails constraining the minimum training loss, emphasizing that an optimal scenario that avoids learning from incorrect labels will incur a finite loss (Toner and Storkey [2023](https://arxiv.org/html/2306.05497v3#bib.bib30)).\nSelf-supervised methods that iteratively adjust labels by extracting information directly from data structures \u2014 via dynamic label learning (Chen et\u00a0al. [2020](https://arxiv.org/html/2306.05497v3#bib.bib3)) and contrastive learning (Hendrycks et\u00a0al. [2019](https://arxiv.org/html/2306.05497v3#bib.bib13); Zheltonozhskii et\u00a0al. [2022](https://arxiv.org/html/2306.05497v3#bib.bib42); Ghosh and Lan [2021](https://arxiv.org/html/2306.05497v3#bib.bib7); Xue, Whitecross, and Mirzasoleiman [2022](https://arxiv.org/html/2306.05497v3#bib.bib38); Yi et\u00a0al. [2022](https://arxiv.org/html/2306.05497v3#bib.bib40)) \u2014 have also been shown to improve model generalization in noisy conditions.\n\nOpting for a noise-robust loss function can complement and enhance many of the strategies described above.\nIn (Ghosh, Kumar, and Sastry [2017](ht",
          "original_query": "Noisy training-loss explanation for generalization (Vastola, 2025)",
          "cleaned_query": "Noisy training-loss explanation for generalization",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Memorization-Aware Flow Matching via EMM-Regularized Vector Fields\nDesign a Flow Matching (FM) training objective that adds an Effective Model Memorization (EMM)-based regularizer, penalizing vector fields that concentrate probability mass near nearest-neighbor training samples along the probability path. Evaluate whether OT displacement interpolation paths reduce EMM compared to diffusion paths at fixed compute, and quantify privacy leakage changes under standard membership inference/extraction attacks.",
        "Noise-Robust Conditional FM for Training with Noisy or Pseudo Labels\nExtend FM to conditional generation with noisy class labels and adapt the \u201clogit bias\u201d idea by adding a class-logit offset inside the conditional network (or classifier-free guidance branch) to prevent early underfitting while maintaining robustness. Benchmark on WebVision-like noisy supervision for class-conditional image generation, measuring fidelity/diversity and label-noise sensitivity across different probability paths (diffusion vs OT).",
        "Random-Label Memorization Stress Test for FM vs Score Models\nReproduce the finding that uninformative random labels trigger memorization, but compare across (i) DSM-trained score models and (ii) FM-trained CNFs using the same architectures and data. Systematically vary label entropy and conditioning strength to map \u201cmemorization phase diagrams,\u201d isolating whether memorization arises from the conditioning mechanism, the training objective, or the chosen path.",
        "Adaptive Path Scheduling to Minimize Memorization Under Finite Data\nCreate a controller that selects or blends probability paths over training (e.g., start diffusion-like for stability, anneal toward OT displacement interpolation for efficiency) with the explicit goal of reducing EMM at small dataset sizes. Implement an online criterion based on nearest-neighbor statistics of generated samples and adjust the path family/parameters to maintain novelty while keeping likelihood competitive.",
        "Bounded-Loss-Inspired Weighting for Score Matching and Flow Matching\nTranslate bounded noise-robust loss principles into generative training by down-weighting time-steps or samples that produce vanishing/unstable gradients (analogous to early-phase overlap issues) in DSM or FM. Concretely, derive a time-dependent weighting \u03bb(t) that maximizes gradient-support overlap for typical network initializations, then test whether it improves convergence and reduces memorization on small datasets.",
        "Hybrid FM + Predictor\u2013Corrector Sampling with Learned Correctors\nDevelop a sampling scheme where FM provides a fast ODE \u201cpredictor\u201d and a lightweight score-based MCMC \u201ccorrector\u201d is learned specifically to fix artifacts and increase mode coverage. Train the corrector to approximate a small number of Langevin steps using a distilled score network, and compare compute\u2013quality tradeoffs against standard PC samplers and pure ODE solvers.",
        "Likelihood\u2013FID\u2013EMM Tri-Objective Training for CNFs\nFormulate a multi-objective training approach that jointly optimizes likelihood (via CNF change-of-variables), sample quality (FID), and memorization risk (EMM/proxy privacy metrics). Implement Pareto-front training (e.g., dynamic weight averaging) to identify regimes where OT-based FM achieves better generalization at the same likelihood than diffusion-based paths, and report actionable operating points for practitioners.",
        "Path-Dependent Privacy Accounting for Generative ODE/SDE Models\nCreate a privacy leakage analysis that explicitly depends on the chosen probability path (diffusion kernel vs OT interpolation) and training objective (DSM vs FM). Build a set of empirical estimators linking local contraction/expansion (Jacobian/divergence along probability flow ODE) to extraction success rates, enabling path-aware \u201cprivacy budgets\u201d without full DP training.",
        "Robust FM for Mixed-Quality Datasets via Per-Sample Path Parameters\nIntroduce per-sample or per-batch path parameterization (e.g., noise scale or interpolation strength) conditioned on an estimated label/data reliability score, akin to robust learning with noisy labels. The method would route suspicious samples through \u201cmore diffusive\u201d (smoothing) paths while clean samples use sharper OT-like paths, aiming to improve both robustness and sample diversity on web-scale scraped data.",
        "FLOW-Inspired Evaluation Protocols: Generative Models for Organizational Wellness Data\nApply FM/score-based generative modeling to sensitive, low-sample organizational wellness/leadership survey data (motivated by the FLOW model) and study memorization risk under small-N regimes where EMM predicts replication. Develop a concrete benchmark with synthetic-but-realistic wellness trajectories, comparing OT-FM vs diffusion-score models on utility (downstream prediction of wellness outcomes) and privacy (training-record reconstruction), yielding guidance for safe deployment in social-science contexts."
      ],
      "generated_ideas_raw": "1. **Memorization-Aware Flow Matching via EMM-Regularized Vector Fields**  \nDesign a Flow Matching (FM) training objective that adds an Effective Model Memorization (EMM)-based regularizer, penalizing vector fields that concentrate probability mass near nearest-neighbor training samples along the probability path. Evaluate whether OT displacement interpolation paths reduce EMM compared to diffusion paths at fixed compute, and quantify privacy leakage changes under standard membership inference/extraction attacks.\n\n2. **Noise-Robust Conditional FM for Training with Noisy or Pseudo Labels**  \nExtend FM to conditional generation with noisy class labels and adapt the \u201clogit bias\u201d idea by adding a class-logit offset inside the conditional network (or classifier-free guidance branch) to prevent early underfitting while maintaining robustness. Benchmark on WebVision-like noisy supervision for class-conditional image generation, measuring fidelity/diversity and label-noise sensitivity across different probability paths (diffusion vs OT).\n\n3. **Random-Label Memorization Stress Test for FM vs Score Models**  \nReproduce the finding that uninformative random labels trigger memorization, but compare across (i) DSM-trained score models and (ii) FM-trained CNFs using the same architectures and data. Systematically vary label entropy and conditioning strength to map \u201cmemorization phase diagrams,\u201d isolating whether memorization arises from the conditioning mechanism, the training objective, or the chosen path.\n\n4. **Adaptive Path Scheduling to Minimize Memorization Under Finite Data**  \nCreate a controller that selects or blends probability paths over training (e.g., start diffusion-like for stability, anneal toward OT displacement interpolation for efficiency) with the explicit goal of reducing EMM at small dataset sizes. Implement an online criterion based on nearest-neighbor statistics of generated samples and adjust the path family/parameters to maintain novelty while keeping likelihood competitive.\n\n5. **Bounded-Loss-Inspired Weighting for Score Matching and Flow Matching**  \nTranslate bounded noise-robust loss principles into generative training by down-weighting time-steps or samples that produce vanishing/unstable gradients (analogous to early-phase overlap issues) in DSM or FM. Concretely, derive a time-dependent weighting \u03bb(t) that maximizes gradient-support overlap for typical network initializations, then test whether it improves convergence and reduces memorization on small datasets.\n\n6. **Hybrid FM + Predictor\u2013Corrector Sampling with Learned Correctors**  \nDevelop a sampling scheme where FM provides a fast ODE \u201cpredictor\u201d and a lightweight score-based MCMC \u201ccorrector\u201d is learned specifically to fix artifacts and increase mode coverage. Train the corrector to approximate a small number of Langevin steps using a distilled score network, and compare compute\u2013quality tradeoffs against standard PC samplers and pure ODE solvers.\n\n7. **Likelihood\u2013FID\u2013EMM Tri-Objective Training for CNFs**  \nFormulate a multi-objective training approach that jointly optimizes likelihood (via CNF change-of-variables), sample quality (FID), and memorization risk (EMM/proxy privacy metrics). Implement Pareto-front training (e.g., dynamic weight averaging) to identify regimes where OT-based FM achieves better generalization at the same likelihood than diffusion-based paths, and report actionable operating points for practitioners.\n\n8. **Path-Dependent Privacy Accounting for Generative ODE/SDE Models**  \nCreate a privacy leakage analysis that explicitly depends on the chosen probability path (diffusion kernel vs OT interpolation) and training objective (DSM vs FM). Build a set of empirical estimators linking local contraction/expansion (Jacobian/divergence along probability flow ODE) to extraction success rates, enabling path-aware \u201cprivacy budgets\u201d without full DP training.\n\n9. **Robust FM for Mixed-Quality Datasets via Per-Sample Path Parameters**  \nIntroduce per-sample or per-batch path parameterization (e.g., noise scale or interpolation strength) conditioned on an estimated label/data reliability score, akin to robust learning with noisy labels. The method would route suspicious samples through \u201cmore diffusive\u201d (smoothing) paths while clean samples use sharper OT-like paths, aiming to improve both robustness and sample diversity on web-scale scraped data.\n\n10. **FLOW-Inspired Evaluation Protocols: Generative Models for Organizational Wellness Data**  \nApply FM/score-based generative modeling to sensitive, low-sample organizational wellness/leadership survey data (motivated by the FLOW model) and study memorization risk under small-N regimes where EMM predicts replication. Develop a concrete benchmark with synthetic-but-realistic wellness trajectories, comparing OT-FM vs diffusion-score models on utility (downstream prediction of wellness outcomes) and privacy (training-record reconstruction), yielding guidance for safe deployment in social-science contexts.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Memorization-Aware Flow Matching via EMM-Regularized Vector Fields\nDesign a Flow Matching (FM) training objective that adds an Effective Model Memorization (EMM)-based regularizer, penalizing vector f",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Noise-Robust Conditional FM for Training with Noisy or Pseudo Labels\nExtend FM to conditional generation with noisy class labels and adapt the \u201clogit bias\u201d idea by adding a class-logit offset inside t",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Random-Label Memorization Stress Test for FM vs Score Models\nReproduce the finding that uninformative random labels trigger memorization, but compare across (i) DSM-trained score models and (ii) FM-tr",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Adaptive Path Scheduling to Minimize Memorization Under Finite Data\nCreate a controller that selects or blends probability paths over training (e.g., start diffusion-like for stability, anneal toward ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Bounded-Loss-Inspired Weighting for Score Matching and Flow Matching\nTranslate bounded noise-robust loss principles into generative training by down-weighting time-steps or samples that produce vanish",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Hybrid FM + Predictor\u2013Corrector Sampling with Learned Correctors\nDevelop a sampling scheme where FM provides a fast ODE \u201cpredictor\u201d and a lightweight score-based MCMC \u201ccorrector\u201d is learned specifical",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Likelihood\u2013FID\u2013EMM Tri-Objective Training for CNFs\nFormulate a multi-objective training approach that jointly optimizes likelihood (via CNF change-of-variables), sample quality (FID), and memorization",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Path-Dependent Privacy Accounting for Generative ODE/SDE Models\nCreate a privacy leakage analysis that explicitly depends on the chosen probability path (diffusion kernel vs OT interpolation) and trai",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Robust FM for Mixed-Quality Datasets via Per-Sample Path Parameters\nIntroduce per-sample or per-batch path parameterization (e.g., noise scale or interpolation strength) conditioned on an estimated la",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "FLOW-Inspired Evaluation Protocols: Generative Models for Organizational Wellness Data\nApply FM/score-based generative modeling to sensitive, low-sample organizational wellness/leadership survey data ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 9,
      "paper_title": "Why Diffusion Models Don\u2019t Memorize:  The Role of Implicit Dynamical Regularization in Training",
      "contribution": "The paper shows that training dynamics impose an implicit dynamical regularization in diffusion models: there are two distinct timescales (\u03c4gen and \u03c4mem) so that models generalize for a wide, growing window of training times (\u03c4 \u2208 [\u03c4gen, \u03c4mem]) because \u03c4mem scales linearly with dataset size n while \u03c4gen remains constant, explaining why memorization is avoided in practice and giving a tractable random-features theory that matches experiments.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11428,
      "output_tokens": 1024,
      "predecessor_details": [
        {
          "success": true,
          "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
          "url": "https://proceedings.mlr.press/v37/sohl-dickstein15.html",
          "content": "\\[ [edit](https://github.com/mlresearch/v37/edit/gh-pages/_posts/2015-06-01-sohl-dickstein15.md)\\]\n\n# Deep Unsupervised Learning using Nonequilibrium Thermodynamics\n\nJascha Sohl-Dickstein,\u00a0Eric Weiss,\u00a0Niru Maheswaranathan,\u00a0Surya Ganguli\n\n_Proceedings of the 32nd International Conference on Machine Learning_,\u00a0PMLR 37:2256-2265,\u00a02015.\n\n#### Abstract\n\nA central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.\n\n#### Cite this Paper\n\nBibTeX\n\n`@InProceedings{pmlr-v37-sohl-dickstein15,\ntitle = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},\nauthor = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},\nbooktitle = {Proceedings of the 32nd International Conference on Machine Learning},\npages = {2256--2265},\nyear = {2015},\neditor = {Bach, Francis and Blei, David},\nvolume = {37},\nseries = {Proceedings of Machine Learning Research},\naddress = {Lille, France},\nmonth = {07--09 Jul},\npublisher = {PMLR},\npdf = {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},\nurl = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},\nabstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}\n}`\n\nCopy to ClipboardDownload\n\nEndnote\n\n`%0 Conference Paper\n%T Deep Unsupervised Learning using Nonequilibrium Thermodynamics\n%A Jascha Sohl-Dickstein\n%A Eric Weiss\n%A Niru Maheswaranathan\n%A Surya Ganguli\n%B Proceedings of the 32nd International Conference on Machine Learning\n%C Proceedings of Machine Learning Research\n%D 2015\n%E Francis Bach\n%E David Blei\n%F pmlr-v37-sohl-dickstein15\n%I PMLR\n%P 2256--2265\n%U https://proceedings.mlr.press/v37/sohl-dickstein15.html\n%V 37\n%X A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.`\n\nCopy to ClipboardDownload\n\nRIS\n\n`TY - CPAPER\nTI - Deep Unsupervised Learning using Nonequilibrium Thermodynamics\nAU - Jascha Sohl-Dickstein\nAU - Eric Weiss\nAU - Niru Maheswaranathan\nAU - Surya Ganguli\nBT - Proceedings of the 32nd International Conference on Machine Learning\nDA - 2015/06/01\nED - Francis Bach\nED - David Blei\nID - pmlr-v37-sohl-dickstein15\nPB - PMLR\nDP - Proceedings of Machine Learning Research\nVL - 37\nSP - 2256\nEP - 2265\nL1 - http://proceedings.mlr.press/v37/sohl-dickstein15.pdf\nUR - https://proceedings.mlr.press/v37/sohl-dickstein15.html\nAB - A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.\nER -`\n\nCopy to ClipboardDownload\n\nAPA\n\n`Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N. & Ganguli, S.. (2015). Deep Unsupervised Learning using Nonequilibrium Thermodynamics. Proceedings of the 32nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 37:2256-2265 Available from https://proceedings.mlr.press/v37/sohl-dickstein15.html.`\n\nCopy to ClipboardDownload\n\n#### Related Material\n\n- [Download PDF](http://proceedings.mlr.press/v37/sohl-dickstein15.pdf)\n- [Supplementary Material](http://proceedings.mlr.press/v37/sohl-dickstein15-supp.pdf)",
          "original_query": "Deep unsupervised learning using nonequilibrium thermodynamics",
          "cleaned_query": "Deep unsupervised learning using nonequilibrium thermodynamics",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Estimation of Non-Normalized Statistical Models by Score Matching",
          "url": "https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf",
          "content": "Journal of Machine Learning Research 6 (2005) 695\u2013709 Submitted 11/04; Revised 3/05; Published 4/05\nEstimation of Non-Normalized Statistical Models\nby Score Matching\nAapo Hyv\u00a8arinen aapo.hyvarinen@helsinki.fi\nHelsinki Institute for Information Technology (BRU)\nDepartment of Computer Science\nFIN-00014 University of Helsinki, Finland\nEditor: Peter Dayan\nAbstract\nOne often wants to estimate statistical models where the probability density function is\nknown only up to a multiplicative normalization constant. Typically, one then has to resort\nto Markov Chain Monte Carlo methods, or approximations of the normalization constant.\nHere, we propose that such models can be estimated by minimizing the expected squared\ndistance between the gradient of the log-density given by the model and the gradient of\nthe log-density of the observed data. While the estimation of the gradient of log-density\nfunction is, in principle, a very difficult non-parametric problem, we prove a surprising\nresult that gives a simple formula for this objective function. The density function of the\nobserved data does not appear in this formula, which simplifies to a sample average of a\nsum of some derivatives of the log-density given by the model. The validity of the method\nis demonstrated on multivariate Gaussian and independent component analysis models,\nand by estimating an overcomplete filter set for natural image data.\nKeywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov\nchain Monte Carlo, contrastive divergence\n1. Introduction\nIn many cases, probabilistic models in machine learning, statistics, or signal processing are\ngiven in the form of non-normalized probability densities. That is, the model contains an\nunknown normalization constant whose computation is too difficult for practical purposes.\nAssume we observe a random vector x \u2208 R\nn which has a probability density function\n(pdf) denoted by px(.). We have a parametrized density model p(.; \u03b8), where \u03b8 is an m\u0002dimensional vector of parameters. We want to estimate the parameter \u03b8 from x, i.e. we\nwant to approximate px(.) by p(.; \u03b8\u02c6) for the estimated parameter value \u03b8\u02c6. (We shall here\nconsider the case of continuous-valued variables only.)\nThe problem we consider here is that we only are able to compute the pdf given by the\nmodel up to a multiplicative constant Z(\u03b8):\np(\u03be; \u03b8) =\n1\nZ(\u03b8)\nq(\u03be; \u03b8).\nThat is, we do know the functional form of q as an analytical expression (or any form that\ncan be easily computed), but we do not know how to easily compute Z which is given by\n c 2005 Aapo Hyv\u00a8arinen.\nHyvarinen \u00a8\nan integral that is often analytically intractable:\nZ(\u03b8) =\nZ\n\u03be\u2208Rn\nq(\u03be; \u03b8) d\u03be.\nIn higher dimensions (in fact, for almost any n > 2), the numerical computation of this\nintegral is practically impossible as well.\nUsually, estimation of non-normalized models is approached by Markov Chain Monte\nCarlo (MCMC) methods, which are very slow, or by making some approximations, which\nmay be quite poor (Mackay, 2003).\nNon-normalized models are often encountered in continous-valued Markov random fields,\nwhich are widely used in image modelling, see e.g. (Bouman and Sauer, 1993; Li, 2001).\nIn general, undirected graphical models cannot be normalized except in the Gaussian case.\nOther recent work in image modelling also includes non-normalized models (Hyv\u00a8arinen and\nHoyer, 2001; Teh et al., 2003). Presumably, the number of useful applications for non\u0002normalized models is much larger than the present literature suggests. Non-normalized\nmodels have been avoided because their estimation has been considered too difficult; the\nadvent of efficient estimation methods may significantly increase their utility.\nIn this paper, we propose a simple method for estimating such non-normalized models.\nThis is based on minimizing the expected squared distance of the score function of x and\nthe score function given by the model. (By score function, we mean here the gradient\nof log-density.) We show that this distance can be estimated by a very simple formula\ninvolving only sample averages of some derivatives of the logarithm of the pdf given by the\nmodel. Thus, the computations involved are essentially not more complicated than in the\ncase where we know an analytical expression for the normalization constant. The proposed\nformula is exact and does not involve any approximations, which is why we are able to\nprove the local consistency of the resulting method. Minimization of the proposed objective\nfunction thus provides an estimation method that is computationally simple yet statistically\nlocally consistent.\n2. Estimation by Score Matching\nIn the following, we use extensively the gradient of the log-density with respect to the data\nvector. For simplicity, we call this the score function, although according the conventional\ndefinition, it is actually the score function with respect to a hypothetical location parameter\n(Schervish, 1995). For the model density, we denote the score function by \u03c8(\u03be; \u03b8):\n\u03c8(\u03be; \u03b8) =\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u2202 log p(\u03be;\u03b8)\n\u2202\u03be1\n.\n.\n.\n\u2202 log p(\u03be;\u03b8)\n\u2202\u03ben\n\uf8f6\n\uf8f7\uf8f7\uf8f8 =\n\uf8eb\n\uf8ec\uf8ed\n\u03c81(\u03be; \u03b8)\n.\n.\n.\n\u03c8n(\u03be; \u03b8)\n\uf8f6\n\uf8f7\uf8f8 = \u2207\u03be log p(\u03be; \u03b8).\nThe point in using the score function is that it does not depend on Z(\u03b8). In fact we\nobviously have\n\u03c8(\u03be; \u03b8) = \u2207\u03be log q(\u03be; \u03b8). (1)\nLikewise, we denote by \u03c8x(.) = \u2207\u03be log px(.) the score function of the distribution of observed\ndata x. This could in principle be estimated by computing the gradient of the logarithm of\n696\nEstimation by Score Matching\na non-parametric estimate of the pdf\u2014but we will see below that no such computation is\nnecessary. Note that score functions are mappings from R\nn\nto R\nn\n.\nWe now propose that the model is estimated by minimizing the expected squared dis\u0002tance between the model score function \u03c8(.; \u03b8) and the data score function \u03c8x\n(.). We define\nthis squared distance as\nJ(\u03b8) =\n1\n2\nZ\n\u03be\u2208Rn\npx(\u03be)k\u03c8(\u03be; \u03b8) \u2212 \u03c8x(\u03be)k\n2\nd\u03be. (2)\nThus, our score matching estimator of \u03b8 is given by\n\u03b8\u02c6 = arg min\n\u03b8\nJ(\u03b8).\nThe motivation for this estimator is that the score function can be directly computed\nfrom q as in (1), and we do not need to compute Z. However, this may still seem to be a\nvery difficult way of estimating \u03b8, since we might have to compute an estimator of the data\nscore function \u03c8xfrom the observed sample, which is basically a non-parametric estimation\nproblem. However, no such non-parametric estimation is needed. This is because we can\nuse a simple trick of partial integration to compute the objective function very easily, as\nshown by the following theorem:\nTheorem 1 Assume that the model score function \u03c8(\u03be; \u03b8) is differentiable, as well as some\nweak regularity conditions.1\nThen, the objective function J in (2) can be expressed as\nJ(\u03b8) =\nZ\n\u03be\u2208Rn\npx(\u03be)\nXn\ni=1\n\u0014\n\u2202i\u03c8i(\u03be; \u03b8) +\n1\n2\n\u03c8i(\u03be; \u03b8)\n2\n\u0015\nd\u03be + const. (3)\nwhere the constant does not depend on \u03b8,\n\u03c8i(\u03be; \u03b8) =\n\u2202 log q(\u03be; \u03b8)\n\u2202\u03bei\nis the i-th element of the model score function, and\n\u2202i\u03c8i(\u03be; \u03b8) =\n\u2202\u03c8i(\u03be; \u03b8)\n\u2202\u03bei\n=\n\u2202\n2\nlog q(\u03be; \u03b8)\n\u2202\u03be\n2\ni\nis the partial derivative of the i-th element of the model score function with respect to the\ni-th variable.\nThe proof, given in the Appendix, is based a simple trick of partial integration that has\npreviously been used in the theory of independent component analysis for modelling the\ndensities of the independent components (Pham and Garrat, 1997).\nWe have thus proven the remarkable fact that the squared distance of the model score\nfunction from the data score function can be computed as a simple expectation of certain\n1. Namely: the data pdf px(\u03be) is differentiable, the expectations Ex{k\u03c8(x; \u03b8)k\n2\n} and Ex{k\u03c8x(x)k\n2\n} are\nfinite for any \u03b8, and px(\u03be)\u03c8(\u03be; \u03b8) goes to zero for any \u03b8 when k\u03bek \u2192 \u221e.\n697\nHyvarinen \u00a8\nfunctions of the non-normalized model pdf. If we have an analytical expression for the\nnon-normalized density function q, these functions are readily obtained by derivation using\n(1) and taking further derivatives.\nIn practice, we have T observations of the random vector x, denoted by x(1), . . . , x(T).\nThe sample version of J is obviously obtained from (3) as\nJ\u02dc(\u03b8) =\n1\nT\nX\nT\nt=1\nXn\ni=1\n\u0014\n\u2202i\u03c8i(x(t); \u03b8) +\n1\n2\n\u03c8i(x(t); \u03b8)\n2\n\u0015\n+ const. (4)\nwhich is asymptotically equivalent to J due to the law of large numbers. We propose to\nestimate the model by minimization of J\u02dc in the case of a real, finite sample.\nOne may wonder whether it is enough to minimize J to estimate the model, or whether\nthe distance of the score functions can be zero for different parameter values. Obviously, if\nthe model is degenerate in the sense that two different values of \u03b8 give the same pdf, we\ncannot estimate \u03b8. If we assume that the model is not degenerate, and that q > 0 always,\nwe have local consistency as shown by the following theorem and the corollary:\nTheorem 2 Assume the pdf of x follows the model: px(.) = p(.; \u03b8\n\u2217\n) for some \u03b8\n\u2217\n. Assume\nfurther that no other parameter value gives a pdf that is equal2to p(.; \u03b8\n\u2217\n), and that q(\u03be; \u03b8) >\n0 for all \u03be, \u03b8. Then\nJ(\u03b8) = 0 \u21d4 \u03b8 = \u03b8\n\u2217\n.\nFor a proof, see the Appendix.\nCorollary 3 Under the assumptions of the preceding Theorems, the score matching esti\u0002mator obtained by minimization of J\u02dc is consistent, i.e. it converges in probability towards\nthe true value of \u03b8 when sample size approaches infinity, assuming that the optimization\nalgorithm is able to find the global minimum.\nThe corollary is proven by applying the law of large numbers. As sample size approaches\ninfinity, J\u02dc converges to J (in probability). Thus, the estimator converges to a point where\nJ is globally minimized. By Theorem 2, the global minimum is unique and found at the\ntrue parameter value (obviously, J cannot be negative).\nThis result of consistency assumes that the global minimum of J\u02dc is found by the opti\u0002mization algorithm used in the estimation. In practice, this may not be true, in particular\nbecause there may be several local minima. Then, the consistency is of local nature, i.e.,\nthe estimator is consistent if the optimization iteration is started sufficiently close to the\ntrue value. Note that consistency imp",
          "original_query": "Estimation of non-normalized statistical models by score matching",
          "cleaned_query": "Estimation of non-normalized statistical models by score matching",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Random Features for Large-Scale Kernel Machines - People @EECS",
          "url": "https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf",
          "content": "Random Features for Large-Scale Kernel Machines\nAli Rahimi and Ben Recht\nAbstract\nTo accelerate the training of kernel machines, we propose to map the input data\nto a randomized low-dimensional feature space and then apply existing fast linear\nmethods. Our randomized features are designed so that the inner products of the\ntransformed data are approximately equal to those in the feature space of a user\nspecified shift-invariant kernel. We explore two sets of random features, provide\nconvergence bounds on their ability to approximate various radial basis kernels,\nand show that in large-scale classification and regression tasks linear machine\nlearning algorithms that use these features outperform state-of-the-art large-scale\nkernel machines.\n1 Introduction\nKernel machines such as the Support Vector Machine are attractive because they can approximate\nany function or decision boundary arbitrarily well with enough training data. Unfortunately, meth\u0002ods that operate on the kernel matrix (Gram matrix) of the data scale poorly with the size of the\ntraining dataset. For example, a dataset with half a million training examples might take days to\ntrain on modern workstations. On the other hand, specialized algorithms for linear Support Vector\nMachines and regularized regression run much more quickly when the dimensionality of the data\nis small because they operate on the covariance matrix rather than the kernel matrix of the training\ndata [1, 2]. We propose a way to combine the advantages of the linear and nonlinear approaches.\nInspired by randomized algorithms for approximating kernel matrices (e.g., [3, 4]), we efficiently\nconvert the training and evaluation of any kernel machine into the corresponding operations of a\nlinear machine by mapping data into a relatively low-dimensional randomized feature space. Our\nexperiments show that random features combined with very simple linear learning techniques com\u0002pete favorably with state-of-the-art kernel-based classification and regression algorithms. Random\nfeatures significantly reduce the computation needed for training, and obtain similar or better testing\nerror.\nThe kernel trick is a simple way to generate features for algorithms that depend only on the inner\nproduct between pairs of input points. It relies on the observation that any positive definite function\nk(x, y) with x, y \u2208 Rd defines an inner product and a lifting \u03c6 so that the inner product between\nlifted datapoints can be quickly computed as h\u03c6(x), \u03c6(y)i = k(x, y). The cost of this convenience\nis that algorithms access the data only through evaluations of k(x, y), or through the kernel ma\u0002trix consisting of k applied to all pairs of datapoints. As a result, large training sets incur large\ncomputational and storage costs.\nInstead of relying on the implicit lifting provided by the kernel trick, we propose explicitly mapping\nthe data to a low-dimensional Euclidean inner product space using a randomized feature map z :\nRd \u2192 RD so that the inner product between a pair of transformed points approximates their kernel\nevaluation:\nk(x, y) = h\u03c6(x), \u03c6(y)i \u2248 z(x)\n0\nz(y). (1)\nUnlike the kernel\u2019s lifting \u03c6, z is low-dimensional. Thus, we can simply transform the input with\nz, and then apply fast linear learning methods to approximate the answer of the corresponding\nnonlinear kernel machine. In what follows, we show how to construct feature spaces that uniformly\napproximate popular shift-invariant kernels k(x \u2212 y) to within \u000f with only D = O(d\u000f\u22122\nlog 1\n\u000f\n2 )\n1\ndimensions, and empirically show that excellent regression and classification performance can be\nobtained for even smaller D.\nIn addition to giving us access to extremely fast learning algorithms, these randomized feature maps\nalso provide a way to quickly evaluate the machine. With the kernel trick, evaluating the machine\nat a test point x requires computing f(x) = PN\ni=1 cik(xi\n, x), which requires O(N d) operations to\ncompute and requires retaining much of the dataset unless the machine is very sparse. This is often\nunacceptable for large datasets. On the other hand, after learning a hyperplane w, a linear machine\ncan be evaluated by simply computing f(x) = w0z(x), which, with the randomized feature maps\npresented here, requires only O(D + d) operations and storage.\nWe demonstrate two randomized feature maps for approximating shift invariant kernels. Our first\nrandomized map, presented in Section 3, consists of sinusoids randomly drawn from the Fourier\ntransform of the kernel function we seek to approximate. Because this map is smooth, it is well\u0002suited for interpolation tasks. Our second randomized map, presented in Section 4, partitions the\ninput space using randomly shifted grids at randomly chosen resolutions. This mapping is not\nsmooth, but leverages the proximity between input points, and is well-suited for approximating ker\u0002nels that depend on the L1 distance between datapoints. Our experiments in Section 5 demonstrate\nthat combining these randomized maps with simple linear learning algorithms competes favorably\nwith state-of-the-art training algorithms in a variety of regression and classification scenarios.\n2 Related Work\nThe most popular methods for large-scale kernel machines are decomposition methods for solving\nSupport Vector Machines (SVM). These methods iteratively update a subset of the kernel machine\u2019s\ncoefficients using coordinate ascent until KKT conditions are satisfied to within a tolerance [5,\n6]. While such approaches are versatile workhorses, they do not always scale to datasets with\nmore than hundreds of thousands of datapoints for non-linear problems. To extend learning with\nkernel machines to these scales, several approximation schemes have been proposed for speeding\nup operations involving the kernel matrix.\nThe evaluation of the kernel function can be sped up using linear random projections [3]. Throwing\naway individual entries [3] or entire rows [4, 7, 8] of the kernel matrix lowers the storage and compu\u0002tational cost of operating on the kernel matrix. These approximations either preserve the separability\nof the data [4], or produce good low-rank or sparse approximations of the true kernel matrix [3, 7].\nFast multipole and multigrid methods have also been proposed for this purpose, but, while they ap\u0002pear to be effective on small and low-dimensional problems, to our knowledge, their effectiveness\nhas not been demonstrated on large datasets. Further, the quality of the Hermite or Taylor approxi\u0002mation that these methods rely on degrades exponentially with the dimensionality of the dataset [9].\nFast nearest neighbor lookup with KD-Trees has been used to approximate multiplication with the\nkernel matrix, and in turn, a variety of other operations [10].\nWe compare our work to the Core Vector Machine (CVM), a state-of-the-art technique that takes\nan altogether different approach than those thus far discussed [12]. CVM transforms a classifica\u0002tion problem into a support vector data-description problem, and solves this using a fast minimum\u0002enclosing ball algorithm that randomly samples the training data.\nUnlike previous work, instead of approximating the kernel matrix, our work approximates the kernel\nfunction directly. The feature map we present in Section 4 is reminiscent of KD-trees in that it\npartitions the input space using multi-resolution axis-aligned grids similar to those developed in\n[11] for embedding linear assignment problems.\n3 Random Fourier Features\nOur first set of random features consists of random Fourier bases cos(\u03c9\n0x + b) where \u03c9 \u2208 Rd\nand b \u2208 R are random variables. These mappings project data points on a randomly chosen line,\nand then pass the resulting scalar through a sinusoidal function (see Figure 1 and Algorithm 1).\nDrawing the direction of these lines from an appropriate distribution guarantees that the product of\ntwo transformed points will approximate a desired shift-invariant kernel.\n2\nRD\nR2 \u03c9\nx\nKernel Name k(\u2206) p(\u03c9)\nGaussian e\n\u2212\nk\u2206k\n2\n2\n2 (2\u03c0)\n\u2212 D\n2 e\n\u2212\nk\u03c9k\n2\n2\n2\nLaplacian e\n\u2212k\u2206k1\nQ\nd\n1\n\u03c0(1+\u03c92\nd\n)\nCauchy Q\nd\n2\n1+\u22062\nd\ne\n\u2212k\u2206k1\nFigure 1: Random Fourier Features. Each component of the feature map z(x) projects x onto a random\ndirection \u03c9 drawn from the Fourier transform p(\u03c9) of k(\u2206), and wraps this line onto the unit circle in R2.\nAfter transforming two points x and y in this way, their inner product is an unbiased estimator of k(x, y). The\nmapping z(x) = cos(\u03c9\n0x + b) additionally rotates this circle by a random amount b and projects the points\nonto the interval [0, 1]. The table lists some popular shift-invariant kernels and their Fourier transforms. To\ndeal with non-isotropic kernels, we can first whiten the data and apply one of these kernels\nThe following classical theorem from harmonic analysis provides the key insight behind this trans\u0002formation:\nTheorem 1 (Bochner [13]). A continuous kernel k(x, y) = k(x \u2212 y) on Rdis positive definite if\nand only if k(\u03b4) is the Fourier transform of a non-negative measure.\nIf a shift-invariant kernel k(\u03b4) is properly scaled, Bochner\u2019s theorem guarantees that its Fourier\ntransform p(\u03c9) is a proper probability distribution. Defining \u03b6\u03c9(x) = e\nj\u03c90x\n, we have\nk(x \u2212 y) = Z\nRd\np(\u03c9)e\nj\u03c90(x\u2212y)\nd\u03c9 = E\u03c9[\u03b6\u03c9(x)\u03b6\u03c9(y)\n\u2217\n], (2)\nso \u03b6\u03c9(x)\u03b6\u03c9(y)\n\u2217\nis an unbiased estimate of k(x, y) when \u03c9 is drawn from p.\nSince both the probability distribution p(\u03c9) and the kernel k(\u2206) are real, the integral (2) converges\nwhen the complex exponentials are replaced with cosines. Therefore, we may obtain a real-valued\nmapping that satisfies the condition E[z\u03c9(x)z\u03c9(y)] = k(x, y) by setting z\u03c9(x) = \u221a\n2 cos(\u03c9\n0x+b),\nwhere \u03c9 is drawn from p(\u03c9) and b is drawn uniformly from [0, 2\u03c0]. That z\u03c9(x)z\u03c9(y) has expected\nvalue k(x, y) is a consequence of the sum of angles formula.\nWe can lower the variance of the estimate of the kernel by concatenating D randomly chosen z\u03c9\ninto one D-dimensional vector z and normalizing each component by \u221a\nD. The inner product\nz(x)\n0z(y) = 1\nD\nPD\nj=1 z\u03c9j\n(x)z\u03c9j(y) is a sample average of z\u03c9 and ",
          "original_query": "Random features for large-scale kernel machines",
          "cleaned_query": "Random features for large-scale kernel machines",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Dynamical regimes of diffusion models",
          "url": "https://www.nature.com/articles/s41467-024-54281-3",
          "content": "Dynamical regimes of diffusion models | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-024-54281-3?error=cookies_not_supported&code=f8f26ad1-5ae3-45c0-a378-b48be6ece2c9)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nDynamical regimes of diffusion models\n[Download PDF](https://www.nature.com/articles/s41467-024-54281-3.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-024-54281-3.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:17 November 2024# Dynamical regimes of diffusion models\n* [Giulio Biroli](#auth-Giulio-Biroli-Aff1)[1](#Aff1),\n* [Tony Bonnaire](#auth-Tony-Bonnaire-Aff1)[ORCID:orcid.org/0000-0003-2149-8795](https://orcid.org/0000-0003-2149-8795)[1](#Aff1),\n* [Valentin de Bortoli](#auth-Valentin-Bortoli-Aff2)[2](#Aff2)&amp;\n* \u2026* [Marc M\u00e9zard](#auth-Marc-M_zard-Aff3)[3](#Aff3)Show authors\n[*Nature Communications*](https://www.nature.com/ncomms)**volume15**, Article\u00a0number:9957(2024)[Cite this article](#citeas)\n* 19kAccesses\n* 44Citations\n* 104Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-024-54281-3/metrics)\n### Subjects\n* [Computer science](https://www.nature.com/subjects/computer-science)\n* [Phase transitions and critical phenomena](https://www.nature.com/subjects/phase-transitions-and-critical-phenomena)\n* [Statistical physics](https://www.nature.com/subjects/statistical-physics)\n## Abstract\nWe study generative diffusion models in the regime where both the data dimension and the sample size are large, and the score function is trained optimally. Using statistical physics methods, we identify three distinct dynamical regimes during the generative diffusion process. The generative dynamics, starting from pure noise, first encounters a speciation transition, where the broad structure of the data emerges, akin to symmetry breaking in phase transitions. This is followed by a collapse phase, where the dynamics is attracted to a specific training point through a mechanism similar to condensation in a glass phase. The speciation time can be obtained from a spectral analysis of the data\u2019s correlation matrix, while the collapse time relates to an excess entropy measure, and reveals the existence of a curse of dimensionality for diffusion models. These theoretical findings are supported by analytical solutions for Gaussian mixtures and confirmed by numerical experiments on real datasets.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-53165-w/MediaObjects/41467_2024_53165_Fig1_HTML.png)\n### [Generative learning for forecasting the dynamics of high-dimensional complex systems](https://www.nature.com/articles/s41467-024-53165-w?fromPaywallRec=false)\nArticleOpen access16 October 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41592-024-02377-5/MediaObjects/41592_2024_2377_Fig1_HTML.png)\n### [DynaMight: estimating molecular motions with improved reconstruction from cryo-EM images](https://www.nature.com/articles/s41592-024-02377-5?fromPaywallRec=false)\nArticleOpen access09 August 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42005-023-01516-2/MediaObjects/42005_2023_1516_Fig1_HTML.png)\n### [Automatically discovering ordinary differential equations from data with sparse regression](https://www.nature.com/articles/s42005-023-01516-2?fromPaywallRec=false)\nArticleOpen access09 January 2024\n## Introduction\nMachine learning has recently witnessed thrilling advancements, especially in the realm of generative models. At the forefront of this progress are diffusion models (DMs), which have emerged as powerful tools for modeling complex data distributions and generating new realistic samples. They have become the state of the art in generating images, videos, audio or 3D scenes[1](#ref-CR1),[2](#ref-CR2),[3](#ref-CR3),[4](#ref-CR4),[5](#ref-CR5),[6](#ref-CR6),[7](#ref-CR7),[8](https://www.nature.com/articles/s41467-024-54281-3#ref-CR8). Although the practical success of generative DMs is widely recognized, their full theoretical understanding remains an open challenge. Rigorous results assessing their convergence on finite-dimensional data have been obtained in refs.[9](#ref-CR9),[10](#ref-CR10),[11](#ref-CR11),[12](#ref-CR12),[13](#ref-CR13),[14](https://www.nature.com/articles/s41467-024-54281-3#ref-CR14). However, realistic data live in high-dimensional spaces, where interpolation between data points should face the curse of dimensionality[15](https://www.nature.com/articles/s41467-024-54281-3#ref-CR15). A thorough understanding of how generative models escape this curse is still lacking. This requires approaches able to take into account that the number and the dimension of the data are simultaneously very large. In this work, we face this challenge using statistical physics methods which have been developed to study probability distributions, disordered systems and stochastic processes in high dimensions[16](#ref-CR16),[17](#ref-CR17),[18](https://www.nature.com/articles/s41467-024-54281-3#ref-CR18).\nDMs work in two stages. In the forward diffusion, one starts from a data point (e.g., an image) and gradually adds noise to it, until the image has become pure noise. In the backward process, one gradually denoises the image using a diffusion in an effective force field (the score) which is learnt by leveraging techniques from score matching[19](https://www.nature.com/articles/s41467-024-54281-3#ref-CR19),[20](https://www.nature.com/articles/s41467-024-54281-3#ref-CR20)and deep neural networks. In this study, we focus on DMs which are efficient enough to learn the exact empirical score, i.e., the one obtained by noising the empirical distribution of data. In practical implementations, this should happen when one performs a long training of a strongly over-parameterized deep network to learn the score, in the situation when the number of data is not too large.\nWithin this framework, we develop a theoretical approach able to characterize the dynamics of DMs in the simultaneous limit of large dimensions and large dataset. We show that the backward generative diffusion process consists of three subsequent dynamical regimes. The first one is basically pure Brownian motion. In the second one, the backward trajectory finds one of the main classes of the data (for instance if the data consist of images of horses and cars, a given trajectory will specialize towards one of these two categories). In the third regime, the diffusion \u201ccollapses\u201d onto one of the examples of the dataset: a given trajectory commits to the attraction basin of one of the data points, and the backward evolution brings it back to that exact data point. Since DMs are defined as the time reversal of a forward noising process, the generative process has to collapse on the training dataset under the exact empirical score assumption[21](https://www.nature.com/articles/s41467-024-54281-3#ref-CR21),[22](https://www.nature.com/articles/s41467-024-54281-3#ref-CR22). We show, by performing a thorough analysis of the curse of dimensionality for DMs, that this memorization can be avoided at finite times only if the size of the dataset is exponentially large in the dimension. An alternative, which is the one used in practice, is to rely on regularization and approximate learning of the score, departing from its exact form. Understanding this crucial aspect of generative diffusion is a key open challenge[23](#ref-CR23),[24](#ref-CR24),[25](https://www.nature.com/articles/s41467-024-54281-3#ref-CR25)for which analyzing what happens when the exact empirical score is used represents a first step.\nSeparating these three dynamical regimes, we identify two characteristic cross-over times. The speciation time*t**S*is the transition from pure noise to the commitment of a trajectory towards one category. The collapse time*t**C*is the time where the backward trajectory falls into the attractor of one given data point. We provide mathematical tools to predict these times in terms of structure of data. We will first study simple models such as high-dimensional Gaussian mixtures, where we obtain a full analytical solution and hence a very detailed understanding. Within this setting, we show that in the simultaneous limit of large number and dimension of the data, the speciation and collapse cross-overs become sharp thresholds. Interestingly, both of them are related to phase transitions studied in physics. We then extend our results to more general settings and discuss the key role played by the dimensionality of data and the number of samples. Finally, we perform numerical experiments and confront the theory to real data such as CIFAR-10, ImageNet, and LSUN, showing that our main findings hold in realistic cases. We conclude by highlighting the consequences and the guidelines offered by our results, and discussing the next research steps, in particular how to go b",
          "original_query": "Dynamical regimes of diffusion models",
          "cleaned_query": "Dynamical regimes of diffusion models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models",
          "url": "https://arxiv.org/html/2410.03039",
          "content": "Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models\n# Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models\nXiaoyu Wu\\*Jiaru ZhangZhiwei Steven Wu\n###### Abstract\nDiffusion Models (DMs) have become powerful image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small image set to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the data leakage risks when releasing fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning. In this paper, we ask:\u201cCan training data be extracted from these fine-tuned DMs shared online?\u201dA successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data. Our method approximates fine-tuning as a gradual shift in the model\u2019s learned distribution\u2014from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets including WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting about 20% of fine-tuning data in most cases.\nThe code is available111[https://github.com/Nicholas0228/FineXtract](https://github.com/Nicholas0228/FineXtract).\nData Extraction, Copyright Protection, Privacy and Security, Diffusion Models, Trustworthy AI\n## 1Introduction\nRecent years have witnessed the advancement of Diffusion Models (DMs) in computer vision. These models demonstrate exceptional capabilities across various tasks, including image editing> (Kawar et\u00a0al., [> 2022\n](https://arxiv.org/html/2410.03039v3#bib.bib16)> )\n, and video editing> (Yang et\u00a0al., [> 2022\n](https://arxiv.org/html/2410.03039v3#bib.bib39)> )\n, among others. Particularly noteworthy is the advent of few-shot fine-tuning methods> (Hu et\u00a0al., [> 2021\n](https://arxiv.org/html/2410.03039v3#bib.bib14)> ; Ruiz et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.03039v3#bib.bib26)> ; Qiu et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.03039v3#bib.bib23)> )\n, in which a pretrained model is fine-tuned to personalize generation based on a small set of training images. These approaches have significantly reduced both memory and time costs in training. Moreover, these techniques offer powerful tools for adaptively generating images based on specific subjects or objects, embodying personalized AI and making AI accessible to everyone.\nBuilding on these innovations, several communities, such as Civitai> (\n[> civ, ](https://arxiv.org/html/2410.03039v3#bib.bib1)> )\nand HuggingFace> (\n[> hug, ](https://arxiv.org/html/2410.03039v3#bib.bib2)> )\n, have emerged, hosting tens of thousands of fine-tuned checkpoints and attracting millions of downloads. Although many users willingly share their fine-tuned models, they may be unaware of the risk of data leakage inherent in this process. This is particularly concerning when fine-tuning involves sensitive data, such as medical images, human faces, or copyrighted material. Moreover, many of these checkpoints are fine-tuned using unauthorized data, including artists\u2019 work. This unauthorized fine-tuning process raises significant concerns regarding \u201creputational damage, economic loss, plagiarism and copyright infringement\u201d as mentioned in> Jiang et\u00a0al. (\n[> 2023\n](https://arxiv.org/html/2410.03039v3#bib.bib15)> )\n, and has prompted numerous objections from data owners> (Liang et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.03039v3#bib.bib18)> ; Wu et\u00a0al., [> 2024\n](https://arxiv.org/html/2410.03039v3#bib.bib38)> ; Shan et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.03039v3#bib.bib27)> )\n.\nIn this paper, we pose a critical question:\u201cIs it possible to extract fine-tuning data from these fine-tuned DM checkpoints released online?\u201dSuccessfully doing so would\nconfirm that fine-tuning data is indeed leaked within these checkpoints. Moreover, the extracted images could serve as strong evidence that specific data was used in the fine-tuning process, thereby aiding those whose rights have been infringed to seek legal protection and take necessary legal action.\n![Refer to caption](x1.png)Figure 1:Extraction results from real-world fine-tuned checkpoints on HuggingFace using our FineXtract.Top:Extracted images.Bottom:Corresponding training images.\nMore specifically, extracting fine-tuning data can be seen as targeting specific portions of the training data, whereas previous work on extracting data from diffusion models has mainly focused on general generative processes> (Carlini et\u00a0al., [> 2023\n](https://arxiv.org/html/2410.03039v3#bib.bib5)> ; Somepalli et\u00a0al., [> 2023a\n](https://arxiv.org/html/2410.03039v3#bib.bib29)> , [> b\n](https://arxiv.org/html/2410.03039v3#bib.bib30)> )\n, often overlooking more detailed or interesting data.\nTo address this gap, we propose a new framework, calledFineXtract, for efficiently and accurately extracting training data from the extrapolated guidance between DMs before and after fine-tuning.\nWe begin by providing a parametric approximation of the fine-tuned DMs distribution, modeling it as an interpolation between the pretrained DMs\u2019 distribution and the fine-tuned data distribution.\nWith this approximation, we demonstrate that extrapolating the score functions of the pretrained and fine-tuned DMs can effectively*guide*the denoising process toward the high-density regions of the fine-tuned data distribution, a process we refer to as*model guidance*.\nWe then generate a set of images within such high-density regions and apply a clustering algorithm to identify the images that are most likely to match the training data within the fine-tuning dataset.\nOur method can be applied to both unconditional and conditional DMs. Specifically, when the training captionccis available, we approximate the learned distribution of DMs conditional on captionccas an interpolation between the unconditional DM learned distribution and the conditional data distribution. Combined with model guidance, this leads to an extrapolation from the noise predicted by the pretrained unconditional DM to that by the fine-tuned conditional DM, guiding generation toward the high-density region of the fine-tuned data distribution conditioned oncc.\nExperiments across different datasets, DM structures, and real-world checkpoints from HuggingFace demonstrate the effectiveness of our method, which extracts around 20% of images in most cases (See Fig.[1](https://arxiv.org/html/2410.03039v3#S1.F1)for visual examples).\nIn summary, our contributions are as follows:\n1. 1.\nWe approximate the learned distribution during the fine-tuning process of DMs and demonstrate how this guides the model towards the high-density regions of the fine-tuned data distribution.\n2. 2.\nWe propose a new framework, FineXtract, for extracting fine-tuning datasets using this approximation. With a clustering algorithm, our method can extract images visually close to fine-tuning dataset.\n3. 3.\nExperimental results on fine-tuned checkpoints on various datasets (WikiArt, DreamBooth), various DMs and real-world checkpoints from HuggingFace validate the effectiveness of our methods.\n## 2Background and Related Works\n### 2.1Diffusion Models and Few-shot Fine-tuning\nDiffusion Models and Score Matching.Diffusion Models\n(DMs)> (Ho et\u00a0al., [> 2020\n](https://arxiv.org/html/2410.03039v3#bib.bib13)> ; Sohl-Dickstein et\u00a0al., [> 2015\n](https://arxiv.org/html/2410.03039v3#bib.bib28)> )\nare generative models that approximate data distributions by gradually denoising a variable initially sampled from a Gaussian distribution. These models consist of a forward diffusion process and a backward denoising process. In the forward process, noise\u03b5\u2208\ud835\udca9\u200b(0,1)\\\\varepsilon\\\\in\\\\mathcal{N}(0,1)is progressively added to the input imagex0x\\_{0}over timett, following the equationxt=\u03b1t\u200bx0+1\u2212\u03b1t\u200b\u03b5x\\_{t}=\\\\sqrt{\\\\alpha\\_{t}}x\\_{0}+\\\\sqrt{1-\\\\alpha\\_{t}}\\\\varepsilon. Conversely, in the backward process, DMs aim to estimate and remove the noise using a noise-prediction module,\u03f5\u03b8\\\\epsilon\\_{\\\\theta}, from the noisy imagextx\\_{t}. The difference between the actual and predicted noise forms the basis of the training loss, known as the diffusion loss, which is defined as\u2112D\u200bM=\ud835\udd3c\u03b5\u223c\ud835\udca9\u200b(0,1),t\u200b[\u2016\u03f5\u03b8\u200b(xt,t)\u2212\u03b5\u201622]\\\\mathcal{L}\\_{DM}=\\\\mathbb{E}\\_{\\\\varepsilon\\\\sim\\\\mathcal{N}(0,1),t}\\\\left[\\\\|\\\\epsilon\\_{\\\\theta}(x\\_{t},t)-\\\\varepsilon\\\\|\\_{2}^{2}\\\\right].\nAnother series of works focus on score matching, offering insights into DMs from a different perspective> (Vincent, [> 2011\n](https://arxiv.org/html/2410.03039v3#bib.bib34)> ; Song &amp; Ermon, [> 2019\n](https://arxiv.org/html/2410.03039v3#bib.bib31)> ; Song et\u00a0al., [> 2020\n](https://arxiv.org/html/2410.03039v3#bib.bib32)> )\n. Score matching aims to learn a score networks\u03b8\u200b(x)s\\_{\\\\theta}(x)trained to predict the score (i.e., the gradient of the log probability function)\u2207xlog\u2061q\u200b(x)\\\\nabla\\_{x}\\\\log q(x)of dataxxwithin real data distributionq\u200b(x)q(x)> (Vincent, [> 2011\n](https://arxiv.org/html/2410.03039v3#bib.bib34)> )\n. To improve accuracy and stability,\nsubsequent research proposes predicting the score of the Gaussian-perturbed data distributionq\u200b(xt)q(x\\_{t})> (Song &amp; Ermon, [> 2019\n](https://arxiv.org/html/2410.03039v3#bib.bib31)> ; Song et\u00a0al., [> 2020\n](https://arxiv.org/html/2410.03039v3#bib.bib32)> )\n:s\u03b8\u200b(xt,t)\u2248\u2207xtlog\u2061q\u200b(xt)=\u2212\u03f5\u03b8\u200b(xt,t)1\u2212\u03b1t\u00afs\\_{\\\\theta}(x\\_{t},t)\\\\approx\\\\nabla\\_{x\\_{t}}\\\\log q(x\\_{t})=-\\\\frac{\\\\epsilon\\_{\\\\theta}(x\\_{t},t)}{\\\\",
          "original_query": "Extracting training data from diffusion models",
          "cleaned_query": "Extracting training data from diffusion models",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity",
          "url": "https://arxiv.org/abs/2503.10587",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2503.10587** (cs)\n\n\\[Submitted on 13 Mar 2025\\]\n\n# Title:The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity\n\nAuthors: [Justin Sahs](https://arxiv.org/search/cs?searchtype=author&query=Sahs,+J), [Ryan Pyle](https://arxiv.org/search/cs?searchtype=author&query=Pyle,+R), [Fabio Anselmi](https://arxiv.org/search/cs?searchtype=author&query=Anselmi,+F), [Ankit Patel](https://arxiv.org/search/cs?searchtype=author&query=Patel,+A)\n\nView a PDF of the paper titled The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity, by Justin Sahs and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.10587) [HTML (experimental)](https://arxiv.org/html/2503.10587v1)\n\n> Abstract:Despite classical statistical theory predicting severe overfitting, modern massively overparameterized neural networks still generalize well. This unexpected property is attributed to the network's so-called implicit bias, which describes its propensity to converge to solutions that generalize effectively, among the many possible that correctly label the training data. The aim of our research is to explore this bias from a new perspective, focusing on how non-linear activation functions contribute to shaping it. First, we introduce a reparameterization which removes a continuous weight rescaling symmetry. Second, in the kernel regime, we leverage this reparameterization to generalize recent findings that relate shallow Neural Networks to the Radon transform, deriving an explicit formula for the implicit bias induced by a broad class of activation functions. Specifically, by utilizing the connection between the Radon transform and the Fourier transform, we interpret the kernel regime's inductive bias as minimizing a spectral seminorm that penalizes high-frequency components, in a manner dependent on the activation function. Finally, in the adaptive regime, we demonstrate the existence of local dynamical attractors that facilitate the formation of clusters of hyperplanes where the input to a neuron's activation function is zero, yielding alignment between many neurons' response functions. We confirm these theoretical results with simulations. All together, our work provides a deeper understanding of the mechanisms underlying the generalization capabilities of overparameterized neural networks and its relation with the implicit bias, offering potential pathways for designing more efficient and robust models.\n\n| | |\n| --- | --- |\n| Comments: | 18 pages, 10 figures in main text |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2503.10587](https://arxiv.org/abs/2503.10587) \\[cs.LG\\] |\n| (or [arXiv:2503.10587v1](https://arxiv.org/abs/2503.10587v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.10587](https://doi.org/10.48550/arXiv.2503.10587) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Justin Sahs \\[ [view email](https://arxiv.org/show-email/2546b6dc/2503.10587)\\] **\\[v1\\]**\nThu, 13 Mar 2025 17:36:46 UTC (9,455 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity, by Justin Sahs and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.10587)\n- [HTML (experimental)](https://arxiv.org/html/2503.10587v1)\n- [TeX Source](https://arxiv.org/src/2503.10587)\n- [Other Formats](https://arxiv.org/format/2503.10587)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2503.10587&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2503.10587&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-03](https://arxiv.org/list/cs.LG/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.10587?context=cs) [cs.AI](https://arxiv.org/abs/2503.10587?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.10587)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.10587)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.10587)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.10587) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "On the spectral bias of neural networks",
          "cleaned_query": "On the spectral bias of neural networks",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Regime-Aware Noise Scheduling via Speciation/Collapse Predictors\nDevelop an adaptive diffusion noise schedule that explicitly targets the \u201cspeciation time\u201d and delays/avoids the \u201ccollapse time\u201d identified in high-dimensional theory. Implement a scheduler that monitors spectral statistics of batch covariance (speciation proxy) and an excess-entropy surrogate (collapse proxy) to adjust timestep spacing and SNR, then test whether it improves sample diversity without sacrificing fidelity.",
        "Score-Matching Diagnostics for Detecting Onset of Training-Point Collapse\nBuild a practical diagnostic that estimates when a trained diffusion model\u2019s reverse dynamics begins to over-attract toward individual training points. Use Hyv\u00e4rinen-style score matching identities to compute tractable per-timestep curvature/divergence statistics (from the model score network) and correlate these with memorization/extraction success rates, yielding an actionable early-warning metric during training and fine-tuning.",
        "Spectral-Bias\u2013Controlled Score Networks to Suppress Memorization\nDesign score-network parameterizations/activations that enforce a tunable frequency penalty consistent with the activation-dependent spectral seminorm characterized for shallow nets. Train diffusion score models with controlled nonlinearities (or explicit spectral regularizers derived from that theory) and measure the impact on (i) collapse phase strength, (ii) reconstruction of high-frequency details, and (iii) vulnerability to FineXtract-style extraction attacks.",
        "Random-Feature Score Approximation for Fast, High-Dimensional Diffusion\nReplace parts of the score network with Rahimi\u2013Recht random Fourier features to approximate shift-invariant components of the score field, enabling much cheaper training/inference in very high dimensions. Concretely, implement a hybrid score model where early/noisy timesteps use random-feature linear heads (kernelized score estimation) and later timesteps use a smaller NN, then benchmark speed\u2013quality\u2013likelihood tradeoffs.",
        "Extrapolated Guidance Auditing: Quantifying Leakage from Fine-Tuning Trajectories\nExtend FineXtract by modeling fine-tuning as a continuous path in score space and estimating \u201cextrapolatability\u201d from a sequence of checkpoints rather than only pre/post. Fit a parametric interpolation/extrapolation model to intermediate score fields, derive an audit score that predicts how many training images are extractable, and validate it across LoRA/DreamBooth/textual-inversion settings.",
        "Diffusion with Anti-Collapse Repulsion Using Training-Point Energy Barriers\nIntroduce a modification to the reverse SDE/ODE that adds a repulsive term around training examples (or their learned embeddings) once the dynamics approaches the collapse regime. Implement this as a controllable guidance term (analogous to classifier-free guidance but data-repulsive), and evaluate whether it reduces memorization/extraction while preserving generative performance.",
        "Score-Matching\u2013Based Membership Inference for Personalized Diffusion Models\nCreate a membership test that avoids direct image reconstruction: given a candidate image, evaluate per-timestep score residuals and Hyv\u00e4rinen objective terms under the fine-tuned model vs the base model. Calibrate a likelihood-free membership statistic that predicts whether the image was in the personalization set, and compare against reconstruction-based evidence (e.g., FineXtract) for privacy auditing.",
        "Activation-Dependent Phase Diagrams: Linking Network Nonlinearities to Diffusion Regimes\nSystematically map how score-network activation choices shift the speciation and collapse transitions predicted by statistical-physics analysis. Train matched diffusion models differing only in activation (ReLU/GELU/sine/softplus, etc.), estimate their regime transition points empirically (spectral and entropy proxies), and produce actionable design rules connecting activation-induced spectral bias to diffusion dynamical phases and generalization."
      ],
      "generated_ideas_raw": "1. **Regime-Aware Noise Scheduling via Speciation/Collapse Predictors**  \n   Develop an adaptive diffusion noise schedule that explicitly targets the \u201cspeciation time\u201d and delays/avoids the \u201ccollapse time\u201d identified in high-dimensional theory. Implement a scheduler that monitors spectral statistics of batch covariance (speciation proxy) and an excess-entropy surrogate (collapse proxy) to adjust timestep spacing and SNR, then test whether it improves sample diversity without sacrificing fidelity.\n\n2. **Score-Matching Diagnostics for Detecting Onset of Training-Point Collapse**  \n   Build a practical diagnostic that estimates when a trained diffusion model\u2019s reverse dynamics begins to over-attract toward individual training points. Use Hyv\u00e4rinen-style score matching identities to compute tractable per-timestep curvature/divergence statistics (from the model score network) and correlate these with memorization/extraction success rates, yielding an actionable early-warning metric during training and fine-tuning.\n\n3. **Spectral-Bias\u2013Controlled Score Networks to Suppress Memorization**  \n   Design score-network parameterizations/activations that enforce a tunable frequency penalty consistent with the activation-dependent spectral seminorm characterized for shallow nets. Train diffusion score models with controlled nonlinearities (or explicit spectral regularizers derived from that theory) and measure the impact on (i) collapse phase strength, (ii) reconstruction of high-frequency details, and (iii) vulnerability to FineXtract-style extraction attacks.\n\n4. **Random-Feature Score Approximation for Fast, High-Dimensional Diffusion**  \n   Replace parts of the score network with Rahimi\u2013Recht random Fourier features to approximate shift-invariant components of the score field, enabling much cheaper training/inference in very high dimensions. Concretely, implement a hybrid score model where early/noisy timesteps use random-feature linear heads (kernelized score estimation) and later timesteps use a smaller NN, then benchmark speed\u2013quality\u2013likelihood tradeoffs.\n\n5. **Extrapolated Guidance Auditing: Quantifying Leakage from Fine-Tuning Trajectories**  \n   Extend FineXtract by modeling fine-tuning as a continuous path in score space and estimating \u201cextrapolatability\u201d from a sequence of checkpoints rather than only pre/post. Fit a parametric interpolation/extrapolation model to intermediate score fields, derive an audit score that predicts how many training images are extractable, and validate it across LoRA/DreamBooth/textual-inversion settings.\n\n6. **Diffusion with Anti-Collapse Repulsion Using Training-Point Energy Barriers**  \n   Introduce a modification to the reverse SDE/ODE that adds a repulsive term around training examples (or their learned embeddings) once the dynamics approaches the collapse regime. Implement this as a controllable guidance term (analogous to classifier-free guidance but data-repulsive), and evaluate whether it reduces memorization/extraction while preserving generative performance.\n\n7. **Estimating Speciation Time from Learned Representations Instead of Raw Data**  \n   Paper 4 ties speciation to the spectrum of the data correlation matrix; extend this to modern pipelines where diffusion operates in latent spaces (e.g., autoencoder latents). Compute spectral predictors on (i) raw pixels, (ii) encoder latents, and (iii) intermediate feature spaces, and test which spectrum best predicts speciation/collapse boundaries and downstream diversity across datasets.\n\n8. **Nonequilibrium-Thermodynamics View of Fine-Tuning: Measuring Irreversibility and Leakage**  \n   Using the forward \u201cstructure-destroying\u201d diffusion perspective (Paper 1), define thermodynamic quantities (e.g., entropy production / irreversibility proxies along the denoising trajectory) for pretrained vs personalized checkpoints. Hypothesize and test that unusually low irreversibility (or abrupt changes under fine-tuning) correlates with higher training data extractability, yielding a physics-inspired leakage indicator.\n\n9. **Score-Matching\u2013Based Membership Inference for Personalized Diffusion Models**  \n   Create a membership test that avoids direct image reconstruction: given a candidate image, evaluate per-timestep score residuals and Hyv\u00e4rinen objective terms under the fine-tuned model vs the base model. Calibrate a likelihood-free membership statistic that predicts whether the image was in the personalization set, and compare against reconstruction-based evidence (e.g., FineXtract) for privacy auditing.\n\n10. **Activation-Dependent Phase Diagrams: Linking Network Nonlinearities to Diffusion Regimes**  \n   Systematically map how score-network activation choices shift the speciation and collapse transitions predicted by statistical-physics analysis. Train matched diffusion models differing only in activation (ReLU/GELU/sine/softplus, etc.), estimate their regime transition points empirically (spectral and entropy proxies), and produce actionable design rules connecting activation-induced spectral bias to diffusion dynamical phases and generalization.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Regime-Aware Noise Scheduling via Speciation/Collapse Predictors\nDevelop an adaptive diffusion noise schedule that explicitly targets the \u201cspeciation time\u201d and delays/avoids the \u201ccollapse time\u201d identi",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Score-Matching Diagnostics for Detecting Onset of Training-Point Collapse\nBuild a practical diagnostic that estimates when a trained diffusion model\u2019s reverse dynamics begins to over-attract toward in",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Spectral-Bias\u2013Controlled Score Networks to Suppress Memorization\nDesign score-network parameterizations/activations that enforce a tunable frequency penalty consistent with the activation-dependent sp",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Random-Feature Score Approximation for Fast, High-Dimensional Diffusion\nReplace parts of the score network with Rahimi\u2013Recht random Fourier features to approximate shift-invariant components of the sc",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Extrapolated Guidance Auditing: Quantifying Leakage from Fine-Tuning Trajectories\nExtend FineXtract by modeling fine-tuning as a continuous path in score space and estimating \u201cextrapolatability\u201d from ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Diffusion with Anti-Collapse Repulsion Using Training-Point Energy Barriers\nIntroduce a modification to the reverse SDE/ODE that adds a repulsive term around training examples (or their learned embedd",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Score-Matching\u2013Based Membership Inference for Personalized Diffusion Models\nCreate a membership test that avoids direct image reconstruction: given a candidate image, evaluate per-timestep score resid",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Activation-Dependent Phase Diagrams: Linking Network Nonlinearities to Diffusion Regimes\nSystematically map how score-network activation choices shift the speciation and collapse transitions predicted",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 10,
      "paper_title": "Adjoint Schr\u00f6dinger Bridge Sampler",
      "contribution": "Combines Schr\u00f6dinger-bridge stochastic optimal control with adjoint matching to learn scalable, importance-weight-free diffusion samplers that transport arbitrary source distributions to unnormalized energy-defined targets.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "input_tokens": 13504,
      "output_tokens": 1229,
      "predecessor_details": [
        {
          "success": true,
          "title": "Denoising diffusion probabilistic models | Proceedings of the 34th ...",
          "url": "https://dl.acm.org/doi/abs/10.5555/3495724.3496298",
          "content": "[skip to main content](https://dl.acm.org/dl.acm.org#skip-to-main-content)\n\nContents\n\n## Abstract\n\nWe present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.\n\n## Formats available\n\nYou can view the full content in the following formats:\n\n[PDF](https://dl.acm.org/doi/pdf/10.5555/3495724.3496298)\n\n## Supplementary Material\n\nSupplemental material.\n\n- [Download](https://dl.acm.org/doi/suppl/10.5555/3495724.3496298/suppl_file/3495724.3496298_supp.pdf)\n- 63.91 MB\n\n## References\n\n\\[1\\]\n\nGuillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and Pascal Vincent. GSNs: generative stochastic networks. _Information and Inference: A Journal of the IMA_, 5(2):210\u2013249, 2016.\n\n[Crossref](https://doi.org/10.1093/imaiai/iaw003)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1093%2Fimaiai%2Fiaw003)\n\n\\[2\\]\n\nFlorian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion training. In _International Conference on Learning Representations_, 2017.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Florian+Bordes%2C+Sina+Honari%2C+and+Pascal+Vincent.+Learning+to+generate+samples+from+noise+through+infusion+training.+In+International+Conference+on+Learning+Representations%2C+2017.)\n\n\\[3\\]\n\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In _International Conference on Learning Representations_, 2019.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Andrew+Brock%2C+Jeff+Donahue%2C+and+Karen+Simonyan.+Large+scale+GAN+training+for+high+fidelity+natural+image+synthesis.+In+International+Conference+on+Learning+Representations%2C+2019.)\n\n\\[4\\]\n\nTong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua Bengio. Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling. _arXiv preprint arXiv:2003.06060_, 2020.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Tong+Che%2C+Ruixiang+Zhang%2C+Jascha+Sohl-Dickstein%2C+Hugo+Larochelle%2C+Liam+Paull%2C+Yuan+Cao%2C+and+Yoshua+Bengio.+Your+GAN+is+secretly+an+energy-based+model+and+you+should+use+discriminator+driven+latent+sampling.+arXiv+preprint+arXiv%3A2003.06060%2C+2020.)\n\n\\[5\\]\n\nTian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In _Advances in Neural Information Processing Systems_, pages 6571\u20136583, 2018.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Tian+Qi+Chen%2C+Yulia+Rubanova%2C+Jesse+Bettencourt%2C+and+David+K+Duvenaud.+Neural+ordinary+differential+equations.+In+Advances+in+Neural+Information+Processing+Systems%2C+pages+6571%E2%80%936583%2C+2018.)\n\n\\[6\\]\n\nXi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved autoregres-sive generative model. In _International Conference on Machine Learning_, pages 863\u2013871, 2018.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Xi+Chen%2C+Nikhil+Mishra%2C+Mostafa+Rohaninejad%2C+and+Pieter+Abbeel.+PixelSNAIL%3A+An+improved+autoregres-sive+generative+model.+In+International+Conference+on+Machine+Learning%2C+pages+863%E2%80%93871%2C+2018.)\n\n\\[7\\]\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Rewon+Child%2C+Scott+Gray%2C+Alec+Radford%2C+and+Ilya+Sutskever.+Generating+long+sequences+with+sparse+transformers.+arXiv+preprint+arXiv%3A1904.10509%2C+2019.)\n\n\\[8\\]\n\nYuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc'Aurelio Ranzato. Residual energy-based models for text generation. _arXiv preprint arXiv:2004.11714_, 2020.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Yuntian+Deng%2C+Anton+Bakhtin%2C+Myle+Ott%2C+Arthur+Szlam%2C+and+Marc%27Aurelio+Ranzato.+Residual+energy-based+models+for+text+generation.+arXiv+preprint+arXiv%3A2004.11714%2C+2020.)\n\n\\[9\\]\n\nLaurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. _arXiv preprint arXiv:1410.8516_, 2014.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Laurent+Dinh%2C+David+Krueger%2C+and+Yoshua+Bengio.+NICE%3A+Non-linear+independent+components+estimation.+arXiv+preprint+arXiv%3A1410.8516%2C+2014.)\n\n\\[10\\]\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. _arXiv preprint arXiv:1605.08803_, 2016.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Laurent+Dinh%2C+Jascha+Sohl-Dickstein%2C+and+Samy+Bengio.+Density+estimation+using+Real+NVP.+arXiv+preprint+arXiv%3A1605.08803%2C+2016.)\n\n\\[11\\]\n\nYilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In _Advances in Neural Information Processing Systems_, pages 3603\u20133613, 2019.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Yilun+Du+and+Igor+Mordatch.+Implicit+generation+and+modeling+with+energy+based+models.+In+Advances+in+Neural+Information+Processing+Systems%2C+pages+3603%E2%80%933613%2C+2019.)\n\n\\[12\\]\n\nRuiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative ConvNets via multi-grid modeling and sampling. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 9155\u20139164, 2018.\n\n[Crossref](https://doi.org/10.1109/CVPR.2018.00954)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109%2FCVPR.2018.00954)\n\n\\[13\\]\n\nRuiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow contrastive estimation of energy-based models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7518\u20137528, 2020.\n\n[Crossref](https://doi.org/10.1109/CVPR42600.2020.00754)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109%2FCVPR42600.2020.00754)\n\n\\[14\\]\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In _Advances in Neural Information Processing Systems_, pages 2672\u20132680, 2014.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Ian+Goodfellow%2C+Jean+Pouget-Abadie%2C+Mehdi+Mirza%2C+Bing+Xu%2C+David+Warde-Farley%2C+Sherjil+Ozair%2C+Aaron+Courville%2C+and+Yoshua+Bengio.+Generative+adversarial+nets.+In+Advances+in+Neural+Information+Processing+Systems%2C+pages+2672%E2%80%932680%2C+2014.)\n\n\\[15\\]\n\nAnirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net. In _Advances in Neural Information Processing Systems_, pages 4392\u20134402, 2017.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Anirudh+Goyal%2C+Nan+Rosemary+Ke%2C+Surya+Ganguli%2C+and+Yoshua+Bengio.+Variational+walkback%3A+Learning+a+transition+operator+as+a+stochastic+recurrent+net.+In+Advances+in+Neural+Information+Processing+Systems%2C+pages+4392%E2%80%934402%2C+2017.)\n\n\\[16\\]\n\nWill Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. FFJORD: Free-form continuous dynamics for scalable reversible generative models. In _International Conference on Learning Representations_, 2019.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Will+Grathwohl%2C+Ricky+T.+Q.+Chen%2C+Jesse+Bettencourt%2C+and+David+Duvenaud.+FFJORD%3A+Free-form+continuous+dynamics+for+scalable+reversible+generative+models.+In+International+Conference+on+Learning+Representations%2C+2019.)\n\n\\[17\\]\n\nWill Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In _International Conference on Learning Representations_, 2020.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Will+Grathwohl%2C+Kuan-Chieh+Wang%2C+Joern-Henrik+Jacobsen%2C+David+Duvenaud%2C+Mohammad+Norouzi%2C+and+Kevin+Swersky.+Your+classifier+is+secretly+an+energy+based+model+and+you+should+treat+it+like+one.+In+International+Conference+on+Learning+Representations%2C+2020.)\n\n\\[18\\]\n\nKarol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In _Advances In Neural Information Processing Systems_, pages 3549\u20133557, 2016.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Karol+Gregor%2C+Frederic+Besse%2C+Danilo+Jimenez+Rezende%2C+Ivo+Danihelka%2C+and+Daan+Wierstra.+Towards+conceptual+compression.+In+Advances+In+Neural+Information+Processing+Systems%2C+pages+3549%E2%80%933557%2C+2016.)\n\n\\[19\\]\n\nPrahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In _Twenty-Second Annual IEEE Conference on Computational Complexity (CCC'07)_, pages 10\u201323. IEEE, 2007.\n\n[Google Scholar](https://scholar.google.com/scholar?q=Prahladh+Harsha%2C+Rahul+Jain%2C+David+McAllester%2C+and+Jaikumar+Radhakrishnan.+The+communication+complexity+of+correlation.+In+Twenty-Second+Annual+IEEE+Conference+on+Computational+Complexity+%28CCC%2707%29%2C+pages+10%E2%80%9323.+IEEE%2C+2007.)\n\n\\[20\\]\n\nMarton Havasi, Robert Peharz, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. In _International Conference on Learning Representations_, ",
          "original_query": "Denoising Diffusion Probabilistic Models",
          "cleaned_query": "Denoising Diffusion Probabilistic Models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Score-Based Generative Modeling through Stochastic Differential ...",
          "url": "https://arxiv.org/abs/2011.13456",
          "content": "[2011.13456] Score-Based Generative Modeling through Stochastic Differential Equations[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2011.13456\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2011.13456**(cs)\n[Submitted on 26 Nov 2020 ([v1](https://arxiv.org/abs/2011.13456v1)), last revised 10 Feb 2021 (this version, v2)]\n# Title:Score-Based Generative Modeling through Stochastic Differential Equations\nAuthors:[Yang Song](https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+Y),[Jascha Sohl-Dickstein](https://arxiv.org/search/cs?searchtype=author&amp;query=Sohl-Dickstein,+J),[Diederik P. Kingma](https://arxiv.org/search/cs?searchtype=author&amp;query=Kingma,+D+P),[Abhishek Kumar](https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+A),[Stefano Ermon](https://arxiv.org/search/cs?searchtype=author&amp;query=Ermon,+S),[Ben Poole](https://arxiv.org/search/cs?searchtype=author&amp;query=Poole,+B)\nView a PDF of the paper titled Score-Based Generative Modeling through Stochastic Differential Equations, by Yang Song and 4 other authors\n[View PDF](https://arxiv.org/pdf/2011.13456)> > Abstract:\n> Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model. Comments:|ICLR 2021 (Oral)|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2011.13456](https://arxiv.org/abs/2011.13456)[cs.LG]|\n|(or[arXiv:2011.13456v2](https://arxiv.org/abs/2011.13456v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2011.13456](https://doi.org/10.48550/arXiv.2011.13456)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Yang Song [[view email](https://arxiv.org/show-email/18c3b4ae/2011.13456)]\n**[[v1]](https://arxiv.org/abs/2011.13456v1)**Thu, 26 Nov 2020 19:39:10 UTC (32,781 KB)\n**[v2]**Wed, 10 Feb 2021 18:17:04 UTC (56,849 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Score-Based Generative Modeling through Stochastic Differential Equations, by Yang Song and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2011.13456)\n* [TeX Source](https://arxiv.org/src/2011.13456)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2011.13456&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2011.13456&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-11](https://arxiv.org/list/cs.LG/2020-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2011.13456?context=cs)\n[stat](https://arxiv.org/abs/2011.13456?context=stat)\n[stat.ML](https://arxiv.org/abs/2011.13456?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2011.13456)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2011.13456)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2011.13456)\n### [3 blog links](https://arxiv.org/tb/2011.13456)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2011.html#abs-2011-13456)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2011-13456)\n[Yang Song]()\n[Jascha Sohl-Dickstein]()\n[Diederik P. Kingma]()\n[Abhishek Kumar]()\n[Stefano Ermon]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2011.13456)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Score-Based Generative Modeling through Stochastic Differential Equations",
          "cleaned_query": "Score-Based Generative Modeling through Stochastic Differential Equations",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[1308.0215] A survey of the Schr\u00f6dinger problem and some of its ...",
          "url": "https://arxiv.org/abs/1308.0215",
          "content": "# Mathematics > Probability\n\n**arXiv:1308.0215** (math)\n\n\\[Submitted on 1 Aug 2013\\]\n\n# Title:A survey of the Schr\u00f6dinger problem and some of its connections with optimal transport\n\nAuthors: [Christian L\u00e9onard](https://arxiv.org/search/math?searchtype=author&query=L%C3%A9onard,+C) (MODAL'X)\n\nView a PDF of the paper titled A survey of the Schr\\\\\"odinger problem and some of its connections with optimal transport, by Christian L\\\\'eonard (MODAL'X)\n\n[View PDF](https://arxiv.org/pdf/1308.0215)\n\n> Abstract:This article is aimed at presenting the Schr\u00f6dinger problem and some of its connections with optimal transport. We hope that it can be used as a basic user's guide to Schr\u00f6dinger problem. We also give a survey of the related literature. In addition, some new results are proved.\n\n| | |\n| --- | --- |\n| Comments: | To appear in Discrete \\\\& Continuous Dynamical Systems - Series A. Special issue on optimal transport |\n| Subjects: | Probability (math.PR); Functional Analysis (math.FA); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:1308.0215](https://arxiv.org/abs/1308.0215) \\[math.PR\\] |\n| (or [arXiv:1308.0215v1](https://arxiv.org/abs/1308.0215v1) \\[math.PR\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1308.0215](https://doi.org/10.48550/arXiv.1308.0215) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Discrete Contin. Dyn. Syst. A, 2014, 34(4): 1533-1574 |\n\n## Submission history\n\nFrom: Christian Leonard \\[ [view email](https://arxiv.org/show-email/6240ebf0/1308.0215)\\]\u00a0\\[via CCSD proxy\\] **\\[v1\\]**\nThu, 1 Aug 2013 14:16:38 UTC (45 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled A survey of the Schr\\\\\"odinger problem and some of its connections with optimal transport, by Christian L\\\\'eonard (MODAL'X)\n\n- [View PDF](https://arxiv.org/pdf/1308.0215)\n- [TeX Source](https://arxiv.org/src/1308.0215)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.PR\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1308.0215&function=prev&context=math.PR)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1308.0215&function=next&context=math.PR)\n\n[new](https://arxiv.org/list/math.PR/new) \\| [recent](https://arxiv.org/list/math.PR/recent) \\| [2013-08](https://arxiv.org/list/math.PR/2013-08)\n\nChange to browse by:\n\n[math](https://arxiv.org/abs/1308.0215?context=math) [math.FA](https://arxiv.org/abs/1308.0215?context=math.FA) [math.OC](https://arxiv.org/abs/1308.0215?context=math.OC)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1308.0215)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1308.0215)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1308.0215)\n\n### [1 blog link](https://arxiv.org/tb/1308.0215)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1308.0215) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "A survey of the Schr\u00f6dinger problem and its links with optimal transport",
          "cleaned_query": "A survey of the Schr\u00f6dinger problem and its links with optimal transport",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Schr\u00f6dinger Bridge Samplers - Bayeslab",
          "url": "https://bayeslab.unibocconi.eu/sites/default/files/media/attach/ssb_pres.pdf",
          "content": "Schr\u00a8odinger Bridge Samplers\nEspen Bernton\nColumbia University\nJoint work with J. Heng, A. Doucet, P. E. Jacob\nJB\u02c63, July 9, 2020\nSchr\u00a8odinger Bridge Samplers\n(+ a note on exchangeability and optimal transport)\nEspen Bernton\nColumbia University\nJoint work with J. Heng, A. Doucet, P. E. Jacob\n(+ joint work with P. Ghosal, M. Nutz)\nJB\u02c63, July 9, 2020\nOutline\nI Problem setup and Monte Carlo\nI The Schr\u00a8odinger bridge problem\nI Sequential Schr\u00a8odinger bridge sampling\nI Examples and numerical experiments\nI Conclusion and future directions\nProblem setup\nSuppose that \u03c0T is a Lebesgue density on E = R\nd\n, expressed\n\u03c0T (x) = \u03b3T (x)\nZT\n, ZT =\nZ\nE\n\u03b3T (x)dx.\nWe want to calculate\nI expectations with respect to \u03c0T ,\nI the unknown normalizing constant ZT .\nCan only evaluate \u03b3T (and later, \u2207 log \u03b3T ) pointwise.\nA stylized Monte Carlo problem\nSuppose we can sample x0 from and evaluate the density of \u03c00.\nChoose and sample a Markov kernel xT \u223c MT (x0, dxT ) such that\nqT = L(xT ) is closer to \u03c0T than \u03c00.\nWe want to use qT as the proposal in importance sampling.\nTwo challenges:\n1. How do we choose MT ?\n2. The density of qT is typically intractable.\nA stylized Monte Carlo problem\nTwo challenges:\n1. How do we choose MT ?\n2. The density of qT is typically intractable.\nSecond challenge\nExtend the domain of integration to E\n2\n:\nI Define Q(dx0, dxT ) = \u03c00(dx0)MT (x0, dxT ).\nI Choose an auxiliary \u201cbackward\u201d kernel L0 and define the\nauxiliary target P(dx0, dxT ) = \u03c0T (dxT )L0(xT , dx0),\nsuch that P \u001c Q and w0,T (x0, xT ) = dL0\u2297\u03b3T\nd\u03c00\u2297MT\n(x0, xT ) can be\nevaluated pointwise.\nIf (x\nn\n0\n, xn\nT\n) \u223c Q and w\nn\n0,T = w0,T (x\nn\n0\n, xn\nT\n), then \bx\nn\nT\n, wn\n0,T N\nn=1\nI is a weighted sample from \u03c0T , and\nI Z\u02c6\nT = N \u22121 PN\nn=1 w\nn\n0,T is an unbiased estimator of ZT .\nFirst challenge\nMain idea: Approximate M?\nT\ncorresponding to the Schr\u00a8odinger\nbridge between \u03c00 and \u03c0T for a class of kernels.\nThe Schr\u00a8odinger bridge problem\nGiven a reference distribution Q(dx0, dxT ) and marginal\nconstraints \u03c00 and \u03c0T , find\nS(dx0, dxT ) = argmin\nh0=\u03c00,hT =\u03c0T\nKL(H|Q),\nRemark:\nConsider Q\u03c8(dx0, dxT ) = \u03c00(dx0)M\u03c8\nT\n(x0, dxT ), where \u03c8 is a\nstrictly positive function, or policy, and\nM\u03c8\nT\n(x0, dxT ) = \u03c8(xT )MT (x0, dxT )\nR\nE\n\u03c8(xT )MT (x0, dxT )\n.\nThen, S(dx0, dxT ) = Q\u03c8\n?\n(dx0, dxT ), where \u03c8?is the solution to a\nSchr\u00a8odinger equation.\nSome notes\nI Original formulation by Schr\u00a8odinger in 1931: gas with very\nlarge number of particles N.\nI The modern formulation is derived by a large deviations\nprinciple as N \u2192 \u221e, where the KL is the rate functional.\nI Connection to optimal transport: Suppose Schr\u00a8odinger\u2019s\nparticles are Brownian with scale \u03c3, denoted Q\u03c3, then\nlim\u03c3\u21920\u03c3\n2KL(S\u03c3\n|Q\n\u03c3\n) = inf\n\u03b30=\u03c00,\u03b3T =\u03c0T\nZ\nE2\nkx0 \u2212 xT k\n2\n\u03b3(dx0, dxT )\n= W2\n2\n(\u03c00, \u03c0T ).\nI Important in computation, idea behind entropically\nregularized optimal transport (Cuturi, 2013).\nI We will use a formulation from optimal control which is\namenable to computation (Heng et al., 2019).\nHigh-level algorithm to compute S(dx0, dxT )\nIterative proportional fitting (or Sinkhorn\u2019s algorithm):\nLet Q(0) = Q, and for i \u2265 1, define\nP\n(i)\n(dx0, dxT ) = argmin\nhT =\u03c0T\nKL(H|Q\n(i\u22121)),\nQ\n(i)\n(dx0, dxT ) = argmin\nh0=\u03c00\nKL(H|P\n(i)\n).\nLet S\n(2i+1) = P(i+1) and S(2i) = Q(i)\nfor any i \u2265 0.\nRemark: Given Q as the reference, P\n(1) is the optimal auxiliary\ntarget in the sense of Del Moral et al. (2006).\nConvergence of iterative proportional fitting\nR\u00a8uschendorf (1995) shows that if there exists c > 0 such that\nMT (x0, dxT ) \u2265 c\u03c0T (dxT ), for \u03c00-a.e. x0 \u2208 E,\nthen S(i) converges to S in KL and TV as i \u2192 \u221e.\nProposition: For any \u03b5 > 0, IPF returns an S\n(i)\nthat satisfies\nKL(\u03c00|s\n(i)\n0\n) + KL(\u03c0T |s\n(i)\nT\n) < \u03b5\nin fewer than dKL(S|Q)/\u03b5e iterations.\nIPF as policy refinement\nUsing the \u03c8-parameterization, it turns out that we can express\nQ\n(i) = Q\u03c8\n(i)\n,\nfor two sequences \u03c8\n(i) and \u03c6(i)\n, satisfying\n\u03c6\n(i)\n(xT ) = d\u03c0T\ndq\n\u03c8(i\u22121)\nT\n(xT ), \u03c8(i) = \u03c8\n(i\u22121)\n\u00b7 \u03c6\n(i)\n.\nThe sequence \u03c8\n(i) \u2192 \u03c8? as i \u2192 \u221e.\nIPF as policy refinement\nFor any H \u001c Q\u03c8 such that hT = \u03c0T , we have that\nd\u03c0T\ndq\n\u03c8\nT\n(xT ) = Z\nE\ndH\ndQ\u03c8\n(x0, xT )Q\n\u03c8\n(dx0|xT ).\nIf (x0, xT ) \u223c Q\u03c8, then, conditional on xT , we have x0 \u223c Q\u03c8(dx0|xT ).\nThus, if H(dx0, dxT ) = \u03c0T (dxT )L\n\u03c8\n0\n(xT , dx0), then w\n\u03c8\n0,T (x0, xT ) is an\nunbiased estimator of d\u03c0T\ndq\n\u03c8\nT\n(xT ).\nI Can borrow ideas from conditional SMC to reduce variance.\nApproximate IPF\nGiven \b(x\nn\n0\n, xn\nT\n)\n N\nn=1 \u223c Q\u03c8\u02c6(i\u22121) , approximate \u03c6\n(i) with\n\u03c6\u02c6(i) = argmin\nf\u2208F\nX\nN\nn=1\n\f\n\f\n\flog f(x\nn\nT\n) \u2212 log R\n\u03c8\u02c6(i\u22121) (x\nn\nT\n)\n\f\n\f\n\f\n2\n,\nI F is a function class,\nI R\u03c8\u02c6(i\u22121) (xT ) is an estimator of d\u03c0T\ndq\n\u03c8\u02c6(i\u22121)\nT\n(xT ).\nChoice of kernels and function classes\nRestrictions: Must be able to\nI sample from Q\u03c8\u02c6(i\u22121) , i.e. sample from M\u03c8\u02c6(i\u22121)\nT\n,\nI evaluate w\n\u03c8\u02c6(i\u22121)\n0,T at the points \b\n(x\nn\n0\n, xn\nT\n)\n N\nn=1 \u223c Q\u03c8\u02c6(i\u22121)\n.\nImportant example:\nI the kernel MT (x0, dxT ) is Gaussian,\nI the function class log F is the quadratic forms,\nI approximate the optimal backward kernel L\n(i)\n0\n, in the sense of\nDel Moral et al. (2006), with similar regressions.\nToy example\nSuppose \u03c00 = N(0, I), \u03c0T = N(\u00b5T , \u03a3T ), where\n\u00b5T = (17.9, 17.9), \u03a3T =\n\u0012\n0.40 0.24\n0.24 0.40\u0013\nLet MT be the kernel arising from an Euler-Maruyama\ndiscretization of the Langevin diffusion\ndXs =\n1\n2\n\u2207 log \u03c0s(Xs)ds + dWs, for s \u2208 [0, \u03c4 ], X0 \u223c \u03c00,\nwhere (\u03c0s)s\u2208[0,\u03c4]is the geometric interpolation of \u03c00 and \u03c0T .\nSuppose we take \u03c4 = 2 and 40 steps of Euler-Maruyama, and i = 5\niterations of IPF.\nToy example: Illustration of first marginal\nSequential Schr\u00a8odinger bridge sampling\nInstead of targeting \u03c0T directly, we introduce an interpolation\n{\u03c0t}\nT\nt=0, for example\n\u03b3t(xt) = \u03c00(xt)\n1\u2212\u03bbt \u03b3T (xt)\u03bbt\n, \u03c0t(xt) = \u03b3t(xt)/Zt,\nwhere {\u03bbt}\nT\nt=0 \u2282 [0, 1] is increasing, \u03bb0 = 0 and \u03bbT = 1.\nIntroduce a sequence of Markov kernels {Mt}\nT\nt=1, and let\nQ(dx0:T ) = \u03c00(dx0)\nY\nT\nt=1\nMt(xt\u22121, dxt).\nSequential Schr\u00a8odinger bridge sampling\nConsider the multi-marginal Schr\u00a8odinger bridge problem:\nS(dx0:T ) = argmin\nht=\u03c0t, \u2200 t\u2208{0,...,T}\nKL(H|Q).\nProposition: Can be solved sequentially. Consider the sequence of\nintermediate problems\nSt\u22121,t(dxt\u22121, dxt) = argmin\nht\u22121=\u03c0t\u22121,ht=\u03c0t\nKL(Ht\u22121,t|Qt\u22121,t),\n= \u03c0t\u22121(dxt\u22121)M\u03c8?\nt\nt\n(xt\u22121, dxt).\nThen, S(dx0:T ) = \u03c00(dx0)\nQT\nt=1 M\u03c8?\nt\nt\n(xt\u22121, dxt), where {\u03c8\n?\nt }\nT\nt=1\nsimilarly solve a set of Schr\u00a8odinger equations.\nAlgorithm\nInitialize {x\nn\n0 }Nn=1 \u223c \u03c00. For each t = 1, . . . , T,\nI Perform i iterations of approximate IPF to obtain\nx\nn\nt \u223c M\n(i)\nt\n(x\nn\nt\u22121\n, dx\nn\nt\n) and\nw\n(i)\nt\u22121,t(x\nn\nt\u22121\n, xn\nt\n) = dL\n(i)\nt\u22121 \u2297 \u03b3t\nd\u03b3t\u22121 \u2297 M\n(i)\nt\n(x\nn\nt\u22121\n, xn\nt\n),\nfor n = 1, . . . , N.\nReturn {(x\nn\nT\n, wn\n0:T\n)}N\nn=1, where w\nn\n0:T =\nQT\nt=1 w\n(i)\nt\u22121,t(x\nn\nt\u22121\n, xn\nt\n).\nOptional: Add resampling steps.\nGeneric choice of kernels\nFor t = 1, . . . , T, let Mt denote the t-th step of the Euler-Maruyama\ndiscretization of Langevin diffusion:\ndXs =\n1\n2\n\u2207 log \u03c0s(Xs)ds + dWs, for s \u2208 [0, \u03c4 ], X0 \u223c \u03c00.\nLet log Ft be the quadratic forms, then M\u03c8\nt\nis Gaussian for every\nt and \u03c8.\nCan similarly approximate the optimal backward kernels using\nquadratic forms.\nSmall step-size regime\nFor sufficiently large \u03c4 and small step size h > 0, qt should provide a\nreasonable approximation of \u03c0t.\nFor small h, we can also leverage flexible function classes by\napproximating the underlying continuous-time SBP:\nM\u03c8\nt\n(xt\u22121, dxt) \u2248 N dxt; xt\u22121 +\nh\n2 \u2207 log \u03c0t(xt\u22121) + h\u2207 log \u03c8t(xt\u22121), hId\n\u0001\n.\nContinuous-time Schr\u00a8odinger bridge problem:\nFind (\u03c8\n?\ns\n)s\u2208[0,\u03c4] such that X0 \u223c \u03c00, X\u03c4 \u223c \u03c0T ,\ndXs =\n1\n2\n\u2207 log \u03c0s(Xs)ds + \u2207 log \u03c8s(Xs)ds + dWs, for s \u2208 [0, \u03c4 ],\nand (\u03c8\n?\ns\n)s\u2208[0,\u03c4] minimizes R \u03c4\n0\nEk\u2207 log \u03c8s(Xs)k\n2ds.\nExample: Linear Quadratic Gaussian\nPrior: \u03c00(dx0) = N (dx0; 0, I).\nLog-likelihood: `(x) = \u2212(y \u2212 x)\n>R\u22121\n(y \u2212 x)/2, observation y \u2208 R\nd\n,\nsymmetric positive definite R \u2208 R\n2\u00d72\n.\nPosterior: \u03c0T (dxT ) = N (dxT ; \u00b5T , \u03a3T ) with \u03a3T =\n\n\u03a3\n\u22121\n0 + R\u22121\n\u0001\u22121\n,\n\u00b5T = \u03a3T\n\n\u03a3\n\u22121\n0 \u00b50 + R\u22121y\n\u0001\n.\nParameters: y = (8, 8)>, R11 = R22 = 1, R12 = R21 = 0.8.\nExample: Linear Quadratic Gaussian\nKernels: Discretized Langevin diffusion with h = 1/20.\nInterpolation: \u03c4 = 2, T = 40, \u03bbt = t/T.\nFunction classes: If f \u2208 Ft, then log f is quadratic.\nExample: Linear Quadratic Gaussian\nPlot: log W2(\u03c0t, q\n(i)\nt\n) as a function of t, for different i \u2265 0.\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u22128\n\u22126\n\u22124\n\u22122\n0\n0 10 20 30 40\ntime index\nlog\u2212distance\niter\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n0\n20\n40\n60\n80\n100\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf\n\u22128\n\u22126\n\u22124\n\u22122\n0\n0 10 20 30 40\ntime index\nlog\u2212distance\niter\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n0\n20\n40\n60\n80\n100\nLeft: Exact IPF. Right: SSB with N = 1, 000.\nExample: Linear Quadratic Gaussian\nComparing the reference sampler with the SSB sampler for\nN = 1, 000,\nI The MSE of log Z\u02c6\nT obtained with reference sampler was\n7396 times higher than the SSB estimator.\nI The wall-clock time consumed by the SSB sampler was\n7.4 times higher than the reference sampler.\nSSB about 1, 000 times more efficient in terms of MSE per unit of\ncomputation time.\nExample: 1D mixture\nTarget distribution: \u03c0T (dxT ) = Pp\ni=1 wi N (dxT ; \u00b5i\n, \u03c32\ni\n).\nParameters: p = 3, \u00b5 = (\u22121.5, 0, 1.5), \u03c3 = (0.6, 0.15, 1.8),\nw = (1/3, 1/3, 1/3).\n\u22124 \u22122 0 2 4\n0.000 0.005 0.010 0.015 0.020\nExample: 1D mixture\nKernels: Discretized Langevin diffusion with h = 1/50.\nInterpolation: \u03c00(dx0) = N (dx0;",
          "original_query": "Sequential Monte Carlo for Schr\u00f6dinger Bridges / Sequential Schr\u00f6dinger Bridge methods",
          "cleaned_query": "Sequential Monte Carlo for Schr\u00f6dinger Bridges",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Feynman\u2019s Path Integral Formulation of Schr\u00f6dinger\u2019s Wave Mechanics",
          "url": "https://link.springer.com/chapter/10.1007/978-3-642-58847-1_13?error=cookies_not_supported&code=12374892-2a93-4cf5-8b4e-a865c93475c7",
          "content": "# Feynman\u2019s Path Integral Formulation of Schr\u00f6dinger\u2019s Wave Mechanics\n\n- Chapter\n\n- pp 361\u2013376\n- [Cite this chapter](https://link.springer.com/chapter/10.1007/978-3-642-58847-1_13?error=cookies_not_supported&code=12374892-2a93-4cf5-8b4e-a865c93475c7#citeas)\n\n[![](https://media.springernature.com/w144/springer-static/cover-hires/book/978-3-642-58847-1?as=webp)Quantum Mechanics](https://link.springer.com/book/10.1007/978-3-642-58847-1)\n\n## Abstract\n\nIn this final chapter we present an alternative formulation of quantum mechanics operating with new mathematical methods: the so-called path integrals. The first step towards such a description were made by Dirac, but the mathematical foundation and beauty was put forward by Feynman. It contributes essentially to a fundamental understanding of quantum mechanics and allows a derivation of exact equations in complex quantum field theory. It should be noted from the beginning that up to now path integral formulations have not played such an important role for solving certain field-theoretical problems; on the one hand, analytical solutions are only possible in very simple cases, and on the other hand, numerical calculations are extremely computer intense. However, path integrals often allow an approximate solution for physical processes, such as phase transitions, where perturbative methods fail.\n\nThis is a preview of subscription content, [log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-642-58847-1_13%3Ferror%3Dcookies_not_supported%26code%3D12374892-2a93-4cf5-8b4e-a865c93475c7) to check access.\n\n## Access this chapter\n\n[Log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-642-58847-1_13%3Ferror%3Dcookies_not_supported%26code%3D12374892-2a93-4cf5-8b4e-a865c93475c7)\n\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n\n## Preview\n\nUnable to display preview.\u00a0[Download preview\\\nPDF.](https://page-one.springer.com/pdf/preview/10.1007/978-3-642-58847-1_13)\n\nUnable to display preview.\u00a0[Download preview\\\nPDF.](https://page-one.springer.com/pdf/preview/10.1007/978-3-642-58847-1_13)\n\n## Notes\n\n1. R.P. Feynman, A.-R. Hibbs: _Quantum Mechanics and Path Integrals_ (McGraw-Hill, New York 1965); C. Itzykson, J.-B. Zuber: _Quantum Field Theory_ (McGraw-Hill, New York 1980); W. Greiner, J. Reinhardt: _Field Quantization_ (Springer, Berlin, Heidelberg 1996).\n\n[MATH](http://www.emis.de/MATH-item?0176.54902) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=Quantum%20Mechanics%20and%20Path%20Integrals&publication_year=1965&author=Feynman%2CRP&author=Hibbs%2CA-R)\n\n2. E. Schr\u00f6dinger: Ann. d. Physik **79**(4) (1926) 361; **79** (4) (1926) 489.\n\n[Article](https://doi.org/10.1002%2Fandp.19263840404) [MATH](http://www.emis.de/MATH-item?JFM%2052.0965.08) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=&journal=Ann.%20d.%20Physik&volume=79&issue=4&publication_year=1926&author=Schr%C3%B6dinger%2CE)\n\n3. See e.g. W. Greiner, J. Reinhardt: _Quantum Electrodynamics_, 2nd ed. (Springer, Berlin, Heidelberg 1994).\n\n[Book](https://link.springer.com/doi/10.1007/978-3-642-88022-3) [MATH](http://www.emis.de/MATH-item?0803.00010) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=Quantum%20Electrodynamics&publication_year=1994&author=Greiner%2CW&author=Reinhardt%2CJ)\n\n\n[Download references](https://citation-needed.springer.com/v2/references/10.1007/978-3-642-58847-1_13?format=refman&flavour=references)\n\n## Author information\n\n### Authors and Affiliations\n\n1. Institut f\u00fcr Theoretische Physik der Johann Wolfgang Goethe-Universit\u00e4t Frankfurt, Postfach 11 19 32, D-60054, Frankfurt am Main, Germany\n\nProfessor Dr. Walter Greiner\n\n2. Robert-Mayer-Strasse 8-10, D-60325, Frankfurt am Main, Germany\n\nProfessor Dr. Walter Greiner\n\n\nAuthors\n\n1. Professor Dr. Walter Greiner\n\n\n[View author publications](https://link.springer.com/search?sortBy=newestFirst&dc.creator=Walter%20Greiner)\n\n\n\n\n\nYou can also search for this author in\n[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Walter%20Greiner) [Google Scholar](http://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Walter%20Greiner%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n\n\n## Rights and permissions\n\n[Reprints and permissions](https://s100.copyright.com/AppDispatchServlet?publisherName=SpringerNature&orderBeanReset=true&orderSource=SpringerLink&title=Feynman%E2%80%99s%20Path%20Integral%20Formulation%20of%20Schr%C3%B6dinger%E2%80%99s%20Wave%20Mechanics&author=Professor%20Dr.%20Walter%20Greiner&contentID=10.1007%2F978-3-642-58847-1_13&copyright=Springer-Verlag%20Berlin%20Heidelberg&publication=eBook&publicationDate=1998&startPage=361&endPage=376&imprint=Springer-Verlag%20Berlin%20Heidelberg)\n\n## Copyright information\n\n\u00a9 1998 Springer-Verlag Berlin Heidelberg\n\n## About this chapter\n\n### Cite this chapter\n\nGreiner, W. (1998). Feynman\u2019s Path Integral Formulation of Schr\u00f6dinger\u2019s Wave Mechanics.\nIn: Quantum Mechanics. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-58847-1\\_13\n\n### Download citation\n\n- [.RIS](https://citation-needed.springer.com/v2/references/10.1007/978-3-642-58847-1_13?format=refman&flavour=citation)\n- [.ENW](https://citation-needed.springer.com/v2/references/10.1007/978-3-642-58847-1_13?format=endnote&flavour=citation)\n- [.BIB](https://citation-needed.springer.com/v2/references/10.1007/978-3-642-58847-1_13?format=bibtex&flavour=citation)\n\n- DOI: https://doi.org/10.1007/978-3-642-58847-1\\_13\n\n- Publisher Name: Springer, Berlin, Heidelberg\n\n- Print ISBN: 978-3-540-60073-2\n\n- Online ISBN: 978-3-642-58847-1\n\n- eBook Packages: [Springer Book Archive](https://metadata.springernature.com/metadata/books)\n\n\n### Share this chapter\n\nAnyone you share the following link with will be able to read this content:\n\nGet shareable link\n\nSorry, a shareable link is not currently available for this article.\n\nCopy to clipboard\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n## Publish with us\n\n[Policies and ethics](https://www.springernature.com/gp/policies/book-publishing-policies)",
          "original_query": "Path-integral / Importance-weighted diffusion samplers for sampling from energy functions",
          "cleaned_query": "Path-integral",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Highly Scalable Diffusion Samplers via Adjoint Matching - arXiv",
          "url": "https://arxiv.org/html/2504.11713",
          "content": "Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching\n1]FAIR at Meta\n2]University of Illinois\n3]New York University\n4]Microsoft Research New England\\\\contribution[\\*]Core contributors\\\\contribution[\u2020]Work done during internship at FAIR\n# Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching\nAaron HavensBenjamin Kurt MillerBing YanCarles Domingo-EnrichAnuroop SriramBrandon WoodDaniel LevineBin HuBrandon AmosBrian KarrerXiang FuGuan-Horng LiuRicky T. Q. Chen[[[[\n###### Abstract\nWe introduce*Adjoint Sampling*, a highly scalable and efficient algorithm for learning diffusion processes that sample from unnormalized densities, or energy functions. It is the first on-policy approach that allows significantly more gradient updates than the number of energy evaluations and model samples, allowing us to scale to much larger problem settings than previously explored by similar methods.\nOur framework is theoretically grounded in stochastic optimal control and shares the same theoretical guarantees as Adjoint Matching, being able to train without the need for corrective measures that push samples towards the target distribution.\nWe show how to incorporate key symmetries, as well as periodic boundary conditions, for modeling molecules in both cartesian and torsional coordinates.\nWe demonstrate the effectiveness of our approach through extensive experiments on classical energy functions, and further scale up to neural network-based energy models where we perform amortized conformer generation across many molecular systems.\nTo encourage further research in developing highly scalable sampling methods, we plan to open source these challenging benchmarks, where successful methods can directly impact progress in computational chemistry.\n\\\\metadata\n[Code &amp; benchmark][https://github.com/facebookresearch/adjoint\\_sampling](https://github.com/facebookresearch/adjoint_sampling)\\\\metadata[Models][https://huggingface.co/facebook/adjoint\\_sampling](https://huggingface.co/facebook/adjoint_sampling)\n## 1Introduction\nSampling from complex, high-dimensional distributions underlies many important problems in computational science, with applications spanning molecular modeling, Bayesian inference , and generative modeling. In particular, we are interested in sampling from the target distribution with only access to its unnormalized energy functionE\ud835\udc38Eitalic\\_E, which defines the Boltzmann distribution\n|\u03bc\u2062(x)=exp\u2061(\u22121\u03c4\u2062E\u2062(x))Z,\ud835\udf07\ud835\udc651\ud835\udf0f\ud835\udc38\ud835\udc65\ud835\udc4d\\\\mu(x)=\\\\frac{\\\\exp\\\\left(-\\\\frac{1}{\\\\tau}E(x)\\\\right)}{Z},italic\\_\u03bc ( italic\\_x ) = divide start\\_ARG roman\\_exp ( - divide start\\_ARG 1 end\\_ARG start\\_ARG italic\\_\u03c4 end\\_ARG italic\\_E ( italic\\_x ) ) end\\_ARG start\\_ARG italic\\_Z end\\_ARG ,||(1)|\nwhereZ=\u222b\u211ddexp\u2061(\u22121\u03c4\u2062E\u2062(x))\u2062d\u2061x&lt;\u221e\ud835\udc4dsubscriptsuperscript\u211d\ud835\udc511\ud835\udf0f\ud835\udc38\ud835\udc65d\ud835\udc65Z=\\\\int\\_{\\\\mathbb{R}^{d}}\\\\exp\\\\left(-\\\\tfrac{1}{\\\\tau}E(x)\\\\right)\\\\operatorname{d\\\\!}%\n{}{x}&lt;&lt;\\\\inftyitalic\\_Z = \u222bstart\\_POSTSUBSCRIPT blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_d end\\_POSTSUPERSCRIPT end\\_POSTSUBSCRIPT roman\\_exp ( - divide start\\_ARG 1 end\\_ARG start\\_ARG italic\\_\u03c4 end\\_ARG italic\\_E ( italic\\_x ) ) start\\_OPFUNCTION roman\\_d end\\_OPFUNCTION italic\\_x &lt;&lt; \u221eis the unknown normalization constant. The Boltzmann distribution describes the equilibrium state of many physical systems, whereE\u2062(x)\ud835\udc38\ud835\udc65E(x)italic\\_E ( italic\\_x )denotes the energy of a configurationx\ud835\udc65xitalic\\_x, and\u03c4&gt;0\ud835\udf0f0\\\\tau&gt;&gt;0italic\\_\u03c4 &gt;&gt; 0is a temperature parameter. Efficiently sampling from such distributions remains challenging, especially for high-dimensional systems with intricate energy landscapes. Additionally many energy functions are extremely computationally expensive, e.g. requiring physics simulations.\nTraditional approaches, such as Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) using well-designed Markov Chains> (Neal, [> 2001\n](https://arxiv.org/html/2504.11713v3#bib.bib58)> ; Neal et\u00a0al., [> 2011\n](https://arxiv.org/html/2504.11713v3#bib.bib59)> ; Del\u00a0Moral et\u00a0al., [> 2006\n](https://arxiv.org/html/2504.11713v3#bib.bib19)> )\n, provide asymptotically unbiased samples but often suffer from slow mixing and poor scalability to high-dimensional settings. This necessitates the design of better transition densities and smarter proposal distributions. Recent works try to address this by\naugmenting sampling with learned proposal distribution> (Albergo et\u00a0al., [> 2019\n](https://arxiv.org/html/2504.11713v3#bib.bib3)> ; Arbel et\u00a0al., [> 2021\n](https://arxiv.org/html/2504.11713v3#bib.bib5)> ; Gabri\u00e9 et\u00a0al., [> 2022\n](https://arxiv.org/html/2504.11713v3#bib.bib27)> )\nvia normalizing flows> (Chen et\u00a0al., [> 2018\n](https://arxiv.org/html/2504.11713v3#bib.bib13)> ; Rezende and Mohamed, [> 2015\n](https://arxiv.org/html/2504.11713v3#bib.bib69)> )\n.\nIt may seem natural to look towards the recent explosion of diffusion and flow-based generative models> (Song and Ermon, [> 2019\n](https://arxiv.org/html/2504.11713v3#bib.bib77)> ; Ho et\u00a0al., [> 2020\n](https://arxiv.org/html/2504.11713v3#bib.bib34)> ; Lipman et\u00a0al., [> 2023\n](https://arxiv.org/html/2504.11713v3#bib.bib49)> ; Albergo et\u00a0al., [> 2023\n](https://arxiv.org/html/2504.11713v3#bib.bib4)> )\n.\nHowever, a na\u00efve adaptation of these data-driven generative modeling frameworks requires access to ground truth data.\nThis limitation is particularly significant in applications such as molecular simulations and physics-based inference, where direct access to samples from the target distribution is often unavailable.\nAs a result, prior attempts often require an augmentation with sequential Monte Carlo or importance-sampling> (Phillips et\u00a0al., [> 2024\n](https://arxiv.org/html/2504.11713v3#bib.bib65)> ; De\u00a0Bortoli et\u00a0al., [> 2024\n](https://arxiv.org/html/2504.11713v3#bib.bib18)> ; Akhound-Sadegh et\u00a0al., [> 2024\n](https://arxiv.org/html/2504.11713v3#bib.bib1)> )\n, making these methods highly inefficient in terms of energy function evaluations.\nThe connection between sampling and diffusion processes was established by> Tzen and Raginsky (\n[> 2019b\n](https://arxiv.org/html/2504.11713v3#bib.bib82)> )\nthrough using classical results of stochastic optimal control (SOC) and Schrodinger-Bridge problems> (Pavon, [> 1989\n](https://arxiv.org/html/2504.11713v3#bib.bib62)> ; Dai\u00a0Pra, [> 1991\n](https://arxiv.org/html/2504.11713v3#bib.bib16)> ; F\u00f6llmer, [> 2005\n](https://arxiv.org/html/2504.11713v3#bib.bib24)> ; Chen et\u00a0al., [> 2016\n](https://arxiv.org/html/2504.11713v3#bib.bib15)> )\n. Using this formulation,> Zhang and Chen (\n[> 2022\n](https://arxiv.org/html/2504.11713v3#bib.bib90)> )\nshow that by directly parameterizing the drift of the controlled process, one could solve an SOC problem given unnormalized density for sampling tasks.\nThis concept is further generalize by> Berner et\u00a0al. (\n[> 2023\n](https://arxiv.org/html/2504.11713v3#bib.bib10)> )\nand> Richter and Berner (\n[> 2024\n](https://arxiv.org/html/2504.11713v3#bib.bib71)> )\n; however, all of these method require computationally expensive simulation of the diffusion process per gradient update. Furthermore, they require at least one\u2014sometimes many\u2014energy evaluations per gradient update.\nTo overcome these challenges, we introduce*Adjoint Sampling*, a novel and extremely efficient variational inference framework based on stochastic control of diffusion processes, which we apply at much larger scale than previous methods.\nOur method is built on top of Adjoint Matching> (Domingo-Enrich et\u00a0al., [> 2024\n](https://arxiv.org/html/2504.11713v3#bib.bib21)> )\n, a recent method developed for solving general stochastic control problems which we specialize and improve for efficiently learning to sample.\n![Refer to caption](x1.png)Figure 1:Starting from the uncontrolled diffusion processp1basesubscriptsuperscript\ud835\udc5dbase1p^{\\\\text{base}}\\_{1}italic\\_p start\\_POSTSUPERSCRIPT base end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT(left-most panel), Adjoint Sampling uses the Reciprocal Projection(Xt,X1)\u223cpt|1base\u2062p1usimilar-tosubscript\ud835\udc4b\ud835\udc61subscript\ud835\udc4b1subscriptsuperscript\ud835\udc5dbaseconditional\ud835\udc611subscriptsuperscript\ud835\udc5d\ud835\udc621(X\\_{t},X\\_{1})\\\\sim p^{\\\\text{base}}\\_{t|1}p^{u}\\_{1}( italic\\_X start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT , italic\\_X start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ) \u223citalic\\_p start\\_POSTSUPERSCRIPT base end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT italic\\_t | 1 end\\_POSTSUBSCRIPT italic\\_p start\\_POSTSUPERSCRIPT italic\\_u end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPTof the current controlled SDE to approximate the joint trajectory distributionptusubscriptsuperscript\ud835\udc5d\ud835\udc62\ud835\udc61p^{u}\\_{t}italic\\_p start\\_POSTSUPERSCRIPT italic\\_u end\\_POSTSUPERSCRIPT start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT, allowing us to take several gradient steps on the RAM objective per evaluated sample and energy gradient(X1(X\\_{1}( italic\\_X start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT,\u2207g(X1))\\\\nabla g(X\\_{1}))\u2207 italic\\_g ( italic\\_X start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ) ). After several iterations, Adjoint Sampling converges to the target Boltzmann\u03bc\ud835\udf07\\\\muitalic\\_\u03bc(right-most panel).\nOur contributions are as follows:\n* \u2022Efficiency: Our method is the first on-policy approach to allow far more gradient updates per model sample and energy evaluation. This is extremely important for scaling up to difficult amortized settings.\n* \u2022Theoretically Grounded: Our method is grounded in stochastic control, following the recent work of Adjoint Matching> (Domingo-Enrich et\u00a0al., [> 2024\n](https://arxiv.org/html/2504.11713v3#bib.bib21)> )\n. We improve upon Adjoint Matching by proposing a new objective that implicitly projects the model onto a set of optimal controls.\nWe do not require corrective measures such as importance sampling or sequential Monte Carlo for our theoretical guarantees.\n* \u2022Structure: Graph and Euclidean symmetries can be easily incorporated. We also adopt the method to handle periodic boundary conditions for modeling torsion angle representation",
          "original_query": "Adjoint Sampling",
          "cleaned_query": "Adjoint Sampling",
          "content_quality": "good_from_html"
        }
      ],
      "generated_ideas": [
        "Schr\u00f6dinger-Bridge Noise Schedules for Diffusion Models\nDesign a diffusion forward process (variance/noise schedule) by solving a Schr\u00f6dinger bridge (SB) between the data distribution and the Gaussian prior under a chosen reference SDE (e.g., Brownian or OU). Train the score model on trajectories generated by the SB reference to minimize discretization bias in the reverse-time sampler. Evaluate whether SB-designed schedules reduce steps needed for a fixed FID/likelihood versus hand-tuned schedules.",
        "Path-Integral Control for Learned Predictor\u2013Corrector Samplers\nCast predictor\u2013corrector sampling as a stochastic optimal control problem where the control minimizes a path-integral cost combining terminal mismatch (to the data distribution) and control energy (KL to a reference diffusion). Use the Feynman path-integral viewpoint to derive a reweighting/importance correction that can be estimated from sampled trajectories and used to adapt corrector strength per time step. This yields an adaptive sampler that targets either the SDE or probability-flow ODE with fewer function evaluations.",
        "On-Policy Score Distillation from Unnormalized Energies via Adjoint Sampling\nExtend Adjoint Sampling to directly distill a time-dependent score network for an unnormalized target density \\( \\gamma(x) \\) by training on-policy rollouts while matching the reverse-time SDE drift implied by \\(\\nabla \\log \\gamma\\). Concretely, learn a score \\(s_\\theta(x,t)\\) that minimizes a control-matching loss where energy gradients appear only through queried \\(\\nabla \\log \\gamma(x)\\) at visited states. Benchmark on high-dimensional EBMs and molecular Boltzmann targets to compare energy-evaluation efficiency against SMC-augmented diffusion samplers.",
        "Sequential Schr\u00f6dinger Bridge (IPF/Sinkhorn) as a Training-Time Curriculum for Diffusion\nUse the multi-marginal SB formulation (sequential IPF) to generate an explicit intermediate family \\(\\{\\pi_t\\}\\) and corresponding reweighting functions that progressively tighten marginal constraints. Train diffusion/score models stage-wise on these IPF-refined intermediates, reusing the same network but changing the target perturbed distributions across stages. This provides a principled curriculum that may stabilize training and improve mode coverage on challenging multimodal datasets.",
        "Exact-Likelihood Schr\u00f6dinger Flows: Probability-Flow ODE with SB Regularization\nAugment the probability-flow ODE (from the SDE framework) with an SB-inspired regularizer that penalizes deviation from a reference diffusion path measure (KL to Brownian/OU) while preserving exact likelihood computation. Implement by adding a controllable drift term parameterized by a neural network and training it jointly with the score via a weighted variational bound plus the SB KL term. Measure tradeoffs between likelihood, sample quality, and ODE solver stiffness/step count.",
        "Boundary-Condition-Aware Diffusions for Periodic and Manifold Data via SB Constraints\nGeneralize diffusion models to periodic/manifold-valued domains (e.g., torsion angles, spheres) by defining the reference process with correct boundary conditions and solving an SB on that space to connect prior and data. Combine the symmetry/periodic handling ideas from Adjoint Sampling with score-based SDE training on manifolds. Validate on molecular torsions and other periodic signals, testing whether respecting topology improves sampling efficiency and physical validity.",
        "Variance-Reduced Inverse Problems Using SB-Guided Posterior Sampling\nFor inverse problems (inpainting, colorization) reformulate posterior sampling as an SB between the prior (unconditional diffusion) and a likelihood-tilted terminal distribution induced by measurements. Implement a sequential SB sampler where the \u201cbackward kernel\u201d is approximated by a learned conditional score (or classifier-free guidance-like term) and refined with IPF-style updates. Compare reconstruction fidelity and uncertainty calibration to standard conditional score methods and plug-and-play diffusion.",
        "Energy-Efficient Molecular Conformer Generation with Hybrid Score + Proposal-IPF\nCombine score-based generative modeling with SB samplers by using a diffusion model as an amortized proposal and then running a small number of IPF refinements (Sinkhorn-style updates) using only pointwise energy/gradient queries to match the Boltzmann target. The key contribution is a practical hybrid pipeline: fast generation from the diffusion proposal, followed by principled SB-based reweighting/refinement that improves thermodynamic accuracy. Benchmark on conformer sets where exact sampling is hard, reporting effective sample size per energy evaluation and downstream property prediction error."
      ],
      "generated_ideas_raw": "1. **Schr\u00f6dinger-Bridge Noise Schedules for Diffusion Models**  \nDesign a diffusion forward process (variance/noise schedule) by solving a Schr\u00f6dinger bridge (SB) between the data distribution and the Gaussian prior under a chosen reference SDE (e.g., Brownian or OU). Train the score model on trajectories generated by the SB reference to minimize discretization bias in the reverse-time sampler. Evaluate whether SB-designed schedules reduce steps needed for a fixed FID/likelihood versus hand-tuned schedules.\n\n2. **Path-Integral Control for Learned Predictor\u2013Corrector Samplers**  \nCast predictor\u2013corrector sampling as a stochastic optimal control problem where the control minimizes a path-integral cost combining terminal mismatch (to the data distribution) and control energy (KL to a reference diffusion). Use the Feynman path-integral viewpoint to derive a reweighting/importance correction that can be estimated from sampled trajectories and used to adapt corrector strength per time step. This yields an adaptive sampler that targets either the SDE or probability-flow ODE with fewer function evaluations.\n\n3. **On-Policy Score Distillation from Unnormalized Energies via Adjoint Sampling**  \nExtend Adjoint Sampling to directly distill a time-dependent score network for an unnormalized target density \\( \\gamma(x) \\) by training on-policy rollouts while matching the reverse-time SDE drift implied by \\(\\nabla \\log \\gamma\\). Concretely, learn a score \\(s_\\theta(x,t)\\) that minimizes a control-matching loss where energy gradients appear only through queried \\(\\nabla \\log \\gamma(x)\\) at visited states. Benchmark on high-dimensional EBMs and molecular Boltzmann targets to compare energy-evaluation efficiency against SMC-augmented diffusion samplers.\n\n4. **Sequential Schr\u00f6dinger Bridge (IPF/Sinkhorn) as a Training-Time Curriculum for Diffusion**  \nUse the multi-marginal SB formulation (sequential IPF) to generate an explicit intermediate family \\(\\{\\pi_t\\}\\) and corresponding reweighting functions that progressively tighten marginal constraints. Train diffusion/score models stage-wise on these IPF-refined intermediates, reusing the same network but changing the target perturbed distributions across stages. This provides a principled curriculum that may stabilize training and improve mode coverage on challenging multimodal datasets.\n\n5. **Exact-Likelihood Schr\u00f6dinger Flows: Probability-Flow ODE with SB Regularization**  \nAugment the probability-flow ODE (from the SDE framework) with an SB-inspired regularizer that penalizes deviation from a reference diffusion path measure (KL to Brownian/OU) while preserving exact likelihood computation. Implement by adding a controllable drift term parameterized by a neural network and training it jointly with the score via a weighted variational bound plus the SB KL term. Measure tradeoffs between likelihood, sample quality, and ODE solver stiffness/step count.\n\n6. **Boundary-Condition-Aware Diffusions for Periodic and Manifold Data via SB Constraints**  \nGeneralize diffusion models to periodic/manifold-valued domains (e.g., torsion angles, spheres) by defining the reference process with correct boundary conditions and solving an SB on that space to connect prior and data. Combine the symmetry/periodic handling ideas from Adjoint Sampling with score-based SDE training on manifolds. Validate on molecular torsions and other periodic signals, testing whether respecting topology improves sampling efficiency and physical validity.\n\n7. **Variance-Reduced Inverse Problems Using SB-Guided Posterior Sampling**  \nFor inverse problems (inpainting, colorization) reformulate posterior sampling as an SB between the prior (unconditional diffusion) and a likelihood-tilted terminal distribution induced by measurements. Implement a sequential SB sampler where the \u201cbackward kernel\u201d is approximated by a learned conditional score (or classifier-free guidance-like term) and refined with IPF-style updates. Compare reconstruction fidelity and uncertainty calibration to standard conditional score methods and plug-and-play diffusion.\n\n8. **Learned Time-Discretization via Adjoint Matching to Minimize Reverse-SDE Bias**  \nInstead of fixing discretization (uniform or cosine), learn a nonuniform time grid and solver parameters by differentiating through sampling using adjoint matching objectives that target minimal KL between discretized and continuous-time reverse dynamics. Optimize step locations and corrector iterations jointly with the score network under a compute budget constraint (fixed number of network/energy evals). Evaluate improvements in FID per NFE and robustness across resolutions (CIFAR-10 \u2192 ImageNet subsets \u2192 1024\u00b2).\n\n9. **Coupled Diffusion\u2013Transport Models: Interpolating OT and Diffusion with Temperature Annealing**  \nExploit the SB-to-optimal-transport limit (\\(\\sigma\\to 0\\)) by introducing an explicit \u201centropic temperature\u201d parameter that anneals from diffusion-like behavior (high entropy) to transport-like behavior (low entropy) during sampling. Implement as a family of SDEs whose diffusion coefficient is scheduled and whose score is trained across temperatures, enabling controllable diversity\u2013fidelity tradeoffs. Test whether low-temperature late-stage dynamics sharpen samples without collapsing modes compared to standard guidance.\n\n10. **Energy-Efficient Molecular Conformer Generation with Hybrid Score + Proposal-IPF**  \nCombine score-based generative modeling with SB samplers by using a diffusion model as an amortized proposal and then running a small number of IPF refinements (Sinkhorn-style updates) using only pointwise energy/gradient queries to match the Boltzmann target. The key contribution is a practical hybrid pipeline: fast generation from the diffusion proposal, followed by principled SB-based reweighting/refinement that improves thermodynamic accuracy. Benchmark on conformer sets where exact sampling is hard, reporting effective sample size per energy evaluation and downstream property prediction error.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Schr\u00f6dinger-Bridge Noise Schedules for Diffusion Models\nDesign a diffusion forward process (variance/noise schedule) by solving a Schr\u00f6dinger bridge (SB) between the data distribution and the Gaussian",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Path-Integral Control for Learned Predictor\u2013Corrector Samplers\nCast predictor\u2013corrector sampling as a stochastic optimal control problem where the control minimizes a path-integral cost combining term",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "On-Policy Score Distillation from Unnormalized Energies via Adjoint Sampling\nExtend Adjoint Sampling to directly distill a time-dependent score network for an unnormalized target density \\( \\gamma(x) ",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Sequential Schr\u00f6dinger Bridge (IPF/Sinkhorn) as a Training-Time Curriculum for Diffusion\nUse the multi-marginal SB formulation (sequential IPF) to generate an explicit intermediate family \\(\\{\\pi_t\\}\\",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Exact-Likelihood Schr\u00f6dinger Flows: Probability-Flow ODE with SB Regularization\nAugment the probability-flow ODE (from the SDE framework) with an SB-inspired regularizer that penalizes deviation from ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Boundary-Condition-Aware Diffusions for Periodic and Manifold Data via SB Constraints\nGeneralize diffusion models to periodic/manifold-valued domains (e.g., torsion angles, spheres) by defining the re",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Variance-Reduced Inverse Problems Using SB-Guided Posterior Sampling\nFor inverse problems (inpainting, colorization) reformulate posterior sampling as an SB between the prior (unconditional diffusion)",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Energy-Efficient Molecular Conformer Generation with Hybrid Score + Proposal-IPF\nCombine score-based generative modeling with SB samplers by using a diffusion model as an amortized proposal and then r",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 11,
      "paper_title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies",
      "contribution": "Demonstrates that adding an explicit, compute-aware inference phase (using search/optimization strategies such as tree search, sampling and adaptation) on top of trained RL policies substantially breaks zero-shot performance ceilings in complex multi-agent and combinatorial tasks, yielding large empirical gains with modest extra wall-clock time.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 10293,
      "output_tokens": 996,
      "predecessor_details": [
        {
          "success": true,
          "title": "(PDF) Mastering the game of Go with deep neural ...",
          "url": "https://www.researchgate.net/publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search",
          "content": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses \u2018value networks\u2019 to evaluate board positions and \u2018policy networks\u2019 to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.\n\nContent may be subject to copyright.\n\n**Discover the world's research**\n\n- 25+ million members\n- 160+ million publication pages\n- 2.3+ billion citations\n\n[Join for free](https://www.researchgate.net/publication/signup.SignUp.html)\n\nMasteringtheGameofGowithDeepNeuralNetworksand\n\nTreeSearch\n\nDavidSilver1\\*,AjaHuang1\\*,ChrisJ.Maddison1,ArthurGuez1,LaurentSifre1,Georgevanden\n\nDriessche1,JulianSchrittwieser1,IoannisAntonoglou1,VedaPanneershelvam1,MarcLanctot1,\n\nSanderDieleman1,DominikGrewe1,JohnNham2,NalKalchbrenner1,IlyaSutskever2,Timothy\n\nLillicrap1,MadeleineLeach1,KorayKavukcuoglu1,ThoreGraepel1,DemisHassabis1.\n\n1GoogleDeepMind,5NewStreetSquare,LondonEC4A3TW.\n\n2Google,1600AmphitheatreParkway,MountainViewCA94043.\n\n\\*Theseauthorscontributedequallytothiswork.\n\nCorrespondenceshouldbeaddressedtoeitherDavidSilver(davidsilver@google.com)orDemis\n\nHassabis(demishassabis@google.com).\n\nThegameofGohaslongbeenviewedasthemostchallengingofclassicgamesforar-\n\nti\ufb01cialintelligenceduetoitsenormoussearchspaceandthedif\ufb01cultyofevaluatingboard\n\npositionsandmoves.WeintroduceanewapproachtocomputerGothatusesvaluenetworks\n\ntoevaluateboardpositionsandpolicynetworkstoselectmoves.Thesedeepneuralnetworks\n\naretrainedbyanovelcombinationofsupervisedlearningfromhumanexpertgames,and\n\nreinforcementlearningfromgamesofself-play.Withoutanylookaheadsearch,theneural\n\nnetworksplayGoatthelevelofstate-of-the-artMonte-Carlotreesearchprogramsthatsim-\n\nulatethousandsofrandomgamesofself-play.Wealsointroduceanewsearchalgorithm\n\nthatcombinesMonte-Carlosimulationwithvalueandpolicynetworks.Usingthissearchal-\n\ngorithm,ourprogramAlphaGoachieveda99.8%winningrateagainstotherGoprograms,\n\nanddefeatedtheEuropeanGochampionby5gamesto0.Thisisthe\ufb01rsttimethatacom-\n\nputerprogramhasdefeatedahumanprofessionalplayerinthefull-sizedgameofGo,afeat\n\npreviouslythoughttobeatleastadecadeaway.\n\nAllgamesofperfectinformationhaveanoptimalvaluefunction,v\u2217(s),whichdetermines\n\ntheoutcomeofthegame,fromeveryboardpositionorstates,underperfectplaybyallplayers.\n\nThesegamesmaybesolvedbyrecursivelycomputingtheoptimalvaluefunctioninasearchtree\n\ncontainingapproximatelybdpossiblesequencesofmoves,wherebisthegame\u2019sbreadth(number\n\n1\n\noflegalmovesperposition)anddisitsdepth(gamelength).Inlargegames,suchaschess\n\n(b\u224835,d\u224880)1andespeciallyGo(b\u2248250,d\u2248150)1,exhaustivesearchisinfeasible2,3,\n\nbuttheeffectivesearchspacecanbereducedbytwogeneralprinciples.First,thedepthofthe\n\nsearchmaybereducedbypositionevaluation:truncatingthesearchtreeatstatesandreplacing\n\nthesubtreebelowsbyanapproximatevaluefunctionv(s)\u2248v\u2217(s)thatpredictstheoutcomefrom\n\nstates.Thisapproachhasledtosuper-humanperformanceinchess4,checkers5andothello6,but\n\nitwasbelievedtobeintractableinGoduetothecomplexityofthegame7.Second,thebreadthof\n\nthesearchmaybereducedbysamplingactionsfromapolicyp(a\\|s)thatisaprobabilitydistribution\n\noverpossiblemovesainpositions.Forexample,Monte-Carlorollouts8searchtomaximumdepth\n\nwithoutbranchingatall,bysamplinglongsequencesofactionsforbothplayersfromapolicyp.\n\nAveragingoversuchrolloutscanprovideaneffectivepositionevaluation,achievingsuper-human\n\nperformanceinbackgammon8andScrabble9,andweakamateurlevelplayinGo10.\n\nMonte-Carlotreesearch(MCTS)11,12usesMonte-Carlorolloutstoestimatethevalueof\n\neachstateinasearchtree.Asmoresimulationsareexecuted,thesearchtreegrowslargerandthe\n\nrelevantvaluesbecomemoreaccurate.Thepolicyusedtoselectactionsduringsearchisalsoim-\n\nprovedovertime,byselectingchildrenwithhighervalues.Asymptotically,thispolicyconverges\n\ntooptimalplay,andtheevaluationsconvergetotheoptimalvaluefunction12.Thestrongestcurrent\n\nGoprogramsarebasedonMCTS,enhancedbypoliciesthataretrainedtopredicthumanexpert\n\nmoves13.Thesepoliciesareusedtonarrowthesearchtoabeamofhighprobabilityactions,and\n\ntosampleactionsduringrollouts.Thisapproachhasachievedstrongamateurplay13\u201315.How-\n\never,priorworkhasbeenlimitedtoshallowpolicies13\u201315orvaluefunctions16basedonalinear\n\ncombinationofinputfeatures.\n\nRecently,deepconvolutionalneuralnetworkshaveachievedunprecedentedperformance\n\ninvisualdomains:forexampleimageclassi\ufb01cation17,facerecognition18,andplayingAtari\n\ngames19.Theyusemanylayersofneurons,eacharrangedinoverlappingtiles,toconstructin-\n\ncreasinglyabstract,localisedrepresentationsofanimage20.Weemployasimilararchitecturefor\n\nthegameofGo.Wepassintheboardpositionasa19\u00d719imageanduseconvolutionallayers\n\n2\n\ntoconstructarepresentationoftheposition.Weusetheseneuralnetworkstoreducetheeffective\n\ndepthandbreadthofthesearchtree:evaluatingpositionsusingavaluenetwork,andsampling\n\nactionsusingapolicynetwork.\n\nWetraintheneuralnetworksusingapipelineconsistingofseveralstagesofmachinelearning\n\n(Figure1).Webeginbytrainingasupervisedlearning(SL)policynetwork,p\u03c3,directlyfrom\n\nexperthumanmoves.Thisprovidesfast,ef\ufb01cientlearningupdateswithimmediatefeedbackand\n\nhighqualitygradients.Similartopriorwork13,15,wealsotrainafastpolicyp\u03c0thatcanrapidly\n\nsampleactionsduringrollouts.Next,wetrainareinforcementlearning(RL)policynetwork,p\u03c1,\n\nthatimprovestheSLpolicynetworkbyoptimisingthe\ufb01naloutcomeofgamesofself-play.This\n\nadjuststhepolicytowardsthecorrectgoalofwinninggames,ratherthanmaximizingpredictive\n\naccuracy.Finally,wetrainavaluenetworkv\u03b8thatpredictsthewinnerofgamesplayedbythe\n\nRLpolicynetworkagainstitself.OurprogramAlphaGoef\ufb01cientlycombinesthepolicyandvalue\n\nnetworkswithMCTS.\n\n1SupervisedLearningofPolicyNetworks\n\nForthe\ufb01rststageofthetrainingpipeline,webuildonpriorworkonpredictingexpertmoves\n\ninthegameofGousingsupervisedlearning13,21\u201324.TheSLpolicynetworkp\u03c3(a\\|s)alternates\n\nbetweenconvolutionallayerswithweights\u03c3,andrecti\ufb01ernon-linearities.A\ufb01nalsoftmaxlayer\n\noutputsaprobabilitydistributionoveralllegalmovesa.Theinputstothepolicynetworkis\n\nasimplerepresentationoftheboardstate(seeExtendedDataTable2).Thepolicynetworkis\n\ntrainedonrandomlysampledstate-actionpairs(s,a),usingstochasticgradientascenttomaximize\n\nthelikelihoodofthehumanmoveaselectedinstates,\n\n\u2206\u03c3\u221d\u2202logp\u03c3(a\\|s)\n\n\u2202\u03c3.(1)\n\nWetraineda13layerpolicynetwork,whichwecalltheSLpolicynetwork,from30million\n\npositionsfromtheKGSGoServer.Thenetworkpredictedexpertmoveswithanaccuracyof\n\n3\n\nFigure1:Neuralnetworktrainingpipelineandarchitecture.aAfastrolloutpolicyp\u03c0andsu-\n\npervisedlearning(SL)policynetworkp\u03c3aretrainedtopredicthumanexpertmovesinadata-setof\n\npositions.Areinforcementlearning(RL)policynetworkp\u03c1isinitialisedtotheSLpolicynetwork,\n\nandisthenimprovedbypolicygradientlearningtomaximizetheoutcome(i.e.winningmore\n\ngames)againstpreviousversionsofthepolicynetwork.Anewdata-setisgeneratedbyplaying\n\ngamesofself-playwiththeRLpolicynetwork.Finally,avaluenetworkv\u03b8istrainedbyregression\n\ntopredicttheexpectedoutcome(i.e.whetherthecurrentplayerwins)inpositionsfromtheself-\n\nplaydata-set.bSchematicrepresentationoftheneuralnetworkarchitectureusedinAlphaGo.The\n\npolicynetworktakesarepresentationoftheboardpositionsasitsinput,passesitthroughmany\n\nconvolutionallayerswithparameters\u03c3(SLpolicynetwork)or\u03c1(RLpolicynetwork),andoutputs\n\naprobabilitydistributionp\u03c3(a\\|s)orp\u03c1(a\\|s)overlegalmovesa,representedbyaprobabilitymap\n\novertheboard.Thevaluenetworksimilarlyusesmanyconvolutionallayerswithparameters\u03b8,but\n\noutputsascalarvaluev\u03b8(s0)thatpredictstheexpectedoutcomeinpositions0.\n\n4\n\nFigure2:Strengthandaccuracyofpolicyandvaluenetworks.aPlotshowingtheplaying\n\nstrengthofpolicynetworksasafunctionoftheirtrainingaccuracy.Policynetworkswith128,\n\n192,256and384convolutional\ufb01ltersperlayerwereevaluatedperiodicallyduringtraining;the\n\nplotshowsthewinningrateofAlphaGousingthatpolicynetworkagainstthematchversionof\n\nAlphaGo.bComparisonofevaluationaccuracybetweenthevaluenetworkandrolloutswith\n\ndifferentpolicies.Positionsandoutcomesweresampledfromhumanexpertgames.Eachposition\n\nwasevaluatedbyasingleforwardpassofthevaluenetworkv\u03b8,orbythemeanoutcomeof100\n\nrollouts,playedoutusingeitheruniformrandomrollouts,thefastrolloutpolicyp\u03c0,theSLpolicy\n\nnetworkp\u03c3ortheRLpolicynetworkp\u03c1.Themeansquarederrorbetweenthepredictedvalue\n\nandtheactualgameoutcomeisplottedagainstthestageofthegame(howmanymoveshadbeen\n\nplayedinthegivenposition).\n\n57.0%onaheldouttestset,usingallinputfeatures,and55.7%usingonlyrawboardposition\n\nandmovehistoryasinputs,comparedtothestate-of-the-artfromotherresearchgroupsof44.4%\n\natdateofsubmission24(fullresultsinExtendedDataTable3).Smallimprovementsinaccuracy\n\nledtolargeimprovementsinplayingstrength(Figure2,a);largernetworksachievebetteraccuracy\n\nbutareslowertoevaluateduringsearch.Wealsotrainedafasterbutlessaccuraterolloutpolicy\n\np\u03c0(a\\|s),usingalinearsoftmaxofsmallpatternfeatures(seeExtendedDataTable4)withweights\n\n\u03c0;thisachievedanaccuracyof24.2%,usingjust2\u00b5stoselectanaction,ratherthan3msforthe\n\npolicynetwork.\n\n5\n\n2ReinforcementLearningofPolicyNetworks\n\nThesecondstageofthetrainingpipelineaimsatimprovingthepolicynetworkbypolicygradient\n\nreinforcementlearning(RL)25,26.TheRLpolicynetworkp\u03c1isidenticalinstructuretotheSL\n\npolicynetwork,anditsweights\u03c1areinitialisedtothesamevalues,\u03c1=\u03c3.Weplaygames\n\nb",
          "original_query": "Mastering the game of Go with deep neural networks and tree search",
          "cleaned_query": "Mastering the game of Go with deep neural networks and tree search",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Bandit based Monte-Carlo Planning - General Game Playing",
          "url": "http://ggp.stanford.edu/readings/uct.pdf",
          "content": "Bandit based Monte-Carlo Planning\nLevente Kocsis and Csaba Szepesv\u00b4ari\nComputer and Automation Research Institute of the\nHungarian Academy of Sciences, Kende u. 13-17, 1111 Budapest, Hungary\nkocsis@sztaki.hu\nAbstract. For large state-space Markovian Decision Problems Monte\u0002Carlo planning is one of the few viable approaches to find near-optimal\nsolutions. In this paper we introduce a new algorithm, UCT, that ap\u0002plies bandit ideas to guide Monte-Carlo planning. In finite-horizon or\ndiscounted MDPs the algorithm is shown to be consistent and finite\nsample bounds are derived on the estimation error due to sampling. Ex\u0002perimental results show that in several domains, UCT is significantly\nmore efficient than its alternatives.\n1 Introduction\nConsider the problem of finding a near optimal action in large state-space\nMarkovian Decision Problems (MDPs) under the assumption a generative\nmodel of the MDP is available. One of the few viable approaches is to carry\nout sampling based lookahead search, as proposed by Kearns et al. [8], whose\nsparse lookahead search procedure builds a tree with its nodes labelled by either\nstates or state-action pairs in an alternating manner, and the root corresponding\nto the initial state from where planning is initiated. Each node labelled by a\nstate is followed in the tree by a fixed number of nodes associated with the\nactions available at that state, whilst each corresponding state-action labelled\nnode is followed by a fixed number of state-labelled nodes sampled using the\ngenerative model of the MDP. During sampling, the sampled rewards are stored\nwith the edges connecting state-action nodes and state nodes. The tree is built\nin a stage-wise manner, from the root to the leafs. Its depth is fixed. The\ncomputation of the values of the actions at the initial state happens from the\nleafs by propagating the values up in the tree: The value of a state-action\nlabelled node is computed based on the average of the sum of the rewards\nalong the edges originating at the node and the values at the corresponding\nsuccessor nodes, whilst the value of a state node is computed by taking the\nmaximum of the values of its children. Kearns et al. showed that in order to\nfind an action at the initial state whose value is within the \u00b2-vicinity of that\nof the best, for discounted MPDs with discount factor 0 < \u03b3 < 1, K actions\nand uniformly bounded rewards, regardless of the size of the state-space fixed\nsize trees suffice [8]. In particular, the depth of the tree is proportional to\n1/(1 \u2212 \u03b3) log(1/(\u00b2(1 \u2212 \u03b3))), whilst its width is proportional to K/(\u00b2(1 \u2212 \u03b3)).\nAlthough this result looks promising,1\nin practice, the amount of work needed\nto compute just a single almost-optimal action at a given state can be over\u0002whelmingly large. In this paper we are interested in improving the performance\nof this vanilla Monte-Carlo planning algorithm. In particular, we are interested in\nMonte-Carlo planning algorithms with two important characteristics: (1) small\nerror probability if the algorithm is stopped prematurely, and (2) convergence\nto the best action if enough time is given.\nBesides MPDs, we are also interested in game-tree search. Over the years,\nMonte-Carlo simulation based search algorithms have been used successfully in\nmany non-deterministic and imperfect information games, including backgam\u0002mon [14], poker [4] and Scrabble [12]. Recently, Monte-Carlo search proved to\nbe competitive in deterministic games with large branching factors, viz. in Go\n[5]. For real-time strategy games, due to their enormous branching factors and\nstochasticity, Monte-Carlo simulations seems to be one of the few feasible ap\u0002proaches for planning [7]. Intriguingly, Monte-Carlo search algorithms used by\ntoday\u2019s games programs use either uniform sampling of actions or some heuristic\nbiasing of the action selection probabilities that come with no guarantees.\nThe main idea of the algorithm proposed in this paper is to sample actions\nselectively. In order to motivate our approach let us consider problems with a\nlarge number of actions and assume that the lookahead is carried out at a fixed\ndepth D. If sampling can be restricted to say half of the actions at all stages\nthen the overall work reduction is (1/2)D. Hence, if one is able to identify a\nlarge subset of the suboptimal actions early in the sampling procedure then\nhuge performance improvements can be expected.\nBy definition, an action is suboptimal for a given state, if its value is less than\nthe best of the action-values for the same state. Since action-values depend on the\nvalues of successor states, the problem boils down to getting the estimation error\nof the state-values for such states decay fast. In order to achieve this, an efficient\nalgorithm must balance between testing alternatives that look currently the best\nso as to obtain precise estimates, and the exploration of currently suboptimal\u0002looking alternatives, so as to ensure that no good alternatives are missed because\nof early estimation errors. Obviously, these criteria are contradictory and the\nproblem of finding the right balance is known as the the exploration-exploitation\ndilemma. The most basic form of this dilemma shows up in multi-armed bandit\nproblems [1].\nThe main idea in this paper it to apply a particular bandit algorithm, UCB1\n(UCB stands for Upper Confidence Bounds), for rollout-based Monte-Carlo plan\u0002ning. The new algorithm, called UCT (UCB applied to trees) described in Section\n2 is called UCT. Theoretical results show that the new algorithm is consistent,\nwhilst experimental results (Section 3) for artificial game domains (P-games)\nand the sailing domain (a specific MDP) studied earlier in a similar context by\nothers [11] indicate that UCT has a significant performance advantage over its\nclosest competitors.\n1\nIn fact, as also noted by [8] the bound might be unimprovable, though this still\nremains an open problem.\n2 The UCT algorithm\n2.1 Rollout-based planning\nIn this paper we consider Monte-Carlo planning algorithms that we call rollout\u0002based. As opposed to the algorithm described in the introduction (stage-wise\ntree building), a rollout-based algorithm builds its lookahead tree by repeatedly\nsampling episodes from the initial state. An episode is a sequence of state-action\u0002reward triplets that are obtained using the domains generative model. The tree\nis built by adding the information gathered during an episode to it in an incre\u0002mental manner.\nThe reason that we consider rollout-based algorithms is that they allow us to\nkeep track of estimates of the actions\u2019 values at the sampled states encountered\nin earlier episodes. Hence, if some state is reencountered then the estimated\naction-values can be used to bias the choice of what action to follow, potentially\nspeeding up the convergence of the value estimates. If the portion of states that\nare encountered multiple times in the procedure is small then the performance\nof rollout-based sampling degenerates to that of vanilla (non-selective) Monte\u0002Carlo planning. On the other hand, for domains where the set of successor states\nconcentrates to a few states only, rollout-based algorithms implementing selective\nsampling might have an advantage over other methods.\nThe generic scheme of rollout-based Monte-Carlo planning is given in Fig\u0002ure 1. The algorithm iteratively generates episodes (line 3), and returns the\naction with the highest average observed long-term reward (line 5).2In pro\u0002cedure UpdateValue the total reward q is used to adjust the estimated value\nfor the given state-action pair at the given depth, completed by increasing the\ncounter that stores the number of visits of the state-action pair at the given\ndepth. Episodes are generated by the search function that selects and effectu\u0002ates actions recursively until some terminal condition is satisfied. This can be\nthe reach of a terminal state, or episodes can be cut at a certain depth (line 8).\nAlternatively, as suggested by Peret and Garcia [11] and motivated by itera\u0002tive deepening, the search can be implemented in phases where in each phase\nthe depth of search is inceased. An approximate way to implement iterative\ndeepening, that we also follow in our experiments, is to stop the episodes with\nprobability that is inversely proportional to the number of visits to the state.\nThe effectiveness of the whole algorithm will crucially depend on how the\nactions are selected in line 9. In vanilla Monte-Carlo planning (referred by MC\nin the following) the actions are sampled uniformly. The main contribution of the\npresent paper is the introduction of a bandit-algorithm for the implementation\nof the selective sampling of actions.\n2.2 Stochastic bandit problems and UCB1\nA bandit problem with K arms (actions) is defined by the sequence of random\npayoffs Xit, i = 1, . . . , K, t \u2265 1, where each i is the index of a gambling machine\n2 The function bestMove is trivial, and is omitted due to the lack of space.\n1: function MonteCarloPlanning(state)\n2: repeat\n3: search(state, 0)\n4: until Timeout\n5: return bestAction(state,0)\n6: function search(state, depth)\n7: if Terminal(state) then return 0\n8: if Leaf(state, d) then return Evaluate(state)\n9: action := selectAction(state, depth)\n10: (nextstate, reward) := simulateAction(state, action)\n11: q := reward + \u03b3 search(nextstate, depth + 1)\n12: UpdateValue(state, action, q, depth)\n13: return q\nFig. 1. The pseudocode of a generic Monte-Carlo planning algorithm.\n(the \u201carm\u201d of a bandit). Successive plays of machine i yield the payoffs Xi1,\nXi2, . . .. For simplicity, we shall assume that Xit lies in the interval [0, 1]. An\nallocation policy is a mapping that selects the next arm to be played based\non the sequence of past selections and payoffs obtained. The expected regret\nof an allocation policy A after n plays is defined by Rn = maxi E [\nPn\nt=1 Xit] \u2212\nE\nhPK\nj=1\nPTj (n)\nt=1 Xj,ti\n, where It \u2208 {1, . . . , K} is the index of the arm selected\nat time t by policy A, and where Ti(t) = Pt\ns=",
          "original_query": "Bandit based Monte\u2011Carlo planning (UCT)",
          "cleaned_query": "Bandit based Monte\u2011Carlo planning (UCT)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Neural combinatorial optimization with reinforcement learning in ...",
          "url": "https://link.springer.com/article/10.1007/s10462-024-11045-1",
          "content": "Neural combinatorial optimization with reinforcement learning in industrial engineering: a survey | Artificial Intelligence Review\n[Skip to main content](#main)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/s10462-024-11045-1?)\n# Neural combinatorial optimization with reinforcement learning in industrial engineering: a survey\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:14 February 2025\n* Volume\u00a058, article\u00a0number130, (2025)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s10462-024-11045-1.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/10462?as=webp)Artificial Intelligence Review](https://link.springer.com/journal/10462)[Aims and scope](https://link.springer.com/journal/10462/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/10462/3)\nNeural combinatorial optimization with reinforcement learning in industrial engineering: a survey\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s10462-024-11045-1.pdf)\n* [K. T. Chung](#auth-K__T_-Chung-Aff1)[ORCID:orcid.org/0000-0001-7427-0917](https://orcid.org/0000-0001-7427-0917)[1](#Aff1),\n* [C. K. M. Lee](#auth-C__K__M_-Lee-Aff1)[ORCID:orcid.org/0000-0001-8577-4547](https://orcid.org/0000-0001-8577-4547)[1](#Aff1)&amp;\n* [Y. P. Tsang](#auth-Y__P_-Tsang-Aff1)[ORCID:orcid.org/0000-0002-6128-345X](https://orcid.org/0000-0002-6128-345X)[1](#Aff1)\n* 6553Accesses\n* 8Citations\n* 1Altmetric\n* [Explore all metrics](https://link.springer.com/article/10.1007/s10462-024-11045-1/metrics)\n## Abstract\nIn recent trends, machine learning is widely used to support decision-making in various domains and industrial operations. Because of the increasing complexity of modern industries, industrial engineering aims not only to increase cost-effectiveness and productivity but also to consider sustainability, resilience, and human centricity, resulting in many-objective, constrained, and stochastic operations research. Based on the above stringent requirements, combinatorial optimization (CO) problems are thus developed to support the complicated decision-making process in operations research. Due to the computational complexity of exact algorithms and the uncertain solution quality of heuristic methods, there is a growing trend to leverage the power of machine learning in solving CO problems, known as neural combinatorial optimization (NCO), where reinforcement learning (RL) is the core to achieve the sequential decision support. This survey study provides a comprehensive investigation of the theories and recent advancements in applying RL to solve hard CO problems, such as vehicle routing, bin packing, assignment, scheduling, and planning problems, and, in addition, summarizes the applications of neural combinatorial optimization with reinforcement learning (NCO-RL). The detailed review found that although the research domain of NCO-RL is still under-explored, its research potential has been proven to address environmental sustainability, adaptability, and human factors. Last but not least, the technical challenges and opportunities of the NCO-RL to embrace the industry 5.0 paradigm are discussed.\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-86286-2?as&#x3D;webp)\n### [Reinforcement Learning for the Knapsack Problem](https://link.springer.com/10.1007/978-3-030-86286-2_1?fromPaywallRec=false)\nChapter\u00a9 2021\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs43153-023-00422-y/MediaObjects/43153_2023_422_Fig1_HTML.png)\n### [Comparison of reinforcement learning techniques for controlling a CSTR process](https://link.springer.com/10.1007/s43153-023-00422-y?fromPaywallRec=false)\nArticle11 December 2023\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-25312-6?as&#x3D;webp)\n### [An Architecture for\u00a0Deploying Reinforcement Learning in\u00a0Industrial Environments](https://link.springer.com/10.1007/978-3-031-25312-6_67?fromPaywallRec=false)\nChapter\u00a9 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Combinatorial Chemistry](https://link.springer.com/subjects/combinatorial-chemistry)\n* [Discrete Optimization](https://link.springer.com/subjects/discrete-optimization)\n* [Dynamic Combinatorial Chemistry](https://link.springer.com/subjects/dynamic-combinatorial-chemistry)\n* [Industrial and Production Engineering](https://link.springer.com/subjects/industrial-and-production-engineering)\n* [Operations Research and Decision Theory](https://link.springer.com/subjects/operations-research-and-decision-theory)\n* [Operations Research, Management Science](https://link.springer.com/subjects/operations-research-management-science-)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=10462)\nAvoid common mistakes on your manuscript.\n## 1Introduction\nIndustrial engineering is concerned with the integrated system of people, materials, information, equipment, and energy to predict, evaluate, and improve the performance of the processes and different stages of the systems (Salvendy[2001](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR62)). Decision-making is a fundamental part of industrial engineering applications, ranging from investment decisions, layout design, production scheduling, inventory control, and routing problems in various domains, including manufacturing, logistics, and supply chain management (Sgarbossa et al.[2020](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR64); Triantaphyllou and Mann[1995](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR79)). Because of the increasing complexity of modern industries, industrial engineering aims not only to enhance cost-effectiveness and productivity but also to consider sustainability, adaptability, and human factors to achieve a broad range of objectives and operational requirements (Colabianchi et al.[2021](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR15); Kadir et al.[2019](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR28); Manavalan and Jayakrishna[2019](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR48); Sgarbossa et al.[2020](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR64)). In addition, facing the challenges in today\u2019s competitive markets, modern manufacturing has evolved to adopt the demand dynamics and unexpected disruptions for timely response. Since conventional static models are inadequate in these areas, dynamic models have been studied to capture the relationships among variables better (Mittal et al.[2008](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR52)).\nWhen there is a need to model and optimize complex decision-making problems, operations research provides a wide range of methods and techniques for decision-makers to reliably determine optimal solutions for specific operational problems (Shannon et al.[1980](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR65)). Typically, operations research focuses on the cost minimization of existing processes and formulating decision intelligence (Bengio et al.[2021](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR5); Dekker et al.[2012](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR17)). This paper primarily focuses on combinatorial optimization (CO), which is essential in planning and decision-making in sizeable discrete configuration space to optimize objectives such as minimal cost, maximal payoff, or other performance indicators under different constraints.\nIn general, CO problems can be solved by exact methods, such as branch-and-bound (B&amp;B) using a mixed-integer linear programming (MILP) formulation (Clausen[1999](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR14)), but the running time grows exponentially as the problem size increases. No exact algorithm exists to solve the hard CO problems (i.e., NP-hard and NP-complete) within a polynomial time (Korte et al.[2011](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR34)). Solving large-scale problem instances with an exact solver is impractical, especially when computational resources and decision-making time are limited. Although heuristic methods can solve CO problems in a polynomial time, the solutions have no guarantee of global optimality, which is regarded as a well-known dilemma illustrated by the No Free Lunch Theorem (Wolpert and Macready[1997](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR90)). Due to the complexity of CO problems and new requirements arising from industry application domains, an effective algorithm to solve CO problems has remained an active research area over the past years. Because of the above challenges, a growing trend is to adopt machine learning to provide a better trade-off between solution quality and execution time. Once the parameterized models are trained on real-world datasets, the results can be generated in polynomial time. The potential capabilities of these models are vast, e.g., forecasting demands and predicting vulnerabilities, which can be further used as variables to support decision-making (Mittal and Panchal[2023](https://link.springer.com/article/10.1007",
          "original_query": "Neural Combinatorial Optimization with Reinforcement Learning",
          "cleaned_query": "Neural Combinatorial Optimization with Reinforcement Learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "ATTENTION, LEARN TO SOLVE ROUTING PROBLEMS!",
          "url": "https://openreview.net/pdf?id=ByxBFsRqYm",
          "content": "Published as a conference paper at ICLR 2019\nATTENTION, LEARN TO SOLVE ROUTING PROBLEMS!\nWouter Kool\nUniversity of Amsterdam\nORTEC\nw.w.m.kool@uva.nl\nHerke van Hoof\nUniversity of Amsterdam\nh.c.vanhoof@uva.nl\nMax Welling\nUniversity of Amsterdam\nCIFAR\nm.welling@uva.nl\nABSTRACT\nThe recently presented idea to learn heuristics for combinatorial optimization\nproblems is promising as it can save costly development. However, to push this\nidea towards practical implementation, we need better models and better ways\nof training. We contribute in both directions: we propose a model based on at\u0002tention layers with benefits over the Pointer Network and we show how to train\nthis model using REINFORCE with a simple baseline based on a deterministic\ngreedy rollout, which we find is more efficient than using a value function. We\nsignificantly improve over recent learned heuristics for the Travelling Salesman\nProblem (TSP), getting close to optimal results for problems up to 100 nodes.\nWith the same hyperparameters, we learn strong heuristics for two variants of the\nVehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochas\u0002tic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of\nbaselines and getting results close to highly optimized and specialized algorithms.\n1 INTRODUCTION\nImagine yourself travelling to a scientific conference. The field is popular, and surely you do not\nwant to miss out on anything. You have selected several posters you want to visit, and naturally you\nmust return to the place where you are now: the coffee corner. In which order should you visit the\nposters, to minimize your time walking around? This is the Travelling Scientist Problem (TSP).\nYou realize that your problem is equivalent to the Travelling Salesman Problem (conveniently also\nTSP). This seems discouraging as you know the problem is (NP-)hard (Garey & Johnson, 1979).\nFortunately, complexity theory analyzes the worst case, and your Bayesian view considers this un\u0002likely. In particular, you have a strong prior: the posters will probably be laid out regularly. You\nwant a special algorithm that solves not any, but this type of problem instance. You have some\nmonths left to prepare. As a machine learner, you wonder whether your algorithm can be learned?\nMotivation Machine learning algorithms have replaced humans as the engineers of algorithms to\nsolve various tasks. A decade ago, computer vision algorithms used hand-crafted features but today\nthey are learned end-to-end by Deep Neural Networks (DNNs). DNNs have outperformed classic\napproaches in speech recognition, machine translation, image captioning and other problems, by\nlearning from data (LeCun et al., 2015). While DNNs are mainly used to make predictions, Rein\u0002forcement Learning (RL) has enabled algorithms to learn to make decisions, either by interacting\nwith an environment, e.g. to learn to play Atari games (Mnih et al., 2015), or by inducing knowledge\nthrough look-ahead search: this was used to master the game of Go (Silver et al., 2017).\nThe world is not a game, and we desire to train models that make decisions to solve real problems.\nThese models must learn to select good solutions for a problem from a combinatorially large set\nof potential solutions. Classically, approaches to this problem of combinatorial optimization can\nbe divided into exact methods, that guarantee finding optimal solutions, and heuristics, that trade\noff optimality for computational cost, although exact methods can use heuristics internally and vice\nversa. Heuristics are typically expressed in the form of rules, which can be interpreted as policies to\nmake decisions. We believe that these policies can be parameterized using DNNs, and be trained to\nobtain new and stronger algorithms for many different combinatorial optimization problems, similar\nto the way DNNs have boosted performance in the applications mentioned before. In this paper, we\nfocus on routing problems: an important class of practical combinatorial optimization problems.\n1\nPublished as a conference paper at ICLR 2019\nThe promising idea to learn heuristics has been tested on TSP (Bello et al., 2016). In order to\npush this idea, we need better models and better ways of training. Therefore, we propose to use a\npowerful model based on attention and we propose to train this model using REINFORCE with a\nsimple but effective greedy rollout baseline. The goal of our method is not to outperform a non\u0002learned, specialized TSP algorithm such as Concorde (Applegate et al., 2006). Rather, we show\nthe flexibility of our approach on multiple (routing) problems of reasonable size, with a single set\nof hyperparameters. This is important progress towards the situation where we can learn strong\nheuristics to solve a wide range of different practical problems for which no good heuristics exist.\n2 RELATED WORK\nThe application of Neural Networks (NNs) for optimizing decisions in combinatorial optimization\nproblems dates back to Hopfield & Tank (1985), who applied a Hopfield-network for solving small\nTSP instances. NNs have been applied to many related problems (Smith, 1999), although in most\ncases in an online manner, starting \u2018from scratch\u2019 and \u2018learning\u2019 a solution for every instance. More\nrecently, (D)NNs have also been used offline to learn about an entire class of problem instances.\nVinyals et al. (2015) introduce the Pointer Network (PN) as a model that uses attention to output\na permutation of the input, and train this model offline to solve the (Euclidean) TSP, supervised\nby example solutions. Upon test time, their beam search procedure filters invalid tours. Bello\net al. (2016) introduce an Actor-Critic algorithm to train the PN without supervised solutions. They\nconsider each instance as a training sample and use the cost (tour length) of a sampled solution for\nan unbiased Monte-Carlo estimate of the policy gradient. They introduce extra model depth in the\ndecoder by an additional glimpse (Vinyals et al., 2016) at the embeddings, masking nodes already\nvisited. For small instances (n = 20), they get close to the results by Vinyals et al. (2015), they\nimprove for n = 50 and additionally include results for n = 100. Nazari et al. (2018) replace\nthe LSTM encoder of the PN by element-wise projections, such that the updated embeddings after\nstate-changes can be effectively computed. They apply this model on the Vehicle Routing Problem\n(VRP) with split deliveries and a stochastic variant.\nDai et al. (2017) do not use a separate encoder and decoder, but a single model based on graph\nembeddings. They train the model to output the order in which nodes are inserted into a partial tour,\nusing a helper function to insert at the best possible location. Their 1-step DQN (Mnih et al., 2015)\ntraining method trains the algorithm per step and incremental rewards provided to the agent at every\nstep effectively encourage greedy behavior. As mentioned in their appendix, they use the negative\nof the reward, which combined with discounting encourages the agent to insert the farthest nodes\nfirst, which is known to be an effective heuristic (Rosenkrantz et al., 2009).\nNowak et al. (2017) train a Graph Neural Network in a supervised manner to directly output a tour\nas an adjacency matrix, which is converted into a feasible solution by a beam search. The model\nis non-autoregressive, so cannot condition its output on the partial tour and the authors report an\noptimality gap of 2.7% for n = 20, worse than autoregressive approaches mentioned in this section.\nKaempfer & Wolf (2018) train a model based on the Transformer architecture (Vaswani et al., 2017)\nthat outputs a fractional solution to the multiple TSP (mTSP). The result can be seen as a solution to\nthe linear relaxation of the problem and they use a beam search to obtain a feasible integer solution.\nIndependently of our work, Deudon et al. (2018) presented a model for TSP using attention in the OR\ncommunity. They show performance can improve using 2OPT local search, but do not show benefit\nof their model in direct comparison to the PN. We use a different decoder and improved training\nalgorithm, both contributing to significantly improved results, without 2OPT and additionally show\napplication to different problems. For a full discussion of the differences, we refer to Appendix B.4.\n3 ATTENTION MODEL\nWe define the Attention Model in terms of the TSP. For other problems, the model is the same but the\ninput, mask and decoder context need to be defined accordingly, which is discussed in the Appendix.\nWe define a problem instance s as a graph with n nodes, where node i \u2208 {1, . . . , n} is represented\nby features xi. For TSP, xiis the coordinate of node i and the graph is fully connected (with self\u0002connections) but in general, the model can be considered a Graph Attention Network (Velickovic\n2\nPublished as a conference paper at ICLR 2019\net al., 2018) and take graph structure into account by a masking procedure (see Appendix A). We\ndefine a solution (tour) \u03c0 = (\u03c01, . . . , \u03c0n) as a permutation of the nodes, so \u03c0t \u2208 {1, . . . n} and\n\u03c0t 6= \u03c0t\n0 \u2200t 6= t\n0\n. Our attention based encoder-decoder model defines a stochastic policy p(\u03c0|s) for\nselecting a solution \u03c0 given a problem instance s. It is factorized and parameterized by \u03b8 as\np\u03b8(\u03c0|s) = Yn\nt=1\np\u03b8(\u03c0t|s,\u03c01:t\u22121). (1)\nThe encoder produces embeddings of all input nodes. The decoder produces the sequence \u03c0 of input\nnodes, one node at a time. It takes as input the encoder embeddings and a problem specific mask and\ncontext. For TSP, when a partial tour has been constructed, it cannot be changed and the remaining\nproblem is to find a path from the last node, through all unvisited nodes, to the first node. The order\nand coordinates of other nodes already visited are irrelevant. To know the first and last node, the\ndecoder context consists (next to the graph embedding) of embeddings of the first and last node.\nSimilar to Bello et al. (2016), the decoder observes a mas",
          "original_query": "Attention, Learn to Solve Routing Problems!",
          "cleaned_query": "Attention, Learn to Solve Routing Problems!",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "The Cross-Entropy Method - Springer Link",
          "url": "https://link.springer.com/book/10.1007/978-1-4757-4321-0",
          "content": "The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning | Springer Nature Link (formerly SpringerLink)\n[Skip to main content](#main-content)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/book/10.1007/978-1-4757-4321-0?)\n[![](https://media.springernature.com/w90/springer-static/cover-hires/book/978-1-4757-4321-0?as=webp)](https://link.springer.com/book/10.1007/978-1-4757-4321-0/cover)\n# The Cross-Entropy Method\nA Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning\n* Book\n* &copy;2004\n* 1st edition\n* [View latest edition](https://link.springer.com/book/9780387212401)\n[Accessibility Information](#accessibility-information)\n## Overview\nAuthors:\n* [Reuven Y. Rubinstein](#author-0-0)[0](#Aff-0-0),\n* [Dirk P. Kroese](#author-0-1)[1](#Aff-0-1)\n1. Reuven Y. Rubinstein\n1. Department of Industrial Engineering and Management, Technion, Technion City, Haifa, Israel\n[View author publications](https://link.springer.com/search?dc.creator=Reuven+Y.+Rubinstein&sortBy=newestFirst)\nSearch author on:[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Reuven+Y.+Rubinstein)[Google Scholar](http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=\"Reuven+Y.+Rubinstein\"&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en)\n2. Dirk P. Kroese\n1. Department of Mathematics, University of Queensland, Brisbane, Australia\n[View author publications](https://link.springer.com/search?dc.creator=Dirk+P.+Kroese&sortBy=newestFirst)\nSearch author on:[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Dirk+P.+Kroese)[Google Scholar](http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=\"Dirk+P.+Kroese\"&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en)\n* A comprehensive and accessible introduction to the cross-entropy (CE) method\n* Based on an advanced undergraduate course on the CE method, given at the Israel Institute of Technology (Technion) for the last three years\n* Includes supplementary material:[sn.pub/extras](https://extras.springer.com/?query=978-0-387-21240-1)\nPart of the book series:[Information Science and Statistics](https://link.springer.com/series/3816)(ISS)\n* 13kAccesses\n* 936Citations\n* 8[Altmetric](https://link.altmetric.com/details/32599055)\nThis is a preview of subscription content,[log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/book/10.1007/978-1-4757-4321-0?error=cookies_not_supported&code=5facf87d-0eba-451f-a77f-ad63f58c53a5)to check access.\n## Access this book\n[Log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/book/10.1007/978-1-4757-4321-0?error=cookies_not_supported&code=5facf87d-0eba-451f-a77f-ad63f58c53a5)\nSoftcover BookUSD129.99\nPrice excludes VAT (USA)\n* Compact, lightweight edition\n* Dispatched in 3 to 5 business days\n* Free shipping worldwide -[see info](https://support.springernature.com/en/support/solutions/articles/6000233448-coronavirus-disease-covid-19-delivery-information)Buy Softcover Book\nHardcover BookUSD179.99\nPrice excludes VAT (USA)\n* Durable hardcover edition\n* Dispatched in 3 to 5 business days\n* Free shipping worldwide -[see info](https://support.springernature.com/en/support/solutions/articles/6000233448-coronavirus-disease-covid-19-delivery-information)Buy Hardcover Book\nTax calculation will be finalised at checkout\n[Licence this eBook for your library](https://single-ebooks.springernature.com/search?query=10.1007/978-1-4757-4321-0)\n[Learn about institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n## Other ways to access\n[Licence this eBook for your library](https://single-ebooks.springernature.com/search?query=10.1007/978-1-4757-4321-0)\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n## About this book\nThis book is a comprehensive and accessible introduction to the cross-entropy (CE) method. The CE method started life around 1997 when the first author proposed an adaptive algorithm for rare-event simulation using a cross-entropy minimization technique. It was soon realized that the underlying ideas had a much wider range of application than just in rare-event simulation; they could be readily adapted to tackle quite general combinatorial and multi-extremal optimization problems, including many problems associated with the field of learning algorithms and neural computation. The book is based on an advanced undergraduate course on the CE method, given at the Israel Institute of Technology (Technion) for the last three years. It is aimed at a broad audience of engineers, computer scientists, mathematicians, statisticians and in general anyone, theorist or practitioner, who is interested in smart simulation, fast optimization, learning algorithms, image processing, etc. Our aim was to write a book on the CE method which was accessible to advanced undergraduate students and engineers who simply want to apply the CE method in their work, while at the same time accentu\u00ad ating the unifying and novel mathematical ideas behind the CE method, so as to stimulate further research at a postgraduate level.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs40430-025-05764-x/MediaObjects/40430_2025_5764_Fig1_HTML.png)\n### [A single-loop Kriging model coupled with cross-entropy importance sampling for time-variant reliability analysis of rare events](https://link.springer.com/10.1007/s40430-025-05764-x?fromPaywallRec=true)\nArticle10 July 2025\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-35897-5?as&#x3D;webp)\n### [Introducing Computer Science and Arts for All (CSA4ALL): Developing an Inclusive Curriculum and Portal for K5 Children](https://link.springer.com/10.1007/978-3-031-35897-5_24?fromPaywallRec=true)\nChapter\u00a9 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10639-017-9677-z/MediaObjects/10639_2017_9677_Fig1_HTML.gif)\n### [Lines, roamers, and squares: Oh my! using floor robots to enhance Hispanic students\u2019 understanding of programming](https://link.springer.com/10.1007/s10639-017-9677-z?fromPaywallRec=true)\nArticle21 December 2017\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects.\n* [Computer Modelling](https://link.springer.com/subjects/computer-modelling)\n* [Computational Intelligence](https://link.springer.com/subjects/computational-intelligence)\n* [Operations Research and Decision Theory](https://link.springer.com/subjects/operations-research-and-decision-theory)\n* [Probability and Statistics in Computer Science](https://link.springer.com/subjects/probability-and-statistics-in-computer-science)\n* [Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences](https://link.springer.com/subjects/statistics-in-engineering-physics-computer-science-chemistry-and-earth-sciences)\n* [Operations Research, Management Science](https://link.springer.com/subjects/operations-research-management-science-)\nSearch within this book\nSearch\n## Table of contents (8 chapters)\n1. ### Front Matter\nPages i-xx\n[Download chapterPDF](https://link.springer.com/content/pdf/bfm:978-1-4757-4321-0/1)\n2. ### [Preliminaries](https://link.springer.com/chapter/10.1007/978-1-4757-4321-0_1)\n* Reuven Y. Rubinstein, Dirk P. Kroese\nPages 1-28\n* ### [A Tutorial Introduction to the Cross-Entropy Method](https://link.springer.com/chapter/10.1007/978-1-4757-4321-0_2)\n* Reuven Y. Rubinstein, Dirk P. Kroese\nPages 29-58\n* ### [Efficient Simulation via Cross-Entropy](https://link.springer.com/chapter/10.1007/978-1-4757-4321-0_3)\n* Reuven Y. Rubinstein, Dirk P. Kroese\nPages 59-128\n* ### [Combinatorial Optimization via Cross-Entropy](https://link.springer.com/chapter/10.1007/978-1-4757-4321-0_4)\n* Reuven Y. Rubinstein, Dirk P. Kroese\nPages 129-186\n* ### [Continuous Optimization and Modifications](https://link.springer.com/chapter/10.1007/978-1-4757-4321-0_5)\n* Reuven Y. Rubinstein, Dirk P. Kroese\nPages 187-201\n* ### [Noisy Optimization with CE](https://link.springer.com/chapter/10.1007/978-1-4757-4321-0_6)\n* Reuven Y. Rubinstein, Dirk P. Kroese\nPages 203-225\n* ### [Applications of CE to COPs](https://link.springer.com/chapter/10.1007/978-1-4757-4321-0_7)\n* Reuven Y. Rubinstein, Dirk P. Kroese\nPages 227-250\n* ### [Applications of CE to Machine Learning](https://link.springer.com/chapter/10.1007/978-1-4757-4321-0_8)\n* Reuven Y. Rubinstein, Dirk P. Kroese\nPages 251-270\n* ### Back Matter\nPages 271-301\n[Download chapterPDF](https://link.springer.com/content/pdf/bbm:978-1-4757-4321-0/1)\n[Back to top](#back-to-top)\n## Reviews\nFrom the reviews:\n\"Rarely have I seen such a dense and straight to the point pedagogical monograph on such a modern subject. This excellent book, on the simulated cross-entropy method (CEM) pioneered by one of the authors (Rubinstein), is very well written...\"*Computing Reviews, Stochastic Programming November, 2004*\n\"...I wholeheartedly recommend this book to anybody who is interested in stochastic optimization or simulation-based performance analysis of stochastic systems.\"*Gazette of the Australian Mathematical Society, vol. 32 (3) 2005*\n\"This book describes the cross-entropy method for a range of optimization problems. \u2026It is a substantial contribution to stochastic optimization and more generally to the stochastic numerical methods",
          "original_query": "The Cross\u2011Entropy Method: A Unified Approach to Combinatorial Optimization, Monte\u2011Carlo Simulation and Machine Learning",
          "cleaned_query": "The Cross\u2011Entropy Method: A Unified Approach to Combinatorial Optimization, Monte\u2011Carlo Simulation and Machine Learning",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "UCT-Guided Attention Decoding for Routing (MCTS over pointer policies)\nCombine an attention-based routing policy (as in Kool et al.) with a shallow UCT search at decoding time, where actions are candidate next nodes and rollouts are completed by the neural policy. Train the policy/value heads to predict UCT-improved targets (policy from visit counts, value from backed-up returns), mirroring AlphaGo\u2019s policy/value + tree search loop for TSP/VRP.",
        "Cross-Entropy Fine-Tuning of Neural Routing Policies with Elite Trajectory Distillation\nUse the Cross-Entropy Method (CEM) to sample many solution tours from a neural policy, select elite tours by cost/constraint satisfaction, and update the policy by minimizing KL divergence to the elite distribution. Distill the resulting \u201cCEM-improved\u201d policy back into the attention model, comparing against REINFORCE baselines on constrained/stochastic VRP variants.",
        "Value-Network Bootstrapping for Many-Objective Industrial Routing and Scheduling\nExtend the AlphaGo-style value network to predict a vector of outcomes (cost, CO\u2082, tardiness, robustness) and use it to provide low-variance baselines and pruning signals during solution construction. Implement Pareto-aware backup rules in search (e.g., scalarization schedules or dominance-based UCT) to target industrial many-objective instances highlighted in the NCO-RL survey.",
        "Risk-Sensitive UCT for Stochastic Prize-Collecting and Resilient VRP\nReplace expected-return backups in UCT with risk measures (CVaR, entropic risk) and train the neural policy to optimize these criteria under demand/travel-time uncertainty. Evaluate whether risk-aware tree search + learned priors yields more reliable solutions than expectation-optimized attention policies in stochastic PCTSP/VRP.",
        "Constraint-Satisfaction-Aware Policy Heads via Feasibility Priors and Repair Rollouts\nAdd a feasibility head that predicts probability of completing a partial solution without violating constraints (capacity, time windows, precedence) and inject it into UCT selection and rollout policies as a prior. Couple this with learned repair operators (local search actions) so rollouts can \u201cfix\u201d near-feasible partial solutions, improving performance on heavily constrained industrial instances.",
        "Self-Play Curriculum for Combinatorial Optimization via Adversarial Instance Generation\nCreate a \u201cself-play\u201d loop where an instance generator adversary proposes hard routing/scheduling instances and the solver improves against them, analogous to AlphaGo\u2019s self-play but over instance distributions. Use bandit-style selection to focus training on failure modes (high regret/constraint violations), aiming for better out-of-distribution generalization than training on i.i.d. synthetic instances.",
        "Bandit-Based Adaptive Action Sets for Large-Branching Routing (Top-k is learned online)\nInstead of fixed candidate lists (nearest neighbors/top-k), learn an online bandit mechanism that adaptively chooses which subset of next-node actions to expand at each step in UCT. The key contribution is a principled exploration\u2013exploitation controller for branching reduction that can be plugged into attention decoders and shown to reduce compute while preserving solution quality.",
        "Neural Priors for CEM Parameterization in Discrete CO (Hybrid distribution learning)\nParameterize the CEM sampling distribution with an attention model that outputs per-step categorical distributions, then run CEM updates in that parameter space (rather than simple factorized heuristics). This yields a unified \u201cneural CEM\u201d method that can be compared to policy-gradient training for sample efficiency and stability across routing and scheduling tasks.",
        "Multi-Fidelity Planning: Cheap Rollouts + Expensive Value Net Scheduling\nDevelop a planner that mixes rollout types: fast heuristic/greedy rollouts for breadth and expensive value-network evaluations for depth, with UCT choosing which fidelity to use based on uncertainty. The contribution is an allocation strategy (a bandit over rollout fidelities) that reduces inference cost while retaining the benefits of value networks in large industrial instances.",
        "Interpretable Attention-to-Policy Distillation from Tree Search for Human-Centric Decision Support\nRun UCT guided by neural priors to generate search traces, then distill them into a compact, interpretable attention policy (e.g., sparse attention maps + rule-like feature attributions) tailored for operator trust in Industry"
      ],
      "generated_ideas_raw": "1. **UCT-Guided Attention Decoding for Routing (MCTS over pointer policies)**  \n   Combine an attention-based routing policy (as in Kool et al.) with a shallow UCT search at decoding time, where actions are candidate next nodes and rollouts are completed by the neural policy. Train the policy/value heads to predict UCT-improved targets (policy from visit counts, value from backed-up returns), mirroring AlphaGo\u2019s policy/value + tree search loop for TSP/VRP.\n\n2. **Cross-Entropy Fine-Tuning of Neural Routing Policies with Elite Trajectory Distillation**  \n   Use the Cross-Entropy Method (CEM) to sample many solution tours from a neural policy, select elite tours by cost/constraint satisfaction, and update the policy by minimizing KL divergence to the elite distribution. Distill the resulting \u201cCEM-improved\u201d policy back into the attention model, comparing against REINFORCE baselines on constrained/stochastic VRP variants.\n\n3. **Value-Network Bootstrapping for Many-Objective Industrial Routing and Scheduling**  \n   Extend the AlphaGo-style value network to predict a vector of outcomes (cost, CO\u2082, tardiness, robustness) and use it to provide low-variance baselines and pruning signals during solution construction. Implement Pareto-aware backup rules in search (e.g., scalarization schedules or dominance-based UCT) to target industrial many-objective instances highlighted in the NCO-RL survey.\n\n4. **Risk-Sensitive UCT for Stochastic Prize-Collecting and Resilient VRP**  \n   Replace expected-return backups in UCT with risk measures (CVaR, entropic risk) and train the neural policy to optimize these criteria under demand/travel-time uncertainty. Evaluate whether risk-aware tree search + learned priors yields more reliable solutions than expectation-optimized attention policies in stochastic PCTSP/VRP.\n\n5. **Constraint-Satisfaction-Aware Policy Heads via Feasibility Priors and Repair Rollouts**  \n   Add a feasibility head that predicts probability of completing a partial solution without violating constraints (capacity, time windows, precedence) and inject it into UCT selection and rollout policies as a prior. Couple this with learned repair operators (local search actions) so rollouts can \u201cfix\u201d near-feasible partial solutions, improving performance on heavily constrained industrial instances.\n\n6. **Self-Play Curriculum for Combinatorial Optimization via Adversarial Instance Generation**  \n   Create a \u201cself-play\u201d loop where an instance generator adversary proposes hard routing/scheduling instances and the solver improves against them, analogous to AlphaGo\u2019s self-play but over instance distributions. Use bandit-style selection to focus training on failure modes (high regret/constraint violations), aiming for better out-of-distribution generalization than training on i.i.d. synthetic instances.\n\n7. **Bandit-Based Adaptive Action Sets for Large-Branching Routing (Top-k is learned online)**  \n   Instead of fixed candidate lists (nearest neighbors/top-k), learn an online bandit mechanism that adaptively chooses which subset of next-node actions to expand at each step in UCT. The key contribution is a principled exploration\u2013exploitation controller for branching reduction that can be plugged into attention decoders and shown to reduce compute while preserving solution quality.\n\n8. **Neural Priors for CEM Parameterization in Discrete CO (Hybrid distribution learning)**  \n   Parameterize the CEM sampling distribution with an attention model that outputs per-step categorical distributions, then run CEM updates in that parameter space (rather than simple factorized heuristics). This yields a unified \u201cneural CEM\u201d method that can be compared to policy-gradient training for sample efficiency and stability across routing and scheduling tasks.\n\n9. **Multi-Fidelity Planning: Cheap Rollouts + Expensive Value Net Scheduling**  \n   Develop a planner that mixes rollout types: fast heuristic/greedy rollouts for breadth and expensive value-network evaluations for depth, with UCT choosing which fidelity to use based on uncertainty. The contribution is an allocation strategy (a bandit over rollout fidelities) that reduces inference cost while retaining the benefits of value networks in large industrial instances.\n\n10. **Interpretable Attention-to-Policy Distillation from Tree Search for Human-Centric Decision Support**  \n   Run UCT guided by neural priors to generate search traces, then distill them into a compact, interpretable attention policy (e.g., sparse attention maps + rule-like feature attributions) tailored for operator trust in Industry 5.0 settings. Quantify interpretability\u2013performance trade-offs and test whether distilled policies better match human constraints/preferences while retaining near-search-level solution quality.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "UCT-Guided Attention Decoding for Routing (MCTS over pointer policies)\nCombine an attention-based routing policy (as in Kool et al.) with a shallow UCT search at decoding time, where actions are candi",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Cross-Entropy Fine-Tuning of Neural Routing Policies with Elite Trajectory Distillation\nUse the Cross-Entropy Method (CEM) to sample many solution tours from a neural policy, select elite tours by cos",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Value-Network Bootstrapping for Many-Objective Industrial Routing and Scheduling\nExtend the AlphaGo-style value network to predict a vector of outcomes (cost, CO\u2082, tardiness, robustness) and use it to",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Risk-Sensitive UCT for Stochastic Prize-Collecting and Resilient VRP\nReplace expected-return backups in UCT with risk measures (CVaR, entropic risk) and train the neural policy to optimize these crite",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Constraint-Satisfaction-Aware Policy Heads via Feasibility Priors and Repair Rollouts\nAdd a feasibility head that predicts probability of completing a partial solution without violating constraints (c",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Self-Play Curriculum for Combinatorial Optimization via Adversarial Instance Generation\nCreate a \u201cself-play\u201d loop where an instance generator adversary proposes hard routing/scheduling instances and t",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Bandit-Based Adaptive Action Sets for Large-Branching Routing (Top-k is learned online)\nInstead of fixed candidate lists (nearest neighbors/top-k), learn an online bandit mechanism that adaptively cho",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Neural Priors for CEM Parameterization in Discrete CO (Hybrid distribution learning)\nParameterize the CEM sampling distribution with an attention model that outputs per-step categorical distributions,",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Multi-Fidelity Planning: Cheap Rollouts + Expensive Value Net Scheduling\nDevelop a planner that mixes rollout types: fast heuristic/greedy rollouts for breadth and expensive value-network evaluations ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Interpretable Attention-to-Policy Distillation from Tree Search for Human-Centric Decision Support\nRun UCT guided by neural priors to generate search traces, then distill them into a compact, interpre",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 12,
      "paper_title": "High-Dimensional Calibration from Swap Regret",
      "contribution": "Shows that multi-dimensional online calibration over any convex P and norm ||\u00b7|| reduces to a swap-regret control implied by optimal regularizers for online linear optimization, and uses TreeSwap+FTL to obtain efficient high-dimensional calibration rates (T = exp(O(\u03c1/\u03b5^2))) recovering and generalizing prior polynomial-in-d bounds without requiring OLO subroutines or knowledge of \u03c1.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 10783,
      "output_tokens": 1009,
      "predecessor_details": [
        {
          "success": true,
          "title": "Asymptotic Calibration",
          "url": "https://www.jstor.org/stable/2337364",
          "content": "\n \n \n This is a preview.\n \n Log in through your library.\n \n \n \n \n \n Preview\n \n \n \n \n \n \n \n Abstract\n \n Can we forecast the probability of an arbitrary sequence of events happening so that the stated probability of an event happening is close to its empirical probability? We can view this prediction problem as a game played against Nature, where at the beginning of the game Nature picks a data sequence and the forecaster picks a forecasting algorithm. If the forecaster is not allowed to randomise, then Nature wins; there will always be data for which the forecaster does poorly. This paper shows that, if the forecaster can randomise, the forecaster wins in the sense that the forecasted probabilities and the empirical probabilities can be made arbitrarily close to each other. \n \n \n Journal Information\n Biometrika is primarily a journal of statistics in which emphasis is\nplaced on papers containing original theoretical contributions of direct\nor potential value in applications. From time to time, papers in bordering\nfields are published. \n \n \n Publisher Information\n \nOxford University Press is a department of the University of Oxford. It furthers the University's objective of excellence in research, scholarship, and education by publishing worldwide. OUP is the world's largest university press with the widest global presence. It currently publishes more than 6,000 new publications a year, has offices in around fifty countries, and employs more than 5,500 people worldwide. It has become familiar to millions through a diverse publishing program that includes scholarly works in all academic disciplines, bibles, music, school and college textbooks, business books, dictionaries and reference books, and academic journals.\n \n \n \n Rights &amp; Usage\n \n This item is part of a JSTOR Collection. \n For terms and use, please refer to our Terms and Conditions \n \n \n Biometrika\n \u00a9 1998 Biometrika Trust \n \n \n \n \n \n \n Request Permissions\n \n \n \n \n \n ||||I|||| Don't have an account? Register for free\n\n Username or email address You can use your Artstor username and password to log in.\n Password SHOW\n Stay logged in Forgot password?\n Log in\n or\n Log in with Google\n Find my institution\n\n Your use of JSTOR indicates your acceptance of the Terms & Conditions of Use, the Privacy Policy, and that you are 16 or older.\n\n Have library access? Log in through your library\n Log inRegister\n WorkspaceAdvanced SearchImagesBy subjectBy titlePublishersCollectionsImagesWorkspaceText AnalyzerThe JSTOR Understanding SeriesData for ResearchAboutSupport\n Log in\n Skip to Main Content\n Have library access? Log in through your library\n All Content Images\n RegisterLog in\n WorkspaceSearchAdvanced SearchImagesBrowseBy subject\n Journals and books By title\n Journals and books PublishersCollectionsImagesToolsWorkspaceText AnalyzerThe JSTOR Understanding SeriesData for Research\n AboutSupport\n journal article\n Asymptotic Calibration\n Dean P. Foster and Rakesh V. Vohra\n Biometrika\n Vol. 85, No. 2 (Jun., 1998), pp. 379-390 (12 pages)\n Published By: Oxford University Press\n https://www. jstor .org /stable/2337364\n Cite\n Read and download\n Log in through your school or library\n Alternate access options\n For independent researchers\n Read Online\n Read 100 articles/month free\n Subscribe to JPASS\n Unlimited reading + 10 downloads\n This is a preview. Log in through your library.\n Preview\n Abstract\n\n Can we forecast the probability of an arbitrary sequence of events happening so that the stated probability of an event happening is close to its empirical probability? We can view this prediction problem as a game played against Nature, where at the beginning of the game Nature picks a data sequence and the forecaster picks a forecasting algorithm. If the forecaster is not allowed to randomise, then Nature wins; there will always be data for which the forecaster does poorly. This paper shows that, if the forecaster can randomise, the forecaster wins in the sense that the forecasted probabilities and the empirical probabilities can be made arbitrarily close to each other.\n\n Journal Information\n\n Biometrika is primarily a journal of statistics in which emphasis is placed on papers containing original theoretical contributions of direct or potential value in applications. From time to time, papers in bordering fields are published.\n\n Publisher Information\n\n Oxford University Press is a department of the University of Oxford. It furthers the University's objective of excellence in research, scholarship, and education by publishing worldwide. OUP is the world's largest university press with the widest global presence. It currently publishes more than 6,000 new publications a year, has offices in around fifty countries, and employs more than 5,500 people worldwide. It has become familiar to millions through a diverse publishing program that includes scholarly works in all academic disciplines, bibles, music, school and college textbooks, business books, dictionaries and reference books, and academic journals.\n\n Rights & Usage\n\n This item is part of a JSTOR Collection.\n For terms and use, please refer to our Terms and Conditions\n Biometrika \u00a9 1998 Biometrika Trust\n Request Permissions\n\n * By Subject\n * By Title\n * Collections\n * Publisher\n * Advanced Search\n * Images\n * Data for Research\n * Get Access\n * Get Support\n * LibGuides\n * Research Basics\n * About JSTOR\n * Mission and History\n * What's in JSTOR\n * Get JSTOR\n * News\n * Webinars\n * JSTOR Labs\n * JSTOR Daily\n * Careers\n * Contact Us\n * For Librarians\n * For Publishers\n * \n * \n * \n * \n * \n * \n JSTOR is part of ITHAKA , a not-for-profit organization helping the academic community use digital technologies to preserve the scholarly record and to advance research and teaching in sustainable ways. \u00a92000\u200d\u20132023 ITHAKA. All Rights Reserved. JSTOR\u00ae, the JSTOR logo, JPASS\u00ae, Artstor\u00ae, Reveal Digital\u2122 and ITHAKA\u00ae are registered trademarks of ITHAKA.\n * Terms & Conditions of Use\n * Privacy Policy\n * Accessibility\n * Cookie Policy\n * Cookie Settings\n Select Language \u200b \u25bc\n\n ITHAKA websites, which ITHAKA manages from its location in the United States, use cookies for different purposes, such as to ensure web site function, display non-targeted ads, provide social media features, and track usage, engaging with third party service providers such as Google Analytics. You may manage non-essential cookies in \u201cCookie Settings\u201d. For more information, please see our Cookie Policy.\n\n Cookie Settings OK, proceed\n\n Cookie Preference Center\n\n * Cookie Settings\n\n * Strictly Necessary Cookies\n\n * Performance and Analytics Cookies\n\n * Social Media Cookies\n\n * Advertising Cookies\n\n * Functional Cookies\n\n Cookie Settings\n\n When you visit our websites, we store cookies on your browser to collect information. The information collected might relate to you, your preferences or your device, and is mostly used to make the sites work as you expect them to and to provide a more personalized web experience. However, you can choose not to allow certain types of cookies, which may impact your experience of the sites and the services we are able to offer. Click on the different category headings to find out more and change our default settings according to your preference. Please refresh the web page or navigate to another page on the site to apply your changes. You cannot opt-out of our strictly necessary cookies as they are deployed in order to ensure the proper functioning of our website (such as prompting the cookie banner and remembering your settings, to log into your account, to redirect you when you log out, etc.). For more information about the first and third party cookies used please follow this link:\n ITHAKA Cookie Policy\n\n Strictly Necessary Cookies\n\n Always Active\n\n These cookies are necessary for our websites to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. Some may be set by us or by third party providers whose services we have added to our pages. You can set your browser to block or alert you about these cookies, but some parts of our sites will not then work. These cookies do not store any personally identifiable information.\n\n Performance and Analytics Cookies\n\n Performance and Analytics Cookies\n\n These cookies, which include Google Analytics, allow us to count visits and traffic sources so we can measure and improve the performance of our sites. They help us to know which pages are the most and least popular and see how users interact with each of ITHAKA\u2019s sites. This information is also used to compile reports to help ITHAKA improve the respective site, including reports on the number of visitors to the site, where the visitors have come from and what pages the users visit on the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies, we will not know when you have visited our sites, and will not be able to monitor their performance.\n\n Social Media Cookies\n\n Social Media Cookies\n\n These cookies are set by a range of social media services that we have added to the site to enable you to share our content with your friends and networks. They are capable of tracking your browser across other sites and building up a profile of your interests. This may impact the content and messages you see on other websites you visit. If you do not allow these cookies, you may not be able to use or see these sharing tools.\n\n Advertising Cookies\n\n Advertising Cookies\n\n For ITHAKA websites that display advertising, cookies identify the beginning of a unique user session in order to display generic ads during the session. ITHAKA does not capture information about a user session in order to display targeted ads. However, advertising cookies may also be set through our site by advertising entities. They may be used by those companies to buil",
          "original_query": "Foster, D. P. and Vohra, R. (1998). Calibration of Forecasters",
          "cleaned_query": "Foster, D. P. and Vohra, R.. Calibration of Forecasters",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "An analog of the minimax theorem for vector payoffs - MSP",
          "url": "https://msp.org/pjm/1956/6-1/pjm-v6-n1-p01-s.pdf",
          "content": "Pacific Journal of\nMathematics\nAN ANALOG OF THE MINIMAX THEOREM FOR VECTOR\nPAYOFFS\nDAVID BLACKWELL\nVol. 6, No. 1 November 1956\nAN ANALOG OF THE MINIMAX THEOREM FOR\nVECTOR PAYOFFS\nDAVID BLACKWELL\n1. Introduction* The von Neumann minimax theorem [2] for finite\ngames asserts that for every rxs matrix M=\\\\m(i, j)\\\\ with real elements\nthere exist a number v and vectors\nP=(Pi , \u2022\u2022\u2022, Pr)f Q={Q U \u2022\u2022\u2022> Qs)f Pi, Qj>\u03b2,\nsuch that\ni> 3)\nfor all i, j . Thus in the (two-person, zero-sum) game with matrix \u039bf,\nplayer I has a strategy insuring an expected gain of at least v, and\nplayer II has a strategy insuring an expected loss of at most v. An\nalternative statement, which follows from the von Neumann theorem\nand an appropriate law of large numbers is that, for any \u03b5>0, I can,\nin a long series of plays of the game with matrix M, guarantee, with\nprobability approaching 1 as the number of plays becomes infinite, that\nhis average actual gain per play exceeds v \u2014 \u03b5 and that II can similarly\nrestrict his average actual loss to v-he. These facts are assertions about\nthe extent to which each player can control the center of gravity of\nthe actual payoffs in a long series of plays. In this paper we investigate\nthe extent to which this center of gravity can be controlled by the\nplayers for the case of matrices M whose elements m(i9 j) are points\nof \u039b\u0393-space. Roughly, we seek to answer the following question. Given\na matrix M and a set S in iV-space, can I guarantee that the center of\ngravity of the payoffs in a long series of plays is in or arbitrarily near\nSt with probability approaching 1 as the number of plays becomes in\nfinite ? The question is formulated more precisely below, and a complete\nsolution is given in two cases: the case JV=1 and the case of convex S.\nLet\nf = \\\\m(i, j) 1, l^i^r, l^j^s\nbe an rx s matrix, each element of which is a probability distribution\nover a closed bounded convex set X in Euclidean iV-space. By a\nstrategy for Player I is meant a sequence /={/\u201e} , n=0, 1, 2, \u2022\u2022\u2022 of\nfunctions, where f\nn\n is defined on the set of ^-tuples (xu \u2022\u2022\u2022, xn), xteX\nReceived September 14, 1954. This paper was written under contract Nonr 1197(00)\nwith the Office of Naval Research.\n2 DAVID BLACKWELL\nand has values in the set P of vectors p=(Pu , pr) with\n1 /o is simply a point in P. A strategy g= {gn} for Player II is defined\nsimilarly, except that the values of gn are in the set Q of vectors q=\n(Qu \u2022\u2014, Qs) with qj^O, \u03a3f^=l . The interpretation is that I, II select\ni, j according to the distributions / 0, g0 respectively, and a point x\u00b1eX\nis selected according to the distribution m(i, j). The players are told\nxl9 after which they again select i, j , this time according to the distri\nbutions fi(xi), g\u03b9{%\u03b9), a point x2 is chosen according to the m(i, j) cor\nresponding to their second choices, they are told x2 and select a third\ni, j according to f%(xly #2), g*{xu x2), etc. Thus each pair (/, g) of\nstrategies, together with M, determines a sequence of (vector-valued)\nrandom variables xu x2, .\nLet S be any set in iV-space. We shall say that S is approachable\nwith / * in M, if for every e>0 there is an JV0 such that, for every g,\nProb {dn^>\u03b5 for some n^>NQ} < ,\nwhere \u03b4n denotes the distance of the point \u03a3\u03ca iln from S and xu x2,\n\u2022 are the variables determined by /*, g. We shall say that S is ex\u0002cludable with g* in M, if there exists d>0 such that for every \u03b5>0\nthere is an NQ such that, for every / ,\nProb {\u03b4^d for all rc^\nwhere xlf x%, \u2022\u2022\u2022 are the variables determined by / , g*. We shall say\nthat & is approachable (excludable) in M, if there exists / * (g*) such\nthat S is approachable with / * (excludable with g*). Approachability\nand excludability are clearly the same for S and its closure, so that we\nmay suppose S closed.\nIn terms of these concepts, von Neumann's theorem has the follow\ning analog.\nFor N=l, associated with every M are a number v and vectors pe P,\nqeQ such that the set S= {#\u00a3>\u00a3} is approachable for t v with g : gn=q.\nA slightly more complete result for N=l, characterizing all ap\nproachable and excludable sets S for a given M, is given in \u00a7 4 below.\nObviously any superset of an approachable set is approachable, any\nsubset of an excludable set is excludable, and no set is both approach\nable and excludable. Another obvious fact which will be useful is that\nif a closed set & is approachable in the sxr matrix \u039bf', the transpose\nof jfcf, then any closed set T not intersecting S is excludable in M with\nany strategy with which S is approachable in M'. Thus any sufficient\ncondition for approachability yields immediately a sufficient condition for\nexcludability. A sufficient condition for approachability is given in \u00a7 2.\nIt turns out that every convex S satisfies either this condition for\nAN ANALOG OF THE MINIMAX THEOREM FOR VECTOR PAYOFFS 3\napproachability or the corresponding condition for excludability, enabling\nus to give in \u00a7 3 a complete solution for convex S. For non-convex S,\nthe problem is not solved except for 2V=1. An example of a set which\nis neither approachable nor excludable in a given M is given in \u00a7 5, the\nconcepts of weak approachability and excludability are introduced, and\nit is conjectured that every set is either weakly approachable or weakly\nexcludable.\n2 A sufficient condition for approachability. If x9 y are distinct\npoints in iV-space, H is the hyperplane through y perpendicular to the\nline segment xy, and z is any point on H or on the opposite side of H\nfrom xy then all points interior to the line segment xz and sufficiently\nnear x are closer to y than is x. This fact is the basis for our sufficient\ncondition for approachability.\nFor any matrix M, denote by M the matrix whose elements m{i, j)\nare the mean values of the distributions m(i, j). For any peP denote\nby R(p) the convex hull of the s points \u03a3\u00ab P\u03b9m(i, j). The sufficient\ncondition for approachability is given in the following theorem.\nTHEOREM 1. Let S be any closed set. If for every x\u03c6S there is a\np (=p(x))e P such that the hyperplane through y, the closest point in S\nto x, perpendicular to the line segment xy separates x from R{p), then\nS is approachable with the strategy f:f\nn\n, where\nfn\u0002xn\n) if n>0 and xn = (^\n\\n\n^arbitrary if n=0 or xneS.\nProof. Suppose the hypotheses satisfied, let I use the specified\nstrategy, let II use any strategy, and let xlf x2, be the resulting\nsequence of chance variables. For\nlet yn be the point of S closest to xn, and write un=yn \u2014 xn. Then, for\n(1) E((un, xn+\nwhere E(x\\y) denotes the conditional expectation of x given y and (u, v)\ndenotes the inner product of the vectors u and v.\nLet \u03b4n denote the squared distance from xn to S. If <5w>0, then\n(2) dn+1<:\\xn+1-yn\\\n2\n^\\xn-yn\\\n2\n + 2(xn-ynf xn+1-xn)+ \\xn+\u03b9-xn\\\\\n4 DAVID BLACKWELL\nSince xn+i\u2014Sn=(a?n+i \u20145n)/(w + l), we have\n/ Q \\ (7\u03b3. n, \u2122 \u2122 \\ fan Vn> \u2022^n + l^^Vn) i fan Vn* Vn ^n)\n71 + 1 92 + 1\nand\n( 4 ) l\u00bb\u00ab +i-Sn\nwhere c depends only on the size of the bounded set X. From (2), using\n(1), (3), and (4), we obtain, replacing n by n\u20141,\n( 5 ) E{dn\\\u03b4l9 -.., a n _ 1) ( 5\n\\ n / n\n2\nMoreover\n( 6 ) O^r^\nand\n( 7 ) |3n-3\u00bb-il^- .\n7Z\nThus it remains only to establish the following.\nLEMMA. A sequence of chance variables \u03b4lf \u03b42, satisfying (5),\n(6), and (7) converges to zero with probability 1 at a rate depending only\non a, \u03b4, c, that is, for every \u03b5>0 there is an NQ depending only on \u03b5,\n, bf c such that for auy {Sn} satisfying (5), (6), and (7), we have\nProb {\u03b4n^>e for some\nProof of Lemma. Let rc0 be any integer. There exists\ndepending only on nQf \u03b5, a, c such that\nProb {<5w:>\u03b5/2 for n^r\u03b9 n0, an=Sn if <54>O for no 0 at a rate depending only on n0, a, c, and there is an nx\ndepending only on n0, \u03b5, \u03b1, c for which E( l-(\u03b5/2).\nFor every n, k with n ej2, znk=0 for all k. If \u03b4n\n^<\u03b5j2 and feo. If \u03b4 w^ for some w^?ii, either <5wI>\u03b5/2\nfor all w such that no for some \u038eQ^UQ. The former\nevent has already been shown to have probability less than \u03b5/2 it\nremains to show that the probability of the latter event can be made\nless than 6/2 by choosing n0 sufficiently large.\nFix f\u00a3>n0 and write \u03b2jc=znk\u2014zn fc_\u03c7, &>rc, \u00a3n= 0 . Then, if zn fc-i^\u03b5/2\nfor sufficiently large ?z0 depending on c and \u03b5, and I\u03b2J <*&/&. If \u00ab\u00bb\u00bb-i \u00a3 for some fc}^(\u2014--Y\n1\nThe variables zk=(nlb)\u03b2k^n+1 satisfy the hypotheses of Theorem 2,\nwith w=(\u03b5/2\u03b4), so that\n/I \\l/6\nProb {\u00ab\u201e*-\u00ab\u201e\u201e>* for some k>\nFor large w0, ^ww<3\u03b5/4, so that ^TO\u039b^ for some k implies s\u00ab*\u2014sflf\u03b9>e/4.\nThus\nProb fefc^\u03b5 for some\nwhere s=r\n\u03b5/4\n, so that\nDAVID BLACKWELL\nProb {zn3bi>e for some ri^n0, \u039bC>w}<^\nwhich will be less than \u03b5/2 for n0 sufficiently large. This completes the\nproof.\n3 The case of convex S.\nTHEOREM 3. Let T{q) denote the convex hull of the r points\n Qj\u03caniif i) A closed convex set S is approachable if and only if it\nintersects every set T(q). If it fails to intersect T(q0), it is excludable\nwith g: gn^q0.\nProof Suppose S intersects every T(q), let xQ $ S, let y be the point\nof S closest to x09 and consider the game with matrix A=\\\\a(i, j)\\\\, where\n\u03b1(i, i)=(2/\u2014a?o> \u03can(i,j)) Its value is\nmin max {y \u2014 xQy \u03a3 Qjmfi* j))=mm max (y\u2014x0, t)^>min (y\u2014xo,s).\nq i j q teTQq) sGS\nConsequently there is a p e P such that\n(V-Xof \u03a3 Pim(i, i))^min (y-xOf s)\n ses\nfor all j , that is,\n(y-x0, r)^>(y-x0, y)\nfor all reR{p). Since (y\u2014xQ, Xo) min (u, s),\nwhere viu) is the value of the game with matrix |(%, mii, j))\\.\nProof of Corollary 2. If for some u0 the inequality fails, then T(q0)\nis disjoint from S, where q0 is a good strategy for II in the game with\nmatrix \\\\(uOf m(i9 j))\\\\f and conversely if any T(q0) is disjoint from Sand\nuQ is a vector with\nAN ANALOG OF THE MINIMAX THEOREM FOR VECTOR PAYOFFS\nmax (uQ, \u00a3) \u2014B. If v' v, they are the conditions that AB\ncontain vv\nf\n. Thus if v'<^v every point in v'v is approachable, so that\nany set S intersecting v'v contains an approachable subset and is hence\napproachable, while if v'^>v, the interval vv' and hence any set con\ntaining it, is approachable. The last sentence, applied to M', yields\nthat if v'",
          "original_query": "Blackwell, D. (1956). An analog of the minimax theorem for vector payoffs (Approachability)",
          "cleaned_query": "Blackwell, D.. An analog of the minimax theorem for vector payoffs (Approachability)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Online Convex Programming and Generalized Infinitesimal Gradient ...",
          "url": "https://people.eecs.berkeley.edu/~brecht/cs294docs/week1/03.Zinkevich.pdf",
          "content": "Online Convex Programming\nand Generalized Infinitesimal Gradient Ascent\nMartin Zinkevich maz@cs.cmu.edu\nCarnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213 USA\nAbstract\nConvex programming involves a convex set\nF \u2286 R\nn and a convex cost function c :\nF \u2192 R. The goal of convex programming\nis to find a point in F which minimizes c.\nIn online convex programming, the convex\nset is known in advance, but in each step\nof some repeated optimization problem, one\nmust select a point in F before seeing the cost\nfunction for that step. This can be used to\nmodel factory production, farm production,\nand many other industrial optimization prob\u0002lems where one is unaware of the value of the\nitems produced until they have already been\nconstructed. We introduce an algorithm for\nthis domain. We also apply this algorithm\nto repeated games, and show that it is re\u0002ally a generalization of infinitesimal gradient\nascent, and the results here imply that gen\u0002eralized infinitesimal gradient ascent (GIGA)\nis universally consistent.\n1. Introduction\nConvex programming is a generalization of linear pro\u0002gramming with many applications to machine learn\u0002ing. For example, one wants to find a hypothesis\nin a hypothesis space H that minimizes absolute er\u0002ror (Boyd & Vandenberghe, 2003), minimizes squared\nerror (Hastie et al., 2001), or maximizes the mar\u0002gin (Boser et al., 1992) for the training set X. If\nthe hypothesis space consists of linear functions, then\nthese problems can be solved using linear program\u0002ming, least-squares regression, and support vector ma\u0002chines respectively. These are all examples of convex\nprogramming problems.\nConvex programming has other applications, such as\nnonlinear facility location problems (Boyd & Vanden\u0002berghe, 2003, pages 421-422), network routing prob\u0002lems (Bansal et al., 2003), and consumer optimiza\u0002tion problems (Boot, 2003, pages 4\u20136). Other exam\u0002ples of linear programming problems are meeting nu\u0002tritional requirements, balancing production and con\u0002sumption in the national economy, and production\nplanning(Cameron, 1985, pages 36\u201339).\nConvex programming consists of a convex feasible set\nF \u2286 R\nn and a convex (valley-shaped) cost function\nc : F \u2192 R. In this paper, we discuss online convex\nprogramming, in which an algorithm faces a sequence\nof convex programming problems, each with the same\nfeasible set but different cost functions. Each time\nthe algorithm must choose a point before it observes\nthe cost function. This models a number of optimiza\u0002tion problems including industrial production and net\u0002work routing, in which decisions must be made before\ntrue costs or values are known1. This is a generaliza\u0002tion of both work in minimizing error online (Cesa\u0002Bianchi et al., 1994; Kivinen & Warmuth, 1997; Gor\u0002don, 1999; Herbster & Warmuth, 2001; Kivinen &\nWarmuth, 2001) and of the experts problem (Freund\n& Schapire, 1999; Littlestone & Warmuth, 1989).\nIn the experts problem, one has n experts, each of\nwhich has a plan at each step with some cost. At\neach round, one selects a probability distribution over\nexperts. If x \u2208 R\nn is defined such that xi\nis the prob\u0002ability that one selects expert i, then the set of all\nprobability distributions is a convex set. Also, the\ncost function on this set is linear, and therefore con\u0002vex. Repeated games are closely related to the experts\nproblem.\nIn minimizing error online, one sees an unlabeled in\u0002stance, assigns it a label, and then receives some error\nbased on how divergent the label given was from the\ntrue label. The divergences used in previous work are\n1We expand on the network routing domain at the end\nof this section. In particular, our results have been ap\u0002plied (Bansal et al., 2003) to solve the \u201conline oblivous\nrouting problem\u201d.\nProceedings of the Twentieth International Conference on Machine Learning (ICML-2003), Washington DC, 2003.\nfixed Bregman divergences, e.g. squared error.\nIn this paper, we make no distributional assumptions2\nabout the convex cost functions. Also, we make no as\u0002sumptions about any relationships between successive\ncost functions. Thus, expecting to choose the optimal\npoint at each time step is unrealistic. Instead, as in\nthe analysis of the experts problem, we compare our\ncost to the cost of some other \u201coffline\u201d algorithm that\nselects a fixed vector. However, this other algorithm\nknows in advance all of the cost functions before it\nselects this single fixed vector. We formalize this in\nSection 2.1.\nWe present an algorithm for general convex functions\nbased on gradient descent, called greedy projection.\nThe algorithm applies gradient descent in R\nn, and\nthen moves back to the set of feasible points. There\nare three advantages to this algorithm. The first is\nthat gradient descent is a simple, natural algorithm\nthat is widely used, and studying its behavior is of in\u0002trinsic value. Secondly, this algorithm is more general\nthan the experts setting, in that it can handle an ar\u0002bitrary sequence of convex functions, which has yet to\nbe solved. Finally, in online linear programs this algo\u0002rithm can in some circumstances perform better than\nan experts algorithm. While the bounds on the per\u0002formance of most experts algorithms depends on the\nnumber of experts, these bounds are based on other\ncriteria which may sometimes be lower. This relation\u0002ship is discussed further in Section 4, and further com\u0002ments on related work can be found in Section 5. The\nmain theorem is stated and proven in Section 2.1.\nAnother measure of the performance of gradient pro\u0002jection is found in Section 2.2, where we establish re\u0002sults unlike those usually found in online algorithms.\nWe establish that the algorithm can perform well, even\nin comparison to an agent that knows the sequence in\nadvance and can move for some short distance. This\nresult establishes that greedy projection can handle en\u0002vironments that are slowly changing over time and re\u0002quire frequent but small modifications to handle well.\nThe algorithm that motivated this study was infinites\u0002imal gradient ascent (Singh et al., 2000), which is an\nalgorithm for repeated games. First, this result shows\nthat infinitesimal gradient ascent is universally con\u0002sistent (Fudenberg & Levine, 1995), and secondly it\nshows that GIGA, a nontrivial extension developed\nhere of infinitesimal gradient ascent to games with\nmore than two actions, is universally consistent. GIGA\nis defined in Section 3.2, and the proof is similar to that\n2The assumptions we do make are listed in the begin\u0002ning of Section 2.\nin (Freund & Schapire, 1999).\nBansal et al. (2003) formulate an online oblivious rout\u0002ing problem as an online convex programming prob\u0002lem, and apply greedy projection to obtain good per\u0002formance. In online oblivious routing, one is in charge\nof minimizing network congestion by programming a\nvariety of routers. At the beginning of each day, one\nchooses a flow for each source-destination pair. The\nset of all such flows is convex. Then, an adversary\nchooses a demand (number of packets) for each source\u0002destination pair. The cost is the maximum congestion\nalong any edge that the algorithm has divided by the\nmaximum congestion of the optimal routing given the\ndemand.\nThe contribution of this paper is a general solution\nfor a wide variety of problems, some solved, some un\u0002solved. Sometimes, these results show new properties\nof existing algorithms, like IGA, and sometimes, this\nwork has resulted in new algorithms, like GIGA. Fi\u0002nally, the flexibility to choose arbitrary convex func\u0002tions has already resulted in a solution to a practical\nonline problem (Bansal et al., 2003).\n2. Online Convex Programming\nDefinition 1 A set of vectors S \u2286 R\nn is convex if\nfor all x, x0 \u2208 S, and all \u03bb \u2208 [0, 1], \u03bbx + (1 \u2212 \u03bb)x\n0 \u2208 S.\nDefinition 2 For a convex set F, a function f : F \u2192\nR is convex if for all x, y \u2208 F, for all \u03bb \u2208 [0, 1],\n\u03bbf(x) + (1 \u2212 \u03bb)f(y) \u2265 f(\u03bbx + (1 \u2212 \u03bb)y)\nIf one were to imagine a convex function R\n2 \u2192 R,\nwhere the function described the altitude, then the\nfunction would look like a valley.\nDefinition 3 A convex programming problem\nconsists of a convex feasible set F and a convex cost\nfunction c : F \u2192 R. The optimal solution is the\nsolution that minimizes the cost.\nDefinition 4 An online convex programming\nproblem consists of a feasible set F \u2286 R\nn and an\ninfinite sequence {c\n1\n, c2, . . . } where each c\nt\n: F \u2192 R is\na convex function.\nAt each time step t, an online convex programming\nalgorithm selects a vector x\nt \u2208 F. After the vector is\nselected, it receives the cost function c\nt\n.\nBecause all information is not available before deci\u0002sions are made, online algorithms do not reach \u201cso\u0002lutions\u201d, but instead achieve certain goals. See Sec\u0002tion 2.1.\nDefine kxk =\n\u221a\nx \u00b7 x and d(x, y) = kx \u2212 y k. Through\u0002out the remainder of the paper we will make seven\nassumptions:\n1. The feasible set F is bounded. There exists N \u2208\nR such that for all x, y \u2208 F, d(x, y) \u2264 N.\n2. The feasible set F is closed. For all sequences\n{x\n1\n, x2, . . . } where x\nt \u2208 F for all t, if there exists\na x \u2208 R\nn such that x = limt\u2192\u221e xt\n, then x \u2208 F.\n3. The feasible set F is nonempty. There exists an\nx \u2208 F.\n4. For all t, c\nt\nis differentiable3.\n5. There exists an N \u2208 R such that for all t, for all\nx \u2208 F, k\u2207c\nt\n(x)k \u2264 N.\n6. For all t, there exists an algorithm, given x, which\nproduces \u2207c\nt\n(x).\n7. For all y \u2208 R\nn, there exists an algorithm which\ncan produce argminx\u2208F d(x, y). We define the pro\u0002jection P(y) = argminx\u2208F d(x, y).\nGiven this machinery, we can describe our algorithm.\nAlgorithm 1 Greedy Projection Select an arbi\u0002trary x\n1 \u2208 F and a sequence of learning rates\n\u03b71, \u03b72, . . . \u2208 R\n+. In time step t, after receiving a cost\nfunction, select the next vector x\nt+1 according to:\nx\nt+1 = P\n\nx\nt \u2212 \u03b7t\u2207ct\n(x\nt\n)\n\u0001\n.\nThe basic principle at work in this algorithm is quite\nclear if we consider the case where the sequence\n{c\n1\n, c2, . . . } is constant. In this case, our algorithm\nis operating in an unchanging valley. The boundary\nof the feasible set is the edge of the valley. By pro\u0002cee",
          "original_query": "Zinkevich, M. (2003). Online Convex Programming and Generalized Infinitesimal Gradient Ascent",
          "cleaned_query": "Zinkevich, M.. Online Convex Programming and Generalized Infinitesimal Gradient Ascent",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] From External to Internal Regret",
          "url": "https://www.jmlr.org/papers/volume8/blum07a/blum07a.pdf",
          "content": "Journal of Machine Learning Research 8 (2007) 1307-1324 Submitted 12/05; Revised 1/07; Published 6/07\nFrom External to Internal Regret\nAvrim Blum\u2217 AVRIM@CS.CMU.EDU\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213\nYishay Mansour\u2020 MANSOUR@CS.TAU.AC.IL\nSchool of Computer Science\nTel Aviv University\nTel Aviv, Israel\nEditor: Ron Meir\nAbstract\nExternal regret compares the performance of an online algorithm, selecting among N actions, to\nthe performance of the best of those actions in hindsight. Internal regret compares the loss of an\nonline algorithm to the loss of a modified online algorithm, which consistently replaces one action\nby another.\nIn this paper we give a simple generic reduction that, given an algorithm for the external regret\nproblem, converts it to an efficient online algorithm for the internal regret problem. We provide\nmethods that work both in the full information model, in which the loss of every action is observed\nat each time step, and the partial information (bandit) model, where at each time step only the loss\nof the selected action is observed. The importance of internal regret in game theory is due to the fact\nthat in a general game, if each player has sublinear internal regret, then the empirical frequencies\nconverge to a correlated equilibrium.\nFor external regret we also derive a quantitative regret bound for a very general setting of regret,\nwhich includes an arbitrary set of modification rules (that possibly modify the online algorithm) and\nan arbitrary set of time selection functions (each giving different weight to each time step). The\nregret for a given time selection and modification rule is the difference between the cost of the\nonline algorithm and the cost of the modified online algorithm, where the costs are weighted by the\ntime selection function. This can be viewed as a generalization of the previously-studied sleeping\nexperts setting.\nKeywords: online learning, internal regret, external regret, multi-arm bandit, sleeping experts,\nreductions\n1. Introduction\nThe motivation behind regret analysis might be viewed as the following: we design a sophisticated\nonline algorithm that deals with various issues of uncertainty and decision making, and sell it to a\nclient. Our online algorithm runs for some time and incurs a certain loss. We would like to avoid\n\u2217. This work was supported in part by NSF grants CCR-0105488 and IIS-0312814.\n\u2020. The work was done while the author was a fellow in the Institute of Advance studies, Hebrew University. This\nwork was supported in part by the IST Programme of the European Community, under the PASCAL Network of\nExcellence, IST-2002-506778, by a grant no. 1079/04 from the Israel Science Foundation and an IBM faculty award.\nThis publication only reflects the authors\u2019 views.\n c 2007 Avrim Blum and Yishay Mansour.\nBLUM AND MANSOUR\nthe embarrassment that our client will come back to us and claim that in retrospect we could have\nincurred a much lower loss if we used his simple alternative policy \u03c0. The regret of our online\nalgorithm is the difference between the loss of our algorithm and the loss using \u03c0. Different notions\nof regret quantify differently what is considered to be a \u201csimple\u201d alternative policy.\nAt a high level one can split alternative policies into two categories. The first consists of alterna\u0002tive policies that are independent from the online algorithm\u2019s action selection, as is done in external\nregret. External regret, also called the best expert problem, compares the online algorithm\u2019s cost\nto the best of N actions in retrospect (see Hannan, 1957; Foster and Vohra, 1993; Littlestone and\nWarmuth, 1994; Freund and Schapire, 1997, 1999; Cesa-Bianchi et al., 1997). This implies that the\nsimple alternative policy performs the same action in all time steps, which indeed is quite simple.\nNonetheless, one important application of external regret is a general methodology for develop\u0002ing online algorithms whose performance matches that of an optimal static offline algorithm, by\nmodeling the possible static solutions as different actions.\nThe second category of alternative policies are those that consider the online sequence of actions\nand suggest a simple modification to it, such as \u201cevery time you bought IBM, you should have\nbought Microsoft instead.\u201d This notion is captured by internal regret, introduced in Foster and\nVohra (1998). Specifically, internal regret allows one to modify the online action sequence by\nchanging every occurrence of a given action i to an alternative action j. Specific low internal regret\nalgorithms were derived by Hart and Mas-Colell (2000), Foster and Vohra (1997, 1998, 1999), and\nCesa-Bianchi and Lugosi (2003), where the use of the approachability theorem of Blackwell (1956)\nhas played an important role in some of the algorithms.\nOne of the main contributions of our work is to show a simple online way to efficiently convert\nany low external regret algorithm into a low internal regret algorithm. Our guarantee is somewhat\nstronger than internal regret and we call it swap regret, which allows one to simultaneously swap\nmultiple pairs of actions. (If there are N actions total, then swap-regret is bounded by N times\nthe internal regret.) Using known results for external regret we can derive a swap regret bound of\nO(\n\u221a\nTN logN), where T is the number of time steps, which is the best known bound on swap regret\nfor efficient algorithms. We also show an \u2126(\n\u221a\nTN) lower bound for the case of randomized online\nalgorithms against an adaptive adversary.\nThe importance of internal and swap regret is due to their tight connection to correlated equi\u0002libria, introduced by Aumann (1974). For a general-sum game of any finite number of players, a\ndistribution Q over the joint action space is a correlated equilibrium if every player would have zero\ninternal regret when playing it. In a repeated game scenario, if each player uses an action selection\nalgorithm whose regret of this form is sublinear in T, then the empirical distribution of the players\nactions converges to a correlated equilibrium (see, e.g., Hart and Mas-Colell, 2000), and in fact,\nthe benefit of a deviation from a correlated equilibrium is bounded exactly by R/T, where R is the\nlargest swap regret of any of the players.\nWe also extend our results to the partial information model, also called the adversarial multi\u0002armed bandit (MAB) problem in Auer et al. (2002a). In this model, the online algorithm only gets to\nobserve the loss of the action actually selected, and does not see the losses of the actions not chosen.\nFor example, if you are driving to work and need to select which of several routes to take, you only\nobserve the travel time on the route actually taken. If we view this as an online problem, each day\nselecting which route to take on that day, then this fits the MAB setting. Furthermore, the route\u0002choosing problem can be viewed as a general-sum game: your travel time depends on the choices of\nthe other drivers as well. Thus, if every driver uses a low internal-regret algorithm, then the uniform\n1308\nFROM EXTERNAL TO INTERNAL REGRET\ndistribution over observed traffic patterns will converge to a correlated equilibrium. For the MAB\nproblem, our combining algorithm requires additional assumptions on the base external-regret MAB\nalgorithm: a smoothness in behavior when the actions played are taken from a somewhat different\ndistribution than the one proposed by the algorithm. Luckily, these conditions are satisfied by\nexisting external-regret MAB algorithms such as that of Auer et al. (2002a). For the multi-armed\nbandit setting, we derive an O(N\n\u221a\nNT logN) swap-regret bound. Thus, after T = O(\n1\n\u03b5\n2N\n3\nlogN)\nrounds, the empirical distribution on the history is an \u03b5-correlated equilibrium. In a recent work,\nStoltz (2005) gives an improved swap regret bound of O(N\n\u221a\nT logN). (The work of Hart and Mas\u0002Colell (2001) also gives a multi-armed bandit algorithm whose swap regret is sublinear in T, but\ndoes not derive explicit bounds.)\nOne can also envision broader classes of regret. Lehrer (2003) defines a notion of wide range\nregret that allows for arbitrary action-modification rules, which might depend on history, and also\nBoolean time selection functions (that determine which subset of times is relevant). Using the ap\u0002proachability theorem, he shows a scheme that in the limit achieves no regret (i.e., regret is sublinear\nin T). While Lehrer (2003) derives the regret bounds in the limit, we derive finite-time regret bounds\nfor this setting. We show that for any family of N actions, M time selection functions and K mod\u0002ification rules, the maximum regret with respect to any selection function and modification rule is\nbounded by O(\np\nTN log(MK)). Our model also handles the case where the time selection functions\nare not Boolean, but rather real valued in [0,1].\nThis latter result can be viewed as a generalization of the sleeping experts setting of Freund\net al. (1997) and Blum (1997). In the sleeping experts problem, we again have a set of experts, but\non any given time step, each expert may be awake (making a prediction) or asleep (not predicting).\nThis is a natural model for combining a collection of if-then rules that only make predictions when\nthe \u201cif\u201d portion of the rule is satisfied, and this setting has had application in domains ranging from\nmanaging a calendar (Blum, 1997) and text-categorization (Cohen and Singer, 1999) to learning\nhow to formulate web search-engine queries (Cohen and Singer, 1996). By converting each such\nsleeping-expert into a pair hexpert, time-selection functioni, we achieve the desired guarantee that\nfor each sleeping-expert, our loss during the time that expert was awake is not much more than\nits loss in that period. Moreover, by using non-Boolean time-selection functions, we can naturally\nhandle prediction rules that have varying degrees of confidence in their predictions and achieve a\nconfidence-weighted notion of regret.\n",
          "original_query": "Blum, A. and Mansour, Y. (2007). From External to Internal Regret",
          "cleaned_query": "Blum, A. and Mansour, Y.. From External to Internal Regret",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] High-Dimensional Calibration from Swap Regret - OpenReview",
          "url": "https://openreview.net/pdf?id=UVDihUz0iT",
          "content": "High-Dimensional Calibration from Swap Regret\nMaxwell Fishelson\u2217\nmaxfish@mit.edu\nNoah Golowich\u2020\nnzg@mit.edu\nMehryar Mohri\u2021\nmohri@google.com\nJon Schneider\u00a7\njschnei@google.com\nAbstract\nWe study the online calibration of multi-dimensional forecasts over an arbitrary\nconvex set P \u2282 R\nd\nrelative to an arbitrary norm k\u00b7k. We connect this with the\nproblem of external regret minimization for online linear optimization, showing that\nif it is possible to guarantee O(\n\u221a\n\u03c1T) worst-case regret after T rounds when actions\nare drawn from P and losses are drawn from the dual k\u00b7k\u2217unit norm ball, then it\nis also possible to obtain \u000f-calibrated forecasts after T = exp(O(\u03c1/\u000f2)) rounds.\nWhen P is the d-dimensional simplex and k\u00b7k is the `1-norm, the existence of\nO(\n\u221a\nT log d)-regret algorithms for learning with experts implies that it is possible\nto obtain \u000f-calibrated forecasts after T = exp(O(log d/\u000f2)) = d\nO(1/\u000f2)\nrounds,\nrecovering a recent result of [Pen25].\nInterestingly, our algorithm obtains this guarantee without requiring access to any\nonline linear optimization subroutine or knowledge of the optimal rate \u03c1 \u2013 in fact,\nour algorithm is identical for every setting of P and k\u00b7k. Instead, we show that\nthe optimal regularizer for the above OLO problem can be used to upper bound\nthe above calibration error by a swap regret, which we then minimize by running\nthe recent TreeSwap algorithm ([DDFG24, PR24]) with Follow-The-Leader as a\nsubroutine. The resulting algorithm is highly efficient and plays a distribution over\nsimple averages of past observations in each round.\nFinally, we prove that any online calibration algorithm that guarantees \u000fT `1-\ncalibration error over the d-dimensional simplex requires T \u2265 exp(poly(1/\u000f))\n(assuming d \u2265 poly(1/\u000f)). This strengthens the corresponding d\n\u2126(log 1/\u000f)\nlower\nbound of [Pen25], and shows that an exponential dependence on 1/\u000f is necessary.\n1 Introduction\nConsider the problem faced by a forecaster who must report probabilistic predictions for a sequence\nof events (e.g. whether it will rain or not tomorrow). One of the most common methods to evaluate\nthe quality of such a forecaster is to verify whether they are calibrated: for example, does it indeed\nrain with probability 40% on days where the forecaster makes this prediction? In addition to\ncalibration being a natural property to expect from predictions, several applications across machine\nlearning, fairness, and game theory require the ability to produce online calibrated predictions\n[ZME20, GPSW17, HJKRR18, FV97].\nWhen events have binary outcomes, calibration can be quantified by the notion of expected calibration\nerror, which measures the expected distance between a prediction made by a forecaster and the actual\nempirical probability of the outcome on the days where they made that prediction. In a seminal result\nby Foster and Vohra [FV98], it was proved that it is possible for an online forecaster to efficiently\n\u2217MIT.\n\u2020MIT. Supported by a NSF Graduate Research Fellowship and a Fannie & Hertz Foundation Graduate\nFellowship.\n\u2021Google Research and Courant Institute of Mathematical Sciences, New York.\n\u00a7Google Research.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\nguarantee a sublinear calibration error of O(T\n2/3\n) against any adversarial sequence of T binary\nevents. Equivalently, this can be interpreted as requiring at most O(\u000f\n\u22123\n) rounds of forecasting to\nguarantee an \u000f per-round calibration error on average.\nHowever, many applications require forecasting sequences of multi-dimensional outcomes. The\nprevious definition of calibration error easily extends to the multi-dimensional setting where pre\u0002dictions and outcomes belong to a d-dimensional convex set P \u2282 R\nd\n. Specifically, if a forecaster\nmakes a sequence of predictions p1, p2, . . . , pT \u2208 P for the outcomes y1, y2, . . . , yT \u2208 P, their\nk\u00b7k-calibration error (for any norm k\u00b7k over R\nd\n) is given by\nCalk\u00b7k\nT =\nX\nT\nt=1\nkpt \u2212 \u03bdpt k\nwhere \u03bdpt\nis the average of the outcomes yt on rounds where the learner predicted pt.\nThe algorithm of Foster and Vohra extends to the multidimensional calibration setting, but at the cost\nof producing bounds that decay exponentially in the dimension d. In particular, their algorithm only\nguarantees that the forecaster achieves an average calibration error of \u000f after (1/\u000f)\n\u2126(d)\nrounds. Until\nrecently, no known algorithm achieved a sub-exponential dependence on d in any non-trivial instance\nof multi-dimensional calibration.\nIn 2025, [Pen25] presented a new algorithm for high-dimensional calibration, demonstrating that it is\npossible to obtain `1-calibration rates of \u000fT in d\nO(1/\u000f2)\nrounds for predictions over the d-dimensional\nsimplex (i.e., multi-class calibration). In particular, this is the first known algorithm achieving\npolynomial calibration rates in d for fixed constant \u000f. [Pen25] complements this with a lower bound,\nshowing that in the worst case d\n\u2126(log 1/\u000f)\nrounds are necessary to obtain this rate (implying that a\nfully polynomial bound poly(d, 1/\u000f) is impossible).\n1.1 Our results\nAlthough the algorithm of [Pen25] is simple to describe, its analysis is fairly nuanced and tailored\nto `1-calibration over the simplex (e.g., by analyzing the KL divergence between predictions and\ndistributions of historical outcomes). We present a very similar algorithm (TreeCal) for multi\u0002dimensional calibration over an arbitrary convex set P \u2282 R\nd\n, but with a simple, unified analysis\nthat provides simultaneous guarantees for calibration with respect to any norm k\u00b7k. In particular, we\nprove the following theorem.\nTheorem 1.1 (Informal restatement of Corollary C.5). Fix a convex set P and a norm k \u00b7 k. Assume\nthere exists a function R : P \u2192 R that is 1-strongly-convex with respect to k \u00b7 k and has range\n(maxx\u2208P R(x) \u2212 minp\u2208P R(x)) at most \u03c1. Then TreeCal guarantees that the calibration error of\nits predictions is bounded by Calk\u00b7k\nT \u2264 \u000fT for T \u2265 (diamk\u00b7k(P)/\u000f)\nO(\u03c1/\u000f2)\n.\nInterestingly, the function R(p) and parameter \u03c1 appearing in the statement of Theorem 1.1 have an\nindependent learning-theoretic interpretation: if we consider the online linear optimization problem\nwhere a learner plays actions in P and the adversary plays linear losses that are unit bounded in the\ndual norm k\u00b7k\u2217, then it is possible for the learner to guarantee a regret bound of at most O(\n\u221a\n\u03c1T) by\nplaying Follow-The-Regularized-Leader (FTRL) with R(p) as a regularizer. In fact, since universality\nresults for mirror descent guarantee that some instantiation of FTRL achieves near-optimal rates for\nonline linear optimization (as long as the action and loss sets are centrally convex) [SST11, GSJ24],\nthis allows us to relate the performance of Theorem 3.1 directly to what rates are possible in online\nlinear optimization.\nCorollary 1.2 (Informal restatement of Corollary C.6). Let P \u2286 R\nd be a centrally symmetric convex\nset, and let L = {y \u2208 R\nd\n| kyk\u2217 \u2264 1} for some norm k\u00b7k. Then if there exists an algorithm for\nonline linear optimization with action set P and loss set L that incurs regret at most O(\n\u221a\n\u03c1T),\nTreeCal guarantees that the calibration error of its predictions is bounded by Calk\u00b7k\nT \u2264 \u000fT for\nT \u2265 (diamk\u00b7k(P)/\u000f)\nO(\u03c1/\u000f2)\n.\nTheorem 1.1 and its corollary allow us to immediately recover several existing and novel bounds on\ncalibration error in a variety of settings:\n2\n\u2022 When P is the d-simplex \u2206d and k\u00b7k is the `1-norm, the existence of the negative entropy\nregularizer R(x) = Pd\ni=1 xi\nlog xi (which is 1-strongly convex w.r.t. the `1 norm with range\n\u03c1 = log d) implies that the `1 calibration error of TreeCal is at most (1/\u000f)\nO(log d/\u000f2) =\nd\nO\u02dc(1/\u000f2)\n. This recovers the result of [Pen25].\n\u2022 When P is the `2 ball and k\u00b7k is the `2 norm, the Euclidean regularizer (R(x) = kxk\n2\n)\nimplies a calibration bound of (1/\u000f)\nO(1/\u000f2)\n(notably, this bound is independent of d).\nIt should be emphasized here that running TreeCal does not require any online linear optimization\nsubroutine, nor any knowledge of these regularizers R(x) or optimal rates \u03c1. TreeCal has no\nfunctional dependence on any specific k\u00b7k. It achieves k\u00b7k-calibration at the above rate (Theorem 1.1)\nfor all k\u00b7k simultaneously. The TreeCal algorithm is nearly identical5to the algorithm of [Pen25] \u2013\nboth algorithms initialize a tree of sub-forecasters and at each round play a uniform combination of\nsome subset of them (see Figure 1).\nThe novelty in our analysis stems from the observation that TreeCal is simply a specific instantiation\nof the TreeSwap swap regret minimization algorithm [DDFG24, PR24] and can be analyzed directly\nin this way. In particular, our analysis consists of the following steps:\n1. First, minimizing calibration error can be reduced to minimizing swap regret, generalizing\nan idea of [LSS25, FKO+25]. That is, it is possible to assign the learner loss functions\n`t : P \u2192 R at each round such that their calibration error is upper bounded by the gap\nbetween the total loss they received, and the minimal loss they could have received after\napplying an arbitrary \u201cswap function\u201d \u03c0 : P \u2192 P to their predictions.\nIn fact, any strongly convex function R (w.r.t. the norm k\u00b7k) gives rise to one such reduction,\nby setting the loss function `t(p) to equal the Bregman divergence DR(yt|p).\n2. Second, the TreeSwap algorithm of [DDFG24, PR24] provides a general recipe for convert\u0002ing external regret minimization algorithms into swap regret minimization algorithms. We\nobtain TreeCal by plugging in the Follow-The-Leader algorithm (the learning algorithm\nwhich simply always best responds to the current history) into TreeSwap.\n3. Instead of analyzing the swap regret bound of TreeSwap with Follow-The-Leader (which\nmay not have a good enough external regret bound, as discussed in Section 3.3), we instead\nanalyze the swap regret of TreeSwap with Be-The-Leader (the fictitious algorithm that best\nresponds to the current history, including the current round). Though it is not possible to\nactually implement Be-The-Leader due to its cla",
          "original_query": "[Pen25] (2025). High\u2011dimensional calibration over the simplex",
          "cleaned_query": "[Pen25]. High\u2011dimensional calibration over the simplex",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Directive-type Memorandum 24-009, \"Public Complaints or Requests Regarding Public Displays or Public Expressions of Religion on DoD Property,\" December 16, 2024",
          "url": "https://www.esd.whs.mil/Portals/54/Documents/DD/issuances/dtm/DTM-24-009.PDF?ver=TZcYWPIK7aeN7QXdat7Xxw==",
          "content": "OFFICE OF THE UNDER SECRETARY OF DEFENSE\n4000 DEFENSE PENTAGON\nWASHINGTON, D.C. 20301-4000\n PERSONNEL AND\n READINESS\nDecember 16, 2024 \nMEMORANDUM FOR SENIOR PENTAGON LEADERSHIP\nDEFENSE AGENCY AND DOD FIELD ACTIVITY DIRECTORS\nSUBJECT: Directive-type Memorandum 24-009 \u2013 \u201cPublic Complaints or Requests Regarding \nPublic Displays or Public Expressions of Religion on DoD Property\u201d \nReferences: See Attachment 1.\nPurpose. In accordance with the authority of DoD Directive 5124.02 and Section 1049 of \nPublic Law 118-31 (also known and referred to in this issuance as the \u201cNational Defense \nAuthorization Act for Fiscal Year 2024\u201d), this directive-type memorandum (DTM): \n\u2022 Establishes policy, assigns responsibilities, and provides procedures for the \ntimely determination of a covered complaint or request as defined in the Glossary \nregarding public displays or expressions of religion on DoD property. \n\u2022 Is effective December 16, 2024; it must be incorporated into DoD Instruction \n(DoDI) 1300.17. This DTM will expire effective December 16, 2025. \nApplicability. This DTM: \n\u2022 Applies to: \no OSD, the Military Departments, the Office of the Chairman of the \nJoint Chiefs of Staff and the Joint Staff, the Combatant Commands, the \nOffice of Inspector General of the Department of Defense, the Defense \nAgencies, the DoD Field Activities, and all other organizational entities \nwithin the DoD, referred to in this DTM as \u201cthe DoD Components.\u201d \no Covered complaints or requests regarding public display or expression \nof religion that have occurred or taken place on DoD property. \n\u2022 Does not apply to requests for access to DoD installations for the purpose of \nconducting or performing a public display or expression of religion. Such \nrequests will be in accordance with Volume 3 of DoD Manual 5200.08. \n\u2022 Requests by Service members for the accommodation of religious practices \nare governed by DoDI 1300.17 and applicable Military Department and Service \npolicies; such requests are not subject to this DTM. \nDTM-24-009, December 16, 2024 \n2\nDefinitions. See Glossary.\nPolicy. \nIn accordance with Section 1049 of the National Defense Authorization Act for Fiscal \nYear 2024, the DoD will: \n\u2022 Provide a timely determination for a covered complaint or request regarding a \npublic display or public expression of religion on DoD property made by an \nindividual or entity other than a Service member of the DoD or a civilian \nemployee or contractor of the DoD. \n\u2022 Ensure compliance with the requirements in Section 1049 of the National \nDefense Authorization Act for Fiscal Year 2024. \n\u2022 Allow for the continued public display or expression of religion that is the \nsubject of a covered complaint or request until a determination is made in \naccordance with this DTM. However, military installation commanders may \norder the immediate removal of a display or cessation of expression upon a \ndetermination in writing to the official authorized to act on covered complaints \nand requests, by an installation commander that continuation of the display or \nexpression will have an adverse impact on military readiness, unit cohesion, good \norder and discipline, or health and safety. \nResponsibilities. \n\u2022 Under Secretary of Defense for Personnel and Readiness. The Under \nSecretary of Defense for Personnel and Readiness oversees implementation of and \ncompliance with this DTM.\n\u2022 Assistant Secretary of Defense for Manpower and Reserve Affairs \n(ASD(M&RA)). Under the authority, direction, and control of the Under \nSecretary of Defense for Personnel and Readiness, the ASD(M&RA) monitors \ndeterminations to ensure religious liberty policy compliance and consistency of \ndeterminations throughout the DoD. \n\u2022 DoD Component Heads. The DoD Component heads: \no Ensure complaints or requests received by their personnel regarding \npublic displays or public expressions of religion on DoD property are \nprocessed in accordance with Attachment 2 and Section 1049 of the \nNational Defense Authorization Act for Fiscal Year 2024. \no May delegate the authority to act on covered complaints or requests \nsubject to this DTM. Such delegation must be in writing and may be no \nlower than a Presidentially Appointed, Senate-confirmed official. Further \nre-delegation is not authorized. \nDTM-24-009, December 16, 2024 \n3\no Ensure any official to whom authority is delegated under this DTM, \nand subordinate officers and officials, including commanders, judge \nadvocate generals, and chaplains who may review and endorse covered \ncomplaints or requests, are trained on religious liberty policy, timelines, \nauthority restrictions, and the policies and procedures in this DTM. \no Ensure records and information established and created in accordance \nwith this DTM are retained in accordance with DoDI 5015.02 and DoD \nComponent records management disposition schedules. \no Ensure any action in accordance with this DTM will follow protocols \noutlined in DoDI 5400.11. \nProcedures. See Attachment 2. \nReleasability. Cleared for public release. Available on the Directives Division Website \nat https://www.esd.whs.mil/DD/.\nAshish S. Vazirani \nPerforming the Duties of the Under Secretary of \nDefense for Personnel and Readiness\nAttachment:\nAs stated\nDTM-24-009, December 16, 2024 \n4 Attachment 1 \nATTACHMENT 1\nREFERENCES\nDoD Directive 5124.02, \u201cUnder Secretary of Defense for Personnel and Readiness \n(USD(P&R)),\u201d June 23, 2008 \nDoD Instruction 1300.17, \u201cReligious Liberty in the Military Services,\u201d September 1, 2020 \nDoD Instruction 4165.14, \u201cReal Property Inventory and Reporting,\u201d September 8, 2023 \nDoD Instruction 5015.02, \u201cDoD Records Management Program,\u201d February 24, 2015, as \namended\nDoD Instruction 5120.08, \u201cArmed Forces Chaplains Board,\u201d April 24, 2024 \nDoD Instruction 5400.11, \u201cDoD Privacy and Civil Liberties Program,\u201d January 29, 2019, as \namended\nDoD Manual 5200.08, Volume 3, \u201cPhysical Security Program: Access to DoD Installations,\u201d \nJanuary 2, 2019, as amended \nPublic Law 118-31, Section 1049, \u201cNational Defense Authorization Act for Fiscal Year 2024,\u201d \nDecember 22, 2023 \nDTM-24-009, December 16, 2024 \n5 Attachment 2\nATTACHMENT 2\nPROCEDURES\n1. GENERAL. The DoD Component heads will follow the procedures in this attachment to \nreview and determine resolution of covered complaints or requests.\n2. INITIATION OF THE COMPLAINT OR REQUEST. All covered complaints received \nwithin DoD will be forwarded to the official authorized to act on them for the DoD Component \nconcerned not later than 10 working days after such receipt. The command or activity \nforwarding a covered complaint will provide, to the extent practicable, sufficient contextual \ninformation to allow for consideration of the complaint. \n3. PROCESSING OF NON-COVERED COMPLAINTS OR REQUESTS. Non-covered \ncomplaints or requests will be reviewed to determine an appropriate resolution pursuant to \napplicable laws and DoD, Military Department, and Service-level policies and regulations. The \nDoD Components may, as appropriate, use the determination of covered complaints or requests \nto inform the decision-making process for a non-covered complaint. \n4. PROCESSING OF COVERED COMPLAINTS OR REQUESTS. Not later than 30 working \ndays after receiving the covered complaint or request, and after consultation as described in \nParagraph 4, the official authorized to act on a covered complaint or request will make a \ndetermination regarding the complaint or request and: \na. Provides timely notification of such determination to: \n(1) The individual or entity who made the complaint or request. \n(2) The officer or official who forwarded the complaint or request. \nb. Provides a copy of the determination of the complaint or request, and documentation \nof any action taken, to: \n(1) All impacted commanders or officials.\n(2) The Office of the ASD(M&RA).\nDTM-24-009, December 16, 2024 \n6 Attachment 2\n4. CONSULTATION. \na. Officials authorized to act on covered complaints or requests, other than those for a \nMilitary Department, will consult with the Armed Forces Chaplains Board, established in \nDoDI 5120.08, and a civilian attorney under the jurisdiction of the DoD Component concerned \nor a Military Service Judge Advocate. \nb. Military Department officials authorized to act on covered complaints and requests \nwill consult with the Chief of Chaplains for the Military Department concerned and a civilian \nattorney under the jurisdiction of the Military Department concerned or a Military Service Judge \nAdvocate.\nDTM-24-009, December 16, 2024 \n7 Glossary\nGLOSSARY\nPART I. ABBREVIATIONS AND ACRONYMS\nACRONYM MEANING\nASD(M&RA) Assistant Secretary of Defense for Manpower and Reserve Affairs\nDoDI DoD instruction\nDTM directive-type memorandum\nPART II. DEFINITIONS\nThese terms and their definitions are for the purpose of this issuance. \nTERM DEFINITION\ncommander or \ncommandant \nAny commissioned officer who exercises command authority over a \nService member.\ncovered complaint or \nrequest\nA complaint or request regarding a public display or expression of \nreligion on DoD property and made by an individual or entity other \nthan a Service member, a DoD civilian employee, or a DoD \ncontractor. Complaints made by an individual or entity on behalf of a \nService member, a DoD civilian employee, or a DoD contractor are \nconsidered a covered complaint or request for the purposes of this \nDTM. \ndetermination A final decision by the designated DoD authority concerning a \ncovered complaint or request.\nDoD property Installation real property (i.e., land and improvements to land), \nincluding leased facilities, and DoD-owned items, systems, or \nequipment as defined in DoDI 4165.14. \nreligious practice An action, behavior, or course of conduct constituting individual \nexpressions of religious beliefs, whether or not compelled by, or \ncentral to, the religion concerned.",
          "original_query": "[DDFG24 / PR24] (2024). TreeSwap: Efficient swap-regret minimization",
          "cleaned_query": "[DDFG24",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Approachability-Based Procedures for Multi-Stage Determinations with Override Conditions\nModel the complaint-handling process as a repeated vector-payoff game where the target set encodes compliance, consistency, and timeliness, and an \u201coverride\u201d action corresponds to the commander\u2019s immediate removal clause. Use Blackwell approachability to design strategies that keep the running average of outcomes within a convex compliance set while minimizing the frequency/cost of overrides.",
        "Internal-to-Calibration Reductions for Constrained Forecast Spaces via Swap Regret\nExtend the Blum\u2013Mansour external\u2192internal regret reduction to settings where forecasts lie in an arbitrary convex set \\(P\\subset\\mathbb{R}^d\\) with norm constraints (as in high-dimensional calibration from swap regret). Provide a constructive reduction showing how to obtain \\(\\varepsilon\\)-calibration (TreeCal-style) while enforcing additional linear constraints (e.g., feasibility, policy consistency) through swap-regret minimization.",
        "Norm-Adaptive High-Dimensional Calibration with Unknown Strongly Convex Regularizer\nBuild a single algorithm that adaptively selects (or learns) an appropriate regularizer \\(R\\) to match the unknown geometry of \\(P\\) and the relevant evaluation norm, improving practical performance over a fixed-choice TreeCal setup. Prove calibration bounds that compete with the best \\(R\\) in hindsight, similar in spirit to parameter-free online convex programming.",
        "Calibration with Distributional Randomization Minimizing Operational Randomness\nFoster\u2013Vohra style results require randomization to beat adversarial sequences; propose a framework that quantifies and minimizes the *amount* of randomness (entropy or seed length) needed to achieve a target calibration error. Provide algorithms that achieve asymptotic calibration with provably near-minimal randomness usage, and characterize lower bounds as a function of \\(d\\), \\(\\varepsilon\\), and norm choice.",
        "Non-Convex Target Sets in Compliance Games: Weak Approachability Meets Policy Rules\nMany real policy goals are effectively non-convex (e.g., discrete categories of determinations, threshold-based triggers); formalize these as non-convex target sets for vector payoffs. Develop new sufficient conditions (or algorithms) for weak approachability in such non-convex compliance sets, bridging Blackwell\u2019s conjectural weak dichotomy with modern regret-minimization primitives.",
        "Anytime Calibration Certificates for Auditable Decision Systems\nDesign \u201ccalibration certificates\u201d that can be produced at any time \\(t\\) to justify that forecasts/determinations remain within a specified error tolerance, aligning with records-retention and oversight requirements. Technically, build martingale-style confidence sequences around calibration error that remain valid under adversarial data, and integrate them with online convex programming updates.",
        "Correlated-Equilibrium Interpretation of Multi-Actor Complaint Resolution\nModel multiple stakeholders (complainant, installation, oversight authority) as players in a repeated general-sum game where payoffs are multidimensional (timeliness, readiness impact, rights compliance). Use internal/swap regret dynamics to show convergence to correlated equilibria representing stable procedural outcomes, and identify mechanisms (penalties/regularizers) that steer equilibria toward desired fairness/compliance regions."
      ],
      "generated_ideas_raw": "1. **Calibration Under Administrative Deadlines (DTM-Constrained Online Forecasting)**\n   Develop an online calibration algorithm that must issue \u201cdeterminations\u201d within hard time limits (modeled after DTM-24-009 timelines), while maintaining asymptotic calibration guarantees. Formalize a regret\u2013calibration tradeoff where delayed decisions reduce calibration error but violate deadlines, and derive optimal policies under adversarial event sequences.\n\n2. **Approachability-Based Procedures for Multi-Stage Determinations with Override Conditions**\n   Model the complaint-handling process as a repeated vector-payoff game where the target set encodes compliance, consistency, and timeliness, and an \u201coverride\u201d action corresponds to the commander\u2019s immediate removal clause. Use Blackwell approachability to design strategies that keep the running average of outcomes within a convex compliance set while minimizing the frequency/cost of overrides.\n\n3. **Internal-to-Calibration Reductions for Constrained Forecast Spaces via Swap Regret**\n   Extend the Blum\u2013Mansour external\u2192internal regret reduction to settings where forecasts lie in an arbitrary convex set \\(P\\subset\\mathbb{R}^d\\) with norm constraints (as in high-dimensional calibration from swap regret). Provide a constructive reduction showing how to obtain \\(\\varepsilon\\)-calibration (TreeCal-style) while enforcing additional linear constraints (e.g., feasibility, policy consistency) through swap-regret minimization.\n\n4. **Norm-Adaptive High-Dimensional Calibration with Unknown Strongly Convex Regularizer**\n   Build a single algorithm that adaptively selects (or learns) an appropriate regularizer \\(R\\) to match the unknown geometry of \\(P\\) and the relevant evaluation norm, improving practical performance over a fixed-choice TreeCal setup. Prove calibration bounds that compete with the best \\(R\\) in hindsight, similar in spirit to parameter-free online convex programming.\n\n5. **Bandit Calibration for Complaint Outcomes with Partial Observability**\n   Develop a bandit-feedback variant of multidimensional calibration where only the realized outcome for the chosen \u201caction\u201d (e.g., a procedural path or response template) is observed, not the full loss/outcome vector. Combine the bandit internal-regret tools from Blum\u2013Mansour with TreeSwap/TreeCal ideas to obtain explicit sample complexity bounds for \\(\\ell_1\\) or general-norm calibration under partial information.\n\n6. **Calibration with Distributional Randomization Minimizing Operational Randomness**\n   Foster\u2013Vohra style results require randomization to beat adversarial sequences; propose a framework that quantifies and minimizes the *amount* of randomness (entropy or seed length) needed to achieve a target calibration error. Provide algorithms that achieve asymptotic calibration with provably near-minimal randomness usage, and characterize lower bounds as a function of \\(d\\), \\(\\varepsilon\\), and norm choice.\n\n7. **Non-Convex Target Sets in Compliance Games: Weak Approachability Meets Policy Rules**\n   Many real policy goals are effectively non-convex (e.g., discrete categories of determinations, threshold-based triggers); formalize these as non-convex target sets for vector payoffs. Develop new sufficient conditions (or algorithms) for weak approachability in such non-convex compliance sets, bridging Blackwell\u2019s conjectural weak dichotomy with modern regret-minimization primitives.\n\n8. **Anytime Calibration Certificates for Auditable Decision Systems**\n   Design \u201ccalibration certificates\u201d that can be produced at any time \\(t\\) to justify that forecasts/determinations remain within a specified error tolerance, aligning with records-retention and oversight requirements. Technically, build martingale-style confidence sequences around calibration error that remain valid under adversarial data, and integrate them with online convex programming updates.\n\n9. **Correlated-Equilibrium Interpretation of Multi-Actor Complaint Resolution**\n   Model multiple stakeholders (complainant, installation, oversight authority) as players in a repeated general-sum game where payoffs are multidimensional (timeliness, readiness impact, rights compliance). Use internal/swap regret dynamics to show convergence to correlated equilibria representing stable procedural outcomes, and identify mechanisms (penalties/regularizers) that steer equilibria toward desired fairness/compliance regions.\n\n10. **Exponential-in-\\(1/\\varepsilon\\) Lower Bounds Beyond the Simplex: Geometry-Dependent Barriers**\n   Generalize the recent lower bound phenomena for \\(\\ell_1\\)-calibration over the simplex to broader convex sets \\(P\\) and norms, characterizing when exponential dependence on \\(1/\\varepsilon\\) is unavoidable. Provide a geometry-dependent lower bound in terms of quantities like strong convexity range \\(\\rho\\), normed diameter, or covering numbers, and match it with near-tight upper bounds using swap-regret-based constructions.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Approachability-Based Procedures for Multi-Stage Determinations with Override Conditions\nModel the complaint-handling process as a repeated vector-payoff game where the target set encodes compliance, ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Internal-to-Calibration Reductions for Constrained Forecast Spaces via Swap Regret\nExtend the Blum\u2013Mansour external\u2192internal regret reduction to settings where forecasts lie in an arbitrary convex set",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Norm-Adaptive High-Dimensional Calibration with Unknown Strongly Convex Regularizer\nBuild a single algorithm that adaptively selects (or learns) an appropriate regularizer \\(R\\) to match the unknown g",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Calibration with Distributional Randomization Minimizing Operational Randomness\nFoster\u2013Vohra style results require randomization to beat adversarial sequences; propose a framework that quantifies and ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Non-Convex Target Sets in Compliance Games: Weak Approachability Meets Policy Rules\nMany real policy goals are effectively non-convex (e.g., discrete categories of determinations, threshold-based trig",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Anytime Calibration Certificates for Auditable Decision Systems\nDesign \u201ccalibration certificates\u201d that can be produced at any time \\(t\\) to justify that forecasts/determinations remain within a specif",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Correlated-Equilibrium Interpretation of Multi-Actor Complaint Resolution\nModel multiple stakeholders (complainant, installation, oversight authority) as players in a repeated general-sum game where p",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 13,
      "paper_title": "In Search of Adam\u2019s Secret Sauce",
      "contribution": "Through a large empirical study and a focused theoretical simplification (\u03b21 = \u03b22), the paper shows that Adam\u2019s empirical advantage over signed/momentum methods largely stems from its coupled mean/variance estimation \u2014 giving a near\u2011optimal, interpretable optimizer that can be seen as an online mean/variance estimator arising from a mean\u2011field Gaussian variational inference view.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11941,
      "output_tokens": 1100,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Adam: A Method for Stochastic Optimization - Intel",
          "url": "https://www.intel.com/content/dam/www/public/us/en/ai/documents/1412.6980.pdf",
          "content": "Published as a conference paper at ICLR 2015\nADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\nDiederik P. Kingma*\nUniversity of Amsterdam, OpenAI\ndpkingma@openai.com\nJimmy Lei Ba\u2217\nUniversity of Toronto\njimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for first-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo\u0002ments. The method is straightforward to implement, is computationally efficient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre\u0002tations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con\u0002vergence properties of the algorithm and provide a regret bound on the conver\u0002gence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the infinity norm.\n1 INTRODUCTION\nStochastic gradient-based optimization is of core practical importance in many fields of science and\nengineering. Many problems in these fields can be cast as the optimization of some scalar parameter\u0002ized objective function requiring maximization or minimization with respect to its parameters. If the\nfunction is differentiable w.r.t. its parameters, gradient descent is a relatively efficient optimization\nmethod, since the computation of first-order partial derivatives w.r.t. all the parameters is of the same\ncomputational complexity as just evaluating the function. Often, objective functions are stochastic.\nFor example, many objective functions are composed of a sum of subfunctions evaluated at different\nsubsamples of data; in this case optimization can be made more efficient by taking gradient steps\nw.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself\nas an efficient and effective optimization method that was central in many machine learning success\nstories, such as recent advances in deep learning (Deng et al., 2013; Krizhevsky et al., 2012; Hinton\n& Salakhutdinov, 2006; Hinton et al., 2012a; Graves et al., 2013). Objectives may also have other\nsources of noise than data subsampling, such as dropout (Hinton et al., 2012b) regularization. For\nall such noisy objectives, efficient stochastic optimization techniques are required. The focus of this\npaper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In\nthese cases, higher-order optimization methods are ill-suited, and discussion in this paper will be\nrestricted to first-order methods.\nWe propose Adam, a method for efficient stochastic optimization that only requires first-order gra\u0002dients with little memory requirement. The method computes individual adaptive learning rates for\ndifferent parameters from estimates of first and second moments of the gradients; the name Adam\nis derived from adaptive moment estimation. Our method is designed to combine the advantages\nof two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gra\u0002dients, and RMSProp (Tieleman & Hinton, 2012), which works well in on-line and non-stationary\nsettings; important connections to these and other stochastic optimization methods are clarified in\nsection 5. Some of Adam\u2019s advantages are that the magnitudes of parameter updates are invariant to\nrescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter,\nit does not require a stationary objective, it works with sparse gradients, and it naturally performs a\nform of step size annealing.\n\u2217Equal contribution. Author ordering determined by coin flip over a Google Hangout.\n1\narXiv:1412.6980v9 [cs.LG] 30 Jan 2017\nPublished as a conference paper at ICLR 2015\nAlgorithm 1: Adam, our proposed algorithm for stochastic optimization. See section 2 for details,\nand for a slightly more efficient (but less clear) order of computation. g\n2\nt\nindicates the elementwise\nsquare gt \f gt. Good default settings for the tested machine learning problems are \u03b1 = 0.001,\n\u03b21 = 0.9, \u03b22 = 0.999 and \u000f = 10\u22128. All operations on vectors are element-wise. With \u03b2\nt\n1\nand \u03b2\nt\n2\nwe denote \u03b21 and \u03b22 to the power t.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208 [0, 1): Exponential decay rates for the moment estimates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u2190 0 (Initialize 1st moment vector)\nv0 \u2190 0 (Initialize 2nd moment vector)\nt \u2190 0 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190 t + 1\ngt \u2190 \u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190 \u03b21 \u00b7 mt\u22121 + (1 \u2212 \u03b21) \u00b7 gt (Update biased first moment estimate)\nvt \u2190 \u03b22 \u00b7 vt\u22121 + (1 \u2212 \u03b22) \u00b7 g\n2\nt\n(Update biased second raw moment estimate)\nmb t \u2190 mt/(1 \u2212 \u03b2\nt\n1\n) (Compute bias-corrected first moment estimate)\nvbt \u2190 vt/(1 \u2212 \u03b2\nt\n2\n) (Compute bias-corrected second raw moment estimate)\n\u03b8t \u2190 \u03b8t\u22121 \u2212 \u03b1 \u00b7 mb t/(\n\u221a\nvbt + \u000f) (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam\u2019s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2 ALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(\u03b8) be a noisy objec\u0002tive function: a stochastic scalar function that is differentiable w.r.t. parameters \u03b8. We are in\u0002terested in minimizing the expected value of this function, E[f(\u03b8)] w.r.t. its parameters \u03b8. With\nf1(\u03b8), ..., , fT (\u03b8) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = \u2207\u03b8ft(\u03b8) we denote the gradient, i.e.\nthe vector of partial derivatives of ft, w.r.t \u03b8 evaluated at timestep t.\nThe algorithm updates exponential moving averages of the gradient (mt) and the squared gradient\n(vt) where the hyper-parameters \u03b21, \u03b22 \u2208 [0, 1) control the exponential decay rates of these moving\naverages. The moving averages themselves are estimates of the 1st moment (the mean) and the\n2\nnd raw moment (the uncentered variance) of the gradient. However, these moving averages are\ninitialized as (vectors of) 0\u2019s, leading to moment estimates that are biased towards zero, especially\nduring the initial timesteps, and especially when the decay rates are small (i.e. the \u03b2s are close to 1).\nThe good news is that this initialization bias can be easily counteracted, resulting in bias-corrected\nestimates mb t and vbt. See section 3 for more details.\nNote that the efficiency of algorithm 1 can, at the expense of clarity, be improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\n\u03b1t = \u03b1 \u00b7\np\n1 \u2212 \u03b2\nt\n2\n/(1 \u2212 \u03b2\nt\n1\n) and \u03b8t \u2190 \u03b8t\u22121 \u2212 \u03b1t \u00b7 mt/(\n\u221a\nvt + \u02c6\u000f).\n2.1 ADAM\u2019S UPDATE RULE\nAn important property of Adam\u2019s update rule is its careful choice of stepsizes. Assuming \u000f = 0, the\neffective step taken in parameter space at timestep t is \u2206t = \u03b1 \u00b7 mb t/\n\u221a\nvbt. The effective stepsize has\ntwo upper bounds: |\u2206t| \u2264 \u03b1 \u00b7 (1 \u2212 \u03b21)/\n\u221a\n1 \u2212 \u03b22 in the case (1 \u2212 \u03b21) >\n\u221a\n1 \u2212 \u03b22, and |\u2206t| \u2264 \u03b1\n2\nPublished as a conference paper at ICLR 2015\notherwise. The first case only happens in the most severe case of sparsity: when a gradient has\nbeen zero at all timesteps except at the current timestep. For less sparse cases, the effective stepsize\nwill be smaller. When (1 \u2212 \u03b21) = \u221a\n1 \u2212 \u03b22 we have that |mb t/\n\u221a\nvbt| < 1 therefore |\u2206t| < \u03b1. In\nmore common scenarios, we will have that mb t/\n\u221a\nvbt \u2248 \u00b11 since |E[g]/\np\nE[g\n2]| \u2264 1. The effective\nmagnitude of the steps taken in parameter space at each timestep are approximately bounded by\nthe stepsize setting \u03b1, i.e., |\u2206t| / \u03b1. This can be understood as establishing a trust region around\nthe current parameter value, beyond which the current gradient estimate does not provide sufficient\ninformation. This typically makes it relatively easy to know the right scale of \u03b1 in advance. For\nmany machine learning models, for instance, we often know in advance that good optima are with\nhigh probability within some set region in parameter space; it is not uncommon, for example, to\nhave a prior distribution over the parameters. Since \u03b1 sets (an upper bound of) the magnitude of\nsteps in parameter space, we can often deduce the right order of magnitude of \u03b1 such that optima\ncan be reached from \u03b80 within some number of iterations. With a slight abuse of terminology,\nwe will call the ratio mb t/\n\u221a\nvbt the signal-to-noise ratio (SNR). With a smaller SNR the effective\nstepsize \u2206t will be closer to zero. This is a desirable property, since a smaller SNR means that\nthere is greater uncertainty about whether the direction of mb t corresponds to the direction of the true\ngradient. For example, the SNR value typically becomes closer to 0 towards an optimum, leading\nto smaller effective steps in parameter space: a form of automatic annealing. The effective stepsize\n\u2206t is also invariant to the scale of the gradients; rescaling the gradients g with factor c will scale mb t\nwith a factor c and vbt with a factor c\n2\n, which cancel out: (c \u00b7 mb t)/(\n\u221a\nc\n2 \u00b7 vbt) = mb t/\n\u221a\nvbt.\n3 INITIALIZATION BIAS CORRECTION\nAs explained in section 2",
          "original_query": "Adam: A Method for Stochastic Optimization (Kingma & Ba, 2014)",
          "cleaned_query": "Adam: A Method for Stochastic Optimization",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Adaptive Subgradient Methods for Online Learning and ...",
          "url": "https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf",
          "content": "Journal of Machine Learning Research 12 (2011) 2121-2159 Submitted 3/10; Revised 3/11; Published 7/11\nAdaptive Subgradient Methods for\nOnline Learning and Stochastic Optimization\u2217\nJohn Duchi JDUCHI@CS.BERKELEY.EDU\nComputer Science Division\nUniversity of California, Berkeley\nBerkeley, CA 94720 USA\nElad Hazan EHAZAN@IE.TECHNION.AC.IL\nTechnion - Israel Institute of Technology\nTechnion City\nHaifa, 32000, Israel\nYoram Singer SINGER@GOOGLE.COM\nGoogle\n1600 Amphitheatre Parkway\nMountain View, CA 94043 USA\nEditor: Tong Zhang\nAbstract\nWe present a new family of subgradient methods that dynamically incorporate knowledge of the\ngeometry of the data observed in earlier iterations to perform more informative gradient-based\nlearning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very\npredictive but rarely seen features. Our paradigm stems from recent advances in stochastic op\u0002timization and online learning which employ proximal functions to control the gradient steps of\nthe algorithm. We describe and analyze an apparatus for adaptively modifying the proximal func\u0002tion, which significantly simplifies setting a learning rate and results in regret guarantees that are\nprovably as good as the best proximal function that can be chosen in hindsight. We give several\nefficient algorithms for empirical risk minimization problems with common and important regu\u0002larization functions and domain constraints. We experimentally study our theoretical analysis and\nshow that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient\nalgorithms.\nKeywords: subgradient methods, adaptivity, online learning, stochastic convex optimization\n1. Introduction\nIn many applications of online and stochastic learning, the input instances are of very high di\u0002mension, yet within any particular instance only a few features are non-zero. It is often the case,\nhowever, that infrequently occurring features are highly informative and discriminative. The infor\u0002mativeness of rare features has led practitioners to craft domain-specific feature weightings, such as\nTF-IDF (Salton and Buckley, 1988), which pre-emphasize infrequently occurring features. We use\nthis old idea as a motivation for applying modern learning-theoretic techniques to the problem of\nonline and stochastic learning, focusing concretely on (sub)gradient methods.\n\u2217. A preliminary version of this work was published in COLT 2010.\n c 2011 John Duchi, Elad Hazan and Yoram Singer.\nDUCHI, HAZAN AND SINGER\nStandard stochastic subgradient methods largely follow a predetermined procedural scheme that\nis oblivious to the characteristics of the data being observed. In contrast, our algorithms dynamically\nincorporate knowledge of the geometry of the data observed in earlier iterations to perform more\ninformative gradient-based learning. Informally, our procedures give frequently occurring features\nvery low learning rates and infrequent features high learning rates, where the intuition is that each\ntime an infrequent feature is seen, the learner should \u201ctake notice.\u201d Thus, the adaptation facilitates\nfinding and identifying very predictive but comparatively rare features.\n1.1 The Adaptive Gradient Algorithm\nBefore introducing our adaptive gradient algorithm, which we term ADAGRAD, we establish no\u0002tation. Vectors and scalars are lower case italic letters, such as x \u2208 X. We denote a sequence of\nvectors by subscripts, that is, xt, xt+1,..., and entries of each vector by an additional subscript, for\nexample, xt, j. The subdifferential set of a function f evaluated at x is denoted \u2202 f(x), and a partic\u0002ular vector in the subdifferential set is denoted by f\n\u2032\n(x) \u2208 \u2202 f(x) or gt \u2208 \u2202 ft(xt). When a function\nis differentiable, we write \u2207 f(x). We use hx, yi to denote the inner product between x and y. The\nBregman divergence associated with a strongly convex and differentiable function \u03c8 is\nB\u03c8(x, y) = \u03c8(x)\u2212\u03c8(y)\u2212h\u2207\u03c8(y), x\u2212yi .\nWe also make frequent use of the following two matrices. Let g1:t = [g1 \u00b7\u00b7\u00b7 gt] denote the matrix\nobtained by concatenating the subgradient sequence. We denote the ith row of this matrix, which\namounts to the concatenation of the ith component of each subgradient we observe, by g1:t,i. We\nalso define the outer product matrix Gt = \u2211\nt\n\u03c4=1\ng\u03c4g\u03c4\n\u22a4.\nOnline learning and stochastic optimization are closely related and basically interchangeable\n(Cesa-Bianchi et al., 2004). In order to keep our presentation simple, we confine our discussion and\nalgorithmic descriptions to the online setting with the regret bound model. In online learning, the\nlearner repeatedly predicts a point xt \u2208 X \u2286 R\nd\n, which often represents a weight vector assigning\nimportance values to various features. The learner\u2019s goal is to achieve low regret with respect to a\nstatic predictor x\n\u2217\nin the (closed) convex set X \u2286 R\nd\n(possibly X = R\nd\n) on a sequence of functions\nft(x), measured as\nR(T) =\nT\n\u2211\nt=1\nft(xt)\u2212 inf\nx\u2208X\nT\n\u2211\nt=1\nft(x) .\nAt every timestep t, the learner receives the (sub)gradient information gt \u2208 \u2202 ft(xt). Standard sub\u0002gradient algorithms then move the predictor xt\nin the opposite direction of gt while maintaining\nxt+1 \u2208 X via the projected gradient update (e.g., Zinkevich, 2003)\nxt+1 = \u03a0X (xt \u2212\u03b7gt) = argmin\nx\u2208X\nkx\u2212(xt \u2212\u03b7gt)k\n2\n2\n.\nIn contrast, let the Mahalanobis norm k\u00b7kA =\np\nh\u00b7,A\u00b7i and denote the projection of a point y onto X\naccording to A by \u03a0A\nX\n(y) = argminx\u2208X\nkx\u2212ykA = argminx\u2208X\nhx\u2212y,A(x\u2212y)i. Using this notation,\nour generalization of standard gradient descent employs the update\nxt+1 = \u03a0\nG\n1/2\nt\nX\n\u0010\nxt \u2212\u03b7G\n\u22121/2\nt gt\n\u0011\n.\n2122\nADAPTIVE SUBGRADIENT METHODS\nThe above algorithm is computationally impractical in high dimensions since it requires computa\u0002tion of the root of the matrix Gt\n, the outer product matrix. Thus we specialize the update to\nxt+1 = \u03a0\ndiag(Gt)\n1/2\nX\n\u0010\nxt \u2212\u03b7diag(Gt)\n\u22121/2\ngt\n\u0011\n. (1)\nBoth the inverse and root of diag(Gt) can be computed in linear time. Moreover, as we discuss later,\nwhen the gradient vectors are sparse the update above can often be performed in time proportional\nto the support of the gradient. We now elaborate and give a more formal discussion of our setting.\nIn this paper we consider several different online learning algorithms and their stochastic convex\noptimization counterparts. Formally, we consider online learning with a sequence of composite\nfunctions \u03c6t. Each function is of the form \u03c6t(x) = ft(x) +\u03d5(x) where ft and \u03d5 are (closed) convex\nfunctions. In the learning settings we study, ftis either an instantaneous loss or a stochastic estimate\nof the objective function in an optimization task. The function \u03d5 serves as a fixed regularization\nfunction and is typically used to control the complexity of x. At each round the algorithm makes a\nprediction xt \u2208 X and then receives the function ft. We define the regret with respect to the fixed\n(optimal) predictor x\n\u2217\nas\nR\u03c6(T) ,\nT\n\u2211\nt=1\n[\u03c6t(xt)\u2212\u03c6t(x\n\u2217\n)] =\nT\n\u2211\nt=1\n[ ft(xt) +\u03d5(xt)\u2212 ft(x\n\u2217\n)\u2212\u03d5(x\n\u2217\n)] . (2)\nOur goal is to devise algorithms which are guaranteed to suffer asymptotically sub-linear regret,\nnamely, R\u03c6(T) = o(T).\nOur analysis applies to related, yet different, methods for minimizing the regret (2). The first\nis Nesterov\u2019s primal-dual subgradient method (2009), and in particular Xiao\u2019s (2010) extension,\nregularized dual averaging, and the follow-the-regularized-leader (FTRL) family of algorithms (see\nfor instance Kalai and Vempala, 2003; Hazan et al., 2006). In the primal-dual subgradient method\nthe algorithm makes a prediction xt on round t using the average gradient \u00afgt =\n1\nt \u2211\nt\n\u03c4=1\ng\u03c4. The update\nencompasses a trade-off between a gradient-dependent linear term, the regularizer \u03d5, and a strongly\u0002convex term \u03c8t for well-conditioned predictions. Here \u03c8t\nis the proximal term. The update amounts\nto solving\nxt+1 = argmin\nx\u2208X\n\u001a\n\u03b7hg\u00aft, xi+\u03b7\u03d5(x) + 1\nt\n\u03c8t(x)\n\u001b\n, (3)\nwhere \u03b7 is a fixed step-size and x1 = argminx\u2208X \u03d5(x). The second method similarly has numer\u0002ous names, including proximal gradient, forward-backward splitting, and composite mirror descent\n(Tseng, 2008; Duchi et al., 2010). We use the term composite mirror descent. The composite mirror\ndescent method employs a more immediate trade-off between the current gradient gt, \u03d5, and staying\nclose to xt using the proximal function \u03c8,\nxt+1 = argmin\nx\u2208X\n\b\n\u03b7hgt, xi+\u03b7\u03d5(x) +B\u03c8t(x, xt)\n \n. (4)\nOur work focuses on temporal adaptation of the proximal function in a data driven way, while\nprevious work simply sets \u03c8t \u2261 \u03c8, \u03c8t(\u00b7) = \u221a\nt\u03c8(\u00b7), or \u03c8t(\u00b7) = t\u03c8(\u00b7) for some fixed \u03c8.\nWe provide formal analyses equally applicable to the above two updates and show how to au\u0002tomatically choose the function \u03c8t so as to achieve asymptotically small regret. We describe and\nanalyze two algorithms. Both algorithms use squared Mahalanobis norms as their proximal func\u0002tions, setting \u03c8t(x) = hx,Htxi for a symmetric matrix Ht \u0017 0. The first uses diagonal matrices while\n2123\nDUCHI, HAZAN AND SINGER\nthe second constructs full dimensional matrices. Concretely, for some small fixed \u03b4 \u2265 0 (specified\nlater, though in practice \u03b4 can be set to 0) we set\nHt = \u03b4I +diag(Gt)\n1/2\n(Diagonal) and Ht = \u03b4I +G\n1/2\nt\n(Full) . (5)\nPlugging the appropriate matrix from the above equation into \u03c8tin (3) or (4) gives rise to our\nADAGRAD family of algorithms. Informally, we obtain algorithms which are similar to second\u0002order gradient descent by constructing approximations to the Hessian of the functions ft\n, though we\nuse roots of the matrices.\n1.2 Outline of Results\nWe now outline our results, deferring formal statements of the theorems to later sections. Recall the\ndefinitions of g1:t as the matrix of concatenated subgradients and Gt as the outer product matrix in\nthe prequel. The ADAGRAD algorithm with full matrix divergences entertains bounds of the form\nR\u03c6(T) = O\n\u0010\nkx\n\u2217\nk2\ntr(G\n1/2\nT\n)\n\u0011\nand R\u03c6(T) = O\n\u0012\nmax\nt\u2264T\nkxt \u2212x\n\u2217\nk2\ntr(G\n1/2\nT\n)\n\u0013\n.\nWe further show that\ntr\u0010G\n1/2\nT\n\u0011\n= d\n1/2\nvuutinf\nS\n(\nT\n\u2211\nt=1\nhgt\n,S\n\u22121gti : S \u0017 0,tr(S) \u2264 d\n)\n.\nThese results are formally given in Theorem ",
          "original_query": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (AdaGrad) (Duchi, Hazan & Singer, 2011)",
          "cleaned_query": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (AdaGrad)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[1802.04434] signSGD: Compressed Optimisation for Non- ...",
          "url": "https://arxiv.org/abs/1802.04434",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1802.04434** (cs)\n\n\\[Submitted on 13 Feb 2018 ( [v1](https://arxiv.org/abs/1802.04434v1)), last revised 7 Aug 2018 (this version, v3)\\]\n\n# Title:signSGD: Compressed Optimisation for Non-Convex Problems\n\nAuthors: [Jeremy Bernstein](https://arxiv.org/search/cs?searchtype=author&query=Bernstein,+J), [Yu-Xiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Y), [Kamyar Azizzadenesheli](https://arxiv.org/search/cs?searchtype=author&query=Azizzadenesheli,+K), [Anima Anandkumar](https://arxiv.org/search/cs?searchtype=author&query=Anandkumar,+A)\n\nView a PDF of the paper titled signSGD: Compressed Optimisation for Non-Convex Problems, by Jeremy Bernstein and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/1802.04434)\n\n> Abstract:Training large neural networks requires distributing learning across multiple workers, where the cost of communicating gradients can be a significant bottleneck. signSGD alleviates this problem by transmitting just the sign of each minibatch stochastic gradient. We prove that it can get the best of both worlds: compressed gradients and SGD-level convergence rate. The relative $\\\\ell\\_1/\\\\ell\\_2$ geometry of gradients, noise and curvature informs whether signSGD or SGD is theoretically better suited to a particular problem. On the practical side we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models. We extend our theory to the distributed setting, where the parameter server uses majority vote to aggregate gradient signs from each worker enabling 1-bit compression of worker-server communication in both directions. Using a theorem by Gauss we prove that majority vote can achieve the same reduction in variance as full precision distributed SGD. Thus, there is great promise for sign-based optimisation schemes to achieve fast communication and fast convergence. Code to reproduce experiments is to be found at [this https URL](https://github.com/jxbz/signSGD) .\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:1802.04434](https://arxiv.org/abs/1802.04434) \\[cs.LG\\] |\n| | (or [arXiv:1802.04434v3](https://arxiv.org/abs/1802.04434v3) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1802.04434](https://doi.org/10.48550/arXiv.1802.04434) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jeremy Bernstein \\[ [view email](https://arxiv.org/show-email/c30de5d0/1802.04434)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1802.04434v1)**\nTue, 13 Feb 2018 02:14:35 UTC (201 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1802.04434v2)**\nSat, 23 Jun 2018 18:01:27 UTC (478 KB)\n\n**\\[v3\\]**\nTue, 7 Aug 2018 18:55:19 UTC (477 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled signSGD: Compressed Optimisation for Non-Convex Problems, by Jeremy Bernstein and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/1802.04434)\n- [TeX Source](https://arxiv.org/src/1802.04434)\n- [Other Formats](https://arxiv.org/format/1802.04434)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1802.04434&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1802.04434&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2018-02](https://arxiv.org/list/cs.LG/2018-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1802.04434?context=cs)\n\n[cs.DC](https://arxiv.org/abs/1802.04434?context=cs.DC)\n\n[math](https://arxiv.org/abs/1802.04434?context=math)\n\n[math.OC](https://arxiv.org/abs/1802.04434?context=math.OC)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1802.04434)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1802.04434)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1802.04434)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1802.html#abs-1802-04434) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1802-04434)\n\n[Jeremy Bernstein](https://dblp.uni-trier.de/search/author?author=Jeremy%20Bernstein)\n\n[Yu-Xiang Wang](https://dblp.uni-trier.de/search/author?author=Yu-Xiang%20Wang)\n\n[Kamyar Azizzadenesheli](https://dblp.uni-trier.de/search/author?author=Kamyar%20Azizzadenesheli)\n\n[Anima Anandkumar](https://dblp.uni-trier.de/search/author?author=Anima%20Anandkumar)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1802.04434&description=signSGD: Compressed Optimisation for Non-Convex Problems) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1802.04434&title=signSGD: Compressed Optimisation for Non-Convex Problems)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1802.04434) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "signSGD: Compressed Optimization for Non\u2011Convex Problems (Bernstein et al., 2018)",
          "cleaned_query": "signSGD: Compressed Optimization for Non\u2011Convex Problems",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
          "url": "https://arxiv.org/abs/1704.04289",
          "content": "# Statistics > Machine Learning\n\n**arXiv:1704.04289** (stat)\n\n\\[Submitted on 13 Apr 2017 ( [v1](https://arxiv.org/abs/1704.04289v1)), last revised 19 Jan 2018 (this version, v2)\\]\n\n# Title:Stochastic Gradient Descent as Approximate Bayesian Inference\n\nAuthors: [Stephan Mandt](https://arxiv.org/search/stat?searchtype=author&query=Mandt,+S), [Matthew D. Hoffman](https://arxiv.org/search/stat?searchtype=author&query=Hoffman,+M+D), [David M. Blei](https://arxiv.org/search/stat?searchtype=author&query=Blei,+D+M)\n\nView a PDF of the paper titled Stochastic Gradient Descent as Approximate Bayesian Inference, by Stephan Mandt and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/1704.04289)\n\n> Abstract:Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.\n\n| | |\n| --- | --- |\n| Comments: | 35 pages, published version (JMLR 2017) |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1704.04289](https://arxiv.org/abs/1704.04289) \\[stat.ML\\] |\n| (or [arXiv:1704.04289v2](https://arxiv.org/abs/1704.04289v2) \\[stat.ML\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1704.04289](https://doi.org/10.48550/arXiv.1704.04289) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Journal of Machine Learning Research 18 (2017) 1-35 |\n\n## Submission history\n\nFrom: Stephan Mandt \\[ [view email](https://arxiv.org/show-email/046b0812/1704.04289)\\] **[\\[v1\\]](https://arxiv.org/abs/1704.04289v1)**\nThu, 13 Apr 2017 22:17:30 UTC (3,148 KB)\n**\\[v2\\]**\nFri, 19 Jan 2018 21:07:09 UTC (3,018 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Stochastic Gradient Descent as Approximate Bayesian Inference, by Stephan Mandt and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/1704.04289)\n- [TeX Source](https://arxiv.org/src/1704.04289)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1704.04289&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1704.04289&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2017-04](https://arxiv.org/list/stat.ML/2017-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1704.04289?context=cs) [cs.LG](https://arxiv.org/abs/1704.04289?context=cs.LG) [stat](https://arxiv.org/abs/1704.04289?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1704.04289)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1704.04289)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1704.04289)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1704.04289) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Stochastic Gradient Descent as Approximate Bayesian Inference (Mandt, Hoffman & Blei, 2017)",
          "cleaned_query": "Stochastic Gradient Descent as Approximate Bayesian Inference",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Decoupled Weight Decay Regularization - OpenReview",
          "url": "https://openreview.net/forum?id=Bkg6RiCqY7",
          "content": "[![back arrow](https://openreview.net/images/arrow_left.svg)Go to **ICLR 2019 Conference** homepage](https://openreview.net/group?id=ICLR.cc/2019/Conference)\n\n**Error:** Could not verify browser. Please make sure third-party scripts are not being blocked and try again.\n\n## Decoupled Weight Decay Regularization [![](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=Bkg6RiCqY7)\n\n## Blind Submission by Conference \u2022 Decoupled Weight Decay Regularization\n\n[Ilya Loshchilov](https://openreview.net/profile?email=ilya.loshchilov%40gmail.com), [Frank Hutter](https://openreview.net/profile?email=fh%40cs.uni-freiburg.de)\n\nPublished: 20 Dec 2018, Last Modified: 19 May 2023ICLR 2019 Conference Blind SubmissionReaders: Everyone[Show Bibtex](https://openreview.net/forum?id=Bkg6RiCqY7) [Show Revisions](https://openreview.net/revisions?id=Bkg6RiCqY7)\n\nKeywords: optimization, regularization, weight decay, Adam\n\nAbstract: L2 regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\\\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L2 regularization (often calling it \\`\\`weight decay'' in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\\\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at \\\\url{ [https://github.com/loshchil/AdamW-and-SGDW}](https://github.com/loshchil/AdamW-and-SGDW%7D)\n\nCode: [![github](https://openreview.net/images/github_icon.svg) loshchil/AdamW-and-SGDW](https://github.com/loshchil/AdamW-and-SGDW) \\+ [![Papers with Code](https://openreview.net/images/pwc_icon.svg) 19 community implementations](https://paperswithcode.com/paper/?openreview=Bkg6RiCqY7)\n\nData: [CIFAR-10](https://paperswithcode.com/dataset/cifar-10), [ImageNet-32](https://paperswithcode.com/dataset/imagenet-32)\n\n* * *\n\nReply Type:\n\nall\n\n- Select All\n- Paper939 Meta Review\n- Paper939 Official Review\n- Paper939 Official Comment\n\nAuthor:\n\neverybody\n\n- Select All\n- Paper939 Authors\n- Paper939 AnonReviewer1\n- Paper939 AnonReviewer2\n- Paper939 AnonReviewer3\n- Paper939 Area Chair1\n\nVisible To:\n\nall readers\n\n- Select All\n- everyone\n\nHidden From:\n\nnobody\n\n- Select All\n- everyone\n\n10 Replies\n\n[\\[\u2013\\]](https://openreview.net/forum?id=Bkg6RiCqY7) [\\[+\\]](https://openreview.net/forum?id=Bkg6RiCqY7)\n\n## a useful and influential finding\n\n## Meta Review of Paper939 by Area Chair1 \u2022 a useful and influential finding\n\nICLR 2019 Conference Paper939 Area Chair1\n\n05 Dec 2018, 09:49 (modified: 20 Dec 2018, 20:08)ICLR 2019 Conference Paper939 Meta ReviewReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=rkeqKBDSkV)\n\nMetareview: Evaluating this paper is somewhat awkward because it has already been through multiple reviewing cycles, and in the meantime, the trick has already become widely adopted and inspired interesting follow-up work. Much of the paper is devoted to reviewing this follow-up work. I think it's clearly time for this to be made part of the published literature, so I recommend acceptance. (And all reviewers are in agreement that the paper ought to be accepted.)\nThe paper proposes, in the context of Adam, to apply literal weight decay in place of L2 regularization. An impressively thorough set of experiments are given to demonstrate the improved generalization performance, as well as a decoupling of the hyperparameters.\nPrevious versions of the paper suffered from a lack of theoretical justification for the proposed method. Ordinarily, in such cases, one would worry that the improved results could be due to some sort of experimental confound. But AdamW has been validated by so many other groups on a range of domains that the improvement is well established. And other researchers have offered possible explanations for the improvement.\n\nRecommendation: Accept (Poster)\n\nConfidence: 5: The area chair is absolutely certain\n\n[\\[\u2013\\]](https://openreview.net/forum?id=Bkg6RiCqY7) [\\[+\\]](https://openreview.net/forum?id=Bkg6RiCqY7)\n\n## Response to All Reviewers\n\n## Official Comment by Paper939 Authors \u2022 Response to All Reviewers\n\nICLR 2019 Conference Paper939 Authors\n\n26 Nov 2018, 18:32ICLR 2019 Conference Paper939 Official CommentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=HJlCOfb90X)\n\nComment: We thank all reviewers for their positive evaluation and their valuable comments. We've uploaded a revision to address the issues raised and individually reply to the reviewers' concerns. We kindly ask you to update your rating if our replies clarified your concerns.\nThank you again for your reviews!\n\n[\\[\u2013\\]](https://openreview.net/forum?id=Bkg6RiCqY7) [\\[+\\]](https://openreview.net/forum?id=Bkg6RiCqY7)\n\n## Good paper, several concerns\n\n## Official Review of Paper939 by AnonReviewer1 \u2022 Good paper, several concerns\n\nICLR 2019 Conference Paper939 AnonReviewer1\n\n02 Nov 2018, 18:21 (modified: 06 Nov 2018, 14:46)ICLR 2019 Conference Paper939 Official ReviewReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=rkeDkABcnm)\n\nReview: In this paper, the authors investigate a very simple but still very interesting idea of decoupling weight decay and gradient step. It is a well known problem that Adam optimization method leads to worse generalization and stronger overfitting than SGD with momentum on classification tasks despite its faster convergence. The authors tried to find a reason for such behavior. They noticed that while SGD with L2 regularization is equivalent to SGD with weight decay, it is not the case for adaptive methods, such as Adam. The main contributions include the following:\n1\\. Improvement of Adam method via decoupling weight decay and optimization step and using warm restarts. The authors thoroughly investigated the proposed idea on different learning rate schedules and different datasets. It would also be interesting to see results on architectures other than ResNet. In section 4.5 the authors claim that the proposed idea was used in different settings by many authors. So, I would recommend to elaborate on this section in the final version of the paper.\n2\\. Reducing sensitivity of SGD to weight decay parameter. The authors noticed that the optimal weight decay parameter depends on the number of training epochs, therefore they proposed a functional form of dependency between weight decay and the number of batch passes.\nI also have the following concerns:\n1\\. One of the main advantages of Adam is the speed of convergence. Does AdamW or AdamWR converge faster than the corresponding SGD method? Figure 4 is not quite representative since it contains an experiment with a very large number of training epochs.\n2\\. While AdamWR delivers much better test accuracy than Adam, it is still slightly worse than SGDWR method.\nI would also recommend to change scale of y-axis, Figure 4, right. Since 0.5% percent difference can be significant for state-of-the-art classification results.\nOverall, the paper is written clearly and organized well. It contains a lot of experiments and proposes an explanation of the observed phenomena. While the idea is very simple, the experimental results show its efficiency.\n\nRating: 6: Marginally above acceptance threshold\n\nConfidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n\n[\\[\u2013\\]](https://openreview.net/forum?id=Bkg6RiCqY7) [\\[+\\]](https://openreview.net/forum?id=Bkg6RiCqY7)\n\n## Response\n\n## Official Comment by Paper939 Authors \u2022 Response\n\nICLR 2019 Conference Paper939 Authors\n\n26 Nov 2018, 18:25ICLR 2019 Conference Paper939 Official CommentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=B1xxyZZqCm)\n\nComment: Thank you for the positive detailed review and your questions and comments. We reply to them below.\n\\\\*\\\\*\\\\*\n\u201cIt would also be interesting to see results on architectures other than ResNet. In section 4.5 the authors claim that the proposed idea was used in different settings by many authors. So, I would recommend to elaborate on this section in the final version of the paper.\u201d\nPlease see Section 4.5 where we now mention additional applications of our decoupled weight decay and AdamW.\n\\\\*\\\\*\\\\*\n\u201c1\\. One of the main advantages of Adam is the speed of convergence. Does AdamW or AdamWR converge faster than the corresponding SGD method? Figure 4 is not quite representative since it contains an experiment with a very large number of training epochs.\u201d\nTo address this question, we added SuppFigure 5, SuppFigure 6 and the following text in the supplementary material:\n\u201cSuppFigure 5 and SuppFigure 6 are the equivalents of Figure 4 in the main paper but supplemented with training loss curves in its bottom row. The results show that Adam and its variants with decoupled weight decay converge faster (in terms of training loss) on CIFAR-10 than the corresponding SGD variants (the difference for ImageNet32x32 is small). As is discussed in the main paper, when the same values of training loss are considered, AdamW demonstrates better values of test error than Adam. Interestingly, SuppFigure 5 and SuppFigure 6 show that restart variants AdamWR and SGDWR also demonstrate bet",
          "original_query": "Decoupled Weight Decay Regularization (AdamW) (Loshchilov & Hutter, 2019)",
          "cleaned_query": "Decoupled Weight Decay Regularization (AdamW)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Training Compute-Optimal Large Language Models - NeurIPS",
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf",
          "content": "Training Compute-Optimal Large Language Models\nJordan Hoffmann\u2217 Sebastian Borgeaud\u2217 Arthur Mensch\u2217 Elena Buchatskaya\nTrevor Cai Eliza Rutherford Diego de Las Casas Lisa Anne Hendricks\nJohannes Welbl Aidan Clark Tom Hennigan Eric Noland Katie Millican\nGeorge van den Driessche Bogdan Damoc Aurelia Guy Simon Osindero\nKaren Simonyan Erich Elsen Oriol Vinyals Jack W. Rae Laurent Sifre\u2217\n\u2217 Equal contributions\nDeepMind\n(sborgeaud|amensch|sifre)@deepmind.com\nAbstract\nWe investigate the optimal model size and number of tokens for training a Trans\u0002former language model under a given compute budget. We find that current large\nlanguage models are significantly undertrained, a consequence of the recent focus\non scaling language models whilst keeping the amount of training data constant.\nBy training over 400 language models ranging from 70 million to over 16 billion\nparameters on 5 to 500 billion tokens, we find that for compute-optimal training, the\nmodel size and the number of training tokens should be scaled equally: for every\ndoubling of model size the number of training tokens should also be doubled. We\ntest this hypothesis by training a predicted compute-optimal model, Chinchilla, that\nuses the same compute budget as Gopher but with 70B parameters and 4\u00d7 more\nmore data. Chinchilla uniformly and significantly outperforms Gopher (280B),\nGPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large\nrange of downstream evaluation tasks. This also means that Chinchilla uses substan\u0002tially less compute for fine-tuning and inference, greatly facilitating downstream\nusage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of\n67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.\n1 Introduction\nA series of Large Language Models (LLMs) have recently been introduced [6, 30, 38, 48, 52], with the\nlargest dense language models now having over 500 billion parameters. These large autoregressive\ntransformers [53] have demonstrated impressive performance on many tasks using a variety of\nevaluation protocols: zero-shot generalization, few-shot training, and as a basis for fine-tuning. The\ncompute and energy cost for training large language models is substantial [38, 52] and rises with\nincreasing model size. In practice, the allocated training compute budget is often known in advance:\npractitioners have access to a certain number of accelerators for a given period of time. Since it\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n10\n17 10\n19 1021 1023 1025\nFLOPs\n10M\n100M\n1.0B\n10B\n100B\n1T\nParameters\nApproach 1\nApproach 2\nApproach 3\nKaplan et al (2020)\nChinchilla (70B)\nGopher (280B)\nGPT-3 (175B)\nMegatron-Turing NLG (530B)\nFigure 1: Overlaid predictions. We overlay the predictions from our three different approaches, along\nwith projections from [23]. We find that all three methods predict that current large models should be\nsubstantially smaller and therefore trained much longer than is currently done. In Figure A3, we show\nthe results with the predicted optimal tokens plotted against the optimal number of parameters for\nfixed FLOP budgets. Chinchilla outperforms Gopher and the other large models (see Section 4.2).\nTable 1: Current LLMs. We show five of the current largest dense transformer models, their size, and\nthe number of training tokens. Other than LaMDA [52], most models are trained for approximately\n300 billion tokens. We introduce Chinchilla, a substantially smaller model, trained for much longer\nthan 300B tokens. Table A3 shows our projected optimal relation between model size and tokens.\nModel Size (# Parameters) Training Tokens\nLaMDA [52] 137 Billion 768 Billion\nGPT-3 [6] 175 Billion 300 Billion\nJurassic [30] 178 Billion 300 Billion\nGopher [38] 280 Billion 300 Billion\nMT-NLG 530B [48] 530 Billion 270 Billion\nChinchilla 70 Billion 1.4 Trillion\nis typically only feasible to train these large models once, accurately estimating the best model\nhyperparameters for a given compute budget is critical [51].\nKaplan et al. [23] showed that there is a power law relationship between the number of parameters\nin an autoregressive language model (LM) and its performance (measured in evaluation perplexity).\nOne notable conclusion in [23] is that large models should not be trained to their lowest possible loss\nto be compute optimal; they argue that model size should grow faster than the size of the training\nset for a given increase of computational budget. As a result, the field has been training larger and\nlarger models while keeping the size of the training set to approximately 300 billion tokens, expecting\nperformance improvements (Table 1). While we find that there is effectively a trade-off between\nmodel size and training set size, we estimate that large models should be trained for many more\ntraining tokens than recommended by [23]. Specifically, given a 10\u00d7 increase computational budget\nwe find that model size and the number of training tokens should be scaled in equal proportions.\nIn this work, we revisit the question: Given a fixed FLOPs budget,1 how should one trade-off model\nsize and the number of training tokens? To answer this question, we model the final pre-training\nloss2 L(N, D) as a function of the number of model parameters N, and the number of training\ntokens, D. Since the computational budget C is a deterministic function FLOPs(N, D) of the number\nof seen training tokens and model parameters, we are interested in minimizing L under the constraint\n1\nFor example, knowing the number of accelerators and a target training duration.\n2\nFor simplicity, we perform our analysis on the smoothed training loss which is an unbiased estimate of the\ntest loss, as the number of training tokens is less than the number of tokens in the entire corpus.\n2\nFLOPs(N, D) = C:\nNopt(C), Dopt(C) = argmin\nN,D s.t. FLOPs(N,D)=C\nL(N, D). (1)\nThe functions Nopt(C), and Dopt(C) describe the optimal allocation of a computational budget C.\nWe empirically estimate these functions based on the losses of over 400 models, ranging from under\n70M to over 16B parameters, and trained on 5B to over 400B tokens \u2013 with each model configuration\ntrained for several different training horizons. Our approach leads to considerably different results\nthan that of [23]. We highlight our results in Figure 1 and how our approaches differ in Section 2.\nBased on our estimated compute-optimal frontier, we predict that for the compute budget used to\ntrain Gopher, an optimal model should be 4 times smaller, while being training on 4 times more\ntokens. We verify this by training a more compute-optimal 70B model, called Chinchilla, on 1.4\ntrillion tokens. Not only does Chinchilla outperform its much larger counterpart, Gopher, but its\nreduced model size reduces inference cost considerably and greatly facilitates downstream uses on\nsmaller hardware. The energy cost of a large language model is amortized through its usage for\ninference and fine-tuning. The benefits of a more optimally trained smaller model, therefore, extend\nbeyond the immediate benefits of its improved performance.\n2 Related Work\nLarge language models. A variety of large language models have been introduced in the last few\nyears. These include both dense transformer models [6, 30, 48, 38, 52] and mixture-of-expert (MoE)\nmodels [11, 12, 60]. The largest dense transformers have passed 500 billion parameters [48, 8]. The\ndrive to train larger and larger models is clear\u2014so far increasing the size of language models has been\nresponsible for improving the state-of-the-art in many language modelling tasks. Nonetheless, large\nlanguage models face several challenges, including their overwhelming computational requirements\n(the cost of training and inference increase with model size) [38, 52] and the need for acquiring more\nhigh-quality training data. In fact, in this work we find that larger, high quality datasets will play a key\nrole in any further scaling of language models. Concurrent to our work, a 540 billion parameter model\ntrained on 768 billion tokens was released\u2013 PaLM [8]. While this model outperforms Chinchilla, it\nuses approximately 5\u00d7 the compute and is nearly 8\u00d7 larger, making it more difficult to use.\nModelling the scaling behavior. Understanding the scaling behaviour of language models and their\ntransfer properties has been important in the development of recent large models [23, 18]. Kaplan\net al. [23] first showed a predictable relationship between model size and loss over many orders of\nmagnitude. The authors investigate the question of choosing the optimal model size to train for a\ngiven compute budget. Similar to us, they address this question by training various models. Our\nwork differs from Kaplan et al. [23] in several important ways. First, the authors use a fixed number\nof training tokens and learning rate schedule for all models; this prevents them from modelling the\nimpact of these hyperparameters on the loss. In contrast, we find that setting the learning rate schedule\nto approximately match the number of training tokens results in the best final loss regardless of model\nsize\u2014see Figure A1. For a fixed learning rate cosine schedule to 130B tokens, the intermediate loss\nestimates (for D\u2032 << 130B) are therefore overestimates of the loss of a model trained with a schedule\nlength matching D\u2032. Using these intermediate losses results in underestimating the effectiveness\nof training models on less data than 130B tokens, and eventually contributes to the conclusion that\nmodel size should increase faster than training data size as compute budget increases. In contrast, our\nanalysis predicts that both quantities should scale at roughly the same rate. Secondly, we include\nmodels with up to 16B parameters, as we observe that there is slight curvature in the FLOP-loss\nfrontier (see Appendix E)\u2014in fact, the majority of the models used in our analysis have more than\n500 million parameters, in contrast the majority of runs in [23] are significa",
          "original_query": "Training Compute\u2011Optimal Large Language Models (Chinchilla) (Hoffmann et al., 2022)",
          "cleaned_query": "Training Compute\u2011Optimal Large Language Models (Chinchilla)",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Compute-Optimal Optimizer Scheduling for LLM Pretraining\n- Extend Chinchilla-style compute-optimal scaling by jointly optimizing *optimizer hyperparameters* (Adam/AdamW \u03b2\u2019s, \u03b5, weight decay) and *training tokens/model size* under a fixed FLOPs budget. Concretely, run controlled sweeps where learning-rate/\u03b2 schedules are treated as decision variables in the compute-allocation problem \\( \\min_{N,D,\\text{sched}} L(N,D,\\text{sched}) \\) to see whether the \u201cscale tokens with parameters\u201d rule shifts when the optimizer dynamics change.",
        "Adam as Approximate Bayesian Inference with Decoupled Weight Decay\n- Combine Mandt et al.\u2019s \u201cconstant SGD as a stationary distribution\u201d view with AdamW\u2019s decoupled weight decay to derive a stationary-distribution approximation for *adaptive-moment* methods. The actionable outcome is a principled procedure to set AdamW\u2019s learning rate and weight decay to match a target posterior covariance (e.g., diagonal Laplace/Fisher), then validate calibration and downstream uncertainty on large Transformer pretraining.",
        "SignAdamW: 1-bit Communication with AdamW-Style Regularization\n- Design a distributed optimizer that transmits only gradient signs (as in signSGD) while maintaining AdamW-like second-moment tracking locally and applying decoupled weight decay globally. Implement majority-vote aggregation for signs, but update the preconditioner using a low-precision sketch (e.g., blockwise variance estimates) to preserve Adam-like speed; evaluate wall-clock convergence on multi-node LLM training.",
        "Adaptive Geometry Meets Compression: Diagonal AdaGrad-Preconditioned signSGD\n- Create a hybrid where workers communicate only signs, but the server maintains a diagonal AdaGrad/Adam-style accumulator to scale sign updates (i.e., \u201cpreconditioned sign steps\u201d). The key experiment is to compare communication\u2013accuracy Pareto curves against signSGD and full-precision Adam on sparse-gradient regimes (e.g., large vocab embeddings, MoE routing), testing whether adaptive geometry offsets sign quantization bias.",
        "Rare-Feature-Aware Weight Decay for Sparse Components in Transformers\n- Building on AdaGrad\u2019s motivation (\u201cneedles in haystacks\u201d) and AdamW\u2019s decoupled regularization, develop *parameter-group-specific weight decay* that depends on feature/activation frequency (e.g., per-embedding-row decay scaled by token frequency). Implement frequency-tracked decay multipliers and test whether it improves both generalization and retention of rare-token knowledge in compute-optimal LLM training.",
        "Stationary-Distribution Diagnostics for \u201cUndertrained vs Over-regularized\u201d LLMs\n- Use the approximate Bayesian inference view to separate two failure modes in LLM scaling: insufficient tokens (optimization not equilibrated) vs overly strong regularization (stationary distribution too concentrated). Implement online estimators of gradient noise covariance and parameter diffusion during training to detect whether training has reached a stable stationary regime, then tie this diagnostic to Chinchilla-style decisions about training longer vs changing weight decay.",
        "Compute-Optimal Data/Model/Optimizer Co-Scaling Laws\n- Chinchilla gives a compute-optimal \\(N\\)\u2013\\(D\\) tradeoff; extend the scaling law to include an \u201coptimizer efficiency\u201d factor that depends on adaptivity (SGD, AdaGrad, AdamW, signSGD+m) and its hyperparameters. Fit empirical laws \\(L(N,D,\\mathcal{O})\\) across optimizers and show how the optimal \\(N:D\\) ratio changes when communication constraints force compressed optimization.",
        "Polyak Averaging + AdamW: A Practical Averaged Stochastic Gradient Sampler for LLMs\n- Mandt et al. argue Polyak averaging is optimal and propose averaged samplers; build an \u201cAdamW-averaged sampler\u201d that periodically averages weights (or EMA) to approximate posterior samples while keeping decoupled weight decay. Evaluate whether ensembles drawn from checkpoints along the stationary trajectory improve calibration, uncertainty, and robustness of compute-optimal LLMs without extra training compute."
      ],
      "generated_ideas_raw": "1. **Compute-Optimal Optimizer Scheduling for LLM Pretraining**\n   - Extend Chinchilla-style compute-optimal scaling by jointly optimizing *optimizer hyperparameters* (Adam/AdamW \u03b2\u2019s, \u03b5, weight decay) and *training tokens/model size* under a fixed FLOPs budget. Concretely, run controlled sweeps where learning-rate/\u03b2 schedules are treated as decision variables in the compute-allocation problem \\( \\min_{N,D,\\text{sched}} L(N,D,\\text{sched}) \\) to see whether the \u201cscale tokens with parameters\u201d rule shifts when the optimizer dynamics change.\n\n2. **Adam as Approximate Bayesian Inference with Decoupled Weight Decay**\n   - Combine Mandt et al.\u2019s \u201cconstant SGD as a stationary distribution\u201d view with AdamW\u2019s decoupled weight decay to derive a stationary-distribution approximation for *adaptive-moment* methods. The actionable outcome is a principled procedure to set AdamW\u2019s learning rate and weight decay to match a target posterior covariance (e.g., diagonal Laplace/Fisher), then validate calibration and downstream uncertainty on large Transformer pretraining.\n\n3. **SignAdamW: 1-bit Communication with AdamW-Style Regularization**\n   - Design a distributed optimizer that transmits only gradient signs (as in signSGD) while maintaining AdamW-like second-moment tracking locally and applying decoupled weight decay globally. Implement majority-vote aggregation for signs, but update the preconditioner using a low-precision sketch (e.g., blockwise variance estimates) to preserve Adam-like speed; evaluate wall-clock convergence on multi-node LLM training.\n\n4. **Adaptive Geometry Meets Compression: Diagonal AdaGrad-Preconditioned signSGD**\n   - Create a hybrid where workers communicate only signs, but the server maintains a diagonal AdaGrad/Adam-style accumulator to scale sign updates (i.e., \u201cpreconditioned sign steps\u201d). The key experiment is to compare communication\u2013accuracy Pareto curves against signSGD and full-precision Adam on sparse-gradient regimes (e.g., large vocab embeddings, MoE routing), testing whether adaptive geometry offsets sign quantization bias.\n\n5. **Rare-Feature-Aware Weight Decay for Sparse Components in Transformers**\n   - Building on AdaGrad\u2019s motivation (\u201cneedles in haystacks\u201d) and AdamW\u2019s decoupled regularization, develop *parameter-group-specific weight decay* that depends on feature/activation frequency (e.g., per-embedding-row decay scaled by token frequency). Implement frequency-tracked decay multipliers and test whether it improves both generalization and retention of rare-token knowledge in compute-optimal LLM training.\n\n6. **Majority-Vote Aggregation Under Non-IID and Heterogeneous Curvature**\n   - signSGD theory highlights geometry (\\(\\ell_1/\\ell_2\\)) and majority vote variance reduction; extend this to realistic distributed LLM settings where workers see non-IID shards and experience different curvature/noise. The concrete contribution is a new aggregation rule (e.g., curvature-weighted or entropy-weighted voting) plus a diagnostic that predicts when majority vote fails, validated on heterogeneous data partitions.\n\n7. **Stationary-Distribution Diagnostics for \u201cUndertrained vs Over-regularized\u201d LLMs**\n   - Use the approximate Bayesian inference view to separate two failure modes in LLM scaling: insufficient tokens (optimization not equilibrated) vs overly strong regularization (stationary distribution too concentrated). Implement online estimators of gradient noise covariance and parameter diffusion during training to detect whether training has reached a stable stationary regime, then tie this diagnostic to Chinchilla-style decisions about training longer vs changing weight decay.\n\n8. **Compute-Optimal Data/Model/Optimizer Co-Scaling Laws**\n   - Chinchilla gives a compute-optimal \\(N\\)\u2013\\(D\\) tradeoff; extend the scaling law to include an \u201coptimizer efficiency\u201d factor that depends on adaptivity (SGD, AdaGrad, AdamW, signSGD+m) and its hyperparameters. Fit empirical laws \\(L(N,D,\\mathcal{O})\\) across optimizers and show how the optimal \\(N:D\\) ratio changes when communication constraints force compressed optimization.\n\n9. **Bias-Corrected Moment Estimation for Non-Stationary Objectives in Long Pretraining**\n   - Adam\u2019s bias correction is derived for certain moment dynamics; develop a variant explicitly targeting *distribution shift over tokens* (curriculum, changing data mixtures) by tracking moments with change-point-aware decay rates. Implement an adaptive \u03b22 schedule driven by detected gradient-statistics drift, and test stability/perplexity improvements during long-horizon LLM training with changing data compositions.\n\n10. **Polyak Averaging + AdamW: A Practical Averaged Stochastic Gradient Sampler for LLMs**\n   - Mandt et al. argue Polyak averaging is optimal and propose averaged samplers; build an \u201cAdamW-averaged sampler\u201d that periodically averages weights (or EMA) to approximate posterior samples while keeping decoupled weight decay. Evaluate whether ensembles drawn from checkpoints along the stationary trajectory improve calibration, uncertainty, and robustness of compute-optimal LLMs without extra training compute.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Compute-Optimal Optimizer Scheduling for LLM Pretraining\n- Extend Chinchilla-style compute-optimal scaling by jointly optimizing *optimizer hyperparameters* (Adam/AdamW \u03b2\u2019s, \u03b5, weight decay) and *trai",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Adam as Approximate Bayesian Inference with Decoupled Weight Decay\n- Combine Mandt et al.\u2019s \u201cconstant SGD as a stationary distribution\u201d view with AdamW\u2019s decoupled weight decay to derive a stationary-",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "SignAdamW: 1-bit Communication with AdamW-Style Regularization\n- Design a distributed optimizer that transmits only gradient signs (as in signSGD) while maintaining AdamW-like second-moment tracking l",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Adaptive Geometry Meets Compression: Diagonal AdaGrad-Preconditioned signSGD\n- Create a hybrid where workers communicate only signs, but the server maintains a diagonal AdaGrad/Adam-style accumulator ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Rare-Feature-Aware Weight Decay for Sparse Components in Transformers\n- Building on AdaGrad\u2019s motivation (\u201cneedles in haystacks\u201d) and AdamW\u2019s decoupled regularization, develop *parameter-group-specifi",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Stationary-Distribution Diagnostics for \u201cUndertrained vs Over-regularized\u201d LLMs\n- Use the approximate Bayesian inference view to separate two failure modes in LLM scaling: insufficient tokens (optimiz",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Compute-Optimal Data/Model/Optimizer Co-Scaling Laws\n- Chinchilla gives a compute-optimal \\(N\\)\u2013\\(D\\) tradeoff; extend the scaling law to include an \u201coptimizer efficiency\u201d factor that depends on adapt",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Polyak Averaging + AdamW: A Practical Averaged Stochastic Gradient Sampler for LLMs\n- Mandt et al. argue Polyak averaging is optimal and propose averaged samplers; build an \u201cAdamW-averaged sampler\u201d th",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 14,
      "paper_title": "An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds",
      "contribution": "They refine the Franz\u2013Parisi (FP) geometric criterion to better capture overlap structure and prove that this optimized FP is equivalent to Statistical Query (SQ) lower bounds under a mild, verifiable assumption, thereby unifying physics-inspired geometry with SQ complexity for a broad class of statistical models.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12628,
      "output_tokens": 1113,
      "predecessor_details": [
        {
          "success": true,
          "title": "The Franz-Parisi Criterion and Computational Trade-offs in ... - arXiv",
          "url": "https://arxiv.org/abs/2205.09727",
          "content": "# Mathematics > Statistics Theory\n\n**arXiv:2205.09727** (math)\n\n\\[Submitted on 19 May 2022 ( [v1](https://arxiv.org/abs/2205.09727v1)), last revised 13 Oct 2022 (this version, v2)\\]\n\n# Title:The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics\n\nAuthors: [Afonso S. Bandeira](https://arxiv.org/search/math?searchtype=author&query=Bandeira,+A+S), [Ahmed El Alaoui](https://arxiv.org/search/math?searchtype=author&query=Alaoui,+A+E), [Samuel B. Hopkins](https://arxiv.org/search/math?searchtype=author&query=Hopkins,+S+B), [Tselil Schramm](https://arxiv.org/search/math?searchtype=author&query=Schramm,+T), [Alexander S. Wein](https://arxiv.org/search/math?searchtype=author&query=Wein,+A+S), [Ilias Zadik](https://arxiv.org/search/math?searchtype=author&query=Zadik,+I)\n\nView a PDF of the paper titled The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics, by Afonso S. Bandeira and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2205.09727)\n\n> Abstract:Many high-dimensional statistical inference problems are believed to possess inherent computational hardness. Various frameworks have been proposed to give rigorous evidence for such hardness, including lower bounds against restricted models of computation (such as low-degree functions), as well as methods rooted in statistical physics that are based on free energy landscapes. This paper aims to make a rigorous connection between the seemingly different low-degree and free-energy based approaches. We define a free-energy based criterion for hardness and formally connect it to the well-established notion of low-degree hardness for a broad class of statistical problems, namely all Gaussian additive models and certain models with a sparse planted signal. By leveraging these rigorous connections we are able to: establish that for Gaussian additive models the \"algebraic\" notion of low-degree hardness implies failure of \"geometric\" local MCMC algorithms, and provide new low-degree lower bounds for sparse linear regression which seem difficult to prove directly. These results provide both conceptual insights into the connections between different notions of hardness, as well as concrete technical tools such as new methods for proving low-degree lower bounds.\n\n| | |\n| --- | --- |\n| Comments: | 52 pages, 1 figure |\n| Subjects: | Statistics Theory (math.ST); Statistical Mechanics (cond-mat.stat-mech); Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2205.09727](https://arxiv.org/abs/2205.09727) \\[math.ST\\] |\n| | (or [arXiv:2205.09727v2](https://arxiv.org/abs/2205.09727v2) \\[math.ST\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2205.09727](https://doi.org/10.48550/arXiv.2205.09727) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Alexander Wein \\[ [view email](https://arxiv.org/show-email/7ea7824d/2205.09727)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2205.09727v1)**\nThu, 19 May 2022 17:39:29 UTC (80 KB)\n\n**\\[v2\\]**\nThu, 13 Oct 2022 05:17:58 UTC (81 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics, by Afonso S. Bandeira and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2205.09727)\n- [TeX Source](https://arxiv.org/src/2205.09727)\n- [Other Formats](https://arxiv.org/format/2205.09727)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.ST\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2205.09727&function=prev&context=math.ST)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2205.09727&function=next&context=math.ST)\n\n[new](https://arxiv.org/list/math.ST/new) \\| [recent](https://arxiv.org/list/math.ST/recent) \\| [2022-05](https://arxiv.org/list/math.ST/2022-05)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/2205.09727?context=cond-mat)\n\n[cond-mat.stat-mech](https://arxiv.org/abs/2205.09727?context=cond-mat.stat-mech)\n\n[cs](https://arxiv.org/abs/2205.09727?context=cs)\n\n[cs.CC](https://arxiv.org/abs/2205.09727?context=cs.CC)\n\n[cs.DS](https://arxiv.org/abs/2205.09727?context=cs.DS)\n\n[math](https://arxiv.org/abs/2205.09727?context=math)\n\n[stat](https://arxiv.org/abs/2205.09727?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2205.09727?context=stat.ML)\n\n[stat.TH](https://arxiv.org/abs/2205.09727?context=stat.TH)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2205.09727)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2205.09727)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2205.09727)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2205.09727&description=The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2205.09727&title=The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2205.09727) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "The franz-parisi criterion and computational trade-offs in high dimensional statistics",
          "cleaned_query": "The franz-parisi criterion and computational trade-offs in high dimensional statistics",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Recipes for Metastable States in Spin Glasses - ADS",
          "url": "https://ui.adsabs.harvard.edu/abs/1995JPhy1...5.1401F/abstract",
          "content": "Recipes for Metastable States in Spin Glasses - ADS\nNow on home page\n## ADS\n## Recipes for Metastable States in Spin Glasses[]()\n* [Franz, Silvio](https://ui.adsabs.harvard.edu/search/?q=author:%22Franz,+Silvio%22);\n* [Parisi, Giorgio](https://ui.adsabs.harvard.edu/search/?q=author:%22Parisi,+Giorgio%22)\n#### Abstract\nIn this paper we develop further a method recently introduced by one of us to study metastable states in spin glasses. We consider a `potentional function', defined as the free energy of a system at a given temperature T constrained to have a fixed overlap with a reference configuartion of equilibrium at temperature T'. We apply the method to the spherical p-spin glass, and to some generalization, of this model in the range of temperatures between the dynamic and the static transition. The analysis suggests a correspondence among local minima of the potential and metastable states. This correspondence is confirmed studying the relaxation dynamics at temperature T of a system starting from an initial configuration equilibrated at a different\nPublication:\nJournal de Physique I\nPub Date:November 1995DOI:\n[10.1051/jp1:1995201](https://ui.adsabs.harvard.edu/link_gateway/1995JPhy1...5.1401F/doi:10.1051/jp1:1995201)**\n[10.48550/arXiv.cond-mat/9503167](https://ui.adsabs.harvard.edu/link_gateway/1995JPhy1...5.1401F/doi:10.48550/arXiv.cond-mat/9503167)**\narXiv:[arXiv:cond-mat/9503167](https://ui.adsabs.harvard.edu/link_gateway/1995JPhy1...5.1401F/arXiv:cond-mat/9503167)**Bibcode:[1995JPhy1...5.1401F](https://ui.adsabs.harvard.edu/abs/1995JPhy1...5.1401F/abstract)**Keywords:\n* Condensed MatterE-Print:21 pages, latex, 4 uuencoded figures.\n**\nfull text sources\nPublisher\n|\n[**](https://ui.adsabs.harvard.edu/link_gateway/1995JPhy1...5.1401F/PUB_HTML)\nPreprint\n[**](https://ui.adsabs.harvard.edu/link_gateway/1995JPhy1...5.1401F/EPRINT_PDF)\n|\n[**](https://ui.adsabs.harvard.edu/link_gateway/1995JPhy1...5.1401F/EPRINT_HTML)\n",
          "original_query": "Recipes for metastable states in spin glasses",
          "cleaned_query": "Recipes for metastable states in spin glasses",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Predictions using the Low-Degree Likelihood Ratio - arXiv",
          "url": "https://arxiv.org/abs/1907.11636",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: These notes survey and explore an emerging method, which we call the\nlow-degree method, for predicting and understanding\nstatistical-versus-computational tradeoffs in high-dimensional inference\nproblems. In short, the method posits that a certain quantity -- the second\nmoment of the low-degree likelihood ratio -- gives insight into how much\ncomputational time is required to solve a given hypothesis testing problem,\nwhich can in turn be used to predict the computational hardness of a variety of\nstatistical inference tasks. While this method originated in the study of the\nsum-of-squares (SoS) hierarchy of convex programs, we present a self-contained\nintroduction that does not require knowledge of SoS. In addition to showing how\nto carry out predictions using the method, we include a discussion\ninvestigating both rigorous and conjectural consequences of these predictions.\n These notes include some new results, simplified proofs, and refined\nconjectures. For instance, we point out a formal connection between spectral\nmethods and the low-degree likelihood ratio, and we give a sharp low-degree\nlower bound against subexponential-time algorithms for tensor PCA.\n \n \n \n \n Submission history From: Alexander Wein [ view email]\n [v1] \nFri, 26 Jul 2019 15:46:05 UTC (45 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > math > arXiv:1907.11636\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Mathematics > Statistics Theory\n\n arXiv:1907.11636 (math)\n [Submitted on 26 Jul 2019]\n\n Title: Notes on Computational Hardness of Hypothesis Testing: Predictions using the Low-Degree Likelihood Ratio\n\n Authors: Dmitriy Kunisky, Alexander S. Wein, Afonso S. Bandeira\n Download PDF\n Abstract: These notes survey and explore an emerging method, which we call the low-degree method, for predicting and understanding statistical-versus-computational tradeoffs in high-dimensional inference problems. In short, the method posits that a certain quantity -- the second moment of the low-degree likelihood ratio -- gives insight into how much computational time is required to solve a given hypothesis testing problem, which can in turn be used to predict the computational hardness of a variety of statistical inference tasks. While this method originated in the study of the sum-of-squares (SoS) hierarchy of convex programs, we present a self-contained introduction that does not require knowledge of SoS. In addition to showing how to carry out predictions using the method, we include a discussion investigating both rigorous and conjectural consequences of these predictions.\n These notes include some new results, simplified proofs, and refined conjectures. For instance, we point out a formal connection between spectral methods and the low-degree likelihood ratio, and we give a sharp low-degree lower bound against subexponential-time algorithms for tensor PCA.\n Comments: 44 pages \n Subjects: Statistics Theory (math.ST) ; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)\n Cite as: arXiv:1907.11636 [math.ST] \n (or arXiv:1907.11636v1 [math.ST] for this version) \n https://doi.org/10.48550/arXiv.1907.11636 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Alexander Wein [view email]\n [v1] Fri, 26 Jul 2019 15:46:05 UTC (45 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * PostScript\n * Other formats\n (license)\n Current browse context:\n math.ST\n < prev | next >\n new | recent | 1907\n Change to browse by:\n cs\n cs.CC\n cs.DS\n math\n stat\n stat.ML\n stat.TH\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio",
          "cleaned_query": "Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Statistical Algorithms and a Lower Bound for Detecting Planted Clique",
          "url": "https://arxiv.org/abs/1201.1214",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: We introduce a framework for proving lower bounds on computational problems\nover distributions against algorithms that can be implemented using access to a\nstatistical query oracle. For such algorithms, access to the input distribution\nis limited to obtaining an estimate of the expectation of any given function on\na sample drawn randomly from the input distribution, rather than directly\naccessing samples. Most natural algorithms of interest in theory and in\npractice, e.g., moments-based methods, local search, standard iterative methods\nfor convex optimization, MCMC and simulated annealing can be implemented in\nthis framework. Our framework is based on, and generalizes, the statistical\nquery model in learning theory (Kearns, 1998).\n Our main application is a nearly optimal lower bound on the complexity of any\nstatistical query algorithm for detecting planted bipartite clique\ndistributions (or planted dense subgraph distributions) when the planted clique\nhas size $O(n^{1/2-\\delta})$ for any constant $\\delta &gt; 0$. The assumed\nhardness of variants of these problems has been used to prove hardness of\nseveral other problems and as a guarantee for security in cryptographic\napplications. Our lower bounds provide concrete evidence of hardness, thus\nsupporting these assumptions.\n \n \n \n \n Submission history From: Vitaly Feldman [ view email]\n \n [v1] \n Thu, 5 Jan 2012 16:39:21 UTC (23 KB) \n [v2] \n Wed, 9 May 2012 19:34:30 UTC (35 KB) \n [v3] \n Fri, 22 Mar 2013 03:54:58 UTC (53 KB) \n [v4] \n Wed, 3 Apr 2013 15:08:58 UTC (53 KB) \n [v5] \n Mon, 8 Jun 2015 17:38:56 UTC (47 KB) [v6] \nMon, 15 Aug 2016 01:17:38 UTC (48 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > cs > arXiv:1201.1214\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Computational Complexity\n\n arXiv:1201.1214 (cs)\n [Submitted on 5 Jan 2012 (v1), last revised 15 Aug 2016 (this version, v6)]\n\n Title: Statistical Algorithms and a Lower Bound for Detecting Planted Clique\n\n Authors: Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, Ying Xiao\n Download PDF\n Abstract: We introduce a framework for proving lower bounds on computational problems over distributions against algorithms that can be implemented using access to a statistical query oracle. For such algorithms, access to the input distribution is limited to obtaining an estimate of the expectation of any given function on a sample drawn randomly from the input distribution, rather than directly accessing samples. Most natural algorithms of interest in theory and in practice, e.g., moments-based methods, local search, standard iterative methods for convex optimization, MCMC and simulated annealing can be implemented in this framework. Our framework is based on, and generalizes, the statistical query model in learning theory (Kearns, 1998).\n Our main application is a nearly optimal lower bound on the complexity of any statistical query algorithm for detecting planted bipartite clique distributions (or planted dense subgraph distributions) when the planted clique has size $O(n^{1/2-\\delta})$ for any constant $\\delta > 0$. The assumed hardness of variants of these problems has been used to prove hardness of several other problems and as a guarantee for security in cryptographic applications. Our lower bounds provide concrete evidence of hardness, thus supporting these assumptions.\n Subjects: Computational Complexity (cs.CC) ; Data Structures and Algorithms (cs.DS)\n ACM classes: F.2; G.1.6; G.3 \n Cite as: arXiv:1201.1214 [cs.CC] \n (or arXiv:1201.1214v6 [cs.CC] for this version) \n https://doi.org/10.48550/arXiv.1201.1214 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Vitaly Feldman [view email]\n [v1] Thu, 5 Jan 2012 16:39:21 UTC (23 KB)\n [v2] Wed, 9 May 2012 19:34:30 UTC (35 KB)\n [v3] Fri, 22 Mar 2013 03:54:58 UTC (53 KB)\n [v4] Wed, 3 Apr 2013 15:08:58 UTC (53 KB)\n [v5] Mon, 8 Jun 2015 17:38:56 UTC (47 KB)\n [v6] Mon, 15 Aug 2016 01:17:38 UTC (48 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * PostScript\n * Other formats\n (license)\n Current browse context:\n cs.CC\n < prev | next >\n new | recent | 1201\n Change to browse by:\n cs\n cs.DS\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n\n DBLP - CS Bibliography\n\n listing | bibtex\n Vitaly Feldman\n Elena Grigorescu\n Lev Reyzin\n Santosh Vempala\n Santosh S. Vempala\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Statistical algorithms and a lower bound for detecting planted cliques",
          "cleaned_query": "Statistical algorithms and a lower bound for detecting planted cliques",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Statistical Query Algorithms and Low-Degree Tests Are ... - arXiv",
          "url": "https://arxiv.org/abs/2009.06107",
          "content": "# Computer Science > Computational Complexity\n\n**arXiv:2009.06107** (cs)\n\n\\[Submitted on 13 Sep 2020 ( [v1](https://arxiv.org/abs/2009.06107v1)), last revised 26 Jun 2021 (this version, v3)\\]\n\n# Title:Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent\n\nAuthors: [Matthew Brennan](https://arxiv.org/search/cs?searchtype=author&query=Brennan,+M), [Guy Bresler](https://arxiv.org/search/cs?searchtype=author&query=Bresler,+G), [Samuel B. Hopkins](https://arxiv.org/search/cs?searchtype=author&query=Hopkins,+S+B), [Jerry Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+J), [Tselil Schramm](https://arxiv.org/search/cs?searchtype=author&query=Schramm,+T)\n\nView a PDF of the paper titled Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent, by Matthew Brennan and Guy Bresler and Samuel B. Hopkins and Jerry Li and Tselil Schramm\n\n[View PDF](https://arxiv.org/pdf/2009.06107)\n\n> Abstract:Researchers currently use a number of approaches to predict and substantiate information-computation gaps in high-dimensional statistical estimation problems. A prominent approach is to characterize the limits of restricted models of computation, which on the one hand yields strong computational lower bounds for powerful classes of algorithms and on the other hand helps guide the development of efficient algorithms. In this paper, we study two of the most popular restricted computational models, the statistical query framework and low-degree polynomials, in the context of high-dimensional hypothesis testing. Our main result is that under mild conditions on the testing problem, the two classes of algorithms are essentially equivalent in power. As corollaries, we obtain new statistical query lower bounds for sparse PCA, tensor PCA and several variants of the planted clique problem.\n\n| | |\n| --- | --- |\n| Comments: | Version 3 fixes typos and adds note on presentation at COLT 2021 |\n| Subjects: | Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2009.06107](https://arxiv.org/abs/2009.06107) \\[cs.CC\\] |\n| | (or [arXiv:2009.06107v3](https://arxiv.org/abs/2009.06107v3) \\[cs.CC\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2009.06107](https://doi.org/10.48550/arXiv.2009.06107) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Samuel Hopkins \\[ [view email](https://arxiv.org/show-email/d6d24264/2009.06107)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2009.06107v1)**\nSun, 13 Sep 2020 22:55:18 UTC (130 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2009.06107v2)**\nWed, 11 Nov 2020 05:28:15 UTC (130 KB)\n\n**\\[v3\\]**\nSat, 26 Jun 2021 17:06:23 UTC (125 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent, by Matthew Brennan and Guy Bresler and Samuel B. Hopkins and Jerry Li and Tselil Schramm\n\n- [View PDF](https://arxiv.org/pdf/2009.06107)\n- [TeX Source](https://arxiv.org/src/2009.06107)\n- [Other Formats](https://arxiv.org/format/2009.06107)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2009.06107&function=prev&context=cs.CC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2009.06107&function=next&context=cs.CC)\n\n[new](https://arxiv.org/list/cs.CC/new) \\| [recent](https://arxiv.org/list/cs.CC/recent) \\| [2020-09](https://arxiv.org/list/cs.CC/2020-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2009.06107?context=cs)\n\n[cs.DS](https://arxiv.org/abs/2009.06107?context=cs.DS)\n\n[cs.LG](https://arxiv.org/abs/2009.06107?context=cs.LG)\n\n[math](https://arxiv.org/abs/2009.06107?context=math)\n\n[math.ST](https://arxiv.org/abs/2009.06107?context=math.ST)\n\n[stat](https://arxiv.org/abs/2009.06107?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2009.06107?context=stat.ML)\n\n[stat.TH](https://arxiv.org/abs/2009.06107?context=stat.TH)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2009.06107)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2009.06107)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2009.06107)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2009.html#abs-2009-06107) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2009-06107)\n\n[Matthew Brennan](https://dblp.uni-trier.de/search/author?author=Matthew%20Brennan)\n\n[Guy Bresler](https://dblp.uni-trier.de/search/author?author=Guy%20Bresler)\n\n[Samuel B. Hopkins](https://dblp.uni-trier.de/search/author?author=Samuel%20B.%20Hopkins)\n\n[Jerry Li](https://dblp.uni-trier.de/search/author?author=Jerry%20Li)\n\n[Tselil Schramm](https://dblp.uni-trier.de/search/author?author=Tselil%20Schramm)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2009.06107&description=Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2009.06107&title=Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2009.06107) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Statistical query algorithms and low degree tests are almost equivalent",
          "cleaned_query": "Statistical query algorithms and low degree tests are almost equivalent",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] A simple proof of the Gaussian correlation conjecture ... - arXiv",
          "url": "https://arxiv.org/vc/arxiv/papers/1408/1408.1028v1.pdf",
          "content": "-1-\nA simple proof of the Gaussian correlation conjecture\nextended to multivariate gamma distributions\nT. Royen\nUniversity of applied sciences Bingen, Berlinstrasse 109, D-55411 Bingen, Germany,\ne-mail: thomas.royen@t-online.de\nAbstract\nAn extension of the Gaussian correlation conjecture (GCC) is proved for multivariate gamma distributions (in the \nsense of Krishnamoorthy and Parthasarathy). The classical GCC for Gaussian probability measures is obtained by \nthe special case with an integer degree of freedom \n\uf06e \uf03d1.\nKeywords and phrases: probability inequalities, Gaussian correlation conjecture, multivariate gamma distribution\n2010 Mathematics Subject Classification: 60E15\n1. Introduction\nLet \nP\nbe a probability measure on \n, 1, n\uf020 \uf03e n\ngiven by a Gaussian density \n/2 1/2 1 1\n2\n(2 ) | | exp( ) n T\uf070\n\uf02d \uf02d \uf02d \u03a3 x \u03a3 x \uf02d with \na non-singular covariance matrix \n\u03a3\n. The Gaussian correlation conjecture (GCC) asserts the inequality\n \n1 2 1 P C C P C P C ( ) ( ) ( ) \uf0b3 (1.1)\nfor all convex and central-symmetric sets \n1 2 ,\nn C C\uf020 \uf0cd\n, (see [2]). The bivariate case was proved in [8]. Further \nmilestones towards a complete proof were the papers [12] and [4]. In [12] the GCC is verified for all centered ellip\u0002soids and for all sufficiently small \n1 2 C C, .\nIn [4] a proof is given if only \nC1\nis a centered ellipsoid. Recently, a com\u0002plete very long proof has been put into the arXiv.org server by Y. Memarian [7], which seems to be still under \nexamination. The here presented proof is totally different and comparatively short. According to [12] the GCC is \nequivalent to\n \n1 1 1\nn k n\ni i i\ni i i k\nP P P A A A\n\uf03d \uf03d \uf03d \uf02b\n\uf0e6 \uf0f6 \uf0e6 \uf0f6 \uf0e6 \uf0f6\n\uf0b3 \uf0e7 \uf0f7 \uf0e7 \uf0f7 \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf0e8 \uf0f8 \uf0e8 \uf0f8\n (1.2)\nwith \n\uf07b \uf07d 1\n| | , ,..., 0, 1 , A X x x x k n i i i n \uf03d \uf020 \uf020\uf020\uf0a3\uf020\uf020 \uf020\uf020 \uf03e \uf020\uf020 \uf0a3 \uf03c\nand any \n( , )\nn N 0 \u03a3 -Gaussian vector \n1\n( ,..., ). X X\nn\nThe inequality\n(1.2) was independently proved for \nk \uf03d1\nin [5] and [13]. Here, (1.2) is proved for \n1\n( ,..., ) X X\nn\nwith an n-variate \ngamma distribution (\n( , ) \uf047\nn \uf061 R -distribution in the sense of Krishnamoorthy and Parthasarathy [6]), defined by its La\u0002place transform (Lt)\n | |\nn\nI RT \uf02d\uf061\uf02b (1.3)\nwith the identity matrix \nn\nI\n, a non-singular correlation matrix \n( ) R \uf03d \uf072ij\nand \n1 1 ( ,..., ), ,..., 0. T Diag t t t t \uf03d \uf020\uf020 \uf0b3n n\nAd\u0002missible values for \n\uf061\nare \n2 , \uf061 \uf0ce\nall values \n2 2, 2, \uf061 \uf03e \uf02d \uf020 \uf0b3 n n\n(for \nn n \uf02d \uf03c \uf0a3 \uf02d 2 2 1 \uf061\nsee [10]) and all \n\uf061 \uf03e 0\nif \nthe Lt is infinitely divisible, for which sufficient and necessary conditions are found in [1] and [3]. The \n( , ) \uf047\nn \uf061 R -\ndistribution was originally derived from the joint distribution of the diagonal elements of a \n(2 , ) W R n \uf061 -Wishart matrix \nwith the Lt \n| 2 |\nn\nI RT \uf02d\uf061\uf02b\n. The classical GCC is obtained from the special case \n\uf061 \uf03d1/ 2.\nA proof of it could be \neasily extended inductively for increasing degrees of freedom \n\uf06e \uf061 \uf03d 2\nby means of convolution integrals. However, \nthe here provided proof includes also non-integer values of \n2 . \uf061\n We need the non-central gamma-pdf\n-2-\n \n0\n( , ) ( ) , , 0, 0,\n!\nk\ny\nk\nk\ny\ng x y e g x x y\nk\n\uf061 \uf061 \uf061\n\uf02d\n\uf02b\n\uf03d\n\uf0a5\n\uf03d \uf020\uf020 \uf03e \uf020 \uf0b3 \uf0e5 (1.4) \nwith the central gamma-pdf\n1 1 ( ) ( ( )) x\ng x x e\n\uf061\n\uf061 \uf061\n\uf02d \uf02d \uf02d \uf03d \uf047\nand the corresponding non-central gamma-cdf\n \n0 0\n( , ) ( ) , ( ) ( ) .\n!\nk x\ny\nk\nk\ny\nG x y e G x G x g d\nk\n\uf061 \uf061 \uf061 \uf061 \uf078 \uf078 \uf02d\uf02b\n\uf03d\n\uf0a5\n\uf03d \uf020\uf020\uf020 \uf03d \uf0e5 \uf0f2\n (1.5)\nIf \nR\nis represented by\n \n1 1/2\n, ,\nT T R I AA R I BB A \uf06c n n \uf06c \uf06c B\uf02d \uf02d \uf03d \uf02b \uf03d \uf02b \uf020\uf0db\uf020 \uf020\uf020\uf020 \uf03d (1.6)\nwith the minimal eigenvalue \n\uf06c\nof \nR\nand an \nn n \uf0b4 \uf02d ( 1)-matrix A of rank m n \uf0a3 \uf02d1,\nthen the \n( , ) \uf047\nn \uf061 R - pdf and the \n( , ) \uf047\nn \uf061 R - cdf, \n2\uf061 \uf0ce\nor \n2 2, \uf061 \uf03e \uf02dn\ncan be represented by\n \n1 1\n1\n1\n1\n2\n( ,..., ; , ) ( , T\nn\nn j j j\nj\nf x x R g x b Sb \uf061 \uf06c \uf06c E \uf061\n\uf02d \uf02d\n\uf03d\n\uf0e6 \uf0f6\n\uf03d \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf0d5 (1.7)\nand\n \n1\n1\n1\n1\n2\n( ,..., ; , ) ( , T\nn\nn j j j\nj\nF x x R G x b Sb \uf061 \uf06c E \uf061\n\uf02d\n\uf03d\n\uf0e6 \uf0f6\n\uf03d \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf0d5 (1.8)\nrespectively with the rows \nj b\nfrom \nB\nand the expectation referring to the \n1 1 (2 , I ) Wn n \uf02d \uf02d \uf061 - Wishart matrix \nS.\nThese \nformulas are derived in a slightly more general form in [9] and [10]. They can be verified by Lt.\n2. Proof of the Gaussian correlation conjecture for \n( , ) \uf047n \uf061 R - distributions\nTheorem 1. Let \n11 12\n21 22\nR R\nR R\nR\n\uf0e6 \uf0f6\n\uf03d \uf0e7 \uf0f7 \uf0e8 \uf0f8\nbe a non-singular partitioned \nn n \uf0b4 -correlation matrix with \ni i n n \uf0b4 -submatrices \nRii\nand a positive rank of \n12 R .\nThen, it holds for the \n( , ) \uf047\nn \uf061 R - cdf\n \n1 1 1 1 11 1 22 ( ,..., ; , ) ( ,..., ; , ) ( ,..., ; , ) F x x R F x x R F x x R n n \uf061 \uf061 \uf061 \uf03e n n \uf02b\n (2.1)\nfor all positive numbers \n1\n,...,\nn\nx x\nand \n2\uf061 \uf0ce\nor \n2 2. \uf061 \uf03e \uf02dn\nProof. All the matrices \n \n11 12\n21 22\n, 0 1,\nR R\nR R\nR\uf074\n\uf074\n\uf074\n\uf074\n\uf020\uf020 \uf0a3 \uf0a3\n\uf0e6 \uf0f6\n\uf03d \uf0e7 \uf0f7 \uf0e8 \uf0f8\n (2.2)\nare non-singular correlation matrices. Theorem 1 will be proved if we show\n \n1\n( ,..., ; , ) 0, 0 1. F x x R n \uf061 \uf074 \uf074\n\uf074\n\uf0b6\n\uf03e \uf020\uf020\uf020 \uf03c \uf03c\n\uf0b6\n (2.3)\nBy means of the Lt we shall represent the lhs of (2.3) by a sum of not identically vanishing non-negative terms.\n Let \nAJ\ndenote the submatrices with row and column indices \ni J n \uf0ce \uf0cd\uf07b1,..., \uf07d\nfrom any \nn n \uf0b4 - matrix A. The deter\u0002minant \n| |\nn\nI R T \uf02b \uf074\nis equal to \n1 | || |, 1,..., .\n,J J \uf07b \uf07d J\nR T J n \uf02b \uf020\uf020\uf0c6 \uf0b9 \uf0cd \uf0e5 \uf074\n With \nJ J J J J n \uf03d \uf020\uf020 \uf03d \uf0b9 \uf0c6 1 2 1 1 , 1,..., , \uf07b \uf07d\nJ J n n 2 1 \uf03d \uf02b \uf0b9 \uf0c6 \uf07b 1,..., , \uf07d\n1 1 2\n1 2 1 2\n2 1 2\n,\n, , ,\n,\n, ( ) J J J\nJ J J J J\nJ J J\nR R\nR r rank R\nR R \uf074\n\uf074\n\uf074\n\uf0e6 \uf0f6\n\uf03d \uf020\uf020 \uf03d \uf0e7 \uf0f7 \uf0e7 \uf0f7 \uf0e8 \uf0f8\nand the canonical correlations\n1 2\n\uf072J J i , ,\n-3-\nbelonging to \n1 2 1 2 1 2 , ,\n, , , ,..., , 1 R R R i r J J J J J J \uf020 \uf020 \uf020\uf020 \uf03d\nwe find\n \uf028 \uf029\n1 2\n1 2\n, 1 2\n1 2 1 2\n2\n, ,\n2 2\n,\n, 1 2\n2 2\n, , , ,\n1 1 ,\n1\n| | || | 1 2 | |\nJ J\nJ J\nJ J\nJ J r\ni\nJ J J J J i J\ni\nr\ni i\nR R R R \uf074 \uf074\n\uf072\n\uf074 \uf072\n\uf074 \uf072 \uf074\n\uf074 \uf074 \uf03d \uf03d \uf02d\n\uf0b6 \uf0b6 \uf020\uf020\uf03d\uf020\uf020\uf07c \uf02d \uf020\uf020\uf03d\uf020\uf020\uf02d\n\uf0b6 \uf0b6 \uf0d5 \uf0e5\nand\n \n1 2\n1 2\n, 1 2\n2\n, ,\n2 2\n,\n,\n1 ,\n1 1\n1\n| | ( )\n| | ,\n| | | |\n2\nJ J\nJ J\nJ J\nr\ni\nJ j J j\nJ i i j J J j J\nn\nn n\nR t c t\nI R T\nI R T I R T\n\uf061\n\uf061 \uf061\n\uf074\n\uf074\n\uf074 \uf074\n\uf072\n\uf074 \uf072\n\uf074\n\uf074\n\uf061\uf074 \uf02d\n\uf03d \uf0ce \uf0ce\n\uf02b \uf02b\n\uf02d\n\uf0e6 \uf0f6 \uf0e7 \uf0f7 \uf0e7 \uf0f7 \uf0b6 \uf0e8 \uf0f8 \uf02b \uf020\uf03d\uf020 \uf020\uf03d\uf020\n\uf0b6 \uf02b \uf02b\n\uf0e5 \uf0e5 \uf0d5 \uf0e5 \uf0d5\n (2.4)\nwhich is the Lt of \n*\n1\n( ,..., ; , )\nn h t t R \uf061 \uf074\nof\n \n1 1 (x ,..., x ; , ) ( ) ( ,..., ; 1, ).\nj\nn J n\nJ j J x\nh R c f x x R \uf061 \uf074 \uf061 \uf074 \uf074\n\uf0ce\n\uf0b6\n\uf0b6\n\uf0e6 \uf0f6\n\uf03d \uf02b \uf0e7 \uf0f7 \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf0e5 \uf0d5 (2.5)\n We have to verify that\n \n1 1 (x ,..., x ; , ) ( ,..., ; , ).n n\nh R f x x R \uf061 \uf061 \uf074 \uf074\n\uf074\n\uf0b6\n\uf03d\n\uf0b6\n (2.6)\nIntegration of the lhs in (2.4) over \n[ , ] 0 \uf074\nprovides\n \n0 1 1 0 \uf028 \uf029\n1\n| | | | ( ,..., ; , ) ( ,..., ; , ) j j\nn\nn\nn n n n j\nj\nt x I R T I R T f x x R f x x R e dx \uf061 \uf061\n\uf074 \uf074 \uf061 \uf061\n\uf02b\n\uf02d \uf02d \uf02d\n\uf03d\n\uf02b \uf02d \uf02b \uf03d \uf02d \uf03d \uf0f2 \uf0d5\n \n*\n1 1\n0 0 1\n( ,..., ; , ) (x ,..., x ; , ) d , j j\nn\nn\nn n j\nj\nt x h t t R d h R e dx\n\uf074 \uf074\n\uf061 \uf04a \uf061 \uf04a \uf04a \uf04a\n\uf02b\n\uf02d\n\uf03d\n\uf0e6 \uf0f6\n\uf020 \uf03d \uf020 \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf0f2 \uf0f2 \uf0f2 \uf0d5 (2.7)\nwhere the change of the order of integration can be justified by Fubini\u2019s criterion in the following way:\n With (1.7), \n1 1 g g g (x, y) (x, y) (x, y),\nx\n\uf061 \uf061 \uf061 \uf02b \uf02b\n\uf0b6\n\uf03d \uf02d\n\uf0b6\ndecompositions \n1\n( ( )) ( )( ( ))T\uf06c \uf04a \uf04a \uf04a R I B B \uf04a n\n\uf02d\n\uf03d \uf02b - similar as in \n(1.6) - with rows \n( ) j b \uf04a\nin \nB( ) \uf04a\nand indicator functions \nJ\ne\nof \nJ\nwe obtain\n \n1 1 | (x ,..., x ; , ) | ( ) ( ,..., ; 1, ) | |( )\nj\nn J n\nJ j J x\nh R c f x x R \uf061 \uf074 \uf061 \uf04a \uf04a\n\uf0ce\n\uf0b6\n\uf0b6\n\uf020\uf020\uf0a3\uf020\uf020 \uf02b \uf020\uf020\uf0a3 \uf0e5 \uf0d5 \n\uf028 \uf029\n1\n| | 1\n1\n( ) ( )\n1 1\n2 2\n( ) ( ( )) ( ) , ( ) ( ) , ( ) ( ) ,\n( ( ))\n( ) ( ) T T\nn\nJ\nJ J j j j j\nJ j\nj j x x\nj\nc\nE e g b Sb g b Sb \uf061 \uf061 \uf06c \uf04a \uf06c \uf04a\n\uf04a\n\uf06c \uf04a \uf04a \uf04a \uf04a \uf04a\n\uf06c \uf04a\n\uf02d\n\uf02b\n\uf03d\n\uf0e6 \uf0f6\n\uf020\uf020\uf020\uf020\uf020\uf020\uf020 \uf02b \uf020 \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf0e5 \uf0d5\n S W I\nn n \uf02d \uf02d 1 1 \uf0282( 1) \uf061 \uf02b , .\uf029\nTherefore, the Lt of \n| | h\nis bounded by a linear combination of integrals of the form\n \uf07b \uf07d\n1\n( ) 1\n1 1\n( )\n1\n2\n( t ) 1 ( )\n( ( )) , ( ) ( ) , K 1,..., .\n| |\n( )\nK\nT\nc\nj j\nn\nn n\ni\ni K\ne j j j j\nj j n\nj\nx t x g b Sb e dx n\nI R T\nE \uf061 \uf061\n\uf04a\n\uf06c \uf04a\n\uf06c \uf04a\n\uf06c \uf04a \uf04a \uf04a\n\uf02b\n\uf02d \uf02d \uf0ce\n\uf02b \uf02b\n\uf03d \uf03d\n\uf0e6 \uf0f6 \uf02b\n\uf0e7 \uf0f7 \uf03d\uf020\uf020 \uf020\uf020 \uf0cd\n\uf02b \uf0e8 \uf0f8\n\uf0d5\n\uf0f2 \uf0d5 \uf0d5\nThe coefficients of this linear combination are non-negative continuous functions of \n\uf04a \uf0ce[ , ]. 0 \uf074\nIntegration over \n[ , ] 0 \uf074\nyields a finite value, which implies (2.7) and (2.6). \n Integration over \n1\n,...,\nn\nx x\nin (2.5) leads to\n \n1 1 ( ,..., ; , ) ( ) ( ,..., ; 1, ) 0, ( )\nj\nn J n\nJ j J x\nF x x R c F x x R \uf061 \uf074 \uf061 \uf074 \uf074\n\uf074 \uf0ce\n\uf0b6\n\uf0b6\n\uf0b6\n\uf03d \uf02b \uf03e\n\uf0b6\n\uf0e5 \uf0d5 (2.8)\nsince the canonical correlations in (2.4) do not identically vanish because of the positive rank of\n12 R . \n-4-\nRemarks. By continuity, theorem 1 holds also for a singular\nR,\nat least with \u201c\n\uf0b3\n\u201d instead of \u201c\n\uf03e\n\u201d. \n A modified proof uses the char. function \n\u02c6\nh\ncorresponding to (2.4). It is the continuous limit of the char. functions of \nthe signed measures defined by \n1\n1 1 ( ( ,..., ; , ( ,..., ; , 0 ) )), . F x x R F x x R n n \uf074 \uf065 \uf074 \uf065 \uf061 \uf061 \uf065\n\uf02d\n\uf02b \uf02d \uf020\uf020 \uf0ae\nThus, \n\u02c6\nh\nis the char.\nfunction of the signed measure, which is defined by \n1\n( ,..., ; , ) F x x R n \uf074\n\uf074\n\uf061\n\uf0b6\n\uf0b6\nand which is the difference of two finite \npositive measures on \n.\nn\n An approximation by a series of univariate integrals for the difference of both sides in (2.1) is proposed in [11] with \nidentical values \ni\nx x \uf03d\nunder the additional conditions\n \n1 1 2 2 1 2 1 1\n1\n1\n2\n1 1 1\n2\n1 2 1 2\n1\n2 2 1\n( 1) ( 1)\n0, 0, .\nij ij ij\ni j n n i j n\nn n\nn n n n n n i j n\n\uf072 \uf072 \uf072 \uf072 \uf072 \uf072 \uf072 \uf072\n\uf02d \uf02d \uf0a3 \uf03c \uf0a3 \uf02b \uf0a3 \uf03c \uf0a3 \uf03d \uf03d \uf02b\n\uf0e6 \uf0f6\n\uf03d \uf020\uf03e\uf020\uf020 \uf020\uf020\uf020\uf020 \uf03d \uf020\uf03e\uf020\uf020 \uf020\uf020\uf020\uf020 \uf03d \uf0a3 \uf0e7 \uf0f7 \uf0e8 \uf0f8 \uf0e5 \uf0e5 \uf0e5 \uf0e5\nThe error of this approximation tends to zero with a decreasing variability of the correlations within the submatrices \n11 22 R R,\nand \n12 R .\nThis approximation is recommended in particular for small values of \n1 ( ,..., ; ; ). \uf02d F x x R \uf061\nIn a mo\u0002dified approximation \n2\n\uf072\nis replaced by \n1 2\n1\n1\n2\n1 1\n1\n.\nij\nn n\nn n i j n\n\uf072\n\uf03d \uf03d \uf02b\n\uf0e5 \uf0e5\nReferences\n[1] Bapat, R.B. (1989). Infinite divisibility of multivariate gamma distributions and M-matrices, Sankhy\u0101 51, 73-78.\n[2] Das Gupta, S., Eaton, M.L., Olkin, I., Perlman, M.D., Savage, L.J. and Sobel, M. (1972). Inequalities on the\n probability content of convex regions for elliptically contoured distributions, Proc. Sixth Berkeley Symp. Math.\n Statist. Probab. 2, 241-264, Univ. of California Press, Berkeley.\n[3] Griffiths, R.C. (1984). Characterization of infinitely divisible multivariate gamma distributions, J. Multivariate\n Anal. 15, 13-20.\n[4] Harg\u00e9, G. (1999). A particular case of correlation inequality for the Gaussian measure, Ann. Probab. 27,1939-\n 1951.\n[5] Khatri, C.G. (1967). On certain inequalities for normal distributions and their application to simultaneous confi-\n dence bounds, Ann. Math. Stat. 38, 1853-1867.\n[6] Krishnamoorthy, A.S. and Parthasarathy, M. (1951). A multivariate gamma type distribution",
          "original_query": "A simple proof of the gaussian correlation conjecture extended to multivariate gamma distributions",
          "cleaned_query": "A simple proof of the gaussian correlation conjecture extended to multivariate gamma distributions",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Franz\u2013Parisi Criterion for Non-Gaussian Additive Models (Gamma/Elliptical Noise)\nExtend the rigorous Franz\u2013Parisi/low-degree connection beyond Gaussian additive models to additive models with multivariate gamma (via Royen-style Laplace transform structure) and elliptical noise. Prove an analogue of the Franz\u2013Parisi \u201cgeometric hardness\u201d criterion and show it implies low-degree hardness (and failure of local MCMC) under explicit moment/Laplace conditions.",
        "Computational Phase Diagrams for Sparse Regression with Heavy-Tailed Designs\nUse the low-degree likelihood ratio framework to compute sharp predicted hardness thresholds for sparse linear regression when the design is non-Gaussian (e.g., sub-Weibull or gamma-mixture designs) and compare to the Franz\u2013Parisi landscape predicted metastability. Provide rigorous low-degree lower bounds under verifiable tail/moment assumptions and map when existing polynomial-time estimators (Lasso/SoS) should fail.",
        "A \u201cMetastability Index\u201d from Low-Degree Projections of the Free-Energy Landscape\nDefine a quantitative index that measures metastable structure by relating the Franz\u2013Parisi potential\u2019s local minima/barriers to the growth rate of the low-degree likelihood ratio second moment. Show on canonical planted models (sparse PCA, tensor PCA, planted dense subgraph) that changes in this index coincide with algorithmic transitions, yielding a computable certificate of landscape ruggedness from low-degree statistics.",
        "Local MCMC Lower Bounds via Low-Degree for Graphical Planting (Dense Subgraph Variants)\nStarting from the paper\u2019s implication \u201clow-degree hardness \u21d2 failure of geometric local MCMC\u201d for Gaussian additive models, develop a parallel theorem for planted dense subgraph/clique detection by embedding it into an approximate exponential-family (statistical physics) free-energy formalism. Prove that below the low-degree/SQ threshold, any local-update MCMC (single-vertex flips, short-range proposals) mixes exponentially slowly, using a Franz\u2013Parisi-style constrained overlap potential on graphs.",
        "Finite-Sample, Finite-Degree Calibration of SQ vs Low-Degree Equivalence\nConvert the \u201calmost equivalence\u201d between statistical query algorithms and low-degree tests into explicit finite-\\(n\\), finite-tolerance bounds: given SQ tolerance and query complexity, output an explicit polynomial degree and coefficient norm guaranteeing comparable testing power. This yields actionable guidance for practitioners: how many samples/queries are needed to match a degree-\\(d\\) moment method, with constants that can be numerically evaluated in specific inference problems.",
        "Canonical Correlation Inequalities as Tools for Low-Degree Moment Control\nExploit Royen-style correlation inequalities (and their determinant/Laplace expansions) to bound low-degree likelihood ratio moments for models with correlated features, where independence-based calculations break. Develop a general technique: reduce low-degree second-moment bounds to verifying a monotonicity/correlation inequality over a parametrized correlation matrix path, enabling new hardness predictions for planted signals under feature correlation.",
        "Franz\u2013Parisi Potentials for Multi-Temperature Inference and \u201cWarm-Start\u201d Algorithms\nGeneralize the Franz\u2013Parisi constrained free energy to inference settings where initialization is drawn from a different temperature/regularization level (mirroring Franz\u2013Parisi\u2019s \\(T\\) vs \\(T'\\)). Use this to predict when warm-started local methods (annealing schedules, homotopy/LARS-type paths, tempering MCMC) can cross barriers that cold starts cannot, and validate predictions against low-degree hardness thresholds.",
        "Algorithm Design: Spectral Features Derived from Low-Degree Likelihood Ratio Expansions\nUse the formal connection between spectral methods and low-degree likelihood ratios to design new \u201cdegree-aware\u201d spectral algorithms: construct data-dependent kernels whose top eigenvectors approximate the optimal degree-\\(d\\) polynomial test. Evaluate on tensor PCA and planted dense subgraph to see whether these kernels match the best-known polynomial-time thresholds, and identify regimes where they provably saturate the low-degree prediction.",
        "Unified Benchmark Suite: Empirical Validation of Landscape\u2013Low-Degree\u2013SQ Predictions\nCreate a benchmark that, for a curated set of planted models, simultaneously (i) estimates the Franz\u2013Parisi potential numerically (via constrained sampling/replica-inspired approximations), (ii) computes low-degree second moments numerically/symbolically up to moderate degree, and (iii) measures SQ-algorithm performance under controlled tolerances. The contribution is an end-to-end validation (and falsification) pipeline that isolates where the three notions agree or diverge, guiding refinements of computational hardness criteria."
      ],
      "generated_ideas_raw": "1. **Franz\u2013Parisi Criterion for Non-Gaussian Additive Models (Gamma/Elliptical Noise)**\n   Extend the rigorous Franz\u2013Parisi/low-degree connection beyond Gaussian additive models to additive models with multivariate gamma (via Royen-style Laplace transform structure) and elliptical noise. Prove an analogue of the Franz\u2013Parisi \u201cgeometric hardness\u201d criterion and show it implies low-degree hardness (and failure of local MCMC) under explicit moment/Laplace conditions.\n\n2. **Computational Phase Diagrams for Sparse Regression with Heavy-Tailed Designs**\n   Use the low-degree likelihood ratio framework to compute sharp predicted hardness thresholds for sparse linear regression when the design is non-Gaussian (e.g., sub-Weibull or gamma-mixture designs) and compare to the Franz\u2013Parisi landscape predicted metastability. Provide rigorous low-degree lower bounds under verifiable tail/moment assumptions and map when existing polynomial-time estimators (Lasso/SoS) should fail.\n\n3. **A \u201cMetastability Index\u201d from Low-Degree Projections of the Free-Energy Landscape**\n   Define a quantitative index that measures metastable structure by relating the Franz\u2013Parisi potential\u2019s local minima/barriers to the growth rate of the low-degree likelihood ratio second moment. Show on canonical planted models (sparse PCA, tensor PCA, planted dense subgraph) that changes in this index coincide with algorithmic transitions, yielding a computable certificate of landscape ruggedness from low-degree statistics.\n\n4. **Local MCMC Lower Bounds via Low-Degree for Graphical Planting (Dense Subgraph Variants)**\n   Starting from the paper\u2019s implication \u201clow-degree hardness \u21d2 failure of geometric local MCMC\u201d for Gaussian additive models, develop a parallel theorem for planted dense subgraph/clique detection by embedding it into an approximate exponential-family (statistical physics) free-energy formalism. Prove that below the low-degree/SQ threshold, any local-update MCMC (single-vertex flips, short-range proposals) mixes exponentially slowly, using a Franz\u2013Parisi-style constrained overlap potential on graphs.\n\n5. **Finite-Sample, Finite-Degree Calibration of SQ vs Low-Degree Equivalence**\n   Convert the \u201calmost equivalence\u201d between statistical query algorithms and low-degree tests into explicit finite-\\(n\\), finite-tolerance bounds: given SQ tolerance and query complexity, output an explicit polynomial degree and coefficient norm guaranteeing comparable testing power. This yields actionable guidance for practitioners: how many samples/queries are needed to match a degree-\\(d\\) moment method, with constants that can be numerically evaluated in specific inference problems.\n\n6. **Canonical Correlation Inequalities as Tools for Low-Degree Moment Control**\n   Exploit Royen-style correlation inequalities (and their determinant/Laplace expansions) to bound low-degree likelihood ratio moments for models with correlated features, where independence-based calculations break. Develop a general technique: reduce low-degree second-moment bounds to verifying a monotonicity/correlation inequality over a parametrized correlation matrix path, enabling new hardness predictions for planted signals under feature correlation.\n\n7. **Franz\u2013Parisi Potentials for Multi-Temperature Inference and \u201cWarm-Start\u201d Algorithms**\n   Generalize the Franz\u2013Parisi constrained free energy to inference settings where initialization is drawn from a different temperature/regularization level (mirroring Franz\u2013Parisi\u2019s \\(T\\) vs \\(T'\\)). Use this to predict when warm-started local methods (annealing schedules, homotopy/LARS-type paths, tempering MCMC) can cross barriers that cold starts cannot, and validate predictions against low-degree hardness thresholds.\n\n8. **Hardness of Estimation (Not Just Testing) via a Franz\u2013Parisi-to-Low-Degree Pipeline**\n   Build a systematic reduction from estimation tasks (e.g., recovering sparse vectors/tensors) to families of constrained-overlap free energies, then translate the resulting landscape properties into low-degree impossibility for weak recovery. Deliver concrete new lower bounds for partial recovery regimes (correlation \\(>0\\) but vanishing) where testing is easy but estimation is conjectured hard.\n\n9. **Algorithm Design: Spectral Features Derived from Low-Degree Likelihood Ratio Expansions**\n   Use the formal connection between spectral methods and low-degree likelihood ratios to design new \u201cdegree-aware\u201d spectral algorithms: construct data-dependent kernels whose top eigenvectors approximate the optimal degree-\\(d\\) polynomial test. Evaluate on tensor PCA and planted dense subgraph to see whether these kernels match the best-known polynomial-time thresholds, and identify regimes where they provably saturate the low-degree prediction.\n\n10. **Unified Benchmark Suite: Empirical Validation of Landscape\u2013Low-Degree\u2013SQ Predictions**\n   Create a benchmark that, for a curated set of planted models, simultaneously (i) estimates the Franz\u2013Parisi potential numerically (via constrained sampling/replica-inspired approximations), (ii) computes low-degree second moments numerically/symbolically up to moderate degree, and (iii) measures SQ-algorithm performance under controlled tolerances. The contribution is an end-to-end validation (and falsification) pipeline that isolates where the three notions agree or diverge, guiding refinements of computational hardness criteria.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Franz\u2013Parisi Criterion for Non-Gaussian Additive Models (Gamma/Elliptical Noise)\nExtend the rigorous Franz\u2013Parisi/low-degree connection beyond Gaussian additive models to additive models with multivar",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Computational Phase Diagrams for Sparse Regression with Heavy-Tailed Designs\nUse the low-degree likelihood ratio framework to compute sharp predicted hardness thresholds for sparse linear regression w",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "A \u201cMetastability Index\u201d from Low-Degree Projections of the Free-Energy Landscape\nDefine a quantitative index that measures metastable structure by relating the Franz\u2013Parisi potential\u2019s local minima/ba",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Local MCMC Lower Bounds via Low-Degree for Graphical Planting (Dense Subgraph Variants)\nStarting from the paper\u2019s implication \u201clow-degree hardness \u21d2 failure of geometric local MCMC\u201d for Gaussian addit",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Finite-Sample, Finite-Degree Calibration of SQ vs Low-Degree Equivalence\nConvert the \u201calmost equivalence\u201d between statistical query algorithms and low-degree tests into explicit finite-\\(n\\), finite-t",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Canonical Correlation Inequalities as Tools for Low-Degree Moment Control\nExploit Royen-style correlation inequalities (and their determinant/Laplace expansions) to bound low-degree likelihood ratio m",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Franz\u2013Parisi Potentials for Multi-Temperature Inference and \u201cWarm-Start\u201d Algorithms\nGeneralize the Franz\u2013Parisi constrained free energy to inference settings where initialization is drawn from a diffe",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Algorithm Design: Spectral Features Derived from Low-Degree Likelihood Ratio Expansions\nUse the formal connection between spectral methods and low-degree likelihood ratios to design new \u201cdegree-aware\u201d",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Unified Benchmark Suite: Empirical Validation of Landscape\u2013Low-Degree\u2013SQ Predictions\nCreate a benchmark that, for a curated set of planted models, simultaneously (i) estimates the Franz\u2013Parisi potenti",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 15,
      "paper_title": "MaxSup: Overcoming Representation Collapse in Label Smoothing",
      "contribution": "A theoretical decomposition of label smoothing that exposes an error-amplification term, and a simple logit-level regularizer (Max Suppression) that penalizes the top-1 logit to retain LS\u2019s benefits while avoiding overconfident misclassifications and representation collapse.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11070,
      "output_tokens": 932,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Rethinking the Inception Architecture for Computer Vision",
          "url": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf",
          "content": "Rethinking the Inception Architecture for Computer Vision\nChristian Szegedy\nGoogle Inc.\nszegedy@google.com\nVincent Vanhoucke\nvanhoucke@google.com\nSergey Ioffe\nsioffe@google.com\nJon Shlens\nshlens@google.com\nZbigniew Wojna\nUniversity College London\nzbigniewwojna@gmail.com\nAbstract\nConvolutional networks are at the core of most state\u0002of-the-art computer vision solutions for a wide variety of\ntasks. Since 2014 very deep convolutional networks started\nto become mainstream, yielding substantial gains in vari\u0002ous benchmarks. Although increased model size and com\u0002putational cost tend to translate to immediate quality gains\nfor most tasks (as long as enough labeled data is provided\nfor training), computational efficiency and low parameter\ncount are still enabling factors for various use cases such as\nmobile vision and big-data scenarios. Here we are explor\u0002ing ways to scale up networks in ways that aim at utilizing\nthe added computation as efficiently as possible by suitably\nfactorized convolutions and aggressive regularization. We\nbenchmark our methods on the ILSVRC 2012 classification\nchallenge validation set demonstrate substantial gains over\nthe state of the art: 21.2% top-1 and 5.6% top-5 error for\nsingle frame evaluation using a network with a computa\u0002tional cost of 5 billion multiply-adds per inference and with\nusing less than 25 million parameters. With an ensemble of\n4 models and multi-crop evaluation, we report 3.5% top-5\nerror and 17.3% top-1 error on the validation set and 3.6%\ntop-5 error on the official test set.\n1. Introduction\nSince the 2012 ImageNet competition [16] winning en\u0002try by Krizhevsky et al [9], their network \u201cAlexNet\u201d has\nbeen successfully applied to a larger variety of computer\nvision tasks, for example to object-detection [5], segmen\u0002tation [12], human pose estimation [22], video classifica\u0002tion [8], object tracking [23], and superresolution [3].\nThese successes spurred a new line of research that fo\u0002cused on finding higher performing convolutional neural\nnetworks. Starting in 2014, the quality of network architec\u0002tures significantly improved by utilizing deeper and wider\nnetworks. VGGNet [18] and GoogLeNet [20] yielded simi\u0002larly high performance in the 2014 ILSVRC [16] classifica\u0002tion challenge. One interesting observation was that gains\nin the classification performance tend to transfer to signifi\u0002cant quality gains in a wide variety of application domains.\nThis means that architectural improvements in deep con\u0002volutional architecture can be utilized for improving perfor\u0002mance for most other computer vision tasks that are increas\u0002ingly reliant on high quality, learned visual features. Also,\nimprovements in the network quality resulted in new appli\u0002cation domains for convolutional networks in cases where\nAlexNet features could not compete with hand engineered,\ncrafted solutions, e.g. proposal generation in detection[4].\nAlthough VGGNet [18] has the compelling feature of\narchitectural simplicity, this comes at a high cost: evalu\u0002ating the network requires a lot of computation. On the\nother hand, the Inception architecture of GoogLeNet [20]\nwas also designed to perform well even under strict con\u0002straints on memory and computational budget. For ex\u0002ample, GoogleNet employed around 7 million parameters,\nwhich represented a 9\u00d7 reduction with respect to its prede\u0002cessor AlexNet, which used 60 million parameters. Further\u0002more, VGGNet employed about 3x more parameters than\nAlexNet.\nThe computational cost of Inception is also much lower\nthan VGGNet or its higher performing successors [6]. This\nhas made it feasible to utilize Inception networks in big-data\nscenarios[17], [13], where huge amount of data needed to\nbe processed at reasonable cost or scenarios where memory\nor computational capacity is inherently limited, for example\nin mobile vision settings. It is certainly possible to mitigate\nparts of these issues by applying specialized solutions to tar\u0002get memory use [2], [15] or by optimizing the execution of\ncertain operations via computational tricks [10]. However,\nthese methods add extra complexity. Furthermore, these\nmethods could be applied to optimize the Inception archi\u0002tecture as well, widening the efficiency gap again.\n12818\nStill, the complexity of the Inception architecture makes\nit more difficult to make changes to the network. If the ar\u0002chitecture is scaled up naively, large parts of the computa\u0002tional gains can be immediately lost. Also, [20] does not\nprovide a clear description about the contributing factors\nthat lead to the various design decisions of the GoogLeNet\narchitecture. This makes it much harder to adapt it to new\nuse-cases while maintaining its efficiency. For example,\nif it is deemed necessary to increase the capacity of some\nInception-style model, the simple transformation of just\ndoubling the number of all filter bank sizes will lead to a\n4x increase in both computational cost and number of pa\u0002rameters. This might prove prohibitive or unreasonable in a\nlot of practical scenarios, especially if the associated gains\nare modest. In this paper, we start with describing a few\ngeneral principles and optimization ideas that that proved\nto be useful for scaling up convolution networks in efficient\nways. Although our principles are not limited to Inception\u0002type networks, they are easier to observe in that context as\nthe generic structure of the Inception style building blocks\nis flexible enough to incorporate those constraints naturally.\nThis is enabled by the generous use of dimensional reduc\u0002tion and parallel structures of the Inception modules which\nallows for mitigating the impact of structural changes on\nnearby components. Still, one needs to be cautious about\ndoing so, as some guiding principles should be observed to\nmaintain high quality of the models.\n2. General Design Principles\nHere we will describe a few design principles based\non large-scale experimentation with various architectural\nchoices with convolutional networks. At this point, the util\u0002ity of the principles below are speculative and additional fu\u0002ture experimental evidence will be necessary to assess their\ndomain of validity. Still, grave deviations from these prin\u0002ciples tended to result in deterioration in the quality of the\nnetworks and fixing situations where those deviations were\ndetected resulted in improved architectures.\n1. Avoid representational bottlenecks, especially early in\nthe network. Feed-forward networks can be repre\u0002sented by an acyclic graph from the input layer(s) to\nthe classifier or regressor. This defines a clear direction\nfor the information flow. For any cut separating the in\u0002puts from the outputs, one can access the amount of\ninformation passing though the cut. One should avoid\nbottlenecks with extreme compression. In general the\nrepresentation size should gently decrease from the in\u0002puts to the outputs before reaching the final represen\u0002tation used for the task at hand. Theoretically, infor\u0002mation content can not be assessed merely by the di\u0002mensionality of the representation as it discards impor\u0002tant factors like correlation structure; the dimensional\u0002ity merely provides a rough estimate of information\ncontent.\n2. Higher dimensional representations are easier to pro\u0002cess locally within a network. Increasing the activa\u0002tions per tile in a convolutional network allows for\nmore disentangled features. The resulting networks\nwill train faster.\n3. Spatial aggregation can be done over lower dimen\u0002sional embeddings without much or any loss in rep\u0002resentational power. For example, before performing a\nmore spread out (e.g. 3 \u00d7 3) convolution, one can re\u0002duce the dimension of the input representation before\nthe spatial aggregation without expecting serious ad\u0002verse effects. We hypothesize that the reason for that\nis the strong correlation between adjacent unit results\nin much less loss of information during dimension re\u0002duction, if the outputs are used in a spatial aggrega\u0002tion context. Given that these signals should be easily\ncompressible, the dimension reduction even promotes\nfaster learning.\n4. Balance the width and depth of the network. Optimal\nperformance of the network can be reached by balanc\u0002ing the number of filters per stage and the depth of\nthe network. Increasing both the width and the depth\nof the network can contribute to higher quality net\u0002works. However, the optimal improvement for a con\u0002stant amount of computation can be reached if both are\nincreased in parallel. The computational budget should\ntherefore be distributed in a balanced way between the\ndepth and width of the network.\nAlthough these principles might make sense, it is not\nstraightforward to use them to improve the quality of net\u0002works out of box. The idea is to use them judiciously in\nambiguous situations only.\n3. Factorizing Convolutions with Large Filter\nSize\nMuch of the original gains of the GoogLeNet net\u0002work [20] arise from a very generous use of dimension re\u0002duction, just like in the \u201cNetwork in network\u201d architecture\nby Lin et al [?]. This can be viewed as a special case of fac\u0002torizing convolutions in a computationally efficient manner.\nConsider for example the case of a 1\u00d71 convolutional layer\nfollowed by a 3 \u00d7 3 convolutional layer. In a vision net\u0002work, it is expected that the outputs of near-by activations\nare highly correlated. Therefore, we can expect that their\nactivations can be reduced before aggregation and that this\nshould result in similarly expressive local representations.\nHere we explore other ways of factorizing convolutions\nin various settings, especially in order to increase the com\u00022819\nFigure 1. Mini-network replacing the 5 \u00d7 5 convolutions.\nputational efficiency of the solution. Since Inception net\u0002works are fully convolutional, each weight corresponds to\none multiplication per activation. Therefore, any reduction\nin computational cost results in reduced number of param\u0002eters. This means that with suitable factorization, we can\nend up with more disentangled parameters and therefore\nw",
          "original_query": "Rethinking the Inception Architecture for Computer Vision",
          "cleaned_query": "Rethinking the Inception Architecture for Computer Vision",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] When does label smoothing help? - NeurIPS",
          "url": "http://papers.neurips.cc/paper/8717-when-does-label-smoothing-help.pdf",
          "content": "When Does Label Smoothing Help?\nRafael M\u00fcller\u2217, Simon Kornblith, Geoffrey Hinton\nGoogle Brain\nToronto\nrafaelmuller@google.com\nAbstract\nThe generalization and learning speed of a multi-class neural network can often\nbe significantly improved by using soft targets that are a weighted average of the\nhard targets and the uniform distribution over labels. Smoothing the labels in this\nway prevents the network from becoming over-confident and label smoothing has\nbeen used in many state-of-the-art models, including image classification, language\ntranslation and speech recognition. Despite its widespread use, label smoothing is\nstill poorly understood. Here we show empirically that in addition to improving\ngeneralization, label smoothing improves model calibration which can significantly\nimprove beam-search. However, we also observe that if a teacher network is\ntrained with label smoothing, knowledge distillation into a student network is much\nless effective. To explain these observations, we visualize how label smoothing\nchanges the representations learned by the penultimate layer of the network. We\nshow that label smoothing encourages the representations of training examples\nfrom the same class to group in tight clusters. This results in loss of information\nin the logits about resemblances between instances of different classes, which is\nnecessary for distillation, but does not hurt generalization or calibration of the\nmodel\u2019s predictions.\n1 Introduction\nIt is widely known that neural network training is sensitive to the loss that is minimized. Shortly\nafter Rumelhart et al. [1] derived backpropagation for the quadratic loss function, several researchers\nnoted that better classification performance and faster convergence could be attained by performing\ngradient descent to minimize cross entropy [2, 3]. However, even in these early days of neural\nnetwork research, there were indications that other, more exotic objectives could outperform the\nstandard cross entropy loss [4, 5]. More recently, Szegedy et al. [6] introduced label smoothing,\nwhich improves accuracy by computing cross entropy not with the \u201chard\" targets from the dataset,\nbut with a weighted mixture of these targets with the uniform distribution.\nLabel smoothing has been used successfully to improve the accuracy of deep learning models across\na range of tasks, including image classification, speech recognition, and machine translation (Table 1).\nSzegedy et al. [6] originally proposed label smoothing as a strategy that improved the performance of\nthe Inception architecture on the ImageNet dataset, and many state-of-the-art image classification\nmodels have incorporated label smoothing into training procedures ever since [7, 8, 9]. In speech\nrecognition, Chorowski and Jaitly [10] used label smoothing to reduce the word error rate on the\nWSJ dataset. In machine translation, Vaswani et al. [11] attained a small but important improvement\nin BLEU score, despite a reduction in perplexity.\nAlthough label smoothing is a widely used \u201ctrick\" to improve network performance, not much is\nknown about why and when label smoothing should work. This paper tries to shed light upon behavior\n\u2217This work was done as part of the Google AI Residency.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nTable 1: Survey of literature label smoothing results on three supervised learning tasks.\nDATA SET ARCHITECTURE METRIC VALUE W/O LS VALUE W/ LS\nIMAGENET INCEPTION-V2 [6] TOP-1 ERROR 23.1 22.8\nTOP-5 ERROR 6.3 6.1\nEN-DE TRANSFORMER [11] BLEU 25.3 25.8\nPERPLEXITY 4.67 4.92\nWSJ BILSTM+ATT.[10] WER 8.9 7.0/6.7\nof neural networks trained with label smoothing, and we describe several intriguing properties of\nthese networks. Our contributions are as follows:\n\u2022 We introduce a novel visualization method based on linear projections of the penultimate\nlayer activations. This visualization provides intuition regarding how representations differ\nbetween penultimate layers of networks trained with and without label smoothing.\n\u2022 We demonstrate that label smoothing implicitly calibrates learned models so that the confi\u0002dences of their predictions are more aligned with the accuracies of their predictions.\n\u2022 We show that label smoothing impairs distillation, i.e., when teacher models are trained with\nlabel smoothing, student models perform worse. We further show that this adverse effect\nresults from loss of information in the logits.\n1.1 Preliminaries\nBefore describing our findings, we provide a mathematical description of label smoothing. Suppose\nwe write the prediction of a neural network as a function of the activations in the penultimate layer\nas pk =\ne\nxT wk PL\nl=1 e\nxT wl\n, where pk is the likelihood the model assigns to the k-th class, wk represents\nthe weights and biases of the last layer, x is the vector containing the activations of the penultimate\nlayer of a neural network concatenated with \"1\" to account for the bias. For a network trained with\nhard targets, we minimize the expected value of the cross-entropy between the true targets yk and\nthe network\u2019s outputs pk as in H(y, p) = PK\nk=1 \u2212yk log(pk), where yk is \"1\" for the correct class\nand \"0\" for the rest. For a network trained with a label smoothing of parameter \u03b1, we minimize\ninstead the cross-entropy between the modified targets y\nLS\nk\nand the networks\u2019 outputs pk, where\ny\nLS\nk = yk(1 \u2212 \u03b1) + \u03b1/K.\n2 Penultimate layer representations\nTraining a network with label smoothing encourages the differences between the logit of the correct\nclass and the logits of the incorrect classes to be a constant dependent on \u03b1. By contrast, training\na network with hard targets typically results in the correct logit being much larger than the any\nof the incorrect logits and also allows the incorrect logits to be very different from one another.\nIntuitively, the logit x\nT wk of the k-th class can be thought of as a measure of the squared Euclidean\ndistance between the activations of the penultimate layer x and a template wk, as ||x \u2212 wk||2 =\nx\nT x \u2212 2xT wk + wT\nk wk. Here, each class has a template wk, x\nT x is factored out when calculating\nthe softmax outputs and wT\nk wk is usually constant across classes. Therefore, label smoothing\nencourages the activations of the penultimate layer to be close to the template of the correct class and\nequally distant to the templates of the incorrect classes. To observe this property of label smoothing,\nwe propose a new visualization scheme based on the following steps: (1) Pick three classes, (2)\nFind an orthonormal basis of the plane crossing the templates of these three classes, (3) Project the\npenultimate layer activations of examples from these three classes onto this plane. This visualization\nshows in 2-D how the activations cluster around the templates and how label smoothing enforces a\nstructure on the distance between the examples and the clusters from the other classes.\nIn Fig. 1, we show results of visualizing penultimate layer representations of image classifiers trained\non the datasets CIFAR-10, CIFAR-100 and ImageNet with the architectures AlexNet [12], ResNet-56\n[13] and Inception-v4 [14], respectively. Table 2 shows the effect of label smoothing on the accuracy\nof these models. We start by describing visualization results for CIFAR-10 (first row of Fig. 1) for\nthe classes \u201cairplane,\" \u201cautomobile\" and \u201cbird.\" The first two columns represent examples from the\ntraining and validation set for a network trained without label smoothing (w/o LS). We observe that\n2\nFigure 1: Visualization of penultimate layer\u2019s activations of: AlexNet/CIFAR-10 (first row), CIFAR\u0002100/ResNet-56 (second row) and ImageNet/Inception-v4 with three semantically different classes\n(third row) and two semantically similar classes plus a third one (fourth row).\nTable 2: Top-1 classification accuracies of networks trained with and without label smoothing used in\nvisualizations.\nDATA SET ARCHITECTURE ACCURACY (\u03b1 = 0.0) ACCURACY (\u03b1 = 0.1)\nCIFAR-10 ALEXNET 86.8 \u00b1 0.2 86.7 \u00b1 0.3\nCIFAR-100 RESNET-56 72.1 \u00b1 0.3 72.7 \u00b1 0.3\nIMAGENET INCEPTION-V4 80.9 80.9\nthe projections are spread into defined but broad clusters. The last two columns show a network\ntrained with a label smoothing factor of 0.1. We observe that now the clusters are much tighter,\nbecause label smoothing encourages that each example in training set to be equidistant from all the\nother class\u2019s templates. Therefore, when looking at the projections, the clusters organize in regular\ntriangles when training with label smoothing, whereas the regular triangle structure is less discernible\nin the case of training with hard-targets (no label smoothing). Note that these networks have similar\naccuracies despite qualitatively different clustering of activations.\nIn the second row, we investigate the activation\u2019s geometry for a different pair of dataset/architecture\n(CIFAR-100/ResNet-56). Again, we observe the same behavior for classes \u201cbeaver,\" \u201cdolphin,\"\n\u201cotter.\" In contrast to the previous example, now the networks trained with label smoothing have\nbetter accuracy. Additionally, we observe the different scale of the projections between the network\ntrained with and without label smoothing. With label smoothing, the difference between logits of two\nclasses has to be limited in absolute value to get the desired soft target for the correct and incorrect\n3\nclasses. Without label smoothing, however, the projection can take much higher absolute values,\nwhich represent over-confident predictions.\nFinally, we test our visualization scheme in an Inception-v4/ImageNet experiment and observe the\neffect of label smoothing for semantically similar classes, since ImageNet has many fine-grained\nclasses (e.g. different breeds of dogs). The third row represents projections for semantically different\nclasses (tench, meerkat and cleaver) with the behavior similar to previous experiments. The fourth\nrow is more interesting, since we pick two semantically similar classes (toy poo",
          "original_query": "When does label smoothing help?",
          "cleaned_query": "When does label smoothing help?",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Why Do Better Loss Functions Lead to Less Transferable Features?",
          "url": "https://arxiv.org/abs/2010.16402",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2010.16402** (cs)\n\n\\[Submitted on 30 Oct 2020 ( [v1](https://arxiv.org/abs/2010.16402v1)), last revised 3 Nov 2021 (this version, v2)\\]\n\n# Title:Why Do Better Loss Functions Lead to Less Transferable Features?\n\nAuthors: [Simon Kornblith](https://arxiv.org/search/cs?searchtype=author&query=Kornblith,+S), [Ting Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+T), [Honglak Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+H), [Mohammad Norouzi](https://arxiv.org/search/cs?searchtype=author&query=Norouzi,+M)\n\nView a PDF of the paper titled Why Do Better Loss Functions Lead to Less Transferable Features?, by Simon Kornblith and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2010.16402)\n\n> Abstract:Previous work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks.\n\n| | |\n| --- | --- |\n| Comments: | NeurIPS 2021 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2010.16402](https://arxiv.org/abs/2010.16402) \\[cs.CV\\] |\n| | (or [arXiv:2010.16402v2](https://arxiv.org/abs/2010.16402v2) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2010.16402](https://doi.org/10.48550/arXiv.2010.16402) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Simon Kornblith \\[ [view email](https://arxiv.org/show-email/743c4656/2010.16402)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2010.16402v1)**\nFri, 30 Oct 2020 17:50:31 UTC (304 KB)\n\n**\\[v2\\]**\nWed, 3 Nov 2021 18:32:53 UTC (652 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Why Do Better Loss Functions Lead to Less Transferable Features?, by Simon Kornblith and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2010.16402)\n- [TeX Source](https://arxiv.org/src/2010.16402)\n- [Other Formats](https://arxiv.org/format/2010.16402)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2010.16402&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2010.16402&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2020-10](https://arxiv.org/list/cs.CV/2020-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2010.16402?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2010.16402?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2010.16402)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2010.16402)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2010.16402)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2010.html#abs-2010-16402) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2010-16402)\n\n[Simon Kornblith](https://dblp.uni-trier.de/search/author?author=Simon%20Kornblith)\n\n[Honglak Lee](https://dblp.uni-trier.de/search/author?author=Honglak%20Lee)\n\n[Ting Chen](https://dblp.uni-trier.de/search/author?author=Ting%20Chen)\n\n[Mohammad Norouzi](https://dblp.uni-trier.de/search/author?author=Mohammad%20Norouzi)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2010.16402&description=Why Do Better Loss Functions Lead to Less Transferable Features?) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2010.16402&title=Why Do Better Loss Functions Lead to Less Transferable Features?)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2010.16402) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Why do better loss functions lead to less transferable features?",
          "cleaned_query": "Why do better loss functions lead to less transferable features?",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Rethinking Confidence Calibration for Failure Prediction",
          "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850512.pdf",
          "content": "Rethinking Confidence Calibration for\nFailure Prediction\nFei Zhu1,2, Zhen Cheng1,2, Xu-Yao Zhang1,2?, and Cheng-Lin Liu1,2\n1 NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China\n2 University of Chinese Academy of Sciences, Beijing, 100049, China\n{zhufei2018, chengzhen2019}@ia.ac.cn, {xyz, liucl}@nlpr.ia.ac.cn\nAbstract. Reliable confidence estimation for the predictions is impor\u0002tant in many safety-critical applications. However, modern deep neural\nnetworks are often overconfident for their incorrect predictions. Recently,\nmany calibration methods have been proposed to alleviate the overcon\u0002fidence problem. With calibrated confidence, a primary and practical\npurpose is to detect misclassification errors by filtering out low-confidence\npredictions (known as failure prediction). In this paper, we find a general,\nwidely-existed but actually-neglected phenomenon that most confidence\ncalibration methods are useless or harmful for failure prediction. We\ninvestigate this problem and reveal that popular confidence calibration\nmethods often lead to worse confidence separation between correct and\nincorrect samples, making it more difficult to decide whether to trust a\nprediction or not. Finally, inspired by the natural connection between\nflat minima and confidence separation, we propose a simple hypothesis:\nflat minima is beneficial for failure prediction. We verify this hypothesis\nvia extensive experiments and further boost the performance by com\u0002bining two different flat minima techniques. Our code is available at\nhttps://github.com/Impression2805/FMFP.\nKeywords: Failure prediction \u00b7 Confidence Calibration \u00b7 Flat minima \u00b7\nUncertainty \u00b7 Misclassification Detection \u00b7 Selective Classification\n1 Introduction\nDeep neural networks (DNNs), especially vision models, has been widely deployed\nin risk-sensitive applications such as computer-aided medical diagnosis [11, 44],\nautonomous driving [2, 29], and robotics [38]. For such applications, besides the\nprediction accuracy, another crucial requirement is to provide reliable confidence\nfor users to make safe decisions. For example, an autonomous driving car should\nrely more on other sensors or trigger an alarm when the detection network is\nunable to confidently predict obstructions [29]. Another example is the control\nshould be handed over to human doctors when the confidence of a disease diagnosis\nnetwork is low [44]. Unfortunately, modern DNNs are generally overconfident for\ntheir predictions, and can easily assign high confidence for misclassified samples\n? Corresponding author.\n2 F. Zhu et al.\naverage confidence: 94%\nConfidence Calibration\nconfidence \u2248 accuracy\n(matched)\nFailure Prediction\ncorrectly classified\nincorrectly classified\nOverconfident\nClassifier\nconfidence of correct : 97%\nconfidence of incorrect: 60%\nlegend:\nprediction confidence\nclassifier (DNN)\nThis is a cat !\nCan I trust the\nprediction?\ninput\n(a) (b)\nFig. 1. Confidence calibration aims to reduce the mismatch between a model\u2019s confi\u0002dence and accuracy from the perspective of global statistics, while failure prediction\nis to distinguish correct from incorrect predictions from the perspective of confidence\nseparability. They both focus on in-distribution data and share the same motivation\nto provide reliable confidence for trustworthy AI. Therefore, we explore a natural but\nignored question: is calibrated confidence useful for failure prediction?\n[7, 15, 17, 22]. The overconfident issue makes DNNs models untrustworthy, and\ntherefore brings great concerns when DNNs are deployed in practical applications.\nRecently, many approaches have been developed to alleviate the overconfidence\nproblem by calibrating the confidence, i.e., matching the accuracy and confidence\nscores to reflect the predictive uncertainty [43]. Specifically, one category of\napproaches [20, 41, 47, 48, 54, 61, 66, 68, 70, 75] aim to learn well-calibrated models\nduring training. For instance, mixup [61], label smoothing [48] and focal loss [47]\nhave been demonstrated to be effective for confidence calibration. Another class of\napproaches [15,16,35,53,56,58] use post-processing techniques to calibrate DNNs.\nThe most famous post-processing calibration method is temperature scaling [15]\nwhich learns a single scalar parameter to calibrate the probabilities.\nIn this paper, we study a natural but ignored question: can we use cali\u0002brated confidence to detect misclassified samples by filtering out low-confidence\npredictions? This, perhaps, is the most direct and practical way to evaluate the\nquality of the uncertainty. Actually, this problem is studied in the literature as\nfailure prediction (also known as misclassification detection or selective classifica\u0002tion) [7, 14, 22], whose purpose is to determine whether the prediction yielded by\na classifier is correct or incorrect. Note that failure prediction aims to detect the\nerroneously classified natural example from seen class (e.g., misclassified samples\nin test dataset), which is different from the widely studied out-of-distribution\ndetection [23, 37, 39] that focuses on judging whether an input sample is from\nunseen classes. Compared with confidence calibration and out-of-distribution\ndetection, failure prediction is far less explored in the literature.\nRethinking Confidence Calibration for Failure Prediction 3\n90.76\n86.93\n81.89\n90.70\n85.89\n87.19\n92.86\n93.76\n78\n81\n84\n87\n90\n93\n96\nbaseline\nMixup\nLS\nFL\nCS-KD\nL1\nCRL\nours\nA\nU\nR\nO\nC (%) mixup\nfocal\n0\n1\n2\n3\n4\n5\n6\ns\nele\nctiv\ne ris\nk (%) coverage\nbaseline mixup LS\nfocal CS-KD L1\nCRL ours\n0 0.2 0.4 0.6 0.8 1.0\n(a) (b)\nlower is better\nhigher is better\nFig. 2. A comparison of (a) AUROC and (b) risk-coverage curves. We observed that\nmany popular confidence calibration methods are useless or harmful for failure prediction.\nWe propose a simple flat minima based method that can outperform the state-of-the-art\nfailure prediction method CRL [45]. ResNet110 [19] on CIFAR-10 [33].\nAs shown in Fig. 1, confidence calibration and failure prediction both focus on\nthe confidence of in-distribution data and share the same motivation that enables\nthe model to provide reliable confidence to make safe decisions. Therefore, common\nwisdom in the community suggests that calibrated confidence could be useful\nfor failure prediction. However, we find a surprising pathology: many popular\nconfidence calibration methods (including both training-time [24,48,54,61,68,70]\nand post-processing [15] calibration methods) are more of a hindrance than a help\nfor failure prediction, as illustrated in Fig. 2. Empirical study shows that those\nmethods often reduce overconfidence by simply aligning the accuracy and average\nconfidence. Such calibration could lead to worse separability between correct and\nmisclassified samples, which is harmful for failure prediction. Consequently, one\ncan not effectively detect misclassified samples by filtering out low-confidence\npredictions based on the calibrated confidence.\nFinally, how can we improve the failure prediction performance of DNNs?\nIntuitively, failure prediction requires better discrimination between the confi\u0002dence of correct and incorrect samples, which would increase the difficulty of\nchanging the correct samples to be incorrect due to the larger confidence margins.\nInterestingly, this is closely related to the notion of \u201cflatness\u201d in DNNs, which\nreflects how sensitive the correct samples become misclassified when perturbing\nthe model parameters [12, 27, 28]. Inspired by the natural connection between\nflat minima and confidence separation, we propose a simple hypothesis: flat\nminima is beneficial for failure prediction. We verify this hypothesis by extensive\nexperiments and propose a simple and effective technique that combines different\nkinds of flat methods to achieve state-of-the-art performance on failure prediction.\nContributions. Motivated by the widely confirmed confidence calibration effect\nof recently proposed techniques, we rethink the confidence reliability by evaluating\nthem on the challenging and practical failure prediction task. Surprisingly, we\nfind that they often have negative effect on failure prediction. From a detailed\nanalysis, we identify a compounding less-separability effect of training-time\ncalibration methods [31,40,48,61,70], and further find that failure prediction can\n4 F. Zhu et al.\nnot be improved by post-hoc calibration strategies like temperature scaling [15].\nFinally, inspired by the connection between flat minima [12,27,28] and confidence\nseparation, we propose to find flat minima to significantly reduce the confidence\nof misclassified samples while maintaining the confidence of correct samples.\nExtensive experiments show the strong performance of our method on both\nfailure prediction and confidence calibration.\n2 Problem Formulation and Background\nConsidering the multi-class classification problem, we assume a sample (x, y) is\ndrawn i.i.d. from an unknown joint distribution over X \u00d7Y where X = R\nd donates\nthe feature space and Y = {1, 2, ..., k} is a label space. Utilizing a standard softmax\nfunction, a deep neural network classifier f : X \u2192 Y produces a probability\ndistribution over k classes. Specifically, given an input x, f produces the predicted\nclass probabilities p\u02c6 = P\u02c6(y|x, \u03b8), where \u03b8 is the parameters of the classification\nmodel. With these probabilities, y\u02c6 = arg maxy\u2208Y P\u02c6(y|x, \u03b8) can be returned as\nthe predicted class and the associated probability p\u02c6 = maxy\u2208Y P\u02c6(y|x, \u03b8), i.e., the\nmaximum class probability, can be viewed as the predicted confidence.\n2.1 Confidence Calibration\nDefinitions and notation. Intuitively, the predictive confidence of a well\u0002calibrated model could be indicative of the actual likelihood of correctness [15].\nFor example, if a calibrated model predicts a set of inputs x to be class y with\n40% probability, then we expect 40% of the inputs indeed belong to class y.\nFormally, a model is perfectly calibrated if [15, 36]:\nP(\u02c6y = y|p\u02c6 = p\n\u2217\n) = p\n\u2217\n, \u2200p\n\u2217 \u2208",
          "original_query": "Rethinking confidence calibration for failure prediction",
          "cleaned_query": "Rethinking confidence calibration for failure prediction",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Regularizing Neural Networks by Penalizing Confident Output...",
          "url": "https://openreview.net/forum?id=HyhbYrGYe",
          "content": "[![back arrow](https://openreview.net/images/arrow_left.svg)Go to **ICLR 2017 conference** homepage](https://openreview.net/group?id=ICLR.cc/2017/conference)\n\n\u00d7\n\n## Regularizing Neural Networks by Penalizing Confident Output Distributions [![](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=HyhbYrGYe)\n\n## submission by Gabriel Pereyra \u2022 Regularizing Neural Networks by Penalizing Confident Output Distributions\n\n[Gabriel Pereyra](https://openreview.net/profile?email=pereyra%40google.com), [George Tucker](https://openreview.net/profile?email=gjt%40google.com), [Jan Chorowski](https://openreview.net/profile?email=chorowski%40google.com), [Lukasz Kaiser](https://openreview.net/profile?email=lukaszkaiser%40google.com), [Geoffrey Hinton](https://openreview.net/profile?email=geoffhinton%40google.com)\n\n15 Feb 2017 (modified: 21 Oct 2023)Submitted to ICLR 2017Readers: Everyone[Show Bibtex](https://openreview.net/forum?id=HyhbYrGYe) [Show Revisions](https://openreview.net/revisions?id=HyhbYrGYe)\n\nTL;DR: We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.\n\nAbstract: We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n\nConflicts: google.com\n\nCommunity Implementations: [![CatalyzeX](https://openreview.net/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:1701.06548/code)\n\nRevealed to Everyone\n\n* * *\n\n04 Nov 2016 (modified: 21 Oct 2023)Submitted to ICLR 2017\n\nAbstract: We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n\nKeywords: Deep learning, Supervised Learning, Speech, Structured prediction\n\n* * *\n\nReply Type:\n\nall\n\n- Select All\n- official review\n- public comment\n- acceptance\n\nAuthor:\n\neverybody\n\n- Select All\n- AnonReviewer1\n- AnonReviewer2\n- pcs\n- George Tucker\n\nVisible To:\n\nall readers\n\n- Select All\n- everyone\n\nHidden From:\n\nnobody\n\n- Select All\n- everyone\n\n4 Replies\n\n[\\[\u2013\\]](https://openreview.net/forum?id=HyhbYrGYe) [\\[+\\]](https://openreview.net/forum?id=HyhbYrGYe)\n\n## ICLR committee final decision\n\n## acceptance by pcs \u2022 ICLR committee final decision\n\nICLR 2017 pcs\n\n20 Mar 2017, 09:49ICLR 2017 workshop acceptanceReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=B1lYzOFpse)\n\nDecision: Accept\n\n[\\[\u2013\\]](https://openreview.net/forum?id=HyhbYrGYe) [\\[+\\]](https://openreview.net/forum?id=HyhbYrGYe)\n\n## Thorough evaluation of smoothing techniques, interesting introduction of confidence penalization\n\n## official review by AnonReviewer2 \u2022 Thorough evaluation of smoothing techniques, interesting introduction of confidence penalization\n\nICLR 2017 workshop AnonReviewer2\n\n10 Mar 2017, 15:29ICLR 2017 workshop official reviewReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=Hykbw3gjx)\n\nReview: The paper proposes using confidence as a term for regularization in neural networks, helping to prevent overfitting by penalizing overly confident predictions. The experiments range across a number of fields and architectures, helping to show both the generality of the technique and where it appears to be most helpful.\nThe work and experiments are rather detailed and exhaustive, especially when delving in to the Appendix for specific details of the various experiments. The confidence penalty regularization is compared to and combined with dropout and label smoothing. I do agree with another reviewer that some of the baseline systems are weaker than others. Seeing an LSTM used without recurrent dropout (variational dropout, zoneout, ...) as a baseline for language modeling is unfortunate for example. Even with that acknowledged, the results and analysis over a variety of datasets is enough to convince me of the capability of confidence penalization as a regularization technique.\nOverall, I think the paper makes a good contribution to the knowledge and application of various smoothing techniques and introduces the benefits of confidence penalization as a competing and/or complementary regularization technique. The paper is clearly written and detailed in the number and variety of experiments performed.\n\nRating: 7: Good paper, accept\n\nConfidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n\n[\\[\u2013\\]](https://openreview.net/forum?id=HyhbYrGYe) [\\[+\\]](https://openreview.net/forum?id=HyhbYrGYe)\n\n## Another softmax smoothing technique\n\n## official review by AnonReviewer1 \u2022 Another softmax smoothing technique\n\nICLR 2017 workshop AnonReviewer1\n\n08 Mar 2017, 14:36ICLR 2017 workshop official reviewReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=rkTFDWA5e)\n\nReview: The paper proposes to add a regularizing term to the objective function which penalizes the estimation of distributions with small entropy. It is one of these small tricks that were tried out by various groups even though only few people mention it in publications because the changes in performance are small and the additional hyperparameter makes it unattractive. This is also reflected in the paper here, as the improvements are very small compared to the baselines and usually vanish if more care is taking w.r.t. traditional regularization approaches.\nFurther remarks:\n\\- The evaluation is done on a broad spectrum of tasks, but the selections of the baseline systems is questionable. Especially on WSJ, there is no good reason to take an attention based seq2seq model but not also a network trained in a hybrid fashion or with CTC. Especially the CTC experiment would have been of great interest since the criterion tends to favor sharp probabilities.\n\\- A theoretical perspective on the convergence is not well established and a proper justification why this is should be able to improve neural network training is missing (except for the norms of the gradients on MNIST). If the argument is that gradients saturate too quickly if probabilities go too high then I would like to see an experiment with the squared error criterion as comparison, where this effect is not that large.\nIn total I appreciate the work and broad evaluation but would suggest to include this method in a larger comparison paper that describes several of these tricks. The paper is well written and certainly correct, and the required scope is clearly limited within a workshop. Yet I would like to see some of the points here addressed before recommending acceptance.\n\nRating: 5: Marginally below acceptance threshold\n\nConfidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature\n\n[\\[\u2013\\]](https://openreview.net/forum?id=HyhbYrGYe) [\\[+\\]](https://openreview.net/forum?id=HyhbYrGYe)\n\n## RE: Review\n\n## public comment by George Tucker \u2022 RE: Review\n\n[George Tucker](https://openreview.net/profile?id=~George_Tucker1)\n\n17 Mar 2017, 10:46 (modified: 20 Mar 2017, 07:47)ICLR 2017 workshop public commentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=Hk_xWjtoe)\n\nComment: Thank you for the careful review and helpful feedback.\n1) Yes, we agree that in some cases, more competitive baselines exist. There was a tradeoff in implementing state-of-the-art baselines with all of the bells and whistles and trying the technique across multiple domains and different model architectures. For this workshop submission, we decided it would of more interest to focus on broadly evaluating the technique, but we see the merit of the other approach too.\nPreliminary results of label smoothing with the CTC objective yielded a small improvement when no language model was used. We smoothed all non-blank tokens at all timesteps using an auxiliary cost function. However, note that unlike the seq2seq networks that directly output next-token predictions, CTC comes with its own loss function and it is not obvious how to best apply the smoothing - Do you force a smooth distribution of the non-blank tokens? Do you smooth the blank? Do you extract alignments and only smooth the emission locations, or you indiscriminately smooth all locations? Furthermore, CTC needs a language model for optimal decoding. This will need to be tuned together with smoothing. Exploring CTC and label smoothing is thus an interesting topic, but may be of more interest to a speech focused venue, and",
          "original_query": "Regularizing neural networks by penalizing confident output distributions",
          "cleaned_query": "Regularizing neural networks by penalizing confident output distributions",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Are All Losses Created Equal: A Neural Collapse Perspective - arXiv",
          "url": "https://arxiv.org/abs/2210.02192",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2210.02192** (cs)\n\n\\[Submitted on 4 Oct 2022 ( [v1](https://arxiv.org/abs/2210.02192v1)), last revised 8 Oct 2022 (this version, v2)\\]\n\n# Title:Are All Losses Created Equal: A Neural Collapse Perspective\n\nAuthors: [Jinxin Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou,+J), [Chong You](https://arxiv.org/search/cs?searchtype=author&query=You,+C), [Xiao Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+X), [Kangning Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+K), [Sheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+S), [Qing Qu](https://arxiv.org/search/cs?searchtype=author&query=Qu,+Q), [Zhihui Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu,+Z)\n\nView a PDF of the paper titled Are All Losses Created Equal: A Neural Collapse Perspective, by Jinxin Zhou and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2210.02192)\n\n> Abstract:While cross entropy (CE) is the most commonly used loss to train deep neural networks for classification tasks, many alternative losses have been developed to obtain better empirical performance. Among them, which one is the best to use is still a mystery, because there seem to be multiple factors affecting the answer, such as properties of the dataset, the choice of network architecture, and so on. This paper studies the choice of loss function by examining the last-layer features of deep networks, drawing inspiration from a recent line work showing that the global optimal solution of CE and mean-square-error (MSE) losses exhibits a Neural Collapse phenomenon. That is, for sufficiently large networks trained until convergence, (i) all features of the same class collapse to the corresponding class mean and (ii) the means associated with different classes are in a configuration where their pairwise distances are all equal and maximized. We extend such results and show through global solution and landscape analyses that a broad family of loss functions including commonly used label smoothing (LS) and focal loss (FL) exhibits Neural Collapse. Hence, all relevant losses(i.e., CE, LS, FL, MSE) produce equivalent features on training data. Based on the unconstrained feature model assumption, we provide either the global landscape analysis for LS loss or the local landscape analysis for FL loss and show that the (only!) global minimizers are neural collapse solutions, while all other critical points are strict saddles whose Hessian exhibit negative curvature directions either in the global scope for LS loss or in the local scope for FL loss near the optimal solution. The experiments further show that Neural Collapse features obtained from all relevant losses lead to largely identical performance on test data as well, provided that the network is sufficiently large and trained until convergence.\n\n| | |\n| --- | --- |\n| Comments: | 32 page, 10 figures, NeurIPS 2022 |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Theory (cs.IT); Optimization and Control (math.OC); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2210.02192](https://arxiv.org/abs/2210.02192) \\[cs.LG\\] |\n| | (or [arXiv:2210.02192v2](https://arxiv.org/abs/2210.02192v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2210.02192](https://doi.org/10.48550/arXiv.2210.02192) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jinxin Zhou \\[ [view email](https://arxiv.org/show-email/c5eac529/2210.02192)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2210.02192v1)**\nTue, 4 Oct 2022 00:36:45 UTC (8,578 KB)\n\n**\\[v2\\]**\nSat, 8 Oct 2022 22:59:07 UTC (8,576 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Are All Losses Created Equal: A Neural Collapse Perspective, by Jinxin Zhou and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2210.02192)\n- [TeX Source](https://arxiv.org/src/2210.02192)\n- [Other Formats](https://arxiv.org/format/2210.02192)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2210.02192&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2210.02192&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-10](https://arxiv.org/list/cs.LG/2022-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2210.02192?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2210.02192?context=cs.AI)\n\n[cs.IT](https://arxiv.org/abs/2210.02192?context=cs.IT)\n\n[math](https://arxiv.org/abs/2210.02192?context=math)\n\n[math.IT](https://arxiv.org/abs/2210.02192?context=math.IT)\n\n[math.OC](https://arxiv.org/abs/2210.02192?context=math.OC)\n\n[stat](https://arxiv.org/abs/2210.02192?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2210.02192?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2210.02192)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2210.02192)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2210.02192)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2210.02192&description=Are All Losses Created Equal: A Neural Collapse Perspective) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2210.02192&title=Are All Losses Created Equal: A Neural Collapse Perspective)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2210.02192) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Are all losses created equal: A neural collapse perspective",
          "cleaned_query": "Are all losses created equal: A neural collapse perspective",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Transfer-Aware Label Smoothing via Similarity-Preserving Targets\nReplace uniform label smoothing with adaptive soft targets derived from class/instance similarity (e.g., feature-space neighbors or confusion statistics) so logits preserve \u201cdark knowledge\u201d useful for distillation and transfer. Evaluate on ImageNet pretraining with linear-probe and multi-task transfer, measuring CKA changes in late layers and student distillation gap.",
        "Flat-Minima Regularization Targeted at Failure Prediction (Not Calibration)\nBuild on the \u201cflat minima helps failure prediction\u201d hypothesis by explicitly optimizing confidence separability between correct/incorrect samples (e.g., margin between max-softmax for correct vs incorrect) while using flat-minima techniques (SAM/entropy-SGD) as constraints. Benchmark against temperature scaling, mixup, label smoothing, and focal loss on risk-coverage curves and selective classification.",
        "Neural-Collapse \u201cKnob\u201d: Controlling Class Separation to Optimize Transfer\nUse neural collapse metrics (within-class variance collapse, class-mean simplex equiangularity) as explicit training-time controls: add a penalty that prevents over-collapse in the penultimate layer while keeping classifier performance high. Test whether \u201cpartial collapse\u201d improves downstream transfer without hurting in-domain accuracy, and map optimal collapse regimes per dataset size and label noise.",
        "Distillation-Compatible Confidence Penalty with Logit-Relational Preservation\nModify confidence penalty/label smoothing by adding a term that preserves pairwise logit differences among non-true classes (to retain information needed for distillation identified in M\u00fcller et al.). Train teacher models with this augmented objective and measure student performance, calibration, and penultimate-layer geometry vs standard smoothing.",
        "Selective Classification Loss that Optimizes Risk\u2013Coverage Directly\nIntroduce a differentiable surrogate for risk-coverage that trains the network to separate correct vs incorrect confidence scores (the failure prediction goal), rather than matching global calibration statistics. Compare to post-hoc calibration and flat-minima methods, and analyze whether the learned representation geometry resembles (or diverges from) neural collapse.",
        "Smoothing Schedules Driven by Representation Geometry Diagnostics (CKA/Collapse)\nImplement an automated controller that adjusts label smoothing strength \u03b1 during training based on online measurements of (i) penultimate-layer class separation and (ii) late-layer CKA drift. The contribution is a practical recipe that maintains calibration gains early while preventing the late-layer over-separation linked to poor transfer.",
        "Unified Evaluation Protocol: Calibration vs Failure Prediction vs Transfer\nCreate a benchmark suite and reporting standard that jointly evaluates (a) calibration (ECE/NLL), (b) failure prediction (AUROC, AURC, risk-coverage), (c) distillation efficacy, and (d) transfer (linear probe + few-shot + full finetune). Apply it systematically across CE/LS/confidence penalty/focal loss and flat-minima training to identify Pareto-optimal objectives.",
        "From Neural Collapse to Reliable Confidence: Geometry-Based Confidence Estimators\nDerive confidence scores from distances to class means and angles in the penultimate layer (aligned with neural collapse structure), instead of relying on softmax probabilities that calibration can distort for failure prediction. Evaluate whether geometry-based confidence improves misclassification detection and selective prediction, especially under label smoothing where softmax loses inter-class resemblance information."
      ],
      "generated_ideas_raw": "1. **Transfer-Aware Label Smoothing via Similarity-Preserving Targets**  \n   Replace uniform label smoothing with adaptive soft targets derived from class/instance similarity (e.g., feature-space neighbors or confusion statistics) so logits preserve \u201cdark knowledge\u201d useful for distillation and transfer. Evaluate on ImageNet pretraining with linear-probe and multi-task transfer, measuring CKA changes in late layers and student distillation gap.\n\n2. **Two-Phase Training to Decouple Accuracy/Calibration from Transferability**  \n   Train Inception-style models first with label smoothing/confidence penalty to reach good calibration and top-1 accuracy, then switch to a transfer-friendly objective (e.g., reduced smoothing + representation dispersion regularizer) to recover inter-class relational structure. Quantify trade-offs across (i) ECE/NLL, (ii) failure prediction AUROC, and (iii) downstream transfer with frozen features.\n\n3. **Flat-Minima Regularization Targeted at Failure Prediction (Not Calibration)**  \n   Build on the \u201cflat minima helps failure prediction\u201d hypothesis by explicitly optimizing confidence separability between correct/incorrect samples (e.g., margin between max-softmax for correct vs incorrect) while using flat-minima techniques (SAM/entropy-SGD) as constraints. Benchmark against temperature scaling, mixup, label smoothing, and focal loss on risk-coverage curves and selective classification.\n\n4. **Neural-Collapse \u201cKnob\u201d: Controlling Class Separation to Optimize Transfer**  \n   Use neural collapse metrics (within-class variance collapse, class-mean simplex equiangularity) as explicit training-time controls: add a penalty that prevents over-collapse in the penultimate layer while keeping classifier performance high. Test whether \u201cpartial collapse\u201d improves downstream transfer without hurting in-domain accuracy, and map optimal collapse regimes per dataset size and label noise.\n\n5. **Distillation-Compatible Confidence Penalty with Logit-Relational Preservation**  \n   Modify confidence penalty/label smoothing by adding a term that preserves pairwise logit differences among non-true classes (to retain information needed for distillation identified in M\u00fcller et al.). Train teacher models with this augmented objective and measure student performance, calibration, and penultimate-layer geometry vs standard smoothing.\n\n6. **Architecture\u2013Objective Co-Design: Inception Factorization for Transfer-Friendly Late Layers**  \n   Extend Inception\u2019s factorized convolutions by adding lightweight \u201clate-layer diversity\u201d branches (e.g., parallel 1\u00d71/3\u00d73 factorized heads) specifically to prevent the last few layers from over-specializing under accuracy-boosting losses. Use CKA to verify reduced late-layer drift across objectives and evaluate improvements in frozen-feature transfer on detection/segmentation.\n\n7. **Selective Classification Loss that Optimizes Risk\u2013Coverage Directly**  \n   Introduce a differentiable surrogate for risk-coverage that trains the network to separate correct vs incorrect confidence scores (the failure prediction goal), rather than matching global calibration statistics. Compare to post-hoc calibration and flat-minima methods, and analyze whether the learned representation geometry resembles (or diverges from) neural collapse.\n\n8. **Smoothing Schedules Driven by Representation Geometry Diagnostics (CKA/Collapse)**  \n   Implement an automated controller that adjusts label smoothing strength \u03b1 during training based on online measurements of (i) penultimate-layer class separation and (ii) late-layer CKA drift. The contribution is a practical recipe that maintains calibration gains early while preventing the late-layer over-separation linked to poor transfer.\n\n9. **Unified Evaluation Protocol: Calibration vs Failure Prediction vs Transfer**  \n   Create a benchmark suite and reporting standard that jointly evaluates (a) calibration (ECE/NLL), (b) failure prediction (AUROC, AURC, risk-coverage), (c) distillation efficacy, and (d) transfer (linear probe + few-shot + full finetune). Apply it systematically across CE/LS/confidence penalty/focal loss and flat-minima training to identify Pareto-optimal objectives.\n\n10. **From Neural Collapse to Reliable Confidence: Geometry-Based Confidence Estimators**  \n   Derive confidence scores from distances to class means and angles in the penultimate layer (aligned with neural collapse structure), instead of relying on softmax probabilities that calibration can distort for failure prediction. Evaluate whether geometry-based confidence improves misclassification detection and selective prediction, especially under label smoothing where softmax loses inter-class resemblance information.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Transfer-Aware Label Smoothing via Similarity-Preserving Targets\nReplace uniform label smoothing with adaptive soft targets derived from class/instance similarity (e.g., feature-space neighbors or con",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Flat-Minima Regularization Targeted at Failure Prediction (Not Calibration)\nBuild on the \u201cflat minima helps failure prediction\u201d hypothesis by explicitly optimizing confidence separability between corr",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Neural-Collapse \u201cKnob\u201d: Controlling Class Separation to Optimize Transfer\nUse neural collapse metrics (within-class variance collapse, class-mean simplex equiangularity) as explicit training-time cont",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Distillation-Compatible Confidence Penalty with Logit-Relational Preservation\nModify confidence penalty/label smoothing by adding a term that preserves pairwise logit differences among non-true classe",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Selective Classification Loss that Optimizes Risk\u2013Coverage Directly\nIntroduce a differentiable surrogate for risk-coverage that trains the network to separate correct vs incorrect confidence scores (t",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Smoothing Schedules Driven by Representation Geometry Diagnostics (CKA/Collapse)\nImplement an automated controller that adjusts label smoothing strength \u03b1 during training based on online measurements ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Unified Evaluation Protocol: Calibration vs Failure Prediction vs Transfer\nCreate a benchmark suite and reporting standard that jointly evaluates (a) calibration (ECE/NLL), (b) failure prediction (AUR",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "From Neural Collapse to Reliable Confidence: Geometry-Based Confidence Estimators\nDerive confidence scores from distances to class means and angles in the penultimate layer (aligned with neural collap",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 16,
      "paper_title": "Memory Mosaics at scale",
      "contribution": "Scaled and redesigned networks of associative key\u2013value memories (Memory Mosaics v2) that match transformers on training\u2011knowledge storage while substantially improving new\u2011task and in\u2011context learning at large scale.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 4,
      "input_tokens": 11229,
      "output_tokens": 960,
      "predecessor_details": [
        {
          "success": true,
          "title": "[2405.06394] Memory Mosaics - arXiv",
          "url": "https://arxiv.org/html/2405.06394",
          "content": "Memory Mosaics\n# Memory Mosaics\nJianyu Zhang\u2020\u21bf, Niklas Nolte\u2020, Ranajoy Sadhukhan\u2021,\nBeidi Chen\u2020\u2021, L\u00e9on Bottou\u2020\u21bf\n\u2020FAIR, Meta\u2021Carnegie Mellon University\u21bfNew York University\n(December 29, 2025)\n###### Abstract\nMemory Mosaics are networks of associative memories working in concert to achieve a prediction task of interest. Like transformers, memory mosaics possess compositional capabilities and in-context learning capabilities. Unlike transformers, memory mosaics achieve these capabilities in comparatively transparent way (\u201cpredictive disentanglement\u201d). We illustrate these capabilities on a toy example and also show that memory mosaics perform as well or better than transformers on medium-scale language modeling tasks.\n## 1Introduction\nThis paper presents a learning system architecture,*Memory Mosaics*, in which multiple associative memories work in concert to carry out a prediction task of interest. Such systems are closely related to memory networks> (Weston et al., [> 2014\n](https://arxiv.org/html/2405.06394v3#bib.bib39)> ; Sukhbaatar et al., [> 2015\n](https://arxiv.org/html/2405.06394v3#bib.bib32)> )\nand resemble transformers> (Vaswani et al., [> 2017\n](https://arxiv.org/html/2405.06394v3#bib.bib36)> )\ndespite significant differences. Like transformers, Memory Mosaics possesses some of the disentanglement and compositional capabilities that have long eluded machine learning systems> (Lake &amp; Baroni, [> 2018\n](https://arxiv.org/html/2405.06394v3#bib.bib18)> )\n. Unlike transformers whose internal mechanism are hard to decipher> (Olsson et al., [> 2022\n](https://arxiv.org/html/2405.06394v3#bib.bib21)> ; Bietti et al., [> 2024\n](https://arxiv.org/html/2405.06394v3#bib.bib6)> )\n, Memory Mosaics achieve these capabilities in comparatively transparent ways.\nThe three main contributions of this work are (a) defining an architecture that exploits the direct similarity between self-attention and associative memories implemented with kernel regression, (b) identifying and illustrating the predictive disentanglement principle which explains how training decomposes the overall task in interesting ways, and (c) showing that this comparatively transparent architecture matches thei.i.d.performance of decoding transformers on a language modeling task, and outperforms them ono.o.d.tasks such as in-context learning.\nSection[2](https://arxiv.org/html/2405.06394v3#S2)reviews related work. Section[3](https://arxiv.org/html/2405.06394v3#S3)describes simple associative memory units than can be inserted in a deep network. Section[4](https://arxiv.org/html/2405.06394v3#S4)explains how training such a network splits a prediction task into disentangled sub-tasks.\nSection[5](https://arxiv.org/html/2405.06394v3#S5)illustrates this \u201cpredictive disentanglement\u201d using a network with only 54 parameters, showing that this is not a mysterious effect of scale but a property of the architecture. Section[6](https://arxiv.org/html/2405.06394v3#S6)extends these ideas to fully formed memory mosaics. Section[7](https://arxiv.org/html/2405.06394v3#S7)reports on medium-scale language modeling experiments.\n## 2Related Work\nSeveral recent papers (e.g.,> Katharopoulos et al., [> 2020\n](https://arxiv.org/html/2405.06394v3#bib.bib16)> ; Peng et al., [> 2023\n](https://arxiv.org/html/2405.06394v3#bib.bib22)> ; Sun et al., [> 2023\n](https://arxiv.org/html/2405.06394v3#bib.bib34)> ; Gu &amp; Dao, [> 2023\n](https://arxiv.org/html/2405.06394v3#bib.bib10)) propose transformer alternatives that use efficient recurrences to cut the quadratic computational cost of transformers. Closer to our interests, other authors (e.g.,> Ramsauer et al., [> 2020\n](https://arxiv.org/html/2405.06394v3#bib.bib26)> ; Krotov, [> 2023\n](https://arxiv.org/html/2405.06394v3#bib.bib17)> ; Hoover et al., [> 2024\n](https://arxiv.org/html/2405.06394v3#bib.bib12)) rethink transformers with Hopfield-style associative memories and their associated energy function. In contrast, we leverage elementary associative memories that interpolate stored key/value pairs with a kernel regression (therefore incurring a quadratic runtime cost) in order to construct an architecture that remains very close to standard transformers but cast a new light on properties that play an important role in their compositional learning capabilities.\nClosely related to predictive disentanglement,> (Bengio et al., [> 2019\n](https://arxiv.org/html/2405.06394v3#bib.bib5)> )\nproposes a meta-learning training objective that achieves causal disentanglement by seeking quick adaptation to new distributions. We argue that a similar effect happens in our architecture, as a consequence of the normal training process interpreted as a meta-learning process, revealing an important aspect of the still mysterious compositional learning abilities of transformer-like architectures.\n## 3Memories\n#### Associative memory\nGenerally speaking, an associative memory is a device that can store key-value pairs and retrieve values given a corresponding key. This definition omits important details about dealing with duplicate keys and approximate matches. For our purposes, both keys and values shall be vectors in\u211dd\\\\mathbb{R}^{d}. The retrieval process can then be represented as a function of the queried keykkand all the stored pairs(k1,v1)\u200b\u2026\u200b(kn,vn)(k\\_{1},v\\_{1})\\\\dots(k\\_{n},v\\_{n}).\n|{\u211dd\u2192\u211ddk\u21a6f\u200b(k;{(k1,v1)\u200b\u2026\u200b(kn,vn)})\\\\left\\\\{\\\\begin{array}[]{lcl}\\\\mathbb{R}^{d}&amp;&amp;\\\\rightarrow&amp;&amp;\\\\mathbb{R}^{d}\\\\\\\\\nk&amp;&amp;\\\\mapsto&amp;&amp;f\\\\big(k;\\\\&gt;&gt;\\\\{(k\\_{1},v\\_{1})\\\\dots(k\\_{n},v\\_{n})\\\\}\\\\big)\\\\end{array}\\\\right.||\nExcept perhaps when duplicate keys are involved, an associative memory stores key-value pairs without consideration for their temporal ordering. Therefore the retrieval function can be assumed invariant with respect to any permutation of the stored pairs. This exchangeability property suggests that we can also view an associative memory as a device that estimates a conditional probability distributionP\u200b(V|K)P(V|K)on the basis of the sample(k1,v1)\u200b\u2026\u200b(kn,vn)(k\\_{1},v\\_{1})\\\\dots(k\\_{n},v\\_{n})of key-value pairs. The retrieval function is then a conditional expectation over this estimated distribution:\n|f\u200b(k;{(k1,v1)\u200b\u2026\u200b(kn,vn)})=\ud835\udd3c\u200b(V|K=k).f\\\\big(k;\\\\&gt;&gt;\\\\{(k\\_{1},v\\_{1})\\\\dots(k\\_{n},v\\_{n})\\\\}\\\\big)\\\\penalty 10000\\\\ =\\\\penalty 10000\\\\ \\\\mathbb{E}\\\\/(V\\\\&gt;&gt;|\\\\&gt;&gt;K=k)\\\\,.||(1)|\nSuch a conditional expectation can be constructed with Gaussian kernel regression,111Expression ([2](https://arxiv.org/html/2405.06394v3#S3.E2)) is known as the Nadaraya-Watson estimator> (Nadaraya, [> 1964\n](https://arxiv.org/html/2405.06394v3#bib.bib20)> ; Watson, [> 1964\n](https://arxiv.org/html/2405.06394v3#bib.bib38)> )\n. It is known to converge to the true conditional expectation\ud835\udd3c\u200b(K|V)\\\\mathbb{E}(K|V)whenn\u2192\u221en\\\\rightarrow\\\\inftyand\u03b2=n\\\\beta=\\\\sqrt{n}.\n|f\u200b(k;{(k1,v1)\u200b\u2026\u200b(kn,vn)})=\u2211i=1n1Z\u200be\u2212\u03b2\u200b\u2016k\u2212ki\u20162\u200bviwithZ=\u2211i=1ne\u2212\u03b2\u200b\u2016k\u2212ki\u20162.f\\\\big(k;\\\\&gt;&gt;\\\\{(k\\_{1},v\\_{1})\\\\dots(k\\_{n},v\\_{n})\\\\}\\\\big)\\\\penalty 10000\\\\ =\\\\penalty 10000\\\\ {\\\\sum\\_{i=1}^{n}\\\\frac{1}{Z}}\\\\penalty 10000\\\\ e^{-\\\\beta\\\\|k-k\\_{i}\\\\|^{2}}v\\_{i}\\\\penalty 10000\\\\ \\\\quad\\\\text{with}\\\\quad Z={\\\\sum\\_{i=1}^{n}}e^{-\\\\beta\\\\|k-k\\_{i}\\\\|^{2}}\\\\,.||(2)|\nThe close connection between this Gaussian kernel smoothing and attention> (Bahdanau et al., [> 2015\n](https://arxiv.org/html/2405.06394v3#bib.bib3)> )\nis obvious when all key vectorskik\\_{i}share a same squared norm because expression ([2](https://arxiv.org/html/2405.06394v3#S3.E2)) becomes\n|f\u200b(k;{(k1,v1)\u200b\u2026\u200b(kn,vn)})=\u2211i=1ne\u03b2\u200bk\u22a4\u200bki\u2211j=1ne\u03b2\u200bk\u22a4\u200bkj\u200bvi.f\\\\big(k;\\\\&gt;&gt;\\\\{(k\\_{1},v\\_{1})\\\\dots(k\\_{n},v\\_{n})\\\\}\\\\big)\\\\penalty 10000\\\\ =\\\\penalty 10000\\\\ \\\\sum\\_{i=1}^{n}\\\\penalty 10000\\\\ \\\\frac{e^{\\\\,\\\\beta\\\\,k^{\\\\!\\\\top}k\\_{i}}}{\\\\sum\\_{j=1}^{n}e^{\\\\,\\\\beta\\\\,k^{\\\\!\\\\top}k\\_{j}}}\\\\penalty 10000\\\\ v\\_{i}\\\\penalty 10000\\\\ .||(3)|\nThere are of course more advantageous ways to implement associative memories. Although some will certainly prove useful in the future, this paper only relies on associative memories implemented with Gaussian kernel smoothing, not least because that makes it easy to compute gradients.\n#### Predicting with associative memories\nConsider now a sequence(xt)(x\\_{t})of observations, discrete tokens or continuous values. We would like to leverage the past observations(xt)t\u2264T(x\\_{t})\\_{t\\\\leq T}to predict some useful property of the future observations(xt)t&gt;T(x\\_{t})\\_{t&gt;&gt;T}. For instance we might want to predict the next observationxT+1x\\_{T+1}to construct an auto-regressive model of the sequence.\n![Refer to caption](x1.png)Figure 1:Elementary memory unit. The keyskTk\\_{T}are computed as a function of past observations(xt)t\u2264T(x\\_{t})\\_{t\\\\leq T}. The valuesvTv\\_{T}peek into the future. In this example, the value also depend on the next observationxT+1x\\_{T+1}. At timeTT, the associative memory uses the known keykTk\\_{T}to compute an estimateyTy\\_{T}of\ud835\udd3c\u200b(vT|kT)\\\\mathbb{E}(v\\_{T}|k\\_{T})using only the previously stored pairs(kt,vt)(k\\_{t},v\\_{t}),t&lt;Tt&lt;T. One time step later, the inputxT+1x\\_{T+1}is revealed, the valuevTv\\_{T}can be computed, and the pair(kT,vT)(k\\_{T},v\\_{T})is added to the memory.\nOur elementary memory unit (Figure[1](https://arxiv.org/html/2405.06394v3#S3.F1)) consists of an associative memory and a trainable feature extractor that computes suitable keys and values for the memory. The keyskTk\\_{T}are computed as a function of the past observations(xt)t\u2264T(x\\_{t})\\_{t\\\\leq T}and trainable weights\ud835\udc30{\\\\mathbf{w}},\n|kT=\u03c6\u200b(xT,xT\u22121,\u2026;\ud835\udc30).k\\_{T}=\\\\varphi(x\\_{T},x\\_{T-1},\\\\dots;{\\\\mathbf{w}})\\\\,.||(4)|\nIn contrast, the valuesvTv\\_{T}are allowed to peek in the future because they represent what the memory module aims to predict. For instance, the systems described in this paper merely allow values to depend on the next observationxT+1x\\_{T+1},\n|vT=\u03c8\u200b(\ud835\udc31\ud835\udc13+\ud835\udfcf,xT,xT\u22121,\u2026;\ud835\udc30).v\\_{T}=\\\\psi({\\\\color[rgb]{.75,0,.25}\\\\mathbf{x\\_{T+1}}},x\\_{T},x\\_{T-1},\\\\dots;{\\\\mathbf{w}})\\\\,.||(5)|\nThe memory units oper",
          "original_query": "Memory Mosaics (Zhang et al., 2025)",
          "cleaned_query": "Memory Mosaics",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "[1706.03762] Attention Is All You Need - arXiv",
          "url": "https://arxiv.org/abs/1706.03762",
          "content": "[1706.03762] Attention Is All You Need[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.03762\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1706.03762**(cs)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03762v1)), last revised 2 Aug 2023 (this version, v7)]\n# Title:Attention Is All You Need\nAuthors:[Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani,+A),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+N),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Llion Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+L),[Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gomez,+A+N),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[Illia Polosukhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Polosukhin,+I)\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n[View PDF](https://arxiv.org/pdf/1706.03762)[HTML (experimental)](https://arxiv.org/html/1706.03762v7)> > Abstract:\n> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments:|15 pages, 5 figures|\nSubjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)[cs.CL]|\n|(or[arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Llion Jones [[view email](https://arxiv.org/show-email/f53b7360/1706.03762)]\n**[[v1]](https://arxiv.org/abs/1706.03762v1)**Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\n**[[v2]](https://arxiv.org/abs/1706.03762v2)**Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.03762v3)**Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\n**[[v4]](https://arxiv.org/abs/1706.03762v4)**Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\n**[[v5]](https://arxiv.org/abs/1706.03762v5)**Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\n**[[v6]](https://arxiv.org/abs/1706.03762v6)**Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\n**[v7]**Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03762)\n* [HTML (experimental)](https://arxiv.org/html/1706.03762v7)\n* [TeX Source](https://arxiv.org/src/1706.03762)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03762&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03762&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2017-06](https://arxiv.org/list/cs.CL/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03762?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.03762?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03762)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03762)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03762)\n### [123 blog links](https://arxiv.org/tb/1706.03762)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17)\n[Ashish Vaswani]()\n[Noam Shazeer]()\n[Niki Parmar]()\n[Jakob Uszkoreit]()\n[Llion Jones]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1706.03762)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Attention Is All You Need (Vaswani et al., 2017)",
          "cleaned_query": "Attention Is All You Need",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2005.14165] Language Models are Few-Shot Learners - arXiv",
          "url": "https://arxiv.org/abs/2005.14165",
          "content": "[2005.14165] Language Models are Few-Shot Learners[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2005.14165\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2005.14165**(cs)\n[Submitted on 28 May 2020 ([v1](https://arxiv.org/abs/2005.14165v1)), last revised 22 Jul 2020 (this version, v4)]\n# Title:Language Models are Few-Shot Learners\nAuthors:[Tom B. Brown](https://arxiv.org/search/cs?searchtype=author&amp;query=Brown,+T+B),[Benjamin Mann](https://arxiv.org/search/cs?searchtype=author&amp;query=Mann,+B),[Nick Ryder](https://arxiv.org/search/cs?searchtype=author&amp;query=Ryder,+N),[Melanie Subbiah](https://arxiv.org/search/cs?searchtype=author&amp;query=Subbiah,+M),[Jared Kaplan](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaplan,+J),[Prafulla Dhariwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Dhariwal,+P),[Arvind Neelakantan](https://arxiv.org/search/cs?searchtype=author&amp;query=Neelakantan,+A),[Pranav Shyam](https://arxiv.org/search/cs?searchtype=author&amp;query=Shyam,+P),[Girish Sastry](https://arxiv.org/search/cs?searchtype=author&amp;query=Sastry,+G),[Amanda Askell](https://arxiv.org/search/cs?searchtype=author&amp;query=Askell,+A),[Sandhini Agarwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+S),[Ariel Herbert-Voss](https://arxiv.org/search/cs?searchtype=author&amp;query=Herbert-Voss,+A),[Gretchen Krueger](https://arxiv.org/search/cs?searchtype=author&amp;query=Krueger,+G),[Tom Henighan](https://arxiv.org/search/cs?searchtype=author&amp;query=Henighan,+T),[Rewon Child](https://arxiv.org/search/cs?searchtype=author&amp;query=Child,+R),[Aditya Ramesh](https://arxiv.org/search/cs?searchtype=author&amp;query=Ramesh,+A),[Daniel M. Ziegler](https://arxiv.org/search/cs?searchtype=author&amp;query=Ziegler,+D+M),[Jeffrey Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+J),[Clemens Winter](https://arxiv.org/search/cs?searchtype=author&amp;query=Winter,+C),[Christopher Hesse](https://arxiv.org/search/cs?searchtype=author&amp;query=Hesse,+C),[Mark Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+M),[Eric Sigler](https://arxiv.org/search/cs?searchtype=author&amp;query=Sigler,+E),[Mateusz Litwin](https://arxiv.org/search/cs?searchtype=author&amp;query=Litwin,+M),[Scott Gray](https://arxiv.org/search/cs?searchtype=author&amp;query=Gray,+S),[Benjamin Chess](https://arxiv.org/search/cs?searchtype=author&amp;query=Chess,+B),[Jack Clark](https://arxiv.org/search/cs?searchtype=author&amp;query=Clark,+J),[Christopher Berner](https://arxiv.org/search/cs?searchtype=author&amp;query=Berner,+C),[Sam McCandlish](https://arxiv.org/search/cs?searchtype=author&amp;query=McCandlish,+S),[Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford,+A),[Ilya Sutskever](https://arxiv.org/search/cs?searchtype=author&amp;query=Sutskever,+I),[Dario Amodei](https://arxiv.org/search/cs?searchtype=author&amp;query=Amodei,+D)\nView a PDF of the paper titled Language Models are Few-Shot Learners, by Tom B. Brown and 30 other authors\n[View PDF](https://arxiv.org/pdf/2005.14165)> > Abstract:\n> Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3&#39;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. Comments:|40+32 pages|\nSubjects:|Computation and Language (cs.CL)|\nCite as:|[arXiv:2005.14165](https://arxiv.org/abs/2005.14165)[cs.CL]|\n|(or[arXiv:2005.14165v4](https://arxiv.org/abs/2005.14165v4)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2005.14165](https://doi.org/10.48550/arXiv.2005.14165)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Tom B Brown [[view email](https://arxiv.org/show-email/b5cb66e9/2005.14165)]\n**[[v1]](https://arxiv.org/abs/2005.14165v1)**Thu, 28 May 2020 17:29:03 UTC (6,995 KB)\n**[[v2]](https://arxiv.org/abs/2005.14165v2)**Mon, 1 Jun 2020 17:08:53 UTC (6,997 KB)\n**[[v3]](https://arxiv.org/abs/2005.14165v3)**Fri, 5 Jun 2020 02:52:35 UTC (6,998 KB)\n**[v4]**Wed, 22 Jul 2020 19:47:17 UTC (6,998 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Language Models are Few-Shot Learners, by Tom B. Brown and 30 other authors\n* [View PDF](https://arxiv.org/pdf/2005.14165)\n* [TeX Source](https://arxiv.org/src/2005.14165)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2005.14165&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2005.14165&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2020-05](https://arxiv.org/list/cs.CL/2020-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/2005.14165?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2005.14165)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2005.14165)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2005.14165)\n### [74 blog links](https://arxiv.org/tb/2005.14165)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2005.html#abs-2005-14165)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2005-14165)\n[Tom B. Brown]()\n[Nick Ryder]()\n[Jared Kaplan]()\n[Prafulla Dhariwal]()\n[Arvind Neelakantan]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://scie",
          "original_query": "Language Models are Few\u2011Shot Learners (Brown et al., 2020)",
          "cleaned_query": "Language Models are Few\u2011Shot Learners",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2008.02217] Hopfield Networks is All You Need - arXiv",
          "url": "https://arxiv.org/abs/2008.02217",
          "content": "# Computer Science > Neural and Evolutionary Computing\n\n**arXiv:2008.02217** (cs)\n\n\\[Submitted on 16 Jul 2020 ( [v1](https://arxiv.org/abs/2008.02217v1)), last revised 28 Apr 2021 (this version, v3)\\]\n\n# Title:Hopfield Networks is All You Need\n\nAuthors: [Hubert Ramsauer](https://arxiv.org/search/cs?searchtype=author&query=Ramsauer,+H), [Bernhard Sch\u00e4fl](https://arxiv.org/search/cs?searchtype=author&query=Sch%C3%A4fl,+B), [Johannes Lehner](https://arxiv.org/search/cs?searchtype=author&query=Lehner,+J), [Philipp Seidl](https://arxiv.org/search/cs?searchtype=author&query=Seidl,+P), [Michael Widrich](https://arxiv.org/search/cs?searchtype=author&query=Widrich,+M), [Thomas Adler](https://arxiv.org/search/cs?searchtype=author&query=Adler,+T), [Lukas Gruber](https://arxiv.org/search/cs?searchtype=author&query=Gruber,+L), [Markus Holzleitner](https://arxiv.org/search/cs?searchtype=author&query=Holzleitner,+M), [Milena Pavlovi\u0107](https://arxiv.org/search/cs?searchtype=author&query=Pavlovi%C4%87,+M), [Geir Kjetil Sandve](https://arxiv.org/search/cs?searchtype=author&query=Sandve,+G+K), [Victor Greiff](https://arxiv.org/search/cs?searchtype=author&query=Greiff,+V), [David Kreil](https://arxiv.org/search/cs?searchtype=author&query=Kreil,+D), [Michael Kopp](https://arxiv.org/search/cs?searchtype=author&query=Kopp,+M), [G\u00fcnter Klambauer](https://arxiv.org/search/cs?searchtype=author&query=Klambauer,+G), [Johannes Brandstetter](https://arxiv.org/search/cs?searchtype=author&query=Brandstetter,+J), [Sepp Hochreiter](https://arxiv.org/search/cs?searchtype=author&query=Hochreiter,+S)\n\nView a PDF of the paper titled Hopfield Networks is All You Need, by Hubert Ramsauer and 15 other authors\n\n[View PDF](https://arxiv.org/pdf/2008.02217)\n\n> Abstract:We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: [this https URL](https://github.com/ml-jku/hopfield-layers)\n\n| | |\n| --- | --- |\n| Comments: | 10 pages (+ appendix); 12 figures; Blog: [this https URL](https://ml-jku.github.io/hopfield-layers/;) GitHub: [this https URL](https://github.com/ml-jku/hopfield-layers) |\n| Subjects: | Neural and Evolutionary Computing (cs.NE); Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2008.02217](https://arxiv.org/abs/2008.02217) \\[cs.NE\\] |\n| (or [arXiv:2008.02217v3](https://arxiv.org/abs/2008.02217v3) \\[cs.NE\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2008.02217](https://doi.org/10.48550/arXiv.2008.02217) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hubert Ramsauer \\[ [view email](https://arxiv.org/show-email/9c145fe9/2008.02217)\\] **[\\[v1\\]](https://arxiv.org/abs/2008.02217v1)**\nThu, 16 Jul 2020 17:52:37 UTC (3,823 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2008.02217v2)**\nTue, 22 Dec 2020 14:16:15 UTC (15,003 KB)\n**\\[v3\\]**\nWed, 28 Apr 2021 07:24:49 UTC (15,038 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Hopfield Networks is All You Need, by Hubert Ramsauer and 15 other authors\n\n- [View PDF](https://arxiv.org/pdf/2008.02217)\n- [TeX Source](https://arxiv.org/src/2008.02217)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.NE\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2008.02217&function=prev&context=cs.NE)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2008.02217&function=next&context=cs.NE)\n\n[new](https://arxiv.org/list/cs.NE/new) \\| [recent](https://arxiv.org/list/cs.NE/recent) \\| [2020-08](https://arxiv.org/list/cs.NE/2020-08)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2008.02217?context=cs) [cs.CL](https://arxiv.org/abs/2008.02217?context=cs.CL) [cs.LG](https://arxiv.org/abs/2008.02217?context=cs.LG) [stat](https://arxiv.org/abs/2008.02217?context=stat) [stat.ML](https://arxiv.org/abs/2008.02217?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2008.02217)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2008.02217)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2008.02217)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2008.html#abs-2008-02217) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2008-02217)\n\n[Hubert Ramsauer](https://dblp.uni-trier.de/search/author?author=Hubert%20Ramsauer) [Michael Widrich](https://dblp.uni-trier.de/search/author?author=Michael%20Widrich) [Lukas Gruber](https://dblp.uni-trier.de/search/author?author=Lukas%20Gruber) [Geir Kjetil Sandve](https://dblp.uni-trier.de/search/author?author=Geir%20Kjetil%20Sandve) [Michael Kopp](https://dblp.uni-trier.de/search/author?author=Michael%20Kopp)\n\n\u2026\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2008.02217) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Hopfield Networks is All You Need (Ramsauer et al., 2020)",
          "cleaned_query": "Hopfield Networks is All You Need",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Key-Value Memory Networks for Directly Reading Documents - arXiv",
          "url": "https://arxiv.org/abs/1606.03126",
          "content": "# Computer Science > Computation and Language\n\n**arXiv:1606.03126** (cs)\n\n\\[Submitted on 9 Jun 2016 ( [v1](https://arxiv.org/abs/1606.03126v1)), last revised 10 Oct 2016 (this version, v2)\\]\n\n# Title:Key-Value Memory Networks for Directly Reading Documents\n\nAuthors: [Alexander Miller](https://arxiv.org/search/cs?searchtype=author&query=Miller,+A), [Adam Fisch](https://arxiv.org/search/cs?searchtype=author&query=Fisch,+A), [Jesse Dodge](https://arxiv.org/search/cs?searchtype=author&query=Dodge,+J), [Amir-Hossein Karimi](https://arxiv.org/search/cs?searchtype=author&query=Karimi,+A), [Antoine Bordes](https://arxiv.org/search/cs?searchtype=author&query=Bordes,+A), [Jason Weston](https://arxiv.org/search/cs?searchtype=author&query=Weston,+J)\n\nView a PDF of the paper titled Key-Value Memory Networks for Directly Reading Documents, by Alexander Miller and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/1606.03126)\n\n> Abstract:Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark.\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL) |\n| Cite as: | [arXiv:1606.03126](https://arxiv.org/abs/1606.03126) \\[cs.CL\\] |\n| | (or [arXiv:1606.03126v2](https://arxiv.org/abs/1606.03126v2) \\[cs.CL\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1606.03126](https://doi.org/10.48550/arXiv.1606.03126) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jason Weston \\[ [view email](https://arxiv.org/show-email/a67ba61d/1606.03126)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1606.03126v1)**\nThu, 9 Jun 2016 21:33:55 UTC (110 KB)\n\n**\\[v2\\]**\nMon, 10 Oct 2016 20:14:10 UTC (122 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Key-Value Memory Networks for Directly Reading Documents, by Alexander Miller and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/1606.03126)\n- [TeX Source](https://arxiv.org/src/1606.03126)\n- [Other Formats](https://arxiv.org/format/1606.03126)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1606.03126&function=prev&context=cs.CL)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1606.03126&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2016-06](https://arxiv.org/list/cs.CL/2016-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1606.03126?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1606.03126)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1606.03126)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1606.03126)\n\n### [1 blog link](https://arxiv.org/tb/1606.03126)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1606.html#MillerFDKBW16) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/MillerFDKBW16)\n\n[Alexander Miller](https://dblp.uni-trier.de/search/author?author=Alexander%20Miller)\n\n[Alexander H. Miller](https://dblp.uni-trier.de/search/author?author=Alexander%20H.%20Miller)\n\n[Adam Fisch](https://dblp.uni-trier.de/search/author?author=Adam%20Fisch)\n\n[Jesse Dodge](https://dblp.uni-trier.de/search/author?author=Jesse%20Dodge)\n\n[Amir-Hossein Karimi](https://dblp.uni-trier.de/search/author?author=Amir-Hossein%20Karimi)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1606.03126&description=Key-Value Memory Networks for Directly Reading Documents) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1606.03126&title=Key-Value Memory Networks for Directly Reading Documents)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1606.03126) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Key\u2013Value Memory Networks (Miller et al., 2016 and related works)",
          "cleaned_query": "Key\u2013Value Memory Networks",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Linear-Time Memory Mosaics via Nystr\u00f6m/Random-Feature Kernel Memories\nReplace the quadratic kernel-regression associative memories in Memory Mosaics with Nystr\u00f6m approximations or random Fourier features to obtain near-linear time retrieval while preserving predictive disentanglement. Evaluate tradeoffs between approximation rank, in-context learning accuracy, and interpretability on medium-scale language modeling and algorithmic ICL benchmarks.",
        "Disentanglement-Aware Regularization for Mosaic Heads\nIntroduce an auxiliary loss that explicitly encourages \u201cpredictive disentanglement\u201d by penalizing redundant contributions across memory modules (e.g., low mutual information between module outputs or orthogonality of retrieved value subspaces). Measure whether this yields more compositional generalization and more stable OOD in-context learning compared to both vanilla Memory Mosaics and standard multi-head attention.",
        "Energy-Guided Memory Mosaics (Hopfield-Minima Routing)\nCombine modern Hopfield energy interpretations with Memory Mosaics by using an energy-based routing mechanism that selects which memory modules to consult based on predicted convergence to (meta)stable states. Test whether \u201cmetastable subset averaging\u201d can be exploited as an explicit mechanism for selecting relevant demonstrations in few-shot prompts and for resisting distractor examples.",
        "Causal/Counterfactual Memory Modules for Robust In-Context Learning\nBuild a Mosaic where each associative memory is trained on a counterfactually-augmented view of the context (e.g., shuffled labels, swapped entities, or intervention tokens), then learn a gating function that prefers modules whose predictions remain invariant under specific interventions. This makes the architecture operationalize the link between meta-learning-style objectives and predictive disentanglement, targeting robustness to spurious correlations in ICL.",
        "Key-Value Document Reading with Mosaic Memories for Long-Context QA\nExtend Key-Value Memory Networks by replacing the single KV memory read with a Mosaic of specialized memories (entity memory, relation memory, temporal memory, and raw-span memory), each implemented as kernel-regression retrieval. Evaluate on document QA and long-context settings where transformers struggle, and quantify interpretability by attributing answers to specific memory modules and retrieved key-value pairs.",
        "Online-Write Memory Mosaics for Continual Adaptation Without Fine-Tuning\nAdd a differentiable \u201cwrite\u201d mechanism that stores new key-value pairs at inference time (e.g., new facts or user preferences) into dedicated Mosaic modules, while keeping core parameters fixed. Benchmark continual learning and personalization: measure retention, interference, and the ability to perform rapid adaptation in the few-shot paradigm without gradient updates.",
        "Module-Level Interpretability Toolkit: Retrieval Audits and Disentanglement Maps\nDevelop a standardized auditing framework for Memory Mosaics that logs retrieved neighbors, kernel weights, and per-module contribution to logits, producing \u201cdisentanglement maps\u201d over tokens and prompt examples. Use this to create quantitative interpretability metrics (e.g., sparsity of responsibility, module agreement/disagreement) and compare transparency claims against transformer attention heads.",
        "Compositional Prompt Decomposition Using Mosaic Sub-Tasks\nTrain Memory Mosaics with an objective that predicts not only next-token logits but also an explicit decomposition of the prompt into sub-tasks (e.g., instruction parsing, schema induction, label mapping) aligned to different memories. Evaluate whether enforcing alignment between module roles and prompt structure improves systematic generalization on compositional benchmarks (SCAN-like, synthetic grammar tasks, multi-step instruction following).",
        "Demonstration Selection as Kernel Design: Learned Similarity for Few-Shot Retrieval\nTreat in-context learning performance as primarily governed by the kernel used in associative memory retrieval, and learn task-adaptive kernels that emphasize example relevance (e.g., label-consistency, semantic type matching, or reasoning-step similarity). Implement as a meta-learned kernel network inside Memory Mosaics and test on few-shot benchmarks with adversarially inserted irrelevant demonstrations to quantify improved selectivity."
      ],
      "generated_ideas_raw": "1. **Linear-Time Memory Mosaics via Nystr\u00f6m/Random-Feature Kernel Memories**  \n   Replace the quadratic kernel-regression associative memories in Memory Mosaics with Nystr\u00f6m approximations or random Fourier features to obtain near-linear time retrieval while preserving predictive disentanglement. Evaluate tradeoffs between approximation rank, in-context learning accuracy, and interpretability on medium-scale language modeling and algorithmic ICL benchmarks.\n\n2. **Disentanglement-Aware Regularization for Mosaic Heads**  \n   Introduce an auxiliary loss that explicitly encourages \u201cpredictive disentanglement\u201d by penalizing redundant contributions across memory modules (e.g., low mutual information between module outputs or orthogonality of retrieved value subspaces). Measure whether this yields more compositional generalization and more stable OOD in-context learning compared to both vanilla Memory Mosaics and standard multi-head attention.\n\n3. **Energy-Guided Memory Mosaics (Hopfield-Minima Routing)**  \n   Combine modern Hopfield energy interpretations with Memory Mosaics by using an energy-based routing mechanism that selects which memory modules to consult based on predicted convergence to (meta)stable states. Test whether \u201cmetastable subset averaging\u201d can be exploited as an explicit mechanism for selecting relevant demonstrations in few-shot prompts and for resisting distractor examples.\n\n4. **Causal/Counterfactual Memory Modules for Robust In-Context Learning**  \n   Build a Mosaic where each associative memory is trained on a counterfactually-augmented view of the context (e.g., shuffled labels, swapped entities, or intervention tokens), then learn a gating function that prefers modules whose predictions remain invariant under specific interventions. This makes the architecture operationalize the link between meta-learning-style objectives and predictive disentanglement, targeting robustness to spurious correlations in ICL.\n\n5. **Key-Value Document Reading with Mosaic Memories for Long-Context QA**  \n   Extend Key-Value Memory Networks by replacing the single KV memory read with a Mosaic of specialized memories (entity memory, relation memory, temporal memory, and raw-span memory), each implemented as kernel-regression retrieval. Evaluate on document QA and long-context settings where transformers struggle, and quantify interpretability by attributing answers to specific memory modules and retrieved key-value pairs.\n\n6. **Online-Write Memory Mosaics for Continual Adaptation Without Fine-Tuning**  \n   Add a differentiable \u201cwrite\u201d mechanism that stores new key-value pairs at inference time (e.g., new facts or user preferences) into dedicated Mosaic modules, while keeping core parameters fixed. Benchmark continual learning and personalization: measure retention, interference, and the ability to perform rapid adaptation in the few-shot paradigm without gradient updates.\n\n7. **Module-Level Interpretability Toolkit: Retrieval Audits and Disentanglement Maps**  \n   Develop a standardized auditing framework for Memory Mosaics that logs retrieved neighbors, kernel weights, and per-module contribution to logits, producing \u201cdisentanglement maps\u201d over tokens and prompt examples. Use this to create quantitative interpretability metrics (e.g., sparsity of responsibility, module agreement/disagreement) and compare transparency claims against transformer attention heads.\n\n8. **Compositional Prompt Decomposition Using Mosaic Sub-Tasks**  \n   Train Memory Mosaics with an objective that predicts not only next-token logits but also an explicit decomposition of the prompt into sub-tasks (e.g., instruction parsing, schema induction, label mapping) aligned to different memories. Evaluate whether enforcing alignment between module roles and prompt structure improves systematic generalization on compositional benchmarks (SCAN-like, synthetic grammar tasks, multi-step instruction following).\n\n9. **Hybrid Transformer\u2013Mosaic Architectures with Drop-In Memory Layers**  \n   Create a plug-and-play layer that can replace a subset of transformer attention layers with Mosaic associative memories while keeping the rest of the transformer intact (same residual/MLP structure). Run controlled ablations on language modeling and few-shot tasks to identify where Mosaic layers provide the largest OOD gains, and whether they reduce required scale to achieve GPT-3-like few-shot behavior.\n\n10. **Demonstration Selection as Kernel Design: Learned Similarity for Few-Shot Retrieval**  \n   Treat in-context learning performance as primarily governed by the kernel used in associative memory retrieval, and learn task-adaptive kernels that emphasize example relevance (e.g., label-consistency, semantic type matching, or reasoning-step similarity). Implement as a meta-learned kernel network inside Memory Mosaics and test on few-shot benchmarks with adversarially inserted irrelevant demonstrations to quantify improved selectivity.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Linear-Time Memory Mosaics via Nystr\u00f6m/Random-Feature Kernel Memories\nReplace the quadratic kernel-regression associative memories in Memory Mosaics with Nystr\u00f6m approximations or random Fourier featu",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Disentanglement-Aware Regularization for Mosaic Heads\nIntroduce an auxiliary loss that explicitly encourages \u201cpredictive disentanglement\u201d by penalizing redundant contributions across memory modules (e",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Energy-Guided Memory Mosaics (Hopfield-Minima Routing)\nCombine modern Hopfield energy interpretations with Memory Mosaics by using an energy-based routing mechanism that selects which memory modules t",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Causal/Counterfactual Memory Modules for Robust In-Context Learning\nBuild a Mosaic where each associative memory is trained on a counterfactually-augmented view of the context (e.g., shuffled labels, ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Key-Value Document Reading with Mosaic Memories for Long-Context QA\nExtend Key-Value Memory Networks by replacing the single KV memory read with a Mosaic of specialized memories (entity memory, relati",
          "is_match": true
        },
        {
          "idea_idx": 5,
          "idea_text": "Online-Write Memory Mosaics for Continual Adaptation Without Fine-Tuning\nAdd a differentiable \u201cwrite\u201d mechanism that stores new key-value pairs at inference time (e.g., new facts or user preferences) ",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Module-Level Interpretability Toolkit: Retrieval Audits and Disentanglement Maps\nDevelop a standardized auditing framework for Memory Mosaics that logs retrieved neighbors, kernel weights, and per-mod",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Compositional Prompt Decomposition Using Mosaic Sub-Tasks\nTrain Memory Mosaics with an objective that predicts not only next-token logits but also an explicit decomposition of the prompt into sub-task",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Demonstration Selection as Kernel Design: Learned Similarity for Few-Shot Retrieval\nTreat in-context learning performance as primarily governed by the kernel used in associative memory retrieval, and ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 17,
      "paper_title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
      "contribution": "Shows that sparse-attention circuits emerge as predictable phase-transitions in training dynamics driven by task structure, optimizer/architecture choices, and data distribution\u2014and that repeating examples can dramatically accelerate this emergence.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 12919,
      "output_tokens": 1089,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1706.03762] Attention Is All You Need - arXiv",
          "url": "https://arxiv.org/abs/1706.03762",
          "content": "[1706.03762] Attention Is All You Need[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.03762\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1706.03762**(cs)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03762v1)), last revised 2 Aug 2023 (this version, v7)]\n# Title:Attention Is All You Need\nAuthors:[Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani,+A),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+N),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Llion Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+L),[Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gomez,+A+N),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[Illia Polosukhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Polosukhin,+I)\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n[View PDF](https://arxiv.org/pdf/1706.03762)[HTML (experimental)](https://arxiv.org/html/1706.03762v7)> > Abstract:\n> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments:|15 pages, 5 figures|\nSubjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)[cs.CL]|\n|(or[arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Llion Jones [[view email](https://arxiv.org/show-email/f53b7360/1706.03762)]\n**[[v1]](https://arxiv.org/abs/1706.03762v1)**Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\n**[[v2]](https://arxiv.org/abs/1706.03762v2)**Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.03762v3)**Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\n**[[v4]](https://arxiv.org/abs/1706.03762v4)**Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\n**[[v5]](https://arxiv.org/abs/1706.03762v5)**Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\n**[[v6]](https://arxiv.org/abs/1706.03762v6)**Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\n**[v7]**Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03762)\n* [HTML (experimental)](https://arxiv.org/html/1706.03762v7)\n* [TeX Source](https://arxiv.org/src/1706.03762)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03762&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03762&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2017-06](https://arxiv.org/list/cs.CL/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03762?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.03762?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03762)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03762)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03762)\n### [123 blog links](https://arxiv.org/tb/1706.03762)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17)\n[Ashish Vaswani]()\n[Noam Shazeer]()\n[Niki Parmar]()\n[Jakob Uszkoreit]()\n[Llion Jones]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1706.03762)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Attention is all you need",
          "cleaned_query": "Attention is all you need",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "In-context Learning and Induction Heads - Transformer Circuits Thread",
          "url": "https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html",
          "content": "In-context Learning and Induction Heads\n[](https://anthropic.com)[Transformer Circuits Thread](https://transformer-circuits.pub/)\n# In-context Learning and Induction Heads\n### Authors\nCatherine Olsson\u2217,[Nelson Elhage\u2217](https://nelhage.com/),[Neel Nanda\u2217](https://www.neelnanda.io/),Nicholas Joseph\u2020,Nova DasSarma\u2020,Tom Henighan\u2020,[Ben Mann\u2020](https://benjmann.net/),Amanda Askell,Yuntao Bai,Anna Chen,Tom Conerly,Dawn Drain,Deep Ganguli,[Zac Hatfield-Dodds](https://zhd.dev),Danny Hernandez,Scott Johnston,Andy Jones,Jackson Kernion,Liane Lovitt,Kamal Ndousse,Dario Amodei,Tom Brown,Jack Clark,Jared Kaplan,Sam McCandlish,[Chris Olah\u2021](https://colah.github.io/)\n### Affiliation\n[Anthropic](https://www.anthropic.com/)\n### Published\nMar 8, 2022\n\\* Core Research Contributor;\u2020 Core Infrastructure Contributor;\u2021 Correspondence to[colah@anthropic.com](colah@anthropic.com);[Author contributions statement below](#author-contributions).\nAs Transformer generative models continue to scale and gain increasing real world use,\u00a0addressing their associated safety problems becomes increasingly important.Mechanistic interpretability\u2013 attempting to reverse engineer the detailed computations performed by the model \u2013offers one possible avenue for addressing these safety issues. If we can understand the internal structures that cause Transformer models to produce the outputs they do, then we may be able to address current safety problems more systematically, as well as anticipating safety problems in future more powerful models.Note thatmechanistic interpretabilityis a subset of the broader field ofinterpretability, which encompasses many different methods for explaining the outputs of a neural network. Mechanistic interpretability is distinguished by a specific focus on trying to systematically characterize the internal circuitry of a neural net.\nIn the past, mechanistic interpretability has largely focused on CNN vision models, but recently, we[presented](https://transformer-circuits.pub/2021/framework/index.html)some very preliminary progress on mechanistic interpretability for Transformer language models\u200b\u200b. Specifically, in our prior work we developed a mathematical framework for decomposing the operations of transformers, which allowed us to make sense of small (1 and 2 layer attention-only) models and give a near-complete account of how they function. Perhaps the most interesting finding was theinduction head, a circuit whose function is to look back over the sequence for previous instances of the current token (call it`A`), find the token that came after it last time (call it`B`), and then predict that the same completion will occur again (e.g. forming the sequence`[A][B] \u2026[A] \u2192[B]`). In other words, induction heads \u201ccomplete the pattern\u201d by copying and completing sequences that have occurred before. Mechanically, induction heads in our models are implemented by a circuit of two attention heads: the first head is a \u201cprevious token head\u201d which copies information from the previous token into the next token, while the second head (the actual \u201cinduction head\u201d) uses that information to find tokens preceded by the present token. For 2-layer attention-only models,Note that induction heads don\u2019t occur in 1 layer models, because they require a composition of attention heads in different layers.we were able to show precisely that induction heads implement this pattern copying behavior and appear to be the primary source of in-context learning.\nUltimately, however, our goal is to reverse-engineer frontier language models (which often contain hundreds of layers and billions or trillions of parameters), not merely 2-layer attention-only models. Unfortunately, both the presence of many layers, and the presence of MLPs, makes it much more difficult to mathematically pin down the precise circuitry of these models. However, a different approach is possible: by empirically observing, perturbing, and studying the learning process and the formation of various structures, we can try to assemble anindirectcase for what might be happening mechanistically inside the network. This is somewhat similar to how a neuroscientist might gain understanding of how part of the brain functions by looking at neural development over time, studying patients with an injury to that part of the brain, perturbing brain function in animals, or looking at a select small number of relevant neurons.\nIn this paper, we take the first preliminary steps towards building such an indirect case. In particular, we present preliminary and indirect evidence for a tantalizing hypothesis:that induction heads might constitute the mechanism for the actual majority of all in-context learning in large transformer models. Specifically, the thesis is that there are circuits which have the same or similar mechanism to the 2-layer induction heads and which perform a \u201cfuzzy\u201d or \u201cnearest neighbor\u201d version of pattern completion, completing`[A\\*][B\\*] \u2026[A] \u2192[B]`, where`A\\* \u2248A`and`B\\* \u2248B`are similar in some space; and furthermore, that these circuits implement most in-context learning in large models.\nThe primary way in which we obtain this evidence is via discovery and study of aphase changethat occurs early in training for language models of every size (provided they have more than one layer), and which is visible as a bump in the training loss. During this phase change, the majority of in-context learning ability (as measured by difference in loss between tokens early and late in the sequence) is acquired, and simultaneously induction heads form within the model that are capable of implementing fairly abstract and fuzzy versions of pattern completion. We study this connection in detail to try to establish that it is causal, including showing that if we perturb the transformer architecture in a way that causes the induction bump to occur in a different place in training, then the formation of induction headsas well asformation of in-context learning simultaneously move along with it.\nSpecifically, the paper presents six complementary lines of evidence arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size:\n* [Argument 1](#argument-phase-change)(Macroscopic co-occurence):\u00a0Transformer language models undergo a \u201cphase change\u201d early in training, during which induction heads form and simultaneously in-context learning improves dramatically.\n* [Argument 2](#argument-architectural-requirements)(Macroscopic co-perturbation):When we change the transformer architecture in a way that shifts whether induction heads can form (and when), the dramatic improvement in in-context learning shifts in a precisely matching way.\n* [Argument 3](#argument-ablations)(Direct ablation):When we directly \u201cknock out\u201d induction heads at test-time in small models, the amount of in-context learning greatly decreases.\n* [Argument 4](#argument-induction-heads-can-implement-abstract-behaviors)(Specific examples of induction head generality): Although we define induction heads very narrowly in terms of copying literal sequences, we empirically observe that these same heads also appear to implement more sophisticated types of in-context learning, including highly abstract behaviors, making it plausible they explain a large fraction of in-context learning.\n* [Argument 5](#argument-induction-head-mechanism)(Mechanistic plausibility of induction head generality):For small models, we can explain mechanistically how induction heads work, and can show they contribute to in-context learning. Furthermore, the actual mechanism of operation suggests natural ways in which it could be re-purposed to perform more general in-context learning.\n* [Argument 6](#argument-extrapolation)(Continuity from small to large models): In the previous 5 arguments, the case for induction heads explaining in-context learning is stronger for small models than for large ones. However, many behaviors and data related to both induction heads and in-context learning are smoothly continuous from small to large models, suggesting the simplest explanation is that mechanisms are the same.\nTogether the claims establish a circumstantial case that induction headsmightbe responsible for the majority of in-context learning in state-of-the-art transformer models. We emphasize that our results here are only the beginnings of evidence for such a case, and that like any empirical or interventional study, a large number of subtle confounds or alternative hypotheses are possible \u2013which we discuss in the relevant sections. But we considered these results worth reporting, both because future work could build on our results to establish the claim more firmly, and because this kind of indirect evidence is likely to be common in interpretability as it advances, so we\u2019d like to establish a norm of reporting it even when it is not fully conclusive.\nFinally, in addition to being instrumental for tying induction heads to in-context learning, the phase change may have relevance to safety in its own right. Neural network capabilities \u2014such as multi-digit addition \u2014are known to sometimes abruptly form or change as models train or increase in scale, and are of particular concern for safety as they mean that undesired or dangerous behavior could emerge abruptly. For example reward hacking, a type of safety problem, can emerge in such a phase change. Thus, studying a phase change \u201cup close\u201d and better understanding its internal mechanics could contain generalizable lessons for addressing safety problems in future systems. In particular, the phase change we observe forms an interesting potential bridge between the microscopic domain of interpretability and the macroscopic domain of scaling laws and learning dynamics.\nThe rest of the paper is organized as follows. We start by clarifying several key concepts and definitions, including in-context learning, induction heads, and a \u201cper-token loss analysis\u201d method we use throughout. We then presen",
          "original_query": "In\u2011context learning and induction heads. Transformer circuits thread, 2022",
          "cleaned_query": "In\u2011context learning and induction heads. Transformer circuits thread, 2022",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Exact solutions to the nonlinear dynamics of learning in deep linear ...",
          "url": "https://arxiv.org/abs/1312.6120",
          "content": "# Computer Science > Neural and Evolutionary Computing\n\n**arXiv:1312.6120** (cs)\n\n\\[Submitted on 20 Dec 2013 ( [v1](https://arxiv.org/abs/1312.6120v1)), last revised 19 Feb 2014 (this version, v3)\\]\n\n# Title:Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\n\nAuthors: [Andrew M. Saxe](https://arxiv.org/search/cs?searchtype=author&query=Saxe,+A+M), [James L. McClelland](https://arxiv.org/search/cs?searchtype=author&query=McClelland,+J+L), [Surya Ganguli](https://arxiv.org/search/cs?searchtype=author&query=Ganguli,+S)\n\nView a PDF of the paper titled Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, by Andrew M. Saxe and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/1312.6120)\n\n> Abstract:Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.\n\n| | |\n| --- | --- |\n| Comments: | Submission to ICLR2014. Revised based on reviewer feedback |\n| Subjects: | Neural and Evolutionary Computing (cs.NE); Disordered Systems and Neural Networks (cond-mat.dis-nn); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1312.6120](https://arxiv.org/abs/1312.6120) \\[cs.NE\\] |\n| (or [arXiv:1312.6120v3](https://arxiv.org/abs/1312.6120v3) \\[cs.NE\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1312.6120](https://doi.org/10.48550/arXiv.1312.6120) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Andrew Saxe \\[ [view email](https://arxiv.org/show-email/b8d5672b/1312.6120)\\] **[\\[v1\\]](https://arxiv.org/abs/1312.6120v1)**\nFri, 20 Dec 2013 20:24:00 UTC (249 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1312.6120v2)**\nFri, 24 Jan 2014 20:39:04 UTC (422 KB)\n**\\[v3\\]**\nWed, 19 Feb 2014 17:26:57 UTC (424 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, by Andrew M. Saxe and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/1312.6120)\n- [TeX Source](https://arxiv.org/src/1312.6120)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.NE\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1312.6120&function=prev&context=cs.NE)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1312.6120&function=next&context=cs.NE)\n\n[new](https://arxiv.org/list/cs.NE/new) \\| [recent](https://arxiv.org/list/cs.NE/recent) \\| [2013-12](https://arxiv.org/list/cs.NE/2013-12)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/1312.6120?context=cond-mat) [cond-mat.dis-nn](https://arxiv.org/abs/1312.6120?context=cond-mat.dis-nn) [cs](https://arxiv.org/abs/1312.6120?context=cs) [cs.CV](https://arxiv.org/abs/1312.6120?context=cs.CV) [cs.LG](https://arxiv.org/abs/1312.6120?context=cs.LG) [q-bio](https://arxiv.org/abs/1312.6120?context=q-bio) [q-bio.NC](https://arxiv.org/abs/1312.6120?context=q-bio.NC) [stat](https://arxiv.org/abs/1312.6120?context=stat) [stat.ML](https://arxiv.org/abs/1312.6120?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1312.6120)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1312.6120)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1312.6120)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/conf/iclr/iclr2014.html#SaxeMG13) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/conf/iclr/SaxeMG13)\n\n[Andrew M. Saxe](https://dblp.uni-trier.de/search/author?author=Andrew%20M.%20Saxe) [James L. McClelland](https://dblp.uni-trier.de/search/author?author=James%20L.%20McClelland) [Surya Ganguli](https://dblp.uni-trier.de/search/author?author=Surya%20Ganguli)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1312.6120) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
          "cleaned_query": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Data Distributional Properties Drive Emergent In-Context Learning ...",
          "url": "https://arxiv.org/abs/2205.05055",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2205.05055** (cs)\n\n\\[Submitted on 22 Apr 2022 ( [v1](https://arxiv.org/abs/2205.05055v1)), last revised 17 Nov 2022 (this version, v6)\\]\n\n# Title:Data Distributional Properties Drive Emergent In-Context Learning in Transformers\n\nAuthors: [Stephanie C.Y. Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan,+S+C), [Adam Santoro](https://arxiv.org/search/cs?searchtype=author&query=Santoro,+A), [Andrew K. Lampinen](https://arxiv.org/search/cs?searchtype=author&query=Lampinen,+A+K), [Jane X. Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J+X), [Aaditya Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh,+A), [Pierre H. Richemond](https://arxiv.org/search/cs?searchtype=author&query=Richemond,+P+H), [Jay McClelland](https://arxiv.org/search/cs?searchtype=author&query=McClelland,+J), [Felix Hill](https://arxiv.org/search/cs?searchtype=author&query=Hill,+F)\n\nView a PDF of the paper titled Data Distributional Properties Drive Emergent In-Context Learning in Transformers, by Stephanie C.Y. Chan and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2205.05055)\n\n> Abstract:Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.\n\n| | |\n| --- | --- |\n| Comments: | Accepted at NeurIPS 2022 (Oral). Code is available at: [this https URL](https://github.com/deepmind/emergent_in_context_learning) |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |\n| Cite as: | [arXiv:2205.05055](https://arxiv.org/abs/2205.05055) \\[cs.LG\\] |\n| (or [arXiv:2205.05055v6](https://arxiv.org/abs/2205.05055v6) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2205.05055](https://doi.org/10.48550/arXiv.2205.05055) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Stephanie Chan \\[ [view email](https://arxiv.org/show-email/c28765c7/2205.05055)\\] **[\\[v1\\]](https://arxiv.org/abs/2205.05055v1)**\nFri, 22 Apr 2022 16:10:50 UTC (1,215 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2205.05055v2)**\nThu, 12 May 2022 08:11:29 UTC (1,215 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2205.05055v3)**\nMon, 23 May 2022 09:58:47 UTC (2,148 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2205.05055v4)**\nMon, 30 May 2022 11:39:30 UTC (2,148 KB)\n**[\\[v5\\]](https://arxiv.org/abs/2205.05055v5)**\nMon, 10 Oct 2022 18:57:58 UTC (1,250 KB)\n**\\[v6\\]**\nThu, 17 Nov 2022 18:42:14 UTC (1,250 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Data Distributional Properties Drive Emergent In-Context Learning in Transformers, by Stephanie C.Y. Chan and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2205.05055)\n- [TeX Source](https://arxiv.org/src/2205.05055)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2205.05055&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2205.05055&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-05](https://arxiv.org/list/cs.LG/2022-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2205.05055?context=cs) [cs.AI](https://arxiv.org/abs/2205.05055?context=cs.AI) [cs.CL](https://arxiv.org/abs/2205.05055?context=cs.CL)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2205.05055)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2205.05055)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2205.05055)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2205.05055) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Data distributional properties drive emergent in\u2011context learning in transformers",
          "cleaned_query": "Data distributional properties drive emergent in\u2011context learning in transformers",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Attention layers provably solve single-location regression - arXiv",
          "url": "https://arxiv.org/abs/2410.01537",
          "content": "# Statistics > Machine Learning\n\n**arXiv:2410.01537** (stat)\n\n\\[Submitted on 2 Oct 2024 ( [v1](https://arxiv.org/abs/2410.01537v1)), last revised 25 Feb 2025 (this version, v2)\\]\n\n# Title:Attention layers provably solve single-location regression\n\nAuthors: [Pierre Marion](https://arxiv.org/search/stat?searchtype=author&query=Marion,+P), [Rapha\u00ebl Berthier](https://arxiv.org/search/stat?searchtype=author&query=Berthier,+R), [G\u00e9rard Biau](https://arxiv.org/search/stat?searchtype=author&query=Biau,+G), [Claire Boyer](https://arxiv.org/search/stat?searchtype=author&query=Boyer,+C)\n\nView a PDF of the paper titled Attention layers provably solve single-location regression, by Pierre Marion and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2410.01537) [HTML (experimental)](https://arxiv.org/html/2410.01537v2)\n\n> Abstract:Attention-based models, such as Transformer, excel across various tasks but lack a comprehensive theoretical understanding, especially regarding token-wise sparsity and internal linear representations. To address this gap, we introduce the single-location regression task, where only one token in a sequence determines the output, and its position is a latent random variable, retrievable via a linear projection of the input. To solve this task, we propose a dedicated predictor, which turns out to be a simplified version of a non-linear self-attention layer. We study its theoretical properties, by showing its asymptotic Bayes optimality and analyzing its training dynamics. In particular, despite the non-convex nature of the problem, the predictor effectively learns the underlying structure. This work highlights the capacity of attention mechanisms to handle sparse token information and internal linear structures.\n\n| | |\n| --- | --- |\n| Comments: | 42 pages, 10 figures. Accepted to ICLR 2025 |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2410.01537](https://arxiv.org/abs/2410.01537) \\[stat.ML\\] |\n| | (or [arXiv:2410.01537v2](https://arxiv.org/abs/2410.01537v2) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2410.01537](https://doi.org/10.48550/arXiv.2410.01537) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Pierre Marion \\[ [view email](https://arxiv.org/show-email/ca21f5a3/2410.01537)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2410.01537v1)**\nWed, 2 Oct 2024 13:28:02 UTC (1,501 KB)\n\n**\\[v2\\]**\nTue, 25 Feb 2025 23:15:41 UTC (2,165 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Attention layers provably solve single-location regression, by Pierre Marion and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2410.01537)\n- [HTML (experimental)](https://arxiv.org/html/2410.01537v2)\n- [TeX Source](https://arxiv.org/src/2410.01537)\n- [Other Formats](https://arxiv.org/format/2410.01537)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2410.01537&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2410.01537&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2024-10](https://arxiv.org/list/stat.ML/2024-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2410.01537?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2410.01537?context=cs.LG)\n\n[stat](https://arxiv.org/abs/2410.01537?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.01537)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.01537)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.01537)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2410.01537&description=Attention layers provably solve single-location regression) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2410.01537&title=Attention layers provably solve single-location regression)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2410.01537) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Attention layers provably solve single\u2011location regression",
          "cleaned_query": "Attention layers provably solve single\u2011location regression",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2410.07041] Emergent properties with repeated examples - arXiv",
          "url": "https://arxiv.org/abs/2410.07041",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2410.07041** (cs)\n\n\\[Submitted on 9 Oct 2024\\]\n\n# Title:Emergent properties with repeated examples\n\nAuthors: [Fran\u00e7ois Charton](https://arxiv.org/search/cs?searchtype=author&query=Charton,+F), [Julia Kempe](https://arxiv.org/search/cs?searchtype=author&query=Kempe,+J)\n\nView a PDF of the paper titled Emergent properties with repeated examples, by Fran\\\\c{c}ois Charton and Julia Kempe\n\n[View PDF](https://arxiv.org/pdf/2410.07041) [HTML (experimental)](https://arxiv.org/html/2410.07041v1)\n\n> Abstract:We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2410.07041](https://arxiv.org/abs/2410.07041) \\[cs.LG\\] |\n| | (or [arXiv:2410.07041v1](https://arxiv.org/abs/2410.07041v1) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2410.07041](https://doi.org/10.48550/arXiv.2410.07041) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Fran\u00e7ois Charton \\[ [view email](https://arxiv.org/show-email/80175a4c/2410.07041)\\]\n\n**\\[v1\\]**\nWed, 9 Oct 2024 16:28:23 UTC (3,077 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Emergent properties with repeated examples, by Fran\\\\c{c}ois Charton and Julia Kempe\n\n- [View PDF](https://arxiv.org/pdf/2410.07041)\n- [HTML (experimental)](https://arxiv.org/html/2410.07041v1)\n- [TeX Source](https://arxiv.org/src/2410.07041)\n- [Other Formats](https://arxiv.org/format/2410.07041)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2410.07041&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2410.07041&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2024-10](https://arxiv.org/list/cs.LG/2024-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2410.07041?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2410.07041?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.07041)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.07041)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.07041)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2410.07041&description=Emergent properties with repeated examples) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2410.07041&title=Emergent properties with repeated examples)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2410.07041) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Emergent properties with repeated examples",
          "cleaned_query": "Emergent properties with repeated examples",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Controlling Induction-Head Phase Transitions via Data Burstiness Schedules\nDesign curriculum schedules that systematically vary burstiness/Zipf-skew over training (per Chan et al.) and measure how they shift the \u201cinduction bump\u201d timing and induction-head formation (per Olsson et al.). The key contribution is a causal mapping from distributional knobs (burstiness, rare-class mass) to the emergence time, strength, and diversity of induction circuits, with practical recipes to elicit (or suppress) in-context learning on demand.",
        "Mechanistic Link Between Repetition-Based Training and Induction Circuits in Algorithmic Tasks\nReplicate Charton & Kempe\u2019s repeated-example regimes on algorithmic datasets, then perform head-level circuit analysis to test whether performance gains are mediated by earlier/stronger induction-head formation. The contribution is an evidence-backed explanation of why repetition can beat diversity: repetition may act as a catalyst for specific attention-head compositions that implement copying and pattern completion.",
        "From Single-Location Regression to Multi-Location \u201cSparse Set\u201d Regression with Provable Attention Solutions\nExtend Marion et al.\u2019s single latent relevant token to a setting where \\(k\\) tokens jointly determine the output (unknown positions, possibly interacting), and propose a minimal multi-head attention architecture that provably recovers the set and aggregates their values. The contribution is a new theory result characterizing when attention can reliably solve sparse token-wise tasks, bridging token sparsity with multi-head specialization.",
        "Analytical Training Dynamics of Simplified Induction Heads Using Deep-Linear Methods\nConstruct a linearized or attention-only proxy model whose forward map is linear but whose parameters mimic the two-head induction composition, then apply Saxe et al.-style exact gradient-flow analysis to predict plateau lengths and rapid transitions. The contribution is a mathematically tractable account of \u201cinduction bump\u201d-like phenomena, yielding closed-form predictions for how depth, initialization, and learning rate affect induction emergence.",
        "Induction-Head \u201cFuzziness\u201d as Learned Nearest-Neighbor Retrieval: A Quantitative Geometry Study\nOperationalize Olsson et al.\u2019s \u201cfuzzy induction\u201d by measuring the embedding-space neighborhood structure that induction heads attend to (e.g., top-k similarity mass, anisotropy, and semantic clustering) across training. The contribution is a concrete metric suite and empirical results linking representation geometry to the degree of approximate pattern completion, clarifying when induction behaves like kNN retrieval versus exact copying.",
        "Two-Set Training Meets Zipf: Joint Optimization of In-Context and In-Weights Learning\nCombine Charton & Kempe\u2019s two-set training (small repeated subset + fresh stream) with Chan et al.\u2019s finding that Zipfian skew enables coexistence of in-context and weight-based learning. The contribution is a controlled study (and practical algorithm) that tunes repetition rate and skew to jointly maximize fast in-context adaptation and long-term parametric generalization, instead of trading them off.",
        "Architectural Perturbations that Selectively Promote Previous-Token Heads vs Induction Heads\nIntroduce targeted Transformer modifications (e.g., constrained attention patterns, head-wise positional biases, or layerwise key/value mixing restrictions) to separate the formation of \u201cprevious token heads\u201d from downstream induction heads (Olsson et al.). The contribution is a mechanistic dissection of which architectural degrees of freedom are necessary for head composition, enabling more interpretable and controllable in-context learning pathways.",
        "A Benchmark Suite of Latent-Location Tasks that Interpolate Between Sparse Regression and In-Context Learning\nCreate synthetic tasks that smoothly vary from single-location regression (Marion et al.) to repeated-pattern completion tasks that require induction (Olsson et al.), with tunable burstiness/rare-class structure (Chan et al.). The contribution is a standardized evaluation framework that pinpoints where attention transitions from \u201cfind the one relevant token\u201d to \u201cinfer a rule from context,\u201d supporting apples-to-apples comparisons across models and training regimes.",
        "Head-Level Interventions to Test Causality of Induction for In-Context Learning Under Naturalistic Distributions\nTrain models under the distributional conditions known to induce ICL (burstiness, Zipf skew), then perform surgical interventions: ablate, clamp, or resample attention outputs of identified induction heads and measure the drop in ICL metrics (Olsson et al.). The contribution is a rigorous causal test\u2014beyond correlation\u2014of whether induction heads explain most ICL under the very data regimes that generate it.",
        "Initialization Strategies that Accelerate Induction Emergence Without Sacrificing Stability\nInspired by Saxe et al.\u2019s special initial conditions and orthogonal initializations, develop head- and layer-specific initializations for Transformers that aim to reduce the delay before induction heads form (earlier induction bump) while preserving training stability. The contribution is a principled initialization recipe evaluated via both mechanistic readouts (induction-head diagnostics) and downstream ICL performance, potentially reducing compute needed to obtain strong in-context learners."
      ],
      "generated_ideas_raw": "1. **Controlling Induction-Head Phase Transitions via Data Burstiness Schedules**  \nDesign curriculum schedules that systematically vary burstiness/Zipf-skew over training (per Chan et al.) and measure how they shift the \u201cinduction bump\u201d timing and induction-head formation (per Olsson et al.). The key contribution is a causal mapping from distributional knobs (burstiness, rare-class mass) to the emergence time, strength, and diversity of induction circuits, with practical recipes to elicit (or suppress) in-context learning on demand.\n\n2. **Mechanistic Link Between Repetition-Based Training and Induction Circuits in Algorithmic Tasks**  \nReplicate Charton & Kempe\u2019s repeated-example regimes on algorithmic datasets, then perform head-level circuit analysis to test whether performance gains are mediated by earlier/stronger induction-head formation. The contribution is an evidence-backed explanation of why repetition can beat diversity: repetition may act as a catalyst for specific attention-head compositions that implement copying and pattern completion.\n\n3. **From Single-Location Regression to Multi-Location \u201cSparse Set\u201d Regression with Provable Attention Solutions**  \nExtend Marion et al.\u2019s single latent relevant token to a setting where \\(k\\) tokens jointly determine the output (unknown positions, possibly interacting), and propose a minimal multi-head attention architecture that provably recovers the set and aggregates their values. The contribution is a new theory result characterizing when attention can reliably solve sparse token-wise tasks, bridging token sparsity with multi-head specialization.\n\n4. **Analytical Training Dynamics of Simplified Induction Heads Using Deep-Linear Methods**  \nConstruct a linearized or attention-only proxy model whose forward map is linear but whose parameters mimic the two-head induction composition, then apply Saxe et al.-style exact gradient-flow analysis to predict plateau lengths and rapid transitions. The contribution is a mathematically tractable account of \u201cinduction bump\u201d-like phenomena, yielding closed-form predictions for how depth, initialization, and learning rate affect induction emergence.\n\n5. **Induction-Head \u201cFuzziness\u201d as Learned Nearest-Neighbor Retrieval: A Quantitative Geometry Study**  \nOperationalize Olsson et al.\u2019s \u201cfuzzy induction\u201d by measuring the embedding-space neighborhood structure that induction heads attend to (e.g., top-k similarity mass, anisotropy, and semantic clustering) across training. The contribution is a concrete metric suite and empirical results linking representation geometry to the degree of approximate pattern completion, clarifying when induction behaves like kNN retrieval versus exact copying.\n\n6. **Two-Set Training Meets Zipf: Joint Optimization of In-Context and In-Weights Learning**  \nCombine Charton & Kempe\u2019s two-set training (small repeated subset + fresh stream) with Chan et al.\u2019s finding that Zipfian skew enables coexistence of in-context and weight-based learning. The contribution is a controlled study (and practical algorithm) that tunes repetition rate and skew to jointly maximize fast in-context adaptation and long-term parametric generalization, instead of trading them off.\n\n7. **Architectural Perturbations that Selectively Promote Previous-Token Heads vs Induction Heads**  \nIntroduce targeted Transformer modifications (e.g., constrained attention patterns, head-wise positional biases, or layerwise key/value mixing restrictions) to separate the formation of \u201cprevious token heads\u201d from downstream induction heads (Olsson et al.). The contribution is a mechanistic dissection of which architectural degrees of freedom are necessary for head composition, enabling more interpretable and controllable in-context learning pathways.\n\n8. **A Benchmark Suite of Latent-Location Tasks that Interpolate Between Sparse Regression and In-Context Learning**  \nCreate synthetic tasks that smoothly vary from single-location regression (Marion et al.) to repeated-pattern completion tasks that require induction (Olsson et al.), with tunable burstiness/rare-class structure (Chan et al.). The contribution is a standardized evaluation framework that pinpoints where attention transitions from \u201cfind the one relevant token\u201d to \u201cinfer a rule from context,\u201d supporting apples-to-apples comparisons across models and training regimes.\n\n9. **Head-Level Interventions to Test Causality of Induction for In-Context Learning Under Naturalistic Distributions**  \nTrain models under the distributional conditions known to induce ICL (burstiness, Zipf skew), then perform surgical interventions: ablate, clamp, or resample attention outputs of identified induction heads and measure the drop in ICL metrics (Olsson et al.). The contribution is a rigorous causal test\u2014beyond correlation\u2014of whether induction heads explain most ICL under the very data regimes that generate it.\n\n10. **Initialization Strategies that Accelerate Induction Emergence Without Sacrificing Stability**  \nInspired by Saxe et al.\u2019s special initial conditions and orthogonal initializations, develop head- and layer-specific initializations for Transformers that aim to reduce the delay before induction heads form (earlier induction bump) while preserving training stability. The contribution is a principled initialization recipe evaluated via both mechanistic readouts (induction-head diagnostics) and downstream ICL performance, potentially reducing compute needed to obtain strong in-context learners.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Controlling Induction-Head Phase Transitions via Data Burstiness Schedules\nDesign curriculum schedules that systematically vary burstiness/Zipf-skew over training (per Chan et al.) and measure how the",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Mechanistic Link Between Repetition-Based Training and Induction Circuits in Algorithmic Tasks\nReplicate Charton & Kempe\u2019s repeated-example regimes on algorithmic datasets, then perform head-level cir",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "From Single-Location Regression to Multi-Location \u201cSparse Set\u201d Regression with Provable Attention Solutions\nExtend Marion et al.\u2019s single latent relevant token to a setting where \\(k\\) tokens jointly ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Analytical Training Dynamics of Simplified Induction Heads Using Deep-Linear Methods\nConstruct a linearized or attention-only proxy model whose forward map is linear but whose parameters mimic the two",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Induction-Head \u201cFuzziness\u201d as Learned Nearest-Neighbor Retrieval: A Quantitative Geometry Study\nOperationalize Olsson et al.\u2019s \u201cfuzzy induction\u201d by measuring the embedding-space neighborhood structure",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Two-Set Training Meets Zipf: Joint Optimization of In-Context and In-Weights Learning\nCombine Charton & Kempe\u2019s two-set training (small repeated subset + fresh stream) with Chan et al.\u2019s finding that ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Architectural Perturbations that Selectively Promote Previous-Token Heads vs Induction Heads\nIntroduce targeted Transformer modifications (e.g., constrained attention patterns, head-wise positional bi",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "A Benchmark Suite of Latent-Location Tasks that Interpolate Between Sparse Regression and In-Context Learning\nCreate synthetic tasks that smoothly vary from single-location regression (Marion et al.) ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Head-Level Interventions to Test Causality of Induction for In-Context Learning Under Naturalistic Distributions\nTrain models under the distributional conditions known to induce ICL (burstiness, Zipf ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Initialization Strategies that Accelerate Induction Emergence Without Sacrificing Stability\nInspired by Saxe et al.\u2019s special initial conditions and orthogonal initializations, develop head- and layer",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 18,
      "paper_title": "ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts",
      "contribution": "Introduce a prompt-modulated restoration-and-fusion network trained on physically simulated composite degradations that uses language-vision degradation prompts plus a spatial-frequency visual adapter to produce controllable, degradation-robust infrared-visible image fusion.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12574,
      "output_tokens": 989,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Learning Transferable Visual Models From Natural Language ...",
          "url": "https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf",
          "content": "Learning Transferable Visual Models From Natural Language Supervision\nAlec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1 Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1\nGirish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1Ilya Sutskever 1\nAbstract\nState-of-the-art computer vision systems are\ntrained to predict a fixed set of predetermined\nobject categories. This restricted form of super\u0002vision limits their generality and usability since\nadditional labeled data is needed to specify any\nother visual concept. Learning directly from raw\ntext about images is a promising alternative which\nleverages a much broader source of supervision.\nWe demonstrate that the simple pre-training task\nof predicting which caption goes with which im\u0002age is an efficient and scalable way to learn SOTA\nimage representations from scratch on a dataset\nof 400 million (image, text) pairs collected from\nthe internet. After pre-training, natural language\nis used to reference learned visual concepts (or\ndescribe new ones) enabling zero-shot transfer\nof the model to downstream tasks. We study\nthe performance of this approach by benchmark\u0002ing on over 30 different existing computer vi\u0002sion datasets, spanning tasks such as OCR, action\nrecognition in videos, geo-localization, and many\ntypes of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is\noften competitive with a fully supervised baseline\nwithout the need for any dataset specific training.\nFor instance, we match the accuracy of the orig\u0002inal ResNet-50 on ImageNet zero-shot without\nneeding to use any of the 1.28 million training\nexamples it was trained on.\n1. Introduction and Motivating Work\nPre-training methods which learn directly from raw text\nhave revolutionized NLP over the last few years (Dai &\nLe, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad\u0002ford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).\nTask-agnostic objectives such as autoregressive and masked\nlanguage modeling have scaled across many orders of mag-\n*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\nCorrespondence to: <{alec, jongwook}@openai.com>.\nnitude in compute, model capacity, and data, steadily im\u0002proving capabilities. The development of \u201ctext-to-text\u201d as\na standardized input-output interface (McCann et al., 2018;\nRadford et al., 2019; Raffel et al., 2019) has enabled task\u0002agnostic architectures to zero-shot transfer to downstream\ndatasets removing the need for specialized output heads or\ndataset specific customization. Flagship systems like GPT-3\n(Brown et al., 2020) are now competitive across many tasks\nwith bespoke models while requiring little to no dataset\nspecific training data.\nThese results suggest that the aggregate supervision acces\u0002sible to modern pre-training methods within web-scale col\u0002lections of text surpasses that of high-quality crowd-labeled\nNLP datasets. However, in other fields such as computer\nvision it is still standard practice to pre-train models on\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\nCould scalable pre-training methods which learn directly\nfrom web text result in a similar breakthrough in computer\nvision? Prior work is encouraging.\nOver 20 years ago Mori et al. (1999) explored improving\ncontent based image retrieval by training a model to pre\u0002dict the nouns and adjectives in text documents paired with\nimages. Quattoni et al. (2007) demonstrated it was possi\u0002ble to learn more data efficient image representations via\nmanifold learning in the weight space of classifiers trained\nto predict words in captions associated with images. Sri\u0002vastava & Salakhutdinov (2012) explored deep represen\u0002tation learning by training multimodal Deep Boltzmann\nMachines on top of low-level image and text tag feature\nfeatures. Joulin et al. (2016) modernized this line of work\nand demonstrated that CNNs trained to predict words in\nimage captions learn useful image representations. They\nconverted the title, description, and hashtag metadata of im\u0002ages in the YFCC100M dataset (Thomee et al., 2016) into\na bag-of-words multi-label classification task and showed\nthat pre-training AlexNet (Krizhevsky et al., 2012) to pre\u0002dict these labels learned representations which preformed\nsimilarly to ImageNet-based pre-training on transfer tasks.\nLi et al. (2017) then extended this approach to predicting\nphrase n-grams in addition to individual words and demon\u0002strated the ability of their system to zero-shot transfer to\nother image classification datasets by scoring target classes\nbased on their dictionary of learned visual n-grams and\nLearning Transferable Visual Models From Natural Language Supervision 2\nI1\u00b7T2I1\u00b7T3 \u2026\nI2\u00b7T1I2\u00b7T3 \u2026\nI3\u00b7T1I3\u00b7T2 \u2026\n\u22ee \u22ee \u22ee\nI1\u00b7T1\nI2\u00b7T2\nI3\u00b7T3\n(1) Contrastive pre-training\nImage\nEncoder\nText\nEncoder Pepper the\naussie pup\nT1 T2 T3 \u2026\nI1\nI2\nI3\n\u22ee\n(2) Create dataset classifier from label text\nplane\ncar\ndog\n\u22ee\nbird\nA photo of\na {object}.\n\u22ee\nText\nEncoder\nT1 T2 T3 TN\n\u2026\n(3) Use for zero-shot prediction\nImage\nEncoder\nI1I1\u00b7T2I1I1\u00b7T1\u00b7TN\n\u2026\n\u2026\nA photo of\n a dog.\nTN\nIN\u00b7T1IN\u00b7T2IN\u00b7T3\nI1\u00b7TN\nI2\u00b7TN\nI3\u00b7TN\n\u22ee\nIN \u2026\n\u2026\n\u22ee \u22f1\nIN\u00b7TN\nI1\u00b7T3\nFigure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\nexamples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the\ntarget dataset\u2019s classes.\npredicting the one with the highest score. Adopting more\nrecent architectures and pre-training approaches, VirTex\n(Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al.,\n2020), and ConVIRT (Zhang et al., 2020) have recently\ndemonstrated the potential of transformer-based language\nmodeling, masked language modeling, and contrastive ob\u0002jectives to learn image representations from text.\nWhile exciting as proofs of concept, using natural language\nsupervision for image representation learning is still rare.\nThis is likely because demonstrated performance on com\u0002mon benchmarks is much lower than alternative approaches.\nFor example, Li et al. (2017) reach only 11.5% accuracy\non ImageNet in a zero-shot setting. This is well below the\n88.4% accuracy of the current state of the art (Xie et al.,\n2020). It is even below the 50% accuracy of classic com\u0002puter vision approaches (Deng et al., 2012). Instead, more\nnarrowly scoped but well-targeted uses of weak supervision\nhave improved performance. Mahajan et al. (2018) showed\nthat predicting ImageNet related hashtags on Instagram im\u0002ages is an effective pre-training task. When fine-tuned to\nImageNet these pre-trained models increased accuracy by\nover 5% and improved the overall state of the art at the time.\nKolesnikov et al. (2019) and Dosovitskiy et al. (2020) have\nalso demonstrated large gains on a broader set of transfer\nbenchmarks by pre-training models to predict the classes of\nthe noisily labeled JFT-300M dataset.\nThis line of work represents the current pragmatic middle\nground between learning from a limited amount of super\u0002vised \u201cgold-labels\u201d and learning from practically unlimited\namounts of raw text. However, it is not without compro\u0002mises. Both works carefully design, and in the process limit,\ntheir supervision to 1000 and 18291 classes respectively.\nNatural language is able to express, and therefore supervise,\na much wider set of visual concepts through its general\u0002ity. Both approaches also use static softmax classifiers to\nperform prediction and lack a mechanism for dynamic out\u0002puts. This severely curtails their flexibility and limits their\n\u201czero-shot\u201d capabilities.\nA crucial difference between these weakly supervised mod\u0002els and recent explorations of learning image representations\ndirectly from natural language is scale. While Mahajan et al.\n(2018) and Kolesnikov et al. (2019) trained their models for\naccelerator years on millions to billions of images, VirTex,\nICMLM, and ConVIRT trained for accelerator days on one\nto two hundred thousand images. In this work, we close\nthis gap and study the behaviors of image classifiers trained\nwith natural language supervision at large scale. Enabled\nby the large amounts of publicly available data of this form\non the internet, we create a new dataset of 400 million (im\u0002age, text) pairs and demonstrate that a simplified version of\nConVIRT trained from scratch, which we call CLIP, for Con\u0002trastive Language-Image Pre-training, is an efficient method\nof learning from natural language supervision. We study\nthe scalability of CLIP by training a series of eight models\nspanning almost 2 orders of magnitude of compute and ob\u0002serve that transfer performance is a smoothly predictable\nfunction of compute (Hestness et al., 2017; Kaplan et al.,\n2020). We find that CLIP, similar to the GPT family, learns\nto perform a wide set of tasks during pre-training including\nOCR, geo-localization, action recognition, and many others.\nWe measure this by benchmarking the zero-shot transfer\nperformance of CLIP on over 30 existing datasets and find\nit can be competitive with prior task-specific supervised\nmodels. We also confirm these findings with linear-probe\nLearning Transferable Visual Models From Natural Language Supervision 3\nrepresentation learning analysis and show that CLIP out\u0002performs the best publicly available ImageNet model while\nalso being more computationally efficient. We additionally\nfind that zero-shot CLIP models are much more robust than\nequivalent accuracy supervised ImageNet models which\nsuggests that zero-shot evaluation of task-agnostic models is\nmuch more representative of a model\u2019s capability. These re\u0002sults have significant policy and ethical implications, which\nwe consider in Section 7.\n2M 33M 67M 134M 268M 400M\n# of images processed\n0\n5\n10\n15\n20\n25\n30\n35\n40\nZero-Shot ImageNet Accuracy\n4X efficiency 3X efficiency\nBag of Words Contrastive (CLIP)\nBag of Words Prediction\nTransformer Lan",
          "original_query": "Learning transferable visual models from natural language supervision",
          "cleaned_query": "Learning transferable visual models from natural language supervision",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2403.16387] Text-IF: Leveraging Semantic Text Guidance ...",
          "url": "https://arxiv.org/abs/2403.16387",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2403.16387** (cs)\n\n\\[Submitted on 25 Mar 2024\\]\n\n# Title:Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion\n\nAuthors: [Xunpeng Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi,+X), [Han Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+H), [Hao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+H), [Linfeng Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+L), [Jiayi Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+J)\n\nView a PDF of the paper titled Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion, by Xunpeng Yi and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2403.16387) [HTML (experimental)](https://arxiv.org/html/2403.16387v1)\n\n> Abstract:Image fusion aims to combine information from different source images to create a comprehensively representative image. Existing fusion methods are typically helpless in dealing with degradations in low-quality source images and non-interactive to multiple subjective and objective needs. To solve them, we introduce a novel approach that leverages semantic text guidance image fusion model for degradation-aware and interactive image fusion task, termed as Text-IF. It innovatively extends the classical image fusion to the text guided image fusion along with the ability to harmoniously address the degradation and interaction issues during fusion. Through the text semantic encoder and semantic interaction fusion decoder, Text-IF is accessible to the all-in-one infrared and visible image degradation-aware processing and the interactive flexible fusion outcomes. In this way, Text-IF achieves not only multi-modal image fusion, but also multi-modal information fusion. Extensive experiments prove that our proposed text guided image fusion strategy has obvious advantages over SOTA methods in the image fusion performance and degradation treatment. The code is available at [this https URL](https://github.com/XunpengYi/Text-IF).\n\n| | |\n| --- | --- |\n| Comments: | Accepted by CVPR 2024 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2403.16387](https://arxiv.org/abs/2403.16387) \\[cs.CV\\] |\n| | (or [arXiv:2403.16387v1](https://arxiv.org/abs/2403.16387v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2403.16387](https://doi.org/10.48550/arXiv.2403.16387) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Xunpeng Yi \\[ [view email](https://arxiv.org/show-email/9573e6fd/2403.16387)\\]\n\n**\\[v1\\]**\nMon, 25 Mar 2024 03:06:45 UTC (12,949 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion, by Xunpeng Yi and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2403.16387)\n- [HTML (experimental)](https://arxiv.org/html/2403.16387v1)\n- [TeX Source](https://arxiv.org/src/2403.16387)\n- [Other Formats](https://arxiv.org/format/2403.16387)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2403.16387&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2403.16387&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-03](https://arxiv.org/list/cs.CV/2024-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2403.16387?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2403.16387)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2403.16387)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2403.16387)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2403.16387&description=Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2403.16387&title=Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2403.16387) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Text-IF: Leveraging semantic text guidance for degradation-aware and interactive image fusion",
          "cleaned_query": "Text-IF: Leveraging semantic text guidance for degradation-aware and interactive image fusion",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "PromptIR: Prompting for All-in-One Blind Image Restoration - arXiv",
          "url": "https://arxiv.org/abs/2306.13090",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2306.13090** (cs)\n\n\\[Submitted on 22 Jun 2023\\]\n\n# Title:PromptIR: Prompting for All-in-One Blind Image Restoration\n\nAuthors: [Vaishnav Potlapalli](https://arxiv.org/search/cs?searchtype=author&query=Potlapalli,+V), [Syed Waqas Zamir](https://arxiv.org/search/cs?searchtype=author&query=Zamir,+S+W), [Salman Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan,+S), [Fahad Shahbaz Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan,+F+S)\n\nView a PDF of the paper titled PromptIR: Prompting for All-in-One Blind Image Restoration, by Vaishnav Potlapalli and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2306.13090)\n\n> Abstract:Image restoration involves recovering a high-quality clean image from its degraded version. Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels. This restricts their real-world application since it requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model. We present a prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation. In particular, our method uses prompts to encode degradation-specific information, which is then used to dynamically guide the restoration network. This allows our method to generalize to different degradation types and levels, while still achieving state-of-the-art results on image denoising, deraining, and dehazing. Overall, PromptIR offers a generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image. Our code and pretrained models are available here: [this https URL](https://github.com/va1shn9v/PromptIR)\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2306.13090](https://arxiv.org/abs/2306.13090) \\[cs.CV\\] |\n| | (or [arXiv:2306.13090v1](https://arxiv.org/abs/2306.13090v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2306.13090](https://doi.org/10.48550/arXiv.2306.13090) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Vaishnav Potlapalli \\[ [view email](https://arxiv.org/show-email/e858f0dc/2306.13090)\\]\n\n**\\[v1\\]**\nThu, 22 Jun 2023 17:59:52 UTC (39,008 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled PromptIR: Prompting for All-in-One Blind Image Restoration, by Vaishnav Potlapalli and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2306.13090)\n- [TeX Source](https://arxiv.org/src/2306.13090)\n- [Other Formats](https://arxiv.org/format/2306.13090)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2306.13090&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2306.13090&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2023-06](https://arxiv.org/list/cs.CV/2023-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2306.13090?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2306.13090)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2306.13090)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2306.13090)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2306.13090&description=PromptIR: Prompting for All-in-One Blind Image Restoration) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2306.13090&title=PromptIR: Prompting for All-in-One Blind Image Restoration)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2306.13090) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "PromptIR: Prompting for all-in-one image restoration",
          "cleaned_query": "PromptIR: Prompting for all-in-one image restoration",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "DRMF: Degradation-Robust Multi-Modal Image Fusion via ...",
          "url": "https://dl.acm.org/doi/10.1145/3664647.3681064",
          "content": "DRMF: Degradation-Robust Multi-Modal Image Fusion via Composable Diffusion Prior | Proceedings of the 32nd ACM International Conference on Multimedia[skip to main content](#skip-to-main-content)\n[](#global-menu)\nSearch ACM Digital Library\nSearchSearch\n[Advanced Search](https://dl.acm.org/search/advanced)\n10.1145/3664647.3681064acmconferencesArticle/Chapter ViewAbstractPublication PagesmmConference Proceedingsconference-collections\n[mm](#)\n**## Export Citations\nSelect Citation formatBibTeXEndNoteACM Ref**\n* Please download or close your previous search result export first before starting a new bulk export.\nPreview is not available.\nBy clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.\n* ```\n```\n* [Download citation**](javascript:void(0))\n* [Copy citation**](javascript:void(0))\nresearch-article\nShare on\n* **\n* **\n* **\n* **\n* **\n# DRMF: Degradation-Robust Multi-Modal Image Fusion via Composable Diffusion Prior\nAuthors:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)LinfengTang](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)YuxinDeng](#artseq-00002),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)XunpengYi](#artseq-00003),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)QinglongYan](#artseq-00004),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)YixuanYuan](#artseq-00005),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)JiayiMa](#artseq-00006)[Authors Info &amp; Claims](#tab-contributors)\n[MM '24: Proceedings of the 32nd ACM International Conference on Multimedia](https://dl.acm.org/doi/proceedings/10.1145/3664647)\nPages8546-8555\n[https://doi.org/10.1145/3664647.3681064](https://doi.org/10.1145/3664647.3681064)\nPublished:28 October 2024[Publication History](#core-history)[](#)\n**15citation**2,871Downloads\nMetrics\n[\nTotal Citations15\n](#tab-citations)[\nTotal Downloads2,871\n](#tab-metrics-inner)\nLast 12 Months2,727\nLast 6 weeks244\n**Get Citation Alerts\n**## New Citation Alert added!\nThis alert has been successfully added and will be sent to:\nYou will be notified whenever a record that you have chosen has been cited.\nTo manage your alert preferences, click on the button below.\n[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)\n**## New Citation Alert!\nPlease[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.1145/3664647.3681064)\n**\n**\n[**Get Access](#core-collateral-purchase-access)\n**Contents\n## Abstract\nExisting multi-modal image fusion algorithms are typically designed for high-quality images and fail to tackle degradation (*e.g.,*low light, low resolution, and noise), which restricts image fusion from unleashing the potential in practice. In this work, we present**D**egradation-**R**obust**M**ulti-modality image**F**usion (**DRMF**), leveraging the powerful generative properties of diffusion models to counteract various degradations during image fusion. Our critical insight is that generative diffusion models driven by different modalities and degradation are inherently complementary during the denoising process. Specifically, we pre-train multiple degradation-robust conditional diffusion models for different modalities to handle degradations. Subsequently, the diffusion priori combination module is devised to integrate generative priors from pre-trained uni-modal models, enabling effective multi-modal image fusion. Extensive experiments demonstrate that DRMF excels in infrared-visible and medical image fusion, even under complex degradations. Our code is available at https://github.com/Linfeng-Tang/DRMF.\n## Supplemental Material\nMP4 File - DRMF: Degradation-Robust Multi-Modal Image Fusion via Composable Diffusion Prior\nVideo presentation of our conference paper, DRMF: Degradation-Robust Multi-Modal Image Fusion via Composable Diffusion Prior, presented at ACM MM24. Video showing the motivation, methodology, and experimentation of our DRMF.\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3664647.3681064/suppl_file/ftp2367-video.mp4)\n* 87.35 MB\n## References\n[1]\nFanglin Bao, Xueji Wang, Shree Hari Sureshbabu, Gautam Sreekumar, Liping Yang, Vaneet Aggarwal, Vishnu N Boddeti, and Zubin Jacob. 2023. Heat-assisted detection and ranging. Nature, Vol. 619, 7971 (2023), 743--748.\n[Google Scholar](https://scholar.google.com/scholar?q=Fanglin+Bao,+Xueji+Wang,+Shree+Hari+Sureshbabu,+Gautam+Sreekumar,+Liping+Yang,+Vaneet+Aggarwal,+Vishnu+N+Boddeti,+and+Zubin+Jacob.+2023.+Heat-assisted+detection+and+ranging.+Nature,+Vol.+619,+7971+(2023),+743--748.)\n[2]\nZheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xin Yuan, et al. 2022. Cross aggregation transformer for image restoration. Advances in Neural Information Processing Systems, Vol. 35 (2022), 25478--25490.\n[Google Scholar](https://scholar.google.com/scholar?q=Zheng+Chen,+Yulun+Zhang,+Jinjin+Gu,+Linghe+Kong,+Xin+Yuan,+et+al.+2022.+Cross+aggregation+transformer+for+image+restoration.+Advances+in+Neural+Information+Processing+Systems,+Vol.+35+(2022),+25478--25490.)\n[3]\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, Vol. 34 (2021), 8780--8794.\n[Google Scholar](https://scholar.google.com/scholar?q=Prafulla+Dhariwal+and+Alexander+Nichol.+2021.+Diffusion+models+beat+gans+on+image+synthesis.+Advances+in+Neural+Information+Processing+Systems,+Vol.+34+(2021),+8780--8794.)\n[4]\nAhmet M Eskicioglu and Paul S Fisher. 1995. Image quality measures and their performance. IEEE Transactions on Communications, Vol. 43, 12 (1995), 2959--2965.\n[Crossref](https://doi.org/10.1109/26.477498)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109/26.477498)\n[5]\nBen Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai. 2023. Generative diffusion prior for unified image restoration and enhancement. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 9935--9946.\n[Crossref](https://doi.org/10.1109/CVPR52729.2023.00958)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109/CVPR52729.2023.00958)\n[6]\nYu Han, Yunze Cai, Yin Cao, and Xiaoming Xu. 2013. A new image fusion performance metric based on visual information fidelity. Information Fusion, Vol. 14, 2 (2013), 127--135.\n[Digital Library](https://dl.acm.org/doi/10.1016/j.inffus.2011.08.002)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016/j.inffus.2011.08.002)\n[7]\nHaithem Hermessi, Olfa Mourali, and Ezzeddine Zagrouba. 2021. Multimodal medical image fusion review: Theoretical background and recent advances. Signal Processing, Vol. 183 (2021), 108036.\n[Crossref](https://doi.org/10.1016/j.sigpro.2021.108036)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016/j.sigpro.2021.108036)\n[8]\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, Vol. 33 (2020), 6840--6851.\n[Google Scholar](https://scholar.google.com/scholar?q=Jonathan+Ho,+Ajay+Jain,+and+Pieter+Abbeel.+2020.+Denoising+diffusion+probabilistic+models.+Advances+in+Neural+Information+Processing+Systems,+Vol.+33+(2020),+6840--6851.)\n[9]\nZiqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. 2023. Collaborative diffusion for multi-modal face generation and editing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 6080--6090.\n[Crossref](https://doi.org/10.1109/CVPR52729.2023.00589)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109/CVPR52729.2023.00589)\n[10]\nDeepak Kumar Jain, Xudong Zhao, Germ\u00e1n Gonz\u00e1lez-Almagro, Chenquan Gan, and Ketan Kotecha. 2023. Multimodal pedestrian detection using metaheuristics with deep convolutional neural network in crowded scenes. Information Fusion, Vol. 95 (2023), 401--414.\n[Digital Library](https://dl.acm.org/doi/10.1016/j.inffus.2023.02.014)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016/j.inffus.2023.02.014)\n[11]\nXinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and Wenli Zhou. 2021. LLVIP: A visible-infrared paired dataset for low-light vision. In Proceedings of the IEEE International Conference on Computer Vision. 3496--3504.\n[Crossref](https://doi.org/10.1109/ICCVW54120.2021.00389)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109/ICCVW54120.2021.00389)\n[12]\nHai Jiang, Ao Luo, Haoqiang Fan, Songchen Han, and Shuaicheng Liu. 2023. Low-light image enhancement with wavelet-based diffusion models. ACM Transactions on Graphics, Vol. 42, 6 (2023), 1--14.\n[Digital Library](https://dl.acm.org/doi/10.1145/3618320)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3618320)\n[13]\nShahid Karim, Geng Tong, Jinyang Li, Akeel Qadir, Umar Farooq, and Yiting Yu. 2023. Current advances and future perspectives of image fusion: A comprehensive review. Information Fusion, Vol. 90 (2023), 185--217.\n[Digital Library](https://dl.acm.org/doi/10.1016/j.inffus.2022.09.019)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016/j.inffus.2022.09.019)\n[14]\nJunjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. 2021. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE International Conference on Computer Vision. 5148--5157.\n[Crossref](https://doi.org/10.1109/ICCV48922.2021.00510)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109/ICCV48922.2021.00510)\n[15]\nGwanghyun Kim, Taesung Kwon, and Jong Chul Ye. 2022. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE Conference on Computer Vision and Patte",
          "original_query": "DRMF: Degradation-robust multi-modal image fusion via composable diffusion prior",
          "cleaned_query": "DRMF: Degradation-robust multi-modal image fusion via composable diffusion prior",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2410.23905] Text-DiFuse: An Interactive Multi-Modal Image Fusion ...",
          "url": "https://arxiv.org/abs/2410.23905",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2410.23905** (cs)\n\n\\[Submitted on 31 Oct 2024\\]\n\n# Title:Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model\n\nAuthors: [Hao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+H), [Lei Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao,+L), [Jiayi Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+J)\n\nView a PDF of the paper titled Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model, by Hao Zhang and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2410.23905) [HTML (experimental)](https://arxiv.org/html/2410.23905v1)\n\n> Abstract:Existing multi-modal image fusion methods fail to address the compound degradations presented in source images, resulting in fusion images plagued by noise, color bias, improper exposure, \\\\textit{etc}. Additionally, these methods often overlook the specificity of foreground objects, weakening the salience of the objects of interest within the fused images. To address these challenges, this study proposes a novel interactive multi-modal image fusion framework based on the text-modulated diffusion model, called Text-DiFuse. First, this framework integrates feature-level information integration into the diffusion process, allowing adaptive degradation removal and multi-modal information fusion. This is the first attempt to deeply and explicitly embed information fusion within the diffusion process, effectively addressing compound degradation in image fusion. Second, by embedding the combination of the text and zero-shot location model into the diffusion fusion process, a text-controlled fusion re-modulation strategy is developed. This enables user-customized text control to improve fusion performance and highlight foreground objects in the fused images. Extensive experiments on diverse public datasets show that our Text-DiFuse achieves state-of-the-art fusion performance across various scenarios with complex degradation. Moreover, the semantic segmentation experiment validates the significant enhancement in semantic performance achieved by our text-controlled fusion re-modulation strategy. The code is publicly available at [this https URL](https://github.com/Leiii-Cao/Text-DiFuse).\n\n| | |\n| --- | --- |\n| Comments: | Accepted by the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2410.23905](https://arxiv.org/abs/2410.23905) \\[cs.CV\\] |\n| | (or [arXiv:2410.23905v1](https://arxiv.org/abs/2410.23905v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2410.23905](https://doi.org/10.48550/arXiv.2410.23905) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hao Zhang \\[ [view email](https://arxiv.org/show-email/2cceca29/2410.23905)\\]\n\n**\\[v1\\]**\nThu, 31 Oct 2024 13:10:50 UTC (22,971 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model, by Hao Zhang and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2410.23905)\n- [HTML (experimental)](https://arxiv.org/html/2410.23905v1)\n- [TeX Source](https://arxiv.org/src/2410.23905)\n- [Other Formats](https://arxiv.org/format/2410.23905)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2410.23905&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2410.23905&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-10](https://arxiv.org/list/cs.CV/2024-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2410.23905?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.23905)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.23905)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.23905)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2410.23905&description=Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2410.23905&title=Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2410.23905) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Text-diffuse: An interactive multi-modal image fusion framework based on text-modulated diffusion model",
          "cleaned_query": "Text-diffuse: An interactive multi-modal image fusion framework based on text-modulated diffusion model",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion",
          "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_DDFM_Denoising_Diffusion_Model_for_Multi-Modality_Image_Fusion_ICCV_2023_paper.pdf",
          "content": "DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion\nZixiang Zhao1,2 Haowen Bai1 Yuanzhi Zhu2Jiangshe Zhang1\u2217 Shuang Xu3\nYulun Zhang2 Kai Zhang2 Deyu Meng1,5 Radu Timofte2,4 Luc Van Gool2\n1Xi\u2019an Jiaotong University 2Computer Vision Lab, ETH Zurich \u00a8\n3Northwestern Polytechnical University 4University of Wurzburg \u00a8\n5Macau University of Science and Technology\nzixiangzhao@stu.xjtu.edu.cn, jszhang@mail.xjtu.edu.cn\nAbstract\nMulti-modality image fusion aims to combine different\nmodalities to produce fused images that retain the com\u0002plementary features of each modality, such as functional\nhighlights and texture details. To leverage strong genera\u0002tive priors and address challenges such as unstable train\u0002ing and lack of interpretability for GAN-based generative\nmethods, we propose a novel fusion algorithm based on\nthe denoising diffusion probabilistic model (DDPM). The\nfusion task is formulated as a conditional generation prob\u0002lem under the DDPM sampling framework, which is fur\u0002ther divided into an unconditional generation subproblem\nand a maximum likelihood subproblem. The latter is mod\u0002eled in a hierarchical Bayesian manner with latent vari\u0002ables and inferred by the expectation-maximization (EM)\nalgorithm. By integrating the inference solution into the\ndiffusion sampling iteration, our method can generate high\u0002quality fused images with natural image generative priors\nand cross-modality information from source images. Note\nthat all we required is an unconditional pre-trained gener\u0002ative model, and no fine-tuning is needed. Our extensive\nexperiments indicate that our approach yields promising\nfusion results in infrared-visible image fusion and medical\nimage fusion. The code is available at https://github.\ncom/Zhaozixiang1228/MMIF-DDFM.\n1. Introduction\nImage fusion integrates essential information from multi\u0002ple source images to create high-quality fused images [37,\n70, 27, 42], encompassing various source image types like\ndigital [20, 67, 74], multi-modal [58, 72], and remote sens\u0002ing [62, 76]. This technology provides a clearer repre\u0002sentation of objects and scenes, and has diverse applica-\n\u2217Corresponding author.\nGenerator\n\u201cReal\u201d\n\u201cReal\u201d \u201cFake\u201d\nDiscriminator 2\nDiscriminator 1\n\u201cFake\u201d\nGenerator\n\u201cReal\u201d\n\u201cReal\u201d \u201cFake\u201d\nDiscriminator 2\nDiscriminator 1\n\u201cFake\u201d\nFirst level\nSecond \nlevel\nHierarchical \nBayesian\nModel\nFirst level\nSecond \nlevel\nHierarchical \nBayesian\nModel\n... ...\n... ...\nLikelihood \nRectification\n(a) Vanilla GAN-based Fusion Model (b) Likelihood Rectification\n(c) Our DDFM\nLikelihood \nRectification\nFigure 1: (a) Existing GAN-based fusion method workflow. (b)\nGraph of the hierarchical Bayesian model in likelihood rectification,\nlinking the MMIF loss and our statistical inference model. (c) Our\nDDFM workflow: the unconditional diffusion sampling (UDS)\nmodule generates ft, while the likelihood rectification module,\nbased on (b), rectifies UDS output with source image information.\nEN\nSD\nMI\nVIF\nQabf\nSSIM\n0.2\n0.4\n0.6\n0.8\n1.0\nDataset: MSRS\nFusionGAN\n(IF'19)\nGANMcC\n(TIM'20)\nU2Fusion\n(TPAMI'20)\nRFNet\n(CVPR'22)\nTarDAL\n(CVPR'22)\nDeFusion\n(ECCV'22)\nUMFusion\n(IJCAI'22)\nDDFM\n(Ours)\nEN\nSD\nMI\nVIF\nQabf\nSSIM\n0.2\n0.4\n0.6\n0.8\n1.0\nDataset: RoadScene\nQuantitative Fusion Results for 6 Metrics\nFigure 2: Visualization of results on MSRS [51] and Road\u0002Scene [59] in Tab. 1. Hexagons formed by lines of different colors\nrepresent the values of different methods across six metrics. Our\nDDFM (marked in yellow) outperforms all other methods.\ntions such as saliency detection [43, 40, 41], object detec\u0002tion [12, 2, 10, 55], and semantic segmentation [28, 11, 56].\nAmong the different subcategories of image fusion, Infrared\u0002This ICCV paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore. 8082\nVisible image Fusion (IVF) and Medical Image Fusion (MIF)\nare particularly challenging in Multi-Modality Image Fusion\n(MMIF) since they focus on modeling cross-modality fea\u0002tures and preserving critical information from all sensors and\nmodalities. Specifically, in IVF, fused images aim to retain\nboth thermal radiation from infrared images and detailed\ntexture information from visible images, thereby avoiding\nthe limitations of visible images being sensitive to illumi\u0002nation conditions and infrared images being noisy and low\u0002resolution. While MIF can assist in diagnosis and treatment\nby fusing multiple medical imaging modalities for precise\ndetection of abnormality locations [16, 9].\nThere have been numerous methods devised recently to\naddress the challenges posed by MMIF [26, 65, 29], and\ngenerative models [7, 38] have been extensively utilized to\nmodel the distribution of fused images and achieve satis\u0002factory fusion effects. Among them, models based on Gen\u0002erative Adversarial Networks (GANs) [34, 35, 33, 26] are\ndominant. The workflow of GAN-based models, illustrated\nin Fig. 1a, involves a generator that creates images contain\u0002ing information from source images, and a discriminator that\ndetermines whether the generated images are in a similar\nmanifold to the source images. Although GAN-based meth\u0002ods have the ability to generate high-quality fused images,\nthey suffer from unstable training, lack of interpretability\nand mode collapse, which seriously affect the quality of the\ngenerated samples. Moreover, as a black-box model, it is\ndifficult to comprehend the internal mechanisms and behav\u0002iors of GANs, making it challenging to achieve controllable\ngeneration.\nRecently, Denoising Diffusion Probabilistic Models\n(DDPM) [13] has garnered attention in the machine learning\ncommunity, which generates high-quality images by mod\u0002eling the diffusion process of restoring a noise-corrupted\nimage towards a clean image. Based on the Langevin dif\u0002fusion process, DDPM utilizes a series of reverse diffusion\nsteps to generate promising synthetic samples [46]. Com\u0002pared to GAN, DDPM does not require the discriminator\nnetwork, thus mitigating common issues such as unstable\ntraining and mode collapse in GAN. Moreover, its generation\nprocess is interpretable, as it is based on denoising diffusion\nto generate images, enabling a better understanding of the\nimage generation process [57].\nTherefore, we propose a Denoising Diffusion image\nFusion Model (DDFM), as shown in Fig. 1c. We formu\u0002late the conditional generation task as a DDPM-based pos\u0002terior sampling model, which can be further decomposed\ninto an unconditional generation diffusion problem and a\nmaximum likelihood estimation problem. The former satis\u0002fies natural image prior while the latter is inferred to restrict\nthe similarity with source images via likelihood rectifica\u0002tion. Compared to discriminative approaches, modeling the\nnatural image prior with DDPM enables better generation\nof details that are difficult to control by manually designed\nloss functions, resulting in visually perceptible images. As\na generative method, DDFM achieves stable and control\u0002lable generation of fused images without discriminator, by\napplying likelihood rectification to the DDPM output.\nOur contributions are organized in three aspects:\n\u2022 We introduce a DDPM-based posterior sampling model\nfor MMIF, consisting of an unconditional generation\nmodule and a conditional likelihood rectification mod\u0002ule. The sampling of fused images is achieved solely\nby a pre-trained DDPM without fine-tuning.\n\u2022 In likelihood rectification, since obtaining the likeli\u0002hood explicitly is not feasible, we formulate the op\u0002timization loss as a probability inference problem in\u0002volving latent variables, which can be solved by the\nEM algorithm. Then the solution is integrated into the\nDDPM loop to complete conditional image generation.\n\u2022 Extensive evaluation of IVF and MIF tasks shows that\nDDFM consistently delivers favorable fusion results,\neffectively preserving both the structure and detail in\u0002formation from the source images, while also satisfying\nvisual fidelity requirements.\n2. Background\n2.1. Score-based diffusion models\nScore SDE formulation. Diffusion models aim to gen\u0002erate samples by reversing a predefined forward process\nthat converts a clean sample x0 to almost Gaussian sig\u0002nal xT by gradually adding noise. This forward process\ncan be described by an Ito Stochastic Differential Equation \u02c6\n(SDE) [49]:\ndx = \u2212\n\u03b2(t)\n2\nxtdt +\np\n\u03b2(t)dw, (1)\nwhere dw is standard Wiener process and \u03b2(t) is predefined\nnoise schedule that favors the variance-preserving SDE [49].\nThis forward process can be reversed in time and still in\nthe form of SDE [1]:\ndx =\nh\n\u2212\n\u03b2(t)\n2 xt \u2212 \u03b2(t)\u2207xt\nlog pt(xt)\ni\ndt +\np\n\u03b2(t)dw, (2)\nwhere dw corresponds to the standard Wiener process run\u0002ning backward and the only unknown part \u2207xt\nlog pt(xt)\ncan be modeled as the so-called score function s\u03b8(xt, t) with\ndenoising score matching methods, and this score function\ncan be trained with the following objective [15, 48]:\nEtEx0 Ext|x0\n\u0002\nks\u03b8(xt, t) \u2212 \u2207xtlog p0t(xt|x0)k\n2\n2\n\u0003\n, (3)\nwhere t is uniformly sampled over [0, T] and the data pair\n(x0, xt) \u223c p0(x)p0t(xt|x0).\nSampling with diffusion models. Specifically, an uncon\u0002ditional diffusion generation process starts with a random\n8083\nnoise vector xT \u223c N (0, I) and updates according to the\ndiscretization of Eq. (2). Alternatively, we can understand\nthe sampling process in the DDIM fashion [46], where the\nscore function can also be considered to be a denoiser and\npredict the denoised x\u02dc0|tfrom any state xt at iteration t:\nx\u02dc0|t =\n1\n\u221a\n\u03b1\u00aft\n(xt + (1 \u2212 \u03b1\u00aft)s\u03b8(xt, t)), (4)\nand x\u02dc0|t denotes the estimation of x0 given xt. We use the\nsame notation \u03b1t = 1 \u2212 \u03b2t and \u03b1\u00aft =\nQt\ns=1 \u03b1s following\nHo et al. [13]. With this predicted x\u02dc0|t and the current state\nxt, xt\u22121 is updated from\nxt\u22121 =\n\u221a\n\u03b1t (1 \u2212 \u03b1\u00aft\u22121)\n1 \u2212 \u03b1\u00aft\nxt +\n\u221a\n\u03b1\u00aft\u22121\u03b2t\n1 \u2212 \u03b1\u00aft\nx\u02dc0|t + \u02dc\u03c3tz, (5)\nwhere z \u223c N (0, I) and \u03c3\u02dc\n2\nt\nis the variance which is usu\u0002ally set to 0. This sampled xt\u22121 is then fed into the next\nsampling iteration until the final image x",
          "original_query": "DDFM: Denoising diffusion model for multi-modality image fusion",
          "cleaned_query": "DDFM: Denoising diffusion model for multi-modality image fusion",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "CLIP-Supervised Text Prompts for Zero-Shot Blind Restoration\nBuild a PromptIR-style restoration model where the \u201cdegradation prompt\u201d is generated by optimizing a short text prompt to maximize CLIP similarity between restored output and a target semantic description (e.g., \u201ca clear photo of a street at night\u201d), without needing degradation labels. Evaluate whether CLIP-guided prompt discovery improves generalization to unseen compound degradations compared to fixed learned prompts.",
        "Language-Conditioned Diffusion Priors for Composable Multi-Modal Fusion\nExtend DRMF/DDFM by conditioning each uni-modal diffusion prior not only on modality (IR/visible/medical) but also on a text description of the desired fusion intent (e.g., \u201chighlight pedestrians,\u201d \u201cpreserve fine vessels\u201d). Compose these text-conditioned priors during sampling and quantify controllability/robustness under matched vs mismatched degradations.",
        "Text-to-Fusion Policy Learning via Reinforcement Learning\nFormulate interactive fusion (Text-IF/Text-DiFuse) as sequential decision-making: at each diffusion/restoration step, an agent selects fusion weights, modality attention maps, or prompt tokens to satisfy a textual goal and objective constraints (noise, exposure, color bias). Train with RL using reward terms combining CLIP alignment, perceptual quality, and task utility (e.g., segmentation accuracy).",
        "Grounded Referring-Expression Fusion with Automatic Spatial Constraints\nImprove Text-DiFuse\u2019s foreground highlighting by integrating a CLIP-based \u201creferring expression\u201d localizer that converts text (\u201cthe person on the left\u201d, \u201ctumor region\u201d) into soft spatial masks used inside the diffusion denoising steps. Benchmark on fusion-for-segmentation settings, measuring whether text grounding increases saliency of specified objects without degrading global fidelity.",
        "Unified Degradation Taxonomy from Web-Scale Image-Text for Fusion/Restoration\nMine web image-text pairs to build a language-centric degradation taxonomy (e.g., \u201chazy,\u201d \u201coverexposed,\u201d \u201cmotion-blurred,\u201d \u201cthermal noise\u201d) aligned to CLIP embeddings, and use it to train a prompt library for PromptIR and text controllers for fusion. This enables \u201copen-vocabulary degradation recognition\u201d and controllable restoration/fusion without curated degradation labels.",
        "EM-Rectified Diffusion Sampling with Learnable Text Likelihood Terms\nExtend DDFM\u2019s EM-based likelihood rectification by adding a text likelihood term that scores intermediate samples using a CLIP text-image match (or a captioning-consistency score). The key contribution is a principled probabilistic sampling update that trades off modality fidelity, degradation robustness, and text intent in a single EM-style iterative scheme.",
        "Cross-Task Evaluation: Fusion/Restoration Optimized for Downstream CLIP-Zero-Shot Accuracy\nPropose training objectives for fusion/restoration that explicitly maximize zero-shot recognition performance of CLIP on the restored/fused outputs (e.g., maintaining class separability under degradation). Provide a benchmark showing when higher PSNR/SSIM correlates poorly with zero-shot accuracy, and demonstrate methods that improve both semantic transfer and perceptual quality.",
        "Mixture-of-Experts Prompts for Compound Degradation Decomposition\nCreate a PromptIR variant where multiple lightweight prompt \u201cexperts\u201d represent different degradation factors; a gating network predicts a sparse mixture per image (or per region) to handle compound degradations (noise+low-light+blur). Validate on synthetic and real compound degradations and analyze interpretability: which prompt experts activate under which textual descriptions and modalities.",
        "Text-Driven Safety and Hallucination Controls in Diffusion-Based Fusion\nDevelop mechanisms to prevent diffusion fusion models from hallucinating semantically plausible but non-existent structures (critical in medical/remote sensing) by enforcing \u201cevidence consistency\u201d constraints between modalities and text. Concretely, add penalties for CLIP-driven edits not supported by cross-modal correspondence, and introduce detection metrics for hallucinated anatomy/objects.",
        "Interactive Multi-Objective Fusion via Natural-Language Constraint Parsing\nBuild a system that parses user text into explicit constraints (e.g., \u201creduce noise,\u201d \u201ckeep natural colors,\u201d \u201cemphasize hot objects,\u201d \u201cavoid over-sharpening\u201d) and maps them to controllable knobs inside Text-IF/Text-DiFuse (fusion weights, diffusion guidance scales, prompt tokens). Provide a reproducible evaluation protocol: given a set of natural-language requests, measure constraint satisfaction, trade-off curves, and user-alignment versus non-interactive baselines."
      ],
      "generated_ideas_raw": "1. **CLIP-Supervised Text Prompts for Zero-Shot Blind Restoration**\n   Build a PromptIR-style restoration model where the \u201cdegradation prompt\u201d is generated by optimizing a short text prompt to maximize CLIP similarity between restored output and a target semantic description (e.g., \u201ca clear photo of a street at night\u201d), without needing degradation labels. Evaluate whether CLIP-guided prompt discovery improves generalization to unseen compound degradations compared to fixed learned prompts.\n\n2. **Language-Conditioned Diffusion Priors for Composable Multi-Modal Fusion**\n   Extend DRMF/DDFM by conditioning each uni-modal diffusion prior not only on modality (IR/visible/medical) but also on a text description of the desired fusion intent (e.g., \u201chighlight pedestrians,\u201d \u201cpreserve fine vessels\u201d). Compose these text-conditioned priors during sampling and quantify controllability/robustness under matched vs mismatched degradations.\n\n3. **Text-to-Fusion Policy Learning via Reinforcement Learning**\n   Formulate interactive fusion (Text-IF/Text-DiFuse) as sequential decision-making: at each diffusion/restoration step, an agent selects fusion weights, modality attention maps, or prompt tokens to satisfy a textual goal and objective constraints (noise, exposure, color bias). Train with RL using reward terms combining CLIP alignment, perceptual quality, and task utility (e.g., segmentation accuracy).\n\n4. **Grounded Referring-Expression Fusion with Automatic Spatial Constraints**\n   Improve Text-DiFuse\u2019s foreground highlighting by integrating a CLIP-based \u201creferring expression\u201d localizer that converts text (\u201cthe person on the left\u201d, \u201ctumor region\u201d) into soft spatial masks used inside the diffusion denoising steps. Benchmark on fusion-for-segmentation settings, measuring whether text grounding increases saliency of specified objects without degrading global fidelity.\n\n5. **Unified Degradation Taxonomy from Web-Scale Image-Text for Fusion/Restoration**\n   Mine web image-text pairs to build a language-centric degradation taxonomy (e.g., \u201chazy,\u201d \u201coverexposed,\u201d \u201cmotion-blurred,\u201d \u201cthermal noise\u201d) aligned to CLIP embeddings, and use it to train a prompt library for PromptIR and text controllers for fusion. This enables \u201copen-vocabulary degradation recognition\u201d and controllable restoration/fusion without curated degradation labels.\n\n6. **EM-Rectified Diffusion Sampling with Learnable Text Likelihood Terms**\n   Extend DDFM\u2019s EM-based likelihood rectification by adding a text likelihood term that scores intermediate samples using a CLIP text-image match (or a captioning-consistency score). The key contribution is a principled probabilistic sampling update that trades off modality fidelity, degradation robustness, and text intent in a single EM-style iterative scheme.\n\n7. **Cross-Task Evaluation: Fusion/Restoration Optimized for Downstream CLIP-Zero-Shot Accuracy**\n   Propose training objectives for fusion/restoration that explicitly maximize zero-shot recognition performance of CLIP on the restored/fused outputs (e.g., maintaining class separability under degradation). Provide a benchmark showing when higher PSNR/SSIM correlates poorly with zero-shot accuracy, and demonstrate methods that improve both semantic transfer and perceptual quality.\n\n8. **Mixture-of-Experts Prompts for Compound Degradation Decomposition**\n   Create a PromptIR variant where multiple lightweight prompt \u201cexperts\u201d represent different degradation factors; a gating network predicts a sparse mixture per image (or per region) to handle compound degradations (noise+low-light+blur). Validate on synthetic and real compound degradations and analyze interpretability: which prompt experts activate under which textual descriptions and modalities.\n\n9. **Text-Driven Safety and Hallucination Controls in Diffusion-Based Fusion**\n   Develop mechanisms to prevent diffusion fusion models from hallucinating semantically plausible but non-existent structures (critical in medical/remote sensing) by enforcing \u201cevidence consistency\u201d constraints between modalities and text. Concretely, add penalties for CLIP-driven edits not supported by cross-modal correspondence, and introduce detection metrics for hallucinated anatomy/objects.\n\n10. **Interactive Multi-Objective Fusion via Natural-Language Constraint Parsing**\n   Build a system that parses user text into explicit constraints (e.g., \u201creduce noise,\u201d \u201ckeep natural colors,\u201d \u201cemphasize hot objects,\u201d \u201cavoid over-sharpening\u201d) and maps them to controllable knobs inside Text-IF/Text-DiFuse (fusion weights, diffusion guidance scales, prompt tokens). Provide a reproducible evaluation protocol: given a set of natural-language requests, measure constraint satisfaction, trade-off curves, and user-alignment versus non-interactive baselines.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "CLIP-Supervised Text Prompts for Zero-Shot Blind Restoration\nBuild a PromptIR-style restoration model where the \u201cdegradation prompt\u201d is generated by optimizing a short text prompt to maximize CLIP sim",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Language-Conditioned Diffusion Priors for Composable Multi-Modal Fusion\nExtend DRMF/DDFM by conditioning each uni-modal diffusion prior not only on modality (IR/visible/medical) but also on a text des",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Text-to-Fusion Policy Learning via Reinforcement Learning\nFormulate interactive fusion (Text-IF/Text-DiFuse) as sequential decision-making: at each diffusion/restoration step, an agent selects fusion ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Grounded Referring-Expression Fusion with Automatic Spatial Constraints\nImprove Text-DiFuse\u2019s foreground highlighting by integrating a CLIP-based \u201creferring expression\u201d localizer that converts text (\u201c",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Unified Degradation Taxonomy from Web-Scale Image-Text for Fusion/Restoration\nMine web image-text pairs to build a language-centric degradation taxonomy (e.g., \u201chazy,\u201d \u201coverexposed,\u201d \u201cmotion-blurred,\u201d",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "EM-Rectified Diffusion Sampling with Learnable Text Likelihood Terms\nExtend DDFM\u2019s EM-based likelihood rectification by adding a text likelihood term that scores intermediate samples using a CLIP text",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Cross-Task Evaluation: Fusion/Restoration Optimized for Downstream CLIP-Zero-Shot Accuracy\nPropose training objectives for fusion/restoration that explicitly maximize zero-shot recognition performance",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Mixture-of-Experts Prompts for Compound Degradation Decomposition\nCreate a PromptIR variant where multiple lightweight prompt \u201cexperts\u201d represent different degradation factors; a gating network predic",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Text-Driven Safety and Hallucination Controls in Diffusion-Based Fusion\nDevelop mechanisms to prevent diffusion fusion models from hallucinating semantically plausible but non-existent structures (cri",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Interactive Multi-Objective Fusion via Natural-Language Constraint Parsing\nBuild a system that parses user text into explicit constraints (e.g., \u201creduce noise,\u201d \u201ckeep natural colors,\u201d \u201cemphasize hot o",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 19,
      "paper_title": "Identifiability of Deep Polynomial Neural Networks",
      "contribution": "Provides a comprehensive, constructive characterization of when deep polynomial neural networks are (finitely and/or globally) identifiable by reducing identifiability to low-rank polynomial/tensor decomposition uniqueness and settling open dimension and degree-threshold conjectures for neurovarieties.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 12668,
      "output_tokens": 988,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Recovering a Feed-Forward Net From Its Output",
          "url": "https://proceedings.neurips.cc/paper/1993/file/e49b8b4053df9505e1f48c3a701c0682-Paper.pdf",
          "content": "Recovering a Feed-Forward Net \nFrom Its Output \nCharles Fefferman * and Scott Markel \nDavid Sarnoff Research Center \nCN5300 \nPrinceton, N J 08543-5300 \ne-mail: cf9imath.princeton .edu \nsmarkel@sarnoff.com \nABSTRACT \nWe study feed-forward nets with arbitrarily many layers, using the stan\u0002dard sigmoid, tanh x. Aside from technicalities, our theorems are: \n1. Complete knowledge of the output of a neural net for arbitrary inputs \nuniquely specifies the architecture, weights and thresholds; and 2. There \nare only finitely many critical points on the error surface for a generic \ntraining problem. \nNeural nets were originally introduced as highly simplified models of the nervous \nsystem. Today they are widely used in technology and studied theoretically by \nscientists from several disciplines. However, they remain little understood. \nMathematically, a (feed-forward) neural net consists of: \n(1) A finite sequence of positive integers (Do, D1 , ... , D\u00a3); \n(2) A family of real numbers (wJ d defined for 1 :5 e 5: L, 1 5: j 5: Dl , 1 5: k :5 Dl-l ; \nand \n(3) A family of real numbers (OJ) defined for 15: f 5: L, 15: j 5: Dl. \nThe sequence (Do, D1 , .. \" DL ) is called the architecture of the neural net, while the \nW]k are called weights and the OJ thresholds. \nNeural nets are used to compute non-linear maps from }R.N to }R.M by the following \nconstruction. vVe begin by fixing a nonlinear function 0-( x) of one variable. Analogy \nwith the nervous system suggests that we take o-(x) asymptotic to constants as x \ntends to \u00b1oo; a standard choice, which we adopt throughout this paper, is o-(.r) = \n* Alternate address: Dept. of Mathematics. Princeton University, Princeton, NJ 08544-1000. \n335 \n336 Fefferman and Markel \ntanh ax). Given an \"input\" (tl , ... ,tDo) E JR Do , we define real numbers x; for \nOs l S L, 1 S j S De by the following induction on l . \n( 4) If l = 0 then x; = t j . \n(5) If the x~-l are known with l fixed (1 SlS L), then we set \nfor ISjSDe. \nHere xf , ... , Xhl are interpreted as the outputs of Di \"neurons\" in the lth \"layer\" \nof the net. The output map of the net is defined as the map \nIn practical applications, one tries to pick the neural net [(Do, Dl\"'\" DL), (W]k)' \n(OJ)] so that the output map approximates a given map about which we have \nonly imperfect information. The main result of this paper is that under generic \nconditions, perfect knowledge of the output map uniquely specifies the architec\u0002ture, the weights and the thresholds of a neural net, up to obvious symmetries. \n~Iore precisely, the obvious symmetries are as follows. Let C1o, 11, . .. , ~(L) be per\u0002mutations, with 11.= {I, ... , De} -T {I, . . . , De}; and let {e]: Os f. S L, IS j 50 De} be \na collection of \u00b1 1 'so Assume that Ii = (identity) and e] = + 1 whenever l = 0 or \n\u00a3 = L. Then one checks easily that the neural nets \n(7) [(Do, D1 , .. . , DL), (wh), (eJ)] and \n(8) [(Do , D 1,.\u00b7. , DL), (W]k)' (O'J)] \nhave the same output map if we set \n(9) and \nThis reflects the facts that the neurons in layer l are interchangeable (1 50 f. 50 L - 1), \nand that the function 0'( x) is odd. The nets (7) and (8) will be called isomorphtc \nif they are related by (9). Note in particular that isomorphic neural nets have the \nsame architecture. Our main theorem asserts that, under generic conditions, any \ntwo neural nets with the same output map are isomorphic. \n\\Ve discuss the generic conditions which we impose on neural nets. \\Ve have to \navoid obvious counterexamples such as: \n(10) Suppose all the weights W]k are zero. Then the output map is constant. \nThe architecture and thresholds of the neural net are clearly not uniquely \ndetermined by. \n(11) Fix lo, JI, h with IS fo S L - 1 and Isil < h 50 Dio ' Suppose we have \nelo = O~o and w~o = w~o for all k. Then (5) gi ves x~o = x~o Therefore the 11 J2 11k 12k Jl J2' , \nRecovering a Feed-Forward Net from Its Output 337 \noutput depends on ;,J~j~l and wJj;l only through the sum i. .. ;Jj~l + wJr;-l. So \nthe output map does not uniquely determine the weights. \nOur hypotheses are more than adequate to exclude these counterexamples. Specif\u0002ically, we assume that \n(12) OJ 1= 0 and :0;1 1= I\u00a31J/I for j 1= j'. \n(13) wh 1= 0; and for j 1= j', the ratio WJdW]lk is not equal to any fraction of the \nform pi q with p, q integers and 1 ~ q ~ 100 Dl\u0002Evidently, these conditions hold for generic neural nets. The precise statement of \nour main theorem is as follows. If two neural nets satisfy (12), (13) and ha've the \nsame output, then the nets are isomorphic. It would be interesting to replace (12), \n(13) by minimal hypotheses. and to study functions O'(x) other than tanh (~x). \n\\Ve now sketch the proof of our main result . sacrificing accuracy for simplicity. \nAfter a trivial reduction. we may assume Do = DL = 1. Thus, the outputs of the \nnodes xJ(t) are functions of one variable, and the output map of the neural net is \nt ~ xf (t). The key idea is to continue the xJ (t) analytically to complex values of t, \nand to read off the structure of the net from the set of singularities of the xJ, ~ote \nthat 0'( x) = tanh Ox) is meromorphic, with poles at the points of an arithmetic \nprogression {(2m + l);ri: mE \u00a3:}. This leads to two crucial observations. \n(14) When P. = 1, the poles of X] (t) form an arithmetic progression II;. and \n(15) 'Vhen e. > 1, every pole of any xi-1(t) is an accumulation point of poles of \nany X] (t). \nIn fact, (14) is immediate from the formula x;(t) = O'(WJlt + O}), which is merely \nthe special case Do = 1 of (5). \\Ve obtain \n(16) 1 _ {(2m + l);ri - OJ . } IIj - 1 . mE 2 \nwjl \nTo see (15), fix e., j, 'It, and assume for simplicity that X~-l(t) has a simple pole at \nto, while xi- 1(t) (k 1= t:) is analytic in a neighborhood of to. Then \n(17) t. 1 A \nxr.- (t) = t _ to + /(t), with / analytic in a neighborhood of to. \nFrom (17) and (5), we obtain \n(18) xJ(t) = O'(W;t-;A(t - to)-1 + g(t\u00bb, with \n(19) g(t) = wJtcf(t) + LWJkX~-I(t) + \u00a31J analytic in a neighborhood of to. \nk;c~ \nThus, in a neighborhood of to, the poles of X] (1) are the solutions tm of the equation \n(20) mE::. \n338 Fefferman and Markel \nThere are infinitely many solutions of (20), accumulating at to. Hence. to is an \naccumulation point of poles of xJ(t), which completes the proof of (15). \nIn view of (14), (15), it is natural to make the following definitions. The natural \ndomain of a neural net is the largest open subset of the complex plane to which the \noutput map t ........ xf(t) can be analytically continued. For l? 0 we define the lth \nsingular set Singe C) by setting \nSing(O) = complement of the natural domain in C, and \nSinge e + 1) = the set of all accumulation points of Singe f). \nThese definitions are made entirely in terms of the output map, without reference \nto the structure of the given neural net. On the other hand, the sets Sing( \u00a3) contain \nnearly complete information on the architecture, weights and thresholds of the net. \nThis will allow us to read off the structure of a neural net from the analytic contin\u0002uation of its output map. To see how the sets Sing(f) reflect the structure of the \nnet, we reason as follows. \nFrom (14) and (15) we expect that \n(21) For 1 $f $ L, Sing(L -l) is the union over j = 1, ... , Dl of the set of poles of \nxJ(t), together with their accumulation points (which we ignore here), and \n(22) For f? L, Sing(l) is empty. \nImmediately, then, we can read off the \"depth\"' L of the neural net; it is simply the \nsmallest e for which Sing(l) is empty. \nvVe need to solve for Dt , wh, OJ. We proceed by induction on l. \nWhen f = 1, (14) and (21) show that Sing(L - 1) is the union of arithmetic pro\u0002gressions IT}, j == 1, ... , D1 . Therefore, from Sing(L - 1) we can read off Dl and \nthe IT]. (vVe will return to this point later in the introduction.) In view of (16), \nIT] determines the weights and thresholds at layer 1. modulo signs. Thus. we have \nfound D I , W}k' g}. \nWhen l > 1, we may assume that \n(23) The Dl \" wJ~, Of are already known, for 1 ~ l' < f. \nOur task is to find De, W]k' gJ. In view of (23), we can find a pole to of xk-1(t) for \nour favorite k. Assume for simplicity that to is a simple pole of x~-I(tL and that \nthe X~-l(t) (k ::j:. ~) are analytic in a neighborhood of to. Then X~-I(t) is given by \n(17) in a neighborhood of to, with A already known by virtue of (23). Let U be a \nsmall neighborhood of to. \nWe will look at the image Y of U n Singe L - l) under the map t ........ t:to' Since A, \nto and Sing(L - e) are already known, so is Y. On the other hand, we can relate Y \nto De. WJk' OJ as follows. From (21) we see that Y is the union over j = 1,. \", Dl \nof \n(24) Yj = image of U n { Poles of xJ (t)} under t f---> tt:to)' \nRecovering a Feed-Forward Net from Its Output 339 \nFor fixed j, the poles of xJ(t) in a neighborhood of to are the lm given by (20). \\Ve \nwrite \n(25) \nEquation (20) shows that the first expression in brackets in (25) is equal to (2m + \n1 )'7ri. Also, since tm -+ to as Iml - 00 and 9 is analytic in a neighborhood of to, \nthe second expression in brackets in (25) tends to zero. Hence, \nW~ leA _) = (2m+1)7ri-g(to)+o(1) forlargem. \ntm - to \nComparing this with the definition (24), \\':e see that Yj is asymptotic to the arith\u0002metic progression \n(26) ITl _ {(2m + 1)7ri - g(to). ~} ]- l .mEtL.. . Wjt. \nThus, the known set Y is the union over j = 1 ... \" Dl of sets Yj, with Yj asymptotic \nto the arithmetic progression IT~ . From Y, we can therefore read off Dl and the II~ . \n(\\Ve will return to this point in a moment.) \\Ve see at once from (26) that wJ ~ is \ndetermined up to sign by II]. Thus, we have found Dl and who \\Vith more work, \nwe can also find the OJ, completing the induction on t. \nThe above induction shows that the structure of a neural net may be read off \nfrom the analytic continuation of its output map. \\Ve believe that the analytic \ncontinuation of the output map will lead to further consequences in the study of \nneural nets. \nLet us touch briefly on a few points which we",
          "original_query": "Reconstructing a neural net from its output [46]",
          "cleaned_query": "Reconstructing a neural net from its output",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] On the Expressive Power of Deep Polynomial Neural Networks",
          "url": "http://papers.neurips.cc/paper/9219-on-the-expressive-power-of-deep-polynomial-neural-networks.pdf",
          "content": "On the Expressive Power of\nDeep Polynomial Neural Networks\nJoe Kileel\u21e4\nPrinceton University\nMatthew Trager\u21e4\nNew York University\nJoan Bruna\nNew York University\nAbstract\nWe study deep neural networks with polynomial activations, particularly their\nexpressive power. For a fixed architecture and activation degree, a polynomial\nneural network defines an algebraic map from weights to polynomials. The image\nof this map is the functional space associated to the network, and it is an irreducible\nalgebraic variety upon taking closure. This paper proposes the dimension of this\nvariety as a precise measure of the expressive power of polynomial neural networks.\nWe obtain several theoretical results regarding this dimension as a function of\narchitecture, including an exact formula for high activation degrees, as well as\nupper and lower bounds on layer widths in order for deep polynomials networks to\nfill the ambient functional space. We also present computational evidence that it is\nprofitable in terms of expressiveness for layer widths to increase monotonically and\nthen decrease monotonically. Finally, we link our study to favorable optimization\nproperties when training weights, and we draw intriguing connections with tensor\nand polynomial decompositions.\n1 Introduction\nA fundamental problem in the theory of deep learning is to study the functional space of deep neural\nnetworks. A network can be modeled as a composition of elementary maps, however the family of\nall functions that can be obtained in this way is extremely complex. Many recent papers paint an\naccurate picture for the case of shallow networks (e.g., using mean field theory [7, 27]) and of deep\nlinear networks [2, 3, 21], however a similar investigation of deep nonlinear networks appears to be\nsignificantly more challenging, and require very different tools.\nIn this paper, we consider a general model for deep polynomial neural networks, where the activation\nfunction is a polynomial (r-th power) exponentiation. The advantage of this framework is that\nthe functional space associated with a network architecture is algebraic, so we can use tools from\nalgebraic geometry [17] for a precise investigation of deep neural networks. Indeed, for a fixed\nactivation degree r and architecture d = (d0,...,dh) (expressed as a sequence of widths), the family\nof all networks with varying weights can be identified with an algebraic variety Vd,r, embedded\nin a finite-dimensional Euclidean space. In this setting, an algebraic variety can be thought of as a\nmanifold that may have singularities.\nIn this paper, our main object of study is the dimension of Vd,r as a variety (in practice, as a manifold),\nwhich may be regarded as a precise measure of the architecture\u2019s expressiveness. Specifically, we\nprove that this dimension stabilizes when activations are high degree, and we provide an exact\ndimension formula for this case (Theorem 14). We also investigate conditions under which Vd,r\nfills its ambient space. This question is important from the vantage point of optimization, since an\narchitecture is \u201cfilling\u201d if and only if it corresponds to a convex functional space (Proposition 6). In\n\u21e4Equal contribution.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nthis direction, we prove a bottleneck property, that if a width is not sufficiently large, the network can\nnever fill the ambient space regardless of the size of other layers (Theorem 19).\nIn a broader sense, our work introduces a powerful language and suite of mathematical tools for\nstudying the geometry of network architectures. Although this setting requires polynomial activations,\nit may be used as a testing ground for more general situations and, e.g., to verify rules of thumb\nrigorously. Finally, our results show that polynomial neural networks are intimately related to\nthe theory of tensor decompositions [22]. In fact, representing a polynomial as a deep network\ncorresponds to a type of decomposition of tensors which may be viewed as a composition of\ndecompositions of a recently introduced sort [24]. Using this connection, we establish general\nnon-trivial upper bounds on filling widths (Theorem 10). We believe that our work can serve as a\nfirst step towards many interesting research challenges in developing the theoretical underpinnings of\ndeep learning.\n1.1 Related work\nThe study of the expressive power of neural networks dates back to seminal work on the universality\nof networks as function approximators [10, 19]. More recently, there has been research supporting\nthe hypothesis of \u201cdepth efficiency\u201d, i.e., the fact that deep networks can approximate functions more\nefficiently than shallow networks [11, 25, 8, 9]. In contrast to this line of work, we study the class\nof functions that can be expressed exactly using a network. Our analysis may of course be used to\ninvestigate the problem of approximation, however this is not the focus of this paper.\nMost of the aforementioned studies make strong hypotheses on the network architecture. In par\u0002ticular, [11, 25] focus on arithmetic circuits, or sum-product networks [29]. These are networks\ncomposed of units that compute either the product or a weighted sum of their inputs. In [8], the\nauthors introduce a model of convolutional arithmetic circuits. This is a particular class of arithmetic\ncircuits that includes networks with layers of 1D convolutions and product pooling. This model does\nnot allow for non-linear activations (beside the product pooling), although the follow-up paper [9]\nextends some results to ReLU activations with sum pooling. Interestingly, these networks are related\nto Hierarchical Tucker (HT) decomposition of tensors.\nThe polynomial networks studied in this paper are not arithmetic circuits, but feedforward deep\nnetworks with polynomial r-th power activations. This is a vast generalization of a setting consid\u0002ered in several recent papers [33, 14, 31], that study shallow (two layer) networks with quadratic\nactivations (r = 2). These papers show that if the width of the intermediate layer is at least twice\nthe input dimension, then the quadratic loss has no \u201cbad\u201d local minima. This result in line with our\nProposition 5, which explains in this case the functional space is convex and fills the ambient space.\nWe also point out that polynomial activations are required for the functional space of the network to\nspan a finite dimensional vector space [23, 33].\nThe polynomial networks considered in this paper do not correspond to HT tensor decompositions as\nin [8, 9], rather they are related to a different polynomial/tensor decomposition attracting very recent\ninterest [16, 24]. These generalize usual decompositions, however their algorithmic and theoretical\nunderstanding are, mostly, wide open. Neural networks motivate several questions in this vein.\nFinally, we mention other recent works that study neural networks from the perspective of algebraic\ngeometry [26, 32, 20].\nMain contributions. Our main contributions can be summarized as follows.\n\u2022 We give a precise formulation of the expressiveness of polynomial networks in terms of the\nalgebraic dimension of the functional space as an algebraic variety.\n\u2022 We spell out the close, two-way relationship between polynomial networks and a particular\nfamily of decompositions of tensors.\n\u2022 We prove several theoretical results on the functional space of polynomial networks. Notably,\nwe give a formula for the dimension that holds for sufficiently high activation degrees\n(Theorem 14) and we prove a tight lower bound on the width of the layers for the network\nto be \u201cfilling\u201d in the functional space (Theorem 19).\n2\nNotation. We use Symd(Rn) to denote the space of homogeneous polynomials of degree d in n\nvariables with coefficients in R. This set is a vector space over R of dimension Nd,n = n+d1\nd\n\n,\nspanned by all monomials of degree d in n variables. In practice, Symd(Rn) is isomorphic to RNd,n ,\nand our networks will correspond to points in this high dimensional space. The notation Symd(Rn)\nexpresses the fact that a polynomial of degree d in n variables can always be identified with a\nsymmetric tensor in (Rn)\u2326d that collects all of its coefficients.\n2 Basic setup\nA polynomial network is a function p\u2713 : Rd0 ! Rdh of the form\np\u2713(x) = Wh\u21e2rWh1\u21e2r ... \u21e2rW1x, Wi 2 Rdi\u21e5di1 ,\nwhere the activation \u21e2r(z) raises all elements of z to the r-th power (r 2 N). The parameters\n\u2713 = (Wh,...,W1) 2 Rd\u2713 (with d\u2713 = Ph\ni=1 didi1) are the network\u2019s weights, and the network\u2019s\narchitecture is encoded by the sequence d = (d0,...,dh) (specifying the depth h and widths\ndi). Clearly, p\u2713 is a homogeneous polynomial mapping Rd0 ! Rdh of degree rh1, i.e., p\u2713 2\nSymrh1 (Rd0 )dh .\nFor fixed degree r and architecture d = (d0,...,dh), there exists an algebraic map\nd,r : \u2713 7! p\u2713 =\n2\n6\n4\np\u27131\n.\n.\n.\np\u2713dh+1\n3\n7\n5 , (1)\nwhere each p\u2713i is a polynomial in d0 variables. The image of d,r is a set of vectors of polynomials,\ni.e., a subset Fd,r of Symrh1 (Rd0 )dh , and it is the functional space represented by the network. In\nthis paper, we consider the \u201cZariski closure\u201d Vd,r = Fd,r of the functional space.1 We refer to Vd,r\nas functional variety of the network architecture, as it is in fact an irreducible algebraic variety. In\nparticular, Vd,r can be studied using powerful machinery from algebraic geometry.\nRemark 1. The functional variety Vd,r may be significantly larger than the actual functional space\nFd,r, since the Zariski closure is typically larger than the closure with respect to the standard the\nEuclidean topology. On the other hand, the dimensions of the spaces Vd,r and Fd,r agree, and the\nset Vd,r is usually \u201cnicer\u201d (it can be described by polynomial equations, whereas an exact implicit\ndescription of Fd,r may require inequalities).\n2.1 Examples\nWe present some examples that describe the functional variety Vd,r in simple cases.\nExample 2. A linear network is a polynomial network with r = 1. In this case, the network map\nd,r : Rd\u2713 ",
          "original_query": "On the expressive power of deep polynomial neural networks [7]",
          "cleaned_query": "On the expressive power of deep polynomial neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] X-rank and identifiability for a polynomial decomposition model - CIRM",
          "url": "https://www.cirm-math.fr/ProgWeebly/Renc1506/Usevich.pdf",
          "content": "Matrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nX-rank and identifiability for a polynomial\ndecomposition model\nPierre Comon, Yang Qi and Konstantin Usevich\nGIPSA-lab, CNRS and Univ. Grenoble Alpes\nSIGMA\u20192016 @ CIRM, Luminy, 03.11.2016\nSupported by ERC grant #320594 \u201dDecoda\u201d\n1 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nOverview\nMatrix and tensor rank\nA polynomial decomposition\nWaring decomposition\nX-rank and identifiablility\nResults\n2 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nOverview\nMatrix and tensor rank\nA polynomial decomposition\nWaring decomposition\nX-rank and identifiablility\nResults\n2 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nMatrix rank\nRank of M \u2208 KI\u00d7J(K = R, C)\nTwo definitions:\n1. rank(M)\ndef = dim colspan M = dim rowspan M\n2. rank(M)\ndef = minimal r such that\nM = a1b\n>\n1 + \u00b7 \u00b7 \u00b7 + arb\n>\nr\n, ak \u2208 K\nI\n, bk \u2208 K\nJ\nsum of r rank-one matrices\n3 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nMatrix rank\nRank of M \u2208 KI\u00d7J(K = R, C)\nTwo definitions:\n1. rank(M)\ndef = dim colspan M = dim rowspan M\n2. rank(M)\ndef = minimal r such that\nM = a1b\n>\n1 + \u00b7 \u00b7 \u00b7 + arb\n>\nr\n, ak \u2208 K\nI\n, bk \u2208 K\nJ\nsum of r rank-one matrices\nProperties:\n\u2022 Rank does not exceed dimensions (r \u2264 min(I, J)),\nr = min(I, J) for general (random) M\n3 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nMatrix rank\nRank of M \u2208 KI\u00d7J(K = R, C)\nTwo definitions:\n1. rank(M)\ndef = dim colspan M = dim rowspan M\n2. rank(M)\ndef = minimal r such that\nM = a1b\n>\n1 + \u00b7 \u00b7 \u00b7 + arb\n>\nr\n, ak \u2208 K\nI\n, bk \u2208 K\nJ\nsum of r rank-one matrices\nProperties:\n\u2022 Rank does not exceed dimensions (r \u2264 min(I, J)),\nr = min(I, J) for general (random) M\n\u2022 Not unique: AB = ASS\u22121B for any nonsingular S,\nwe can take ak and/or bk orthogonal (SVD, QR)\n3 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nMatrix rank\nRank of M \u2208 KI\u00d7J(K = R, C)\nTwo definitions:\n1. rank(M)\ndef = dim colspan M = dim rowspan M\n2. rank(M)\ndef = minimal r such that\nM = a1b\n>\n1 + \u00b7 \u00b7 \u00b7 + arb\n>\nr\n, ak \u2208 K\nI\n, bk \u2208 K\nJ\nsum of r rank-one matrices\nProperties:\n\u2022 Rank does not exceed dimensions (r \u2264 min(I, J)),\nr = min(I, J) for general (random) M\n\u2022 Not unique: AB = ASS\u22121B for any nonsingular S,\nwe can take ak and/or bk orthogonal (SVD, QR)\n\u2022 rankC(M) = rankR(M),\n3 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nTensor CP (canonical polyadic) rank\nTensor: d-dimensional array T =\n\u0002\nTi,j,...,k\n\u0003I,J,...,K\ni,j,...,k=1\nRank-one tensor: Ti,j,\u00b7\u00b7\u00b7 ,k = aibj\u00b7 \u00b7 \u00b7 ck\nNotation: T = a \u2297 b \u2297 \u00b7 \u00b7 \u00b7 \u2297 c, a \u2208 KI, b \u2208 KJ, \u00b7 \u00b7 \u00b7 , c \u2208 KK\nDefinition (Hitchock,1927)\nrank(T )\ndef = minimal r such that\nT =\nXr\n`=1\na` \u2297 b` \u2297 \u00b7 \u00b7 \u00b7 \u2297 c` (CP decomposition)\nA picture: T\n= + \u00b7 \u00b7 \u00b7 +\na1\nb1\nc1\nar\nbr\ncr\n4 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nUsefulness of tensor CPD\nT\n= + \u00b7 \u00b7 \u00b7 +\n\u2022 Data mining (identification problems)\nTensor = data/signal, CP rank = # of components in a signal\nExamples: fluorescence spectroscopy ( talk of Caroline Chaux),\nantenna array processing, hyperspectral imaging, etc.\n5 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nUsefulness of tensor CPD\nT\n= + \u00b7 \u00b7 \u00b7 +\n\u2022 Data mining (identification problems)\nTensor = data/signal, CP rank = # of components in a signal\nExamples: fluorescence spectroscopy ( talk of Caroline Chaux),\nantenna array processing, hyperspectral imaging, etc.\n\u2022 function approximation (talk of Anthony Nouy )\n5 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nUsefulness of tensor CPD\nT\n= + \u00b7 \u00b7 \u00b7 +\n\u2022 Data mining (identification problems)\nTensor = data/signal, CP rank = # of components in a signal\nExamples: fluorescence spectroscopy ( talk of Caroline Chaux),\nantenna array processing, hyperspectral imaging, etc.\n\u2022 function approximation (talk of Anthony Nouy )\n\u2022 Complexity of matrix multiplication\n# of operations: O(N\n3\n)\nC = N\nN N\nA \u00b7 B\n5 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nUsefulness of tensor CPD\nT\n= + \u00b7 \u00b7 \u00b7 +\n\u2022 Data mining (identification problems)\nTensor = data/signal, CP rank = # of components in a signal\nExamples: fluorescence spectroscopy ( talk of Caroline Chaux),\nantenna array processing, hyperspectral imaging, etc.\n\u2022 function approximation (talk of Anthony Nouy )\n\u2022 Complexity of matrix multiplication\n(Strassen, 1969):\n# of operations: O(N\n3\n) \u2192 O(N\n2.8\n)\nC = N\nN N\nA \u00b7 B \u2194 vecC = multiplication \u20222 vecA \u20223 vecB\ntensor\n5 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nSymmetric tensor CPD\nT =\n\u0002\nT(i1,...,id )\n\u0003m,...,m\ni1,...,id =1 \u2208 K\nm\u00d7\u00b7\u00b7\u00b7\u00d7m T(i1,...,id ) = T\u03c0(i1,...,id )\n, \u2200\u03c0\n| {z }\nsymmetric\nSymmetric rank of the tensor: minimal r, such that\nT =\nXr\nk=1\nckak \u2297 \u00b7 \u00b7 \u00b7 \u2297 ak (\u2217)\n6 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nSymmetric tensor CPD\nT =\n\u0002\nT(i1,...,id )\n\u0003m,...,m\ni1,...,id =1 \u2208 K\nm\u00d7\u00b7\u00b7\u00b7\u00d7m T(i1,...,id ) = T\u03c0(i1,...,id )\n, \u2200\u03c0\n| {z }\nsymmetric\nSymmetric rank of the tensor: minimal r, such that\nT =\nXr\nk=1\nckak \u2297 \u00b7 \u00b7 \u00b7 \u2297 ak (\u2217)\nUseful for blind source separation (Comon, Jutten, 2010):\nMixing model: x =\n\u0014 x1\n.\n.\n.\nxm\n\u0015\n= A\n|{z}\nunknown\n\u0014 s1\n.\n.\n.\nsr\n\u0015\n|{z}\nunknown\n.\nIf sk are independent (real) random variables, then the cumulant of x has\nthe form (\u2217).\n6 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nTensors (d \u2265 3): bad news\n\u2022 Set {T | rank(T ) \u2264 r} is not closed.\nT = a \u2297 b \u2297 b + b \u2297 a \u2297 b + b \u2297 b \u2297 a\nrank(T ) = 3, but\nT =\n1\n\u03b5\n\n(a + \u03b5b)\n\u22973 \u2212 a\u22973\n\u0001\n+ O(\u03b5)\n\u2022 No polynomial time algorithm to determine rank (Hastad, 1990).\n\u2022 rankC(T ) \u2264 rankR(T ), may be strict\n\u2022 For symmetric tensors: symmetric rank ?= rank (Comon conjecture)\n7 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nTensors (d \u2265 3): good (or interesting) news\n\u2022 Rank can exceed dimensions\nExample: T =\n\u0014\n0 1 1 0\n1 0 0 0 \u0015\n, rank(T ) = 3\n\u2022 CP decomposition is often unique, doesn\u2019t need to be orthogonal\n\u2022 Unusual rank properties (take 2 \u00d7 2 \u00d7 2 tensor):\n\u2022 maximal rank is 3\n\u2022 random (Gaussian i.i.d.) real tensor:\nP(rank(T ) = 2) = \u03c0/4, P(rank(T ) = 3) = 1 \u2212 \u03c0/4.\n\u2022 random complex tensor: rank 2\n8 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nOverview\nMatrix and tensor rank\nA polynomial decomposition\nWaring decomposition\nX-rank and identifiablility\nResults\n8 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nA polynomial decomposition\nGiven a multivariate polynomial f (u1, . . . , um)=\n|i1+...P+im|\u2264d\ni1,...,im=0\nfi1,...,im u\ni1\n1\n\u00b7 \u00b7 \u00b7 u\nim\nm ,\nfind its shortest representation\nf (u) = g1(v\n>\n1 u) + \u00b7 \u00b7 \u00b7 + g1(v\n>\nr u),\nwhere vk \u2208 Km, gk (t) = c1,k t + c2,k t\n2 + \u00b7 \u00b7 \u00b7 + cd,k td\n, deg gk \u2264 d\n\u22123 \u22122 \u22121 0 1 2 3 \u22125\n0\n5\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n\u2248\n\u22123 \u22122 \u22121 0 1 2 3 \u22125\n0\n5\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n+\n\u22123 \u22122 \u22121 0\n1\n2\n3\n\u22124\n\u22122\n0\n2\n4\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n+\n\u22123 \u22122 \u22121 0 1 2 3 \u22125\n0\n5\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\nAppears in\n\u2022 approximation theory (ridge approximation, (Lin, Pinkus, 1993))\n\u2022 machine learning (polynomial neural networks, (Shin, Ghosh, 1995))\n\u2022 blind source separation\n9 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nDecomposition of polynomial maps\nGiven a polynomial map f : Km \u2192 Kn, degree d, f(u) = \u0002\nf1(u)\u00b7 \u00b7 \u00b7 fn(u)\n\u0003>\nfind its shortest representation\nf(u) = w1g1(v\n>\n1 u) + \u00b7 \u00b7 \u00b7 + wr gr(v\n>\nr u),\nwhere vk \u2208 Km, wk \u2208 Kn, gk (t) = c1,k t + c2,k t\n2 + \u00b7 \u00b7 \u00b7 + cd,k td\n10 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nDecomposition of polynomial maps\nGiven a polynomial map f : Km \u2192 Kn, degree d, f(u) = \u0002\nf1(u)\u00b7 \u00b7 \u00b7 fn(u)\n\u0003>\nfind its shortest representation\nf(u) = w1g1(v\n>\n1 u) + \u00b7 \u00b7 \u00b7 + wr gr(v\n>\nr u),\nwhere vk \u2208 Km, wk \u2208 Kn, gk (t) = c1,k t + c2,k t\n2 + \u00b7 \u00b7 \u00b7 + cd,k td\nBlock-structured nonlinear system identification (Schoukens et al., 2014):\nu1\n.\n.\n.\num\nf(u1, . . . , um)\ny1\n.\n.\n.\nyn \u2194\nu1\n.\n.\n.\num\nVT\nt=V\n>u\ng1(t1)\nt1\n.\n.\n.\ngr (tr )\ntr\nW\ny=Wz\nz1\nzr\ny1\n.\n.\n.\nyn\nwhere V =\n\u0002\nv1 \u00b7 \u00b7 \u00b7 vr\n\u0003>\nand W =\n\u0002\nw1 \u00b7 \u00b7 \u00b7 wr\n\u0003>\n.\nRemarks:\n\u2022 Degree 1 \u2014 matrix factorization\n\u2022 Can be also interpreted as polynomial neural network.\n10 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nA tensor-based algorithm\u2217\nGiven a polynomial map f (of degree d), find r, wk , vk , gk such that\nf(u) = w1g1(v\n>\n1 u) + \u00b7 \u00b7 \u00b7 + wr gr(v\n>\nr u),\n(Dreesen et al, 2015), (Van Mulders et al, 2014): transform to a tensor CPD\nAlgorithm.\n1. Evaluate Jf(u) at N points u1, . . . , uN \u2208 Km\n2. Stack it into a tensor:\nT =\nJf (u1)\nJf (u2)\nJf (uN )\n.\n.\n.\nm\nn\nN\n= + \u00b7 \u00b7 \u00b7 +\nw1\nv1\nd1\nwr\nvr\ndr\n| {z }\n3. Retrieve vk ,wk ,gk\nOur questions:\n\u2022 When is the decomposition unique? (identifiability of the model)\n\u2022 What is the maximal/typical number of terms?\n11 / 30\nMatrix and tensor rank A polynomial decomposition Waring decomposition X-rank and identifiablility Results\nExisting results\nFor f of degree d:\nf (u1, . . . , um) = g1(v\n>\n1 u) + \u00b7 \u00b7 \u00b7 + g1(v\n>\nr u),\n\u2022 (Schinzel, 2002), m = 2:\na general (\u201crandom\u201d) polynomial in C has r = d\n2d+5\u2212\n\u221a\n8d+17\n2\ne terms\n\u2022 (Bia lynicki-Birula, Schinzel, 2008):\nany f can be represented with r \u2264\n\u0012\nm + d \u2212 2\nd \u2212 1\n\u0013\nterms\n\u2022 nothing about identifiability\n12 /",
          "original_query": "Identifiability of an X-rank decomposition of polynomial maps []",
          "cleaned_query": "Identifiability of an X-rank decomposition of polynomial maps []",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[1210.7559] Tensor decompositions for learning latent variable models",
          "url": "https://arxiv.org/abs/1210.7559",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1210.7559** (cs)\n\n\\[Submitted on 29 Oct 2012 ( [v1](https://arxiv.org/abs/1210.7559v1)), last revised 13 Nov 2014 (this version, v4)\\]\n\n# Title:Tensor decompositions for learning latent variable models\n\nAuthors: [Anima Anandkumar](https://arxiv.org/search/cs?searchtype=author&query=Anandkumar,+A), [Rong Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge,+R), [Daniel Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu,+D), [Sham M. Kakade](https://arxiv.org/search/cs?searchtype=author&query=Kakade,+S+M), [Matus Telgarsky](https://arxiv.org/search/cs?searchtype=author&query=Telgarsky,+M)\n\nView a PDF of the paper titled Tensor decompositions for learning latent variable models, by Anima Anandkumar and Rong Ge and Daniel Hsu and Sham M. Kakade and Matus Telgarsky\n\n[View PDF](https://arxiv.org/pdf/1210.7559)\n\n> Abstract:This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models---including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation---which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1210.7559](https://arxiv.org/abs/1210.7559) \\[cs.LG\\] |\n| | (or [arXiv:1210.7559v4](https://arxiv.org/abs/1210.7559v4) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1210.7559](https://doi.org/10.48550/arXiv.1210.7559) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Journal of Machine Learning Research, 15(Aug):2773-2832, 2014 |\n\n## Submission history\n\nFrom: Daniel Hsu \\[ [view email](https://arxiv.org/show-email/4e158393/1210.7559)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1210.7559v1)**\nMon, 29 Oct 2012 04:38:41 UTC (56 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1210.7559v2)**\nSun, 9 Dec 2012 00:59:17 UTC (57 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/1210.7559v3)**\nSat, 1 Mar 2014 19:06:31 UTC (62 KB)\n\n**\\[v4\\]**\nThu, 13 Nov 2014 22:43:15 UTC (57 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Tensor decompositions for learning latent variable models, by Anima Anandkumar and Rong Ge and Daniel Hsu and Sham M. Kakade and Matus Telgarsky\n\n- [View PDF](https://arxiv.org/pdf/1210.7559)\n- [TeX Source](https://arxiv.org/src/1210.7559)\n- [Other Formats](https://arxiv.org/format/1210.7559)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1210.7559&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1210.7559&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2012-10](https://arxiv.org/list/cs.LG/2012-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1210.7559?context=cs)\n\n[math](https://arxiv.org/abs/1210.7559?context=math)\n\n[math.NA](https://arxiv.org/abs/1210.7559?context=math.NA)\n\n[stat](https://arxiv.org/abs/1210.7559?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1210.7559?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1210.7559)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1210.7559)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1210.7559)\n\n### [1 blog link](https://arxiv.org/tb/1210.7559)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1210.html#abs-1210-7559) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1210-7559)\n\n[Anima Anandkumar](https://dblp.uni-trier.de/search/author?author=Anima%20Anandkumar)\n\n[Rong Ge](https://dblp.uni-trier.de/search/author?author=Rong%20Ge)\n\n[Daniel Hsu](https://dblp.uni-trier.de/search/author?author=Daniel%20Hsu)\n\n[Daniel J. Hsu](https://dblp.uni-trier.de/search/author?author=Daniel%20J.%20Hsu)\n\n[Sham M. Kakade](https://dblp.uni-trier.de/search/author?author=Sham%20M.%20Kakade)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1210.7559&description=Tensor decompositions for learning latent variable models) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1210.7559&title=Tensor decompositions for learning latent variable models)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1210.7559) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Tensor decompositions for learning latent variable models [10]",
          "cleaned_query": "Tensor decompositions for learning latent variable models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2402.00949] Geometry of Polynomial Neural Networks - arXiv",
          "url": "https://arxiv.org/abs/2402.00949",
          "content": "# Mathematics > Algebraic Geometry\n\n**arXiv:2402.00949** (math)\n\n\\[Submitted on 1 Feb 2024 ( [v1](https://arxiv.org/abs/2402.00949v1)), last revised 4 Nov 2024 (this version, v2)\\]\n\n# Title:Geometry of Polynomial Neural Networks\n\nAuthors: [Kaie Kubjas](https://arxiv.org/search/math?searchtype=author&query=Kubjas,+K), [Jiayi Li](https://arxiv.org/search/math?searchtype=author&query=Li,+J), [Maximilian Wiesmann](https://arxiv.org/search/math?searchtype=author&query=Wiesmann,+M)\n\nView a PDF of the paper titled Geometry of Polynomial Neural Networks, by Kaie Kubjas and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2402.00949) [HTML (experimental)](https://arxiv.org/html/2402.00949v2)\n\n> Abstract:We study the expressivity and learning process for polynomial neural networks (PNNs) with monomial activation functions. The weights of the network parametrize the neuromanifold. In this paper, we study certain neuromanifolds using tools from algebraic geometry: we give explicit descriptions as semialgebraic sets and characterize their Zariski closures, called neurovarieties. We study their dimension and associate an algebraic degree, the learning degree, to the neurovariety. The dimension serves as a geometric measure for the expressivity of the network, the learning degree is a measure for the complexity of training the network and provides upper bounds on the number of learnable functions. These theoretical results are accompanied with experiments.\n\n| | |\n| --- | --- |\n| Comments: | 34 pages, 3 figures. Comments are welcome! |\n| Subjects: | Algebraic Geometry (math.AG); Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| MSC classes: | 68T07, 14P10, 14N07, 14M12 |\n| Cite as: | [arXiv:2402.00949](https://arxiv.org/abs/2402.00949) \\[math.AG\\] |\n| (or [arXiv:2402.00949v2](https://arxiv.org/abs/2402.00949v2) \\[math.AG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2402.00949](https://doi.org/10.48550/arXiv.2402.00949) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Alg. Stat. 15 (2024) 295-328 |\n| Related DOI: | [https://doi.org/10.2140/astat.2024.15.295](https://doi.org/10.2140/astat.2024.15.295) Focus to learn more DOI(s) linking to related resources |\n\n## Submission history\n\nFrom: Maximilian Wiesmann \\[ [view email](https://arxiv.org/show-email/13ac06f2/2402.00949)\\] **[\\[v1\\]](https://arxiv.org/abs/2402.00949v1)**\nThu, 1 Feb 2024 19:06:06 UTC (780 KB)\n**\\[v2\\]**\nMon, 4 Nov 2024 17:39:35 UTC (844 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Geometry of Polynomial Neural Networks, by Kaie Kubjas and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2402.00949)\n- [HTML (experimental)](https://arxiv.org/html/2402.00949v2)\n- [TeX Source](https://arxiv.org/src/2402.00949)\n- [Other Formats](https://arxiv.org/format/2402.00949)\n\n[view license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\n\nCurrent browse context:\n\nmath.AG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2402.00949&function=prev&context=math.AG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2402.00949&function=next&context=math.AG)\n\n[new](https://arxiv.org/list/math.AG/new) \\| [recent](https://arxiv.org/list/math.AG/recent) \\| [2024-02](https://arxiv.org/list/math.AG/2024-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2402.00949?context=cs) [cs.LG](https://arxiv.org/abs/2402.00949?context=cs.LG) [math](https://arxiv.org/abs/2402.00949?context=math) [stat](https://arxiv.org/abs/2402.00949?context=stat) [stat.ML](https://arxiv.org/abs/2402.00949?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.00949)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.00949)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.00949)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2402.00949) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Geometry of polynomial neural networks [35]",
          "cleaned_query": "Geometry of polynomial neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Activation degree thresholds and expressiveness of ...",
          "url": "https://arxiv.org/html/2408.04569",
          "content": "Activation degree thresholds and expressiveness of polynomial neural networks\n# Activation degree thresholds and expressiveness of polynomial neural\u00a0networks\nBella Finkel111This material is based\nupon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No.\n2137424. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the\nauthors and do not necessarily reflect the views of the National Science Foundation. Support was also provided by\nthe Graduate School and the Office of the Vice Chancellor for Research at the University of Wisconsin-Madison\nwith funding from the Wisconsin Alumni Research Foundation., Jose Israel Rodriguez222This research was partially supported by the Alfred P. Sloan Foundation., Chenxi Wu, Thomas Yahl\n###### Abstract\nWe study the expressive power of deep polynomial neural networks through the geometry of their neurovariety. We introduce the notion of the activation degree threshold of a network architecture to express when the dimension of the neurovariety achieves its theoretical maximum. We prove the existence of the activation degree threshold for all polynomial neural networks without width-one bottlenecks and demonstrate a universal upper bound that is quadratic in the width of largest size. In doing so, we prove the high activation degree conjecture of Kileel, Trager, and Bruna.\nCertain structured architectures have exceptional activation degree thresholds, making them especially expressive in the sense of their neurovariety dimension.\nIn this direction, we prove that polynomial neural networks with equi-width\u00a0architectures are maximally expressive by showing their activation degree threshold is one.\n## Introduction\nCharacterizing the functional space of artificial neural networks is a fundamental project in deep learning theory. When the functional space is semialgebraic, its geometric properties offer insight and mathematical intuition into the learning dynamics. Such a characterization is a two-way street: it is foundational to explaining empirical phenomena and offers the possibility of designing innovative architectures based on desirable theoretical and practical properties encoded in the space. Semialgebraic machine learning models form a rich class that encompasses, for example, Rectified Linear Unit (ReLU) and polynomial neural networks.\nThe study of deep polynomial neural networks offers a distinct advantage in theoretical machine learning because the functional spaces of such networks are described by algebraic varieties. In particular, a polynomial neural network of fixed architecture and activation degree gives an algebraic map from the network\u2019s weights to a set of polynomials. The image of this map is the space of functions that can be represented by the network. Its closure is an affine variety known as a neurovariety. The algebro-geometric invariants of a neurovariety, such as its dimension, degree, and singularities, encode information about the network\u2019s training dynamics and expressiveness. For a detailed discussion on neuroalgebraic geometry detailing the relationship between invariants of a neurovariety and certain fundamental aspects of machine learning, see> [\n[> 29\n](https://arxiv.org/html/2408.04569v4#bib.bib29)> ]\n.\nThe most basic instantiation of a polynomial neural network occurs when the activation function is linear. Linear networks have received wide attention as simplified models for understanding the learning behavior of deep neural networks> [\n[> 22\n](https://arxiv.org/html/2408.04569v4#bib.bib22)> , [> 6\n](https://arxiv.org/html/2408.04569v4#bib.bib6)> ]\n. Even in the case of linear networks, when the neuromanifold is a determinantal variety, there is interesting geometry to explore> [\n[> 43\n](https://arxiv.org/html/2408.04569v4#bib.bib43)> , [> 25\n](https://arxiv.org/html/2408.04569v4#bib.bib25)> , [> 26\n](https://arxiv.org/html/2408.04569v4#bib.bib26)> ]\n. New developments in the general polynomial neural network setting have demonstrated the cross-disciplinary connections between algebraic geometry and machine learning and offered results on the viability of polynomial neural networks for applications> [\n[> 24\n](https://arxiv.org/html/2408.04569v4#bib.bib24)> , [> 40\n](https://arxiv.org/html/2408.04569v4#bib.bib40)> ]\n. Results on the choice of the activation degree and the dimension of the neurovariety have improved our understanding of the optimization process of these neural networks and the ability of shallow and deep neural networks to replicate target functions> [\n[> 27\n](https://arxiv.org/html/2408.04569v4#bib.bib27)> , [> 34\n](https://arxiv.org/html/2408.04569v4#bib.bib34)> , [> 5\n](https://arxiv.org/html/2408.04569v4#bib.bib5)> ]\n.\nWhile polynomial neural networks with fixed activation degree are not universal approximators> [\n[> 20\n](https://arxiv.org/html/2408.04569v4#bib.bib20)> ]\n, they become so when the activation degree is allowed to vary. An alternative measure of a polynomial neural network\u2019s expressiveness is the dimension of its neurovariety, which can be defined as the maximum number of independent vectors in the tangent space at a generic point. From the perspective of algebraic geometry, the dimension of an algebraic variety is perhaps its most basic invariant. Characterizations of the neurovariety associated with a neural network represent a distinct description of a network\u2019s expressiveness from universal approximation theorems. The dimension of the neurovariety is a simple and precise measure of the degrees of freedom of the model that admits exact computation.\nIn contrast to universal approximation theorems, the dimension of the neurovariety is a measure of the expressiveness of a given polynomial neural network. Thus, studying polynomial network expressiveness through the dimension of the neurovariety complements universal approximation results. For an explicit toy example illustrating neurovariety dimension, see> [\n[> 29\n](https://arxiv.org/html/2408.04569v4#bib.bib29)> , Section A]\n.\nIn practice, neural networks with polynomial output have been found to perform well in high-impact fields such as healthcare and finance, especially where the nature of the data is polynomial. Such applications include the prediction of epidemic behavior> [\n[> 14\n](https://arxiv.org/html/2408.04569v4#bib.bib14)> ]\n, quantification of the natural frequency of materials> [\n[> 12\n](https://arxiv.org/html/2408.04569v4#bib.bib12)> ]\n, analysis of financial time series data> [\n[> 15\n](https://arxiv.org/html/2408.04569v4#bib.bib15)> , [> 32\n](https://arxiv.org/html/2408.04569v4#bib.bib32)> ]\nand the improvement of three-dimensional shape representation for computer vision> [\n[> 45\n](https://arxiv.org/html/2408.04569v4#bib.bib45)> ]\n.\nIn these applications, by introducing higher-order interactions between inputs, polynomials model non-linear phenomena more efficiently.\nOther applications> [\n[> 21\n](https://arxiv.org/html/2408.04569v4#bib.bib21)> ]\ntake advantage of the fact that a homogeneous polynomial of degreeddinnnvariables can be identified with a symmetric tensor in(\u211dn)\u2297d\\\\left(\\\\mathbb{R}^{n}\\\\right)^{\\\\otimes d}that collects its coefficients. This description is also useful for obtaining theoretical results. It is exhaustive for shallow networks, where the function space coincides with a set of symmetric tensors of bounded rank> [\n[> 24\n](https://arxiv.org/html/2408.04569v4#bib.bib24)> ]\n. Moreover, it provides a lens for studying the loss surface of neural networks from an optimization perspective> [\n[> 44\n](https://arxiv.org/html/2408.04569v4#bib.bib44)> , [> 5\n](https://arxiv.org/html/2408.04569v4#bib.bib5)> ]\n.\nSubstantial literature has shown the expressive power of deep networks from the perspective of the number of linear regions> [\n[> 31\n](https://arxiv.org/html/2408.04569v4#bib.bib31)> ]\n, universal approximation and VC dimensions> [\n[> 42\n](https://arxiv.org/html/2408.04569v4#bib.bib42)> , [> 37\n](https://arxiv.org/html/2408.04569v4#bib.bib37)> ]\n, a measure of complexity called \u201ctrajectory length\u201d> [\n[> 30\n](https://arxiv.org/html/2408.04569v4#bib.bib30)> ]\n, and the exact class of functions representable by ReLU networks of various depths> [\n[> 19\n](https://arxiv.org/html/2408.04569v4#bib.bib19)> ]\n. In another direction,> [\n[> 34\n](https://arxiv.org/html/2408.04569v4#bib.bib34)> ]\nstudied the effect of the width of convolutional neural networks on the loss landscape, and> [\n[> 28\n](https://arxiv.org/html/2408.04569v4#bib.bib28)> , [> 18\n](https://arxiv.org/html/2408.04569v4#bib.bib18)> ]\nprovided limits on the network width for universal approximation for ReLU networks, while> [\n[> 23\n](https://arxiv.org/html/2408.04569v4#bib.bib23)> ]\nprovided such a limit for arbitrary activation functions. The expressiveness of neural networks has also been studied via their topological complexity, for example by> [\n[> 7\n](https://arxiv.org/html/2408.04569v4#bib.bib7)> , [> 17\n](https://arxiv.org/html/2408.04569v4#bib.bib17)> ]\n. In particular,> [\n[> 35\n](https://arxiv.org/html/2408.04569v4#bib.bib35)> ]\nshowed that networks with non-increasing architecture result in a connected decision region. ReLU expressiveness results have also been studied using algebraic geometry, specifically tropical geometry> [\n[> 9\n](https://arxiv.org/html/2408.04569v4#bib.bib9)> , [> 17\n](https://arxiv.org/html/2408.04569v4#bib.bib17)> , [> 19\n](https://arxiv.org/html/2408.04569v4#bib.bib19)> ]\n.\nAn intuitive way of characterizing the expressiveness of a neural network is by the dimension of the space of representable functions as one allows its parameters to vary. For sigmoid activation functions this has been done in> [\n[> 1\n](https://arxiv.org/html/2408.04569v4#bib.bib1)> ]\n, for ReLU activation and non-increasing architectures, as well as a few others, this was done by> [\n[> 36\n](https://arxiv.org/html/2408.04569v4#bib.bib36)> , [> 17\n](https://arxiv.org/html/2408.0",
          "original_query": "Activation degree thresholds and expressiveness of polynomial neural networks [38]",
          "cleaned_query": "Activation degree thresholds and expressiveness of polynomial neural networks",
          "content_quality": "good_from_html"
        }
      ],
      "generated_ideas": [
        "Moment-Based Architecture Recovery for Polynomial Neural Networks\nDevelop an identification algorithm that recovers a PNN\u2019s layer widths (and possibly weights up to symmetries) from finitely many low-order input\u2013output moments, adapting Anandkumar et al.\u2019s tensor power method to the polynomial-feature setting. Prove conditions (activation degree threshold, no bottlenecks) under which the recovered decomposition is unique and stable under sampling noise.",
        "Learning Degree as a Predictor of Spurious-Critical-Point Counts\nConnect Fefferman\u2013Markel\u2019s \u201cfinitely many generic critical points\u201d phenomenon to the *learning degree* of Kubjas\u2013Li\u2013Wiesmann by proving bounds on the number of complex critical points of the empirical risk in terms of algebraic degree of the neurovariety. Provide an actionable pipeline: compute/estimate learning degree for an architecture, then predict optimization hardness and validate via controlled training experiments.",
        "Minimal Identifiability Conditions: From Genericity Assumptions to Verifiable Certificates\nReplace Fefferman\u2013Markel-style genericity conditions (e.g., irrational weight ratios) with checkable algebraic conditions for PNNs (e.g., Jacobian rank tests at random inputs, or nonvanishing discriminants of certain elimination ideals). The contribution would be a practical \u201cidentifiability certificate\u201d that, given trained weights, certifies local/global uniqueness (up to permutation/sign symmetries) of the represented polynomial.",
        "X-Rank View of Deep Polynomial Networks via Secant Varieties\nFormalize representations of deep PNN outputs as points on specific secant varieties, and define an \u201carchitecture-induced X-rank\u201d that measures minimal width profiles needed to realize a target polynomial. Use Comon\u2013Qi\u2013Usevich identifiability tools to derive when this X-rank decomposition is unique, yielding explicit depth/width tradeoffs beyond dimension counting.",
        "Optimal Width Schedules Under Activation Degree Threshold Constraints\nTurn Kileel\u2013Trager\u2013Bruna\u2019s empirical \u201cincrease then decrease\u201d width heuristic into an optimization problem constrained by activation degree thresholds (Finkel et al.). Produce provable width schedules that (i) achieve maximal neurovariety dimension at the smallest parameter count and (ii) minimize learning degree, giving a concrete recipe for architecture design in polynomial networks.",
        "Robust Parameter Recovery for PNNs Under Output Perturbations\nBuild a perturbation theory for recovering PNN weights/architecture from approximate functional evaluations (finite samples + noise), analogous to Wedin-type bounds in tensor methods. The deliverable is an end-to-end robust recovery algorithm with explicit error bounds scaling with sample size, noise level, and conditioning of the relevant moment/tensor maps.",
        "Singularity Stratification of Neurovarieties and Training Dynamics\nCompute and stratify singular loci of neurovarieties for families of PNN architectures (e.g., equi-width vs. bottlenecked) and relate these strata to observed training pathologies (plateaus, sharp minima, implicit bias). The key contribution is a concrete mapping from algebraic-geometric invariants (singular components, multiplicities) to measurable optimization behaviors in gradient-based training.",
        "From Meromorphic-Singularity Fingerprints to Network Extraction in Analytic Activations\nExtend Fefferman\u2013Markel\u2019s analytic continuation/pole-accumulation approach to broader analytic activations (e.g., rational, softplus-approximated meromorphic, or controlled complex-analytic surrogates). Produce a \u201csingularity fingerprint\u201d method that recovers depth and intermediate widths from complex-plane behavior, and compare its identifiability guarantees to algebraic methods used for PNNs.",
        "Degree-Efficient Universal Approximation with Variable Activation Degree Schedules\nUse activation degree thresholds to design networks where activation degrees vary by layer to minimize total polynomial degree while maximizing neurovariety dimension. The actionable result is a constructive scheme (layerwise degree assignment + width constraints) that approximates a target function class with provable bounds on required maximum degree and parameter count.",
        "Hybrid Tensor\u2013Neurovariety Algorithms for Exact Polynomial Fitting\nCombine tensor decomposition (for initialization/structure discovery) with algebraic elimination on neurovarieties (for exact refinement) to fit target polynomials exactly when they lie in the model class. The contribution is a two-stage algorithm: (i) moment/tensor-based coarse decomposition to estimate latent factors, then (ii) neurovariety-constrained solving to obtain exact weights and certify correctness/uniqueness."
      ],
      "generated_ideas_raw": "1. **Moment-Based Architecture Recovery for Polynomial Neural Networks**\n   Develop an identification algorithm that recovers a PNN\u2019s layer widths (and possibly weights up to symmetries) from finitely many low-order input\u2013output moments, adapting Anandkumar et al.\u2019s tensor power method to the polynomial-feature setting. Prove conditions (activation degree threshold, no bottlenecks) under which the recovered decomposition is unique and stable under sampling noise.\n\n2. **Learning Degree as a Predictor of Spurious-Critical-Point Counts**\n   Connect Fefferman\u2013Markel\u2019s \u201cfinitely many generic critical points\u201d phenomenon to the *learning degree* of Kubjas\u2013Li\u2013Wiesmann by proving bounds on the number of complex critical points of the empirical risk in terms of algebraic degree of the neurovariety. Provide an actionable pipeline: compute/estimate learning degree for an architecture, then predict optimization hardness and validate via controlled training experiments.\n\n3. **Minimal Identifiability Conditions: From Genericity Assumptions to Verifiable Certificates**\n   Replace Fefferman\u2013Markel-style genericity conditions (e.g., irrational weight ratios) with checkable algebraic conditions for PNNs (e.g., Jacobian rank tests at random inputs, or nonvanishing discriminants of certain elimination ideals). The contribution would be a practical \u201cidentifiability certificate\u201d that, given trained weights, certifies local/global uniqueness (up to permutation/sign symmetries) of the represented polynomial.\n\n4. **X-Rank View of Deep Polynomial Networks via Secant Varieties**\n   Formalize representations of deep PNN outputs as points on specific secant varieties, and define an \u201carchitecture-induced X-rank\u201d that measures minimal width profiles needed to realize a target polynomial. Use Comon\u2013Qi\u2013Usevich identifiability tools to derive when this X-rank decomposition is unique, yielding explicit depth/width tradeoffs beyond dimension counting.\n\n5. **Optimal Width Schedules Under Activation Degree Threshold Constraints**\n   Turn Kileel\u2013Trager\u2013Bruna\u2019s empirical \u201cincrease then decrease\u201d width heuristic into an optimization problem constrained by activation degree thresholds (Finkel et al.). Produce provable width schedules that (i) achieve maximal neurovariety dimension at the smallest parameter count and (ii) minimize learning degree, giving a concrete recipe for architecture design in polynomial networks.\n\n6. **Robust Parameter Recovery for PNNs Under Output Perturbations**\n   Build a perturbation theory for recovering PNN weights/architecture from approximate functional evaluations (finite samples + noise), analogous to Wedin-type bounds in tensor methods. The deliverable is an end-to-end robust recovery algorithm with explicit error bounds scaling with sample size, noise level, and conditioning of the relevant moment/tensor maps.\n\n7. **Singularity Stratification of Neurovarieties and Training Dynamics**\n   Compute and stratify singular loci of neurovarieties for families of PNN architectures (e.g., equi-width vs. bottlenecked) and relate these strata to observed training pathologies (plateaus, sharp minima, implicit bias). The key contribution is a concrete mapping from algebraic-geometric invariants (singular components, multiplicities) to measurable optimization behaviors in gradient-based training.\n\n8. **From Meromorphic-Singularity Fingerprints to Network Extraction in Analytic Activations**\n   Extend Fefferman\u2013Markel\u2019s analytic continuation/pole-accumulation approach to broader analytic activations (e.g., rational, softplus-approximated meromorphic, or controlled complex-analytic surrogates). Produce a \u201csingularity fingerprint\u201d method that recovers depth and intermediate widths from complex-plane behavior, and compare its identifiability guarantees to algebraic methods used for PNNs.\n\n9. **Degree-Efficient Universal Approximation with Variable Activation Degree Schedules**\n   Use activation degree thresholds to design networks where activation degrees vary by layer to minimize total polynomial degree while maximizing neurovariety dimension. The actionable result is a constructive scheme (layerwise degree assignment + width constraints) that approximates a target function class with provable bounds on required maximum degree and parameter count.\n\n10. **Hybrid Tensor\u2013Neurovariety Algorithms for Exact Polynomial Fitting**\n   Combine tensor decomposition (for initialization/structure discovery) with algebraic elimination on neurovarieties (for exact refinement) to fit target polynomials exactly when they lie in the model class. The contribution is a two-stage algorithm: (i) moment/tensor-based coarse decomposition to estimate latent factors, then (ii) neurovariety-constrained solving to obtain exact weights and certify correctness/uniqueness.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Moment-Based Architecture Recovery for Polynomial Neural Networks\nDevelop an identification algorithm that recovers a PNN\u2019s layer widths (and possibly weights up to symmetries) from finitely many low-",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Learning Degree as a Predictor of Spurious-Critical-Point Counts\nConnect Fefferman\u2013Markel\u2019s \u201cfinitely many generic critical points\u201d phenomenon to the *learning degree* of Kubjas\u2013Li\u2013Wiesmann by proving",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Minimal Identifiability Conditions: From Genericity Assumptions to Verifiable Certificates\nReplace Fefferman\u2013Markel-style genericity conditions (e.g., irrational weight ratios) with checkable algebrai",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "X-Rank View of Deep Polynomial Networks via Secant Varieties\nFormalize representations of deep PNN outputs as points on specific secant varieties, and define an \u201carchitecture-induced X-rank\u201d that meas",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Optimal Width Schedules Under Activation Degree Threshold Constraints\nTurn Kileel\u2013Trager\u2013Bruna\u2019s empirical \u201cincrease then decrease\u201d width heuristic into an optimization problem constrained by activati",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Robust Parameter Recovery for PNNs Under Output Perturbations\nBuild a perturbation theory for recovering PNN weights/architecture from approximate functional evaluations (finite samples + noise), anal",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Singularity Stratification of Neurovarieties and Training Dynamics\nCompute and stratify singular loci of neurovarieties for families of PNN architectures (e.g., equi-width vs. bottlenecked) and relate",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "From Meromorphic-Singularity Fingerprints to Network Extraction in Analytic Activations\nExtend Fefferman\u2013Markel\u2019s analytic continuation/pole-accumulation approach to broader analytic activations (e.g.",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Degree-Efficient Universal Approximation with Variable Activation Degree Schedules\nUse activation degree thresholds to design networks where activation degrees vary by layer to minimize total polynomi",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Hybrid Tensor\u2013Neurovariety Algorithms for Exact Polynomial Fitting\nCombine tensor decomposition (for initialization/structure discovery) with algebraic elimination on neurovarieties (for exact refinem",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 20,
      "paper_title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
      "contribution": "A systematic diagnosis showing that GPU/kernel-level floating-point non\u2011associativity and reduction ordering produce large, reproducibility\u2011breaking output differences in LLM inference, and a lightweight inference pipeline to mitigate these numerical sources of nondeterminism.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 11101,
      "output_tokens": 955,
      "predecessor_details": [
        {
          "success": true,
          "title": "Impacts of floating-point non-associativity on reproducibility ...",
          "url": "https://arxiv.org/html/2408.05148",
          "content": "Impacts of floating-point non-associativity on reproducibility for HPC and deep learning applications This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/ downloads/doe-public-access-plan). \u2225Equal contributions\n# Impacts of floating-point non-associativity on reproducibility for HPC and deep learning applications\u2020\u2020thanks:This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/ downloads/doe-public-access-plan).\n\u2225Equal contributions\nSanjif Shanmugavelu\u22254\u2217,\nMathieu Taillefumier\u22252\u2217,\nChristopher Culver4,\nOscar Hernandez3,\nMark Coletti3 and\nAda Sedova\u2225\u221734Maxeler Technologies, a Groq Company. 3 Hammersmith Grove, W6 0ND, London, UK\n2ETH Zurich/Swiss National Supercomputing Centre (CSCS). Via Trevano 131, 6900 Lugano, Switzerland3Oak Ridge National Laboratory. Oak Ridge, TN, USA\n\u2217Corresponding emails:\nsshanmugavelu@groq.com\ntmathieu@ethz.ch\nsedovaaa@ornl.gov\n###### Abstract\nRun to run variability in parallel programs caused by floating-point non-associativity has been known to significantly affect reproducibility in iterative algorithms, due to accumulating errors. Non-reproducibility can critically affect the efficiency and effectiveness of correctness testing for stochastic programs. Recently, the sensitivity of deep learning training and inference pipelines to floating-point non-associativity has been found to sometimes be extreme. It can prevent certification for commercial applications, accurate assessment of robustness and sensitivity, and bug detection. New approaches in scientific computing applications have coupled deep learning models with high-performance computing, leading to an aggravation of debugging and testing challenges. Here we perform an investigation of the statistical properties of floating-point non-associativity within modern parallel programming models, and analyze performance and productivity impacts of replacing atomic operations with deterministic alternatives on GPUs. We examine the recently-added deterministic options in PyTorch within the context of GPU deployment for deep learning, uncovering and quantifying the impacts of input parameters triggering run to run variability and reporting on the reliability and completeness of the documentation. Finally, we evaluate the strategy of exploiting automatic determinism that could be provided by deterministic hardware, using the GroqLPUTMsuperscriptLPUTM\\\\text{LPU}^{\\\\text{TM}}LPU start\\_POSTSUPERSCRIPT TM end\\_POSTSUPERSCRIPTaccelerator for inference portions of the deep learning pipeline. We demonstrate the benefits that a hardware-based strategy can provide within reproducibility and correctness efforts.\n###### Index Terms:\nReproducibility of results, floating-point arithmetic, parallel programming, high-performance computing, deep learning\n## IIntroduction\nRun to run variability of a program, despite identical inputs and software stack, is usually assumed to be negligible, even in heterogeneous parallel programming> [\n[> 1\n](https://arxiv.org/html/2408.05148v3#bib.bib1)> ]\n. However, variability caused byfloating-point non-associativity (FPNA)coupled with asynchronous parallel operations such as reductions can be substantial> [\n[> 2\n](https://arxiv.org/html/2408.05148v3#bib.bib2)> ]\n, especially for massively parallel programs using iterative stochastic routines, such as those implementing optimization algorithms like conjugate gradient> [\n[> 3\n](https://arxiv.org/html/2408.05148v3#bib.bib3)> ]\n. It can also mask errors within threshold-based correctness testing schemes> [\n[> 4\n](https://arxiv.org/html/2408.05148v3#bib.bib4)> ]\n, which are often used in scientific computing programs such as molecular simulation> [\n[> 5\n](https://arxiv.org/html/2408.05148v3#bib.bib5)> , [> 6\n](https://arxiv.org/html/2408.05148v3#bib.bib6)> , [> 7\n](https://arxiv.org/html/2408.05148v3#bib.bib7)> ]\n, making debugging difficult.Deep neural network (DNN)training also involves iterative, stochastic algorithms coupled with non-linear activation functions. This combination has been found to cause extreme sensitivity to bit-level numerical changes, such as those caused byFPNA> [\n[> 8\n](https://arxiv.org/html/2408.05148v3#bib.bib8)> , [> 9\n](https://arxiv.org/html/2408.05148v3#bib.bib9)> , [> 10\n](https://arxiv.org/html/2408.05148v3#bib.bib10)> ]\n.DNNinference can also suffer from such sensitivity, due to the influence of non-linear activation functions, although the effects may be reduced due to the absence of the compounding errors caused by the iterative training schemes. Recently, a number of studies have shown that this run to run variability in the fullDNNtraining and inference pipeline can lead to unacceptably large differences in the predictions produced by a model. They have thwarted efforts to release reproducible commercial applications in safety-critical sectors such as medical diagnostics> [\n[> 11\n](https://arxiv.org/html/2408.05148v3#bib.bib11)> ]\nand autonomous driving> [\n[> 8\n](https://arxiv.org/html/2408.05148v3#bib.bib8)> ]\n. In addition, recent reports have found that all of the majordeep learning (DL)software frameworks such as TensorFlow and PyTorch contain hundreds of bugs originating from all software levels, including environment misuse, incorrect assignment, and concurrency issues such as data races> [\n[> 12\n](https://arxiv.org/html/2408.05148v3#bib.bib12)> ]\n; bugs in these frameworks can be disastrous> [\n[> 13\n](https://arxiv.org/html/2408.05148v3#bib.bib13)> ]\n, especially when they are silent> [\n[> 14\n](https://arxiv.org/html/2408.05148v3#bib.bib14)> ]\n, and high runtime variability can make it extremely difficult to detect bugs in these deep, multi-language, multi-level parallel stacks.\nWithin scientifichigh-performance computing (HPC), the incorporation ofDLinto traditional approaches for simulation has become increasingly popular> [\n[> 15\n](https://arxiv.org/html/2408.05148v3#bib.bib15)> , [> 16\n](https://arxiv.org/html/2408.05148v3#bib.bib16)> , [> 17\n](https://arxiv.org/html/2408.05148v3#bib.bib17)> ]\n. Molecular simulation, for example, has usedDNNmodels for interatomic potentials, which promise quantum mechanical accuracy at the cost of simpler, empirical models; this translates to speedups of several orders of magnitude and the approach received the ACM Gordon Bell Award in 2020> [\n[> 18\n](https://arxiv.org/html/2408.05148v3#bib.bib18)> ]\n. Our preliminary tests using identical inputs for training and inference pipelines for these types of models revealed a level of non-reproducibility for prediction of forces that would be unacceptable for traditional quantum mechanicalHPCprograms> [\n[> 19\n](https://arxiv.org/html/2408.05148v3#bib.bib19)> ]\n. Besides non-deterministic kernels within the deep learning framework itself, any external kernels programmed by the scientific simulation developers which contain non-determinism, that feed into training pipelines> [\n[> 18\n](https://arxiv.org/html/2408.05148v3#bib.bib18)> ]\n, can also introduce large runtime non-reproducibility.\nHere we present a systematic analysis of the effects ofFPNAwithin asynchronous parallel kernels ongraphics processing unit (GPU)accelerators, together with several metrics that quantify this variability, within parallel programming schemes and in PyTorch functions, as well as within full end-to-end training and inference pipelines. We assess the impact of these effects on the ability to perform correctness testing and to produce reproducible scientific results. We also study solutions, using programming approaches, and via deterministic hardware for which we perform experiments with the GroqLPUTMsuperscriptLPUTM\\\\text{LPU}^{\\\\text{TM}}LPU start\\_POSTSUPERSCRIPT TM end\\_POSTSUPERSCRIPTdeterministic inference chip.\n## IIMetrics for measuring the variability of non-deterministic functions\nWe defined metricsV\ud835\udc49Vitalic\\_Vfor quantifying the run to run variability in output values for implementations of functions with scalar and multidimensional (array) inputs and outputs, following a similar approach used in error analysis> [\n[> 20\n](https://arxiv.org/html/2408.05148v3#bib.bib20)> ]\n. These are explained below.\nIn all cases,V=0\ud835\udc490V=0italic\\_V = 0if two implementations are bitwise identical, nonzero otherwise, and increasing as variability increases.\n#### II-1Scalar-valued outputs\nWe useVs\u2062(f)=1\u2212|fnd/fd|subscript\ud835\udc49\ud835\udc60\ud835\udc531subscript\ud835\udc53ndsubscript\ud835\udc53dV\\_{s}(f)=1-\\\\left|f\\_{\\\\textsc{nd}}/f\\_{\\\\textsc{d}}\\\\right|italic\\_V start\\_POSTSUBSCRIPT italic\\_s end\\_POSTSUBSCRIPT ( italic\\_f ) = 1 - | italic\\_f start\\_POSTSUBSCRIPT nd end\\_POSTSUBSCRIPT / italic\\_f start\\_POSTSUBSCRIPT d end\\_POSTSUBSCRIPT |to quantify the bitwise non-determinism between the outputs of two implementations of some functionf\ud835\udc53fitalic\\_f,\nwhere the subscriptsndanddlabel the non-deterministic and deterministic implementations respectively.\n#### II-2Array outputs\nTo quantify t",
          "original_query": "Impacts of floating-point non-associativity on reproducibility for hpc and deep learning applications (2024)",
          "cleaned_query": "Impacts of floating-point non-associativity on reproducibility for hpc and deep learning applications",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "Efficient GEMM in CUDA \u2014 NVIDIA CUTLASS Documentation",
          "url": "https://docs.nvidia.com/cutlass/media/docs/cpp/efficient_gemm.html",
          "content": "Efficient GEMM in CUDA &#8212; NVIDIA CUTLASS Documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![NVIDIA CUTLASS Documentation - Home](../../../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg)![NVIDIA CUTLASS Documentation - Home](../../../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg)\nNVIDIA CUTLASS Documentation\n](../../../index.html)\n**SearchCtrl+K\n******\n**SearchCtrl+K\n[![NVIDIA CUTLASS Documentation - Home](../../../_static/nvidia-logo-horiz-rgb-blk-for-screen.svg)![NVIDIA CUTLASS Documentation - Home](../../../_static/nvidia-logo-horiz-rgb-wht-for-screen.svg)\nNVIDIA CUTLASS Documentation\n](../../../index.html)\n******\n![ALT](../../../_images/gemm-hierarchy-with-epilogue-no-labels.png)\n# Efficient GEMM in CUDA[#](#efficient-gemm-in-cuda)\nCUTLASS implements the hierarchically blocked structure described in[CUTLASS: Fast Linear Algebra in CUDA C++](https://devblogs.nvidia.com/cutlass-linear-algebra-cuda/)and the[CUTLASS GTC2018 talk](http://on-demand.gputechconf.com/gtc/2018/presentation/s8854-cutlass-software-primitives-for-dense-linear-algebra-at-all-levels-and-scales-within-cuda.pdf).\n## Hierarchical Structure[#](#hierarchical-structure)\nThe basic triple loop nest computing matrix multiply may be blocked and tiled to match\nconcurrency in hardware, memory locality, and parallel programming models. In CUTLASS,\nGEMM is mapped to NVIDIA GPUs with the structure illustrated by the following loop nest.\n```\nfor(intcta\\_n=0;cta\\_n&lt;GemmN;cta\\_n+=CtaTileN){// for each threadblock\\_y } threadblock-level concurrencyfor(intcta\\_m=0;cta\\_m&lt;GemmM;cta\\_m+=CtaTileM){// for each threadblock\\_x }for(intcta\\_k=0;cta\\_k&lt;GemmK;cta\\_k+=CtaTileK){// &quot;GEMM mainloop&quot; - no unrolling// - one iteration of this loop is one &quot;stage&quot;//for(intwarp\\_n=0;warp\\_n&lt;CtaTileN;warp\\_n+=WarpTileN){// for each warp\\_y } warp-level parallelismfor(intwarp\\_m=0;warp\\_m&lt;CtaTileM;warp\\_m+=WarpTileM){// for each warp\\_x }//for(intwarp\\_k=0;warp\\_k&lt;CtaTileK;warp\\_k+=WarpTileK){// fully unroll across CtaTileK// - one iteration of this loop is one &quot;k Group&quot;//for(intmma\\_k=0;mma\\_k&lt;WarpTileK;mma\\_k+=MmaK){// for each mma instruction } instruction-level parallelismfor(intmma\\_n=0;mma\\_n&lt;WarpTileN;mma\\_n+=MmaN){// for each mma instruction }for(intmma\\_m=0;mma\\_m&lt;WarpTileM;mma\\_m+=MmaM){// for each mma instruction }//mma\\_instruction(d,a,b,c);// TensorCore matrix computation}// for mma\\_m}// for mma\\_n}// for mma\\_k}// for warp\\_k}// for warp\\_m}// for warp\\_n}// for cta\\_k}// for cta\\_m}// for cta\\_n\n```\nThis tiled loop nest targets concurrency among\n* threadblocks,\n* warps, and\n* CUDA and Tensor Cores.\nIt takes advantage of memory locality within\n* shared memory and\n* registers.\nThe figure below illustrates the flow of data within this structure.\nThis is the hierarchical GEMM computation embodied by CUTLASS. Each stage depicts a\nnested level of tiling which corresponds to a layer of concurrency within the CUDA execution model and to a\nlevel within the memory hierarchy, becoming increasingly finer moving left to right.\n![ALT](../../../_images/gemm-hierarchy-with-epilogue.png)\n### Threadblock-level GEMM[#](#threadblock-level-gemm)\nEach threadblock computes its portion of the output GEMM by iteratively loading tiles of input\nmatrices and computing an accumulated matrix product. At the threadblock level, data are loaded from\nglobal memory. The blocking strategy in general is key to achieving efficiency. However, the programmer\nmust balance multiple conflicting goals. A\nlarger threadblock means fewer fetches from global memory, thereby ensuring that DRAM bandwidth\ndoes not become a bottleneck.\nHowever, large threadblock tiles may not match the dimensions of the problem well. If either the\nGEMM*M*or*N*dimension is small, some threads within the threadblock may not perform meaningful\nwork, as the threadblock may be partially outside the bounds of the problem. If both*M*and*N*are small while*K*is large, this scheme may launch relatively few threadblocks and fail to\nmake full use of all multiprocessors within the GPU. Strategies to optimize performance for this case,\nas described in the section[Parallelized Reductions](#parallelized-reductions),\npartition the GEMM K dimension across multiple threadblocks or multiple warps. These threadblocks\nor warps compute matrix products in parallel; the products are then reduced to compute the result.\nIn CUTLASS, the dimensions of the threadblock tile are specified as`ThreadblockShape::{kM,kN,kK}`and may be tuned to specialize the GEMM computation for the target processor and dimensions of\nthe GEMM problem.\n### Warp-level GEMM[#](#warp-level-gemm)\nThe warp-level GEMM maps to the warp-level parallelism within the CUDA execution model. Multiple\nwarps within a threadblock fetch data from shared memory into registers and perform computations.\nWarp-level GEMMs may be implemented either by TensorCores issuing[mma.sync](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma)or[wmma](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-wmma-mma)instructions, or by thread-level matrix computations issued to CUDA cores.\nFor maximum performance, access to shared memory should be bank conflict free. To maximize data\nreuse within the warp, a large warp-level GEMM tile should be chosen.\n### Thread-level GEMM[#](#thread-level-gemm)\nAt the lowest level of blocking, each thread is responsible for processing a certain number of\nelements. Threads cannot access each other\u2019s registers, so we choose an organization that enables\nreuse of values held in registers for multiple math instructions. This results in a 2D tiled\nstructure within a thread, in which each thread issues a sequence of independent math instructions\nto the CUDA cores and computes an accumulated outer product.\nSGEMM, IGEMM, HGEMM, and DGEMM are computed by SIMT math instructions issued by thread-level matrix multiply\nprocedures.\n## Epilogue[#](#epilogue)\nThe above code focuses only on the matrix multiply computation**C = AB**whose result is\nheld in the registers of each thread within the threadblock. The mapping of logical elements\nin the output tile to each thread is chosen to maximize performance of the matrix multiply\ncomputation but does not result in efficient, coalesced loads and stores to global memory.\nThe epilogue is a separate phase in which threads exchange data through shared memory then\ncooperatively access global memory using efficient striped access patterns. It is also\nthe phase in which linear scaling and other elementwise operations may be conveniently\ncomputed using the matrix product results as inputs.\nCUTLASS defines several typical epilogue operations such as linear scaling and clamping,\nbut other device-side function call operators may be used to perform custom operations.\n## Optimizations[#](#optimizations)\nThe hierarchical structure described above yields an efficient mapping to the CUDA execution model and\nCUDA/TensorCores in NVIDIA GPUs. The following sections describe strategies for obtaining peak performance\nfor all corners of the design space, maximizing parallelism and exploiting data locality wherever possible.\n### Pipelining[#](#pipelining)\nThe blocked structure demands a large storage allocation within the registers of each CUDA thread. The\naccumulator elements typically occupy at least half a thread\u2019s total register budget. Consequently,\noccupancy \u2013the number of concurrent threads, warps, and threadblocks \u2013is relatively low compared\nto other classes of GPU workloads. This limits the GPU\u2019s ability to hide memory latency and other stalls\nby context switching to other concurrent threads within an SM.\nTo mitigate the effects of memory latency, CUTLASS uses*software pipelining*to overlap memory accesses\nwith other computation within a thread. CUTLASS accomplishes this by double buffering at the\nfollowing scopes.\n* **Threadblock-scoped shared memory tiles:**two tiles are allocated in shared memory.\nOne is used to load data for the current matrix operation,\nwhile the other tile is used to buffer data loaded from global memory\nfor the next mainloop iteration.\n* **Warp-scoped matrix fragments:**two fragments are allocated within registers.\nOne fragment is passed to CUDA and TensorCores during the current matrix computation,\nwhile the other is used to receive shared memory fetch returns\nfor the next warp-level matrix operation.\nThe following diagram illustrates the efficient, pipelined mainloop body used in CUTLASS GEMMs.\n![ALT](../../../_images/software-pipeline.png)\n### Threadblock Rasterization[#](#threadblock-rasterization)\nTo maximize reuse of data held in the last level cache, CUTLASS defines several functions to\naffect the mapping of threadblocks to logical partitions of the GEMM problem. These map\nconsecutively launched threadblocks to packed two-dimensional regions of the partitioned GEMM\nproblem to increase the probability that these will access the same tiles of global memory at\napproximately the same time.\nSeveral functions are defined in[cutlass/gemm/threadblock\\_swizzle.h](https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/gemm/threadblock/threadblock_swizzle.h).\n### Parallelized Reductions[#](#parallelized-reductions)\n**Split K - reduction across threadblocks**\nMatrix product computations expose parallelism among*O(MN)*independent inner product\ncomputations. For sufficiently large problem sizes, a GEMM kernel in CUTLASS may approach\nthe theoretical maximum computational throughput. For small problems, however, there are\ntoo few threadblocks to efficiently occupy the entire GPU.\nAs a recourse, parallelizing the reduction performed during the inner product computation\nenables more threadblocks to execute concurrently while still taking advantage of the throughput\nbenefits of large threadblock-level GEMM tiles.\nCUTLASS implements parallel reductions across threadblocks",
          "original_query": "Efficient GEMM in CUTLASS (NVIDIA Developer Documentation)",
          "cleaned_query": "Efficient GEMM in CUTLASS (NVIDIA Developer Documentation)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Orca: A Distributed Serving System for Transformer-Based ... - USENIX",
          "url": "https://www.usenix.org/system/files/osdi22-yu.pdf",
          "content": "This paper is included in the Proceedings of the\n16th USENIX Symposium on Operating Systems \nDesign and Implementation.\nJuly 11\u201313, 2022 \u2022 Carlsbad, CA, USA\n978-1-939133-28-1\nOpen access to the Proceedings of the \n16th USENIX Symposium on Operating \nSystems Design and Implementation \nis sponsored by\nOrca: A Distributed Serving System for \nTransformer-Based Generative Models\nGyeong-In Yu and Joo Seong Jeong, Seoul National University; \nGeon-Woo Kim, FriendliAI and Seoul National University; Soojeong Kim, FriendliAI; \nByung-Gon Chun, FriendliAI and Seoul National University\nhttps://www.usenix.org/conference/osdi22/presentation/yu\nORCA: A Distributed Serving System for\nTransformer-Based Generative Models\nGyeong-In Yu\nSeoul National University\nJoo Seong Jeong\nSeoul National University\nGeon-Woo Kim\nFriendliAI\nSeoul National University\nSoojeong Kim\nFriendliAI\nByung-Gon Chun\u2217\nFriendliAI\nSeoul National University\nAbstract\nLarge-scale Transformer-based models trained for generation\ntasks (e.g., GPT-3) have recently attracted huge interest, em\u0002phasizing the need for system support for serving models in\nthis family. Since these models generate a next token in an au\u0002toregressive manner, one has to run the model multiple times\nto process an inference request where each iteration of the\nmodel generates a single output token for the request. How\u0002ever, existing systems for inference serving do not perform\nwell on this type of workload that has a multi-iteration char\u0002acteristic, due to their inflexible scheduling mechanism that\ncannot change the current batch of requests being processed;\nrequests that have finished earlier than other requests in a\nbatch cannot return to the client, while newly arrived requests\nhave to wait until the current batch completely finishes.\nIn this paper, we propose iteration-level scheduling, a new\nscheduling mechanism that schedules execution at the gran\u0002ularity of iteration (instead of request) where the scheduler\ninvokes the execution engine to run only a single iteration of\nthe model on the batch. In addition, to apply batching and\niteration-level scheduling to a Transformer model at the same\ntime, we suggest selective batching, which applies batching\nonly to a selected set of operations. Based on these two tech\u0002niques, we have implemented a distributed serving system\ncalled ORCA, with additional designs for scalability to models\nwith hundreds of billions of parameters. Our evaluation on a\nGPT-3 175B model shows that ORCA can significantly out\u0002perform NVIDIA FasterTransformer in terms of both latency\nand throughput: 36.9\u00d7 throughput improvement at the same\nlevel of latency.\n1 Introduction\nLanguage generation tasks are becoming increasingly\nparamount to many types of applications, such as chatbot [9,\n52], summarization [41,45,54], code generation [13], and cap\u0002tion generation [65,66]. Moreover, recent works published by\n\u2217Corresponding author.\nAI21 Labs [37], DeepMind [26,48], Google [15,21,63], Meta\nPlatforms [10,67], Microsoft [50], Microsoft & NVIDIA [59],\nand OpenAI [12] have reported that every language process\u0002ing task, including translation [11, 17], classification [20, 53],\nquestion-answering [32, 33, 40] and more, can be cast as a\nlanguage generation problem and have shown great improve\u0002ments along this direction. The rise of generative models is\nnot limited to the language domain; the AI community has\nalso given growing interest to generation problems in other do\u0002mains such as image, video, speech, or a mixture of multiple\ndomains [19,38,51,62]. At the heart of generative models lies\nthe Transformer architecture [60] and its variants [15, 47\u201349].\nBy relying on the attention mechanism [60], Transformer\nmodels can learn better representations where each element\nof the sequence may have a direct connection with every other\nelement, which was not possible in recurrent models [25].\nTo use generative models in real-world applications, we\noften delegate the inference procedure to a separate service\nresponsible for ML inference serving. The growing demands\nfor this service, which should provide inference results for\nclient requests at low latency and high throughput, have fa\u0002cilitated the development of inference serving systems such\nas Triton Inference Server [7] and TensorFlow Serving [42].\nThese systems can use a separately-developed DNN execution\nengine to perform the actual tensor operations. For example,\nwe can deploy a service for language generation tasks by\nusing a combination of Triton and FasterTransformer [4], an\nexecution engine optimized for the inference of Transformer\u0002based models. In this case, Triton is mainly responsible for\ngrouping multiple client requests into a batch, while Faster\u0002Transformer receives the batch from Triton and conducts the\ninference procedure in the batched manner.\nUnfortunately, we notice that the existing inference sys\u0002tems, including both the serving system layer and the execu\u0002tion engine layer, have limitations in handling requests for\nTransformer-based generative models. Since these models are\ntrained to generate a next token in an autoregressive manner,\none should run the model as many times as the number of to\u0002kens to generate, while for other models like ResNet [24] and\nUSENIX Association 16th USENIX Symposium on Operating Systems Design and Implementation 521\nBERT [18] a request can be processed by running the model\nonce. That is, in order to process a request to the generative\nmodel, we have to run multiple iterations of the model; each\niteration generates a single output token, which is used as\nan input in the following iteration. Such multi-iteration char\u0002acteristic calls into question the current design of inference\nsystems, where the serving system schedules the execution\nof the engine at the granularity of request. Under this design,\nwhen the serving system dispatches a batch of requests to\nthe engine, the engine returns inference results for the entire\nbatch at once after processing all requests within the batch.\nAs different client requests may require different numbers of\niterations for processing, requests that have finished earlier\nthan others in the batch cannot return to the client, resulting\nin an increased latency. Requests arrived after dispatching the\nbatch also should wait for processing the batch, which can\nsignificantly increase the requests\u2019 queueing time.\nIn this paper, we propose to schedule the execution of the\nengine at the granularity of iteration instead of request. In\nparticular, the serving system invokes the engine to run only a\nsingle iteration of the model on the batch. As a result, a newly\narrived request can be considered for processing after waiting\nfor only a single iteration of the model. The serving system\nchecks whether a request has finished processing after every\nreturn from the engine \u2013 hence the finished requests can also\nbe returned to the clients immediately.\nNevertheless, a noticeable challenge arises when we at\u0002tempt to apply batching and the iteration-level scheduling at\nthe same time. Unlike the canonical request-level scheduling,\nthe proposed scheduling can issue a batch of requests where\neach request has so far processed a different number of tokens.\nIn such a case, the requests to the Transformer model cannot\nbe processed in the batched manner because the attention\nmechanism calls for non-batchable tensor operations whose\ninput tensors have variable shapes depending on the number\nof processed tokens.\nTo address this challenge, we suggest to apply batching\nonly to a selected set of operations, which we call selective\nbatching. By taking different characteristics of operations into\naccount, selective batching splits the batch and processes each\nrequest individually for the Attention1 operation while apply\u0002ing batching to other operations of the Transformer model.\nWe observe that the decision not to batch the executions of\nthe Attention operation has only a small impact on efficiency.\nSince the Attention operation is not associated with any model\nparameters, applying batching to Attention has no benefit of\nreducing the amount of GPU memory reads by reusing the\nloaded parameters across multiple requests.\nBased on these techniques, we design and implement\nORCA, a distributed serving system for Transformer-based\ngenerative models. In order to handle large-scale models,\n1\nIn some literature the Attention operation has an extended definition that\nincludes linear layers (QKV Linear and Attn Out Linear; Figure 1b). On the\nother hand, we use a narrow definition as described in Figure 1b.\nORCA adopts parallelization strategies including intra-layer\nand inter-layer model parallelism, which were originally de\u0002veloped by training systems [55, 58] for Transformer models.\nWe also devise a new scheduling algorithm for the proposed\niteration-level scheduling, with additional considerations for\nmemory management and pipelined execution across work\u0002ers.\nWe evaluate ORCA using OpenAI GPT-3 [12] models with\nvarious configurations, scaling up to 341B of parameters. The\nresults show that ORCA significantly outperforms FasterTrans\u0002former [4], showing 36.9\u00d7 throughput improvement at the\nsame level of latency. While we use a language model as\na driving example throughout the paper and conduct experi\u0002ments only on language models, generative models in other\ndomains can benefit from our approach as long as the mod\u0002els are based on the Transformer architecture and use the\nautoregressive generation procedure [19, 38, 51, 62].\n2 Background\nWe provide background on the inference procedure of\nGPT [12, 47], a representative example of Transformer-based\ngenerative models that we use throughout this paper, and ML\ninference serving systems.\nInference procedure of GPT. GPT is an autoregressive\nlanguage model based on one of architectural variants of\nTransformer [60]. It takes text as input and produces new text\nas output. In particular, the model receives a sequence of input\ntokens and then completes the sequence by generating subse\u0002q",
          "original_query": "Orca: A distributed serving system for Transformer-Based generative models (OSDI 2022)",
          "cleaned_query": "Orca: A distributed serving system for Transformer-Based generative models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Nondeterminism and Instability in Neural Network Optimization - arXiv",
          "url": "https://arxiv.org/abs/2103.04514",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2103.04514** (cs)\n\n\\[Submitted on 8 Mar 2021 ( [v1](https://arxiv.org/abs/2103.04514v1)), last revised 10 Jul 2021 (this version, v3)\\]\n\n# Title:Nondeterminism and Instability in Neural Network Optimization\n\nAuthors: [Cecilia Summers](https://arxiv.org/search/cs?searchtype=author&query=Summers,+C), [Michael J. Dinneen](https://arxiv.org/search/cs?searchtype=author&query=Dinneen,+M+J)\n\nView a PDF of the paper titled Nondeterminism and Instability in Neural Network Optimization, by Cecilia Summers and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2103.04514)\n\n> Abstract:Nondeterminism in neural network optimization produces uncertainty in performance, making small improvements difficult to discern from run-to-run variability. While uncertainty can be reduced by training multiple model copies, doing so is time-consuming, costly, and harms reproducibility. In this work, we establish an experimental protocol for understanding the effect of optimization nondeterminism on model diversity, allowing us to isolate the effects of a variety of sources of nondeterminism. Surprisingly, we find that all sources of nondeterminism have similar effects on measures of model diversity. To explain this intriguing fact, we identify the instability of model training, taken as an end-to-end procedure, as the key determinant. We show that even one-bit changes in initial parameters result in models converging to vastly different values. Last, we propose two approaches for reducing the effects of instability on run-to-run variability.\n\n| | |\n| --- | --- |\n| Comments: | ICML 2021 |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2103.04514](https://arxiv.org/abs/2103.04514) \\[cs.LG\\] |\n| (or [arXiv:2103.04514v3](https://arxiv.org/abs/2103.04514v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2103.04514](https://doi.org/10.48550/arXiv.2103.04514) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Cecilia Summers \\[ [view email](https://arxiv.org/show-email/21e7c999/2103.04514)\\] **[\\[v1\\]](https://arxiv.org/abs/2103.04514v1)**\nMon, 8 Mar 2021 02:28:18 UTC (103 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2103.04514v2)**\nTue, 1 Jun 2021 00:54:16 UTC (2,268 KB)\n**\\[v3\\]**\nSat, 10 Jul 2021 21:58:40 UTC (2,268 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Nondeterminism and Instability in Neural Network Optimization, by Cecilia Summers and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2103.04514)\n- [TeX Source](https://arxiv.org/src/2103.04514)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2103.04514&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2103.04514&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-03](https://arxiv.org/list/cs.LG/2021-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2103.04514?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2103.04514)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2103.04514)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2103.04514)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2103.html#abs-2103-04514) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2103-04514)\n\n[Cecilia Summers](https://dblp.uni-trier.de/search/author?author=Cecilia%20Summers) [Michael J. Dinneen](https://dblp.uni-trier.de/search/author?author=Michael%20J.%20Dinneen)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2103.04514) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Nondeterminism and instability in neural network optimization (ICML 2021)",
          "cleaned_query": "Nondeterminism and instability in neural network optimization",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Benchmarking Large Language Model Volatility",
          "url": "https://arxiv.org/html/2311.15180",
          "content": "Benchmarking Large Language Model Volatility\n# Benchmarking Large Language Model Volatility\nBoyang Yu[boy.yu@nyu.edu](mailto:boy.yu@nyu.edu)Center for Data Science, New York University60 Fifth AveNew YorkNew YorkUSA10011\n(19 November 2009)\n###### Abstract.\nThe impact of non-deterministic outputs from Large Language Models (LLMs) is not well examined for financial text understanding tasks. Through a compelling case study on investing in the US equity market via news sentiment analysis, we uncover substantial variability in sentence-level sentiment classification results, underscoring the innate volatility of LLM outputs. These uncertainties cascade downstream, leading to more significant variations in portfolio construction and return. While tweaking the temperature parameter in the language model decoder presents a potential remedy, it comes at the expense of stifled creativity. Similarly, while ensembling multiple outputs mitigates the effect of volatile outputs, it demands a notable computational investment. This work furnishes practitioners with invaluable insights for adeptly navigating uncertainty in the integration of LLMs into financial decision-making, particularly in scenarios dictated by non-deterministic information.\nLarge language model, Uncertainty quantification, Robustness, Sentiment analysis\n\u2020\u2020conference:Workshop on AI Safety and Robustness In Finance; 2023; New York, NY\u2020\u2020booktitle:AISRF \u201923: ICAIF Workshop on AI Safety and Robustness in Finance,\nNovember 27, 2023, New York, NY\n## 1.Introduction\nLarge Language Models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks> (\n[> woodhouse2023can, ](#bib.bib14)> ; [> lopez2023can, ](#bib.bib9)> ; [> lin2023generating, ](#bib.bib7)> )\n, including sentiment prediction. However, their application in the financial domain presents unique challenges, primarily stemming from the inherently abstract nature of the task> (\n[> liu2021deep, ](#bib.bib8)> )\n.\nIn the context of financial decision-making, the ability to quantify volatility has paramount importance. Understanding and managing volatility allows for a more informed and robust approach towards financial strategies, whether it\u2019s determining the allocation of assets in a portfolio or evaluating the potential risks and returns associated with specific investment opportunities.\nThe broader landscape of uncertainty quantification in textual analysis offers crucial insights. In various domains, subjective judgments, vague language, and diverse interpretations necessitate a nuanced approach to uncertainty assessment. For instance, consider sentiment analysis in customer reviews for a product. A statement like \u201dThe product is good, but not great\u201d introduces ambiguity. Similarly, in legal documents, the interpretation of contractual clauses may hinge on subtle linguistic nuances. Here, techniques like semantic ambiguity detection or probabilistic modeling provide avenues for handling uncertainty> (\n[> lin2023generating, ](#bib.bib7)> ; [> liu2021deep, ](#bib.bib8)> )\n. There are many quantitative metrics to evaluate the level of uncertainty, such as model confidence or entropy> (\n[> gal2016dropout, ](#bib.bib2)> ; [> lakshminarayanan2017simple, ](#bib.bib6)> )\n.\nHowever, traditional uncertainty quantification methods often rely on model generated probabilities or embeddings, requiring direct access to language model\u2019s intermediate output. In modern large language models, such as Chat Generative Pre-trained Transformers (ChatGPT)> (\n[> openai2023gpt4, ](#bib.bib10)> )\nand Large Language Model Meta AI (LLaMA)> (\n[> touvron2023llama, ](#bib.bib12)> )\n, end users often lack a direct access to anything other than the generated text. This lack of transparency can make it challenging to understand how and why the model arrives at a particular prediction or output. As a result, it becomes crucial to develop methods that allow for a practical assessment of uncertainty in order to enhance the reliability and trustworthiness of model outputs.\nThe task of news sentiment analysis presents an ideal evaluation ground for assess the level of variation, or volatility. Inherent uncertainties and non-deterministic outputs both contribute to volatility. Furthermore, tracing the effects of volatile outputs in the subsequent investment process provides a tangible demonstration of how these nuances reverberate through each stage of decision-making. A previous study investigates stock price prediction using ChatGPT> (\n[> lopez2023can, ](#bib.bib9)> )\n. Researchers extract news sentiment related to price change, aggregate sentiment scores at a daily ticker level, analyze the relationship between sentiment score and excess return, and ultimately construct a portfolio for the US stock market. Their primary focus was on assessing the impact of increased language model capacity on return predictability. However, the volatility perspective remained unexplored. Our work aims to delve deeper, evaluating how volatile model outputs potentially impact investment returns and to what extent.\nThis study seeks to address a series of pertinent research questions that arise from the interaction between LLMs, financial sentiment prediction, and volatility quantification. We are interested in\n* \u2022Quantifying LLM uncertainty without direct access to model generated probabilities\n* \u2022Understanding the limitations of volatile sentiment predictions on trading signals execution\nWe first demonstrate that the presence of uncertainty in sentiment classification regardless of temperature setting and its profound implications on downstream tasks, particularly in portfolio construction and return. The resulting variability underscores the need for careful consideration when utilizing LLMs in financial decision-making processes. This discovery reinforces the importance of acknowledging and managing the inherent non-determinism in LLM outputs.\nThe paper is organized as follows: Section 2 provides background on uncertainty quantification and sentiment analysis for trading strategy. Section 3 covers methodology, including data description and modeling. Section 4 outlines experiment design and results. Finally, Section 5 concludes and discusses key findings.\n## 2.Background\nIn this section, we describe the specific type of uncertainty under examination in the context of using Large Language Models to perform sentiment analysis for investment decision making, meanwhile introducing terminologies used in the rest of the paper.\n### 2.1.Pre-trained Large Language Models\nPre-trained Large Language Models(LLMs) are powered by deep learning techniques and undergo training on extensive text corpora. Their primary objective is to predict the next token given previous sequence. By learning conditional probability distributions for tokens based on prior sequences, LLMs gain a profound understanding of language nuances.\nThe success of large language models hinges on their information encoding and token sampling processes. State-of-the-art model architectures, such as GPT and LLaMA, employ transformer architectures with self-attention mechanisms> (\n[> vaswani2017attention, ](#bib.bib13)> )\n, enabling them to grasp intricate language nuances. During the sampling phase, tokens with higher probabilities are favored as the next token. This preference is modulated by the temperature parameter, which governs the smoothness of probability distribution. Increasing temperature leads to more creative prediction. In practice, a temperature of 0 is chosen for the goal of determinism> (\n[> lopez2023can, ](#bib.bib9)> ; [> woodhouse2023can, ](#bib.bib14)> ; [> yang2023large, ](#bib.bib15)> )\n.\nAI community have an ongoing debate about model sharing. Some advocate open source, such as Meta LLaMA, promoting that shared tools encourage collaboration, transparency, and accessibility. Some are extremely cautious in open source, such as OpenAI ChatGPT, arguing that unrestricted access to advanced AI models might lead to potential misuse of the technology> (\n[> openai2023gpt4, ](#bib.bib10)> )\n. Despite the attitude, both sides have made outstanding progress in natural language understanding tasks, where GPT-3.5( backend of free version of ChatGPT) and LLaMA2 excel in many benchmarks.\nIn the analysis, we use both the GPT and LLaMA model families. Specifically, we focus on the light weighted versions of their interactive chatbot backends, GPT-3.5-turbo and LLaMA-7b. We study the models in a zero-shot setup, i.e., without providing additional training where we present text to models and ask them to provide a response containing sentiment labels.\n### 2.2.Sentiment Analysis and Investment Decision-Making\nSentiment extraction benefits from LLMs\u2019 linguistic capabilities, while not explicitly designed for predicting asset prices> (\n[> lopez2023can, ](#bib.bib9)> ; [> yang2023large, ](#bib.bib15)> ; [> woodhouse2023can, ](#bib.bib14)> )\n. These models\u2019 ability to discern contextual meanings and linguistic patterns could potentially extract valuable insights about a firm\u2019s prospects from textual data, such as news headlines, even in the absence of direct financial training. This opens up new avenues for utilizing natural language processing in financial analysis.\nRecent studies that use ChatGPT in investment decision-making process include Lopez-Lira and Tang finding ChatGPT is able to predict stock movement> (\n[> lopez2023can, ](#bib.bib9)> )\n, and Yang and Menczer> (\n[> yang2023large, ](#bib.bib15)> )\ndemonstrating ChatGPT successfully identifies credible news outlets. These studies collectively illustrate the potential of Large Language Models (LLMs) in enhancing financial decision-making.\n### 2.3.Volatility Quantification\nAn unspoken rule in combining LLMs and financial text is setting temperature equals 0 to maximize determinism of model output. Now the question is, how much volatility is left and how much will there be if we increase temperature.\nVolatility in non-d",
          "original_query": "Benchmarking large language model volatility (arXiv 2023)",
          "cleaned_query": "Benchmarking large language model volatility",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention",
          "url": "https://vllm-project.github.io/2023/06/20/vllm.html",
          "content": "vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog\n[vLLM Blog](https://vllm-project.github.io/)\n# vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\nJun 20, 2023\u2022Woosuk Kwon\\*, Zhuohan Li\\*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (\\* Equal Contribution)\n[**GitHub**](https://github.com/vllm-project/vllm)|[**Documentation**](https://vllm.readthedocs.io/en/latest/)|[**Paper**](https://arxiv.org/pdf/2309.06180.pdf)\nLLMs promise to fundamentally change how we use AI across all industries. However, actually serving these models is challenging and can be surprisingly slow even on expensive hardware. Today we are excited to introduce vLLM, an open-source library for fast LLM inference and serving. vLLM utilizes**PagedAttention**, our new attention algorithm that effectively manages attention keys and values. vLLM equipped with PagedAttention redefines the new state of the art in LLM serving: it delivers up to 24x higher throughput than HuggingFace Transformers, without requiring any model architecture changes.\nvLLM has been developed at UC Berkeley and deployed at[Chatbot Arena and Vicuna Demo](https://chat.lmsys.org)for the past two months. It is the core technology that makes LLM serving affordable even for a small research team like LMSYS with limited compute resources. Try out vLLM now with a single command at our[GitHub repository](https://github.com/vllm-project/vllm).\n### Beyond State-of-the-art Performance\nWe compare the throughput of vLLM with[HuggingFace Transformers (HF)](https://huggingface.co/docs/transformers/main_classes/text_generation), the most popular LLM library and[HuggingFace Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference), the previous state of the art. We evaluate in two settings: LLaMA-7B on an NVIDIA A10G GPU and LLaMA-13B on an NVIDIA A100 GPU (40GB). We sample the requests\u2019 input/output lengths from the ShareGPT dataset. In our experiments, vLLM achieves up to**24x**higher throughput compared to HF and up to**3.5x**higher throughput than TGI.\n![](https://vllm-project.github.io/assets/figures/perf_a100_n1_light.png)![](https://vllm-project.github.io/assets/figures/perf_a10g_n1_light.png)\nServing throughput when each request asks for*one output completion*. vLLM achieves 14x - 24x higher throughput than HF and 2.2x - 2.5x higher throughput than TGI.\n![](https://vllm-project.github.io/assets/figures/perf_a100_n3_light.png)![](https://vllm-project.github.io/assets/figures/perf_a10g_n3_light.png)\nServing throughput when each request asks for*three parallel output completions*. vLLM achieves 8.5x - 15x higher throughput than HF and 3.3x - 3.5x higher throughput than TGI.\n### The Secret Sauce: PagedAttention\nIn vLLM, we identify that the performance of LLM serving is bottlenecked by memory. In the autoregressive decoding process, all the input tokens to the LLM produce their attention key and value tensors, and these tensors are kept in GPU memory to generate next tokens. These cached key and value tensors are often referred to as KV cache. The KV cache is\n* *Large:*Takes up to 1.7GB for a single sequence in LLaMA-13B.\n* *Dynamic:*Its size depends on the sequence length, which is highly variable and unpredictable.\nAs a result, efficiently managing the KV cache presents a significant challenge. We find that existing systems waste**60% \u201380%**of memory due to fragmentation and over-reservation.\nTo address this problem, we introduce**PagedAttention**, an attention algorithm inspired by the classic idea of virtual memory and paging in operating systems. Unlike the traditional attention algorithms, PagedAttention allows storing continuous keys and values in non-contiguous memory space. Specifically, PagedAttention partitions the KV cache of each sequence into blocks, each block containing the keys and values for a fixed number of tokens. During the attention computation, the PagedAttention kernel identifies and fetches these blocks efficiently.\n![](https://vllm-project.github.io/assets/figures/annimation0.gif)\n*PagedAttention:*KV Cache are partitioned into blocks. Blocks do not need to be contiguous in memory space.\nBecause the blocks do not need to be contiguous in memory, we can manage the keys and values in a more flexible way as in OS\u2019s virtual memory: one can think of blocks as pages, tokens as bytes, and sequences as processes. The contiguous*logical blocks*of a sequence are mapped to non-contiguous*physical blocks*via a block table. The physical blocks are allocated on demand as new tokens are generated.\n![](https://vllm-project.github.io/assets/figures/annimation1.gif)\nExample generation process for a request with PagedAttention.\nIn PagedAttention, memory waste only happens in the last block of a sequence. In practice, this results in near-optimal memory usage, with a mere waste of under 4%. This boost in memory efficiency proves highly beneficial: It allows the system to batch more sequences together, increase GPU utilization, and thereby significantly increase the throughput as shown in the performance result above.\nPagedAttention has another key advantage: efficient memory sharing. For example, in*parallel sampling*, multiple output sequences are generated from the same prompt. In this case, the computation and memory for the prompt can be shared between the output sequences.\n![](https://vllm-project.github.io/assets/figures/annimation2.gif)\nExample of parallel sampling.\nPagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the*Copy-on-Write*mechanism.\n![](https://vllm-project.github.io/assets/figures/annimation3.gif)\nExample generation process for a request that samples multiple outputs.\nPageAttention\u2019s memory sharing greatly reduces the memory overhead of complex sampling algorithms, such as parallel sampling and beam search, cutting their memory usage by up to 55%. This can translate into up to 2.2x improvement in throughput. This makes such sampling methods practical in LLM services.\nPagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface. For more technical details about vLLM and PagedAttention, check out our[GitHub repo](https://github.com/vllm-project/vllm)and stay tuned for our paper.\n### The Silent Hero Behind LMSYS Vicuna and Chatbot Arena\nThis April,[LMSYS](https://lmsys.org)developed the popular Vicuna chatbot models and made them publicly available. Since then, Vicuna has been served in[Chatbot Arena](https://arena.lmsys.org/)for millions of users. Initially, LMSYS FastChat adopted a HF Transformers based[serving backend](https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/model_worker.py)to serve the chat demo. As the demo became more popular, the peak traffic ramped up several times, making the HF backend a significant bottleneck. The LMSYS and vLLM team have worked together and soon developed the FastChat-vLLM integration to use vLLM[as the new backend](https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/vllm_worker.py)in order to support the growing demands (up to 5x more traffic). In an early[internal micro-benchmark](https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/test_throughput.py)by LMSYS, the vLLM serving backend can**achieve up to 30x higher throughput than an initial HF backend.**\nSince mid-April, the most popular models such as Vicuna, Koala, and LLaMA, have all been successfully served using the FastChat-vLLM integration \u2013With FastChat as the multi-model chat serving frontend and vLLM as the inference backend, LMSYS is able to harness a limited number of university-sponsored GPUs to serve Vicuna to millions of users with*high throughput*and*low latency*. LMSYS is expanding the use of vLLM to a wider range of models, including Databricks Dolly, LAION\u2019s OpenAsssiant, and Stability AI\u2019s stableLM. The[support for more models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)is being developed and forthcoming.\n![](https://vllm-project.github.io/assets/figures/lmsys_traffic.png)\nRequests served by FastChat-vLLM integration in the Chatbot Arena between April to May. Indeed, more than half of the requests to Chatbot Arena use vLLM as the inference backend.\nThis utilization of vLLM has also significantly reduced operational costs. With vLLM, LMSYS was able to cut the number of GPUs used for serving the above traffic by 50%. vLLM has been handling an average of 30K requests daily and a peak of 60K, which is a clear demonstration of vLLM\u2019s robustness.\n### Get started with vLLM\nInstall vLLM with the following command (check out our[installation guide](https://docs.vllm.ai/en/latest/getting_started/installation.html)for more):\n```\n`$pipinstallvllm`\n```\nvLLM can be used for both offline inference and online serving. To use vLLM for offline inference, you can import vLLM and use the`LLM`class in your Python scripts:\n```\n`fromvllmimportLLMprompts=[\"Hello, my name is\",\"The capital of France is\"]# Sample prompts.llm=LLM(model=\"lmsys/vicuna-7b-v1.3\")# Create an LLM.outputs=llm.generate(prompts)# Generate texts from the prompts.`\n```\nTo use vLLM for online serving, you can start an OpenAI API-compatible server via:\n```\n`$python-mvllm.entrypoints.openai.api\\_server--modellmsys/vicuna-7b-v1.3`\n```\nYou can query the server with the same format as OpenAI API:\n```\n`$curl http://localhost:8000/v1/completions\\\\-H\"Content-Type: application/json\"\\\\-d'{\n\"model\": \"lmsys/vicuna-7b-v1.3\",\n\"prompt\": \"San Francisco is a\",\n\"max\\_tokens\": 7,\n\"temperature\": 0\n}'`\n```\nFor more ways to use vLLM, please check out the[quickstart g",
          "original_query": "Efficient memory management for large language model serving with pagedattention (vLLM) (SOSP/OSDI 2023)",
          "cleaned_query": "Efficient memory management for large language model serving with pagedattention (vLLM)",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Deterministic PagedAttention for Reproducible LLM Serving\nDevelop a variant of vLLM\u2019s PagedAttention that guarantees bitwise-identical outputs across runs by enforcing a deterministic ordering for block fetch, attention reduction, and softmax accumulation. Evaluate the latency/throughput cost of determinism and identify which attention sub-ops (e.g., reductions over blocks) contribute most to run-to-run variability.",
        "KV-Cache Layout as a Source of Output Volatility in Autoregressive Decoding\nSystematically test whether different KV-cache paging/fragmentation patterns (as in PagedAttention) lead to different numerical reduction orders and measurable token-level divergence, even with fixed seeds and temperature. Produce a \u201clayout sensitivity map\u201d that links memory/block allocation decisions to floating-point non-associativity effects and downstream output drift.",
        "Iteration-Level Scheduling with Volatility-Aware Batching\nExtend ORCA\u2019s iteration-level scheduling to incorporate a volatility estimate (e.g., based on past token disagreement rates or lightweight repeated decoding) that dynamically decides which ops to batch (selective batching) versus run deterministically. The key contribution is a scheduler that trades small throughput reductions for large decreases in output variance on requests flagged as high-stakes (e.g., finance).",
        "Deterministic GEMM Epilogues and Reductions for Transformer Inference\nUsing CUTLASS, implement deterministic variants of common Transformer GEMM epilogues (bias+GELU, bias+LayerNorm precompute, residual adds) by fixing reduction trees and accumulation precision. Quantify how much inference nondeterminism comes from GEMM epilogues versus attention kernels, and provide drop-in kernels with tunable determinism/performance knobs.",
        "Instability-Triggered Ensemble Allocation in LLM Serving\nBuilding on findings that training/optimization is inherently unstable and that ensembling reduces volatility, design a serving-time controller that allocates extra samples (self-ensembles) only when the model is near decision boundaries (high entropy signals inferred from text-only probes). Integrate with vLLM/ORCA to show reduced volatility at near-constant average compute cost by concentrating ensembles on unstable requests.",
        "Text-Only Uncertainty Metrics that Predict Downstream Financial Risk\nExtend the \u201cno logprobs available\u201d volatility benchmarking by creating uncertainty estimators derived purely from multiple constrained decodes (e.g., low-temperature paraphrase probes, counterfactual prompts) and measuring how well they predict portfolio-return variance. Deliver a standardized evaluation suite that connects token-level disagreement to concrete downstream risk metrics (turnover, drawdown, VaR).",
        "Hardware-Assisted Determinism for Hybrid CPU/GPU/Accelerator Pipelines\nPrototype a split pipeline where numerically sensitive subgraphs (e.g., layernorm/softmax reductions) run on deterministic hardware (inspired by Groq LPU determinism) while bulk GEMMs remain on GPUs via CUTLASS. The contribution is a graph partitioning and runtime orchestration strategy that achieves near-bitwise reproducibility with minimal throughput loss.",
        "Selective Precision and Accumulation Strategies to Reduce FPNA in Attention\nDesign mixed-precision attention kernels where only the reduction/normalization steps use higher precision or compensated summation (e.g., Kahan-like) while keeping matmuls in TensorCore-friendly formats. Evaluate whether these targeted changes reduce run-to-run variability and token divergence more efficiently than global \u201cdeterministic mode\u201d switches in frameworks.",
        "A Cross-Stack Reproducibility Test Harness for Distributed LLM Serving\nBuild an automated harness that runs controlled A/B experiments across (i) ORCA-style iteration scheduling, (ii) vLLM cache paging policies, and (iii) CUTLASS kernel variants to attribute nondeterminism to specific layers (scheduler, kernel, memory manager). The output would be a reproducibility \u201cbill of materials\u201d that pinpoints which configuration changes restore determinism and what performance they cost."
      ],
      "generated_ideas_raw": "1. **Deterministic PagedAttention for Reproducible LLM Serving**\n   Develop a variant of vLLM\u2019s PagedAttention that guarantees bitwise-identical outputs across runs by enforcing a deterministic ordering for block fetch, attention reduction, and softmax accumulation. Evaluate the latency/throughput cost of determinism and identify which attention sub-ops (e.g., reductions over blocks) contribute most to run-to-run variability.\n\n2. **KV-Cache Layout as a Source of Output Volatility in Autoregressive Decoding**\n   Systematically test whether different KV-cache paging/fragmentation patterns (as in PagedAttention) lead to different numerical reduction orders and measurable token-level divergence, even with fixed seeds and temperature. Produce a \u201clayout sensitivity map\u201d that links memory/block allocation decisions to floating-point non-associativity effects and downstream output drift.\n\n3. **Iteration-Level Scheduling with Volatility-Aware Batching**\n   Extend ORCA\u2019s iteration-level scheduling to incorporate a volatility estimate (e.g., based on past token disagreement rates or lightweight repeated decoding) that dynamically decides which ops to batch (selective batching) versus run deterministically. The key contribution is a scheduler that trades small throughput reductions for large decreases in output variance on requests flagged as high-stakes (e.g., finance).\n\n4. **Deterministic GEMM Epilogues and Reductions for Transformer Inference**\n   Using CUTLASS, implement deterministic variants of common Transformer GEMM epilogues (bias+GELU, bias+LayerNorm precompute, residual adds) by fixing reduction trees and accumulation precision. Quantify how much inference nondeterminism comes from GEMM epilogues versus attention kernels, and provide drop-in kernels with tunable determinism/performance knobs.\n\n5. **Reproducibility Budgeting: Minimal Determinism to Prevent Token Divergence**\n   Create a method that identifies the smallest set of operations that must be deterministic to maintain identical decoded tokens for a target horizon (e.g., first 50 tokens). The approach would combine microbenchmarks (FP non-associativity statistics) with end-to-end decoding experiments to yield an actionable \u201cdeterminism budget\u201d per model/hardware stack.\n\n6. **Instability-Triggered Ensemble Allocation in LLM Serving**\n   Building on findings that training/optimization is inherently unstable and that ensembling reduces volatility, design a serving-time controller that allocates extra samples (self-ensembles) only when the model is near decision boundaries (high entropy signals inferred from text-only probes). Integrate with vLLM/ORCA to show reduced volatility at near-constant average compute cost by concentrating ensembles on unstable requests.\n\n7. **Text-Only Uncertainty Metrics that Predict Downstream Financial Risk**\n   Extend the \u201cno logprobs available\u201d volatility benchmarking by creating uncertainty estimators derived purely from multiple constrained decodes (e.g., low-temperature paraphrase probes, counterfactual prompts) and measuring how well they predict portfolio-return variance. Deliver a standardized evaluation suite that connects token-level disagreement to concrete downstream risk metrics (turnover, drawdown, VaR).\n\n8. **Hardware-Assisted Determinism for Hybrid CPU/GPU/Accelerator Pipelines**\n   Prototype a split pipeline where numerically sensitive subgraphs (e.g., layernorm/softmax reductions) run on deterministic hardware (inspired by Groq LPU determinism) while bulk GEMMs remain on GPUs via CUTLASS. The contribution is a graph partitioning and runtime orchestration strategy that achieves near-bitwise reproducibility with minimal throughput loss.\n\n9. **Selective Precision and Accumulation Strategies to Reduce FPNA in Attention**\n   Design mixed-precision attention kernels where only the reduction/normalization steps use higher precision or compensated summation (e.g., Kahan-like) while keeping matmuls in TensorCore-friendly formats. Evaluate whether these targeted changes reduce run-to-run variability and token divergence more efficiently than global \u201cdeterministic mode\u201d switches in frameworks.\n\n10. **A Cross-Stack Reproducibility Test Harness for Distributed LLM Serving**\n   Build an automated harness that runs controlled A/B experiments across (i) ORCA-style iteration scheduling, (ii) vLLM cache paging policies, and (iii) CUTLASS kernel variants to attribute nondeterminism to specific layers (scheduler, kernel, memory manager). The output would be a reproducibility \u201cbill of materials\u201d that pinpoints which configuration changes restore determinism and what performance they cost.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Deterministic PagedAttention for Reproducible LLM Serving\nDevelop a variant of vLLM\u2019s PagedAttention that guarantees bitwise-identical outputs across runs by enforcing a deterministic ordering for blo",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "KV-Cache Layout as a Source of Output Volatility in Autoregressive Decoding\nSystematically test whether different KV-cache paging/fragmentation patterns (as in PagedAttention) lead to different numeri",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Iteration-Level Scheduling with Volatility-Aware Batching\nExtend ORCA\u2019s iteration-level scheduling to incorporate a volatility estimate (e.g., based on past token disagreement rates or lightweight rep",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Deterministic GEMM Epilogues and Reductions for Transformer Inference\nUsing CUTLASS, implement deterministic variants of common Transformer GEMM epilogues (bias+GELU, bias+LayerNorm precompute, residu",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Instability-Triggered Ensemble Allocation in LLM Serving\nBuilding on findings that training/optimization is inherently unstable and that ensembling reduces volatility, design a serving-time controller",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Text-Only Uncertainty Metrics that Predict Downstream Financial Risk\nExtend the \u201cno logprobs available\u201d volatility benchmarking by creating uncertainty estimators derived purely from multiple constrai",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Hardware-Assisted Determinism for Hybrid CPU/GPU/Accelerator Pipelines\nPrototype a split pipeline where numerically sensitive subgraphs (e.g., layernorm/softmax reductions) run on deterministic hardwa",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Selective Precision and Accumulation Strategies to Reduce FPNA in Attention\nDesign mixed-precision attention kernels where only the reduction/normalization steps use higher precision or compensated su",
          "is_match": true
        },
        {
          "idea_idx": 8,
          "idea_text": "A Cross-Stack Reproducibility Test Harness for Distributed LLM Serving\nBuild an automated harness that runs controlled A/B experiments across (i) ORCA-style iteration scheduling, (ii) vLLM cache pagin",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 21,
      "paper_title": "PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models",
      "contribution": "PRIMT reduces human labeling and improves reward learning in preference-based RL by using a hierarchical fusion of multimodal foundation models for synthetic feedback together with foresight and hindsight trajectory synthesis (including SCM-based counterfactuals) to reduce query ambiguity and improve credit assignment.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 5,
      "input_tokens": 8795,
      "output_tokens": 984,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Deep Reinforcement Learning from Human Preferences - NIPS papers",
          "url": "http://papers.neurips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences.pdf",
          "content": "Deep Reinforcement Learning\nfrom Human Preferences\nPaul F Christiano\nOpenAI\npaul@openai.com\nJan Leike\nDeepMind\nleike@google.com\nTom B Brown\nGoogle Brain\u21e4\ntombbrown@google.com\nMiljan Martic\nDeepMind\nmiljanm@google.com\nShane Legg\nDeepMind\nlegg@google.com\nDario Amodei\nOpenAI\ndamodei@openai.com\nAbstract\nFor sophisticated reinforcement learning (RL) systems to interact usefully with\nreal-world environments, we need to communicate complex goals to these systems.\nIn this work, we explore goals defined in terms of (non-expert) human preferences\nbetween pairs of trajectory segments. We show that this approach can effectively\nsolve complex RL tasks without access to the reward function, including Atari\ngames and simulated robot locomotion, while providing feedback on less than\n1% of our agent\u2019s interactions with the environment. This reduces the cost of\nhuman oversight far enough that it can be practically applied to state-of-the-art\nRL systems. To demonstrate the flexibility of our approach, we show that we can\nsuccessfully train complex novel behaviors with about an hour of human time.\nThese behaviors and environments are considerably more complex than any which\nhave been previously learned from human feedback.\n1 Introduction\nRecent success in scaling reinforcement learning (RL) to large problems has been driven in domains\nthat have a well-specified reward function (Mnih et al., 2015, 2016; Silver et al., 2016). Unfortunately,\nmany tasks involve goals that are complex, poorly-defined, or hard to specify. Overcoming this\nlimitation would greatly expand the possible impact of deep RL and could increase the reach of\nmachine learning more broadly.\nFor example, suppose that we wanted to use reinforcement learning to train a robot to clean a table or\nscramble an egg. It\u2019s not clear how to construct a suitable reward function, which will need to be a\nfunction of the robot\u2019s sensors. We could try to design a simple reward function that approximately\ncaptures the intended behavior, but this will often result in behavior that optimizes our reward\nfunction without actually satisfying our preferences. This difficulty underlies recent concerns about\nmisalignment between our values and the objectives of our RL systems (Bostrom, 2014; Russell,\n2016; Amodei et al., 2016). If we could successfully communicate our actual objectives to our agents,\nit would be a significant step towards addressing these concerns.\nIf we have demonstrations of the desired task, we can use inverse reinforcement learning (Ng and\nRussell, 2000) or imitation learning to copy the demonstrated behavior. But these approaches are not\ndirectly applicable to behaviors that are difficult for humans to demonstrate (such as controlling a\nrobot with many degrees of freedom but non-human morphology).\n\u21e4Work done while at OpenAI.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nAn alternative approach is to allow a human to provide feedback on our system\u2019s current behavior\nand to use this feedback to define the task. In principle this fits within the paradigm of reinforcement\nlearning, but using human feedback directly as a reward function is prohibitively expensive for RL\nsystems that require hundreds or thousands of hours of experience. In order to practically train deep\nRL systems with human feedback, we need to decrease the amount of feedback required by several\norders of magnitude.\nWe overcome this difficulty by asking humans to compare possible trajectories of the agent, using\nthat data to learn a reward function, and optimizing the learned reward function with RL.\nThis basic approach has been explored in the past, but we confront the challenges involved in scaling\nit up to modern deep RL and demonstrate by far the most complex behaviors yet learned from human\nfeedback.\nOur experiments take place in two domains: Atari games in the Arcade Learning Environment (Belle\u0002mare et al., 2013), and robotics tasks in the physics simulator MuJoCo (Todorov et al., 2012). We\nshow that a small amount of feedback from a non-expert human, ranging from fifteen minutes to five\nhours, suffice to learn both standard RL tasks and novel hard-to-specify behaviors such as performing\na backflip or driving with the flow of traffic.\n1.1 Related Work\nA long line of work studies reinforcement learning from human ratings or rankings, including Akrour\net al. (2011), Pilarski et al. (2011), Akrour et al. (2012), Wilson et al. (2012), Sugiyama et al. (2012),\nWirth and F\u00fcrnkranz (2013), Daniel et al. (2015), El Asri et al. (2016), Wang et al. (2016), and\nWirth et al. (2016). Other lines of research consider the general problem of reinforcement learning\nfrom preferences rather than absolute reward values (F\u00fcrnkranz et al., 2012; Akrour et al., 2014;\nWirth et al., 2016), and optimizing using human preferences in settings other than reinforcement\nlearning (Machwe and Parmee, 2006; Secretan et al., 2008; Brochu et al., 2010; S\u00f8rensen et al.,\n2016).\nOur algorithm follows the same basic approach as Akrour et al. (2012) and Akrour et al. (2014), but\nconsiders much more complex domains and behaviors. The complexity of our environments force us\nto use different RL algorithms, reward models, and training strategies. One notable difference is that\nAkrour et al. (2012) and Akrour et al. (2014) elicit preferences over whole trajectories rather than\nshort clips, and so would require about an order of magnitude more human time per data point. Our\napproach to feedback elicitation closely follows Wilson et al. (2012). However, Wilson et al. (2012)\nassumes that the reward function is the distance to some unknown (linear) \u201ctarget\u201d policy, and is\nnever tested with real human feedback.\nTAMER (Knox, 2012; Knox and Stone, 2013) also learns a reward function from human feedback,\nbut learns from ratings rather than comparisons, has the human observe the agent as it behaves,\nand has been applied to settings where the desired policy can be learned orders of magnitude more\nquickly.\nCompared to all prior work, our key contribution is to scale human feedback up to deep reinforcement\nlearning and to learn much more complex behaviors. This fits into a recent trend of scaling reward\nlearning methods to large deep learning systems, for example inverse RL (Finn et al., 2016), imitation\nlearning (Ho and Ermon, 2016; Stadie et al., 2017), semi-supervised skill generalization (Finn et al.,\n2017), and bootstrapping RL from demonstrations (Silver et al., 2016; Hester et al., 2017).\n2 Preliminaries and Method\n2.1 Setting and Goal\nWe consider an agent interacting with an environment over a sequence of steps; at each time t the\nagent receives an observation ot 2 O from the environment and then sends an action at 2 A to the\nenvironment.\nIn traditional reinforcement learning, the environment would also supply a reward rt 2 R and the\nagent\u2019s goal would be to maximize the discounted sum of rewards. Instead of assuming that the\nenvironment produces a reward signal, we assume that there is a human overseer who can express\n2\npreferences between trajectory segments. A trajectory segment is a sequence of observations and\nactions, = ((o0, a0),(o1, a1),...,(ok1, ak1)) 2 (O \u21e5 A)\nk\n. Write 1 2 to indicate that the\nhuman preferred trajectory segment 1 to trajectory segment 2. Informally, the goal of the agent is\nto produce trajectories which are preferred by the human, while making as few queries as possible to\nthe human.\nMore precisely, we will evaluate our algorithms\u2019 behavior in two ways:\nQuantitative: We say that preferences are generated by a reward function2 r : O \u21e5 A ! R if\no1\n0, a10\n\n,..., o1\nk1, a1k1\n o2\n0, a20\n\n,..., o2\nk1, a2k1\n\nwhenever\nr\n\no1\n0, a10\n\n+ \u00b7\u00b7\u00b7 + r\n\no1\nk1, a1k1\n\n> ro2\n0, a20\n\n+ \u00b7\u00b7\u00b7 + r\n\no2\nk1, a2k1\n\n.\nIf the human\u2019s preferences are generated by a reward function r, then our agent ought to\nreceive a high total reward according to r. So if we know the reward function r, we can\nevaluate the agent quantitatively. Ideally the agent will achieve reward nearly as high as if it\nhad been using RL to optimize r.\nQualitative: Sometimes we have no reward function by which we can quantitatively evaluate\nbehavior (this is the situation where our approach would be practically useful). In these\ncases, all we can do is qualitatively evaluate how well the agent satisfies the human\u2019s\npreferences. In this paper, we will start from a goal expressed in natural language, ask a\nhuman to evaluate the agent\u2019s behavior based on how well it fulfills that goal, and then\npresent videos of agents attempting to fulfill that goal.\nOur model based on trajectory segment comparisons is very similar to the trajectory preference\nqueries used in Wilson et al. (2012), except that we don\u2019t assume that we can reset the system to\nan arbitrary state3 and so our segments generally begin from different states. This complicates the\ninterpretation of human comparisons, but we show that our algorithm overcomes this difficulty even\nwhen the human raters have no understanding of our algorithm.\n2.2 Our Method\nAt each point in time our method maintains a policy \u21e1 : O ! A and a reward function estimate\nr\u02c6 : O \u21e5 A ! R, each parametrized by deep neural networks.\nThese networks are updated by three processes:\n1. The policy \u21e1 interacts with the environment to produce a set of trajectories {\u2327 1,..., \u2327 i}.\nThe parameters of \u21e1 are updated by a traditional reinforcement learning algorithm, in order\nto maximize the sum of the predicted rewards rt = \u02c6r(ot, at).\n2. We select pairs of segments 1, 2from the trajectories {\u2327 1,..., \u2327 i} produced in step 1,\nand send them to a human for comparison.\n3. The parameters of the mapping r\u02c6 are optimized via supervised learning to fit the comparisons\ncollected from the human so far.\nThese processes run asynchronously, with trajectories flowing from process (1) to process (2), human\ncomparisons flowing from process (2) to process (3), and parameters for r\u02c6 flowing from process (3)\nto process (1). The following subsecti",
          "original_query": "Deep Reinforcement Learning from Human Preferences",
          "cleaned_query": "Deep Reinforcement Learning from Human Preferences",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Hindsight Experience Replay With Experience Ranking",
          "url": "https://ieeexplore.ieee.org/document/8850705/",
          "content": "Hindsight Experience Replay With Experience Ranking \\| IEEE Conference Publication \\| IEEE Xplore\n\n### IEEE Account\n\n- [Change Username/Password](https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Update Address](https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Purchase Details\n\n- [Payment Options](https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Order History](https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n\n### Profile Information\n\n- [Communications Preferences](https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Profession and Education](https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Technical Interests](https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Need Help?\n\n- **US & Canada:** +1 800 678 4333\n- **Worldwide:** +1 732 981 0060\n\n- [Contact & Support](https://ieeexplore.ieee.org/xpl/contact)\n\n- [About IEEE _Xplore_](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n- [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n- [Help](https://ieeexplore.ieee.org/Xplorehelp)\n- [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n- [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n- [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n- [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n- [Privacy & Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\n\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n\n\u00a9 Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.",
          "original_query": "Hindsight Experience Replay",
          "cleaned_query": "Hindsight Experience Replay",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Learning to summarize from human feedback",
          "url": "https://www.semanticscholar.org/paper/Learning-to-summarize-from-human-feedback-Stiennon-Ouyang/053b1d7b97eb2c91fc3921d589c160b0923c70b1",
          "content": "@article{Stiennon2020LearningTS,\ntitle={Learning to summarize from human feedback},\nauthor={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan J. Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},\njournal={ArXiv},\nyear={2020},\nvolume={abs/2009.01325},\nurl={https://api.semanticscholar.org/CorpusID:221665105}\n} This work shows that it is possible to significantly improve summary quality by training a model to optimize for human preferences, and establishes that the reward model generalizes to new datasets, and that optimizing the authors' reward model results in better summaries than optimizing ROUGE according to humans. Figures and Tables from this paper 2,317 Citations 84 References",
          "original_query": "Learning to Summarize with Human Feedback (RLHF pipeline applied to language models)",
          "cleaned_query": "Learning to Summarize with Human Feedback (RLHF pipeline applied to language models)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Constitutional AI: Harmlessness from AI Feedback",
          "url": "https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback",
          "content": "[Skip to main content](#main-content)[Skip to footer](#footer)\n[\n](https://www.anthropic.com/)\n[Try Claude](https://claude.ai/)\nAlignmentResearch\n# Constitutional AI: Harmlessness from AI Feedback\nDec 15, 2022\n[Read Paper](https://arxiv.org/abs/2212.08073)\n## Abstract\nAs AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as &#x27;Constitutional AI&#x27;. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use &#x27;RL from AI Feedback&#x27; (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.\n## Policy Memo\n[Constitutional AI Policy Memo](https://www-cdn.anthropic.com/7512771452629584566b6303311496c262da1006/Anthropic_ConstitutionalAI_v2.pdf)\n[](https://twitter.com/intent/tweet?text=https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback)[](https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback)\n## Related content\n### Introducing Bloom: an open source tool for automated behavioral evaluations\n[Read more](https://www.anthropic.com/research/bloom)\n### Project Vend: Phase two\nIn June, we revealed that we\u2019d set up a small shop in our San Francisco office lunchroom, run by an AI shopkeeper. It was part of Project Vend, a free-form experiment exploring how well AIs could do on complex, real-world tasks. How has Claude&#x27;s business been since we last wrote?\n[Read more](https://www.anthropic.com/research/project-vend-2)\n### Introducing Anthropic Interviewer: What 1,250 professionals told us about working with AI\nWe built an interview tool called Anthropic Interviewer. Powered by Claude, Anthropic Interviewer runs detailed interviews automatically and at unprecedented scale.\n[Read more](https://www.anthropic.com/research/anthropic-interviewer)\nConstitutional AI: Harmlessness from AI Feedback \\\\ Anthropic",
          "original_query": "Constitutional AI / LM self-critique (LM-to-LM critique for alignment without humans)",
          "cleaned_query": "Constitutional AI",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] PaLM-E: An Embodied Multimodal Language Model",
          "url": "https://proceedings.mlr.press/v202/driess23a/driess23a.pdf",
          "content": "PaLM-E: An Embodied Multimodal Language Model\nDanny Driess 1 2 Fei Xia 1 Mehdi S. M. Sajjadi 3 Corey Lynch 1 Aakanksha Chowdhery 3\nBrian Ichter 1 Ayzaan Wahid 1 Jonathan Tompson 1 Quan Vuong 1 Tianhe Yu 1 Wenlong Huang 1\nYevgen Chebotar 1 Pierre Sermanet 1 Daniel Duckworth 3 Sergey Levine 1 Vincent Vanhoucke 1\nKarol Hausman 1 Marc Toussaint 2 Klaus Greff 3 Andy Zeng 1Igor Mordatch 3 Pete Florence 1\nGiven. Q: What\u2019s in the \nimage? Answer in emojis.\nA: \ud83c\udf4f\ud83c\udf4c\ud83c\udf47\ud83c\udf50\ud83c\udf51\ud83c\udf48\ud83c\udf52.\nScene Unde\nVisual Q&A\nTBD\nTBD\nTBD\nTask and Motion Planning\nGiven Q: How \nto grasp blue block?\nA: First grasp yellow \nblock and place it on \nthe table, then grasp \nthe blue block.\nGiven Task: Sort \ncolors into corners.\nStep 1. Push the green \nstar to the bottom left.\nStep 2. Push the green \ncircle to the green star.\nTabletop Manipulation\nMobile Manipulation\nVisual Q&A, Captioning \u2026\nHuman: Bring me the rice chips from the \ndrawer. Robot: 1. Go to the drawers, 2. Open \ntop drawer. I see. 3. Pick the green rice \nchip bag from the drawer and place it on the \ncounter.\n A: First, grasp yellow block and \u2026 \nGiven \u2026 Q: How to grasp blue block? A: First, grasp yellow block\nLarge Language Model (PaLM)\n?\nControl\nPaLM-E: An Embodied Multimodal Language Model\n\u2026 \u2026\nViT\nLanguage Only Tasks\nHere is a Haiku about\nembodied language models: \nEmbodied language\nmodels are the future of\nnatural language\nDescribe the \nfollowing: \nA dog jumping \nover a hurdle at a \ndog show.\n\u2026 \u2026\nQ: Miami Beach borders which ocean? A: Atlantic. \nQ: What is 372 x 18? A: 6696.\nLanguage models trained on robot sensor data can \nbe used to guide a robot\u2019s actions.\nPROMPT:\n Q: How can embodied language\nmodels benefit robots? A:\nPREDICTION:\n Embodied language models can\nbenefit robots by allowing them to\nlearn language in a more natural\nway.\nPROMPT:\n Language models which understand\nrobot sensor data can\nPREDICTION:\n be used to generate natural\nlanguage descriptions of the\nrobot's environment.\nFigure 1: PaLM-E is a single general-purpose multimodal language model for embodied reasoning tasks, visual-language tasks,\nand language tasks. PaLM-E transfers knowledge from visual-language domains into embodied reasoning \u2013 from robot planning in\nenvironments with complex dynamics and physical constraints, to answering questions about the observable world. PaLM-E operates on\nmultimodal sentences, i.e. sequences of tokens where inputs from arbitrary modalities (e.g. images, neural 3D representations, or states, in\ngreen and blue) are inserted alongside text tokens (in orange) as input to an LLM, trained end-to-end.\nAbstract\nLarge language models excel at a wide range of\ncomplex tasks. However, enabling general infer\u0002ence in the real world, e.g. for robotics problems,\nraises the challenge of grounding. We propose\nembodied language models to directly incorpo\u0002rate real-world continuous sensor modalities into\nlanguage models and thereby establish the link\nbetween words and percepts. Input to our embod\u0002ied language model are multimodal sentences that\ninterleave visual, continuous state estimation, and\ntextual input encodings. We train these encodings\nend-to-end, in conjunction with a pre-trained large\nlanguage model, for multiple embodied tasks in\u0002cluding sequential robotic manipulation planning,\nvisual question answering, and captioning. Our\n1Robotics at Google 2TU Berlin 3Google Research. Corre\u0002spondence to: Danny Driess, Pete\nFlorence.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nevaluations show that PaLM-E, a single large em\u0002bodied multimodal model, can address a variety\nof embodied reasoning tasks, from a variety of\nobservation modalities, on multiple embodiments,\nand further, exhibits positive transfer: the model\nbenefits from diverse joint training across internet\u0002scale language, vision, and visual-language do\u0002mains. Our largest model with 562B parameters,\nin addition to being trained on robotics tasks, is\na visual-language generalist with state-of-the-art\nperformance on OK-VQA, and retains generalist\nlanguage capabilities with increasing scale.\n1. Introduction\nLarge language models (LLMs) demonstrate strong reason\u0002ing capabilities across various domains, including dialogue\n(Glaese et al., 2022; Thoppilan et al., 2022), step-by-step\nreasoning (Wei et al., 2022; Kojima et al., 2022), math prob\u0002lem solving (Lewkowycz et al., 2022; Polu et al., 2022), and\ncode writing (Chen et al., 2021a). However, a limitation of\nsuch models for inference in the real world is the issue of\ngrounding: while training LLMs on massive textual data\n1\nPaLM-E: An Embodied Multimodal Language Model\nmay lead to representations that relate to our physical world,\nconnecting those representations to real-world visual and\nphysical sensor modalities is essential to solving a wider\nrange of grounded real-world problems in computer vision\nand robotics (Tellex et al., 2020). Previous work (Ahn et al.,\n2022) interfaces the output of LLMs with learned robotic\npolicies and affordance functions to make decisions, but\nis limited in that the LLM itself is only provided with tex\u0002tual input, which is insufficient for many tasks where the\nspatial layout of the scene is important. Further, in our\nexperiments we show that current state-of-the-art visual\u0002language models trained on typical vision-language tasks\nsuch as visual-question-answering (VQA) cannot directly\nsolve robotic reasoning tasks.\nIn this paper we propose embodied language models, which\ndirectly incorporate continuous inputs from sensor modali\u0002ties of an embodied agent and thereby enable the language\nmodel itself to make more grounded inferences for sequen\u0002tial decision making in the real world. Inputs such as images\nand state estimates are embedded into the same latent embed\u0002ding as language tokens and processed by the self-attention\nlayers of a Transformer-based LLM in the same way as text.\nWe start from a pre-trained LLM in which we inject the\ncontinuous inputs through an encoder. These encoders are\ntrained end-to-end to output sequential decisions in terms of\nnatural text that can be interpreted by the embodied agent\nby conditioning low-level policies or give an answer to an\nembodied question. We evaluate the approach in a vari\u0002ety of settings, comparing different input representations\n(e.g. standard vs. object-centric ViT encodings for visual\ninput), freezing vs. finetuning the language model while\ntraining the encoders, and investigating whether co-training\non multiple tasks enables transfer.\nThe approach enables a broad set of capabilities, as we\ndemonstrate on three robotic manipulation domains (two\nof which are closed-loop in the real-world) and a set of\nstandard visual-language tasks such as VQA and image\ncaptioning, while simultaneously retaining the strong pure\u0002language abilities of PaLM. Our results indicate that multi\u0002task training improves performance compared to training\nmodels on individual tasks. We show that this transfer\nacross tasks can lead to high data-efficiency for robotics\ntasks, e.g. significantly increasing learning success from\nhandfuls of training examples, and even demonstrating one\u0002shot or zero-shot generalization to novel combinations of\nobjects or unseen objects.\nWe scale PaLM-E up to 562B parameters, integrating the\n540B PaLM (Chowdhery et al., 2022) LLM and the 22B\nVision Transformer (ViT) (Dehghani et al., 2023) into, to\nour knowledge, the largest vision-language model currently\nreported. PaLM-E-562B achieves state-of-the-art perfor\u0002mance on the OK-VQA (Marino et al., 2019) benchmark,\nwithout relying on task-specific finetuning. Although not\nthe focus of our experimentation, we also find (Fig. 6) that\nPaLM-E-562B exhibits a wide array of capabilities includ\u0002ing zero-shot multimodal chain-of-thought reasoning, few\u0002shot prompting, and multi-image reasoning, despite being\ntrained on only single-image examples.\nTo summarize our main contributions, we (1) propose the\nmethodological idea to train a generalist vision, language,\nand robotics model that addresses robotics tasks through\nvision-language modeling. We also (2) demonstrate the\nnovel scientific result of demonstrating positive transfer\nacross both vision and language into robotics tasks, which\nis enabled by the methodological idea mentioned prior. In\nstudying how to best train such models, we (3) introduce\nnovel architectural ideas such as neural scene representa\u0002tions and entity-labeling multimodal tokens. In addition to\nour focus on PaLM-E as an embodied reasoner we (4) show\nthat PaLM-E is also a quantitatively competent vision and\nlanguage generalist, and (5) demonstrate that scaling the\nlanguage model size enables multimodal finetuning with\nless catastrophic forgetting.\n2. Related Work\nGeneral vision-language modeling. Building on suc\u0002cesses in large language (Brown et al., 2020; Devlin et al.,\n2018) and vision (Dosovitskiy et al., 2020) models, recent\nyears have seen a growing interest in large vision-language\nmodels (VLMs) (Li et al., 2019; Lu et al., 2019; Hao et al.,\n2022; Gan et al., 2022). Unlike their predecessors, VLMs\nare capable of simultaneously understanding both images\nand text, and can be applied to tasks such as visual question\nanswering (Zhou et al., 2020; Zellers et al., 2021b), cap\u0002tioning (Hu et al., 2022), optical character recognition (Li\net al., 2021), and object detection (Chen et al., 2021b). The\nmethods by which images are integrated varies. For exam\u0002ple, Alayrac et al. (2022) introduces cross-attention layers\nto fuse images into a pretrained language model. In con\u0002trast, PaLM-E represents images and text as \u201cmultimodal\nsentences\u201d where both image and text tokens are input to\nthe self-attention layers of the language model. This also\nallows it to process multiple images in a flexible way within\nany part of a sentence. More closely related to our work\nis Frozen (Tsimpoukelli et al., 2021) where vision encoder\nparameters are optimized via backpropagation through a\nfrozen LLM (L",
          "original_query": "PaLM-E: An Embodied Multimodal Language Model for Robotic Perception and Action",
          "cleaned_query": "PaLM-E: An Embodied Multimodal Language Model for Robotic Perception and Action",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Causality: Models, Reasoning and Inference | Guide books | ACM Digital Library",
          "url": "https://dl.acm.org/doi/book/10.5555/1642718",
          "content": "![logo](data:,)\n\n## This website uses cookies\n\nWe occasionally run membership recruitment campaigns on social media channels and use cookies to track post-clicks. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you\u2019ve provided to them or that they\u2019ve collected from your use of their services. Use the check boxes below to choose the types of cookies you consent to have stored on your device.\n\nDo not sell or share my personal information\n\n[Use necessary cookies only](https://dl.acm.org/doi/book/10.5555/1642718) [Allow all cookies](https://dl.acm.org/doi/book/10.5555/1642718) [Show details](https://dl.acm.org/doi/book/10.5555/1642718)\n\n[OK](https://dl.acm.org/doi/book/10.5555/1642718)\n\n[Use necessary cookies only](https://dl.acm.org/doi/book/10.5555/1642718) [Allow selected cookies](https://dl.acm.org/doi/book/10.5555/1642718) [Allow all cookies](https://dl.acm.org/doi/book/10.5555/1642718)\n\nNecessary\n\nPreferences\n\nStatistics\n\nMarketing\n\n[Show details](https://dl.acm.org/doi/book/10.5555/1642718)\n\n[Cookie declaration](https://dl.acm.org/doi/book/10.5555/1642718) [\\[#IABV2SETTINGS#\\]](https://dl.acm.org/doi/book/10.5555/1642718) [About](https://dl.acm.org/doi/book/10.5555/1642718)\n\n[Necessary (8)](https://dl.acm.org/doi/book/10.5555/1642718) [Preferences (5)](https://dl.acm.org/doi/book/10.5555/1642718) [Statistics (15)](https://dl.acm.org/doi/book/10.5555/1642718) [Marketing (24)](https://dl.acm.org/doi/book/10.5555/1642718) [Unclassified (5)](https://dl.acm.org/doi/book/10.5555/1642718)\n\nNecessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. These cookies do not gather information about you that could be used for marketing purposes and do not remember where you have been on the internet.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| \\_\\_cf\\_bm\u00a0\\[x2\\] | [ACM](https://www.acm.org/privacy-policy) | This cookie is used to distinguish between humans and bots. This is beneficial for the website, in order to make valid reports on the use of their website. | 1 day | HTTP Cookie |\n| \\_\\_jid | c.disquscdn.com | Used to add comments to the website and remember the user's Disqus login credentials across websites that use said service. | Session | HTTP Cookie |\n| disqusauth | c.disquscdn.com | Registers whether the user is logged in. This allows the website owner to make parts of the website inaccessible, based on the user's log-in status. | Session | HTTP Cookie |\n| \\_cfuvid | [ACM](https://www.acm.org/privacy-policy) | This cookie is a part of the services provided by Cloudflare - Including load-balancing, deliverance of website content and serving DNS connection for website operators. | Session | HTTP Cookie |\n| CookieConsent | [Cookiebot](https://www.cookiebot.com/goto/privacy-policy/) | Stores the user's cookie consent state for the current domain | 1 year | HTTP Cookie |\n| JSESSIONID | [ACM](https://www.acm.org/privacy-policy) | Preserves users states across page requests. | Session | HTTP Cookie |\n| 1.gif | [Cookiebot](https://www.cookiebot.com/goto/privacy-policy/) | Used to count the number of sessions to the website, necessary for optimizing CMP product delivery. | Session | Pixel Tracker |\n\nPreference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| aet-dismiss | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| drafts.queue | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| submitted\\_posts\\_cache | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| mopDeploy | [Mopinion](https://mopinion.com/privacy/) | Pending | Session | HTML Local Storage |\n| MACHINE\\_LAST\\_SEEN | [ACM](https://www.acm.org/privacy-policy) | Pending | 300 days | HTTP Cookie |\n\nStatistic cookies help website owners understand how visitors interact with websites by collecting and reporting information anonymously.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| \\_ga | [Google](https://business.safety.google/privacy/) | Registers a unique ID that is used to generate statistical data on how the visitor uses the website. | 2 years | HTTP Cookie |\n| \\_ga\\_# | [Google](https://business.safety.google/privacy/) | Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. | 2 years | HTTP Cookie |\n| \\_gat | [Google](https://business.safety.google/privacy/) | Used by Google Analytics to throttle request rate | 1 day | HTTP Cookie |\n| \\_gid | [Google](https://business.safety.google/privacy/) | Registers a unique ID that is used to generate statistical data on how the visitor uses the website. | 1 day | HTTP Cookie |\n| \\_hjSession\\_# | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read. | 1 day | HTTP Cookie |\n| \\_hjSessionUser\\_# | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read. | 1 year | HTTP Cookie |\n| \\_hjTLDTest | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Registers statistical data on users' behaviour on the website. Used for internal analytics by the website operator. | Session | HTTP Cookie |\n| \\_hp2\\_# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| \\_hp2\\_hld#.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| \\_hp2\\_id.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 13 months | HTTP Cookie |\n| \\_hp2\\_ses\\_props.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| disqus\\_unique | c.disquscdn.com | Collects statistics related to the user's visits to the website, such as number of visits, average time spent on the website and loaded pages. | Session | HTTP Cookie |\n| collect | [Google](https://business.safety.google/privacy/) | Used to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels. | Session | Pixel Tracker |\n| hjActiveViewportIds | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | This cookie contains an ID string on the current session. This contains non-personal information on what subpages the visitor enters \u2013 this information is used to optimize the visitor's experience. | Persistent | HTML Local Storage |\n| hjViewportId | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Saves the user's screen size in order to adjust the size of images on the website. | Session | HTML Local Storage |\n\nMarketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| badges-message | c.disquscdn.com | Collects data on the visitor\u2019s use of the comment system on the website, and what blogs/articles the visitor has read. This can be used for marketing purposes. | Persistent | HTML Local Storage |\n| NID | [Google](https://business.safety.google/privacy/) | Pending | 6 months | HTTP Cookie |\n| api/telemetry | [Heap Analytics](https://heap.io/privacy) | Collects data on user behaviour and interaction in order to optimize the website and make advertisement on the website more relevant. | Session | Pixel Tracker |\n| h | [Heap Analytics](https://heap.io/privacy) | Collects data on user behaviour and interaction in order to optimize the website and make advertisement on the website more relevant. | Session | Pixel Tracker |\n| #-# | [YouTube](https://business.safety.google/privacy/) | Used to track user\u2019s interaction with embedded content. | Session | HTML Local Storage |\n| iU5q-!O9@$ | [YouTube](https://business.safety.google/privacy/) | Registers a unique ID to keep statistics of what videos from YouTube the user has seen. | Session | HTML Local Storage |\n| LAST\\_RESULT\\_ENTRY\\_KEY | [YouTube](https://business.safety.google/privacy/) | Used to track user\u2019s interaction with embedded content. | Session | HTTP Cookie |\n| LogsDatabaseV2:V#\\|\\|LogsRequestsStore | [YouTube](https://business.safety.google/privacy/) | Used to track user\u2019s interaction with embedded content. | Persistent | IndexedDB |\n| nextId | [YouTube](https://business.safety.google/privacy/) | Used to track user\u2019s interaction with embedded content. | Session | HTTP Cookie |\n| remote\\_sid | [YouTube](https://business.safety.google/privacy/) | Necessary for the implementation and functionality of YouTube video-content on the website. | Session | HTTP Cookie |\n| requests | [YouTube](https://bus",
          "original_query": "Causality: Models, Reasoning, and Inference (Judea Pearl)",
          "cleaned_query": "Causality: Models, Reasoning, and Inference (Judea Pearl)",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Preference-Based Hindsight Goal Relabeling for Sparse-Reward Robotics\nCombine human preference learning (trajectory clip comparisons) with HER-style goal relabeling by using the learned reward model to rank which hindsight goals make a transition \u201cmore aligned.\u201d Implement a replay-buffer sampler that selects relabeled goals maximizing predicted preference reward, and evaluate on MuJoCo manipulation tasks where the nominal reward is absent or extremely sparse.",
        "Active Query Selection via Causal Uncertainty in Reward Models\nExtend preference-based reward learning with a causal lens by estimating which state-action factors are causally responsible for preference judgments, then query humans on clips that best disambiguate those factors. Concretely, maintain a reward model plus a causal-regularized uncertainty estimator (e.g., interventions approximated via environment randomization) to choose comparison pairs that reduce uncertainty over causal feature weights.",
        "Constitutional Preference Learning for Embodied Agents (Rules \u2192 Reward Model)\nTranslate a \u201cconstitution\u201d (principles like safety, non-destruction, energy limits) into automated critiques for robot trajectory segments, producing AI-generated preference labels akin to RLAIF. Train a reward model on these AI preferences and fine-tune a policy with RL, then measure whether the resulting robot behavior is both task-effective and constraint-respecting under distribution shift (new objects, layouts).",
        "Multimodal Reward Models Grounded in Robot Sensors and Language (PaLM-E + RLHF)\nBuild a reward model that ingests multimodal \u201csentences\u201d (images, proprioception/state tokens, and natural-language task descriptions) in the style of PaLM-E, trained from human clip preferences. This enables a single reward model to generalize across tasks by conditioning on language (\u201cstack carefully,\u201d \u201cavoid collisions\u201d), and can be tested on multi-task tabletop manipulation with shared feedback.",
        "Preference-Conditioned Summarization of Robot Episodes for Scalable Oversight\nAdapt \u201clearning to summarize from human feedback\u201d to generate short textual summaries of robot behavior (\u201copened drawer, grasped wrong item, corrected\u201d) that humans can label faster than watching video. Use these summaries to (i) triage which episodes require detailed preference comparisons and (ii) train an auxiliary reward model that predicts human judgment from summaries plus low-rate video labels.",
        "Counterfactual Preference Queries: \u201cWhat If the Robot Had Done X?\u201d\nUse a learned dynamics model or simulator to generate counterfactual variants of a trajectory segment (e.g., different grasp angle, different approach path), then ask humans to compare factual vs counterfactual clips. Train the reward model to be counterfactually consistent (prefers the better intervention), and evaluate whether this improves robustness to reward hacking and spurious visual cues.",
        "Experience Ranking Replay Buffers Driven by Learned Preference Rewards\nExtend \u201cexperience ranking\u201d by ranking transitions/segments using the preference-trained reward model, prioritizing not only high reward but also high learning value (e.g., high advantage variance under the current policy). Implement a two-tier buffer (aligned-high, uncertain) and show improved sample efficiency and reduced catastrophic behaviors compared to uniform or TD-error prioritization.",
        "Personalized Preference Models with Fast User Adaptation for Robotics and Text\nTrain a base reward model from aggregate preferences, then adapt online to an individual user\u2019s preference style using a small number of comparisons (meta-learning or low-rank adapters). Demonstrate that personalization improves both embodied instruction-following (robot executes \u201ctidy\u201d differently per user) and text tasks (summaries reflecting user priorities like brevity vs coverage) while retaining safety constraints.",
        "Principle-to-Policy Distillation: From Constitutional Critiques to Low-Latency Control\nUse constitutional self-critique/revision to generate \u201crationales\u201d and improved actions/plans, then distill them into a lightweight policy for real-time control. The key contribution is a pipeline where expensive deliberation (LLM critique + preference model) produces supervision for a fast controller, evaluated on latency-sensitive mobile manipulation.",
        "Causal Feature Attribution for Detecting Preference Model Shortcut Learning\nDiagnose when reward models trained from preferences latch onto non-causal correlates (background, camera angle, text artifacts) by applying causal attribution tests (environmental randomization, do-interventions in sim) and measuring preference reversals. Use the detected shortcuts to automatically generate adversarial comparison queries and apply targeted regularization, improving out-of-distribution alignment for both robotics clips and summarization outputs."
      ],
      "generated_ideas_raw": "1. **Preference-Based Hindsight Goal Relabeling for Sparse-Reward Robotics**  \n   Combine human preference learning (trajectory clip comparisons) with HER-style goal relabeling by using the learned reward model to rank which hindsight goals make a transition \u201cmore aligned.\u201d Implement a replay-buffer sampler that selects relabeled goals maximizing predicted preference reward, and evaluate on MuJoCo manipulation tasks where the nominal reward is absent or extremely sparse.\n\n2. **Active Query Selection via Causal Uncertainty in Reward Models**  \n   Extend preference-based reward learning with a causal lens by estimating which state-action factors are causally responsible for preference judgments, then query humans on clips that best disambiguate those factors. Concretely, maintain a reward model plus a causal-regularized uncertainty estimator (e.g., interventions approximated via environment randomization) to choose comparison pairs that reduce uncertainty over causal feature weights.\n\n3. **Constitutional Preference Learning for Embodied Agents (Rules \u2192 Reward Model)**  \n   Translate a \u201cconstitution\u201d (principles like safety, non-destruction, energy limits) into automated critiques for robot trajectory segments, producing AI-generated preference labels akin to RLAIF. Train a reward model on these AI preferences and fine-tune a policy with RL, then measure whether the resulting robot behavior is both task-effective and constraint-respecting under distribution shift (new objects, layouts).\n\n4. **Multimodal Reward Models Grounded in Robot Sensors and Language (PaLM-E + RLHF)**  \n   Build a reward model that ingests multimodal \u201csentences\u201d (images, proprioception/state tokens, and natural-language task descriptions) in the style of PaLM-E, trained from human clip preferences. This enables a single reward model to generalize across tasks by conditioning on language (\u201cstack carefully,\u201d \u201cavoid collisions\u201d), and can be tested on multi-task tabletop manipulation with shared feedback.\n\n5. **Preference-Conditioned Summarization of Robot Episodes for Scalable Oversight**  \n   Adapt \u201clearning to summarize from human feedback\u201d to generate short textual summaries of robot behavior (\u201copened drawer, grasped wrong item, corrected\u201d) that humans can label faster than watching video. Use these summaries to (i) triage which episodes require detailed preference comparisons and (ii) train an auxiliary reward model that predicts human judgment from summaries plus low-rate video labels.\n\n6. **Counterfactual Preference Queries: \u201cWhat If the Robot Had Done X?\u201d**  \n   Use a learned dynamics model or simulator to generate counterfactual variants of a trajectory segment (e.g., different grasp angle, different approach path), then ask humans to compare factual vs counterfactual clips. Train the reward model to be counterfactually consistent (prefers the better intervention), and evaluate whether this improves robustness to reward hacking and spurious visual cues.\n\n7. **Experience Ranking Replay Buffers Driven by Learned Preference Rewards**  \n   Extend \u201cexperience ranking\u201d by ranking transitions/segments using the preference-trained reward model, prioritizing not only high reward but also high learning value (e.g., high advantage variance under the current policy). Implement a two-tier buffer (aligned-high, uncertain) and show improved sample efficiency and reduced catastrophic behaviors compared to uniform or TD-error prioritization.\n\n8. **Personalized Preference Models with Fast User Adaptation for Robotics and Text**  \n   Train a base reward model from aggregate preferences, then adapt online to an individual user\u2019s preference style using a small number of comparisons (meta-learning or low-rank adapters). Demonstrate that personalization improves both embodied instruction-following (robot executes \u201ctidy\u201d differently per user) and text tasks (summaries reflecting user priorities like brevity vs coverage) while retaining safety constraints.\n\n9. **Principle-to-Policy Distillation: From Constitutional Critiques to Low-Latency Control**  \n   Use constitutional self-critique/revision to generate \u201crationales\u201d and improved actions/plans, then distill them into a lightweight policy for real-time control. The key contribution is a pipeline where expensive deliberation (LLM critique + preference model) produces supervision for a fast controller, evaluated on latency-sensitive mobile manipulation.\n\n10. **Causal Feature Attribution for Detecting Preference Model Shortcut Learning**  \n   Diagnose when reward models trained from preferences latch onto non-causal correlates (background, camera angle, text artifacts) by applying causal attribution tests (environmental randomization, do-interventions in sim) and measuring preference reversals. Use the detected shortcuts to automatically generate adversarial comparison queries and apply targeted regularization, improving out-of-distribution alignment for both robotics clips and summarization outputs.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Preference-Based Hindsight Goal Relabeling for Sparse-Reward Robotics\nCombine human preference learning (trajectory clip comparisons) with HER-style goal relabeling by using the learned reward model t",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Active Query Selection via Causal Uncertainty in Reward Models\nExtend preference-based reward learning with a causal lens by estimating which state-action factors are causally responsible for preferen",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Constitutional Preference Learning for Embodied Agents (Rules \u2192 Reward Model)\nTranslate a \u201cconstitution\u201d (principles like safety, non-destruction, energy limits) into automated critiques for robot tra",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Multimodal Reward Models Grounded in Robot Sensors and Language (PaLM-E + RLHF)\nBuild a reward model that ingests multimodal \u201csentences\u201d (images, proprioception/state tokens, and natural-language task",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Preference-Conditioned Summarization of Robot Episodes for Scalable Oversight\nAdapt \u201clearning to summarize from human feedback\u201d to generate short textual summaries of robot behavior (\u201copened drawer, g",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Counterfactual Preference Queries: \u201cWhat If the Robot Had Done X?\u201d\nUse a learned dynamics model or simulator to generate counterfactual variants of a trajectory segment (e.g., different grasp angle, d",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Experience Ranking Replay Buffers Driven by Learned Preference Rewards\nExtend \u201cexperience ranking\u201d by ranking transitions/segments using the preference-trained reward model, prioritizing not only high",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Personalized Preference Models with Fast User Adaptation for Robotics and Text\nTrain a base reward model from aggregate preferences, then adapt online to an individual user\u2019s preference style using a ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Principle-to-Policy Distillation: From Constitutional Critiques to Low-Latency Control\nUse constitutional self-critique/revision to generate \u201crationales\u201d and improved actions/plans, then distill them ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Causal Feature Attribution for Detecting Preference Model Shortcut Learning\nDiagnose when reward models trained from preferences latch onto non-causal correlates (background, camera angle, text artifa",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 22,
      "paper_title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
      "contribution": "Identifies and formalizes 'feature absorption'\u2014a systematic failure mode of Sparse Autoencoders (SAEs) where seemingly monosemantic latents are suppressed by their hierarchical children under sparsity pressure\u2014introduces a metric to detect it, and empirically shows it is pervasive and not remedied by simple SAE size or sparsity tuning.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9116,
      "output_tokens": 1018,
      "predecessor_details": [
        {
          "success": true,
          "title": "Sparse coding with an over-complete basis set: A strategy employed ...",
          "url": "https://www.researchgate.net/publication/13803865_Sparse_coding_with_an_over-complete_basis_set_A_strategy_employed_by_V1_Vision_Research",
          "content": "Article PDF Available Abstract The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli. Discover the world's research 25+ million members 160+ million publication pages 2.3+ billion citations Join for free... Here, approximate orthogonality is argued to be necessary for circumventing the problem of packing more unique vectors (concepts) than the dimensionality of the space allows (a.k.a. the superposition problem [24]), hence enabling reliable estimation of a concept's presence by a linear operator (e.g., the linear map in an MLP) [29,30]. Grounded in this idea, and inspired by its relation to the well-known problem of sparse coding [31, 32], SAEs have been proposed to learn, in an unsupervised manner, a sparse, overcomplete dictionary of directions that (ideally) map onto abstract, interpretable concepts encoded in a neural network, hence enabling its interpretability [33,14]. Figure 1: Conceptual organization in neural representations. ...... Specifically, noting that the model described in Def. 2.1 closely aligns with the generative model assumed in classical work on sparse coding [32, 31,81], wherein one seeks to express data as sparse linear combinations over an overcomplete dictionary, SAEs were recently proposed to re-contextualize that literature's tools for identifying concepts encoded in a neural network's representations [14][15][16][17][18][19][20][21][22][23]. ...... Sparse coding Sparse linear generative models aim to find a sparse representations z \u2208 R p that explains the data x \u2208 R m via a collection of atoms, forming an overcomplete dictionary D \u2208 R m\u00d7p (p \u226b m.) [106, 32]. Sparse representations are ubiquitous in science and engineering, with origins from computational neuroscience [106,32] and applications in medical imaging [107][108][109], image restorations [110][111][112], radar sensing [113], transcriptomics [114,115], and genomics [116,117]. ... Motivated by the hypothesis that neural network representations encode abstract, interpretable features as linearly accessible, approximately orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in interpretability. However, recent work has demonstrated phenomenology of model representations that lies outside the scope of this hypothesis, showing signatures of hierarchical, nonlinear, and multi-dimensional features. This raises the question: do SAEs represent features that possess structure at odds with their motivating hypothesis? If not, does avoiding this mismatch help identify said features and gain further insights into neural network representations? To answer these questions, we take a construction-based approach and re-contextualize the popular matching pursuits (MP) algorithm from sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a sequence of residual-guided steps, allowing it to capture hierarchical and nonlinearly accessible features. Comparing this architecture with existing SAEs on a mixture of synthetic and natural data settings, we show: (i) hierarchical concepts induce conditionally orthogonal features, which existing SAEs are unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE recovers highly meaningful features, helping us unravel shared structure in the seemingly dichotomous representation spaces of different modalities in a vision-language model, hence demonstrating the assumption that useful features are solely linearly accessible is insufficient. We also show that the sequential encoder principle of MP-SAE affords an additional benefit of adaptive sparsity at inference time, which may be of independent interest. Overall, we argue our results provide credence to the idea that interpretability should begin with the phenomenology of representations, with methods emerging from assumptions that fit it.... Sparse coding is inspired by the behavior of neurons in the primary visual cortex (V1) [46]. This method, when applied to an input \u2208 \u211d generates a simpler representation in the form of a sparse vector \u2208 \u211d , where most coefficients are zeros. ... Spike sorting is a crucial step in decoding multichannel extracellular neural signals, enabling the identification of individual neuronal activity. A key challenge in brain-machine interfaces (BMIs) is achieving real-time, low-power spike sorting at the edge while keeping high neural decoding performance. This study introduces the Neuromorphic Sparse Sorter (NSS), a compact two-layer spiking neural network optimized for efficient spike sorting. NSS leverages the Locally Competitive Algorithm (LCA) for sparse coding to extract relevant features from noisy events with reduced computational demands. NSS learns to sort detected spike waveforms in an online fashion and operates entirely unsupervised. To exploit multi-bit spike coding capabilities of neuromorphic platforms like Intel's Loihi 2, a custom neuron model was implemented, enabling flexible power-performance trade-offs via adjustable spike bit-widths. Evaluations on simulated and real-world tetrode signals with biological drift showed NSS outperformed established pipelines such as WaveClus3 and PCA+KMeans. With 2-bit graded spikes, NSS on Loihi 2 outperformed NSS implemented with leaky integrate-and-fire neuron and achieved an F1-score of 77% (+10% improvement) while consuming 8.6mW (+1.65mW) when tested on a drifting recording, with a computational processing time of 0.25ms (+60 us) per inference.... The linear representation hypothesis proposes that semantic concepts are encoded in linear subspaces of neural representations (Mikolov et al., 2013). Additionally, the sparse coding hypothesis assumes that semantic concepts in neural representations can be effectively captured using sparsity priors to identify meaningful features in an unsupervised manner (Olshausen &amp; Field, 1997; Arora et al., 2018;Elhage et al., 2022). Together, these foundational works inspired the development of the SAE architecture and its variants, which produce highly interpretable and semantically meaningful features from neural representations (Cunningham et al., 2023;Bricken et al., 2023;Gao et al., 2024;Rajamanoharan et al., 2024a;b). ... Miles Wang Tom Dupr\u00e9 la Tour Olivia Watkins Dan Mossing Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes \"emergent misalignment,\" where models give stereotypically malicious responses to unrelated prompts. We extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training. To investigate the mechanisms behind this generalized misalignment, we apply a \"model diffing\" approach using sparse autoencoders to compare internal model representations before and after fine-tuning. This approach reveals several \"misaligned persona\" features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to predict whether a model will exhibit such behavior. Additionally, we investigate mitigation strategies, discovering that fine-tuning an emergently misaligned model on just a few hundred benign samples efficiently restores alignment.... Sparse Autoencoders (Huben et al., 2023;Bricken et al., 2023b) address the Superposition Hypothesis in Transformers by disentangling representational patterns through sparse dictionary learning (Olshausen and Field, 1997; Elad, 2010) for the underlying features. These models are structured as overcomplete autoencoders, featuring hidden layers with greater dimensionality than their inputs, while incorporating sparsity constraints through L 1 regularisation or explicit TopK mechanisms (Gao et al., 2024). ... Seonglae Cho Harryn Oh Donghyun Lee Ziad El Sayed Sparse Autoencoders (SAEs) have emerged as a promising solution for decomposing large language model representations into interpretable features. However, Paulo and Belrose (2025) have highlighted instability across different initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not capture model-internal features. These problems likely stem from training SAEs on external datasets - either collected from the Web or generated by another model - which may contain out-of-distribution (OOD) data beyond the model's generalisation capabilities. This can result in hallucinated SAE features, which we term \"Fake Features\", that misrepresent the model's internal activations. To address these issues, we propose FaithfulSAE, a method that trains SAEs on the model",
          "original_query": "Sparse coding with an overcomplete basis set: A strategy employed by v1?",
          "cleaned_query": "Sparse coding with an overcomplete basis set: A strategy employed by v1?",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Toy Models of Superposition",
          "url": "https://transformer-circuits.pub/2022/toy_model/index.html",
          "content": "Toy Models of Superposition\n[Transformer Circuits Thread](https://transformer-circuits.pub/)\n# Toy Models of Superposition\n### Authors\n[Nelson Elhage\u2217](https://nelhage.com/),Tristan Hume\u2217,Catherine Olsson\u2217,Nicholas Schiefer\u2217,[Tom Henighan](https://tomhenighan.com),Shauna Kravec,[Zac Hatfield-Dodds](https://zhd.dev),Robert Lasenby,Dawn Drain,Carol Chen,Roger Grosse,Sam McCandlish,Jared Kaplan,Dario Amodei,[Martin Wattenberg](https://www.bewitched.com/)\u2217,[Christopher Olah\u2021](https://colah.github.io/)\n### Affiliations\n[Anthropic,](https://www.anthropic.com/)[Harvard](https://www.harvard.edu/)\n### Published\nSept 14, 2022\n\\* Core Research Contributor;\u2021 Correspondence to[colah@anthropic.com](colah@anthropic.com);[Author contributions statement below](#).\nIt would be very convenient if the individual neurons of artificial neural networks corresponded to cleanly interpretable features of the input. For example, in an \u201cideal\u201d ImageNet classifier, each neuron would fire only in the presence of a specific visual feature, such as the color red, a left-facing curve, or a dog snout. Empirically, in models we have studied, some of the neurons do cleanly map to features. But it isn't always the case that features correspond so cleanly to neurons, especially in large language models where it actually seems rare for neurons to correspond to clean features. This brings up many questions. Why is it that neurons sometimes align with features and sometimes don't? Why do some models and tasks have many of these clean neurons, while they're vanishingly rare in others?\nIn this paper, we use toy models \u2014small ReLU networks trained on synthetic data with sparse input features \u2014to investigate how and when models represent more features than they have dimensions. We call this phenomenonsuperposition. When features are sparse, superposition allows compression beyond what a linear model would do, at the cost of \"interference\" that requires nonlinear filtering.\nConsider a toy model where we train an embedding of five features of varying importanceWhere \u201cimportance\u201d is a scalar multiplier on mean squared error loss.in two dimensions, add a ReLU afterwards for filtering, and vary the sparsity of the features. With dense features, the model learns to represent an orthogonal basis of the most important two features (similar to what Principal Component Analysis might give us), and the other three features are not represented. But if we make the features sparse, this changes:\nThis figure and a few others can be reproduced using the[toy model framework Colab notebook](https://colab.research.google.com/github/anthropics/toy-models-of-superposition/blob/main/toy_models.ipynb)in our[Github repo](https://github.com/anthropics/toy-models-of-superposition)\nNot only can models store additional features in superposition by tolerating some interference, but we'll show that, at least in certain limited cases,models can perform computation while in superposition. (In particular, we'll show that models can put simple circuits computing the absolute value function in superposition.) This leads us to hypothesize thattheneural networks we observe in practice are in some sense noisily simulating larger, highly sparse networks.\u00a0In other words, it's possible that models we train can be thought of as doing \u201cthe same thing as\u201d an imagined much-larger model, representing the exact same features but with no interference.\nFeature superposition isn't a novel idea. A number of previous interpretability papers have considered it, and it's very closely related to the long-studied topic of compressed sensing in mathematics, as well as the ideas of distributed, dense, and population codes in neuroscienceand deep learning. What, then, is the contribution of this paper?\nFor interpretability researchers, our main contribution is providing a direct demonstration that superposition occurs in artificial neural networks given a relatively natural setup, suggesting this may also occur in practice. That is, we show a case where interpreting neural networks as having sparse structure in superposition isn't just a useful post-hoc interpretation, but actually the \"ground truth\" of a model. We offer a theory of when and why this occurs, revealing a[phase diagram](#phase-change)for superposition. This[explains](#privileged-basis)why neurons are sometimes \"monosemantic\" responding to a single feature, and sometimes \"polysemantic\"responding to many unrelated features. We also discover that, at least in our toy model, superposition exhibits[complex geometric structure](#geometry).\nBut our results may also be of broader interest. We find preliminary evidence that superposition may be linked to adversarial examples and grokking, and might also suggest a theory for the performance of mixture\u00a0of experts\u00a0models. More broadly, the toy model we investigate has unexpectedly rich structure, exhibiting[phase changes](#phase-change), a[geometric structure](#geometry)based on uniform polytopes,[\"energy level\"-like jumps](#learning-jumps)during training, and a[phenomenon](#geometry-uniform)which is qualitatively similar to the fractional quantum Hall effect\u00a0in physics, among other striking phenomena. We originally investigated the subject to gain understanding of cleanly-interpretable neurons in larger models, but we've found these toy models to be surprisingly interesting in their own right.\n#### [Key Results\u00a0From Our Toy Models](#key-results)\nIn our toy models, we are able to demonstrate that:\n* Superposition is a real, observed phenomenon.\n* Both monosemantic and polysemantic neurons can form.\n* At least some kinds of computation can be performed in superposition.\n* Whether features are stored in superposition isgoverned by a phase change.\n* Superposition organizes features into geometric structuressuch as digons, triangles, pentagons, and tetrahedrons.\nOur toy models are simple ReLU networks, so it seems fair to say that neural networks exhibit these properties in at least some regimes, but it's very unclear what to generalize to real networks.\n## [Definitions and Motivation: Features, Directions, and Superposition](#motivation)\nIn our work, we\u00a0often think of neural networks\u00a0as havingfeatures of the inputrepresented asdirectionsin activation space. This isn't a trivial claim. It isn't obvious what kind of structure we should expect neural network representations to have. When we say something like \"word embeddings have a gender direction\" or \"vision models have curve detector neurons\", one is implicitly making strong claims about the structure of network representations.\nDespite this, we believe this kind of \"linear representation hypothesis\" is supported both by significant empirical findings and theoretical arguments. One might think of this as two separate properties, which we'll explore in more detail shortly:\n* Decomposability:Network representations can be\u00a0described in terms of\u00a0independently understandable features.\n* Linearity:Features are represented by direction.\nIf we hope to reverse engineer neural networks, weneeda property like decomposability. Decomposability is what[allows us to reason about the model](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)without fitting the whole thing in our heads! But it's not enough for things to be decomposable: we need to be able to access the decomposition somehow. In order to do this, we need toidentifythe individual features within a representation. In a linear representation, this corresponds to determining which directions in activation space correspond to which independent features of the input.\nSometimes,\u00a0identifying feature directions\u00a0is very easy because features seem to correspond to neurons.\u00a0For example, many neurons in the early layers of InceptionV1 clearly correspond to features (e.g. curve detector neurons). Why is it that we sometimes get this extremely helpful property, but in other cases don't? We hypothesize that there are really two countervailing forces driving this:\n* Privileged Basis:Only some representations have aprivileged basiswhich encourages features to align with basis directions (i.e. to correspond to neurons).\n* Superposition:Linear representations can represent more features than dimensions, using a strategy we callsuperposition. This can be seen as neural networkssimulating larger networks. This pushes featuresawayfrom corresponding to neurons.\nSuperposition has been hypothesized in previous work, and in some cases, assuming something like superposition has been shown to help find interpretable structure. However, we're not aware of feature superposition having been unambiguously demonstrated to occur in neural networks before (demonstrates a closely related phenomenon of model superposition).\u00a0The goal of this paper is to change that, demonstrating superposition and exploring how it interacts with privileged bases. If superposition occurs in networks, it deeply influences what approaches to interpretability research make sense, so unambiguous demonstration seems important.\nThe goal of this section will be to motivate these ideas and unpack them in detail.\nIt's worth noting that many of the ideas in this section have close connections to ideas in other lines of interpretability research (especially disentanglement), neuroscience (distributed representations, population codes, etc), compressed sensing, and many other lines of work. This section will focus on articulating our perspective on the problem. We'll discuss these other lines of work in detail in[Related Work](#related-work).\n### [Empirical Phenomena](#motivation-empirical)\nWhen we talk about \"features\" and how they're represented, this is ultimately theory building around several observed empirical phenomena. Before describing how we conceptualize those results, we'll simply describe some of the major results motivating our thinking:\n* Word Embeddings- A famous result byMikolov et al.found that word embeddings appear\u00a0to have directions which cor",
          "original_query": "Toy models of superposition",
          "cleaned_query": "Toy models of superposition",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Decomposing Language Models With Dictionary Learning",
          "url": "https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning",
          "content": "[Skip to main content](#main-content)[Skip to footer](#footer)\n[\n](https://www.anthropic.com/)\n[Try Claude](https://claude.ai/)\nInterpretabilityResearch\n# Towards Monosemanticity: Decomposing Language Models With Dictionary Learning\nOct 5, 2023\n[Read Paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html)\n## Abstract\nIn our latest paper,[*Towards Monosemanticity: Decomposing Language Models With Dictionary Learning*](https://transformer-circuits.pub/2023/monosemantic-features), we outline evidence that there are better units of analysis than individual neurons, and we have built machinery that lets us find these units in small transformer models. These units, called features, correspond to patterns (linear combinations) of neuron activations. This provides a path to breaking down complex neural networks into parts we can understand, and builds on previous efforts to interpret high-dimensional systems in neuroscience, machine learning, and statistics. In a transformer language model, we decompose a layer with 512 neurons into more than 4000 features which separately represent things like DNA sequences, legal language, HTTP requests, Hebrew text, nutrition statements, and much, much more. Most of these model properties are invisible when looking at the activations of individual neurons in isolation.\n[](https://twitter.com/intent/tweet?text=https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning)[](https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning)\n## Related content\n### Introducing Anthropic Interviewer: What 1,250 professionals told us about working with AI\nWe built an interview tool called Anthropic Interviewer. Powered by Claude, Anthropic Interviewer runs detailed interviews automatically and at unprecedented scale.\n[Read more](https://www.anthropic.com/research/anthropic-interviewer)\n### How AI is transforming work at Anthropic\nWe surveyed Anthropic engineers and researchers, conducted in-depth qualitative interviews, and studied internal Claude Code usage data to find out how AI use is changing how we do our jobs. We found that AI use is radically changing the nature of work for software developers.\n[Read more](https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic)\n### Estimating AI productivity gains from Claude conversations\nAnalyzing 100,000 Claude conversations, this research finds AI reduces task time by 80% on average. If universally adopted over 10 years, current models could increase US labor productivity growth by 1.8% annually\u2014doubling recent rates. Knowledge work like software development and management see the largest gains.\n[Read more](https://www.anthropic.com/research/estimating-productivity-gains)\nTowards Monosemanticity: Decomposing Language Models With Dictionary Learning \\\\ Anthropic",
          "original_query": "Towards monosemanticity: Decomposing language models with dictionary learning",
          "cleaned_query": "Towards monosemanticity: Decomposing language models with dictionary learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Sparse Autoencoders Find Highly Interpretable Features in ...",
          "url": "https://www.matsprogram.org/research/sparse-autoencoders-find-highly-interpretable-features-in-language-models",
          "content": "Sparse Autoencoders Find Highly Interpretable Features in Language Models - MATS Research\n[Research](https://www.matsprogram.org/research)\n# Sparse Autoencoders Find Highly Interpretable Features in Language Models\n[\nView publication\n](https://arxiv.org/abs/2309.08600)\nMATS Fellow:\nHoagy Cunningham\nAuthors:\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, Lee Sharkey\nCitations\n738 Citations\nAbstract:\nOne of the roadblocks to a better understanding of neural networks&apos;&apos; internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\\\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\\\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.\n## Recent research\n[\nAI Futures Model: Timelines &amp; Takeoff\n](https://www.matsprogram.org/research/ai-futures-model-timelines-takeoff)\nAuthors:\nBrendan Halstead, Alex Kastner\nDate:\nDecember 30, 2025\nCitations:\n0\n[\nRecontextualization Mitigates Specification Gaming without Modifying the Specification\n](https://www.matsprogram.org/research/recontextualization-mitigates-specification-gaming-without-modifying-the-specification)\nAuthors:\nAriana Azarbal, Victor Gillioz\nDate:\nDecember 22, 2025\nCitations:\n3\n### Frequently asked questions\nWhat is the MATS Program?\nHow long does the program last?",
          "original_query": "Sparse autoencoders find highly interpretable features in language models",
          "cleaned_query": "Sparse autoencoders find highly interpretable features in language models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Showing SAE Latents Are Not Atomic Using Meta-SAEs",
          "url": "https://www.alignmentforum.org/posts/TMAmHh4DdMr4nCSr5/showing-sae-latents-are-not-atomic-using-meta-saes",
          "content": "[Showing SAE Latents Are Not Atomic Using Meta-SAEs](https://www.alignmentforum.org/www.alignmentforum.org)\n\n23 min read\n\n\u2022\n\n[Introduction](https://www.alignmentforum.org/www.alignmentforum.org#Introduction)\n\n\u2022\n\n[Defining Meta-SAEs](https://www.alignmentforum.org/www.alignmentforum.org#Defining_Meta_SAEs)\n\n\u2022\n\n[Meta-latents form interpretable decompositions of SAE latents](https://www.alignmentforum.org/www.alignmentforum.org#Meta_latents_form_interpretable_decompositions_of_SAE_latents)\n\n\u2022\n\n[Are Meta-Latents different from SAE Latents?](https://www.alignmentforum.org/www.alignmentforum.org#Are_Meta_Latents_different_from_SAE_Latents_)\n\n\u2022\n\n[Using Meta-SAEs to Interpret Split Features](https://www.alignmentforum.org/www.alignmentforum.org#Using_Meta_SAEs_to_Interpret_Split_Features)\n\n\u2022\n\n[Causally Intervening and Making Targeted Edits with Meta-Latents](https://www.alignmentforum.org/www.alignmentforum.org#Causally_Intervening_and_Making_Targeted_Edits_with_Meta_Latents)\n\n\u2022\n\n[Discussion](https://www.alignmentforum.org/www.alignmentforum.org#Discussion)\n\n[Sparse Autoencoders (SAEs)](https://www.alignmentforum.org/w/sparse-autoencoders-saes)[MATS Program](https://www.alignmentforum.org/w/mats-program)[Interpretability (ML & AI)](https://www.alignmentforum.org/w/interpretability-ml-and-ai)[AI](https://www.alignmentforum.org/w/ai)\nFrontpage\n\n# 35\n\n# [Showing SAE Latents Are Not Atomic Using Meta-SAEs](https://www.alignmentforum.org/posts/TMAmHh4DdMr4nCSr5/showing-sae-latents-are-not-atomic-using-meta-saes)\n\nby [Bart Bussmann](https://www.alignmentforum.org/users/bart-bussmann?from=post_header), [Michael Pearce](https://www.alignmentforum.org/users/michael-pearce?from=post_header), [Patrick Leask](https://www.alignmentforum.org/users/patrick-leask?from=post_header), [Joseph Bloom](https://www.alignmentforum.org/users/joseph-bloom?from=post_header), [Lee Sharkey](https://www.alignmentforum.org/users/lee_sharkey?from=post_header), [Neel Nanda](https://www.alignmentforum.org/users/neel-nanda-1?from=post_header)\n\n24th Aug 2024\n\n23 min read\n\n[10](https://www.alignmentforum.org/www.alignmentforum.org#comments)\n\n# 35\n\n_Bart, Michael and Patrick are joint first authors_. \u00a0_Research conducted as part of MATS 6.0 in Lee Sharkey and Neel Nanda\u2019s streams. Thanks to Mckenna Fitzgerald and Robert Krzyzanowski for their feedback!_\n\n_TL;DR:_\n\n- _Sparse Autoencoder (SAE) latents have been shown to typically be monosemantic (i.e. correspond to an interpretable property of the input). It is sometimes implicitly assumed that they are therefore atomic, i.e. simple, irreducible units that make up the model\u2019s computation._\n- _We provide evidence against this assumption by finding sparse, interpretable decompositions of SAE decoder directions into seemingly more atomic latents, e.g. Einstein -> science + famous + German + astronomy + energy + starts with E-_\n- _We do this by training meta-SAEs, an SAE trained to reconstruct the decoder directions of a normal SAE._\n- _We argue that, conceptually, there\u2019s no reason to expect SAE latents to be atomic - when the model is thinking about Albert Einstein, it likely also thinks about Germanness, physicists, etc. Because Einstein\u00a0always\u00a0entails those things, the sparsest solution is to have the Albert Einstein latent also boost them._\n- _Key results_\n - _SAE latents can be decomposed into more atomic, interpretable meta-latents._\n - _We show that when latents in a larger SAE have split out from latents in a smaller SAE, a meta SAE trained on the larger SAE often recovers this structure._\n - _We demonstrate that meta-latents allow for more precise causal interventions on model behavior than SAE latents on a targeted knowledge editing task._\n- _We believe that the alternate, interpretable decomposition using MetaSAEs casts doubt on the implicit assumption that SAE latents are atomic. We show preliminary results that\u00a0 MetaSAE latents have significant ovelap with latents in a normal SAE of the same size but may relate differently to the larger SAEs used in MetaSAE training._\n\n_We made a_ [_dashboard_](https://metasae.streamlit.app/?page=Feature+Explorer&feature=11329) _that lets you explore meta-SAE latents._\n\n**Terminology**: Throughout this post we use \u201clatents\u201d to describe the concrete components of the SAE\u2019s dictionary, whereas \u201cfeature\u201d refers to the abstract concepts, following [Lieberum et al](https://arxiv.org/abs/2408.05147).\n\n# Introduction\n\nMechanistic interpretability (mech interp) attempts to understand neural networks by breaking down their computation into interpretable components. One of the key challenges of this line of research is the [polysemanticity of neurons](https://distill.pub/2020/circuits/zoom-in/#claim-1-polysemantic), meaning they respond to seemingly unrelated inputs. Sparse autoencoders (SAEs) [have](https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition) [been](https://arxiv.org/abs/2309.08600)[proposed](https://transformer-circuits.pub/2023/monosemantic-features/index.html)\u00a0as a method for decomposing model activations into sparse linear sums of latents. Ideally, these latents should be monosemantic, i.e. respond to inputs that clearly share a similar meaning (implicitly, from the perspective of a human interpreter). That is, a human should be able to reason about the latents both in relation to the features to which they are associated, and also use the latents to better understand the model\u2019s overall behavior.\n\nThere is a popular notion, both implicitly in related work on SAEs within mech interp and explicitly by the use of the term \u201catom\u201d in [sparse dictionary learning](https://en.wikipedia.org/wiki/Sparse_dictionary_learning)\u00a0as a whole, that SAE features are atomic or can be [\"true features\"](https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-feature-splitting). However, monosemanticity does not imply atomicity. Consider [the example](https://transformer-circuits.pub/2023/superposition-composition/index.html)\u00a0of shapes of different colors - the set of shapes is \\[circle, triangle, square\\], and the set of colors is \\[white, red, green, black\\], each of which is represented with a linear direction. \u2018Red triangle\u2019 represents a monosemantic feature, but not an atomic feature, as it can be decomposed into red and triangle. It [has been shown](https://www.alignmentforum.org/posts/a5wwqza2cY3W7L9cj/sparse-autoencoders-find-composed-features-in-small-toy) that sufficiently wide SAEs on toy models will learn \u2018red triangle\u2019, rather than representing \u2018red\u2019 and \u2018triangle\u2019 with separate latents.\n\nFurthermore, whilst one may naively reason about SAE latents as bags of words with almost-random directions, there are hints of deeper structure, as argued by [Wattenberg et al](https://arxiv.org/abs/2407.14662): UMAP plots (a distance-based dimensionality reduction method) [group together conceptually similar latents](https://transformer-circuits.pub/2023/monosemantic-features), suggesting that they share components; and local PCA recovers a [globally interpretable](https://www.alignmentforum.org/posts/WsPyunwpXYCM2iN6t/calendar-feature-geometry-in-gpt-2-layer-8-residual-stream)\u00a0timeline direction.\n\nMost notably, feature splitting makes clear that directions are not almost-random - When a latent in a small SAE \u201csplits\u201d into several latents in a larger SAE, the larger SAE latents have significant cosine sim with each other along with semantic connections. Arguably, such results already made it clear that SAE features are not atomic, but we found the results of our investigation sufficiently surprising, that we hope it is valuable to carefully explore and document this phenomena.\n\nWe introduce meta-SAEs, where we train an SAE on the decoder weights of an SAE, effectively decomposing the SAE latents into new, sparse, monosemantic latents. For example, we find a decomposition of a latent relating to Albert Einstein into meta-latents for Physics, German, Famous people, and others. Similarly, we find a decomposition of a Paris-related latent into meta-latents for French city names, capital cities, Romance languages, and words ending in the _-us_\u00a0sound.\n\nIn this post we make the following contributions:\n\n- We show that meta-SAEs are a useful tool for exploring and understanding SAE latents through a series of case studies, and provide a [dashboard](https://metasae.streamlit.app/?page=Feature+Explorer&feature=11329)\u00a0for this exploration.\n- We show that when latents in a larger SAE have split out from latents in a smaller SAE, a meta SAE trained on the larger SAE often recovers this structure.\n- We demonstrate that meta-latents are useful for performing causal interventions to edit factual knowledge associations in language models on a dataset of city attributes. For example, we find a combination of meta-latents that let us steer Tokyo to speak French and use the Euro, but to remain in Japan.\n- We investigate baselines for breaking down larger SAE latents, like taking the latents in a smaller SAE with the highest cosine sim, and show that these are also interpretable, suggesting meta-SAEs are not the only path to these insights.\n\nWhilst our results suggest that SAE latents are not atomic, we do not claim that SAEs are not useful. Rather, we believe that meta-SAEs provide another frame of reference for interpreting the model. In the natural sciences there are multiple levels of abstraction for understanding systems, such as cells and organisms, and atoms and molecules; with each different level being useful in different contexts.\n\nWe also note several limitations. Meta-SAEs give a lossy decomposition, i.e. there is an error term, and meta-features may not be intrinsically lower level than SAE features- Albert Einstein is arguably more fine-grained than man, for example, and may be a more appropriate abstraction in certain contexts. We also do not claim that meta-SAEs have found the \u2018true\u2019 atoms of neural comput",
          "original_query": "Showing SAE latents are not atomic using meta-SAEs",
          "cleaned_query": "Showing SAE latents are not atomic using meta-SAEs",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Proximal Methods for Hierarchical Sparse Coding",
          "url": "https://www.jmlr.org/papers/volume12/jenatton11a/jenatton11a.pdf",
          "content": "Journal of Machine Learning Research 12 (2011) 2297-2334 Submitted 9/10; Revised 3/11; Published 7/11\nProximal Methods for Hierarchical Sparse Coding\nRodolphe Jenatton\u2217 \u2020 RODOLPHE.JENATTON@INRIA.FR\nJulien Mairal\u2217\u2020JULIEN.MAIRAL@INRIA.FR\nGuillaume Obozinski\u2020 GUILLAUME.OBOZINSKI@INRIA.FR\nFrancis Bach\u2020FRANCIS.BACH@INRIA.FR\nINRIA - WILLOW Project-Team\nLaboratoire d\u2019Informatique de l\u2019Ecole Normale Superieure (INRIA/ENS/CNRS UMR 8548) \u00b4\n23, avenue d\u2019Italie\n75214 Paris CEDEX 13, France\nEditor: Tong Zhang\nAbstract\nSparse coding consists in representing signals as sparse linear combinations of atoms selected from\na dictionary. We consider an extension of this framework where the atoms are further assumed to\nbe embedded in a tree. This is achieved using a recently introduced tree-structured sparse regu\u0002larization norm, which has proven useful in several applications. This norm leads to regularized\nproblems that are difficult to optimize, and in this paper, we propose efficient algorithms for solving\nthem. More precisely, we show that the proximal operator associated with this norm is computable\nexactly via a dual approach that can be viewed as the composition of elementary proximal opera\u0002tors. Our procedure has a complexity linear, or close to linear, in the number of atoms, and allows\nthe use of accelerated gradient techniques to solve the tree-structured sparse approximation prob\u0002lem at the same computational cost as traditional ones using the \u21131-norm. Our method is efficient\nand scales gracefully to millions of variables, which we illustrate in two types of applications:\nfirst, we consider fixed hierarchical dictionaries of wavelets to denoise natural images. Then, we\napply our optimization tools in the context of dictionary learning, where learned dictionary ele\u0002ments naturally self-organize in a prespecified arborescent structure, leading to better performance\nin reconstruction of natural image patches. When applied to text documents, our method learns\nhierarchies of topics, thus providing a competitive alternative to probabilistic topic models.\nKeywords: Proximal methods, dictionary learning, structured sparsity, matrix factorization\n1. Introduction\nModeling signals as sparse linear combinations of atoms selected from a dictionary has become\na popular paradigm in many fields, including signal processing, statistics, and machine learning.\nThis line of research, also known as sparse coding, has witnessed the development of several well\u0002founded theoretical frameworks (Tibshirani, 1996; Chen et al., 1998; Mallat, 1999; Tropp, 2004,\n2006; Wainwright, 2009; Bickel et al., 2009) and the emergence of many efficient algorithmic tools\n\u2217. These authors contributed equally.\n\u2020. Rodolphe Jenatton, Guillaume Obozinski, and Francis Bach are now affiliated to INRIA - Sierra Project-Team.\nJulien Mairal is now with the Statistics Department of the University of California at Berkeley. When this work was\nperformed all authors were affiliated to INRIA - Willow Project-Team.\n\u00a92011 Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski and Francis Bach.\nJENATTON, MAIRAL, OBOZINSKI AND BACH\n(Efron et al., 2004; Nesterov, 2007; Beck and Teboulle, 2009; Wright et al., 2009; Needell and\nTropp, 2009; Yuan et al., 2010).\nIn many applied settings, the structure of the problem at hand, such as, for example, the spatial\narrangement of the pixels in an image, or the presence of variables corresponding to several levels\nof a given factor, induces relationships between dictionary elements. It is appealing to use this a\npriori knowledge about the problem directly to constrain the possible sparsity patterns. For instance,\nwhen the dictionary elements are partitioned into predefined groups corresponding to different types\nof features, one can enforce a similar block structure in the sparsity pattern\u2014that is, allow only\nthat either all elements of a group are part of the signal decomposition or that all are dismissed\nsimultaneously (see Yuan and Lin, 2006; Stojnic et al., 2009).\nThis example can be viewed as a particular instance of structured sparsity, which has been\nlately the focus of a large amount of research (Baraniuk et al., 2010; Zhao et al., 2009; Huang et al.,\n2009; Jacob et al., 2009; Jenatton et al., 2009; Micchelli et al., 2010). In this paper, we concentrate\non a specific form of structured sparsity, which we call hierarchical sparse coding: the dictionary\nelements are assumed to be embedded in a directed tree T , and the sparsity patterns are constrained\nto form a connected and rooted subtree of T (Donoho, 1997; Baraniuk, 1999; Baraniuk et al., 2002,\n2010; Zhao et al., 2009; Huang et al., 2009). This setting extends more generally to a forest of\ndirected trees.1\nIn fact, such a hierarchical structure arises in many applications. Wavelet decompositions lend\nthemselves well to this tree organization because of their multiscale structure, and benefit from it for\nimage compression and denoising (Shapiro, 1993; Crouse et al., 1998; Baraniuk, 1999; Baraniuk\net al., 2002, 2010; He and Carin, 2009; Zhao et al., 2009; Huang et al., 2009). In the same vein,\nedge filters of natural image patches can be represented in an arborescent fashion (Zoran and Weiss,\n2009). Imposing these sparsity patterns has further proven useful in the context of hierarchical\nvariable selection, for example, when applied to kernel methods (Bach, 2008), to log-linear models\nfor the selection of potential orders (Schmidt and Murphy, 2010), and to bioinformatics, to exploit\nthe tree structure of gene networks for multi-task regression (Kim and Xing, 2010). Hierarchies of\nlatent variables, typically used in neural networks and deep learning architectures (see Bengio, 2009,\nand references therein) have also emerged as a natural structure in several applications, notably to\nmodel text documents. In particular, in the context of topic models (Blei et al., 2003), a hierarchical\nmodel of latent variables based on Bayesian non-parametric methods has been proposed by Blei\net al. (2010) to model hierarchies of topics.\nTo perform hierarchical sparse coding, our work builds upon the approach of Zhao et al. (2009)\nwho first introduced a sparsity-inducing norm \u2126 leading to this type of tree-structured sparsity\npattern. We tackle the resulting nonsmooth convex optimization problem with proximal methods\n(e.g., Nesterov, 2007; Beck and Teboulle, 2009; Wright et al., 2009; Combettes and Pesquet, 2010)\nand we show in this paper that its key step, the computation of the proximal operator, can be\nsolved exactly with a complexity linear, or close to linear, in the number of dictionary elements\u2014\nthat is, with the same complexity as for classical \u21131-sparse decomposition problems (Tibshirani,\n1996; Chen et al., 1998). Concretely, given an m-dimensional signal x along with a dictionary\nD = [d\n1\n,...,d\np\n] \u2208 R\nm\u00d7p\ncomposed of p atoms, the optimization problem at the core of our paper\ncan be written as\nmin\n\u03b1\u2208Rp\n1\n2\nkx\u2212D\u03b1k\n2\n2 +\u03bb\u2126(\u03b1), with \u03bb \u2265 0.\n1. A tree is defined as a connected graph that contains no cycle (see Ahuja et al., 1993).\n2298\nPROXIMAL METHODS FOR HIERARCHICAL SPARSE CODING\nIn this formulation, the sparsity-inducing norm \u2126 encodes a hierarchical structure among the atoms\nof D, where this structure is assumed to be known beforehand. The precise meaning of hierarchical\nstructure and the definition of \u2126 will be made more formal in the next sections. A particular instance\nof this problem\u2014known as the proximal problem\u2014is central to our analysis and concentrates on\nthe case where the dictionary D is orthogonal.\nIn addition to a speed benchmark that evaluates the performance of our proposed approach in\ncomparison with other convex optimization techniques, two types of applications and experiments\nare considered. First, we consider settings where the dictionary is fixed and given a priori, corre\u0002sponding for instance to a basis of wavelets for the denoising of natural images. Second, we show\nhow one can take advantage of this hierarchical sparse coding in the context of dictionary learn\u0002ing (Olshausen and Field, 1997; Aharon et al., 2006; Mairal et al., 2010a), where the dictionary is\nlearned to adapt to the predefined tree structure. This extension of dictionary learning is notably\nshown to share interesting connections with hierarchical probabilistic topic models.\nTo summarize, the contributions of this paper are threefold:\n\u2022 We show that the proximal operator for a tree-structured sparse regularization can be com\u0002puted exactly in a finite number of operations using a dual approach. Our approach is equiva\u0002lent to computing a particular sequence of elementary proximal operators, and has a complex\u0002ity linear, or close to linear, in the number of variables. Accelerated gradient methods (e.g.,\nNesterov, 2007; Beck and Teboulle, 2009; Combettes and Pesquet, 2010) can then be applied\nto solve large-scale tree-structured sparse decomposition problems at the same computational\ncost as traditional ones using the \u21131-norm.\n\u2022 We propose to use this regularization scheme to learn dictionaries embedded in a tree, which,\nto the best of our knowledge, has not been done before in the context of structured sparsity.\n\u2022 Our method establishes a bridge between hierarchical dictionary learning and hierarchical\ntopic models (Blei et al., 2010), which builds upon the interpretation of topic models as\nmultinomial PCA (Buntine, 2002), and can learn similar hierarchies of topics. This point\nis discussed in Sections 5.5 and 6.\nNote that this paper extends a shorter version published in the proceedings of the international\nconference of machine learning (Jenatton et al., 2010).\n1.1 Notation\nVectors are denoted by bold lower case letters and matrices by upper case ones. We define for q \u2265 1\nthe \u2113q-norm of a vector x in R\nm as kxkq\n\u25b3= (\u2211\nm\ni=1\n|xi|\nq\n)\n1/q\n, where xi denotes the i-th coordinate of x,\nand kxk\u221e\n\u25b3= maxi=1,...,m |xi\n| = limq\u2192\u221e kxkq. We also define the \u21130-pseudo-norm as the number of\nnonzero elements in a vector:2 ",
          "original_query": "Proximal methods for hierarchical sparse coding",
          "cleaned_query": "Proximal methods for hierarchical sparse coding",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Phase Diagram of Superposition Under Structured (Tree) Sparsity\n- Extend the Toy Models of Superposition by replacing i.i.d. sparse features with hierarchical feature families (tree-structured co-occurrence) and train small ReLU networks to map them into low-dimensional embeddings. Measure how the superposition phase boundary shifts as a function of tree depth, branching factor, and hierarchical sparsity penalties (e.g., Jenatton et al.\u2019s norms), producing a predictive theory of when networks prefer \u201cfeature splitting\u201d vs \u201cfeature bundling.\u201d",
        "Hierarchical Sparse Autoencoders for Transformer Activations (Tree-SAE)\n- Train SAEs on LM residual stream activations using a tree-structured sparsity regularizer so that activating a fine-grained latent requires activating its ancestors (topic \u2192 subtopic \u2192 token pattern). Evaluate whether this reduces Meta-SAE-style decomposability (i.e., yields \u201cmore atomic\u201d latents) while maintaining reconstruction fidelity and interpretability on standard feature audits.",
        "Matching-Pursuit SAE with Proximal Tree Constraints (MP-Tree-SAE)\n- Combine MP-SAE\u2019s sequential residual-guided encoder with an exact proximal operator for tree-structured norms to enforce hierarchical selection during each pursuit step. Test whether the resulting encoder better captures nonlinearly accessible and hierarchical concepts in multimodal (VLM) representations, and whether it improves causal localization on tasks like indirect object identification.",
        "Causal Feature Editing Using Meta-Latent Factorization Graphs\n- Use Meta-SAEs to build a directed \u201centailment graph\u201d over latents (e.g., Einstein \u2192 scientist, German, famous) by learning consistent meta-latent decompositions across seeds and SAE sizes. Develop an editing method that intervenes on a target latent while automatically compensating for entailed meta-latents, then quantify improvements in precision/side-effects on targeted knowledge edits and refusal/behavioral steering benchmarks.",
        "Cross-Layer Dictionary Learning to Track Feature \u201cBirth, Split, and Merge\u201d\n- Train coupled dictionaries across consecutive transformer layers with a temporal/transport regularizer so features can be aligned layer-to-layer (rather than learned independently). This enables explicit measurement of when a concept remains stable, splits into subfeatures, or merges\u2014connecting directly to the monosemantic-features \u201cphenomenology\u201d and providing a mechanistic account of feature evolution during depth.",
        "Superposition Geometry in Real Models via Polytope Fitting in SAE Latent Space\n- Test the toy-model claim that superposition organizes features into uniform polytope-like geometries by fitting spherical codes / polytope templates to learned SAE decoder directions in actual LMs. The contribution is a concrete geometric diagnostic that predicts polysemanticity and interference, and a method to detect \u201coverpacked\u201d subspaces where interpretability and causal editing are expected to fail.",
        "Adaptive Inference-Time Sparsity for Interpretability-Compute Tradeoffs\n- Exploit MP-SAE\u2019s sequential encoder to implement an anytime algorithm: stop pursuit early to get a coarse explanation, or continue for finer-grained decompositions. Benchmark how interpretability (feature clarity, causal localization) and runtime scale with the number of pursuit steps, yielding practical guidance for deploying interpretability tools on large models under compute constraints.",
        "Hierarchical Topic Dictionaries vs Probabilistic Topic Models on LM Features\n- Apply hierarchical sparse coding (Jenatton et al.) directly to the corpus of discovered LM features (from dictionary learning/SAEs) to induce a topic hierarchy over features, then compare against hierarchical Bayesian topic models in coherence and predictive utility. The key contribution is a bridge between mechanistic feature dictionaries and classical topic hierarchies, enabling structured navigation and retrieval of \u201cfeature taxonomies\u201d in large models."
      ],
      "generated_ideas_raw": "1. **Phase Diagram of Superposition Under Structured (Tree) Sparsity**\n   - Extend the Toy Models of Superposition by replacing i.i.d. sparse features with hierarchical feature families (tree-structured co-occurrence) and train small ReLU networks to map them into low-dimensional embeddings. Measure how the superposition phase boundary shifts as a function of tree depth, branching factor, and hierarchical sparsity penalties (e.g., Jenatton et al.\u2019s norms), producing a predictive theory of when networks prefer \u201cfeature splitting\u201d vs \u201cfeature bundling.\u201d\n\n2. **Hierarchical Sparse Autoencoders for Transformer Activations (Tree-SAE)**\n   - Train SAEs on LM residual stream activations using a tree-structured sparsity regularizer so that activating a fine-grained latent requires activating its ancestors (topic \u2192 subtopic \u2192 token pattern). Evaluate whether this reduces Meta-SAE-style decomposability (i.e., yields \u201cmore atomic\u201d latents) while maintaining reconstruction fidelity and interpretability on standard feature audits.\n\n3. **Matching-Pursuit SAE with Proximal Tree Constraints (MP-Tree-SAE)**\n   - Combine MP-SAE\u2019s sequential residual-guided encoder with an exact proximal operator for tree-structured norms to enforce hierarchical selection during each pursuit step. Test whether the resulting encoder better captures nonlinearly accessible and hierarchical concepts in multimodal (VLM) representations, and whether it improves causal localization on tasks like indirect object identification.\n\n4. **Causal Feature Editing Using Meta-Latent Factorization Graphs**\n   - Use Meta-SAEs to build a directed \u201centailment graph\u201d over latents (e.g., Einstein \u2192 scientist, German, famous) by learning consistent meta-latent decompositions across seeds and SAE sizes. Develop an editing method that intervenes on a target latent while automatically compensating for entailed meta-latents, then quantify improvements in precision/side-effects on targeted knowledge edits and refusal/behavioral steering benchmarks.\n\n5. **Approximate Orthogonality as a Control Knob for Monosemanticity**\n   - Systematically vary dictionary coherence during SAE training (explicit penalties on decoder mutual inner products / frame potential) to test the paper-1 claim that approximate orthogonality mitigates superposition-like interference. Produce empirical scaling laws linking coherence, sparsity, reconstruction error, and interpretability metrics (e.g., automated description quality, activation specificity, and causal intervention sharpness).\n\n6. **From V1 Sparse Coding Nonlinearities to Transformer Feature Interactions**\n   - Adapt analyses from overcomplete sparse coding in V1 (predicting weak nonlinear interaction patterns between basis elements) to SAE/feature space in transformers by measuring conditional feature suppression/facilitation under controlled input prompts. The contribution is a quantitative \u201cinteraction atlas\u201d predicting when activating one feature should nonlinearly gate another, and validating it via causal interventions in the model\u2019s forward pass.\n\n7. **Cross-Layer Dictionary Learning to Track Feature \u201cBirth, Split, and Merge\u201d**\n   - Train coupled dictionaries across consecutive transformer layers with a temporal/transport regularizer so features can be aligned layer-to-layer (rather than learned independently). This enables explicit measurement of when a concept remains stable, splits into subfeatures, or merges\u2014connecting directly to the monosemantic-features \u201cphenomenology\u201d and providing a mechanistic account of feature evolution during depth.\n\n8. **Superposition Geometry in Real Models via Polytope Fitting in SAE Latent Space**\n   - Test the toy-model claim that superposition organizes features into uniform polytope-like geometries by fitting spherical codes / polytope templates to learned SAE decoder directions in actual LMs. The contribution is a concrete geometric diagnostic that predicts polysemanticity and interference, and a method to detect \u201coverpacked\u201d subspaces where interpretability and causal editing are expected to fail.\n\n9. **Adaptive Inference-Time Sparsity for Interpretability-Compute Tradeoffs**\n   - Exploit MP-SAE\u2019s sequential encoder to implement an anytime algorithm: stop pursuit early to get a coarse explanation, or continue for finer-grained decompositions. Benchmark how interpretability (feature clarity, causal localization) and runtime scale with the number of pursuit steps, yielding practical guidance for deploying interpretability tools on large models under compute constraints.\n\n10. **Hierarchical Topic Dictionaries vs Probabilistic Topic Models on LM Features**\n   - Apply hierarchical sparse coding (Jenatton et al.) directly to the corpus of discovered LM features (from dictionary learning/SAEs) to induce a topic hierarchy over features, then compare against hierarchical Bayesian topic models in coherence and predictive utility. The key contribution is a bridge between mechanistic feature dictionaries and classical topic hierarchies, enabling structured navigation and retrieval of \u201cfeature taxonomies\u201d in large models.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Phase Diagram of Superposition Under Structured (Tree) Sparsity\n- Extend the Toy Models of Superposition by replacing i.i.d. sparse features with hierarchical feature families (tree-structured co-occu",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hierarchical Sparse Autoencoders for Transformer Activations (Tree-SAE)\n- Train SAEs on LM residual stream activations using a tree-structured sparsity regularizer so that activating a fine-grained la",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Matching-Pursuit SAE with Proximal Tree Constraints (MP-Tree-SAE)\n- Combine MP-SAE\u2019s sequential residual-guided encoder with an exact proximal operator for tree-structured norms to enforce hierarchica",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Causal Feature Editing Using Meta-Latent Factorization Graphs\n- Use Meta-SAEs to build a directed \u201centailment graph\u201d over latents (e.g., Einstein \u2192 scientist, German, famous) by learning consistent me",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Cross-Layer Dictionary Learning to Track Feature \u201cBirth, Split, and Merge\u201d\n- Train coupled dictionaries across consecutive transformer layers with a temporal/transport regularizer so features can be a",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Superposition Geometry in Real Models via Polytope Fitting in SAE Latent Space\n- Test the toy-model claim that superposition organizes features into uniform polytope-like geometries by fitting spheric",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Adaptive Inference-Time Sparsity for Interpretability-Compute Tradeoffs\n- Exploit MP-SAE\u2019s sequential encoder to implement an anytime algorithm: stop pursuit early to get a coarse explanation, or cont",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hierarchical Topic Dictionaries vs Probabilistic Topic Models on LM Features\n- Apply hierarchical sparse coding (Jenatton et al.) directly to the corpus of discovered LM features (from dictionary lear",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 23,
      "paper_title": "EvoLM: In Search of Lost Language Model Training Dynamics",
      "contribution": "EvoLM builds a transparent, end-to-end model suite and experimental pipeline (100+ 1B/4B decoder-only LMs trained from scratch on open data) to systematically trace training dynamics across pre-training, continued pre-training, supervised fine-tuning, and RL, revealing practical trade-offs (diminishing returns, forgetting, bridging roles of continued pre-training, and SFT/RL trade-offs) and releasing all models, data, and code for reproducible study.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12117,
      "output_tokens": 1045,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Revisiting Scaling Laws for Language Models: The Role of Data ...",
          "url": "https://aclanthology.org/2025.acl-long.1163.pdf",
          "content": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 23881\u201323899\nJuly 27 - August 1, 2025 \u00a92025 Association for Computational Linguistics\nRevisiting Scaling Laws for Language Models: The Role of Data Quality\nand Training Strategies\nZhengyu Chen1*, Siqi Wang 2*, Teng Xiao3, Yudong Wang 4,\nShiqi Chen 5, Xunliang Cai1, Junxian He 5, Jingang Wang1\n1 Meituan Inc. 2 The University of Hong Kong 3 Pennsylvania State University\n4 Peking University 5 The Hong Kong University of Science and Technology\n{chenzhengyu04,wangjingang02}@meituan.com\nAbstract\nTraditional scaling laws in natural language\nprocessing suggest that increasing model size\nand training data enhances performance. How\u0002ever, recent studies reveal deviations, partic\u0002ularly in large language models, where per\u0002formance improvements decelerate\u2014a phe\u0002nomenon known as sub-scaling. This paper\nrevisits these scaling laws by examining the\nimpact of data quality and training strategies\non model performance. Through extensive em\u0002pirical analysis of over 400 models, we iden\u0002tify high data density and non-optimal resource\nallocation as key factors contributing to sub\u0002scaling. High data density leads to diminishing\nreturns due to redundant information, while\noptimal resource allocation is crucial for sus\u0002tained performance improvements. We propose\na sub-optimal scaling law that better predicts\nperformance in sub-scaling regimes, highlight\u0002ing the importance of data quality and diversity.\n1 Introduction\nThe rapid advancement in natural language pro\u0002cessing (NLP) has been significantly driven by the\ndevelopment of increasingly large language mod\u0002els. These models, such as LLaMA (Touvron et al.,\n2023), Chinchilla (70B) (Hoffmann et al., 2022),\nGopher (280B) (Rae et al., 2021), and Megatron\u0002Turing NLG (530B) (Smith et al., 2022), have\nset new benchmarks across a variety of linguistic\ntasks. There is also a growing body of research on\nscaling strategies (McCandlish et al., 2018; Yang\net al., 2022, 2023; Wang et al., 2024), which could\nbe beneficial for large language models (LLMs).\nThe conventional wisdom suggests that augmenting\nmodel size and corresponding training data gener\u0002ally results in enhanced performance. This trend\nhas led to the popularization of a \u2019bigger is better\u2019\nparadigm within the field. This scaling up has been\n*Equal Contribution.\ndriven by the empirical observation that larger mod\u0002els trained on vast amounts of data tend to perform\nbetter on various natural language processing tasks\n(Komatsuzaki, 2019; Hernandez et al., 2022a).\nHowever, recent empirical studies (Hernandez\net al., 2022a; Hu et al., 2023; Porian et al., 2024;\nMuennighoff et al., 2024) have observed devia\u0002tions from this expected trend, particularly in the\ncontext of exceptionally large language models.\nThese deviations manifest as sub-scaling growth,\nwhere the rate of performance improvement de\u0002celerates as the model or dataset size continues to\nincrease. Specifically, (Hernandez et al., 2022a;\nMuennighoff et al., 2024) observe that sub-scaling\noccurs in scenarios involving repeated training data,\nleading to diminishing returns in performance. (Hu\net al., 2023) highlight that sub-scaling is particu\u0002larly pronounced in tasks requiring complex reason\u0002ing or multi-step processes. Furthermore, (Porian\net al., 2024) find that sub-scaling exists under non\u0002optimal training strategies with sub-optimal hyper\u0002parameters. Figure 1 provides a visualization of the\ndiminishing returns, clearly showing that as train\u0002ing progresses, the actual training loss values tend\nto be higher than those extrapolated from earlier\nstages, indicating how traditional scaling laws fall\nshort when dealing with extensive datasets and sug\u0002gests the need for a modified approach. Moreover,\n(Hernandez et al., 2022a; Muennighoff et al., 2024)\nhave similar sub-scaling observations in repeated\ndata and non-optimal training strategy. However,\nthere is a lack of systematic research on the sub\u0002scaling behavior of large language models (LLMs).\nFurther extending this observation to model per\u0002formance, Figure 2 displays the results of our tests\non the performance scaling law (Yang et al., 2024;\nIsik et al., 2024; Wu and Tang, 2024) with LLaMA\n2 and LLaMA 3 models. Despite LLaMA 3 incor\u0002porating advanced training strategies and improved\ndata quality, the performance improvements from\nLLaMA 2 to LLaMA 3 decelerate as the training\n23881\nFigure 1: Sub-scaling phenomenon in loss. Scaling law fits well with 5B training tokens, but as tokens increase,\nloss curve shows greater curvature, and fitting accuracy decreases, especially for larger models.\nFigure 2: (left) LLaMA 2\u2019s scaling curve outperforms LLaMA 3\u2019s, despite LLaMA 3\u2019s advanced strategies. (right)\nHigher-density datasets lead to sub-scaling. We propose metric density to measure redundancy and diversity: higher\ndensity indicates more redundancy and less diversity, leading to sub-scaling (see Section 2.1).\n10\n7 10\n8 109 1010\nModel Size\n2.5\n3.0\n3.5\n4.0\nLoss\n1e+18\n3e+18\n6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\nOptimal Model/Data Allocation\nScaling Law\n10\n6 10\n7 10\n8 109 1010 1011 1012\nModel Size\n2.0\n2.5\n3.0\n3.5\n4.0\nLoss\nLlama3 80B-1.5T\nLlama3 8B-15T\n1e+18\n3e+18\n6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\nScaling Law\nFigure 3: (left) With a fixed total compute budget, we adjust the model-to-data allocation ratio and plot the training\nloss against model size. A black curve connects the minimum points of each curve, illustrating the optimal\nChinchilla law. (right) However, current large language models, such as Llama3 8B, are trained on 15T tokens, with\na model-to-data allocation strategy that significantly deviates from the optimal Chinchilla law.\nflops increase, LLaMA 2 with 70B parameters out\u0002performs LLaMA 3 with 8B parameters.This dis\u0002crepancy, depicted in Figure 2, underscores the\ninadequacies of traditional scaling laws. Addition\u0002ally, when the scale of training data surpasses an\noptimal threshold relative to the available computa\u0002tional resources, sub-scaling law happens with such\nover-training (Gadre et al., 2024), potentially lead\u0002ing to diminishing returns in model performance.\nMoreover, there is a lack of understanding of the\ntraining dynamics of large language models and\nthe sub-scaling laws governing the training strate\u0002gies of language models. This motivates the ques\u0002tion: Under what conditions do sub-scaling laws\ninfluence the performance and efficiency of large\nlanguage models?\nThis study aims to systematically investigate the\nsub-scaling law phenomenon through an extensive\nempirical analysis involving over 400 models, rang\u0002ing from 20 million to 7 billion parameters, with\nvarying datasets and training strategies. Our find\u0002ings indicate that sub-scaling laws arise primarily\n23882\nfrom high data density and non-optimal training\nresource allocations. Specifically, we observed that\nboth factors contribute more significantly to per\u0002formance deceleration than previously anticipated.\nWe examine the sub-scaling phenomenon from two\nperspectives: data density and training strategy.\nHigh data density leads to diminishing marginal\ngains in performance as shown in Figure 2, while\noptimal resource allocation is crucial for sustain\u0002ing performance improvements as shown in Figure\n3. Further, we propose a sub-optimal scaling law\nthat generalizes the Chinchilla scaling law (Hoff\u0002mann et al., 2022) to better predict performance\nand loss in sub-scaling regimes. Our analysis re\u0002veals that the quality and diversity of training data\nare paramount, often outweighing the benefits of\nmere scale in model size. Key findings from our\nstudy include:\n1. Sub-Scaling Law Phenomenon: Traditional\nscaling laws fail to predict performance improve\u0002ments for large models and datasets. The per\u0002formance gains decelerate, leading to sub-scaling\ngrowth, especially in high data density scenarios\nand with non-optimal resource allocation.\n2. Training Strategies under Over-Training:\nCompared to Gadre et al. (2024), our study goes be\u0002yond identifying optimal hyper-parameters. Based\non the concept of Over-Training Ratio (OTR) to\nbalance model size and training data volume, we\nprovide a critical framework for optimal resource\nallocation, maximizing performance and efficiency.\n3. New Density Computation Method: We in\u0002troduce a new density computation method that\nconsiders both intra-cluster concentration and inter\u0002cluster separation. Unlike Muennighoff et al.\n(2024) that primarily focused on general data analy\u0002sis, this dual consideration offers a more insightful\nunderstanding of data quality and its influence.\n4. Sub-Optimal Scaling Law: Our introduction\nof a sub-optimal scaling law addresses the limi\u0002tations of traditional scaling laws, particularly in\nscenarios involving high-density datasets with re\u0002dundant information. We validate our proposed\nsub-optimal scaling law across various conditions.\n2 Analysis\nIn this section, we investigate the phenomenon\nof sub-scaling law from two perspectives: data\nand training strategy. Our analysis is based on ex\u0002tensive empirical evaluations involving over 400\nmodels, ranging from 20 million to 7 billion pa\u00025000 10000 15000 20000\nSteps\n3.2\n3.6\n4.0\nValidation Loss\n100M\nLow density\nHigh density\n2000 4000 6000 8000\nSteps\n3.0\n3.3\n3.6\nValidation Loss\n800M\nLow density\nHigh density\nFigure 4: We compare models with (left) 100M param\u0002eters and (right) 800M parameters trained on high and\nlow density dataset, which demonstrates that higher den\u0002sity results in a degressive performance increase.\nrameters, in different density datasets, and with\ndifferent model / data allocation and the selection\nof hyper-parameters, to investigate the sub-scaling\nlaw phenomenon. The architecture details of the\nused models are listed in Appendix Table 3.\n2.1 Analysis 1: The Perspective of Data\nPrevious works (Abbas et al., 2024; Sachdeva et al.,\n2024; Sorscher et al., 2022) have used data density\nto measure the quality and diversity of datasets. By\nfocusing on ",
          "original_query": "Scaling laws for neural language models",
          "cleaned_query": "Scaling laws for neural language models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Training Compute-Optimal Large Language Models - NeurIPS",
          "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf",
          "content": "Training Compute-Optimal Large Language Models\nJordan Hoffmann\u2217 Sebastian Borgeaud\u2217 Arthur Mensch\u2217 Elena Buchatskaya\nTrevor Cai Eliza Rutherford Diego de Las Casas Lisa Anne Hendricks\nJohannes Welbl Aidan Clark Tom Hennigan Eric Noland Katie Millican\nGeorge van den Driessche Bogdan Damoc Aurelia Guy Simon Osindero\nKaren Simonyan Erich Elsen Oriol Vinyals Jack W. Rae Laurent Sifre\u2217\n\u2217 Equal contributions\nDeepMind\n(sborgeaud|amensch|sifre)@deepmind.com\nAbstract\nWe investigate the optimal model size and number of tokens for training a Trans\u0002former language model under a given compute budget. We find that current large\nlanguage models are significantly undertrained, a consequence of the recent focus\non scaling language models whilst keeping the amount of training data constant.\nBy training over 400 language models ranging from 70 million to over 16 billion\nparameters on 5 to 500 billion tokens, we find that for compute-optimal training, the\nmodel size and the number of training tokens should be scaled equally: for every\ndoubling of model size the number of training tokens should also be doubled. We\ntest this hypothesis by training a predicted compute-optimal model, Chinchilla, that\nuses the same compute budget as Gopher but with 70B parameters and 4\u00d7 more\nmore data. Chinchilla uniformly and significantly outperforms Gopher (280B),\nGPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large\nrange of downstream evaluation tasks. This also means that Chinchilla uses substan\u0002tially less compute for fine-tuning and inference, greatly facilitating downstream\nusage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of\n67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.\n1 Introduction\nA series of Large Language Models (LLMs) have recently been introduced [6, 30, 38, 48, 52], with the\nlargest dense language models now having over 500 billion parameters. These large autoregressive\ntransformers [53] have demonstrated impressive performance on many tasks using a variety of\nevaluation protocols: zero-shot generalization, few-shot training, and as a basis for fine-tuning. The\ncompute and energy cost for training large language models is substantial [38, 52] and rises with\nincreasing model size. In practice, the allocated training compute budget is often known in advance:\npractitioners have access to a certain number of accelerators for a given period of time. Since it\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n10\n17 10\n19 1021 1023 1025\nFLOPs\n10M\n100M\n1.0B\n10B\n100B\n1T\nParameters\nApproach 1\nApproach 2\nApproach 3\nKaplan et al (2020)\nChinchilla (70B)\nGopher (280B)\nGPT-3 (175B)\nMegatron-Turing NLG (530B)\nFigure 1: Overlaid predictions. We overlay the predictions from our three different approaches, along\nwith projections from [23]. We find that all three methods predict that current large models should be\nsubstantially smaller and therefore trained much longer than is currently done. In Figure A3, we show\nthe results with the predicted optimal tokens plotted against the optimal number of parameters for\nfixed FLOP budgets. Chinchilla outperforms Gopher and the other large models (see Section 4.2).\nTable 1: Current LLMs. We show five of the current largest dense transformer models, their size, and\nthe number of training tokens. Other than LaMDA [52], most models are trained for approximately\n300 billion tokens. We introduce Chinchilla, a substantially smaller model, trained for much longer\nthan 300B tokens. Table A3 shows our projected optimal relation between model size and tokens.\nModel Size (# Parameters) Training Tokens\nLaMDA [52] 137 Billion 768 Billion\nGPT-3 [6] 175 Billion 300 Billion\nJurassic [30] 178 Billion 300 Billion\nGopher [38] 280 Billion 300 Billion\nMT-NLG 530B [48] 530 Billion 270 Billion\nChinchilla 70 Billion 1.4 Trillion\nis typically only feasible to train these large models once, accurately estimating the best model\nhyperparameters for a given compute budget is critical [51].\nKaplan et al. [23] showed that there is a power law relationship between the number of parameters\nin an autoregressive language model (LM) and its performance (measured in evaluation perplexity).\nOne notable conclusion in [23] is that large models should not be trained to their lowest possible loss\nto be compute optimal; they argue that model size should grow faster than the size of the training\nset for a given increase of computational budget. As a result, the field has been training larger and\nlarger models while keeping the size of the training set to approximately 300 billion tokens, expecting\nperformance improvements (Table 1). While we find that there is effectively a trade-off between\nmodel size and training set size, we estimate that large models should be trained for many more\ntraining tokens than recommended by [23]. Specifically, given a 10\u00d7 increase computational budget\nwe find that model size and the number of training tokens should be scaled in equal proportions.\nIn this work, we revisit the question: Given a fixed FLOPs budget,1 how should one trade-off model\nsize and the number of training tokens? To answer this question, we model the final pre-training\nloss2 L(N, D) as a function of the number of model parameters N, and the number of training\ntokens, D. Since the computational budget C is a deterministic function FLOPs(N, D) of the number\nof seen training tokens and model parameters, we are interested in minimizing L under the constraint\n1\nFor example, knowing the number of accelerators and a target training duration.\n2\nFor simplicity, we perform our analysis on the smoothed training loss which is an unbiased estimate of the\ntest loss, as the number of training tokens is less than the number of tokens in the entire corpus.\n2\nFLOPs(N, D) = C:\nNopt(C), Dopt(C) = argmin\nN,D s.t. FLOPs(N,D)=C\nL(N, D). (1)\nThe functions Nopt(C), and Dopt(C) describe the optimal allocation of a computational budget C.\nWe empirically estimate these functions based on the losses of over 400 models, ranging from under\n70M to over 16B parameters, and trained on 5B to over 400B tokens \u2013 with each model configuration\ntrained for several different training horizons. Our approach leads to considerably different results\nthan that of [23]. We highlight our results in Figure 1 and how our approaches differ in Section 2.\nBased on our estimated compute-optimal frontier, we predict that for the compute budget used to\ntrain Gopher, an optimal model should be 4 times smaller, while being training on 4 times more\ntokens. We verify this by training a more compute-optimal 70B model, called Chinchilla, on 1.4\ntrillion tokens. Not only does Chinchilla outperform its much larger counterpart, Gopher, but its\nreduced model size reduces inference cost considerably and greatly facilitates downstream uses on\nsmaller hardware. The energy cost of a large language model is amortized through its usage for\ninference and fine-tuning. The benefits of a more optimally trained smaller model, therefore, extend\nbeyond the immediate benefits of its improved performance.\n2 Related Work\nLarge language models. A variety of large language models have been introduced in the last few\nyears. These include both dense transformer models [6, 30, 48, 38, 52] and mixture-of-expert (MoE)\nmodels [11, 12, 60]. The largest dense transformers have passed 500 billion parameters [48, 8]. The\ndrive to train larger and larger models is clear\u2014so far increasing the size of language models has been\nresponsible for improving the state-of-the-art in many language modelling tasks. Nonetheless, large\nlanguage models face several challenges, including their overwhelming computational requirements\n(the cost of training and inference increase with model size) [38, 52] and the need for acquiring more\nhigh-quality training data. In fact, in this work we find that larger, high quality datasets will play a key\nrole in any further scaling of language models. Concurrent to our work, a 540 billion parameter model\ntrained on 768 billion tokens was released\u2013 PaLM [8]. While this model outperforms Chinchilla, it\nuses approximately 5\u00d7 the compute and is nearly 8\u00d7 larger, making it more difficult to use.\nModelling the scaling behavior. Understanding the scaling behaviour of language models and their\ntransfer properties has been important in the development of recent large models [23, 18]. Kaplan\net al. [23] first showed a predictable relationship between model size and loss over many orders of\nmagnitude. The authors investigate the question of choosing the optimal model size to train for a\ngiven compute budget. Similar to us, they address this question by training various models. Our\nwork differs from Kaplan et al. [23] in several important ways. First, the authors use a fixed number\nof training tokens and learning rate schedule for all models; this prevents them from modelling the\nimpact of these hyperparameters on the loss. In contrast, we find that setting the learning rate schedule\nto approximately match the number of training tokens results in the best final loss regardless of model\nsize\u2014see Figure A1. For a fixed learning rate cosine schedule to 130B tokens, the intermediate loss\nestimates (for D\u2032 << 130B) are therefore overestimates of the loss of a model trained with a schedule\nlength matching D\u2032. Using these intermediate losses results in underestimating the effectiveness\nof training models on less data than 130B tokens, and eventually contributes to the conclusion that\nmodel size should increase faster than training data size as compute budget increases. In contrast, our\nanalysis predicts that both quantities should scale at roughly the same rate. Secondly, we include\nmodels with up to 16B parameters, as we observe that there is slight curvature in the FLOP-loss\nfrontier (see Appendix E)\u2014in fact, the majority of the models used in our analysis have more than\n500 million parameters, in contrast the majority of runs in [23] are significa",
          "original_query": "Training compute-optimal large language models",
          "cleaned_query": "Training compute-optimal large language models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Training Trajectories of Language Models Across Scales - ACL ...",
          "url": "https://aclanthology.org/2023.acl-long.767/",
          "content": "Abstract Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)\u2014from 125M to 175B parameters\u2014on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation. Anthology ID: 2023.acl-long.767 Volume: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) Month: July Year: 2023 Address: Toronto, Canada Editors: Anna Rogers,\n Jordan Boyd-Graber,\n Naoaki Okazaki Venue: ACL SIG: Publisher: Association for Computational Linguistics Note: Pages: 13711\u201313738 Language: URL: https://aclanthology.org/2023.acl-long.767/ DOI: 10.18653/v1/2023.acl-long.767 Bibkey: Cite (ACL): Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Veselin Stoyanov. 2023. Training Trajectories of Language Models Across Scales. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13711\u201313738, Toronto, Canada. Association for Computational Linguistics. Cite (Informal): Training Trajectories of Language Models Across Scales (Xia et al., ACL 2023) Copy Citation: \n PDF: https://aclanthology.org/2023.acl-long.767.pdf Video: \u00a0https://aclanthology.org/2023.acl-long.767.mp4",
          "original_query": "Training trajectories of language models across scales",
          "cleaned_query": "Training trajectories of language models across scales",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Language models scale reliably with over-training and on ... - arXiv",
          "url": "https://arxiv.org/html/2403.08540",
          "content": "Language models scale reliably with over-training and on downstream tasks\n# Language models scale reliably with over-training and on downstream tasks\nSamir Yitzhak Gadre1,2Georgios Smyrnis3Vaishaal Shankar4\nSuchin Gururangan5Mitchell Wortsman5Rulin Shao5Jean Mercat2\nAlex Fang5Jeffrey Li5Sedrick Keh2Rui Xin5Marianna Nezhurina6,7\nIgor Vasiljevic2Jenia Jitsev6,7Luca Soldaini8Alexandros G. Dimakis3\nGabriel Ilharco5Pang Wei Koh5,8Shuran Song9Thomas Kollar2\nYair Carmon10\u2217Achal Dave2\u2217Reinhard Heckel11\u2217Niklas Muennighoff12\u2217Ludwig Schmidt5\u2217\n###### Abstract\nScaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments.\nHowever, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated.\nFor instance, scaling is usually studied in the compute-optimal training regime (i.e., \u201cChinchilla optimal\u201d regime).\nIn contrast, models are often over-trained to reduce inference costs.\nMoreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance.\nTo address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions.\nFirst, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters.\nThis enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32\u00d7\\\\times\u00d7over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)\u2014each from experiments that take 300\u00d7\\\\times\u00d7less compute.\nSecond, we relate the perplexity of a language model to its downstream task performance by proposing a power law.\nWe use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20\u00d7\\\\times\u00d7less compute.\nOur experiments are available at[https://github.com/mlfoundations/scaling](https://github.com/mlfoundations/scaling).\n00footnotetext:Equal advising, ordered alphabetically. Correspondence to[sy@cs.columbia.edu](sy@cs.columbia.edu).1Columbia\u00a0University2Toyota\u00a0Research\u00a0Insitute3UT\u00a0Austin4Apple5University\u00a0of\u00a0Washington6Juelich\u00a0Supercomputing\u00a0Center, Research\u00a0Center\u00a0Juelich7LAION8Allen\u00a0Institute\u00a0for\u00a0AI9Stanford\u00a0University10Tel\u00a0Aviv\u00a0University11TU\u00a0Munich12Contextual\u00a0AI\n## 1Introduction\n![Refer to caption](x1.png)Figure 1:Reliable scaling with over-training and on downstream error prediction.*(left)*We fit a scaling law for model validation loss, parameterized by (i) a token multiplierM=N/D\ud835\udc40\ud835\udc41\ud835\udc37M=N/Ditalic\\_M = italic\\_N / italic\\_D, which is the ratio of training tokensD\ud835\udc37Ditalic\\_Dto parametersN\ud835\udc41Nitalic\\_Nand (ii) the computeC\ud835\udc36Citalic\\_Cin FLOPs used to train a model, approximated byC=6\u2062N\u2062D\ud835\udc366\ud835\udc41\ud835\udc37C=6NDitalic\\_C = 6 italic\\_N italic\\_D.\nLarger values ofM\ud835\udc40Mitalic\\_Mspecify more over-training.\nWe are able to extrapolate, in bothN\ud835\udc41Nitalic\\_NandM\ud835\udc40Mitalic\\_M, the validation performance of models requiring more than300\u00d7300\\\\times300 \u00d7the training compute used to construct the scaling law.*(right)*We also fit a scaling law to predict average downstream top-1 error as a function of validation loss.\nWe find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction.\nWe predict the average error over 17 downstream tasks for models trained with over 20\u00d7\\\\times\u00d7the compute.\nFor this figure, we train all models on RedPajama> [\n[> 112\n](https://arxiv.org/html/2403.08540v2#bib.bib112)> ]\n.\nTraining large language models is expensive.\nFurthermore, training high-quality models requires a complex recipe of algorithmic techniques and training data.\nTo reduce the cost of finding successful training recipes, researchers first evaluate ideas with small experiments and then extrapolate their efficacy to larger model and data regimes via scaling laws.\nWith reliable extrapolation, it is possible to quickly iterate at small scale and still pick the method that will perform best for the final large training run.\nIndeed, this workflow has become commonplace for training state-of-the-art language models like Chinchilla\u00a070B> [\n[> 45\n](https://arxiv.org/html/2403.08540v2#bib.bib45)> ]\n, PaLM\u00a0540B> [\n[> 19\n](https://arxiv.org/html/2403.08540v2#bib.bib19)> ]\n, GPT-4> [\n[> 76\n](https://arxiv.org/html/2403.08540v2#bib.bib76)> ]\n, and many others.\nDespite their importance for model development, published scaling laws differ from the goals of training state-of-the-art models in important ways.\nFor instance, scaling studies usually focus on the compute-optimal training regime (\u201cChinchilla optimality\u201d> [\n[> 45\n](https://arxiv.org/html/2403.08540v2#bib.bib45)> ]\n), where model and dataset size are set to yield minimum loss for a given compute budget. However, this setting ignores inference costs. As larger models are more expensive at inference, it is now common practice to over-train smaller models> [\n[> 113\n](https://arxiv.org/html/2403.08540v2#bib.bib113)> ]\n. Another potential mismatch is that most scaling laws quantify model performance by perplexity in next-token prediction instead of accuracy on widely used benchmark datasets. However, practitioners usually turn to benchmark performance, not loss, to compare models.\nIn this paper, we conduct an extensive set of experiments to address both scaling in the over-trained regime and benchmark performance prediction.\nMotivated by the practice of training beyond compute-optimality, we first investigate whether scaling follows reliable trends in the over-trained regime.\nWe notice, as implied by> Hoffmann et\u00a0al. [\n[> 45\n](https://arxiv.org/html/2403.08540v2#bib.bib45)> ]\n, for a set of models of different sizes trained with a constant ratio of tokens to parameters, models\u2019 reducible lossL\u2032superscript\ud835\udc3f\u2032L^{\\\\prime}italic\\_L start\\_POSTSUPERSCRIPT \u2032end\\_POSTSUPERSCRIPT> [\n[> 43\n](https://arxiv.org/html/2403.08540v2#bib.bib43)> , [> 45\n](https://arxiv.org/html/2403.08540v2#bib.bib45)> ]\nfollows a power law (L\u2032=\u03bb\u22c5C\u2212\u03b7superscript\ud835\udc3f\u2032\u22c5\ud835\udf06superscript\ud835\udc36\ud835\udf02L^{\\\\prime}=\\\\lambda\\\\cdot C^{-\\\\eta}italic\\_L start\\_POSTSUPERSCRIPT \u2032end\\_POSTSUPERSCRIPT = italic\\_\u03bb \u22c5italic\\_C start\\_POSTSUPERSCRIPT - italic\\_\u03b7 end\\_POSTSUPERSCRIPT) in the amount of training computeC\ud835\udc36Citalic\\_C.\nWe find that as one increases the ratio of tokens to parameters, corresponding to more over-training, the scaling exponent\u03b7\ud835\udf02\\\\etaitalic\\_\u03b7remains about the same, while the scalar\u03bb\ud835\udf06\\\\lambdaitalic\\_\u03bbchanges. We explain our observations by reparameterizing existing scaling laws in relation to the amount of over-training.\nTo establish empirically that scaling*extrapolates*in the over-trained regime, we further experiment with a testbed of 104 models, trained from scratch on three different datasets: C4> [\n[> 88\n](https://arxiv.org/html/2403.08540v2#bib.bib88)> , [> 27\n](https://arxiv.org/html/2403.08540v2#bib.bib27)> ]\n, RedPajama> [\n[> 112\n](https://arxiv.org/html/2403.08540v2#bib.bib112)> ]\n, and RefinedWeb> [\n[> 82\n](https://arxiv.org/html/2403.08540v2#bib.bib82)> ]\n.\nWe find that scaling laws fit to small models can accurately predict the performance of larger models that undergo more over-training.\nFigure[1](https://arxiv.org/html/2403.08540v2#S1.F1)*(left)*illustrates our main over-training result, where we invest2.4\u2062e\u2062192.4\ud835\udc52192.4e192.4 italic\\_e 19FLOPs to extrapolate the C4 validation performance of a 1.4B parameter model trained on 900B tokens, which requires300\u00d7300\\\\times300 \u00d7more compute to train.\nIn addition to over-training, we also investigate if scaling laws can predict the performance of a model on downstream tasks.\nWe establish a power law relationship between language modeling perplexity and the average top-1 error on a suite of downstream tasks.\nWhile it can be difficult to predict the error on individual tasks, we find it possible to predict aggregate performance from a model\u2019s perplexity among models trained on the same training data.\nFigure[1](https://arxiv.org/html/2403.08540v2#S1.F1)*(right)*presents our main downstream error prediction result, where we invest2.7\u2062e\u2062202.7\ud835\udc52202.7e202.7 italic\\_e 20FLOPs to predict the average top-1 error over a set of downstream tasks to within 1 percentage point for a 6.9B compute-optimal model, which requires20\u00d720\\\\times20 \u00d7more compute to train.\nOur results suggest that the proposed scaling laws are promising to derisk (i) the effects of over-training models and (ii) the downstream performance of scaling up training recipes.\nTo facilitate further research on reliable scaling, we provide all results of our experiments at[https://github.com/mlfoundations/scaling](https://github.com/mlfoundations/scaling).\n## 2Developing scaling laws for over-training and downstream tasks\nIn this section, we develop scaling laws to predict over-trained and downstream performance.\nFirst, we provide key definitions (Section[2.1](https://arxiv.org/html/2403.08540v2#S2.SS1)).\nWe next present a scaling law for over-training drawing on empirical observation and prior work (Section[2.2](https://arxiv.org/html/2403.08540v2#S2.SS2)).\nTo connect loss scaling and downstream error prediction, we observe that average top-1 error decreases exponentially as a function of validation loss, which we formalize as a novel scaling law (Section[2.3](https://arxiv.org/html/2403.08540v2#S2.SS3)).\nIn later sections, we build an experimental setup (Section[3](https://arxiv.org/html/2403.08540v2#S3)) to quantify the extent to which our scaling laws extrapolate reliably (Section[4](https://arxiv.org/html/2403.08540v2#S4)).\n### 2.1Preliminaries\n#### Scaling laws for loss.\nTypically, scaling laws predict model lossL\ud835\udc3fLitalic\\_Las a function of the computeC\ud835\udc36Citalic\\_Cin FLOPs used for training.\nIf one increases the number of parametersN\ud835\udc41Nitalic\\_Nin a model or the number of tokensD\ud835\udc37Ditalic\\_Dthat a model is trained on, compute requirements naturally increase.\n",
          "original_query": "Language models scale reliably with over-training and on downstream tasks",
          "cleaned_query": "Language models scale reliably with over-training and on downstream tasks",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "Decanting the Web for the Finest Text Data at Scale",
          "url": "https://arxiv.org/html/2406.17557",
          "content": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale\n# The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale\nGuilherme Penedo \u2003Hynek Kydl\u00ed\u010dek \u2003Loubna Ben allal \u2003Anton Lozhkov\nMargaret Mitchell \u2003Colin Raffel \u2003Leandro Von Werra \u2003Thomas Wolf\n![[Uncaptioned image]](x1.png)Hugging Face\n###### Abstract\nThe performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.\n[https://huggingface.co/datasets/HuggingFaceFW/fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)[https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n## 1Introduction\nLarge Language Models (LLMs) have quickly become a ubiquitous technology thanks to their ability to competently perform a wide range of text-based tasks.\nA driving factor in the success of LLMs has been a steady increase in model sizes> [\n[> 1\n](https://arxiv.org/html/2406.17557v2#bib.bib1)> , [> 2\n](https://arxiv.org/html/2406.17557v2#bib.bib2)> , [> 3\n](https://arxiv.org/html/2406.17557v2#bib.bib3)> ]\n, which in turn necessitate ever-larger pretraining datasets.\nBeyond scale, other characteristics of pretraining data have proven to be important, including filtering out \u201clow-quality\u201d content> [\n[> 2\n](https://arxiv.org/html/2406.17557v2#bib.bib2)> , [> 4\n](https://arxiv.org/html/2406.17557v2#bib.bib4)> ]\nand removing duplicate text> [\n[> 5\n](https://arxiv.org/html/2406.17557v2#bib.bib5)> ]\n.\nUltimately, the curation choices made when developing a pretraining dataset can have a huge impact on the downstream capabilities and performance of an LLM.\nAs such, pretraining dataset curation strategies are often treated as closely guarded trade secrets.\nIn fact, there are many popular \u201copen\u201d language models whose parameters are publicly available but whose pretraining datasets were not released and are scarcely documented> [\n[> 6\n](https://arxiv.org/html/2406.17557v2#bib.bib6)> , [> 7\n](https://arxiv.org/html/2406.17557v2#bib.bib7)> ]\n.\nThe lack of access to high-quality large-scale pretraining datasets and lack of information about their curation has led to concerns of a growing gap between proprietary and public knowledge.\nIn this work, we aim to minimize this gap by developing and releasing the FineWeb datasets, a collection of large-scale pretraining datasets that can be used to train performant LLMs.\nSpecifically, we first introduce FineWeb, a 15-trillion token dataset of text sourced from 96 Common Crawl snapshots.\nFineWeb is sufficiently large to train a Chinchilla-optimal model> [\n[> 1\n](https://arxiv.org/html/2406.17557v2#bib.bib1)> ]\nwith more than 500 billion parameters.\nBeyond scale, FineWeb\u2019s recipe involves a principled strategy for choosing and tuning filtering heuristics that helped produce a small set of effective filters out of over fifty candidate filters from past work.\nIn addition, we performed an in-depth exploration of how different deduplication strategies and granularities can impact performance.\nTo validate our design choices, we ultimately demonstrate that models trained on FineWeb perform better than those trained on other public web-based pre-training datasets.\nInspired by recent work advocating for training LLMs on educational data> [\n[> 8\n](https://arxiv.org/html/2406.17557v2#bib.bib8)> , [> 9\n](https://arxiv.org/html/2406.17557v2#bib.bib9)> ]\n, we additionally introduce FineWeb-Edu, a subset of 1.3 trillion tokens from FineWeb that was rated as highly educational by a custom classifier.\nModels trained on FineWeb-Edu exhibit significantly better performance on knowledge- and reasoning-intensive benchmarks like MMLU> [\n[> 10\n](https://arxiv.org/html/2406.17557v2#bib.bib10)> ]\nand ARC> [\n[> 11\n](https://arxiv.org/html/2406.17557v2#bib.bib11)> ]\n. Both datasets are released under the permissive[ODC-By License](https://opendatacommons.org/licenses/by/1-0/).\nApart from contributing datasets, we also releasedatatrove> [\n[> 12\n](https://arxiv.org/html/2406.17557v2#bib.bib12)> ]\n, the data processing library we developed to create FineWeb.\nOn the whole, our work represents a significant step towards improving public knowledge and resources for curating LLM pre-training datasets.\n## 2Background\nIn this work, we focus on the curation of training datasets for autoregressive Transformer-based large language models (LLMs)> [\n[> 13\n](https://arxiv.org/html/2406.17557v2#bib.bib13)> ]\n.\nAt their core, LLMs aim to produce a distribution over the next token of text conditioned on past tokens, where each token is typically a word or subword unit> [\n[> 3\n](https://arxiv.org/html/2406.17557v2#bib.bib3)> ]\n.\nThe generality of this paradigm allows LLMs to be applied to virtually any text-based task by formulating a prefix whose continuation corresponds to performing the task (e.g.\u00a0\u201cThe cat sat on the mat translated to French is\u2026\u201d for English-to-French translation).\nSuch models may undergo many stages of training including pretraining on unstructured text data, fine-tuning to improve performance on a specific task> [\n[> 14\n](https://arxiv.org/html/2406.17557v2#bib.bib14)> ]\n, multitask fine-tuning to improve generalization to new tasks> [\n[> 15\n](https://arxiv.org/html/2406.17557v2#bib.bib15)> ]\n, and learning from human feedback to improve instruction-following capabilities> [\n[> 16\n](https://arxiv.org/html/2406.17557v2#bib.bib16)> , [> 2\n](https://arxiv.org/html/2406.17557v2#bib.bib2)> ]\n.\nIn this work, we focus solely on curating data for the pretraining stage.\nWhile many sources have been considered for pretraining data including text from books> [\n[> 2\n](https://arxiv.org/html/2406.17557v2#bib.bib2)> , [> 3\n](https://arxiv.org/html/2406.17557v2#bib.bib3)> , [> 17\n](https://arxiv.org/html/2406.17557v2#bib.bib17)> , [> 4\n](https://arxiv.org/html/2406.17557v2#bib.bib4)> ]\n, Wikipedia> [\n[> 2\n](https://arxiv.org/html/2406.17557v2#bib.bib2)> , [> 3\n](https://arxiv.org/html/2406.17557v2#bib.bib3)> , [> 17\n](https://arxiv.org/html/2406.17557v2#bib.bib17)> , [> 4\n](https://arxiv.org/html/2406.17557v2#bib.bib4)> ]\n, and research papers> [\n[> 2\n](https://arxiv.org/html/2406.17557v2#bib.bib2)> , [> 4\n](https://arxiv.org/html/2406.17557v2#bib.bib4)> , [> 18\n](https://arxiv.org/html/2406.17557v2#bib.bib18)> ]\n, a highly common choice is to use web text, i.e.\u00a0text scraped from webpages on the public internet> [\n[> 19\n](https://arxiv.org/html/2406.17557v2#bib.bib19)> , [> 20\n](https://arxiv.org/html/2406.17557v2#bib.bib20)> ]\n.\nWhile some companies like OpenAI> [\n[> 21\n](https://arxiv.org/html/2406.17557v2#bib.bib21)> ]\nand Anthropic> [\n[> 22\n](https://arxiv.org/html/2406.17557v2#bib.bib22)> ]\nperform their own web scrapes, designing, implementing, and running a web scraper at scale requires significant resources and expertise.\nMany LLM pretraining datasets have therefore been constructed from text from the Common Crawl> [\n[> 23\n](https://arxiv.org/html/2406.17557v2#bib.bib23)> ]\n, a publicly available and continually updated collection of website snapshots that has been running since 2007.\nAs of writing, Common Crawl has produced 100 web snapshots totaling petabytes of data.\nAlthough Common Crawl has produced more than enough data to train recent LLMs, it has been shown that the performance of an LLM can heavily depend on how web text has been filtered and preprocessed before being used for pretraining> [\n[> 19\n](https://arxiv.org/html/2406.17557v2#bib.bib19)> ]\n.\nIn particular, web text can contain a large amount of \u201cunnatural\u201d language (e.g. \u201cboilerplate\u201d text, gibberish, etc.).\nTraining on unnatural language data can harm the performance of LLMs, possibly because most downstream uses of LLMs do not involve such data.\nOn the other hand, filtering outtoomuch content can produce a dataset that is too small to perform sufficient pretraining (which typically involves only one pass or a few passes over the pretraining dataset> [\n[> 24\n](https://arxiv.org/html/2406.17557v2#bib.bib24)> ]\n) for a general use model.\nSeparately, web text can contain a large amount of duplicated content, which has also been shown to be harmful in the context of pretraining data> [\n[> 5\n](https://arxiv.org/html/2406.17557v2#bib.bib5)> ]\n.\nWhile deduplication may seem as straightforward as \u201cremoving duplicate text\u201d, in practice many design choices must be made (line, paragraph, or document-level deduplication? fuzzy or exact matching? etc.).\nThe curation and relative performance of different web text-based pretraining datasets therefore heavily depends on a given dataset\u2019s filtering and deduplication pipeline.\nGiven that the focus of our work is to carefully design an effective Common Crawl-based pretraining dataset, we now briefly discuss the filtering and deduplication used in past public datasets.OSCAR> [\n[> 25\n](https://arxiv.org/html/2406.17557v2#bib.bib25)> ]\nprocesses Common Crawl using a pipeline inspired by that of> Touvron et\u00a0al. [\n[> 2\n](https://arxiv.org/html/2406.17557v2#bib.bib2)> ]\n, whi",
          "original_query": "The fineweb datasets: Decanting the web for the finest text data at scale",
          "cleaned_query": "The fineweb datasets: Decanting the web for the finest text data at scale",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "LLaMA: Open and Efficient Foundation Language Models - AI at Meta",
          "url": "https://ai.meta.com/research/publications/llama-open-and-efficient-foundation-language-models/",
          "content": "LLaMA: Open and Efficient Foundation Language Models | Research - AI at Meta\n[![Meta](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/252294889_575082167077436_6034106545912333281_n.svg/meta-logo-primary_standardsize.svg?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=ioU79hw3_WoQ7kNvwFtViCz&amp;_nc_oc=Adna3UgLnx8B33Ryuk8Jukmy-T3-vne9u4ammJqwfyRyNqPEoMbcTcx1I7uy5xYvd_XP3trTeC0PsjbORxQrZEyV&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_Afp8kqmlDoEITMz2wDZ2FRXj9geGqAGoufGG0XHITXfvog&amp;oe=695FA7B9)](#)\n* [Meta AI](#)\n* [AI Research](#)\n* [The Latest](https://ai.meta.com/blog/)\n* [About](#)\n* [Get Llama](https://www.llama.com/?utm_source=ai_meta_site&amp;utm_medium=web&amp;utm_content=AI_nav&amp;utm_campaign=09252025_moment)\n* [Try Meta AI](https://www.meta.ai/?utm_source=ai_meta_site&amp;utm_medium=web&amp;utm_content=AI_nav&amp;utm_campaign=09252025_moment)\n* [](https://ai.meta.com/)\n#### NLP\n# LLaMA: Open and Efficient Foundation Language Models\nFebruary 24, 2023\n## Abstract\nWe introduce LLaMA, a collection of founda- tion language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla- 70B and PaLM-540B. We release all our models to the research community.\n[\nDownload the Paper\n](https://scontent-sea1-1.xx.fbcdn.net/v/t39.2365-6/333007794_1182140292435357_4481174526219500228_n.pdf?_nc_cat=101&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=qNF9NIpK4YAQ7kNvwGuOL3w&amp;_nc_oc=AdlR0FCoRZ2-1PDywp8IT4ga2EIwOwbLhey-vUbjqV0IxhTm6GYrUs3C_s3hFJa2okesYlTlrCfoX_pZcrLQ7h1o&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_Afoo2Fafvic6gMyW1_kmc9hM3J4Pr647lVmkJhkBu5_jmQ&amp;oe=695F94D1)\n#### AUTHORS\nWritten by\nFaisal Azhar\n[Hugo Touvron](https://ai.meta.com/people/810001457639122/hugo-touvron/)\nArmand Joulin\nAurelien Rodriguez\n[Baptiste Rozi\u00e8re](https://ai.meta.com/people/944324703766315/baptiste-roziere/)\nEric Hambro\nGautier Izacard\nGuillaume Lample\nMarie-Anne Lachaux\nNaman Goyal\nThibaut Lavril\nTimothee Lacroix\nXavier Martinet\nEdouard Grave\nPublisher\nArXiV\nResearch Topics\n[Natural Language Processing (NLP)](https://ai.meta.com/research/nlp/)\n### Related Publications\nDecember 26, 2025\n#### REINFORCEMENT LEARNING\n#### NLP\n#### Safety Alignment of LMs via Non-cooperative Games\nAnselm Paulus,Ilia Kulikov,Brandon Amos,Remi Munos,Ivan Evtimov,Kamalika Chaudhuri,[Arman Zharmagambetov](https://ai.meta.com/people/815310593758485/arman-zharmagambetov/)\nDecember 26, 2025\n[Read the Paper**](https://ai.meta.com/research/publications/safety-alignment-of-lms-via-non-cooperative-games/)\nDecember 18, 2025\n#### NLP\n#### How Good is Post-Hoc Watermarking With Language Model Rephrasing?\nPierre Fernandez,Tom Sander,Hady Elsahar,Hongyan Chang,Tom\u00e1\u0161 Sou\u010dek,Sylvestre Rebuffi,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko\nDecember 18, 2025\n[Read the Paper**](https://ai.meta.com/research/publications/how-good-is-post-hoc-watermarking-with-language-model-rephrasing/)\nDecember 12, 2025\n#### NLP\n#### COMPUTER VISION\n#### Text-Guided Semantic Image Encoder\nRaghuveer Thirukovalluru,Xiaochuang Han,Bhuwan Dhingra,[Emily Dinan](https://ai.meta.com/people/767581351981566/emily-dinan/),[Maha Elbayad](https://ai.meta.com/people/875226594354961/maha-elbayad/)\nDecember 12, 2025\n[Read the Paper**](https://ai.meta.com/research/publications/text-guided-semantic-image-encoder/)\nNovember 10, 2025\n#### RESEARCH\n#### SPEECH &amp; AUDIO\n#### Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages\nOmnilingual ASR team,Gil Keren,Artyom Kozhevnikov,Yen Meng,Christophe Ropers,Matthew Setzler,Skyler Wang,Ife Adebara,Michael Auli,Can Balioglu,Kevin Chan,Chierh Cheng,Joe Chuang,Caley Drooff,Mark Duppenthaler,Paul-Ambroise Duquenne,Alexander Erben,Cynthia Gao,Gabriel Mejia Gonzalez,Kehan Lyu,Sagar Miglani,Vineel Pratap,Kaushik Ram Sadagopan,Safiyyah Saleem,Arina Turkatenko,Albert Ventayol-Boada,Zheng-Xin Yong,Yu-An Chung,Jean Maillard,Rashel Moritz,Alexandre Mourachko,Mary Williamson,Shireen Yates\nNovember 10, 2025\n[Read the Paper**](https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/)\n[\nSee All Papers\n](https://ai.meta.com/global_search/?content_types[0]=publication&amp;page=1)\n![](https://scontent-sea1-1.xx.fbcdn.net/v/t39.2365-6/91900227_227456494975764_5988561604772364288_n.jpg?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=b5SRT5PLlsYQ7kNvwGqTt5z&amp;_nc_oc=AdlwegLUhbmAu3xvE0E0NksK2UdueQFfQmiWNdVaQn1hk3yZMz6VYe9cM76inLBcregnf4VdVf3cnV8ftLitAPif&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_Afq5nRXZZ7KT52V5zh7A1u0s0iVwjKj8nxtdunKmvTzX4Q&amp;oe=6973F8D7)\n## Help Us Pioneer The Future of AI\n##### We share our open source frameworks, tools, libraries, and models for everything from research exploration to large-scale production deployment.\n[\nJoin our Team\n](https://ai.meta.com/join-us/)\n[Our approach](https://ai.meta.com/about)****\n[About AI at Meta](https://ai.meta.com/about)\n[People](https://ai.meta.com/results/?content_types[0]=person&amp;sort_by=random)\n[Careers]()\n[Research](https://ai.meta.com/research)****\n[Infrastructure](https://ai.meta.com/infrastructure)\n[Resources](https://ai.meta.com/resources)\n[Demos](https://aidemos.meta.com/)\n[Meta AI](https://ai.meta.com/meta-ai/)****\n[Explore Meta AI](https://ai.meta.com/meta-ai/)\n[Get Meta AI](https://ai.meta.com/get-meta-ai/)\n[AI Studio](https://ai.meta.com/ai-studio/)\n[Latest news](https://ai.meta.com/blog)****\n[Blog](https://ai.meta.com/blog)\n[Newsletter](https://ai.meta.com/subscribe)\nFoundational models\n****\n[Llama](https://www.llama.com/)\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.2365-6/87524316_2677189655726266_6338721200264445952_n.svg?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=rcZKdynVREQQ7kNvwHjxvyz&amp;_nc_oc=AdkNtfEPwO9Sfy0ZNXtV3IBb8OMCrqvvHNrv9z01Q1xAjHOV2Bj8Z0g0GYoojv6nnrZyKOpitOVcZrkJ9JYzcUD-&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_AfqwwI1UBCJ8XRjwn6EBBkxaaQGVIIv9P32A9qsNRWTvlA&amp;oe=69741178)\n[\n![](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=JAKrXya_KyYQ7kNvwGddz9N&amp;_nc_oc=AdlOkd-BdoKYIrLwv5OQ9Z8trDVeRFLOJxIaPQq3C678n--L07bu6WlpGBObDEMO2XqNk_IVSwtbUN5mOTl0-iUc&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_AfrKOEq-mET7uOqALvq8Z3swZPu58GjbDuzsLnHhWzd_Eg&amp;oe=695F97E7)\n![](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=JAKrXya_KyYQ7kNvwGddz9N&amp;_nc_oc=AdlOkd-BdoKYIrLwv5OQ9Z8trDVeRFLOJxIaPQq3C678n--L07bu6WlpGBObDEMO2XqNk_IVSwtbUN5mOTl0-iUc&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_AfrKOEq-mET7uOqALvq8Z3swZPu58GjbDuzsLnHhWzd_Eg&amp;oe=695F97E7)\n](https://www.facebook.com/aiatmeta/)\n[\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=ferVOG4eevYQ7kNvwHbYB5h&amp;_nc_oc=AdkmU0bLe93uhh3XMuJq9ja-_eyVjat4XfLK3iEInlPnRiiJi-FAcMvDDRq1enDf0l1ejrsgajDiUIN59Hey2MO4&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_AfqzgdGrXEQQN5_GKGt3IYRmLukT33Nc0uywFPzWwaE28A&amp;oe=695F9022)\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=ferVOG4eevYQ7kNvwHbYB5h&amp;_nc_oc=AdkmU0bLe93uhh3XMuJq9ja-_eyVjat4XfLK3iEInlPnRiiJi-FAcMvDDRq1enDf0l1ejrsgajDiUIN59Hey2MO4&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_AfqzgdGrXEQQN5_GKGt3IYRmLukT33Nc0uywFPzWwaE28A&amp;oe=695F9022)\n](https://twitter.com/aiatmeta/)\n[\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=XmhD4ZAj5z0Q7kNvwE_DlET&amp;_nc_oc=AdnywdLcjlheQVk5QnTEC1x4ugNOzDzJA6p0JZuEmBUoihiXzu1rPL9TOFjUw5OGECKVW2CxI7IjShZr128eenSp&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_AfoLohoFPc-fKHcyPxOxmZPHm_bpDbeGg3dgjW0xz2vg6w&amp;oe=695F83BB)\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=XmhD4ZAj5z0Q7kNvwE_DlET&amp;_nc_oc=AdnywdLcjlheQVk5QnTEC1x4ugNOzDzJA6p0JZuEmBUoihiXzu1rPL9TOFjUw5OGECKVW2CxI7IjShZr128eenSp&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_AfoLohoFPc-fKHcyPxOxmZPHm_bpDbeGg3dgjW0xz2vg6w&amp;oe=695F83BB)\n](https://www.linkedin.com/showcase/aiatmeta)\n[\n![](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=VjxhUTqTCVIQ7kNvwGnOaXN&amp;_nc_oc=Adk7eLHsXv4mwCWDsRr6AA9YLjX2HS4nFGthcaZGLybebLWZLvH6I_cc53261Qwzy-39RfL8jBYimOXZ40Zp3_2C&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;oh=00_AfrlmBYkhap9bXHnQb3fFMHckrmejaZLuUdd1amu7A8KvA&amp;oe=695F9D2E)\n![](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=VjxhUTqTCVIQ7kNvwGnOaXN&amp;_nc_oc=Adk7eLHsXv4mwCWDsRr6AA9YLjX2HS4nFGthcaZGLybebLWZLvH6I_cc53261Qwzy-39RfL8jBYimOXZ40Zp3_2C&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=ZZ9qQKKZuMDtxmV9S4-jwQ&amp;o",
          "original_query": "Llama: Open and efficient foundation language models",
          "cleaned_query": "Llama: Open and efficient foundation language models",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "1. **Density-Aware Compute-Optimal Scaling Laws**\n   Develop a scaling law that augments Chinchilla-style compute-optimality (parameters vs. tokens) with an explicit *data density* term (redundancy/diversity), building on Paper 1\u2019s density metric. Train controlled sweeps where compute is fixed but density is manipulated via dedup granularity and filtering strength (FineWeb tooling), then fit a law that predicts when extra tokens enter a sub-scaling regime.",
        "**Adaptive Token Budgeting via Online Density Estimation**\n   Create a training controller that periodically estimates dataset density from streaming minibatches (e.g., approximate near-duplicate rate + embedding diversity) and dynamically adjusts (i) sampling weights, (ii) effective epoching/over-training multiplier, or (iii) when to stop training. The actionable contribution is an algorithm that prevents \u201cwasted tokens\u201d that Paper 1 attributes to high-density redundancy while retaining the over-training benefits studied in Paper 4.",
        "**Trajectory-Guided Data Pruning Using Per-Token Loss Stagnation**\n   Using the observation from Paper 3 that a consistent subset of tokens drives most loss reduction while others stagnate/double-descend, build a pruning pipeline that removes or downweights \u201cstagnant\u201d shards early. Evaluate whether this targeted pruning shifts models back toward the predictable scaling regime (Paper 4) and reduces sub-scaling under large-token training (Paper 1), using FineWeb as the base corpus.",
        "**Hallucination-Distribution Suppression Through Curriculum Reweighting**\n   Paper 3 finds early training increases probability on grammatical-but-hallucinatory sequences, and larger models later correct this. Design a curriculum that explicitly identifies these sequences (via retrieval inconsistency checks or contradiction detectors) and progressively downweights them so smaller/compute-limited models avoid getting \u201cstuck\u201d in the suboptimal distribution, then measure effects on perplexity-to-downstream transfer (Papers 3\u20134).",
        "**Compute Allocation Under Over-Training with Inference-Cost Constraints**\n   Extend compute-optimal training (Paper 2) by jointly optimizing *training FLOPs + lifetime inference cost* and allowing deliberate over-training of smaller models (Paper 4). Produce a practical recipe: given expected query volume and latency targets, output an optimal (N, D, over-training multiplier M) and validate across multiple FineWeb-derived data distributions.",
        "**Domain-Conditional Scaling Laws for Educational vs. Web Data Mixtures**\n   FineWeb-Edu shows outsized gains on reasoning/knowledge benchmarks; Paper 1 argues data quality/diversity affects sub-scaling. Fit mixture-aware scaling laws where each component (e.g., FineWeb general, FineWeb-Edu) has separate exponents/irreducible loss, and derive an actionable optimizer that chooses the mixture ratio as a function of model size and compute.",
        "**Dedup Granularity as a Control Knob for Sub-Scaling Regimes**\n   FineWeb documents dedup strategies; Paper 1 links redundancy to sub-scaling via density. Systematically vary dedup granularity (document/paragraph/sentence/span) and measure how the fitted sub-optimal scaling law parameters change, producing concrete guidance like \u201cfor 7B at X tokens, sentence-level dedup maximizes downstream score per FLOP.\u201d",
        "**Checkpoint-Efficient Downstream Prediction from Early Perplexity Trajectories**\n   Paper 3 and Paper 4 both connect perplexity to downstream performance; leverage *early* training checkpoints to predict final downstream error with uncertainty bounds. Build a lightweight predictor that uses the first K% of training trajectory (loss curvature + token-stagnation stats) to decide whether to continue, change data mixture, or restart with different allocation\u2014reducing the need for full expensive runs.",
        "**Resource-Allocation Sensitivity Maps for Training Strategy \u201cNon-Optimality\u201d**\n   Paper 1 highlights sub-scaling from non-optimal resource allocation/training strategies. Create \u201csensitivity maps\u201d that quantify how deviations in learning rate schedule, batch size, token-to-parameter ratio, and dedup level perturb scaling-law residuals, using multi-run ablations (like FineWeb\u2019s) across model sizes; the output is a diagnostic tool that flags when a run is likely to enter a sub-scaling regime.",
        "**Density-Regularized Data Sampling to Maximize Effective Diversity per Token**\n   Propose a sampling objective that penalizes selecting examples that are too close to recently seen content (approximate kNN in embedding space), effectively enforcing a \u201cdiversity budget\u201d during training. Evaluate whether this density-regularized sampler preserves Chinchilla-like predictable scaling (Paper 2) at trillion-token regimes and improves over naive over-training (Paper 4) on reasoning benchmarks boosted by FineWeb-Edu (Paper 5)."
      ],
      "generated_ideas_raw": "1. **Density-Aware Compute-Optimal Scaling Laws**\n   Develop a scaling law that augments Chinchilla-style compute-optimality (parameters vs. tokens) with an explicit *data density* term (redundancy/diversity), building on Paper 1\u2019s density metric. Train controlled sweeps where compute is fixed but density is manipulated via dedup granularity and filtering strength (FineWeb tooling), then fit a law that predicts when extra tokens enter a sub-scaling regime.\n\n2. **Adaptive Token Budgeting via Online Density Estimation**\n   Create a training controller that periodically estimates dataset density from streaming minibatches (e.g., approximate near-duplicate rate + embedding diversity) and dynamically adjusts (i) sampling weights, (ii) effective epoching/over-training multiplier, or (iii) when to stop training. The actionable contribution is an algorithm that prevents \u201cwasted tokens\u201d that Paper 1 attributes to high-density redundancy while retaining the over-training benefits studied in Paper 4.\n\n3. **Trajectory-Guided Data Pruning Using Per-Token Loss Stagnation**\n   Using the observation from Paper 3 that a consistent subset of tokens drives most loss reduction while others stagnate/double-descend, build a pruning pipeline that removes or downweights \u201cstagnant\u201d shards early. Evaluate whether this targeted pruning shifts models back toward the predictable scaling regime (Paper 4) and reduces sub-scaling under large-token training (Paper 1), using FineWeb as the base corpus.\n\n4. **Hallucination-Distribution Suppression Through Curriculum Reweighting**\n   Paper 3 finds early training increases probability on grammatical-but-hallucinatory sequences, and larger models later correct this. Design a curriculum that explicitly identifies these sequences (via retrieval inconsistency checks or contradiction detectors) and progressively downweights them so smaller/compute-limited models avoid getting \u201cstuck\u201d in the suboptimal distribution, then measure effects on perplexity-to-downstream transfer (Papers 3\u20134).\n\n5. **Compute Allocation Under Over-Training with Inference-Cost Constraints**\n   Extend compute-optimal training (Paper 2) by jointly optimizing *training FLOPs + lifetime inference cost* and allowing deliberate over-training of smaller models (Paper 4). Produce a practical recipe: given expected query volume and latency targets, output an optimal (N, D, over-training multiplier M) and validate across multiple FineWeb-derived data distributions.\n\n6. **Domain-Conditional Scaling Laws for Educational vs. Web Data Mixtures**\n   FineWeb-Edu shows outsized gains on reasoning/knowledge benchmarks; Paper 1 argues data quality/diversity affects sub-scaling. Fit mixture-aware scaling laws where each component (e.g., FineWeb general, FineWeb-Edu) has separate exponents/irreducible loss, and derive an actionable optimizer that chooses the mixture ratio as a function of model size and compute.\n\n7. **Dedup Granularity as a Control Knob for Sub-Scaling Regimes**\n   FineWeb documents dedup strategies; Paper 1 links redundancy to sub-scaling via density. Systematically vary dedup granularity (document/paragraph/sentence/span) and measure how the fitted sub-optimal scaling law parameters change, producing concrete guidance like \u201cfor 7B at X tokens, sentence-level dedup maximizes downstream score per FLOP.\u201d\n\n8. **Checkpoint-Efficient Downstream Prediction from Early Perplexity Trajectories**\n   Paper 3 and Paper 4 both connect perplexity to downstream performance; leverage *early* training checkpoints to predict final downstream error with uncertainty bounds. Build a lightweight predictor that uses the first K% of training trajectory (loss curvature + token-stagnation stats) to decide whether to continue, change data mixture, or restart with different allocation\u2014reducing the need for full expensive runs.\n\n9. **Resource-Allocation Sensitivity Maps for Training Strategy \u201cNon-Optimality\u201d**\n   Paper 1 highlights sub-scaling from non-optimal resource allocation/training strategies. Create \u201csensitivity maps\u201d that quantify how deviations in learning rate schedule, batch size, token-to-parameter ratio, and dedup level perturb scaling-law residuals, using multi-run ablations (like FineWeb\u2019s) across model sizes; the output is a diagnostic tool that flags when a run is likely to enter a sub-scaling regime.\n\n10. **Density-Regularized Data Sampling to Maximize Effective Diversity per Token**\n   Propose a sampling objective that penalizes selecting examples that are too close to recently seen content (approximate kNN in embedding space), effectively enforcing a \u201cdiversity budget\u201d during training. Evaluate whether this density-regularized sampler preserves Chinchilla-like predictable scaling (Paper 2) at trillion-token regimes and improves over naive over-training (Paper 4) on reasoning benchmarks boosted by FineWeb-Edu (Paper 5).",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Density-Aware Compute-Optimal Scaling Laws**\n   Develop a scaling law that augments Chinchilla-style compute-optimality (parameters vs. tokens) with an explicit *data density* term (redundancy/di",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Adaptive Token Budgeting via Online Density Estimation**\n   Create a training controller that periodically estimates dataset density from streaming minibatches (e.g., approximate near-duplicate rate",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Trajectory-Guided Data Pruning Using Per-Token Loss Stagnation**\n   Using the observation from Paper 3 that a consistent subset of tokens drives most loss reduction while others stagnate/double-desc",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**Hallucination-Distribution Suppression Through Curriculum Reweighting**\n   Paper 3 finds early training increases probability on grammatical-but-hallucinatory sequences, and larger models later corr",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Compute Allocation Under Over-Training with Inference-Cost Constraints**\n   Extend compute-optimal training (Paper 2) by jointly optimizing *training FLOPs + lifetime inference cost* and allowing de",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Domain-Conditional Scaling Laws for Educational vs. Web Data Mixtures**\n   FineWeb-Edu shows outsized gains on reasoning/knowledge benchmarks; Paper 1 argues data quality/diversity affects sub-scali",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Dedup Granularity as a Control Knob for Sub-Scaling Regimes**\n   FineWeb documents dedup strategies; Paper 1 links redundancy to sub-scaling via density. Systematically vary dedup granularity (docum",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Checkpoint-Efficient Downstream Prediction from Early Perplexity Trajectories**\n   Paper 3 and Paper 4 both connect perplexity to downstream performance; leverage *early* training checkpoints to pre",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Resource-Allocation Sensitivity Maps for Training Strategy \u201cNon-Optimality\u201d**\n   Paper 1 highlights sub-scaling from non-optimal resource allocation/training strategies. Create \u201csensitivity maps\u201d th",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Density-Regularized Data Sampling to Maximize Effective Diversity per Token**\n   Propose a sampling objective that penalizes selecting examples that are too close to recently seen content (approxima",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 24,
      "paper_title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions",
      "contribution": "A theoretical and algorithmic treatment of gradient-based training on AIMC devices with general, asymmetric and nonlinear pulse-response functions, proving that residual-learning updates (a bilevel formulation) remove the implicit bias caused by asymmetric responses and recover convergence to true critical points while also handling limited response granularity.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10411,
      "output_tokens": 1018,
      "predecessor_details": [
        {
          "success": true,
          "title": "Towards Exact Gradient-based Training on Analog In-memory ...",
          "url": "https://arxiv.org/abs/2406.12774",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2406.12774** (cs)\n\n\\[Submitted on 18 Jun 2024\\]\n\n# Title:Towards Exact Gradient-based Training on Analog In-memory Computing\n\nAuthors: [Zhaoxian Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+Z), [Tayfun Gokmen](https://arxiv.org/search/cs?searchtype=author&query=Gokmen,+T), [Malte J. Rasch](https://arxiv.org/search/cs?searchtype=author&query=Rasch,+M+J), [Tianyi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+T)\n\nView a PDF of the paper titled Towards Exact Gradient-based Training on Analog In-memory Computing, by Zhaoxian Wu and Tayfun Gokmen and Malte J. Rasch and Tianyi Chen\n\n[View PDF](https://arxiv.org/pdf/2406.12774) [HTML (experimental)](https://arxiv.org/html/2406.12774v1)\n\n> Abstract:Given the high economic and environmental costs of using large vision or language models, analog in-memory accelerators present a promising solution for energy-efficient AI. While inference on analog accelerators has been studied recently, the training perspective is underexplored. Recent studies have shown that the \"workhorse\" of digital AI training - stochastic gradient descent (SGD) algorithm converges inexactly when applied to model training on non-ideal devices. This paper puts forth a theoretical foundation for gradient-based training on analog devices. We begin by characterizing the non-convergent issue of SGD, which is caused by the asymmetric updates on the analog devices. We then provide a lower bound of the asymptotic error to show that there is a fundamental performance limit of SGD-based analog training rather than an artifact of our analysis. To address this issue, we study a heuristic analog algorithm called Tiki-Taka that has recently exhibited superior empirical performance compared to SGD and rigorously show its ability to exactly converge to a critical point and hence eliminates the asymptotic error. The simulations verify the correctness of the analyses.\n\n| | |\n| --- | --- |\n| Comments: | 10 pages, 5 figures,2 tables |\n| Subjects: | Machine Learning (cs.LG); Hardware Architecture (cs.AR); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:2406.12774](https://arxiv.org/abs/2406.12774) \\[cs.LG\\] |\n| | (or [arXiv:2406.12774v1](https://arxiv.org/abs/2406.12774v1) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2406.12774](https://doi.org/10.48550/arXiv.2406.12774) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zhaoxian Wu \\[ [view email](https://arxiv.org/show-email/e9e3b88b/2406.12774)\\]\n\n**\\[v1\\]**\nTue, 18 Jun 2024 16:43:59 UTC (390 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Towards Exact Gradient-based Training on Analog In-memory Computing, by Zhaoxian Wu and Tayfun Gokmen and Malte J. Rasch and Tianyi Chen\n\n- [View PDF](https://arxiv.org/pdf/2406.12774)\n- [HTML (experimental)](https://arxiv.org/html/2406.12774v1)\n- [TeX Source](https://arxiv.org/src/2406.12774)\n- [Other Formats](https://arxiv.org/format/2406.12774)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2406.12774&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2406.12774&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2024-06](https://arxiv.org/list/cs.LG/2024-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2406.12774?context=cs)\n\n[cs.AR](https://arxiv.org/abs/2406.12774?context=cs.AR)\n\n[math](https://arxiv.org/abs/2406.12774?context=math)\n\n[math.OC](https://arxiv.org/abs/2406.12774?context=math.OC)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2406.12774)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2406.12774)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2406.12774)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2406.12774&description=Towards Exact Gradient-based Training on Analog In-memory Computing) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2406.12774&title=Towards Exact Gradient-based Training on Analog In-memory Computing)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2406.12774) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Towards exact gradient-based training on analog in-memory computing",
          "cleaned_query": "Towards exact gradient-based training on analog in-memory computing",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning",
          "url": "https://arxiv.org/html/2510.02516",
          "content": "In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning\n# In-memory Training on Analog Devices with Limited\nConductance States via Multi-tile Residual Learning\nJindan Li1,\nZhaoxian Wu1,\nGaowen Liu3,\nTayfun Gokmen4,\nTianyi Chen1, 2\n1Cornell University, New York, NY 10044\n2Rensselaer Polytechnic Institute, Troy, NY 12180\n3Cisco Research, Naperville, IL 60540\n4IBM T. J. Watson Research Center, Yorktown Heights, NY 10598\n{jl4767, zw868, tianyi.chen}@cornell.edu\n###### Abstract\nAnalog in-memory computing (AIMC) accelerators enable efficient deep neural network computation directly within memory using resistive crossbar arrays, where model parameters are represented by the conductance states of memristive devices.\nHowever, effective in-memory training typically requires at least 8-bit conductance states to match digital baselines. Realizing such fine-grained states is costly and often requires complex noise mitigation techniques that increase circuit complexity and energy consumption. In practice, many promising memristive devices such as ReRAM offer only about 4-bit resolution due to fabrication constraints, and this limited update precision substantially degrades training accuracy.\nTo enable on-chip training with these limited-state devices, this paper proposes a*residual learning*framework that sequentially learns on multiple crossbar tiles to compensate the residual errors from low-precision weight updates.\nOur theoretical analysis shows that the optimality gap shrinks with the number of tiles and achieves a linear convergence rate. Experiments on standard image classification benchmarks demonstrate that our method consistently outperforms state-of-the-art in-memory analog training strategies under limited-state settings, while incurring only moderate hardware overhead as confirmed by our cost analysis.\n### 1Introduction\nWith the growing adoption of AI across various fields, the demand for*accurate and energy-efficient*training hardware is increasing.\nIn this context,*analog in-memory computing*(AIMC) is an emerging solution that performs matrix vector multiplication (MVM) operations directly on weights stored in memory, offering significant efficiency improvements over conventional von Neumann systems.\nIn AIMC hardware, the parameters (matrices) of deep neural networks (DNN) are represented by the conductance states of*memristive devices*in analog crossbar arrays, while the inputs (vectors) are programmed as voltage signals. Using Kirchhoff\u2019s and Ohm\u2019s laws, MVM operations between aD\u00d7DD\\\\times Dmatrix and a vector can be completed in\ud835\udcaa\u200b(1)\\\\mathcal{O}(1)time on AIMC hardware, while it requires at least\ud835\udcaa\u200b(D)\\\\mathcal{O}(D)time in digital MVM> (Hu et\u00a0al., [> 2016\n](https://arxiv.org/html/2510.02516v1#bib.bib21)> )\n.\nA full MVM on analog hardware can be executed with energy in the range of tens of femtojoules (10\u22121510^{-15}joules), whereas accessing a 1 kB SRAM block in digital systems typically costs 1 picojoule (10\u22121210^{-12}joules) per byte> (Murmann, [> 2020\n](https://arxiv.org/html/2510.02516v1#bib.bib37)> )\n.\nThis advantage translates into higher energy efficiency.\nA typical commercial digital accelerator has plateaued around 10 tera-operations per second per watt (TOPS/W)> (Reuther et\u00a0al., [> 2022\n](https://arxiv.org/html/2510.02516v1#bib.bib45)> )\n, which can be significantly surpassed by AIMC accelerators.\nFor example, a monolithic 3D AIMC chip achieves more than 210 TOPS/W> (Chen et\u00a0al., [> 2022\n](https://arxiv.org/html/2510.02516v1#bib.bib9)> )\n, and a 4\u00d7\\\\times4 core array reaches 30 TOPS/W> (Jia et\u00a0al., [> 2021\n](https://arxiv.org/html/2510.02516v1#bib.bib25)> )\n.\nHowever, due to the inherent difficulty in precisely and reliably changing the conductance of the memory elements, in-memory analog training presents significant challenges.\nThis paper focuses on gradient-based in-memory training on AIMC hardware. The objective of training is to solve the standard model optimization problem, formally defined as:\n|W\u2217:=arg\u2061minW\u2208\u211dD\u00d7D\u2061f\u200b(W)W^{\\*}:=\\\\arg\\\\min\\_{W\\\\in\\\\mathbb{R}^{D\\\\times D}}f(W)||(1)|\n![Refer to caption](x1.png)Figure 1:Illustration for rank update via stochastic pulse streams.\nwheref\u200b(\u22c5):\u211dD\u00d7D\u2192\u211df(\\\\cdot):\\\\mathbb{R}^{D\\\\times D}\\\\to\\\\mathbb{R}is the objective andWWis a trainable matrix stored in analog crossbar arrays. In digital accelerators, equation[1](https://arxiv.org/html/2510.02516v1#S1.E1)can be solved by stochastic gradient descent (SGD), whose recursion is given byWt+1=Wt\u2212\u03b1\u200b\u2207f\u200b(Wt;\u03bet)W\\_{t+1}=W\\_{t}-\\\\alpha\\\\nabla f(W\\_{t};\\\\xi\\_{t}). Here,\u03b1\\\\alphais the learning rate and\u03bet\\\\xi\\_{t}denotes a sample randomly drawn in iterationtt. To implement SGD on AIMC hardware, one needs to update the weights stored in the crossbar array using the*rank update*method> (Gokmen and Vlasov, [> 2016\n](https://arxiv.org/html/2510.02516v1#bib.bib18)> )\n. This approach leverages two\ud835\udcaa\u200b(D)\\\\mathcal{O}(D)-dimensional vectors, the backpropagation error\u03b4\\\\deltaand the inputxx, to perform in-memory updates directly on the analog array via stochastic pulse streams, as illustrated in Figure[1](https://arxiv.org/html/2510.02516v1#S1.F1).\nIdeally, each pulse adjusts a weight elementw\u2208Ww\\\\in Wby the minimal increment\u00b1\u0394\u200bwmin\\\\pm\\\\Delta w\\_{\\\\min}, with the sign determined by the pulse polarity.\nThe resulting weight evolution is illustrated in Figure[2](https://arxiv.org/html/2510.02516v1#S1.F2).\n![Refer to caption](figures/my_softbounds_response.png)Figure 2:Illustration of pulsed weight updates on 10 and 20 states softbound devices. Due to asymmetric update, the actual weight increment follows\u0394\u200bwminp=\u0394\u200bwmin\u22c5q+\u200b(w)\\\\Delta w^{p}\\_{\\\\min}=\\\\Delta w\\_{\\\\min}\\\\cdot q\\_{+}(w), whereq+\u200b(\u22c5)q\\_{+}(\\\\cdot)represents the device positive response factor.\nWe define the number of device states by dividing the total weight rangew\u2208[\u03c4min,\u03c4max]w\\\\in[\\\\tau\\_{\\\\min},\\\\tau\\_{\\\\max}]by this minimal weight change:nstates:=(\u03c4max\u2212\u03c4min)/\u0394\u200bwminn\\_{\\\\text{states}}:=(\\\\tau\\_{\\\\max}-\\\\tau\\_{\\\\min})/\\\\Delta w\\_{\\\\min}, wherenstatesn\\_{\\\\text{states}}determines how many distinct values the weight can stably represent.\nA smallernstatesn\\_{\\\\text{states}}(larger\u0394\u200bwmin\\\\Delta w\\_{\\\\min}) amplifying quantization noise\u03b6\\\\zetawhose variance scales with\u0394\u200bwmin\\\\Delta w\\_{\\\\min}. This noise captures the gap between ideal and actual updates and fundamentally limits training accuracy.\nPrevious studies have shown that successful training on crossbar-based architectures typically requires at least 8-bit distinct conductance levels to achieve competitive accuracy> (Li et\u00a0al., [> 2018\n](https://arxiv.org/html/2510.02516v1#bib.bib34)> ; Chen et\u00a0al., [> 2017\n](https://arxiv.org/html/2510.02516v1#bib.bib10)> )\n. However, some devices struggle to provide this level of granularity within a single memory cell. MRAM devices are typically limited to two stable states per cell, whereas ReRAM is usually constrained to 4-bit per cell in practice (detailed survey in Table[3](https://arxiv.org/html/2510.02516v1#A1.T3)and Appendix[A](https://arxiv.org/html/2510.02516v1#A1)), which makes it difficult to achieve the multi-bit precision required for effective training. As illustrated in Figure[3](https://arxiv.org/html/2510.02516v1#S1.F3), reducing the number of states to 20 or fewer results in a convergence failure. While ECRAM can support thousands of states, it remains hindered by practical challenges, including complex three-terminal design, CMOS incompatibility, and material instability> (Kim et\u00a0al., [> 2023\n](https://arxiv.org/html/2510.02516v1#bib.bib29)> ; Kwak et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.02516v1#bib.bib33)> )\n, which lack a scalable fabrication pipeline> (Kwak et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.02516v1#bib.bib32)> )\n.\nIn contrast, ReRAM remains one of the most manufacturable and scalable options> (Stecconi et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.02516v1#bib.bib51)> )\n. In practice, its bi-directional update behavior typically involves limited conductance states together with asymmetric non-idealities> Xi et\u00a0al. (\n[> 2020\n](https://arxiv.org/html/2510.02516v1#bib.bib59)> )\n, which form the primary focus of this paper.Rather thanpushing for increasingly precise devices, our work advocatesalgorithm innovationsto mitigate the limitations of low-state memristive devices, which better align with current fabrication capabilities and offer energy and area efficiency for near-term deployment. Importantly, our goal is not to dismiss high-state devices, but to emphasize the practical and architectural benefits of training with low-state memristive technologies.\n#### 1.1Main results\nThis work addresses the fundamental challenges of limited precision in gradient-based training on AIMC hardware, which stem from the limited number of conductance states and the asymmetric update. We address these challenges by designingcomposite weight representationsthat integrate multiple low-precision tiles to represent high-precision weights, and by developingmulti-timescale residual learningalgorithms that enable each tile to dynamically track the residual training error left by preceding tiles. Together, these techniques ensure stable convergence and high training accuracy under low-precision constraints. This motivates our first question:\nQ1)*How can high-precision weights be represented using limited-conductance states AIMC devices?*\nTo construct a high precision weight, we define the composite weight asW\u00af=\u2211n=0N\u03b3n\u200bW(n)\\\\overline{W}=\\\\sum\\_{n=0}^{N}\\\\gamma^{n}W^{(n)}, whereW(n)W^{(n)}denotes a low precision weight on an AIMC tilenn, and\u03b3\u2208(0,1)\\\\gamma\\\\in(0,1)controls its scaling.\nThis structure increases the total number of representable values exponentially with the number of tiles, thus significantly enhancing the effective numeric precision. The composite weightW\u00af\\\\overline{W}is then used in both forward and backward passes; see the details of circuit implementation in Section[3.3](https://arxiv.org/html/2510.0251",
          "original_query": "Algorithm for training neural networks on resistive device arrays",
          "cleaned_query": "Algorithm for training neural networks on resistive device arrays",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices: Design Considerations",
          "url": "https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/",
          "content": "Back to Top\n[Skip to main content](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#main-content)\n\n![Dot gov](https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg)\n\n**The .gov means it\u2019s official.**\n\nFederal government websites often end in .gov or .mil. Before\nsharing sensitive information, make sure you\u2019re on a federal\ngovernment site.\n\n![Https](https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg)\n\n**The site is secure.**\n\nThe **https://** ensures that you are connecting to the\nofficial website and that any information you provide is encrypted\nand transmitted securely.\n\n[Access keys](https://www.ncbi.nlm.nih.gov/guide/browsers/#ncbi_accesskeys) [NCBI Homepage](https://www.ncbi.nlm.nih.gov) [MyNCBI Homepage](https://ncbi.nlm.nih.gov/myncbi/) [Main Content](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#maincontent) [Main Navigation](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/)\n\n**Preview improvements coming to the PMC website in October 2024.**\n**[Learn More](https://ncbiinsights.ncbi.nlm.nih.gov/2024/03/14/preview-pmc-improvements/) or**\n**[Try it out now](https://pmc.ncbi.nlm.nih.gov/articles/PMC4954855/).**\n\n- [Journal List](https://ncbi.nlm.nih.gov/pmc/journals/)\n- [Front Neurosci](https://ncbi.nlm.nih.gov/pmc/?term=%22Front%20Neurosci%22[journal])\n- PMC4954855\n\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\n\nLearn more:\n[PMC Disclaimer](https://ncbi.nlm.nih.gov/pmc/about/disclaimer/)\n\\|\n[PMC Copyright Notice](https://ncbi.nlm.nih.gov/pmc/about/copyright/)\n\n![Logo of frontneurosci](https://ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/logo-frontneurosci.gif)\n\nFront Neurosci. 2016; 10: 333.\n\nPublished online 2016 Jul 21. doi:\u00a0[10.3389/fnins.2016.00333](https://doi.org/10.3389%2Ffnins.2016.00333)\n\nPMCID: PMC4954855\n\nPMID: [27493624](https://pubmed.ncbi.nlm.nih.gov/27493624)\n\n# Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices: Design Considerations\n\n[Tayfun Gokmen](https://pubmed.ncbi.nlm.nih.gov/?term=Gokmen%20T%5BAuthor%5D)\\* and [Yurii Vlasov](https://pubmed.ncbi.nlm.nih.gov/?term=Vlasov%20Y%5BAuthor%5D)\u2020\n\n[Author information](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/) [Article notes](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/) [Copyright and License information](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/) [PMC Disclaimer](https://ncbi.nlm.nih.gov/pmc/about/disclaimer/)\n\nIBM T. J. Watson Research Center, Yorktown Heights, NY, USA\n\nEdited by: Themis Prodromakis, University of Southampton, UK\n\nReviewed by: Robert Legenstein, Graz University of Technology, Austria; Hesham Mostafa, University of Zurich and ETH Zurich, Switzerland\n\n\\*Correspondence: Tayfun Gokmen [moc.mbi.su@nemkogt](mailto:dev@null)\n\nThis article was submitted to Neuromorphic Engineering, a section of the journal Frontiers in Neuroscience\n\n\u2020Present Address: Yurii Vlasov, Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA\n\nReceived 2016 Apr 4; Accepted 2016 Jul 1.\n\n[Copyright](https://ncbi.nlm.nih.gov/pmc/about/copyright/) \u00a9 2016 Gokmen and Vlasov.\n\nThis is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.\n\n## Abstract\n\nIn recent years, deep neural networks (DNN) have demonstrated significant business impact in large scale analysis and classification tasks such as speech recognition, visual object detection, pattern extraction, etc. Training of large DNNs, however, is universally considered as time consuming and computationally intensive task that demands datacenter-scale computational resources recruited for many days. Here we propose a concept of resistive processing unit (RPU) devices that can potentially accelerate DNN training by orders of magnitude while using much less power. The proposed RPU device can store and update the weight values locally thus minimizing data movement during training and allowing to fully exploit the locality and the parallelism of the training algorithm. We evaluate the effect of various RPU device features/non-idealities and system parameters on performance in order to derive the device and system level specifications for implementation of an accelerator chip for DNN training in a realistic CMOS-compatible technology. For large DNNs with about 1 billion weights this massively parallel RPU architecture can achieve acceleration factors of 30, 000 \u00d7 compared to state-of-the-art microprocessors while providing power efficiency of 84, 000 GigaOps\u2215s\u2215W. Problems that currently require days of training on a datacenter-size cluster with thousands of machines can be addressed within hours on a single RPU accelerator. A system consisting of a cluster of RPU accelerators will be able to tackle Big Data problems with trillions of parameters that is impossible to address today like, for example, natural speech recognition and translation between all world languages, real-time analytics on large streams of business and scientific data, integration, and analysis of multimodal sensory data flows from a massive number of IoT (Internet of Things) sensors.\n\n**Keywords:** deep neural network training, synaptic device, machine learning, artificial neural networks, nanotechnology, materials engineering, electronic devices, memristive devices\n\n## Introduction\n\nDeep Neural Networks (DNNs; LeCun et al., [2015](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B23)) demonstrated significant commercial success in the last years with performance exceeding sophisticated prior methods in speech (Hinton et al., [2012](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B13)) and object recognition (Krizhevsky et al., [2012](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B20); Simonyan and Zisserman, [2015](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B34); Szegedy et al., [2015](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B39)). However, training the DNNs is an extremely computationally intensive task that requires massive computational resources and enormous training time that hinders their further application. For example, a 70% relative improvement has been demonstrated for a DNN with 1 billion connections that was trained on a cluster with 1000 machines for three days (Le et al., [2012](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B22)). Training the DNNs relies in general on the backpropagation algorithm that is intrinsically local and parallel (Rumelhart et al., [1986](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B31)). Various hardware approaches to accelerate DNN training that are exploiting this locality and parallelism have been explored with a different level of success starting from the early 90s (Arima et al., [1991](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B2); Lehmann et al., [1993](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B25)) to current developments with GPU (Coates et al., [2013](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B9); Wu et al., [2015](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B40)), FPGA (Gupta et al., [2015](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B12)) or specially designed ASIC (Chen Y. et al., [2014](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B7)). Further acceleration is possible by fully utilizing the locality and parallelism of the algorithm. For a fully connected DNN layer that maps _N_ neurons to _N_ neurons significant acceleration can be achieved by minimizing data movement using local storage and processing of the weight values on the same node and connecting nodes together into a massive _N_ \u00d7 _N_ systolic array (Lehmann et al., [1993](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B25)) where the whole DNN can fit in. Instead of a usual time complexity of _O_( _N_ 2) the problem can be reduced therefore to a constant time _O_(1) independent of the array size. However, the addressable problem size is limited to the number of nodes in the array that is challenging to scale up to billions even with the most advanced CMOS technologies.\n\nNovel nano-electronic device concepts based on non-volatile memory (NVM) technologies, such as phase change memory (PCM; Jackson et al., [2013](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B15); Kuzum et al., [2013](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B21)) and resistive random access memory (RRAM; Jo et al., [2010](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B17); Indiveri et al., [2013](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B14); Kuzum et al., [2013](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B21); Yu et al., [2013](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B42); Sa\u00efghi et al., [2015](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B32)), have been explored recently for implementing neural networks with a learning rule inspired by spike-timing-dependent plasticity (STDP) observed in biological systems (Bi and Poo, [1998](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B3)). Only recently, their implementation for acceleration of DNN training using backpropagation algorithm have been considered (Burr et al., [2014](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B4); Li et al., [2014](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B26); Xu et al., [2014](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B41); Prezioso et al., [2015](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B30); Soudry et al., [2015](https://ncbi.nlm.nih.gov/pmc/articles/PMC4954855/#B35)) with reported accel",
          "original_query": "Acceleration of deep neural network training with resistive cross-point devices: Design considerations",
          "cleaned_query": "Acceleration of deep neural network training with resistive cross-point devices: Design considerations",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Mitigating methodology of hardware non-ideal characteristics ...",
          "url": "https://link.springer.com/article/10.1007/s11432-023-4021-y",
          "content": "References Yao P, Wu H, Gao B, et al. Fully hardware-implemented memristor convolutional neural network. Nature, 2020, 577: 641\u2013646 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Wan W, Kubendran R, Schaefer C, et al. A compute-in-memory chip based on resistive random-access memory. Nature, 2022, 608: 504\u2013512 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Xue C X, Hung J M, Kao H Y, et al. A 22nm 4Mb 8b-precision ReRAM computing-in-memory macro with 11.91 to 195.7 TOPS/W for tiny AI edge devices. In: Proceedings of IEEE International Solid-State Circuits Conference (ISSCC), 2021. 245\u2013247 MATH \u00a0\n \n Google Scholar \u00a0\n Hung J M, Huang T H, Huang S P, et al. An 8-Mb DC-current-free binary-to-8b precision ReRAM nonvolatile computing-inmemory macro using time-space-readout with 1286.4-21.6 TOPS/W for edge-AI devices. In: Proceedings of IEEE International Solid-State Circuits Conference (ISSCC), 2022 MATH \u00a0\n \n Google Scholar \u00a0\n Song S Y, Huang P, Shen W S, et al. A 3.3-Mbit/s true random number generator based on resistive random access memory. Sci China Inf Sci, 2023, 66: 219402 Article \u00a0\n \n Google Scholar \u00a0\n Khaddam-Aljameh R, Stanisavljevic M, Fornt Mas J, et al. HERMES core-A 14nm CMOS and PCM-based in-memory compute core using an array of 300ps/LSB linearized CCO-based ADCs and local digital processing. In: Proceedings of Symposium on VLSI Circuits, 2021 \n Google Scholar \u00a0\n Khwa W S, Chiu Y C, Jhang C J, et al. A 40-nm, 2M-Cell, 8b-precision, hybrid SLC-MLC PCM computing-in-memory Macro with 20.5\u201365.0 TOPS/W for tiny-Al edge devices. In: Proceedings of IEEE International Solid-State Circuits Conference (ISSCC), 2022 MATH \u00a0\n \n Google Scholar \u00a0\n Chiu Y C, Yang C S, Teng S H, et al. A 22nm 4Mb STT-MRAM data-encrypted near-memory computation Macro with a 192GB/s read-and-decryption bandwidth and 25.1\u201355.1 TOPS/W 8b MAC for AI operations. In: Proceedings of IEEE International Solid-State Circuits Conference (ISSCC), 2022 MATH \u00a0\n \n Google Scholar \u00a0\n Guo Z, Yin J, Bai Y, et al. Spintronics for energy-efficient computing: an overview and outlook. Proc IEEE, 2021, 109: 1398\u20131417 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Xiang Y C, Huang P, Yang H Z, et al. Storage reliability of multi-bit flash oriented to deep neural network. In: Proceedings of IEEE International Electron Devices Meeting (IEDM), 2019 MATH \u00a0\n \n Google Scholar \u00a0\n Zhang D, Wang H, Feng Y, et al. Implementation of image compression by using high-precision in-memory computing scheme based on NOR flash memory. IEEE Electron Dev Lett, 2021, 42: 1603\u20131606 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Yu G H, Huang P, Han R Z, et al. Co-optimization strategy between array operation and weight mapping for flash computing arrays to achieve high computing efficiency and accuracy. Sci China Inf Sci, 2023, 66: 129403 Article \u00a0\n \n Google Scholar \u00a0\n Yang H Z, Huang P, Han R Z, et al. An ultra-high-density and energy-efficient content addressable memory design based on 3D-NAND flash. Sci China Inf Sci, 2023, 66: 142402 Article \u00a0\n \n Google Scholar \u00a0\n Yayla M, Thomann S, Buschjager S, et al. Reliable binarized neural networks on unreliable beyond von-Neumann architecture. IEEE Trans Circ Syst I, 2022, 69: 2516\u20132528 MATH \u00a0\n \n Google Scholar \u00a0\n Soliman T, M\u00fcller F, Kirchner T, et al. Ultra-low power flexible precision FeFET based analog in-memory computing. In: Proceedings of IEEE International Electron Devices Meeting (IEDM), 2020 MATH \u00a0\n \n Google Scholar \u00a0\n Jeong Y J, Zidan M A, Lu W D. Parasitic effect analysis in memristor-array-based neuromorphic systems. IEEE Trans Nanotechnol, 2018, 17: 184\u2013193 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Chen L R, Li J W, Chen Y R, et al. Accelerator-friendly neural-network training: learning variations and defects in RRAM crossbar. In: Proceedings of Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 2017 MATH \u00a0\n \n Google Scholar \u00a0\n Woo J, Moon K, Song J, et al. Improved synaptic behavior under identical pulses using AlO x /HfO 2 bilayer RRAM array for neuromorphic systems. IEEE Electron Dev Lett, 2016, 37: 994\u2013997 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Mao R, Wen B, Jiang M, et al. Experimentally-validated crossbar model for defect-aware training of neural networks. IEEE Trans Circ Syst II, 2022, 69: 2468\u20132472 MATH \u00a0\n \n Google Scholar \u00a0\n Li H, Jiang Z, Huang P, et al. Variation-aware, reliability-emphasized design and optimization of RRAM using SPICE model. In: Proceedings of Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 2015 MATH \u00a0\n \n Google Scholar \u00a0\n Xiang Y C, Huang P, Zhou Z, et al. Analog deep neural network based on nor flash computing array for high speed/energy efficiency computation. In: Proceedings of IEEE International Symposium on Circuits and Systems (ISCAS), 2019 MATH \u00a0\n \n Google Scholar \u00a0\n Wu W, Wu H Q, Gao B, et al. A methodology to improve linearity of analog RRAM for neuromorphic computing. In: Proceedings of IEEE Symposium on VLSI Technology, 2018 MATH \u00a0\n \n Google Scholar \u00a0\n Sun X, Yu S. Impact of non-ideal characteristics of resistive synaptic devices on implementing convolutional neural networks. IEEE J Emerg Sel Top Circ Syst, 2019, 9: 570\u2013579 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Jain S, Sengupta A, Roy K, et al. RxNN: a framework for evaluating deep neural networks on resistive crossbars. IEEE Trans Comput-Aided Des Integr Circ Syst, 2021, 40: 326\u2013338 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Peng X, Huang S, Jiang H, et al. DNN+NeuroSim V2.0: an end-to-end benchmarking framework for compute-in-memory accelerators for on-chip training. IEEE Trans Comput-Aided Des Integr Circ Syst, 2021, 40: 2306\u20132319 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Zhu Z, Sun H, Xie T, et al. MNSIM 2.0: a behavior-level modeling tool for processing-in-memory architectures. IEEE Trans Comput-Aided Des Integr Circ Syst, 2023, 42: 4112\u20134125 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Chakraborty I, Ali M F, Kim D E, et al. GENIEx: a generalized approach to emulating non-ideality in memristive Xbars using neural networks. In: Proceedings of the 57th ACM/IEEE Design Automation Conference (DAC), 2020 MATH \u00a0\n \n Google Scholar \u00a0\n Liu C C, Hu M, Strachan J P, et al. Rescuing memristor-based neuromorphic design with high defects. In: Proceedings of the 54th Annual Design Automation Conference, 2017 MATH \u00a0\n \n Google Scholar \u00a0\n Yan B N, Yang J H, Wu Q, et al. A closed-loop design to enhance weight stability of memristor based neural network chips. In: Proceedings of IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 2017 MATH \u00a0\n \n Google Scholar \u00a0\n Liu B Y, Hu M, Li H, et al. Digital-assisted noise-eliminating training for memristor crossbar-based analog neuromorphic computing engine. In: Proceedings of the 50th ACM/EDAC/IEEE Design Automation Conference (DAC), 2013 MATH \u00a0\n \n Google Scholar \u00a0\n He Z Z, Lin J, Ewetz R, et al. Noise injection adaption: end-to-end ReRAM crossbar non-ideal effect adaption for neural network mapping. In: Proceedings of the 56th Annual Design Automation Conference, 2019 MATH \u00a0\n \n Google Scholar \u00a0\n Chen P Y, Lin B B, Wang I T, et al. Mitigating effects of non-ideal synaptic device characteristics for on-chip learning. In: Proceedings of IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 2015 MATH \u00a0\n \n Google Scholar \u00a0\n Han L X, Xiang Y C, Huang P, et al. Novel weight mapping method for reliable NVM based neural network. In: Proceedings of IEEE International Reliability Physics Symposium (IRPS), 2021 MATH \u00a0\n \n Google Scholar \u00a0\n Liao Y, Gao B, Yao P, et al. Diagonal matrix regression layer: training neural networks on resistive crossbars with interconnect resistance effect. IEEE Trans Comput-Aided Des Integr Circ Syst, 2021, 40: 1662\u20131671 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Sung C, Lim S, Kim H, et al. Effect of conductance linearity and multi-level cell characteristics of TaO x -based synapse device on pattern recognition accuracy of neuromorphic system. Nanotechnology, 2018, 29: 115203 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Liu P, You Z, Wu J, et al. Fault modeling and efficient testing of memristor-based memory. IEEE Trans Circ Syst I, 2021, 68: 4444\u20134455 MATH \u00a0\n \n Google Scholar \u00a0\n Chen C Y, Shih H C, Wu C W, et al. RRAM defect modeling and failure analysis based on march test and a novel squeeze-search scheme. IEEE Trans Comput, 2015, 64: 180\u2013190 Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Chen A. A comprehensive crossbar array model with solutions for line resistance and nonlinear device characteristics. IEEE Trans Electron Dev, 2013, 60: 1318\u20131326 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Han R Z, Huang P, Zhao Y D, et al. Efficient evaluation model including interconnect resistance effect for large scale RRAM crossbar array matrix computing. Sci China Inf Sci, 2019, 62: 22401 Article \u00a0\n \n Google Scholar \u00a0\n Deng J, Dong W, Socher R, et al. ImageNet: a large-scale hierarchical image database. In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2009 MATH \u00a0\n \n Google Scholar \u00a0\n He K M, Zhang X Y, Ren S Q, et al. Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016 MATH \u00a0\n \n Google Scholar \u00a0\n Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. In: Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017 MATH \u00a0\n \n Google Scholar \u00a0\n Download references",
          "original_query": "Mitigating effects of non-ideal synaptic device characteristics for on-chip learning",
          "cleaned_query": "Mitigating effects of non-ideal synaptic device characteristics for on-chip learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Accurate deep neural network inference using computational phase ...",
          "url": "https://www.nature.com/articles/s41467-020-16108-9",
          "content": "Accurate deep neural network inference using computational phase-change memory | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-020-16108-9?error=cookies_not_supported&code=f761899c-58ee-4227-8655-448e3295cfe0)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nAccurate deep neural network inference using computational phase-change memory\n[Download PDF](https://www.nature.com/articles/s41467-020-16108-9.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-020-16108-9.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:18 May 2020# Accurate deep neural network inference using computational phase-change memory\n* [Vinay Joshi](#auth-Vinay-Joshi-Aff1-Aff2)[ORCID:orcid.org/0000-0001-6031-1669](https://orcid.org/0000-0001-6031-1669)[1](#Aff1),[2](#Aff2),\n* [Manuel Le Gallo](#auth-Manuel-Le_Gallo-Aff1)[ORCID:orcid.org/0000-0003-1600-6151](https://orcid.org/0000-0003-1600-6151)[1](#Aff1),\n* [Simon Haefeli](#auth-Simon-Haefeli-Aff1-Aff3)[1](#Aff1),[3](#Aff3),\n* [Irem Boybat](#auth-Irem-Boybat-Aff1-Aff4)[ORCID:orcid.org/0000-0002-4255-8622](https://orcid.org/0000-0002-4255-8622)[1](#Aff1),[4](#Aff4),\n* [S. R. Nandakumar](#auth-S__R_-Nandakumar-Aff1)[ORCID:orcid.org/0000-0002-7930-508X](https://orcid.org/0000-0002-7930-508X)[1](#Aff1),\n* [Christophe Piveteau](#auth-Christophe-Piveteau-Aff1-Aff3)[1](#Aff1),[3](#Aff3),\n* [Martino Dazzi](#auth-Martino-Dazzi-Aff1-Aff3)[1](#Aff1),[3](#Aff3),\n* [Bipin Rajendran](#auth-Bipin-Rajendran-Aff2)[2](#Aff2),\n* [Abu Sebastian](#auth-Abu-Sebastian-Aff1)[ORCID:orcid.org/0000-0001-5603-5243](https://orcid.org/0000-0001-5603-5243)[1](#Aff1)&amp;\n* \u2026* [Evangelos Eleftheriou](#auth-Evangelos-Eleftheriou-Aff1)[1](#Aff1)Show authors\n[*Nature Communications*](https://www.nature.com/ncomms)**volume11**, Article\u00a0number:2473(2020)[Cite this article](#citeas)\n* 42kAccesses\n* 444Citations\n* 62Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-020-16108-9/metrics)\n### Subjects\n* [Computer science](https://www.nature.com/subjects/computer-science)\n* [Electronic devices](https://www.nature.com/subjects/electronic-devices)\n## Abstract\nIn-memory computing using resistive memory devices is a promising non-von Neumann approach for making energy-efficient deep learning inference hardware. However, due to device variability and noise, the network needs to be trained in a specific way so that transferring the digitally trained weights to the analog resistive memory devices will not result in significant loss of accuracy. Here, we introduce a methodology to train ResNet-type convolutional neural networks that results in no appreciable accuracy loss when transferring weights to phase-change memory (PCM) devices. We also propose a compensation technique that exploits the batch normalization parameters to improve the accuracy retention over time. We achieve a classification accuracy of 93.7% on CIFAR-10 and a top-1 accuracy of 71.6% on ImageNet benchmarks after mapping the trained weights to PCM. Our hardware results on CIFAR-10 with ResNet-32 demonstrate an accuracy above 93.5% retained over a one-day period, where each of the 361,722 synaptic weights is programmed on just two PCM devices organized in a differential configuration.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-022-31405-1/MediaObjects/41467_2022_31405_Fig1_HTML.png)\n### [Optimised weight programming for analogue memory-based deep neural networks](https://www.nature.com/articles/s41467-022-31405-1?fromPaywallRec=false)\nArticleOpen access30 June 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-59815-x/MediaObjects/41467_2025_59815_Fig1_HTML.png)\n### [Two-dimensional materials based two-transistor-two-resistor synaptic kernel for efficient neuromorphic computing](https://www.nature.com/articles/s41467-025-59815-x?fromPaywallRec=false)\nArticleOpen access09 May 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-025-23303-5/MediaObjects/41598_2025_23303_Fig1_HTML.png)\n### [An energy efficient processor array and memory controller for accurate processing of convolutional neural network-based inference engines](https://www.nature.com/articles/s41598-025-23303-5?fromPaywallRec=false)\nArticleOpen access12 November 2025\n## Introduction\nDeep neural networks (DNNs) have revolutionized the field of artificial intelligence and have achieved unprecedented success in cognitive tasks such as image and speech recognition. Platforms for deploying the trained model of such networks and performing inference in an energy-efficient manner are highly attractive for edge computing applications. In particular, internet-of-things battery-powered devices and autonomous cars could especially benefit from fast, low-power, and reliably accurate DNN inference engines. Significant progress in this direction has been made with the introduction of specialized hardware for inference operating at reduced digital precision (4\u20138-bit), such as Google\u2019s tensor processing unit[1](https://www.nature.com/articles/s41467-020-16108-9#ref-CR1)and low-power graphical processing units such as NVIDIA T4[2](https://www.nature.com/articles/s41467-020-16108-9#ref-CR2). While these platforms are very flexible, they are based on architectures where there is a physical separation between memory and processing units. The models are typically stored in off-chip memory, leading to constant shuttling of data between memory and processing units, which limits the maximum achievable energy efficiency.\nIn order to reduce the data transfers to a minimum in inference accelerators, a promising avenue is to employ in-memory computing using non-volatile memory devices[3](#ref-CR3),[4](#ref-CR4),[5](https://www.nature.com/articles/s41467-020-16108-9#ref-CR5). Both charge-based storage devices, such as Flash memory[6](https://www.nature.com/articles/s41467-020-16108-9#ref-CR6), and resistance-based (memristive) storage devices, such as metal-oxide resistive random-access memory[7](#ref-CR7),[8](#ref-CR8),[9](#ref-CR9),[10](https://www.nature.com/articles/s41467-020-16108-9#ref-CR10)and phase-change memory (PCM)[11](#ref-CR11),[12](#ref-CR12),[13](#ref-CR13),[14](https://www.nature.com/articles/s41467-020-16108-9#ref-CR14)are being investigated for this. In this approach, the network weights are encoded as the analog charge state or conductance state of these devices organized in crossbar arrays, and the matrix-vector multiplications during inference can be performed in-situ in a single time step by exploiting Kirchhoff\u2019s circuit laws. The fact that these devices are non-volatile (the weights will be retained when the power supply is turned off) and have multi-level storage capability (a single device can encode an analog range of values as opposed to 1 bit) is very attractive for inference applications. However, due to the analog nature of the weights programmed in these devices, only limited precision can be achieved in the matrix-vector multiplications and this could limit the achievable inference accuracy of the accelerator.\nOne potential solution to this problem is to train the network fully on hardware[13](#ref-CR13),[14](#ref-CR14),[15](https://www.nature.com/articles/s41467-020-16108-9#ref-CR15), such that all hardware non-idealities would be de facto included as constraints during training. Another similar approach is to perform partial optimizations of the hardware weights after transferring a trained model to the chip[9](https://www.nature.com/articles/s41467-020-16108-9#ref-CR9),[16](https://www.nature.com/articles/s41467-020-16108-9#ref-CR16),[17](https://www.nature.com/articles/s41467-020-16108-9#ref-CR17). The drawback of these approaches is that every neural network would have to be trained on each individual chip before deployment. Off-line variation-aware training schemes have also been proposed, where hardware non-idealities such as device-to-device variations[18](https://www.nature.com/articles/s41467-020-16108-9#ref-CR18),[19](https://www.nature.com/articles/s41467-020-16108-9#ref-CR19), defective devices[19](https://www.nature.com/articles/s41467-020-16108-9#ref-CR19), or IR drop[18](https://www.nature.com/articles/s41467-020-16108-9#ref-CR18)are first characterized and then fed into the training algorithm running in software. However, these approaches would require characterizing and training the neural network from scratch for every chip. A more practical approach would be to have a single custom generic training algorithm run entirely in software, which would make the network immune to most of the hardware non-idealities, but at the same time would require only very little knowledge about the specific hardware it will be deployed on. In this way, the model would have to be trained only once and could be deployed on a multitude of different chips. To this end, several works have",
          "original_query": "Accurate deep neural network inference using computational phase-change memory",
          "cleaned_query": "Accurate deep neural network inference using computational phase-change memory",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Tile-Adaptive Residual Allocation via Online Optimality-Gap Estimation\n- Create an algorithm that dynamically decides (during training) how many residual tiles to allocate per layer based on an online estimate of the residual norm/optimality gap contributed by limited-state updates. Implement a controller that increases tiles only where the bound is tight, reducing hardware overhead while matching the accuracy of fixed multi-tile designs.",
        "Asymmetry-Calibrated Pulse Scheduling for Exact Analog SGD Emulation\n- Use device response-factor models (e.g., \\(q_+(w)\\), \\(q_-(w)\\)) to design a pulse scheduling rule that equalizes effective positive/negative update magnitudes at the array level. The contribution is a practically implementable \u201ccalibrated rank-update\u201d procedure that aims to recover the exact-gradient behavior without requiring Tiki-Taka\u2019s auxiliary dynamics, and includes a proof/measurement protocol to verify symmetry restoration.",
        "BatchNorm-as-Drift-Controller for On-chip Training (Not Just Inference)\n- Extend the PCM batch-normalization compensation idea from inference retention to active on-chip training where weights drift during optimization. Propose a coupled update rule where BN parameters (and possibly running statistics) are periodically adjusted to counteract measured conductance drift, and quantify how this changes convergence guarantees and final accuracy on long training runs.",
        "Defect- and Stuck-Cell\u2013Aware Residual Learning with Sparse Tile Masking\n- Introduce a residual-learning variant that explicitly models defects (stuck-at, dead devices, high-variance cells) and routes residual corrections around unreliable subarrays using structured sparsity masks. The key contribution is a training procedure that jointly learns weights and a hardware-feasible mask/placement plan, with guarantees on how defect rate impacts the number of tiles needed.",
        "Differential-Pair + Multi-tile Hybrid Mapping for 4-bit Devices\n- Combine differential synapse encoding (two-device per weight as in PCM inference) with multi-tile residual learning to increase effective resolution without requiring true multi-bit per device. Provide an actionable mapping and update scheme (including pulse polarity rules) and measure whether the hybrid reduces tile count for a given accuracy compared to multi-tile alone.",
        "Noise-Injected Digital Pretraining That Matches Analog Update Statistics\n- Build a digital pretraining pipeline that injects *update-asymmetry and limited-state quantization* noise directly into the optimizer dynamics (not only into forward weights), so that subsequent on-chip fine-tuning converges faster and with fewer tiles. The deliverable is a matched \u201canalog-aware optimizer emulator\u201d plus evidence that it reduces on-chip training steps/energy while preserving final accuracy after transfer.",
        "Closed-loop On-chip Metrology for Convergence Monitoring and Early Stopping\n- Design a lightweight on-chip diagnostic that periodically estimates (from readbacks and pulse counts) a proxy for the optimality gap or residual magnitude predicted by the multi-tile theory. Use this signal to trigger early stopping, tile activation/deactivation, or learning-rate changes, creating a practical control layer that ties theoretical convergence indicators to real-time hardware behavior."
      ],
      "generated_ideas_raw": "1. **Unified Convergence Theory for Tiki-Taka + Multi-tile Residual Learning**\n   - Develop a single optimization framework that combines Tiki-Taka\u2019s exact-convergence mechanism (mitigating asymmetric updates) with multi-tile residual learning\u2019s error-compensation for limited conductance states. Prove convergence to critical points under joint non-idealities (asymmetry + quantization) and derive tile-count vs. device-resolution scaling laws that predict when \u201cexact\u201d training is achievable on 4-bit-class devices.\n\n2. **Tile-Adaptive Residual Allocation via Online Optimality-Gap Estimation**\n   - Create an algorithm that dynamically decides (during training) how many residual tiles to allocate per layer based on an online estimate of the residual norm/optimality gap contributed by limited-state updates. Implement a controller that increases tiles only where the bound is tight, reducing hardware overhead while matching the accuracy of fixed multi-tile designs.\n\n3. **Asymmetry-Calibrated Pulse Scheduling for Exact Analog SGD Emulation**\n   - Use device response-factor models (e.g., \\(q_+(w)\\), \\(q_-(w)\\)) to design a pulse scheduling rule that equalizes effective positive/negative update magnitudes at the array level. The contribution is a practically implementable \u201ccalibrated rank-update\u201d procedure that aims to recover the exact-gradient behavior without requiring Tiki-Taka\u2019s auxiliary dynamics, and includes a proof/measurement protocol to verify symmetry restoration.\n\n4. **BatchNorm-as-Drift-Controller for On-chip Training (Not Just Inference)**\n   - Extend the PCM batch-normalization compensation idea from inference retention to active on-chip training where weights drift during optimization. Propose a coupled update rule where BN parameters (and possibly running statistics) are periodically adjusted to counteract measured conductance drift, and quantify how this changes convergence guarantees and final accuracy on long training runs.\n\n5. **End-to-End Co-design: Convergence-Rate\u2013Aware Hardware Cost Minimization**\n   - Formulate a constrained optimization that trades ADC/DAC precision, device-state count, and number of residual tiles against provable convergence rate and asymptotic error (from Paper 1/2 theory). Output a \u201chardware spec recommender\u201d that, given a target accuracy and energy budget, selects the cheapest configuration predicted to converge\u2014validated via circuit-level simulators (e.g., NeuroSim/MNSIM-style flows referenced in Paper 4).\n\n6. **Defect- and Stuck-Cell\u2013Aware Residual Learning with Sparse Tile Masking**\n   - Introduce a residual-learning variant that explicitly models defects (stuck-at, dead devices, high-variance cells) and routes residual corrections around unreliable subarrays using structured sparsity masks. The key contribution is a training procedure that jointly learns weights and a hardware-feasible mask/placement plan, with guarantees on how defect rate impacts the number of tiles needed.\n\n7. **Differential-Pair + Multi-tile Hybrid Mapping for 4-bit Devices**\n   - Combine differential synapse encoding (two-device per weight as in PCM inference) with multi-tile residual learning to increase effective resolution without requiring true multi-bit per device. Provide an actionable mapping and update scheme (including pulse polarity rules) and measure whether the hybrid reduces tile count for a given accuracy compared to multi-tile alone.\n\n8. **Layerwise \u201cExactness Budgeting\u201d for Transformers on Analog IMC**\n   - Use the asymptotic-error lower bound (Paper 1) to define a per-layer \u201cexactness budget\u201d and allocate mitigation techniques (Tiki-Taka dynamics, extra tiles, or higher-precision peripherals) preferentially to layers most sensitive to update bias (e.g., attention projections vs. MLP blocks). The contribution is a concrete recipe for training small-to-mid Transformers on AIMC with minimized overhead, validated by ablations that tie accuracy drops to predicted layer budgets.\n\n9. **Noise-Injected Digital Pretraining That Matches Analog Update Statistics**\n   - Build a digital pretraining pipeline that injects *update-asymmetry and limited-state quantization* noise directly into the optimizer dynamics (not only into forward weights), so that subsequent on-chip fine-tuning converges faster and with fewer tiles. The deliverable is a matched \u201canalog-aware optimizer emulator\u201d plus evidence that it reduces on-chip training steps/energy while preserving final accuracy after transfer.\n\n10. **Closed-loop On-chip Metrology for Convergence Monitoring and Early Stopping**\n   - Design a lightweight on-chip diagnostic that periodically estimates (from readbacks and pulse counts) a proxy for the optimality gap or residual magnitude predicted by the multi-tile theory. Use this signal to trigger early stopping, tile activation/deactivation, or learning-rate changes, creating a practical control layer that ties theoretical convergence indicators to real-time hardware behavior.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Tile-Adaptive Residual Allocation via Online Optimality-Gap Estimation\n- Create an algorithm that dynamically decides (during training) how many residual tiles to allocate per layer based on an online",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Asymmetry-Calibrated Pulse Scheduling for Exact Analog SGD Emulation\n- Use device response-factor models (e.g., \\(q_+(w)\\), \\(q_-(w)\\)) to design a pulse scheduling rule that equalizes effective posit",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "BatchNorm-as-Drift-Controller for On-chip Training (Not Just Inference)\n- Extend the PCM batch-normalization compensation idea from inference retention to active on-chip training where weights drift d",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Defect- and Stuck-Cell\u2013Aware Residual Learning with Sparse Tile Masking\n- Introduce a residual-learning variant that explicitly models defects (stuck-at, dead devices, high-variance cells) and routes ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Differential-Pair + Multi-tile Hybrid Mapping for 4-bit Devices\n- Combine differential synapse encoding (two-device per weight as in PCM inference) with multi-tile residual learning to increase effect",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Noise-Injected Digital Pretraining That Matches Analog Update Statistics\n- Build a digital pretraining pipeline that injects *update-asymmetry and limited-state quantization* noise directly into the o",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Closed-loop On-chip Metrology for Convergence Monitoring and Early Stopping\n- Design a lightweight on-chip diagnostic that periodically estimates (from readbacks and pulse counts) a proxy for the opti",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 25,
      "paper_title": "Discovering Opinion Intervals from Conflicts in Signed Graphs",
      "contribution": "Introduce and study the problem of recovering a small set of interpretable opinion intervals on a line that explain the positive/negative edges of a signed graph, prove hardness results, derive a polynomial-time approximation scheme by connecting the model to interval/indifference graphs and correlation clustering, and provide scalable heuristics with empirical validation.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 9563,
      "output_tokens": 1056,
      "predecessor_details": [
        {
          "success": true,
          "title": "On the notion of balance of a signed graph.",
          "url": "https://projecteuclid.org/journals/michigan-mathematical-journal/volume-2/issue-2/On-the-notion-of-balance-of-a-signed-graph/10.1307/mmj/1028989917.full",
          "content": "Email\n\nPassword [Forgot your password?](https://projecteuclid.org/)\n\nShow\n\nRemember Email on this computer\n\nRemember Password\n\n![](https://projecteuclid.org/Content/themes/SPIEImages/Loading.gif)Please wait...\n\nNo Project Euclid account? [Create an account](https://projecteuclid.org/)\n\nor [Sign in with your institutional credentials](https://projecteuclid.org/Account/institutionalsignin?redirect=https%3a%2f%2fprojecteuclid.org%2fjournals%2fmichigan-mathematical-journal%2fvolume-2%2fissue-2%2fOn-the-notion-of-balance-of-a-signed-graph%2f10.1307%2fmmj%2f1028989917.full)\n\nWe can help you reset your password using the email address linked to your Project Euclid account.\n\nEmail\n\nRegistered users receive a variety of benefits including the ability to customize email alerts, create favorite journals list, and save searches.\nPlease note that a Project Euclid web account does not automatically grant access to full-text content. An institutional or society member subscription is required to view non-Open Access content.\nContact [customer\\_support@projecteuclid.org](mailto:customer_support@projecteuclid.org) with any questions.\n\n[View Project Euclid Privacy Policy](https://projecteuclid.org/policies)\n\nAll Fields are Required\n\n\\*First Name\n\n\\*Last/Family Name\n\n\\*Email\n\n\\*Password\n\nPassword Requirements: Minimum 8 characters, must include as least one uppercase, one lowercase letter, and one number or permitted symbol\nValid Symbols for password:\n\n~ Tilde\n\n! Exclamation Mark\n\n@ At sign\n\n$ Dollar sign\n\n^ Caret\n\n( Opening Parenthesis\n\n) Closing Parenthesis\n\n\\_ Underscore\n\n. Period\n![](https://projecteuclid.org/Content/themes/SPIEImages/InformationQuestionMark.png)\n\n\\*Confirm Password\n\n![](https://projecteuclid.org/Content/themes/SPIEImages/Loading.gif)Please wait...\n\nWeb Account created successfully\n\n![Open Access](https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg)\n\n1953/1954On the notion of balance of a signed graph.\n\nFrank Harary\n\nMichigan Math. J. 2(2): 143-146 (1953/1954). DOI: 10.1307/mmj/1028989917\n\n- ABOUT\n- FIRST PAGE\n- CITED BY\n- [DOWNLOAD PAPER](https://projecteuclid.org/journalArticle/Download?urlid=10.1307%2Fmmj%2F1028989917) [SAVE TO MY LIBRARY](https://projecteuclid.org/)\n\n\nPERSONAL SIGN IN\n\nFull access may be available with your subscription\n\nEmail\n\nPasswordForgot your password?\n\nShow\n\nRemember Email on this computer\n\nRemember Password\n\nNo Project Euclid account? [Create an account](https://projecteuclid.org/)\n\nor [Sign in with your institutional credentials](https://projecteuclid.org/Account/institutionalsignin?redirect=https%3a%2f%2fprojecteuclid.org%2fjournals%2fmichigan-mathematical-journal%2fvolume-2%2fissue-2%2fOn-the-notion-of-balance-of-a-signed-graph%2f10.1307%2fmmj%2f1028989917.full)\n\nPURCHASE SINGLE ARTICLE\n\nThis article is only available to [subscribers](https://projecteuclid.org/subscriptions-and-access). It is not available for individual sale.\n\nThis will count as one of your downloads.\n\nYou will have access to both the presentation and article (if available).\n\n[DOWNLOAD NOW](https://projecteuclid.org/)\n\nThis content is available for download via your institution's subscription. To access this item, please sign in to your personal account.\n\nEmail\n\nPasswordForgot your password?\n\nShow\n\nRemember Email on this computer\n\nRemember Password\n\nNo Project Euclid account? [Create an account](https://projecteuclid.org/)\n\nMy Library\n\nYou currently do not have any folders to save your paper to! Create a new folder below.\n\nCreate New Folder\n\nSAVE >\n\nFirst Page PDF\n\nSorry, your browser doesn't support embedded PDFs, [Download First Page](https://projecteuclid.org/JournalArticle/PreviewFirstPage?urlid=10.1307%2fmmj%2f1028989917)\n\nJOURNAL ARTICLE\n\n4 PAGES\n\n* * *\n\n[DOWNLOAD PDF](https://projecteuclid.org/journalArticle/Download?urlId=10.1307%2Fmmj%2F1028989917)+SAVE TO MY LIBRARY\n\n* * *\n\n![](https://projecteuclid.org/content/themes/spieimages/GetCitation.png)\nGET CITATION\n\nMy Library\n\nYou currently do not have any folders to save your paper to! Create a new folder below.\n\nCreate New Folder\n\nSAVE >\n\nFolder Name\n\nFolder Description\n\nSAVE\n\n< [Previous Article](https://projecteuclid.org/journals/michigan-mathematical-journal/volume-2/issue-2/Order-of-magnitude-of-Fourier-transforms/10.1307/mmj/1028989916.full)\n\n\\|\n\n[Next Article](https://projecteuclid.org/journals/michigan-mathematical-journal/volume-2/issue-2/On-a-conjecture-of-Erd%c5%91s-Herzog-and-Piranian/10.1307/mmj/1028989918.full) >\n\n[**Michigan Math. J.**](https://projecteuclid.org/journals/michigan-mathematical-journal/volume-2/issue-2)\n\nVol.2 \u2022 No. 2 \u2022 1953/1954\n\n![](https://projecteuclid.org/images/journals/cover_mmj.jpg)\n\n[University of Michigan, Department of Mathematics](https://projecteuclid.org/publishers/University-of-Michigan-Department-of-Mathematics)\n\n[![](https://projecteuclid.org/images/publishers/thumbnail/publisher_umich.png)](https://projecteuclid.org/publishers/University-of-Michigan-Department-of-Mathematics)\n\n[![](https://projecteuclid.org/Content/themes/SPIEImages/Subscribe_Check.png)Subscribe to Project Euclid](https://projecteuclid.org/subscribe)\n\n[![](https://projecteuclid.org/Content/themes/SPIEImages/Alerts_Mark.png)Receive erratum alerts for this article](https://projecteuclid.org/)\n\nFrank Harary \"On the notion of balance of a signed graph.,\" Michigan Mathematical Journal, Michigan Math. J. 2(2), 143-146, (1953/1954)\n\nInclude:\n\nCitation Only\n\nCitation & Abstract\n\nFormat:\n\nRIS\n\nEndNote\n\nBibTex\n\nPrint Friendly Version (PDF)\n\nDOWNLOAD CITATION\n\n[![Back to Top](https://projecteuclid.org/Images/Project%20Euclid/Back-Top_Icon.png)](https://projecteuclid.org/#top)\n\n## KEYWORDS/PHRASES\n\nKeywords\n\nin\n\nAll FieldsAbstractAuthor NameAffiliationDOI/ISSN/ISBNFigure & Table CaptionsKeywordsTitle\n\nRemove\n\nANDORNOT\n\nin\n\nAll FieldsAbstractAuthor NameAffiliationDOI/ISSN/ISBNFigure & Table CaptionsKeywordsTitle\n\nRemove\n\nANDORNOT\n\nin\n\nAll FieldsAbstractAuthor NameAffiliationDOI/ISSN/ISBNFigure & Table CaptionsKeywordsTitle\n\nRemove\n\n\\+ Add another field\n\n* * *\n\n## PUBLICATION TITLE:\n\nAll Titles\n\nChoose Title(s)\n\nSelect a title\n\n* * *\n\n## PUBLICATION YEARS\n\nRange\n\nSingle Year\n\n* * *\n\nClear Form",
          "original_query": "On the notion of balance of a signed graph",
          "cleaned_query": "On the notion of balance of a signed graph",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Correlation clustering",
          "url": "https://en.wikipedia.org/wiki/Correlation_clustering",
          "content": "[Jump to content](https://en.wikipedia.org/en.wikipedia.org#bodyContent)\n\nFrom Wikipedia, the free encyclopedia\n\nMethod of partitioning data points into groups based on their similarity\n\nClustering is the problem of partitioning data points into groups based on their similarity. **Correlation clustering** provides a method for clustering a set of objects into the optimum number of clusters without specifying that number in advance.[\\[1\\]](https://en.wikipedia.org/en.wikipedia.org#cite_note-1)\n\n## Description of the problem\n\n\\[ [edit](https://en.wikipedia.org/w/index.php?title=Correlation_clustering&action=edit&section=1)\\]\n\nMain article: [Cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis)\n\nIn [machine learning](https://en.wikipedia.org/wiki/Machine_learning), **correlation clustering** or **cluster editing** operates in a scenario where the relationships between the objects are known instead of the actual representations of the objects. For example, given a [weighted graph](https://en.wikipedia.org/wiki/Weighted_graph) G=(V,E){\\\\displaystyle G=(V,E)} where the edge weight indicates whether two nodes are similar (positive edge weight) or different (negative edge weight), the task is to find a clustering that either maximizes agreements (sum of positive edge weights within a cluster plus the [absolute value](https://en.wikipedia.org/wiki/Absolute_value) of the sum of negative edge weights between clusters) or minimizes disagreements (absolute value of the sum of negative edge weights within a cluster plus the sum of positive edge weights across clusters). Unlike other clustering algorithms this does not require [choosing the number of clusters](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set) k{\\\\displaystyle k} in advance because the objective, to minimize the sum of weights of the cut edges, is independent of the number of clusters.\n\nIt may not be possible to find a perfect clustering, where all similar items are in a cluster while all dissimilar ones are in different clusters. If the graph indeed admits a perfect clustering, then simply deleting all the negative edges and finding the connected components in the remaining graph will return the required clusters. Davis found a necessary and sufficient condition for this to occur: no cycle may contain exactly one negative edge.[\\[2\\]](https://en.wikipedia.org/en.wikipedia.org#cite_note-2)\n\nBut, in general a graph may not have a perfect clustering. For example, given nodes _a,b,c_ such that _a,b_ and _a,c_ are similar while _b,c_ are dissimilar, a perfect clustering is not possible. In such cases, the task is to find a clustering that maximizes the number of agreements (number of + edges inside clusters plus the number of \u2212 edges between clusters) or minimizes the number of disagreements (the number of \u2212 edges inside clusters plus the number of + edges between clusters). This problem of maximizing the agreements is NP-complete (multiway cut problem reduces to maximizing weighted agreements and the problem of partitioning into triangles[\\[3\\]](https://en.wikipedia.org/en.wikipedia.org#cite_note-3) can be reduced to the unweighted version).\n\n## Formal Definitions\n\n\\[ [edit](https://en.wikipedia.org/w/index.php?title=Correlation_clustering&action=edit&section=2)\\]\n\nLet G=(V,E){\\\\displaystyle G=(V,E)} be a graph with nodes V{\\\\displaystyle V} and edges E{\\\\displaystyle E}. A clustering of G{\\\\displaystyle G} is a partition of its node set \u03a0={\u03c01,\u2026,\u03c0k}{\\\\displaystyle \\\\Pi =\\\\{\\\\pi \\_{1},\\\\dots ,\\\\pi \\_{k}\\\\}} with V=\u03c01\u222a\u22ef\u222a\u03c0k{\\\\displaystyle V=\\\\pi \\_{1}\\\\cup \\\\dots \\\\cup \\\\pi \\_{k}} and \u03c0i\u2229\u03c0j=\u2205{\\\\displaystyle \\\\pi \\_{i}\\\\cap \\\\pi \\_{j}=\\\\emptyset } for i\u2260j{\\\\displaystyle i\\\\neq j}.\nFor a given clustering \u03a0{\\\\displaystyle \\\\Pi }, let \u03b4(\u03a0)={{u,v}\u2208E\u2223{u,v}\u2288\u03c0\u2200\u03c0\u2208\u03a0}{\\\\displaystyle \\\\delta (\\\\Pi )=\\\\{\\\\{u,v\\\\}\\\\in E\\\\mid \\\\{u,v\\\\}\\\\not \\\\subseteq \\\\pi \\\\;\\\\forall \\\\pi \\\\in \\\\Pi \\\\}} denote the subset of edges of G{\\\\displaystyle G} whose endpoints are in different subsets of the clustering \u03a0{\\\\displaystyle \\\\Pi }.\nNow, let w:E\u2192R\u22650{\\\\displaystyle w\\\\colon E\\\\to \\\\mathbb {R} \\_{\\\\geq 0}} be a function that assigns a non-negative weight to each edge of the graph and let E=E+\u222aE\u2212{\\\\displaystyle E=E^{+}\\\\cup E^{-}} be a partition of the edges into attractive (E+{\\\\displaystyle E^{+}}) and repulsive (E\u2212{\\\\displaystyle E^{-}}) edges.\n\nThe minimum disagreement correlation clustering problem is the following [optimization problem](https://en.wikipedia.org/wiki/Optimization_problem):\nminimize\u03a0\u2211e\u2208E+\u2229\u03b4(\u03a0)we+\u2211e\u2208E\u2212\u2216\u03b4(\u03a0)we.{\\\\displaystyle {\\\\begin{aligned}&{\\\\underset {\\\\Pi }{\\\\operatorname {minimize} }}&&\\\\sum \\_{e\\\\in E^{+}\\\\cap \\\\delta (\\\\Pi )}w\\_{e}+\\\\sum \\_{e\\\\in E^{-}\\\\setminus \\\\delta (\\\\Pi )}w\\_{e}\\\\;.\\\\end{aligned}}}\nHere, the set E+\u2229\u03b4(\u03a0){\\\\displaystyle E^{+}\\\\cap \\\\delta (\\\\Pi )} contains the attractive edges whose endpoints are in different components with respect to the clustering \u03a0{\\\\displaystyle \\\\Pi } and the set E\u2212\u2216\u03b4(\u03a0){\\\\displaystyle E^{-}\\\\setminus \\\\delta (\\\\Pi )} contains the repulsive edges whose endpoints are in the same component with respect to the clustering \u03a0{\\\\displaystyle \\\\Pi }.\nTogether these two sets contain all edges that disagree with the clustering \u03a0{\\\\displaystyle \\\\Pi }.\n\nSimilarly to the minimum disagreement correlation clustering problem, the maximum agreement correlation clustering problem is defined as\nmaximize\u03a0\u2211e\u2208E+\u2216\u03b4(\u03a0)we+\u2211e\u2208E\u2212\u2229\u03b4(\u03a0)we.{\\\\displaystyle {\\\\begin{aligned}&{\\\\underset {\\\\Pi }{\\\\operatorname {maximize} }}&&\\\\sum \\_{e\\\\in E^{+}\\\\setminus \\\\delta (\\\\Pi )}w\\_{e}+\\\\sum \\_{e\\\\in E^{-}\\\\cap \\\\delta (\\\\Pi )}w\\_{e}\\\\;.\\\\end{aligned}}}\nHere, the set E+\u2216\u03b4(\u03a0){\\\\displaystyle E^{+}\\\\setminus \\\\delta (\\\\Pi )} contains the attractive edges whose endpoints are in the same component with respect to the clustering \u03a0{\\\\displaystyle \\\\Pi } and the set E\u2212\u2229\u03b4(\u03a0){\\\\displaystyle E^{-}\\\\cap \\\\delta (\\\\Pi )} contains the repulsive edges whose endpoints are in different components with respect to the clustering \u03a0{\\\\displaystyle \\\\Pi }.\nTogether these two sets contain all edges that agree with the clustering \u03a0{\\\\displaystyle \\\\Pi }.\n\nInstead of formulating the correlation clustering problem in terms of non-negative edge weights and a partition of the edges into attractive and repulsive edges the problem is also formulated in terms of positive and negative edge costs without partitioning the set of edges explicitly.\nFor given weights w:E\u2192R\u22650{\\\\displaystyle w\\\\colon E\\\\to \\\\mathbb {R} \\_{\\\\geq 0}} and a given partition E=E+\u222aE\u2212{\\\\displaystyle E=E^{+}\\\\cup E^{-}} of the edges into attractive and repulsive edges, the edge costs can be defined by\nce={weif\u00a0e\u2208E+\u2212weif\u00a0e\u2208E\u2212{\\\\displaystyle {\\\\begin{aligned}c\\_{e}={\\\\begin{cases}\\\\;\\\\;w\\_{e}&{\\\\text{if }}e\\\\in E^{+}\\\\\\-w\\_{e}&{\\\\text{if }}e\\\\in E^{-}\\\\end{cases}}\\\\end{aligned}}}\nfor all e\u2208E{\\\\displaystyle e\\\\in E}.\n\nAn edge whose endpoints are in different clusters is said to be cut.\nThe set \u03b4(\u03a0){\\\\displaystyle \\\\delta (\\\\Pi )} of all edges that are cut is often called a multicut[\\[4\\]](https://en.wikipedia.org/en.wikipedia.org#cite_note-4) of G{\\\\displaystyle G}.\n\nThe minimum cost multicut problem is the problem of finding a clustering \u03a0{\\\\displaystyle \\\\Pi } of G{\\\\displaystyle G} such that the sum of the costs of the edges whose endpoints are in different clusters is minimal:\nminimize\u03a0\u2211e\u2208\u03b4(\u03a0)ce.{\\\\displaystyle {\\\\begin{aligned}&{\\\\underset {\\\\Pi }{\\\\operatorname {minimize} }}&&\\\\sum \\_{e\\\\in \\\\delta (\\\\Pi )}c\\_{e}\\\\;.\\\\end{aligned}}}\n\nSimilar to the minimum cost multicut problem, coalition structure generation in weighted graph games[\\[5\\]](https://en.wikipedia.org/en.wikipedia.org#cite_note-5) is the problem of finding a clustering such that the sum of the costs of the edges that are not cut is maximal:\nmaximize\u03a0\u2211e\u2208E\u2216\u03b4(\u03a0)ce.{\\\\displaystyle {\\\\begin{aligned}&{\\\\underset {\\\\Pi }{\\\\operatorname {maximize} }}&&\\\\sum \\_{e\\\\in E\\\\setminus \\\\delta (\\\\Pi )}c\\_{e}\\\\;.\\\\end{aligned}}}\nThis formulation is also known as the clique partitioning problem.[\\[6\\]](https://en.wikipedia.org/en.wikipedia.org#cite_note-6)\n\nIt can be shown that all four problems that are formulated above are equivalent.\nThis means that a clustering that is optimal with respect to any of the four objectives is optimal for all of the four objectives.\n\n## Algorithms\n\n\\[ [edit](https://en.wikipedia.org/w/index.php?title=Correlation_clustering&action=edit&section=3)\\]\n\nBansal et al.[\\[7\\]](https://en.wikipedia.org/en.wikipedia.org#cite_note-7) discuss the NP-completeness proof and also present both a constant factor approximation algorithm and [polynomial-time approximation scheme](https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme) to find the clusters in this setting. Ailon et al.[\\[8\\]](https://en.wikipedia.org/en.wikipedia.org#cite_note-8) propose a randomized 3- [approximation algorithm](https://en.wikipedia.org/wiki/Approximation_algorithm) for the same problem.\n\n```\nCC-Pivot(G=(V,E+,E\u2212))\n Pick random pivot i \u2208 V\n Set C\n =\n {\n i\n }\n {\\displaystyle C=\\{i\\}}, V'=\u00d8\n For all j \u2208 V, j \u2260 i;\n If (i,j) \u2208 E+ then\n Add j to C\n Else (If (i,j) \u2208 E\u2212)\n Add j to V'\n Let G' be the subgraph induced by V'\n Return clustering C,CC-Pivot(G')\n\n```\n\nThe authors show that the above algorithm is a 3- [approximation algorithm](https://en.wikipedia.org/wiki/Approximation_algorithm) for correlation clustering. The best polynomial-time approximation algorithm known at the moment for this problem achieves a ~2.06 approximation by rounding a linear program, as shown by [Chawla](https://en.wikipedia.org/wiki/Shuchi_Chawla), Makarychev, Schramm, and [Yaroslavtsev](https://en.wikipedia.org/wiki/Grigory_Yaroslavtsev).[\\[9\\]](https://en.wikipedia.org/en.wikipedia.org#cite_note-9)\n\nKarpinski and Schudy[\\[10\\]](https://en.wikipedia.org/en.wikipedia.org#cite_note-10) proved existence of a [polynomial time](https://en.wikipedia.org/wiki/Time_complexity) approximation scheme (PTAS) for that problem on complete graphs and fixed number of cluster",
          "original_query": "Correlation Clustering",
          "cleaned_query": "Correlation Clustering",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Correlation Clustering with a Fixed Number of Clusters",
          "url": "http://www.cs.cmu.edu/~venkatg/pubs/papers/corrclus-k.pdf",
          "content": "Correlation Clustering with a Fixed Number of Clusters\u2217\nIoannis Giotis\u2020 Venkatesan Guruswami\u2217\u2021\nAbstract\nWe continue the investigation of problems concerning correlation clustering or clustering with\nqualitative information, which is a clustering formulation that has been studied recently [5, 7,\n8, 3]. The basic setup here is that we are given as input a complete graph on n nodes (which\ncorrespond to nodes to be clustered) whose edges are labeled + (for similar pairs of items) and \u2212\n(for dissimilar pairs of items). Thus we have only as input qualitative information on similarity\nand no quantitative distance measure between items. The quality of a clustering is measured in\nterms of its number of agreements, which is simply the number of edges it correctly classifies,\nthat is the sum of number of \u2212 edges whose endpoints it places in different clusters plus the\nnumber of + edges both of whose endpoints it places within the same cluster.\nIn this paper, we study the problem of finding clusterings that maximize the number of\nagreements, and the complementary minimization version where we seek clusterings that min\u0002imize the number of disagreements. We focus on the situation when the number of clusters is\nstipulated to be a small constant k. Our main result is that for every k, there is a polynomial\ntime approximation scheme for both maximizing agreements and minimizing disagreements.\n(The problems are NP-hard for every k \u2265 2.) The main technical work is for the minimization\nversion, as the PTAS for maximizing agreements follows along the lines of the property tester\nfor Max k-CUT from [13].\nIn contrast, when the number of clusters is not specified, the problem of minimizing dis\u0002agreements was shown to be APX-hard [7], even though the maximization version admits a\nPTAS.\n1 Introduction\nIn this work, we investigate problems concerning an appealing formulation of clustering called\ncorrelation clustering or clustering using qualitative information that has been studied recently in\nseveral works, including [6, 17, 5, 7, 8, 3, 2, 4]. The basic setup here is to cluster a collection of\nn items given as input only qualitative information concerning similarity between pairs of items;\nspecifically for every pair of items, we are given a (Boolean) label as to whether those items are\nsimilar or dissimilar. We are not provided with any quantitative information on how different pairs\nof elements are, as is typically assumed in most clustering formulations. These formulations take as\ninput a metric on the items and then aim to optimize some function of the pairwise distances of the\nitems within and across clusters. The objective in our formulation is to produce a partitioning into\n\u2217A version of this paper will appear in the Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete\nAlgorithms (SODA), January 2006.\n\u2020Department of Computer Science and Engineering, University of Washington, Seattle, WA 98195.\n{giotis,venkat}@cs.washington.edu\n\u2021Research supported in part by NSF Career Award CCF-0343672.\n1\nclusters that places similar objects in the same cluster and dissimilar objects in different clusters,\nto the extent possible.\nAn obvious graph-theoretic formulation of the problem is the following: given a complete graph\non n nodes with each edge labeled either \u201c+\u201d (similar) or \u201c\u2212\u201d (dissimilar), find a partitioning of\nthe vertices into clusters that agrees as much as possible with the edge labels. The maximization\nversion, call it MaxAgree, seeks to maximize the number of agreements: the number of + edges\ninside clusters plus the number of \u2212 edges across clusters. The minimization version, denoted\nMinDisAgree, aims to minimize the number of disagreements: the number of \u2212 edges within\nclusters plus the number of + edges between clusters.\nIn this paper, we study the above problems when the maximum number of clusters that we\nare allowed to use is stipulated to be a fixed constant k. We denote the variants of the above\nproblems that have this constraint as MaxAgree[k] and MinDisAgree[k]. We note that, unlike\nmost clustering formulations, the MaxAgree and MinDisAgree problems are not trivialized if\nwe do not specify the number of clusters k as a parameter \u2014 whether the best clustering uses few\nor many clusters is automatically dictated by the edge labels. However, the variants we study are\nalso interesting formulations, which are well-motivated in settings where the number of clusters\nmight be an external constraint that has to be met, even if there are \u201cbetter\u201d clusterings (i.e., one\nwith more agreements) with a different number of clusters. Moreover, the existing algorithms for,\nsay MinDisAgree, cannot be modified in any easy way to output a quality solution with at most\nk clusters. Therefore k-clustering variants pose new, non-trivial challenges that require different\ntechniques for their solutions.\nIn the above description, we have assumed that every pair of items is labeled as + or \u2212 in the\ninput. In a more general variant, intended to capture situations where the classifier providing the\ninput might be unable to label certain pairs of elements are similar or dissimilar, the input is an\narbitrary graph G together with \u00b1 labels on its edges. We can again study the above problems\nMaxAgree[k] (resp. MinDisAgree[k]) with the objective being to maximize (resp. minimize)\nthe number of agreements (resp. disagreements) on edges of E (that is, we do not count non\u0002edges of G as either agreements or disagreements). In situations where we study this more general\nvariant, we will refer to these problems as MaxAgree[k] on general graphs and MinDisAgree[k]\non general graphs. When we don\u2019t qualify with the phrase \u201con general graphs\u201d, we will always\nmean the problems on complete graphs.\nOur main result in this paper is a polynomial time approximation scheme (PTAS) for MaxAgree[k]\nas well as MinDisAgree[k] for k \u2265 2. We now discuss prior work on these problems, followed by\na more detailed description of results in this paper.\n1.1 Previous and related work\nThe above problem seems to have been first considered by Ben-Dor et al. [6] motivated by some com\u0002putational biology questions. Later, Shamir et al. [17] studied the computational complexity of the\nproblem and showed that MaxAgree (and hence also MinDisAgree), as well as MaxAgree[k]\n(and hence also MinDisAgree[k]) for each k \u2265 2 is NP-hard. They, however, used the term\n\u201cCluster Editing\u201d to refer to this problem.\nPartially motivated by some machine learning problems concerning document classification,\nBansal, Blum, and Chawla [5] also independently formulated and considered this problem. In par\u0002ticular, they initiated the study of approximate solutions to MinDisAgree and MaxAgree, and\npresented a PTAS for MaxAgree and a constant factor approximation algorithm for MinDis\u00022\nAgree (the approximation guarantee was a rather large constant, though). They also noted a\nsimple factor 3 approximation algorithm for MinDisAgree[2]. Charikar, Guruswami and Wirth [7]\nproved that MinDisAgree is APX-hard, and thus one cannot expect a PTAS for the minimization\nproblem similar to the PTAS for MaxAgree. They also gave a factor 4 approximation algorithm\nfor MinDisAgree by rounding a natural LP relaxation using the region growing technique. Re\u0002cently, Ailon et al [2] presented an elegant combinatorial factor 3 approximation algorithm with a\nclever analysis for MinDisAgree; they also get a factor 5/2 approximation using LP techniques\non top of their basic approach.\nThe problems on general graphs have also received attention. It is known that both MaxA\u0002gree and MinDisAgree are APX-hard [5, 7]. Using a connection to minimum multicut, several\ngroups [7, 11, 12] presented an O(log n) approximation algorithm for MinDisAgree. In fact, it was\nnoted in [12] that the problem is as hard to approximate as minimum multicut (and so this log n\nfactor seems very hard to improve). For the maximization version, algorithms with performance\nratio better than 0.766 are known for MaxAgree [7, 18]. The latter work by Swamy [18] shows\nthat a factor 0.7666 approximation can also be achieved when the number of clusters is specified\n(i.e., for MaxAgree[k] for k \u2265 2).\nAnother problem that has been considered, let us call it MaxCorr, is that of maximizing\ncorrelation, defined to be the difference between the number of agreements and disagreements.\nA factor O(log n) approximation for MaxCorr on complete graphs is presented in [16, 8], and\nan O(log \u03b8(G)) approximation is presented in [3] for general graphs G, where \u03b8(\u00b7) is the Lov\u00b4asz\nTheta Function. Alon et al [3] showed an integrality gap of \u2126(log n) for the standard semidefi\u0002nite program relaxation for MaxCorr (the largest such integrality gap for a graph is called the\nGrothendieck constant of the graph \u2014 thus these results establish the Grothendieck constant of the\ncomplete graph on n vertices to be \u0398(log n)). Very recently, Arora et al [4] proved a factor log\u03b1 n\ninapproximability result for the weighted version of MaxCorr for some \u03b1 > 0.\n1.2 Our results\nThe only previous approximation for MinDisAgree[k] was a factor 3 approximation algorithm for\nthe case k = 2 [5]. The problems were shown to be NP-hard for every k \u2265 2 in [17] using a rather\ncomplicated reduction. In this paper, we will provide a much simpler NP-hardness proof and prove\nthat both MaxAgree[k] and MinDisAgree[k] admit a polynomial time approximation scheme\nfor every k \u2265 2.1 The existence of a PTAS for MinDisAgree[k] is perhaps surprising in light of\nthe APX-hardness of MinDisAgree when the number of clusters is not specified to be a constant\n(recall that the maximization version does admit a PTAS even when k is not specified).\nIt is often the case that minimization versions of problems are harder to solve compared to their\ncomplementary maximization versions. The APX-hardness of MinDisAgree despite the existence\nof a PTAS for MaxAgree is a notable example. The difficulty in these cases is when the optimum\nvalue of the minimization v",
          "original_query": "Correlation clustering with a fixed number of clusters",
          "cleaned_query": "Correlation clustering with a fixed number of clusters",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Can Everybody Sit Closer to Their Friends Than Their Enemies?",
          "url": "https://link.springer.com/chapter/10.1007/978-3-642-22993-0_36",
          "content": "\n \n \n Abstract \n Signed graphs are graphs with signed edges. They are commonly used to represent positive and negative relationships in social networks. While balance theory and clusterizable graphs deal with signed graphs, recent empirical studies have proved that they fail to reflect some current practices in real social networks. In this paper we address the issue of drawing signed graphs and capturing such social interactions. We relax the previous assumptions to define a drawing as a model in which every vertex has to be placed closer to its neighbors connected through a positive edge than its neighbors connected through a negative edge in the resulting space. Based on this definition, we address the problem of deciding whether a given signed graph has a drawing in a given \u2113-dimensional Euclidean space. We focus on the 1-dimensional case, where we provide a polynomial time algorithm that decides if a given complete signed graph has a drawing, and provides it when applicable. This work has been supported by the ERC Starting research grant GOSSPLE number 204742, Comunidad de Madrid grant S2009TIC-1692 and Spanish MICINN grant TIN2008\u201306735-C02-01. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Preview \n \n Unable to display preview.\u00a0 Download preview\n PDF. \n \n \n \n \n \n \n \n Similar content being viewed by others \n \n \n \n \n \n \n \n References Antal, T., Krapivsky, P.L., Redner, S.: Dynamics of social balance on networks. Phys. Rev. E\u00a072(3), 036121 (2005) Article \u00a0\n MathSciNet \u00a0\n \n Google Scholar \u00a0\n Bansal, N., Blum, A., Chawla, S.: Correlation clustering. Machine Learning\u00a056(1-3), 89\u2013113 (2004) Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Brandes, U., Fleischer, D., Lerner, J.: Summarizing dynamic bipolar conflict structures. IEEE Trans. Vis. Comput. Graph.\u00a012(6), 1486\u20131499 (2006) Article \u00a0\n \n Google Scholar \u00a0\n Cartwright, D., Harary, F.: Structural balance: a generalization of heider\u2019s theory. Psychological Review\u00a063(5), 277\u2013293 (1956) Article \u00a0\n \n Google Scholar \u00a0\n Davis, J.A.: Clustering and structural balance in graphs. Human Relations\u00a020(2), 181 (1967) Article \u00a0\n \n Google Scholar \u00a0\n Habib, M., McConnell, R.M., Paul, C., Viennot, L.: Lex-bfs and partition refinement, with applications to transitive orientation, interval graph recognition and consecutive ones testing. Theor. Comput. Sci.\u00a0234(1-2), 59\u201384 (2000) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Harary, F.: On the notion of balance of a signed graph. Michigan Mathematical Journal\u00a02(2), 143 (1953) Article \u00a0\n MathSciNet \u00a0\n \n Google Scholar \u00a0\n Harary, F., Kabell, J.A.: A simple algorithm to detect balance in signed graphs. Mathematical Social Sciences\u00a01(1), 131\u2013136 (1980) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Harary, F., Kabell, J.A.: Counting balanced signed graphs using marked graphs. In: Proceedings of the Edinburgh Mathematical Society, vol.\u00a024(2), pp. 99\u2013104 (1981) \n Google Scholar \u00a0\n Harary, F., Palmer, E.: On the number of balanced signed graphs. Bulletin of Mathematical Biology\u00a029, 759\u2013765 (1967) MATH \u00a0\n \n Google Scholar \u00a0\n Kunegis, J., Schmidt, S., Lommatzsch, A., Lerner, J., De Luca, E.W., Albayrak, S.: Spectral analysis of signed graphs for clustering, prediction and visualization. In: SDM, page 559 (2010) \n Google Scholar \u00a0\n Lauterbach, D., Truong, H., Shah, T., Adamic, L.A.: Surfing a web of trust: Reputation and reciprocity on couchsurfing.com. In: CSE (4), pp. 346\u2013353 (2009) \n Google Scholar \u00a0\n Leskovec, J., Huttenlocher, D.P., Kleinberg, J.M.: Governance in social media: A case study of the wikipedia promotion process. In: ICWSM 2010 (2010) \n Google Scholar \u00a0\n Leskovec, J., Huttenlocher, D.P., Kleinberg, J.M.: Predicting positive and negative links in online social networks. In: WWW 2010, pp. 641\u2013650 (2010) \n Google Scholar \u00a0\n Leskovec, J., Huttenlocher, D.P., Kleinberg, J.M.: Signed networks in social media. In: CHI 2010, pp. 1361\u20131370 (2010) \n Google Scholar \u00a0\n Szell, M., Lambiotte, R., Thurner, S.: Multirelational organization of large-scale social networks in an online world. PNAS\u00a0107(31), 13636\u201313641 (2010) Article \u00a0\n \n Google Scholar \u00a0\n Download references Author information Authors and Affiliations INRIA Rennes \u2013 Bretagne Atlantique, France Anne-Marie Kermarrec LADyR, GSyC, Universidad Rey Juan Carlos, Spain Christopher Thraves Authors Anne-Marie Kermarrec Christopher Thraves Editor information Editors and Affiliations Institute of Informatics, University of Warsaw, ul. Banacha 2, 02-097, Warsaw, Poland Filip Murlak \u00a0&amp;\u00a0Piotr Sankowski \u00a0&amp;\u00a0 Rights and permissions Copyright information \u00a9 2011 Springer-Verlag GmbH Berlin Heidelberg About this paper Cite this paper Kermarrec, AM., Thraves, C. (2011). Can Everybody Sit Closer to Their Friends Than Their Enemies?.\n In: Murlak, F., Sankowski, P. (eds) Mathematical Foundations of Computer Science 2011. MFCS 2011. Lecture Notes in Computer Science, vol 6907. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-22993-0_36 Download citation.RIS.ENW.BIB DOI: https://doi.org/10.1007/978-3-642-22993-0_36 \n Publisher Name: Springer, Berlin, Heidelberg \n Print ISBN: 978-3-642-22992-3 \n Online ISBN: 978-3-642-22993-0 eBook Packages: Computer Science Computer Science (R0) Publish with us \n \n \n \n",
          "original_query": "Can everybody sit closer to their friends than their enemies?",
          "cleaned_query": "Can everybody sit closer to their friends than their enemies?",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Indifference graph",
          "url": "https://en.wikipedia.org/wiki/Indifference_graph",
          "content": "[Jump to content](https://en.wikipedia.org/wiki/Indifference_graph#bodyContent)\n\nFrom Wikipedia, the free encyclopedia\n\nIntersection graph of unit intervals on the real number line\n\n[![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Indifference_graph.svg/600px-Indifference_graph.svg.png)](https://en.wikipedia.org/wiki/File:Indifference_graph.svg) An indifference graph, formed from a set of points on the real line by connecting pairs of points whose distance is at most one\n\nIn [graph theory](https://en.wikipedia.org/wiki/Graph_theory), a branch of mathematics, an **indifference graph** is an [undirected graph](https://en.wikipedia.org/wiki/Undirected_graph) constructed by assigning a [real number](https://en.wikipedia.org/wiki/Real_number) to each vertex and connecting two vertices by an edge when their numbers are within one unit of each other.[\\[1\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-roberts-1) Indifference graphs are also the [intersection graphs](https://en.wikipedia.org/wiki/Intersection_graph) of sets of [unit intervals](https://en.wikipedia.org/wiki/Unit_interval), or of properly nested intervals (intervals none of which contains any other one). Based on these two types of interval representations, these graphs are also called **unit interval graphs** or **proper interval graphs**; they form a subclass of the [interval graphs](https://en.wikipedia.org/wiki/Interval_graph).\n\n## Equivalent characterizations\\[ [edit](https://en.wikipedia.org/w/index.php?title=Indifference_graph&action=edit&section=1)\\]\n\n[![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Forbidden_indifference_subgraphs.svg/480px-Forbidden_indifference_subgraphs.svg.png)](https://en.wikipedia.org/wiki/File:Forbidden_indifference_subgraphs.svg) [Forbidden induced subgraphs](https://en.wikipedia.org/wiki/Forbidden_graph_characterization) for the indifference graphs: the claw, sun, and net (top, left\u2013right) and cycles of length four or more (bottom)\n\nThe finite indifference graphs may be equivalently characterized as\n\n- The [intersection graphs](https://en.wikipedia.org/wiki/Intersection_graph) of [unit intervals](https://en.wikipedia.org/wiki/Unit_interval),[\\[1\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-roberts-1)\n- The intersection graphs of sets of intervals no two of which are nested (one containing the other),[\\[1\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-roberts-1)[\\[2\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-bogart-west-2)\n- The [claw-free](https://en.wikipedia.org/wiki/Claw-free_graph) [interval graphs](https://en.wikipedia.org/wiki/Interval_graph),[\\[1\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-roberts-1)[\\[2\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-bogart-west-2)\n- The graphs that do not have an [induced subgraph](https://en.wikipedia.org/wiki/Induced_subgraph) isomorphic to a [claw](https://en.wikipedia.org/wiki/Claw_(graph_theory)) _K_ 1,3, net (a triangle with a degree-one vertex adjacent to each of the triangle vertices), sun (a triangle surrounded by three other triangles that each share one edge with the central triangle), or hole (cycle of length four or more),[\\[3\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-3)\n- The [incomparability graphs](https://en.wikipedia.org/wiki/Comparability_graph) of [semiorders](https://en.wikipedia.org/wiki/Semiorder),[\\[1\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-roberts-1)\n- The undirected graphs that have a [linear order](https://en.wikipedia.org/wiki/Linear_order) such that, for every three vertices ordered u{\\\\displaystyle u}![{\\displaystyle u}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c3e6bb763d22c20916ed4f0bb6bd49d7470cffd8)\u2013v{\\\\displaystyle v}![{\\displaystyle v}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e07b00e7fc0847fbd16391c778d65bc25c452597)\u2013w{\\\\displaystyle w}![{\\displaystyle w}](https://wikimedia.org/api/rest_v1/media/math/render/svg/88b1e0c8e1be5ebe69d18a8010676fa42d7961e6), if uw{\\\\displaystyle uw}![{\\displaystyle uw}](https://wikimedia.org/api/rest_v1/media/math/render/svg/5f03887c6c7a9b5430a38d8b0d405406aa6358a5) is an edge then so are uv{\\\\displaystyle uv}![{\\displaystyle uv}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4e6b4c628431f1c0bdf17baf5b94d2f46caa4c5f) and vw{\\\\displaystyle vw}![{\\displaystyle vw}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8354a09980e3eb2de1a7d376cc69b544c43cced)[\\[4\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-greedy-4)\n- The graphs with no **astral triple**, three vertices connected pairwise by paths that avoid the third vertex and also do not contain two consecutive neighbors of the third vertex,[\\[5\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-5)\n- The graphs in which each connected component contains a path in which each [maximal clique](https://en.wikipedia.org/wiki/Maximal_clique) of the component forms a contiguous sub-path,[\\[6\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-metric-6)\n- The graphs whose vertices can be numbered in such a way that every shortest path forms a [monotonic sequence](https://en.wikipedia.org/wiki/Monotonic_sequence),[\\[6\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-metric-6)\n- The graphs whose [adjacency matrices](https://en.wikipedia.org/wiki/Adjacency_matrix) can be ordered such that, in each row and each column, the nonzeros of the matrix form a contiguous interval adjacent to the main diagonal of the matrix.[\\[7\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-7)\n- The induced subgraphs of powers of chordless paths.[\\[8\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-leafroot-8)\n- The [leaf powers](https://en.wikipedia.org/wiki/Leaf_power) having a leaf root which is a caterpillar.[\\[8\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-leafroot-8)\n\nFor infinite graphs, some of these definitions may differ.\n\n## Properties\\[ [edit](https://en.wikipedia.org/w/index.php?title=Indifference_graph&action=edit&section=2)\\]\n\nBecause they are special cases of [interval graphs](https://en.wikipedia.org/wiki/Interval_graph), indifference graphs have all the properties of interval graphs; in particular they are a special case of the [chordal graphs](https://en.wikipedia.org/wiki/Chordal_graph) and of the [perfect graphs](https://en.wikipedia.org/wiki/Perfect_graph). They are also a special case of the [circle graphs](https://en.wikipedia.org/wiki/Circle_graph), something that is not true of interval graphs more generally.\n\nIn the [Erd\u0151s\u2013R\u00e9nyi model](https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model) of [random graphs](https://en.wikipedia.org/wiki/Random_graph), an n{\\\\displaystyle n}![{\\displaystyle n}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b)-vertex graph whose number of edges is significantly less than n2/3{\\\\displaystyle n^{2/3}}![{\\displaystyle n^{2/3}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0f72521b94103f1ef065fd9c75e6ef096a2ea061) will be an indifference graph with high probability, whereas a graph whose number of edges is significantly more than n2/3{\\\\displaystyle n^{2/3}}![{\\displaystyle n^{2/3}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0f72521b94103f1ef065fd9c75e6ef096a2ea061) will not be an indifference graph with high probability.[\\[9\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-9)\n\nThe [bandwidth](https://en.wikipedia.org/wiki/Graph_bandwidth) of an arbitrary graph G{\\\\displaystyle G}![{\\displaystyle G}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5f3c8921a3b352de45446a6789b104458c9f90b) is one less than the size of the [maximum clique](https://en.wikipedia.org/wiki/Maximum_clique) in an indifference graph that contains G{\\\\displaystyle G}![{\\displaystyle G}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5f3c8921a3b352de45446a6789b104458c9f90b) as a subgraph and is chosen to minimize the size of the maximum clique.[\\[10\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-10) This property parallels similar relations between [pathwidth](https://en.wikipedia.org/wiki/Pathwidth) and [interval graphs](https://en.wikipedia.org/wiki/Interval_graph), and between [treewidth](https://en.wikipedia.org/wiki/Treewidth) and [chordal graphs](https://en.wikipedia.org/wiki/Chordal_graph). A weaker notion of width, the [clique-width](https://en.wikipedia.org/wiki/Clique-width), may be arbitrarily large on indifference graphs.[\\[11\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-11) However, every proper subclass of the indifference graphs that is closed under [induced subgraphs](https://en.wikipedia.org/wiki/Induced_subgraph) has bounded clique-width.[\\[12\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-lozin-12)\n\nEvery [connected](https://en.wikipedia.org/wiki/Connected_graph) indifference graph has a [Hamiltonian path](https://en.wikipedia.org/wiki/Hamiltonian_path).[\\[13\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-bertossi-13) An indifference graph has a [Hamiltonian cycle](https://en.wikipedia.org/wiki/Hamiltonian_cycle) if and only if it is [biconnected](https://en.wikipedia.org/wiki/K-vertex-connected_graph).[\\[14\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-pandas-14)\n\nIndifference graphs obey the [reconstruction conjecture](https://en.wikipedia.org/wiki/Reconstruction_conjecture): they are uniquely determined by their vertex-deleted subgraphs.[\\[15\\]](https://en.wikipedia.org/wiki/Indifference_graph#cite_note-15)\n\n## Algorithms\\[ [edit](https://en.wikipedia.org/w/index.php?title=Indifference_graph&action=edit&section=3)\\]\n\nAs with higher dimensional [unit disk graphs](https://en.wikipedia.org/wiki/Unit_disk_graph), it is possible to transform a set of points into",
          "original_query": "Indifference graphs",
          "cleaned_query": "Indifference graphs",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Balance-Aware Correlation Clustering via Cycle-Parity Constraints\nBuild a correlation clustering objective augmented with explicit signed-cycle constraints derived from Harary\u2019s balance criterion (e.g., penalize cycles with an odd number of negative edges, or enforce them in a Lagrangian relaxation). Develop an ILP/LP+rounding method that trades off standard MinDisAgree with a \u201cdistance-to-balance\u201d regularizer, and empirically test whether it improves cluster interpretability on signed social graphs.",
        "Signed Unit-Interval (Indifference) Graph Model for Social Proximity with Polarity\nIntroduce a \u201csigned indifference graph\u201d where vertices have positions on a line and edges are labeled \\(+\\) if within threshold \\(r_+\\) and \\(-\\) if beyond threshold \\(r_-\\) (with a gap/margin region allowed). Study identifiability: given only signs, recover positions and thresholds approximately, and characterize when the sign pattern is realizable (a signed analogue of forbidden induced subgraphs for unit interval graphs).",
        "Dynamic Balance Repair: Online Correlation Clustering under Edge-Sign Updates\nFormulate an online setting where signs/weights arrive or change over time (e.g., social relationships evolving), and maintain a clustering that approximately minimizes disagreements while controlling the number of vertex moves. Propose competitive algorithms that leverage balance indicators (cycle parities) as local certificates for when updates require global reclustering.",
        "Local Certificates for Near-Perfect Clustering Using \u201cSingle-Negative Cycle\u201d Counting\nDevelop fast sublinear/property-testing algorithms that estimate how far a signed graph is from being perfectly clusterable by sampling cycles and detecting the Davis/Harary obstruction (cycles with exactly one negative edge). Use these estimates to drive a hybrid algorithm: if \u201calmost clusterable,\u201d run a specialized fast routine; otherwise fall back to general correlation clustering approximations.",
        "Fairness-Constrained Correlation Clustering on Signed Graphs with Balance Guarantees\nAdd actionable constraints (e.g., per-cluster size bounds, demographic proportions, or limits on within-cluster negative edges) to MinDisAgree/MaxAgree while preserving approximate solvability for fixed \\(k\\). Prove approximation bounds that explicitly incorporate a balance-related term (e.g., guarantee that each output cluster is internally \u201calmost balanced\u201d by bounding negative-edge density or frustrated cycles).",
        "Spectral-to-PTAS Pipeline: Warm-Start Fixed-k Correlation Clustering via Signed Laplacians\nUse signed spectral embeddings (e.g., from signed Laplacians) to produce a small candidate set of \u201cpivot\u201d vertices or cluster centers, then apply the Giotis\u2013Guruswami sampling/rounding framework as a refinement step for fixed \\(k\\). The contribution is a theoretically motivated and empirically faster PTAS-style method that reduces the search space using spectral structure common in signed networks.",
        "Cluster Editing to Balanced Components with Fixed k and Edge Weights\nDefine a weighted editing problem: add/delete/flip minimum total weight of edges so the signed graph becomes a disjoint union of \\(k\\) balanced signed components (Harary-balanced). Design a PTAS for complete graphs (and an FPT algorithm parameterized by edit budget on general graphs), and evaluate whether enforcing balanced components yields more stable communities than standard correlation clustering alone."
      ],
      "generated_ideas_raw": "1. **Balance-Aware Correlation Clustering via Cycle-Parity Constraints**  \n   Build a correlation clustering objective augmented with explicit signed-cycle constraints derived from Harary\u2019s balance criterion (e.g., penalize cycles with an odd number of negative edges, or enforce them in a Lagrangian relaxation). Develop an ILP/LP+rounding method that trades off standard MinDisAgree with a \u201cdistance-to-balance\u201d regularizer, and empirically test whether it improves cluster interpretability on signed social graphs.\n\n2. **PTAS Extension for \u201cGeometric Friend-Enemy\u201d Drawability with k Clusters**  \n   Combine the fixed-\\(k\\) PTAS ideas (Giotis\u2013Guruswami) with the 1D \u201cfriends closer than enemies\u201d feasibility notion by optimizing a joint objective: choose a partition into \\(k\\) clusters and a 1D embedding that minimizes violations of the closer-than constraints. Provide a PTAS (or EPTAS) for complete signed graphs under a natural margin assumption (e.g., minimum separation between friend/enemy distances).\n\n3. **Recognition and Editing to 1D Drawability with Minimum Sign Flips**  \n   Define the decision/optimization problem: given a complete signed graph, flip the minimum number of edge signs so that it admits a valid 1D \u201csit closer to friends than enemies\u201d drawing. Design a polynomial-time algorithm if the resulting feasible graphs correspond to a known class (or give an FPT algorithm parameterized by flips), and relate the structure to forbidden patterns akin to \u201cone negative edge on a cycle.\u201d\n\n4. **Signed Unit-Interval (Indifference) Graph Model for Social Proximity with Polarity**  \n   Introduce a \u201csigned indifference graph\u201d where vertices have positions on a line and edges are labeled \\(+\\) if within threshold \\(r_+\\) and \\(-\\) if beyond threshold \\(r_-\\) (with a gap/margin region allowed). Study identifiability: given only signs, recover positions and thresholds approximately, and characterize when the sign pattern is realizable (a signed analogue of forbidden induced subgraphs for unit interval graphs).\n\n5. **Dynamic Balance Repair: Online Correlation Clustering under Edge-Sign Updates**  \n   Formulate an online setting where signs/weights arrive or change over time (e.g., social relationships evolving), and maintain a clustering that approximately minimizes disagreements while controlling the number of vertex moves. Propose competitive algorithms that leverage balance indicators (cycle parities) as local certificates for when updates require global reclustering.\n\n6. **Local Certificates for Near-Perfect Clustering Using \u201cSingle-Negative Cycle\u201d Counting**  \n   Develop fast sublinear/property-testing algorithms that estimate how far a signed graph is from being perfectly clusterable by sampling cycles and detecting the Davis/Harary obstruction (cycles with exactly one negative edge). Use these estimates to drive a hybrid algorithm: if \u201calmost clusterable,\u201d run a specialized fast routine; otherwise fall back to general correlation clustering approximations.\n\n7. **Fairness-Constrained Correlation Clustering on Signed Graphs with Balance Guarantees**  \n   Add actionable constraints (e.g., per-cluster size bounds, demographic proportions, or limits on within-cluster negative edges) to MinDisAgree/MaxAgree while preserving approximate solvability for fixed \\(k\\). Prove approximation bounds that explicitly incorporate a balance-related term (e.g., guarantee that each output cluster is internally \u201calmost balanced\u201d by bounding negative-edge density or frustrated cycles).\n\n8. **Spectral-to-PTAS Pipeline: Warm-Start Fixed-k Correlation Clustering via Signed Laplacians**  \n   Use signed spectral embeddings (e.g., from signed Laplacians) to produce a small candidate set of \u201cpivot\u201d vertices or cluster centers, then apply the Giotis\u2013Guruswami sampling/rounding framework as a refinement step for fixed \\(k\\). The contribution is a theoretically motivated and empirically faster PTAS-style method that reduces the search space using spectral structure common in signed networks.\n\n9. **Higher-Dimensional Drawability Thresholds and Hardness Landscape for Signed Graphs**  \n   Extend the \u201csit closer to friends than enemies\u201d model beyond 1D by studying complexity/approximability for \\(\\ell\\ge 2\\): identify graph families where feasibility is polynomial-time decidable, and prove NP-hardness for others via reductions from correlation clustering variants. Provide a practical approximation that minimizes constraint violations in \\(\\mathbb{R}^\\ell\\) and compare against purely combinatorial clustering.\n\n10. **Cluster Editing to Balanced Components with Fixed k and Edge Weights**  \n   Define a weighted editing problem: add/delete/flip minimum total weight of edges so the signed graph becomes a disjoint union of \\(k\\) balanced signed components (Harary-balanced). Design a PTAS for complete graphs (and an FPT algorithm parameterized by edit budget on general graphs), and evaluate whether enforcing balanced components yields more stable communities than standard correlation clustering alone.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Balance-Aware Correlation Clustering via Cycle-Parity Constraints\nBuild a correlation clustering objective augmented with explicit signed-cycle constraints derived from Harary\u2019s balance criterion (e.g",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Signed Unit-Interval (Indifference) Graph Model for Social Proximity with Polarity\nIntroduce a \u201csigned indifference graph\u201d where vertices have positions on a line and edges are labeled \\(+\\) if within",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Dynamic Balance Repair: Online Correlation Clustering under Edge-Sign Updates\nFormulate an online setting where signs/weights arrive or change over time (e.g., social relationships evolving), and main",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Local Certificates for Near-Perfect Clustering Using \u201cSingle-Negative Cycle\u201d Counting\nDevelop fast sublinear/property-testing algorithms that estimate how far a signed graph is from being perfectly cl",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Fairness-Constrained Correlation Clustering on Signed Graphs with Balance Guarantees\nAdd actionable constraints (e.g., per-cluster size bounds, demographic proportions, or limits on within-cluster neg",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Spectral-to-PTAS Pipeline: Warm-Start Fixed-k Correlation Clustering via Signed Laplacians\nUse signed spectral embeddings (e.g., from signed Laplacians) to produce a small candidate set of \u201cpivot\u201d ver",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Cluster Editing to Balanced Components with Fixed k and Edge Weights\nDefine a weighted editing problem: add/delete/flip minimum total weight of edges so the signed graph becomes a disjoint union of \\(",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 26,
      "paper_title": "A Clean Slate for Offline Reinforcement Learning",
      "contribution": "They introduce a rigorous, budget-aware evaluation and a set of minimal single-file implementations, unify prior algorithmic choices into a single hyperparameterized family (Unifloral), and\u2014using that clean infrastructure\u2014develop two new algorithms (TD3-AWR and MoBRAC) that outperform prior baselines under transparent, quantified offline evaluation budgets.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 13603,
      "output_tokens": 1169,
      "predecessor_details": [
        {
          "success": true,
          "title": "[2110.04156] Showing Your Offline Reinforcement Learning Work",
          "url": "https://arxiv.org/abs/2110.04156",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2110.04156** (cs)\n\n\\[Submitted on 8 Oct 2021 ( [v1](https://arxiv.org/abs/2110.04156v1)), last revised 5 Jun 2022 (this version, v3)\\]\n\n# Title:Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters\n\nAuthors: [Vladislav Kurenkov](https://arxiv.org/search/cs?searchtype=author&query=Kurenkov,+V), [Sergey Kolesnikov](https://arxiv.org/search/cs?searchtype=author&query=Kolesnikov,+S)\n\nView a PDF of the paper titled Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters, by Vladislav Kurenkov and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2110.04156)\n\n> Abstract:In this work, we argue for the importance of an online evaluation budget for a reliable comparison of deep offline RL algorithms. First, we delineate that the online evaluation budget is problem-dependent, where some problems allow for less but others for more. And second, we demonstrate that the preference between algorithms is budget-dependent across a diverse range of decision-making domains such as Robotics, Finance, and Energy Management. Following the points above, we suggest reporting the performance of deep offline RL algorithms under varying online evaluation budgets. To facilitate this, we propose to use a reporting tool from the NLP field, Expected Validation Performance. This technique makes it possible to reliably estimate expected maximum performance under different budgets while not requiring any additional computation beyond hyperparameter search. By employing this tool, we also show that Behavioral Cloning is often more favorable to offline RL algorithms when working within a limited budget.\n\n| | |\n| --- | --- |\n| Comments: | ICML 2022, Spotlight; [this https URL](https://tinkoff-ai.github.io/eop/) |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2110.04156](https://arxiv.org/abs/2110.04156) \\[cs.LG\\] |\n| (or [arXiv:2110.04156v3](https://arxiv.org/abs/2110.04156v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2110.04156](https://doi.org/10.48550/arXiv.2110.04156) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Vladislav Kurenkov \\[ [view email](https://arxiv.org/show-email/5157f0ee/2110.04156)\\] **[\\[v1\\]](https://arxiv.org/abs/2110.04156v1)**\nFri, 8 Oct 2021 14:25:21 UTC (1,087 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2110.04156v2)**\nFri, 4 Feb 2022 15:52:03 UTC (1,194 KB)\n**\\[v3\\]**\nSun, 5 Jun 2022 17:03:50 UTC (1,194 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters, by Vladislav Kurenkov and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2110.04156)\n- [TeX Source](https://arxiv.org/src/2110.04156)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2110.04156&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2110.04156&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-10](https://arxiv.org/list/cs.LG/2021-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2110.04156?context=cs) [cs.AI](https://arxiv.org/abs/2110.04156?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2110.04156)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2110.04156)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2110.04156)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2110.html#abs-2110-04156) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2110-04156)\n\n[Sergey Kolesnikov](https://dblp.uni-trier.de/search/author?author=Sergey%20Kolesnikov)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2110.04156) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters",
          "cleaned_query": "Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Hyperparameter Selection for Offline Reinforcement Learning",
          "url": "https://arxiv.org/abs/2007.09055",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2007.09055** (cs)\n\n\\[Submitted on 17 Jul 2020\\]\n\n# Title:Hyperparameter Selection for Offline Reinforcement Learning\n\nAuthors: [Tom Le Paine](https://arxiv.org/search/cs?searchtype=author&query=Paine,+T+L), [Cosmin Paduraru](https://arxiv.org/search/cs?searchtype=author&query=Paduraru,+C), [Andrea Michi](https://arxiv.org/search/cs?searchtype=author&query=Michi,+A), [Caglar Gulcehre](https://arxiv.org/search/cs?searchtype=author&query=Gulcehre,+C), [Konrad Zolna](https://arxiv.org/search/cs?searchtype=author&query=Zolna,+K), [Alexander Novikov](https://arxiv.org/search/cs?searchtype=author&query=Novikov,+A), [Ziyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Z), [Nando de Freitas](https://arxiv.org/search/cs?searchtype=author&query=de+Freitas,+N)\n\nView a PDF of the paper titled Hyperparameter Selection for Offline Reinforcement Learning, by Tom Le Paine and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2007.09055)\n\n> Abstract:Offline reinforcement learning (RL purely from logged data) is an important avenue for deploying RL techniques in real-world scenarios. However, existing hyperparameter selection methods for offline RL break the offline assumption by evaluating policies corresponding to each hyperparameter setting in the environment. This online execution is often infeasible and hence undermines the main aim of offline RL. Therefore, in this work, we focus on \\\\textit{offline hyperparameter selection}, i.e. methods for choosing the best policy from a set of many policies trained using different hyperparameters, given only logged data. Through large-scale empirical evaluation we show that: 1) offline RL algorithms are not robust to hyperparameter choices, 2) factors such as the offline RL algorithm and method for estimating Q values can have a big impact on hyperparameter selection, and 3) when we control those factors carefully, we can reliably rank policies across hyperparameter choices, and therefore choose policies which are close to the best policy in the set. Overall, our results present an optimistic view that offline hyperparameter selection is within reach, even in challenging tasks with pixel observations, high dimensional action spaces, and long horizon.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2007.09055](https://arxiv.org/abs/2007.09055) \\[cs.LG\\] |\n| (or [arXiv:2007.09055v1](https://arxiv.org/abs/2007.09055v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2007.09055](https://doi.org/10.48550/arXiv.2007.09055) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Tom Paine \\[ [view email](https://arxiv.org/show-email/7a8a8b46/2007.09055)\\] **\\[v1\\]**\nFri, 17 Jul 2020 15:30:38 UTC (3,144 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Hyperparameter Selection for Offline Reinforcement Learning, by Tom Le Paine and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2007.09055)\n- [TeX Source](https://arxiv.org/src/2007.09055)\n- [Other Formats](https://arxiv.org/format/2007.09055)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2007.09055&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2007.09055&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2020-07](https://arxiv.org/list/cs.LG/2020-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2007.09055?context=cs) [cs.AI](https://arxiv.org/abs/2007.09055?context=cs.AI) [stat](https://arxiv.org/abs/2007.09055?context=stat) [stat.ML](https://arxiv.org/abs/2007.09055?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2007.09055)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2007.09055)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2007.09055)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2007.html#abs-2007-09055) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2007-09055)\n\n[Tom Le Paine](https://dblp.uni-trier.de/search/author?author=Tom%20Le%20Paine) [Cosmin Paduraru](https://dblp.uni-trier.de/search/author?author=Cosmin%20Paduraru) [\u00c7aglar G\u00fcl\u00e7ehre](https://dblp.uni-trier.de/search/author?author=%C3%87aglar%20G%C3%BCl%C3%A7ehre) [Konrad Zolna](https://dblp.uni-trier.de/search/author?author=Konrad%20Zolna) [Alexander Novikov](https://dblp.uni-trier.de/search/author?author=Alexander%20Novikov)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2007.09055) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Hyperparameter Selection for Offline Reinforcement Learning",
          "cleaned_query": "Hyperparameter Selection for Offline Reinforcement Learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "A Minimalist Approach to Offline Reinforcement Learning - arXiv",
          "url": "https://arxiv.org/abs/2106.06860",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2106.06860** (cs)\n\n\\[Submitted on 12 Jun 2021 ( [v1](https://arxiv.org/abs/2106.06860v1)), last revised 3 Dec 2021 (this version, v2)\\]\n\n# Title:A Minimalist Approach to Offline Reinforcement Learning\n\nAuthors: [Scott Fujimoto](https://arxiv.org/search/cs?searchtype=author&query=Fujimoto,+S), [Shixiang Shane Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu,+S+S)\n\nView a PDF of the paper titled A Minimalist Approach to Offline Reinforcement Learning, by Scott Fujimoto and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2106.06860)\n\n> Abstract:Offline reinforcement learning (RL) defines the task of learning from a fixed batch of data. Due to errors in value estimation from out-of-distribution actions, most offline RL algorithms take the approach of constraining or regularizing the policy with the actions contained in the dataset. Built on pre-existing RL algorithms, modifications to make an RL algorithm work offline comes at the cost of additional complexity. Offline RL algorithms introduce new hyperparameters and often leverage secondary components such as generative models, while adjusting the underlying RL algorithm. In this paper we aim to make a deep RL algorithm work while making minimal changes. We find that we can match the performance of state-of-the-art offline RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a simple to implement and tune baseline, while more than halving the overall run time by removing the additional computational overhead of previous methods.\n\n| | |\n| --- | --- |\n| Comments: | NeurIPS 2021 Spotlight |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2106.06860](https://arxiv.org/abs/2106.06860) \\[cs.LG\\] |\n| | (or [arXiv:2106.06860v2](https://arxiv.org/abs/2106.06860v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2106.06860](https://doi.org/10.48550/arXiv.2106.06860) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Scott Fujimoto \\[ [view email](https://arxiv.org/show-email/e905ddc9/2106.06860)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2106.06860v1)**\nSat, 12 Jun 2021 20:38:59 UTC (348 KB)\n\n**\\[v2\\]**\nFri, 3 Dec 2021 18:58:09 UTC (921 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled A Minimalist Approach to Offline Reinforcement Learning, by Scott Fujimoto and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2106.06860)\n- [TeX Source](https://arxiv.org/src/2106.06860)\n- [Other Formats](https://arxiv.org/format/2106.06860)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2106.06860&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2106.06860&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-06](https://arxiv.org/list/cs.LG/2021-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2106.06860?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2106.06860?context=cs.AI)\n\n[stat](https://arxiv.org/abs/2106.06860?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2106.06860?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2106.06860)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2106.06860)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2106.06860)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2106.html#abs-2106-06860) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2106-06860)\n\n[Scott Fujimoto](https://dblp.uni-trier.de/search/author?author=Scott%20Fujimoto)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2106.06860&description=A Minimalist Approach to Offline Reinforcement Learning) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2106.06860&title=A Minimalist Approach to Offline Reinforcement Learning)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2106.06860) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "A Minimalist Approach to Offline Reinforcement Learning",
          "cleaned_query": "A Minimalist Approach to Offline Reinforcement Learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization",
          "url": "https://arxiv.org/abs/2506.15654",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2506.15654** (cs)\n\n\\[Submitted on 18 Jun 2025\\]\n\n# Title:CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization\n\nAuthors: [Ranting Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu,+R)\n\nView a PDF of the paper titled CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization, by Ranting Hu\n\n[View PDF](https://arxiv.org/pdf/2506.15654) [HTML (experimental)](https://arxiv.org/html/2506.15654v1)\n\n> Abstract:Offline reinforcement learning (offline RL) algorithms often require additional constraints or penalty terms to address distribution shift issues, such as adding implicit or explicit policy constraints during policy optimization to reduce the estimation bias of functions. This paper focuses on a limitation of the Advantage-Weighted Regression family (AWRs), i.e., the potential for learning over-conservative policies due to data corruption, specifically the poor explorations in suboptimal offline data. We study it from two perspectives: (1) how poor explorations impact the theoretically optimal policy based on KL divergence, and (2) how such poor explorations affect the approximation of the theoretically optimal policy. We prove that such over-conservatism is mainly caused by the sensitivity of the loss function for policy optimization to poor explorations, and the proportion of poor explorations in offline datasets. To address this concern, we propose Corruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a set of robust loss functions during policy optimization and an advantage-based prioritized experience replay method to filter out poor explorations. Numerical experiments on the D4RL benchmark show that our method can learn superior policies from suboptimal offline data, significantly enhancing the performance of policy optimization.\n\n| | |\n| --- | --- |\n| Comments: | 23 pages, 14 figures |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2506.15654](https://arxiv.org/abs/2506.15654) \\[cs.LG\\] |\n| (or [arXiv:2506.15654v1](https://arxiv.org/abs/2506.15654v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2506.15654](https://doi.org/10.48550/arXiv.2506.15654) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ranting Hu \\[ [view email](https://arxiv.org/show-email/32f8c31f/2506.15654)\\] **\\[v1\\]**\nWed, 18 Jun 2025 17:31:26 UTC (663 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization, by Ranting Hu\n\n- [View PDF](https://arxiv.org/pdf/2506.15654)\n- [HTML (experimental)](https://arxiv.org/html/2506.15654v1)\n- [TeX Source](https://arxiv.org/src/2506.15654)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2506.15654&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2506.15654&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-06](https://arxiv.org/list/cs.LG/2025-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2506.15654?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2506.15654)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2506.15654)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2506.15654)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2506.15654) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Advantage-weighted Regression: Simple and Scalable Off-policy Reinforcement Learning",
          "cleaned_query": "Advantage-weighted Regression: Simple and Scalable Off-policy Reinforcement Learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2005.13239] MOPO: Model-based Offline Policy Optimization - arXiv",
          "url": "https://arxiv.org/abs/2005.13239",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2005.13239** (cs)\n\n\\[Submitted on 27 May 2020 ( [v1](https://arxiv.org/abs/2005.13239v1)), last revised 22 Nov 2020 (this version, v6)\\]\n\n# Title:MOPO: Model-based Offline Policy Optimization\n\nAuthors: [Tianhe Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+T), [Garrett Thomas](https://arxiv.org/search/cs?searchtype=author&query=Thomas,+G), [Lantao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+L), [Stefano Ermon](https://arxiv.org/search/cs?searchtype=author&query=Ermon,+S), [James Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou,+J), [Sergey Levine](https://arxiv.org/search/cs?searchtype=author&query=Levine,+S), [Chelsea Finn](https://arxiv.org/search/cs?searchtype=author&query=Finn,+C), [Tengyu Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+T)\n\nView a PDF of the paper titled MOPO: Model-based Offline Policy Optimization, by Tianhe Yu and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2005.13239)\n\n> Abstract:Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distributional shift between the offline training data and those states visited by the learned policy. Despite significant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generalization to unseen states. In this paper, we first observe that an existing model-based RL algorithm already produces significant gains in the offline setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the offline setting's distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artificially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maximizes a lower bound of the policy's return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on existing offline RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task. The code is available at [this https URL](https://github.com/tianheyu927/mopo).\n\n| | |\n| --- | --- |\n| Comments: | NeurIPS 2020. First two authors contributed equally. Last two authors advised equally |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2005.13239](https://arxiv.org/abs/2005.13239) \\[cs.LG\\] |\n| | (or [arXiv:2005.13239v6](https://arxiv.org/abs/2005.13239v6) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2005.13239](https://doi.org/10.48550/arXiv.2005.13239) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Tianhe Yu \\[ [view email](https://arxiv.org/show-email/aeaf7671/2005.13239)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2005.13239v1)**\nWed, 27 May 2020 08:46:41 UTC (1,763 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2005.13239v2)**\nWed, 1 Jul 2020 06:01:54 UTC (377 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2005.13239v3)**\nSun, 6 Sep 2020 23:25:56 UTC (746 KB)\n\n**[\\[v4\\]](https://arxiv.org/abs/2005.13239v4)**\nWed, 23 Sep 2020 07:08:58 UTC (377 KB)\n\n**[\\[v5\\]](https://arxiv.org/abs/2005.13239v5)**\nTue, 29 Sep 2020 23:49:26 UTC (377 KB)\n\n**\\[v6\\]**\nSun, 22 Nov 2020 07:04:17 UTC (379 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MOPO: Model-based Offline Policy Optimization, by Tianhe Yu and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2005.13239)\n- [TeX Source](https://arxiv.org/src/2005.13239)\n- [Other Formats](https://arxiv.org/format/2005.13239)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2005.13239&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2005.13239&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2020-05](https://arxiv.org/list/cs.LG/2020-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2005.13239?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2005.13239?context=cs.AI)\n\n[stat](https://arxiv.org/abs/2005.13239?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2005.13239?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2005.13239)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2005.13239)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2005.13239)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2005.html#abs-2005-13239) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2005-13239)\n\n[Tianhe Yu](https://dblp.uni-trier.de/search/author?author=Tianhe%20Yu)\n\n[Garrett Thomas](https://dblp.uni-trier.de/search/author?author=Garrett%20Thomas)\n\n[Lantao Yu](https://dblp.uni-trier.de/search/author?author=Lantao%20Yu)\n\n[Stefano Ermon](https://dblp.uni-trier.de/search/author?author=Stefano%20Ermon)\n\n[James Zou](https://dblp.uni-trier.de/search/author?author=James%20Zou)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2005.13239&description=MOPO: Model-based Offline Policy Optimization) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2005.13239&title=MOPO: Model-based Offline Policy Optimization)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2005.13239) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "MOPO: Model-based Offline Policy Optimization",
          "cleaned_query": "MOPO: Model-based Offline Policy Optimization",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Farama-Foundation/D4RL: A collection of reference ... - GitHub",
          "url": "https://github.com/Farama-Foundation/D4RL",
          "content": "GitHub - Farama-Foundation/D4RL: A collection of reference environments for offline reinforcement learning\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/Farama-Foundation/D4RL)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n \nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n \nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/Farama-Foundation/D4RL)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=Farama-Foundation/D4RL)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[Farama-Foundation](https://github.com/Farama-Foundation)/**[D4RL](https://github.com/Farama-Foundation/D4RL)**Public\n* ### Uh oh!\nThere was an error while loading.[Please reload this page]().\n* [Notifications](https://github.com/login?return_to=/Farama-Foundation/D4RL)You must be signed in to change notification settings\n* [Fork303](https://github.com/login?return_to=/Farama-Foundation/D4RL)\n* [Star1.6k](https://github.com/login?return_to=/Farama-Foundation/D4RL)\nA collection of reference environments for offline reinforcement learning\n### License\n[Apache-2.0 license](https://github.com/Farama-Foundation/D4RL/blob/master/LICENSE)\n[1.6kstars](https://github.com/Farama-Foundation/D4RL/stargazers)[303forks](https://github.com/Farama-Foundation/D4RL/forks)[Branches](https://github.com/Farama-Foundation/D4RL/branches)[Tags](https://github.com/Farama-Foundation/D4RL/tags)[Activity](https://github.com/Farama-Foundation/D4RL/activity)\n[Star](https://github.com/login?return_to=/Farama-Foundation/D4RL)\n[Notifications](https://github.com/login?return_to=/Farama-Foundation/D4RL)You must be signed in to change notification settings\n# Farama-Foundation/D4RL\nmaster\n[Branches](https://github.com/Farama-Foundation/D4RL/branches)[Tags](https://github.com/Farama-Foundation/D4RL/tags)\n[](https://github.com/Farama-Foundation/D4RL/branches)[](https://github.com/Farama-Foundation/D4RL/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[192 Commits](https://github.com/Farama-Foundation/D4RL/commits/master/)\n[](https://github.com/Farama-Foundation/D4RL/commits/master/)\n|\n[.github](https://github.com/Farama-Foundation/D4RL/tree/master/.github)\n|\n[.github](https://github.com/Farama-Foundation/D4RL/tree/master/.github)\n|\n|\n|\n[d4rl](https://github.com/Farama-Foundation/D4RL/tree/master/d4rl)\n|\n[d4rl](https://github.com/Farama-Foundation/D4RL/tree/master/d4rl)\n|\n|\n|\n[scripts](https://github.com/Farama-Foundation/D4RL/tree/master/scripts)\n|\n[scripts](https://github.com/Farama-Foundation/D4RL/tree/master/scripts)\n|\n|\n|\n[.gitignore](https://github.com/Farama-Foundation/D4RL/blob/master/.gitignore)\n|\n[.gitignore](https://github.com/Farama-Foundation/D4RL/blob/master/.gitignore)\n|\n|\n|\n[CODE\\_OF\\_CONDUCT.rst](https://github.com/Farama-Foundation/D4RL/blob/master/CODE_OF_CONDUCT.rst)\n|\n[CODE\\_OF\\_CONDUCT.rst](https://github.com/Farama-Foundation/D4RL/blob/master/CODE_OF_CONDUCT.rst)\n|\n|\n|\n[LICENSE](https://github.com/Farama-Foundation/D4RL/blob/master/LICENSE)\n|\n[LICENSE](https://github.com/Farama-Foundation/D4RL/blob/master/LICENSE)\n|\n|\n|\n[MANIFEST.in](https://github.com/Farama-Foundation/D4RL/blob/master/MANIFEST.in)\n|\n[MANIFEST.in](https://github.com/Farama-Foundation/D4RL/blob/master/MANIFEST.in)\n|\n|\n|\n[README.md](https://github.com/Farama-Foundation/D4RL/blob/master/README.md)\n|\n[README.md](https://github.com/Farama-Foundation/D4RL/blob/master/README.md)\n|\n|\n|\n[d4rl-text.png](https://github.com/Farama-Foundation/D4RL/blob/master/d4rl-text.png)\n|\n[d4rl-text.png](https://github.com/Farama-Foundation/D4RL/blob/master/d4rl-text.png)\n|\n|\n|\n[setup.py](https://github.com/Farama-Foundation/D4RL/blob/master/setup.py)\n|\n[setup.py](https://github.com/Farama-Foundation/D4RL/blob/master/setup.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n## Important Notice\n[](#important-notice)\n### All of online environments libraries in D4RL have been moved[Gymnasium](https://github.com/Farama-Foundation/Gymnasium),[MiniGrid](https://github.com/Farama-Foundation/MiniGrid)and[Gymnasium-Robotics](https://github.com/Farama-Foundation/Gymnasium-Robotics), and all offline datasets in DR4L have been moved to[Minari](https://github.com/Farama-Foundation/Minari). These new versions include large bug fixes, new versions of Python, and are where all new development will continue. Please upgrade these libraries as soon as you're able to do so. If you'd like to read more about the story behind this switch, please check out[this blog post](https://farama.org/Announcing-Minari).\n[](#all-of-online-environments-libraries-in-d4rl-have-been-moved-gymnasium-minigrid-and-gymnasium-robotics-and-all-offline-datasets-in-dr4l-have-been-moved-to-minari-these-new-versions-include-large-bug-fixes-new-versions-of-python-and-are-where-all-new-development-will-continue-please-upgrade-these-libraries-as-soon-as-youre-able-to-do-so-if-youd-like-to-read-more-about-the-story-behind-this-switch-please-check-out-this-blog-post)\n[![](https://raw.githubusercontent.com/jjshoots/D4RL/master/d4rl-text.png)](https://raw.githubusercontent.com/jjshoots/D4RL/master/d4rl-text.png)\nD4RL is an open-source benchmark for offline reinforcement learning. It provides standardized environments and datasets for training and benchmarking algorithms. A supplementary[whitepaper](https://arxiv.org/abs/2004.07219)and[website](https://sites.google.com/view/d4rl/home)are also available.\nThe current maintenance plan for this library is:\n1. Pull the majority of the environments out of D4RL, fix the long standing bugs, and have them depend on the new MuJoCo bindings. The majority of the environments housed in D4RL were already maintained projects in Farama, and all the ones that aren't will be going into[Gymnasium-Robotics](https://github.com/Farama-Foundation/Gymnasium-Robotics), a standard library for housing many different Robotics environments. There are some envrionments that we don't plan to maintain, noteably the PyBullet ones (MuJoCo is not maintained and open source and PyBullet is now longer maintained) and Flow (it was never really used and the original author's don't view it as especially valuable).\n2. Recreate all the datasets in D4RL given the revised versions of environments, and host them in a standard offline RL dataset repostiry we're working on called[Minari](https://github.com/Farama-Foundation/Minari).\n## Setup\n[](#setup)\nD4RL can be installed by cloning the repository as follows:\n```\n`git clone https://github.com/Farama-Foundation/d4rl.git\ncd d4rl\npip install -e .`\n```\nOr, alternatively:\n```\n`pip install git+https://github.com/Farama-Foundation/d4rl@master#egg=d4rl`\n```\nThe control environments require MuJoCo as a dependency. You may need to obtain a[license](https://www.roboti.us/license.html)and follow the setup instructions for mujoco\\_py. This mostly involves copying the key to your MuJoCo installation folder.\nThe Flow and CARLA tasks also require additional installation steps:\n* Instructions for installing CARLA can be found[here](https://github.com/Farama-Foundation/d4rl/wiki/CARLA-Setup)\n* Instructions for installing Flow can be found[here](https://flow.readthedocs.io/en/latest/flow_setup.html). Make sure to install using the SUMO simulator, and add the flow repository to your PYTHONPATH once finished.\n## Using d4rl\n[](#using-d4rl)\nd4rl uses the[OpenAI Gym](https://github.com/openai/gym)API. Tasks are created via the`gym.make`function. A full list of all tasks is[available here](https://github.com/Farama-Foundation/d4rl/wiki/Tasks).\nEach task is associated with a fixed offline dataset, which can be obtained with the`env.get\\_dataset()`method. This method returns a dictionary with:\n* `observations`: An N by observation dimensional array of observations.\n* `actions`: An N by action dimensional array of actions.\n* `rewards`: An N dimensional array of rewards.\n* `terminals`: An N dimensional array of episode termination flags. This is true when episodes end due to termination conditions such as falling over.\n* `timeouts`: An N dimensional array of termination flags. This is true when episodes end due to reaching the maximum episode length.\n* `infos`: Contains optional task-specific debugging information.\nYou can also load data using`d4rl.qlearning\\_dataset(env)`, which formats the data for use by typical Q-learning algorithms by adding a`next\\_observations`key.\n```\nimportgymimportd4rl# Import required to register environments, you may need to also import the submodule# Create the environmentenv=gym.make('maze2d-umaze-v1')# d4rl abides by the OpenAI gym interfaceenv.reset()env.step(env.action\\_space.sample())# Each task is associated with a dataset# dataset contains observations, actions, rewards, terminals, and infosdataset=env.get\\_dataset()print(dataset['observations'])# An N x dim\\_observation Numpy array of observations# Alternatively, use d4rl.qlearning\\_dataset which# also adds next\\_observatio",
          "original_query": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
          "cleaned_query": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "1. **Budget-Aware Offline RL Benchmarking Protocol (EOP Curves as Standard)**\n   Create a standardized evaluation protocol for offline RL that reports *Expected Validation Performance* (EOP) curves across multiple online evaluation budgets (e.g., 0, 1, 5, 20, 100 episodes) rather than a single number. Implement this as an automated harness for D4RL/Minari tasks that outputs \u201cbudget-dependent rankings\u201d and confidence intervals, enabling reproducible comparisons where algorithm preference can flip with budget (as shown in Paper 1).",
        "**Budget-Constrained Hyperparameter Optimization for Offline RL**\n   Develop a hyperparameter selection algorithm that explicitly optimizes expected deployed performance under a *given online evaluation budget*, combining offline selection signals (Paper 2) with EOP-style budget modeling (Paper 1). Concretely, treat each candidate policy as an arm in a Bayesian optimization/bandit scheme where the utility is \u201cbest-of-N evaluations,\u201d and learn a mapping from offline metrics (e.g., Q-estimator diagnostics) to predicted EOP to pick which policies to evaluate online.",
        "**Corruption-Sensitive EOP: Predicting When BC Beats Offline RL**\n   Build a diagnostic that predicts, from dataset statistics alone, the online-evaluation budget threshold below which Behavioral Cloning (BC) is expected to outperform offline RL (Paper 1) due to corruption/poor exploration (Paper 4). The contribution is a measurable \u201ccorruption index\u201d (e.g., advantage tail-heaviness, action-coverage gaps, uncertainty spikes) that forecasts EOP curves for BC vs. IQL/AWR-style vs. TD3+BC-style methods without running extensive online tests.",
        "**Robust TD3+BC via CAWR-Style Losses and Replay Filtering**\n   Extend the minimalist TD3+BC approach (Paper 3) by swapping the BC-regularized actor loss for CAWR-inspired robust regression losses and adding advantage-based prioritized replay to downweight poor explorations (Paper 4). This yields a simple-to-implement algorithm that keeps the \u201cminimal changes\u201d ethos while explicitly targeting over-conservatism and corrupted suboptimal trajectories; evaluate on suboptimal D4RL regimes (random/medium-replay) and pixel tasks.",
        "**Model-Based Corruption-Averse Offline RL (MOPO + CAWR)**\n   Combine MOPO\u2019s uncertainty-penalized synthetic rollouts (Paper 5) with CAWR\u2019s corruption-averse policy optimization (Paper 4) to address two failure modes simultaneously: distribution shift in imagined trajectories and corrupted/low-quality real data. Concretely, use model uncertainty both to penalize rewards (MOPO) and to gate the weighting function in advantage-weighted regression, then compare against pure MOPO and pure CAWR on task-mismatch datasets.",
        "**Offline-Only Policy Ranking with Agreement Across Q-Estimators**\n   Build a hyperparameter/policy ranking method that aggregates multiple Q-estimation pipelines and selects policies based on *rank agreement* and uncertainty, following Paper 2\u2019s finding that the Q-estimator choice strongly impacts selection. The key contribution is a practical selection rule (e.g., minimize expected regret under a Plackett\u2013Luce model of ranks) that flags \u201cunstable winners\u201d and requests minimal online evaluation only when estimator disagreement is high.",
        "**Adaptive Online Evaluation Allocation for Offline RL Leaderboards**\n   Propose an evaluation scheduler that adaptively assigns scarce online evaluation episodes to candidate policies/hyperparameter settings to maximize ranking reliability under a fixed budget (Paper 1). Implement a sequential testing procedure (e.g., racing with early stopping) that uses EOP estimates and variance to decide which policies deserve more rollouts, yielding tighter comparisons than uniform evaluation while staying within strict online constraints.",
        "**Dataset Repair for Offline RL: Advantage-Guided Trajectory Pruning and Relabeling**\n   Introduce a preprocessing pipeline for offline datasets that identifies \u201cpoor exploration\u201d segments (Paper 4) and either prunes them or relabels rewards/advantages using conservative critics (Paper 3/5). The contribution is a concrete, offline-only dataset transformation (exportable back into Minari format) and an ablation study showing when repairing the dataset beats adding algorithmic constraints, especially in suboptimal-data settings.",
        "**Cross-Task Budget Transfer: Meta-Predicting Required Online Budget**\n   Train a meta-model that predicts, from task and dataset features (state/action dimension, horizon proxies, dataset coverage/entropy, model uncertainty), the online evaluation budget needed for stable algorithm ranking (Paper 1) and reliable hyperparameter selection (Paper 2). The actionable output is a \u201crecommended evaluation budget\u201d and \u201crecommended number of hyperparameter candidates\u201d per new task, validated by holding out D4RL domains (robotics vs. finance-like vs. energy-like).",
        "**Unified Objective for Offline RL Selection: Jointly Optimizing Return and Conservatism**\n   Define and test a selection objective that trades off predicted return with a principled conservatism measure derived from distribution shift (MOPO uncertainty penalties, Paper 5) and corruption sensitivity (CAWR analysis, Paper 4). Practically, this becomes a scalar selection score used during hyperparameter search (Paper 2) that aims to pick policies that are not just high-return offline, but also robust under limited online evaluation budgets (Paper 1), reducing \u201cwinner\u2019s curse\u201d in deployment."
      ],
      "generated_ideas_raw": "1. **Budget-Aware Offline RL Benchmarking Protocol (EOP Curves as Standard)**\n   Create a standardized evaluation protocol for offline RL that reports *Expected Validation Performance* (EOP) curves across multiple online evaluation budgets (e.g., 0, 1, 5, 20, 100 episodes) rather than a single number. Implement this as an automated harness for D4RL/Minari tasks that outputs \u201cbudget-dependent rankings\u201d and confidence intervals, enabling reproducible comparisons where algorithm preference can flip with budget (as shown in Paper 1).\n\n2. **Budget-Constrained Hyperparameter Optimization for Offline RL**\n   Develop a hyperparameter selection algorithm that explicitly optimizes expected deployed performance under a *given online evaluation budget*, combining offline selection signals (Paper 2) with EOP-style budget modeling (Paper 1). Concretely, treat each candidate policy as an arm in a Bayesian optimization/bandit scheme where the utility is \u201cbest-of-N evaluations,\u201d and learn a mapping from offline metrics (e.g., Q-estimator diagnostics) to predicted EOP to pick which policies to evaluate online.\n\n3. **Corruption-Sensitive EOP: Predicting When BC Beats Offline RL**\n   Build a diagnostic that predicts, from dataset statistics alone, the online-evaluation budget threshold below which Behavioral Cloning (BC) is expected to outperform offline RL (Paper 1) due to corruption/poor exploration (Paper 4). The contribution is a measurable \u201ccorruption index\u201d (e.g., advantage tail-heaviness, action-coverage gaps, uncertainty spikes) that forecasts EOP curves for BC vs. IQL/AWR-style vs. TD3+BC-style methods without running extensive online tests.\n\n4. **Robust TD3+BC via CAWR-Style Losses and Replay Filtering**\n   Extend the minimalist TD3+BC approach (Paper 3) by swapping the BC-regularized actor loss for CAWR-inspired robust regression losses and adding advantage-based prioritized replay to downweight poor explorations (Paper 4). This yields a simple-to-implement algorithm that keeps the \u201cminimal changes\u201d ethos while explicitly targeting over-conservatism and corrupted suboptimal trajectories; evaluate on suboptimal D4RL regimes (random/medium-replay) and pixel tasks.\n\n5. **Model-Based Corruption-Averse Offline RL (MOPO + CAWR)**\n   Combine MOPO\u2019s uncertainty-penalized synthetic rollouts (Paper 5) with CAWR\u2019s corruption-averse policy optimization (Paper 4) to address two failure modes simultaneously: distribution shift in imagined trajectories and corrupted/low-quality real data. Concretely, use model uncertainty both to penalize rewards (MOPO) and to gate the weighting function in advantage-weighted regression, then compare against pure MOPO and pure CAWR on task-mismatch datasets.\n\n6. **Offline-Only Policy Ranking with Agreement Across Q-Estimators**\n   Build a hyperparameter/policy ranking method that aggregates multiple Q-estimation pipelines and selects policies based on *rank agreement* and uncertainty, following Paper 2\u2019s finding that the Q-estimator choice strongly impacts selection. The key contribution is a practical selection rule (e.g., minimize expected regret under a Plackett\u2013Luce model of ranks) that flags \u201cunstable winners\u201d and requests minimal online evaluation only when estimator disagreement is high.\n\n7. **Adaptive Online Evaluation Allocation for Offline RL Leaderboards**\n   Propose an evaluation scheduler that adaptively assigns scarce online evaluation episodes to candidate policies/hyperparameter settings to maximize ranking reliability under a fixed budget (Paper 1). Implement a sequential testing procedure (e.g., racing with early stopping) that uses EOP estimates and variance to decide which policies deserve more rollouts, yielding tighter comparisons than uniform evaluation while staying within strict online constraints.\n\n8. **Dataset Repair for Offline RL: Advantage-Guided Trajectory Pruning and Relabeling**\n   Introduce a preprocessing pipeline for offline datasets that identifies \u201cpoor exploration\u201d segments (Paper 4) and either prunes them or relabels rewards/advantages using conservative critics (Paper 3/5). The contribution is a concrete, offline-only dataset transformation (exportable back into Minari format) and an ablation study showing when repairing the dataset beats adding algorithmic constraints, especially in suboptimal-data settings.\n\n9. **Cross-Task Budget Transfer: Meta-Predicting Required Online Budget**\n   Train a meta-model that predicts, from task and dataset features (state/action dimension, horizon proxies, dataset coverage/entropy, model uncertainty), the online evaluation budget needed for stable algorithm ranking (Paper 1) and reliable hyperparameter selection (Paper 2). The actionable output is a \u201crecommended evaluation budget\u201d and \u201crecommended number of hyperparameter candidates\u201d per new task, validated by holding out D4RL domains (robotics vs. finance-like vs. energy-like).\n\n10. **Unified Objective for Offline RL Selection: Jointly Optimizing Return and Conservatism**\n   Define and test a selection objective that trades off predicted return with a principled conservatism measure derived from distribution shift (MOPO uncertainty penalties, Paper 5) and corruption sensitivity (CAWR analysis, Paper 4). Practically, this becomes a scalar selection score used during hyperparameter search (Paper 2) that aims to pick policies that are not just high-return offline, but also robust under limited online evaluation budgets (Paper 1), reducing \u201cwinner\u2019s curse\u201d in deployment.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Budget-Aware Offline RL Benchmarking Protocol (EOP Curves as Standard)**\n   Create a standardized evaluation protocol for offline RL that reports *Expected Validation Performance* (EOP) curves ac",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "**Budget-Constrained Hyperparameter Optimization for Offline RL**\n   Develop a hyperparameter selection algorithm that explicitly optimizes expected deployed performance under a *given online evaluati",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Corruption-Sensitive EOP: Predicting When BC Beats Offline RL**\n   Build a diagnostic that predicts, from dataset statistics alone, the online-evaluation budget threshold below which Behavioral Clon",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**Robust TD3+BC via CAWR-Style Losses and Replay Filtering**\n   Extend the minimalist TD3+BC approach (Paper 3) by swapping the BC-regularized actor loss for CAWR-inspired robust regression losses and",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Model-Based Corruption-Averse Offline RL (MOPO + CAWR)**\n   Combine MOPO\u2019s uncertainty-penalized synthetic rollouts (Paper 5) with CAWR\u2019s corruption-averse policy optimization (Paper 4) to address t",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Offline-Only Policy Ranking with Agreement Across Q-Estimators**\n   Build a hyperparameter/policy ranking method that aggregates multiple Q-estimation pipelines and selects policies based on *rank a",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Adaptive Online Evaluation Allocation for Offline RL Leaderboards**\n   Propose an evaluation scheduler that adaptively assigns scarce online evaluation episodes to candidate policies/hyperparameter ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Dataset Repair for Offline RL: Advantage-Guided Trajectory Pruning and Relabeling**\n   Introduce a preprocessing pipeline for offline datasets that identifies \u201cpoor exploration\u201d segments (Paper 4) a",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Cross-Task Budget Transfer: Meta-Predicting Required Online Budget**\n   Train a meta-model that predicts, from task and dataset features (state/action dimension, horizon proxies, dataset coverage/en",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Unified Objective for Offline RL Selection: Jointly Optimizing Return and Conservatism**\n   Define and test a selection objective that trades off predicted return with a principled conservatism meas",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 27,
      "paper_title": "Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy",
      "contribution": "They develop novel high-probability spectral-norm perturbation bounds for the top-p low-rank approximation of a symmetric matrix under arbitrary symmetric noise, using a new 'contour bootstrapping' complex-analytic technique, and apply these bounds to give strictly sharper utility guarantees for differentially private PCA (improvements up to a factor \u221an).",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10949,
      "output_tokens": 1023,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] The Approximation of One Matrix by Another of Lower Rank",
          "url": "https://ccrma.stanford.edu/~dattorro/eckart&young.1936.pdf",
          "content": "PSYCHOMETRIKA--VOL. I, NO. 3\nSEPTEMBER, 193.6\nTHE APPROXIMATION OF ONE MATRIX BY\nANOTHER OF LOWER RANK\nCARL ECKART AND GALE YOUNG\nUniversity of Chicago, Chicago, Illinois\nThe mathematical problem of approximating one matrix by an\u0002other of lower rank is closely related to the fundamental postulate\nof factor-theory. When formulated as a least-squares problem, the\nnormal equations cannot be immediately written do~vn, since the ele\u0002ments of the approximate matrix are not independent of one another.\nThe solution of the problem is simplified by first expressing the mat\u0002rices in a canonic form. It is found that the problem always has a\nsolution which is usually unique. Several conclusions can be drawn\nfrom the form of this solution.\nA hypothetical interpretation of the canonic components of a\nscore matrix is discussed.\nIntroduction\nIf N individuals are each subjected to n tests, it is a fundamental\npostulate of factor theory that the resulting n X N score matrix a\ncan be adequately approximated by another matrix fl whose rank r is\nless than the smaller of n or N. Closely associated to this postulate\nis the purely mathematical problem of finding that matrix fl of rank\nr which most closely approximates a given matrix a of higher rank R.\nIt will be shown that if the least-squares criterion of approximation\nbe adopted, this problem has a general solution which is relatively\nsimple in a theoretical sense, though the amount of numerical work\ninvolved in applications may be prohibitive. Certain conclusions can\nbe drawn from the theoretical solution which may be of importance\nin practical work.\nTo formulate the problem precisely, it is convenient to define the\n\"scalar product\" of two n )~ N matrices as the following numerical\nfunction of their elements:\n(a,,8) ~ ~ ais flis (1)\ni:l ]=I\nwhere .~ is the C]-element of the matrix u. This function has all the\nproperties of the scalar product of vectors:\n(a, fl)~- (flea) (2)\n--211--\n212 PSYCHOMETRIKA\n(a _ fl, r) ~ (a, r) ___ (fi, r) ; (3)\n(za, fl) ~--- z (a, if z i s a number; (4)\n(a,a) >0 ifa=fi0 . (5)\nThe positive number l, defined by l\n2 ~ (a, a) may be called the length\nof the matrix a; the number l\n~ was called the span (Spannung) of \nby Frobenius.* The length of the matrix a -- ~ may be called the dis\u0002tance from a to ft.\nThe problem can now be formulated in a definite manner: to find\nthat matrix ~ of rank r, such that there is no other matrix of rank r\nwhose distance from a is less than the distance from ~ to a. This\namounts to requiring a least-squares solution of the approximation\nproblem, every element of the given matrix being given equal weight.\nSome preliminary theorems and remarks.\nTo simplify the following discussion, let a, b, u, ... denote n X n\nmatrices, A, B, U, ... denote N X N matrices, and a, fl, ... denote\nn ~( N matrices; the special case n --- N is not excluded. The N ~( \nmatrix which is obtained by writing the columns of a as rows is the\ntranspose of a and will be denoted by a\u2019. The products as, aA, aB\u2019,\na\u2019a, ... are defined in the usual way. It is then seen that the following\nequations are correct:\n(a, ~) -- (a\u2019, ~\u2019) ; (6)\n(act, fl) = (a, a\u2019fl) ~-- (a, fla\u2019) ; (7)\n(aA, ~) ~- (a, flA\u2019) = (A, a\u2019fi) (8)\nFrom Eq. (7) and (8) it follows that if u and U are orthogonal \ntrices (u u\u2019 --- u\u2019 u ~- ln, U U\u2019 ----- U\u2019 U ~- 1N), then\n(ua U\u2019, u fl U\u2019) = (a, fl) (9)\nAnother useful proposition is the following: if (a, b) ~ 0 for all\nsymmetric (skew-symmetric) matrices, b, then a is skew-symmetric\n(symmetric).\nThe solution of the problem is much simplified by an appeal to\ntwo theorems which are generalizations of well-known theorems on\nsquare matrices/f They will not be proven here.\n*Quoted from MacDuffee, \"Theory of Matrices\", Ergebn. d. Mathem., v.\n2, No. 5 p. 80 (19~).\ntCourant and Hilbert, \"Methoden der mathematischen Physik\" Berlin, 1924;\npp. 9 et seq., p. 25. MacDuffee, p. 78.\nCARL ECKART AND GALE YOUNG 213\nTheorem I. For any rea~ matrix a, two ortkogonal m~trices u and\nU c~. be found so that ~ ~ uaU\u2019 is a real diagonal matrix with no\nnegative ele~ents.\nA diagonal matrix ~ (square or rectangular) is one for which\ni;; ~- 0 unless i ~- .i. If a diagonal matrix is rectangular, then there\nwill be some rows or columns which consist entirely of zeros. For the\nfollowing, this remark is of some importance, as will be seen. The\nequation of the theorem may also be written\na~u\" ~ U (10)\nwhose right side may be called the canonic resolution of a. If n ~ N,\n~ will have N~n columns of zeros and a is seen to depend only on the\nfirst n rows of U. If u, ~ and the first n rows of U are given, a is\ndetermined.\nLet v be the diagonal n ~ n matrix which consists of the first n\ncolumns of ~, and ~o the n ~( N matrix composed of the first n rows\nof U; then these remarks can be summarized by the equation\na = u\n~ v ~o (10.1)\nwhere ~o o)\u2019 ----- 1,, but ~o\u2019 ~o =/= 1~.. For numerical work, Eq. (10.1) \npreferable to Eq. (10) ; for formal manipulation, Eq. (10) is \nconvenient.\nThe numerical evaluation of u and v (or ~) can be accomplished\nfrom the consideration of the matrix a ~- a a\" alone. This matrix is\nclosely related to the matrix of correlation coefficients of the tests. It\nis seen that\nand since ~ ),\u2019 ~ ~- is adiagonal matrix, it fol lows that u i s oneof t he\northogonal matrices which transform the correlational matrix to di\u0002agonal form. The rows of u are unit vectors along the .principal axes\nof ~ and the squares of the diagonal elements of v (or ~) are the\ncharacteristic values of a; this shows that the latter can never be\nnegative numbers, a result which can also be obtained more directly.*\nThe methods for determining the principal axes and characteristic\nvalues of a symmetric matrix are also known,? so that these remarks\nmay be considered as indicating the method for calculating u and ~.\nIf none of the characteristic values of a is zero (this will presumably\nbe the case in the overwhelming proportion of actual calculations)\n*Courant-Hilbert, p. 20.\n\u00a2Courant-Hilbert, pp. 13, 16.\n214 PSYCHOMETRIKA\nthe matrix v will have a reciprocal, and co can be obtained by solving\nEq. (10.1) \nco--- V-1 U \u00a3t .\nThe numerical values of the elements in the remaining rows of the\nmatrix U will not be needed, but could be found if necessary. For\nsimplicity of manipulation, it is convenient to proceed as though this\nhas been done.\nThe diagonal elements of 2 were called the \"canonical multipliers\"\nof a by Sylvester.* The multipliers and characteristic values of a cor\u0002relational matrix are identical; in the case of a symmetric matrix,\nthere may be a difference in sign; for a general square matrix, there\nis no simple relation between the two; for a rectangular matrix, the\ncharacteristic values are not defined.\nThe correlational matrices, Sylvester called the \"false squares\"\nof a. In the foregoing (and in the usual treatment of factor theory)\nonly the matrix a ~ a a\" has been considered. However, the matrix\nA ~ a\u2019a is related to the correlation coefficients of the individuals in\nthe same manner as a is related to the correlation coefficients of the\ntests. There is complete mathematical symmetry between the two\ncorrelation matrices.\nTo every multiplier, there is associated a row of u and a row of\nU; this complex of n ~ N -~- 1 numbers may be called a canonic com\u0002ponent of a.\nTheorem II. If a ~\" and ~\u2019 a are both symmetric matrices, then\nand only then can two arthogonal matrices u and U be found such\nthat ~ -~ u a U\u2019 and ~ ~ u fl U\" are both real diagonal matrices.\nEither one (but in general, not both) of the diagonal matrices\nmay be further restricted to have no negative elements. This theorem\nis a generalization of the theorem that the principal axes of two sym\u0002metric matrices coincide if and only if ab -- ha.\nSolution of the problem\nThe distance of fl from a is given by x, where\nx2~ (a,a) --2(a, fl) ~- (fl, fl) ; (11)\nx is a function of all the elements of fl, and these are to be determined\ngo that its value is a minimum. The elements of fl are not all indepen-\n.dent, however, because of the requirement that its rank be less\nthan the number of its rows or columns. Theorem I makes it possible\n*Messeng. Math., 1~ p. 45 (1889).\nCARL ECKART AND GALE YOUNG 215\nto eliminate some of the interdependence: suppose ~ to have been re\u0002solved into canonic form:\nfl~u\u2019~ V (12)\nwith ~ diagonal, and u and U orthogonal matrices. Then the rank of\nfl will be r if and only if/~ has this rank i.e., if just r of the diagonal\nelements of ~ are different from zero; the non-vanishing elements of\n~ will be independent. However, the elements of u or U will not be in\u0002dependent, since these matrices must be orthogonal. It is not neces\u0002sary to express these matrices in terms of independent parameters\nbecause of the following proposition:* if u is any orthogonal matrix\nand the independent variables that determine it are given any infini\u0002tesimal increments, the resulting increment of u is\n~u=us , (13)\nwhere s is a skew-symmetric matrix whose elements are infinitesimal,\nbut otherwise arbitrary.\nThe Eq. (11) becomes, because of Eq. (12) and \nx2~ (a,a) --2(a,u\u2019~ U).~t_ (~,~u) (14)\nSince x is to be a minimum, it follows that ~ x\n2 ~ 0 when u is given\nthe increment ~ u (Eq. (13)).\nHence\n0 ~ (a, -- s u\" ~ U) ~ -- (a, s fl) ~ -- (a ~\u2019, (15)\nSince s is an arbitrary skew-symmetric matriX, it follows that a ~t\u2019\nmust be symmetric. Discussing the increment of U in the same man\u0002ner, it will be found that ~/Ya must also be symmetric, and hence, by\nTheorem II, the orthogonal matrices can be found so that Eq. (12)\nand\na ~u\u2019 2 U (12.1)\n(with X the diagonal matrix of the multipliers of a) are both valid.\nThen Eq. (11) becomes\nx\n~ ~ ()~--~, ;t--r)\n--~]~ (2~ -- ~u~) 2 (11.1)\n2i and #~ being the diagonal elements of the corresponding matrices.\nIt remains to determine the matrix ~ so that this expression has\nits minimum value, subject to the condition that just r of the #~ shall\n*Courant-Hitbert, p. 2.7.\n216 PSYCHOMETRIKA\nbe different from zero. It may be supposed th",
          "original_query": "The approximation of one matrix by another of lower rank [9] (Eckart\u2013Young\u2013Mirsky)",
          "cleaned_query": "The approximation of one matrix by another of lower rank (Eckart\u2013Young\u2013Mirsky)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "The Rotation of Eigenvectors by a Perturbation. III",
          "url": "https://epubs.siam.org/doi/10.1137/0707001",
          "content": "[Skip to main content](https://epubs.siam.org/doi/10.1137/0707001#afterNav-co5)\n\n- Contents\n- - [Information & Authors](https://epubs.siam.org/doi/10.1137/0707001#core-collateral-info)\n- [Metrics & Citations](https://epubs.siam.org/doi/10.1137/0707001#core-collateral-metrics)\n- [Get Access](https://epubs.siam.org/doi/10.1137/0707001#core-collateral-purchase-access)\n- [References](https://epubs.siam.org/doi/10.1137/0707001#core-collateral-references)\n- [Media](https://epubs.siam.org/doi/10.1137/0707001#core-collateral-media)\n- [Tables](https://epubs.siam.org/doi/10.1137/0707001#core-collateral-tables)\n- [Share](https://epubs.siam.org/doi/10.1137/0707001#core-collateral-share)\n\n## Abstract\n\nWhen a Hermitian linear operator is slightly perturbed, by how much can its invariant subspaces change? Given some approximations to a cluster of neighboring eigenvalues and to the corresponding eigenvectors of a real symmetric matrix, and given an estimate for the gap that separates the cluster from all other eigenvalues, how much can the subspace spanned by the eigenvectors differ from the subspace spanned by our approximations? These questions are closely related; both are investigated here. The difference between the two subspaces is characterized in terms of certain angles through which one subspace must be rotated in order most directly to reach the other. These angles unify the treatment of natural geometric, operator-theoretic and error-analytic questions concerning those subspaces. Sharp bounds upon trigonometric functions of these angles are obtained from the gap and from bounds upon either the perturbation (1st question) or a computable residual (2nd question). An example is included.\n\n## Get full access to this article\n\nView all available purchase options and get full access to this article.\n\n[Get Access](https://epubs.siam.org/doi/10.1137/0707001#core-collateral-purchase-access)\n\n## References\n\n1.\n\nS. N. Afriat, On the latent vectors and characteristic values of products of pairs of symmetric idempotents, _Quart. J. Math. Oxford Ser. (2)_, 7 (1956), 76\u201378\n\n[Crossref](https://doi.org/10.1093/qmath/7.1.76)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=On+the+latent+vectors+and+characteristic+values+of+products+of+pairs+of+symmetric+idempotents&author=S.+N.+Afriat&publication_year=1956&journal=Quart.+J.+Math.+Oxford+Ser.+%282%29&pages=76&doi=10.1093%2Fqmath%2F7.1.76)\n\n2.\n\nS. N. Afriat, Orthogonal and oblique projectors and the characteristics of pairs of vector spaces, _Proc. Cambridge Philos. Soc._, 53 (1957), 800\u2013816\n\n[Crossref](https://doi.org/10.1017/S0305004100032916)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Orthogonal+and+oblique+projectors+and+the+characteristics+of+pairs+of+vector+spaces&author=S.+N.+Afriat&publication_year=1957&journal=Proc.+Cambridge+Philos.+Soc.&pages=800&doi=10.1017%2FS0305004100032916)\n\n3.\n\nChandler Davis, Separation of two linear subspaces, _Acta Sci. Math. Szeged_, 19 (1958), 172\u2013187\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Separation+of+two+linear+subspaces&author=Chandler+Davis&publication_year=1958&journal=Acta+Sci.+Math.+Szeged&pages=172)\n\n4.\n\nChandler Davis, Notions generalizing convexity for functions defined on spaces of matrices _Proc. Sympos. Pure Math., Vol. VII_, Amer. Math. Soc., Providence, R.I., 1963, 187\u2013201, (Note. This paper has several misprints, and 7 contains an error.)\n\n[Crossref](https://doi.org/10.1090/pspum/007/0155837)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Notions+generalizing+convexity+for+functions+defined+on+spaces+of+matrices&author=Chandler+Davis&publication_year=1963&pages=187&doi=10.1090%2Fpspum%2F007%2F0155837)\n\n5.\n\nChandler Davis, The rotation of eigenvectors by a perturbation, _J. Math. Anal. Appl._, 6 (1963), 159\u2013173\n\n[Crossref](https://doi.org/10.1016/0022-247X(63)90001-5)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+rotation+of+eigenvectors+by+a+perturbation&author=Chandler+Davis&publication_year=1963&journal=J.+Math.+Anal.+Appl.&pages=159&doi=10.1016%2F0022-247X%2863%2990001-5)\n\n6.\n\nChandler Davis, The rotation of eigenvectors by a perturbation. II, _J. Math. Anal. Appl._, 11 (1965), 20\u201327\n\n[Crossref](https://doi.org/10.1016/0022-247X(65)90066-1)\n\n[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=A19656891700003)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=The+rotation+of+eigenvectors+by+a+perturbation.+II&author=Chandler+Davis&publication_year=1965&journal=J.+Math.+Anal.+Appl.&pages=20&doi=10.1016%2F0022-247X%2865%2990066-1)\n\n7.\n\nJacques Dixmier, Position relative de deux vari\u00e9t\u00e9s lin\u00e9aires ferm\u00e9es dans un espace de Hilbert, _Revue Sci._, 86 (1948), 387\u2013399\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Position+relative+de+deux+vari%C3%A9t%C3%A9s+lin%C3%A9aires+ferm%C3%A9es+dans+un+espace+de+Hilbert&author=Jacques+Dixmier&publication_year=1948&journal=Revue+Sci.&pages=387)\n\n8.\n\nJacques Dixmier, \u00c9tude sur les vari\u00e9t\u00e9s et les op\u00e9rateurs de Julia, avec quelques applications, _Bull. Soc. Math. France_, 77 (1949), 11\u2013101\n\n[Crossref](https://doi.org/10.24033/bsmf.1403)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=%C3%89tude+sur+les+vari%C3%A9t%C3%A9s+et+les+op%C3%A9rateurs+de+Julia%2C+avec+quelques+applications&author=Jacques+Dixmier&publication_year=1949&journal=Bull.+Soc.+Math.+France&pages=11&doi=10.24033%2Fbsmf.1403)\n\n9.\n\nNelson Dunford, Jacob T. Schwartz, _Linear operators. Part II: Spectral theory. Self adjoint operators in Hilbert space_, With the assistance of William G. Bade and Robert G. Bartle, Interscience Publishers John Wiley & Sons New York-London, 1963ix+pp. 859\u20131923+7\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Linear+operators.+Part+II%3A+Spectral+theory.+Self+adjoint+operators+in+Hilbert+space&author=Nelson+Dunford&author=Jacob+T.+Schwartz&publication_year=1963)\n\n10.\n\nS. Falk, Einschliessungss\u00e4tze f\u00fcr die Eigenvektoren normaler Matrizenpaare, _Z. Angew. Math. Mech._, 45 (1965), 47\u201356\n\n[Crossref](https://doi.org/10.1002/zamm.19650450107)\n\n[Web of Science](https://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&DestApp=WOS_CPL&UsrCustomerID=5e3815c904498985e796fc91436abd9a&SrcAuth=atyponcel&SrcApp=literatum&DestLinkType=FullRecord&KeyUT=A19656405100006)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Einschliessungss%C3%A4tze+f%C3%BCr+die+Eigenvektoren+normaler+Matrizenpaare&author=S.+Falk&publication_year=1965&journal=Z.+Angew.+Math.+Mech.&pages=47&doi=10.1002%2Fzamm.19650450107)\n\n11.\n\nDonald A. Flanders, K. O. Friedrichs, O. E. Neugebauer, J. J. Stoker, Angles between flat subspaces of a real n-dimensional Euclidean space, _Studies and Essays Presented to R. Courant on his 60th Birthday, January 8, 1948_, Interscience Publishers, Inc., New York, 1948, 129\u2013138\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Angles+between+flat+subspaces+of+a+real+n-dimensional+Euclidean+space&author=Donald+A.+Flanders&publication_year=1948&pages=129)\n\n12.\n\nI. C. Gohberg, M. G. Kre\u02d8\ud835\udc56i\u02d8n, _Introduction to the Theory of Non-Self-Adjoins Linear Operators in Hilbert Space_, Nauka, Moscow, 1965, (In Russian.)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Introduction+to+the+Theory+of+Non-Self-Adjoins+Linear+Operators+in+Hilbert+Space&author=I.+C.+Gohberg&author=M.+G.+Krei%CB%98n&publication_year=1965)\n\n13.\n\nPaul R. Halmos, _Introduction to Hilbert Space and the theory of Spectral Multiplicity_, Chelsea Publishing Company, New York, N. Y., 1951, 114\u2013\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Introduction+to+Hilbert+Space+and+the+theory+of+Spectral+Multiplicity&author=Paul+R.+Halmos&publication_year=1951&pages=114)\n\n14.\n\nCamille Jordan, Essai sur la g\u00e9om\u00e9trie \u00e0 _n_ dimensions, _Bull. Soc. Math. France_, 3 (1875), 103\u2013174\n\n[Crossref](https://doi.org/10.24033/bsmf.90)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Essai+sur+la+g%C3%A9om%C3%A9trie+%C3%A0+n+dimensions&author=Camille+Jordan&publication_year=1875&journal=Bull.+Soc.+Math.+France&pages=103&doi=10.24033%2Fbsmf.90)\n\n15.\n\nW. Kahan, _Inclusion theorems for clusters of eigenvalues of Hermitian matrices_, Department of Computer Science, University of Toronto, Toronto, Ontario, 1967\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Inclusion+theorems+for+clusters+of+eigenvalues+of+Hermitian+matrices&author=W.+Kahan&publication_year=1967)\n\n16.\n\nTosio Kato, On the upper and lower bounds of eigenvalues, _J. Phys. Soc. Japan_, 4 (1949), 334\u2013339\n\n[Crossref](https://doi.org/10.1143/JPSJ.4.334)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=On+the+upper+and+lower+bounds+of+eigenvalues&author=Tosio+Kato&publication_year=1949&journal=J.+Phys.+Soc.+Japan&pages=334&doi=10.1143%2FJPSJ.4.334)\n\n17.\n\nTosio Kato, _Perturbation theory for linear operators_, Die Grundlehren der mathematischen Wissenschaften, Band 132, Springer-Verlag New York, Inc., New York, 1966xix+592, Berlin\n\n[Crossref](https://doi.org/10.1007/978-3-642-53393-8)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Perturbation+theory+for+linear+operators&author=Tosio+Kato&publication_year=1966&doi=10.1007%2F978-3-642-53393-8)\n\n18.\n\nM. G. Kreinodo\u0165n, M. A. Krasnosel'skiinodo\u0165, D. P. Mil'man, Defect numbers of linear operators in Banach space and some geometrical problems, _Sbornik Trudov Institute Matematiki Akademii Nauk SSSR_, (1948), 97\u2013112, (In Russian.)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?title=Defect+numbers+of+linear+operators+in+Banach+space+and+some+geometrical+problems&author=M.+G.+Kreinodo%C5%A5n&author=M.+A.+Krasnosel%27skiinodo%C5%A5&author=D.+P.+Mil%27man&publication_year=1948&journal=Sbornik+Trud",
          "original_query": "The rotation of eigenvectors by a perturbation [6] (Davis\u2013Kahan)",
          "cleaned_query": "The rotation of eigenvectors by a perturbation (Davis\u2013Kahan)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Perturbation Theory for Linear Operators - Springer Link",
          "url": "https://link.springer.com/book/10.1007/978-3-642-66282-9",
          "content": "Perturbation Theory for Linear Operators | Springer Nature Link (formerly SpringerLink)\n[Skip to main content](#main-content)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/book/10.1007/978-3-642-66282-9?)\n[![](https://media.springernature.com/w90/springer-static/cover-hires/book/978-3-642-66282-9?as=webp)](https://link.springer.com/book/10.1007/978-3-642-66282-9/cover)\n# Perturbation Theory for Linear Operators\n* Book\n* &copy;1995\n* 2nd edition\n* [View latest edition](https://link.springer.com/book/9783540586616)\n[Accessibility Information](#accessibility-information)\n## Overview\nAuthors:\n* [Tosio Kato](#author-0-0)[0](#Aff-0-0)\n1. Tosio Kato\n1. University of California, Berkeley, USA\n[View author publications](https://link.springer.com/search?dc.creator=Tosio+Kato&sortBy=newestFirst)\nSearch author on:[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tosio+Kato)[Google Scholar](http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=\"Tosio+Kato\"&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en)\nPart of the book series:[Classics in Mathematics](https://link.springer.com/series/3242)(CLASSICS, volume 132)\n* 105kAccesses\n* 3592Citations\n* 73[Altmetric](https://link.altmetric.com/details/4291888)\nThis is a preview of subscription content,[log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/book/10.1007/978-3-642-66282-9?error=cookies_not_supported&code=63c04109-fddf-48ac-a905-6d1f77d2d17c)to check access.\n## Access this book\n[Log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/book/10.1007/978-3-642-66282-9?error=cookies_not_supported&code=63c04109-fddf-48ac-a905-6d1f77d2d17c)\nSoftcover BookUSD59.99\nPrice excludes VAT (USA)\n* Compact, lightweight edition\n* Dispatched in 3 to 5 business days\n* Free shipping worldwide -[see info](https://support.springernature.com/en/support/solutions/articles/6000233448-coronavirus-disease-covid-19-delivery-information)Buy Softcover Book\nTax calculation will be finalised at checkout\n[Licence this eBook for your library](https://single-ebooks.springernature.com/search?query=10.1007/978-3-642-66282-9)\n[Learn about institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n## Other ways to access\n[Licence this eBook for your library](https://single-ebooks.springernature.com/search?query=10.1007/978-3-642-66282-9)\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n## About this book\nIn view of recent development in perturbation theory, supplementary notes and a supplementary bibliography are added at the end of the new edition. Little change has been made in the text except that the para\u00ad graphs V-\u00a7 4.5, VI-\u00a7 4.3, and VIII-\u00a7 1.4 have been completely rewritten, and a number of minor errors, mostly typographical, have been corrected. The author would like to thank many readers who brought the errors to his attention. Due to these changes, some theorems, lemmas, and formulas of the first edition are missing from the new edition while new ones are added. The new ones have numbers different from those attached to the old ones which they may have replaced. Despite considerable expansion, the bibliography i\" not intended to be complete. Berkeley, April 1976 TosIO RATO Preface to the First Edition This book is intended to give a systematic presentation of perturba\u00ad tion theory for linear operators. It is hoped that the book will be useful to students as well as to mature scientists, both in mathematics and in the physical sciences.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3Aplaceholder%2Fimages/placeholder-figure-springernature.png)\n### [On new exponential-type operators](https://link.springer.com/10.1007/s13398-022-01302-9?fromPaywallRec=true)\nArticle27 July 2022\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-10819-9?as&#x3D;webp)\n### [Spectrum and Pseudo-Spectrum](https://link.springer.com/10.1007/978-3-030-10819-9_2?fromPaywallRec=true)\nChapter\u00a9 2019\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-024-64985-7/MediaObjects/41598_2024_64985_Fig1_HTML.png)\n### [Investigating pseudo parabolic dynamics through phase portraits, sensitivity, chaos and soliton behavior](https://link.springer.com/10.1038/s41598-024-64985-7?fromPaywallRec=true)\nArticleOpen access02 July 2024\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects.\n* [Differential Equations](https://link.springer.com/subjects/differential-equations)\n* [Calculus of Variations and Optimization](https://link.springer.com/subjects/calculus-of-variations-and-optimization)\nSearch within this book\nSearch\n## Table of contents (10 chapters)\n1. ### Front Matter\nPages I-XXI\n[Download chapterPDF](https://link.springer.com/content/pdf/bfm:978-3-642-66282-9/1)\n2. ### [Operator theory in finite-dimensional vector spaces](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_1)\n* Tosio Kato\nPages 1-62\n* ### [Perturbation theory in a finite-dimensional space](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_2)\n* Tosio Kato\nPages 62-126\n* ### [Introduction to the theory of operators in Banach spaces](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_3)\n* Tosio Kato\nPages 126-188\n* ### [Stability theorems](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_4)\n* Tosio Kato\nPages 189-250\n* ### [Operators in Hilbert spaces](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_5)\n* Tosio Kato\nPages 251-308\n* ### [Sesquilinear forms in Hilbert spaces and associated operators](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_6)\n* Tosio Kato\nPages 308-364\n* ### [Analytic perturbation theory](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_7)\n* Tosio Kato\nPages 364-426\n* ### [Asymptotic perturbation theory](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_8)\n* Tosio Kato\nPages 426-479\n* ### [Perturbation theory for semigroups of operators](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_9)\n* Tosio Kato\nPages 479-515\n* ### [Perturbation of continuous spectra and unitary equivalence](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_10)\n* Tosio Kato\nPages 516-567\n* ### Back Matter\nPages 568-623\n[Download chapterPDF](https://link.springer.com/content/pdf/bbm:978-3-642-66282-9/1)\n[Back to top](#back-to-top)\n## Reviews\n\"The monograph by T. Kato is an excellent textbook in the theory of linear operators in Banach and Hilbert spaces. It is a thoroughly worthwhile reference work both for graduate students in functional analysis as well as for researchers in perturbation, spectral, and scattering theory.\nIn chapters 1, 3, 5 operators in finite-dimensional vector spaces, Banach spaces and Hilbert spaces are introduced. Stability and perturbation theory are studied in finite-dimensional spaces (chapter 2) and in Banach spaces (chapter 4). Sesquilinear forms in Hilbert spaces are considered in detail (chapter 6), analytic and asymptotic perturbation theory is described (chapter 7 and 8). The fundamentals of semigroup theory are given in chapter 9. The supplementary notes appearing in the second edition of the book gave mainly additional information concerning scattering theory described in chapter 10.\nThe first edition is now 30 years old. The revised edition is 20 years old. Nevertheless it is a standard textbook for the theory of linear operators. It is user-friendly in the sense that any sought after definitions, theorems or proofs may be easily located. In the last two decades much progress has been made in understanding some of the topics dealt with in the book, for instance in semigroup and scattering theory. However the book has such a high didactical and scientific standard that I can recomment it for any mathematician or physicist interested in this field.\n*Zentralblatt MATH, 836*\n## Authors and Affiliations\n* ### University of California, Berkeley, USA\nTosio Kato\n## About the author\n**Biography of Tosio Kato**\nTosio Kato was born in 1917 in a village to the north of Tokyo. He studied theoretical physics at the Imperial University of Tokyo. After several years of inactivity during World War II due to poor health, he joined the Faculty of Science at the University of Tokyo in 1951. From 1962 he was Professor of Mathematics at the University of California, Berkeley, where he is now Professor Emeritus.\nKato was a pioneer in modern mathematical physics. He worked in te areas of operator theory, quantum mechanics, hydrodynamics, and partial differential equations, both linear and nonlinear.\n## Accessibility Information\n### PDF accessibility summary\nThis PDF is not accessible. It is based on scanned pages and does not support features such as screen reader compatibility or described non-text content (images, graphs etc). However, it likely supports searchable and selectable text based on OCR (Optical Character Recognition). Users with accessibility needs may not be able to use this content effectively. Please contact us at[accessibilitysupport@springernature.com](mailto:accessibilitysupport@springernature.com)if you require assistance or an alternative format.\n### EPUB accessibility summary\nThis ePub was produced prior to the implementation of current accessibility best practices. If Math is present, it is represented either as MathML, LaTeX or in images. Text alternatives are not available for images, graphs and other non-text content. W",
          "original_query": "Perturbation Theory for Linear Operators [18] (Kato)",
          "cleaned_query": "Perturbation Theory for Linear Operators (Kato)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Functions of Matrices | SIAM Publications Library",
          "url": "https://epubs.siam.org/doi/10.1137/1.9780898717778",
          "content": "The FM Web Site (link) Errata (link) Matrix functions are of growing interest due to their fascinating theory and the many applications in which they provide insight and succinct solutions. Functions of Matrices: Theory and Computation gives a thorough treatment of the theory of matrix functions and numerical methods for computing them, as well as an overview of applications. The book is useful for advanced courses and is well-suited to self-study. The broad content\u2014including f(A)-related facts, tricks, and techniques, historical references, and an appendix of background results\u2014makes it convenient as a general reference in matrix analysis and numerical linear algebra. Key features of the book: \u2022 Elegant treatment of the theory of matrix functions, exploiting the equivalent definitions of f(A) via the Jordan form, polynomial interpolation, and the Cauchy integral formula. \u2022 Develops theory of conditioning and properties of the Fr\u00e9chet derivative. \u2022 Emphasizes Schur decomposition, block Parlett recurrence, and judicious use of Pad\u00e9 approximants. \u2022 General results on convergence and stability of matrix iterations. \u2022 Detailed treatment of the matrix sign function, matrix roots, the polar decomposition, and transcendental matrix functions (exponential, logarithm, cosine, sine). \u2022 Thorough analysis of the accuracy, stability, and computational cost of numerical methods. \u2022 A chapter devoted to the f(A)b problem. \u2022 Extensive collection of problems with solutions. \u2022 Matrix Function Toolbox provides MATLAB\u00ae implementations of key algorithms. Audience This book is for specialists in numerical analysis and applied linear algebra as well as anyone wishing to learn about the theory of matrix functions and state of the art methods for computing them. It can be used for a graduate-level course on functions of matrices and is a suitable reference for an advanced course on applied or numerical linear algebra. It is also particularly well suited for self-study. About the Author Nicholas J. Higham, FRS, is Richardson Professor of Applied Mathematics at The University of Manchester, UK. He is the author of more than 100 publications and of the books Accuracy and Stability of Numerical Algorithms (SIAM, 2nd ed., 2002), Handbook of Writing for the Mathematical Sciences, (SIAM, 2nd ed., 1998), and MATLAB Guide, (with Desmond J. Higham, SIAM, 2nd ed., 2005).",
          "original_query": "Functions of Matrices: Theory and Computation [29] (Higham)",
          "cleaned_query": "Functions of Matrices: Theory and Computation (Higham)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Optimal Bounds for Privacy-Preserving Principal Component Analysis",
          "url": "http://kunaltalwar.org/papers/PrivatePCA.pdf",
          "content": "Analyze Gauss: Optimal Bounds for Privacy-Preserving Principal\nComponent Analysis\nCynthia Dwork Kunal Talwar Abhradeep Thakurta\u2217 Li Zhang\nMicrosoft Research Microsoft Research Stanford University Microsoft Research\nMicrosoft Research\nAbstract\nWe consider the problem of privately releasing a low dimensional approximation to a set of data\nrecords, represented as a matrix A in which each row corresponds to an individual and each column to\nan attribute. Our goal is to compute a subspace that captures the covariance of A as much as possible,\nclassically known as principal component analysis (PCA). We assume that each row of A has `2 norm\nbounded by one, and the privacy guarantee is defined with respect to addition or removal of any single\nrow. We show that the well-known, but misnamed, randomized response algorithm, with properly tuned\nparameters, provides nearly optimal additive quality gap compared to the best possible singular subspace\nof A. We further show that when AT A has a large eigenvalue gap \u2013 a reason often cited for PCA \u2013\nthe quality improves significantly. Optimality (up to logarithmic factors) is proved using techniques\ninspired by the recent work of Bun, Ullman, and Vadhan on applying Tardos\u2019s fingerprinting codes to\nthe construction of hard instances for private mechanisms for 1-way marginal queries. Along the way\nwe define a list culling game which may be of independent interest.\nBy combining the randomized response mechanism with the well-known following the perturbed\nleader algorithm of Kalai and Vempala we obtain a private online algorithm with nearly optimal regret.\nThe regret of our algorithm even outperforms all the previously known online non-private follow the\nperturbed leader type of algorithms. We achieve this better bound by, satisfyingly, borrowing insights\nand tools from differential privacy!\n1 Introduction\nIn areas as diverse as machine learning, statistics, information retrieval, earth sciences, archaeology, and\nimage processing, given a data set represented by a matrix A \u2208 R\nm\u00d7n\n, it is often desirable to find a good\napproximation to A that has low rank. Working with low-rank approximations improves space and time\nefficiency. Other benefits include removal of noise and extraction of correlations, useful, for example, in\n(approximate) matrix completion from a small set of observations \u2013 an impossible task if A is arbitrary but\npotentially feasible if A enjoys a good low rank approximation. The problem of low-rank approximation has\nalso received substantial attention in the differential privacy literature [4, 15, 31, 26, 10, 21, 22]. If we think\nof the matrix A \u2208 R\nm\u00d7n\nas containing information about n attributes of m individuals, the goal is to learn\n\u201cabout\u201d A (we intentionally remain vague, for now) without compromising the privacy of any individual.\nThat is, the literature focuses on being able to do, in a differentially private way, whatever is achieved by\nlow-rank approximation in the non-private literature. Our work continues this line of research.\nExisting differentially private algorithms can have errors with an unfortunate dependence on the ambient\ndimension n of the data. This bad dependence may sometimes be due to the suboptimality of our algorithms,\n\u2217\nSupported in part by the Sloan Foundation\n1\nsometimes due to the inherent difficulty of the problem. A driving motivation for our work is to extract better\nperformance from these algorithms when the inherent dimensionality of the input is much lower than the\nambient dimension. For example, the data may be generated according to a low dimensional model and the\nmeasurements may be noisy.\nThe standard method of the principal component analysis (PCA) for low rank approximation is to compute a\nbest low-dimensional eigen-subspace B of the matrix AT A =\nPm\ni=1 a\nT\ni\nai (recall that the ai are row vectors).\nThe underlying intuition is that the projection onto B preserves the important features of the data rows while\nprojecting away the noise. We will focus on a private mechanism for computing B. By (1) privately finding\na low-rank subspace B capturing most of the variance in A, and then (2) running the existing differentially\nprivate algorithm on the projection of A onto B, the hope is that poor dependence on the dimension in the\nsecond step is mitigated by the dimension reduction obtained in the first.\nBecause it was found in a privacy-preserving fashion, B can safely be made public. A key point is that\nthe two-step procedure just described does not require publication of the projection. This, then, will be our\napproach: the projector (\u03a0B) will be public, the projection (\u03a0B(A)) will not be released. 1.\nThe literature sometimes focuses on the case of m \u001d n, and at other times assumes m \u001c n. In the first\ncase, the rows of the data matrix are often assumed to be normalized to have norm at most 1, as is done here;\nwhen m \u001c n the row norms may be unbounded [21, 22]. The literature also varies in terms of granularity\nof the privacy guarantee, protecting, variously, the privacy of each row in its entirety [4, 15, 26, 10], which\nis what we do here, or individual entries [31, 22], or norm 1 changes to any row [21]. Finally, the literature\nvaries on the nature of differential privacy offered: so-called pure, or (\u000f, 0)-differential privacy [15, 26, 10]\nand approximate, or (\u000f, \u03b4), differential privacy [4, 31, 21, 22], which is the notion used in our work.\nRefined Randomization: Blum et al. were the first to suggest privately releasing AT A by adding indepen\u0002dent noise to each of the n\n2\nentries of this matrix [4]. The data analyst is then free to compute best rank k\napproximations to the privacy preserving, noisy, A[T A for any and all k. This na\u00a8\u0131ve noising approach, which\nhas somewhat erroneously become known as randomized response, was refined in [15] to add less noise;\nour main algorithmic result is a careful analysis of a version of this refinement. Specifically, we will use the\nGaussian mechanism [13], which adds independently chosen Gaussian noise to each entry of AT A. When\nthere is a gap in the singular values of A, or even a gap between singular values whose indices are not\nadjacent (formally \u03c3\n2\nk \u2212 \u03c3\n2\nk\n0 \u2208 \u03c9(\n\u221a\nn/(k + k\n0\n))), we see a clear improvement, in captured variance, over\npreviously published results. In this case, the analysis further shows, the space spanned by the top k right\nsingular vectors of the (refined) noisy version of AT A is very close to the space spanned by the top k right\nsingular vectors of A, with the spectral norm of the difference in projectors being independent of k.\nWhen there is no gap the algorithm performs no worse than the best in the literature; when m \u001d n we do\nexpect such a gap: the more data, the better the algorithm\u2019s utility. The algorithm approaches the correct\nsubspace of AT A at a rate faster than 1/m, meaning that as we increase the number of samples the total\nerror decreases.\nOptimality: Our version of the refined noisy release of AT A is, up to logarithmic factors, optimal for\napproximate differential privacy. Pursuing a connection between differentially private algorithms and cryp\u0002tographic traitor-tracing schemes [17], Bun, Ullman, and Vadhan [6] established lower bounds on errors for\napproximately differentially private release of a class of counting queries that are tight to within logarithmic\nfactors. Their query class is based on a class of fingerprinting codes [5] due to Tardos [43]. We show that\n1This was exploited by McSherry and Mironov in their work on differentially private recommendation systems [31]: in many\nnon-private recommendation systems, recommendations made to individual i depend only on the item covariance information\nand the individual\u2019s own item ratings. In our terms, the recommendations to user i depend only on row i of the input matrix A\nand on A\nT A. It makes no sense to hide the user\u2019s own ratings from himself, so it is sufficient that AT A be approximated in a\nprivacy-protective fashion.\n2\ntheir result translates fairly easily to a lower bound for private approximation of the top singular vector. We\nalso extend this to obtain lower bounds for rank k subspace estimation even for k \u2208 \u2126(n), a much more\nchallenging task. Intuitively, for k > 1, we construct k \u201cclusters\u201d of fingerprinting codes. We have to over\u0002come some difficulties to show that these clusters do not interfere much and to identify a \u201cprivacy-violating\u201d\nvector hidden in a subspace. For the first we prove a stronger property of Tardos\u2019s codes, and for the second\nwe introduce a game, called the list culling game, in which one player, using \u201cplanted questions\u201d, has to\nidentify a good answer promised in a large set of answers provided by the other player. We propose a strat\u0002egy for discovering the good answer with high success probability and apply it to constructing the privacy\nlower bound. Both results might be of independent interest.\nOnline Algorithms: Our third contribution merges two lines of research: differentially private regret min\u0002imization in online algorithms [16, 38] inspired by the Follow the Perturbed Leader (FPL) algorithm of\nKalai and Vempala [25], and non-private online algorithms for principal components analysis [44]. A folk\ntheorem says that differential privacy provides stability and hence reduces generalization error. We make\nthis connection explicit in the online setting.\nIn the online model, computation proceeds in steps. At each time step t a rank k subspace Vtis output,\na single data row At of A is received, and a reward is earned equal to ||AtVt||2\n2\n. Regret is the difference\nbetween the sum of the earned rewards and the corresponding quantity for the best rank k matrix V chosen\nin hindsight (call it OPT). It is known, thanks to the pioneering work of [28], that the stability of an\nonline algorithm is useful for achieving the low regret bound2. In [25], the FPL algorithm achieves stability\nby the addition of Laplace noise and is shown to have low regret. This technique",
          "original_query": "Analyze Gauss: Optimal bounds for privacy-preserving principal component analysis [8]",
          "cleaned_query": "Analyze Gauss: Optimal bounds for privacy-preserving principal component analysis",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Bounds for Private Matrix Approximation via Dyson Brownian Motion",
          "url": "https://arxiv.org/abs/2211.06418",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: Given a symmetric matrix $M$ and a vector $\\lambda$, we present new bounds on\nthe Frobenius-distance utility of the Gaussian mechanism for approximating $M$\nby a matrix whose spectrum is $\\lambda$, under\n$(\\varepsilon,\\delta)$-differential privacy. Our bounds depend on both\n$\\lambda$ and the gaps in the eigenvalues of $M$, and hold whenever the top\n$k+1$ eigenvalues of $M$ have sufficiently large gaps. When applied to the\nproblems of private rank-$k$ covariance matrix approximation and subspace\nrecovery, our bounds yield improvements over previous bounds. Our bounds are\nobtained by viewing the addition of Gaussian noise as a continuous-time matrix\nBrownian motion. This viewpoint allows us to track the evolution of eigenvalues\nand eigenvectors of the matrix, which are governed by stochastic differential\nequations discovered by Dyson. These equations allow us to bound the utility as\nthe square-root of a sum-of-squares of perturbations to the eigenvectors, as\nopposed to a sum of perturbation bounds obtained via Davis-Kahan-type theorems.\n \n \n \n \n Submission history From: Oren Mangoubi [ view email]\n [v1] \nFri, 11 Nov 2022 18:54:01 UTC (58 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > cs > arXiv:2211.06418\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Data Structures and Algorithms\n\n arXiv:2211.06418 (cs)\n [Submitted on 11 Nov 2022]\n\n Title: Re-Analyze Gauss: Bounds for Private Matrix Approximation via Dyson Brownian Motion\n\n Authors: Oren Mangoubi, Nisheeth K. Vishnoi\n Download PDF\n Abstract: Given a symmetric matrix $M$ and a vector $\\lambda$, we present new bounds on the Frobenius-distance utility of the Gaussian mechanism for approximating $M$ by a matrix whose spectrum is $\\lambda$, under $(\\varepsilon,\\delta)$-differential privacy. Our bounds depend on both $\\lambda$ and the gaps in the eigenvalues of $M$, and hold whenever the top $k+1$ eigenvalues of $M$ have sufficiently large gaps. When applied to the problems of private rank-$k$ covariance matrix approximation and subspace recovery, our bounds yield improvements over previous bounds. Our bounds are obtained by viewing the addition of Gaussian noise as a continuous-time matrix Brownian motion. This viewpoint allows us to track the evolution of eigenvalues and eigenvectors of the matrix, which are governed by stochastic differential equations discovered by Dyson. These equations allow us to bound the utility as the square-root of a sum-of-squares of perturbations to the eigenvectors, as opposed to a sum of perturbation bounds obtained via Davis-Kahan-type theorems.\n Comments: This is the full version of a paper which was accepted to NeurIPS 2022 \n Subjects: Data Structures and Algorithms (cs.DS) ; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML)\n Cite as: arXiv:2211.06418 [cs.DS] \n (or arXiv:2211.06418v1 [cs.DS] for this version) \n https://doi.org/10.48550/arXiv.2211.06418 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Oren Mangoubi [view email]\n [v1] Fri, 11 Nov 2022 18:54:01 UTC (58 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n (license)\n Current browse context:\n cs.DS\n < prev | next >\n new | recent | 2211\n Change to browse by:\n cs\n cs.CR\n cs.LG\n cs.NA\n math\n math.NA\n stat\n stat.ML\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Re-analyze Gauss: Bounds for private matrix approximation via Dyson Brownian motion [19]",
          "cleaned_query": "Re-analyze Gauss: Bounds for private matrix approximation via Dyson Brownian motion",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Principal-Angle Privacy Accounting for Subspace Release\nIntroduce a new utility metric for private PCA based on the distribution of principal angles (Davis\u2019 rotation framework) rather than variance loss alone. Derive tight angle-based guarantees under Gaussian perturbations using Kato-style resolvent bounds, and show how these translate into downstream performance bounds for algorithms that consume the released subspace (e.g., regression, clustering).",
        "Private Low-Rank Approximation for Matrix Functions via f(A)b\nExtend private low-rank approximation beyond covariance/PCA to enable private evaluation of matrix-function queries of the form \\(f(M)b\\) (e.g., exp/log/sign), using Higham\u2019s algorithms plus an Eckart\u2013Young-style low-rank surrogate of \\(M\\). Prove end-to-end privacy and give error bounds that decompose into (i) low-rank truncation error and (ii) perturbation error propagated through the Fr\u00e9chet derivative of \\(f\\).",
        "Dyson-Brownian Bounds for Streaming/Online Private PCA\nBuild an online private PCA method that models each update as a small perturbation and uses Dyson Brownian motion to accumulate subspace error as a sum-of-squares process (rather than worst-case additive Davis\u2013Kahan bounds). Provide regret-style guarantees (in the style of private FPL) where the dependence on time horizon improves when eigenvalue gaps persist over time.",
        "Robust (Row-Outlier) Differentially Private PCA Using Perturbation-Stable Trimming\nPropose a DP-PCA pipeline that first performs a private, perturbation-stable trimming/winsorization step on rows (bounded influence) and then runs a gap-adaptive private PCA. Analyze how trimming changes the effective spectrum and eigen-gaps, and prove improved subspace recovery when a small fraction of rows are adversarial outliers.",
        "Spectrum-Constrained Private Matrix Release with Optimal Frobenius Error\nGeneralize \u201capproximate \\(M\\) by a matrix with prescribed spectrum \\(\\lambda\\)\u201d to allow releasing a spectrum-constrained low-rank approximation under DP, with \\(\\lambda\\) chosen to optimize a downstream objective (e.g., explained variance at rank k). Use Dyson-Brownian analysis to characterize how the chosen \\(\\lambda\\) interacts with eigen-gaps, yielding an algorithm that selects \\(\\lambda\\) privately and near-optimally.",
        "Certified Private Factor Models: From Eckart\u2013Young to DP Factor Analysis\nTranslate the Eckart\u2013Young canonical low-rank approximation view into a DP factor-analysis procedure that releases factors/loadings with guarantees on reconstruction error and identifiability up to rotation. Use invariant-subspace perturbation theory to quantify how much the latent-factor subspace can rotate under privacy noise, and provide conditions (gap/separation) under which factors are interpretable and stable.",
        "Private Block-Schur/Parlett Methods for Structured Covariances\nDevelop DP mechanisms tailored to structured covariance matrices (e.g., block-diagonal, banded, or Kronecker-structured) by combining (i) private estimation of the structured parameters and (ii) stable computation via Schur decomposition and block Parlett recurrences (Higham). Prove tighter utility bounds than unstructured Gaussian mechanisms by exploiting reduced degrees of freedom and larger effective spectral gaps in structured settings.",
        "Joint DP Guarantees for Eigenvalues and Eigenvectors via Resolvent-Based Sensitivity\nCreate a unified analysis that simultaneously bounds the private release error of eigenvalues and the rotation of eigenvectors using resolvent techniques from Kato (rather than treating them separately). Derive sensitivity/condition-number measures that predict when eigenvectors are stable enough to release accurately under DP, and validate the theory by matching it against Dyson-Brownian predictions on synthetic and real covariance datasets."
      ],
      "generated_ideas_raw": "1. **Gap-Adaptive Private Truncated SVD with Certified Subspace Error**\n   Develop a differentially private truncated-SVD algorithm that explicitly adapts its noise level to an estimated top-(k+1) eigenvalue gap, combining the Analyze Gauss mechanism with Dyson Brownian-motion-inspired tracking of eigenvector drift. Provide a post hoc certificate (computable from released quantities) giving a high-probability bound on principal angles between the released and true top-k subspaces.\n\n2. **Principal-Angle Privacy Accounting for Subspace Release**\n   Introduce a new utility metric for private PCA based on the distribution of principal angles (Davis\u2019 rotation framework) rather than variance loss alone. Derive tight angle-based guarantees under Gaussian perturbations using Kato-style resolvent bounds, and show how these translate into downstream performance bounds for algorithms that consume the released subspace (e.g., regression, clustering).\n\n3. **Private Low-Rank Approximation for Matrix Functions via f(A)b**\n   Extend private low-rank approximation beyond covariance/PCA to enable private evaluation of matrix-function queries of the form \\(f(M)b\\) (e.g., exp/log/sign), using Higham\u2019s algorithms plus an Eckart\u2013Young-style low-rank surrogate of \\(M\\). Prove end-to-end privacy and give error bounds that decompose into (i) low-rank truncation error and (ii) perturbation error propagated through the Fr\u00e9chet derivative of \\(f\\).\n\n4. **Dyson-Brownian Bounds for Streaming/Online Private PCA**\n   Build an online private PCA method that models each update as a small perturbation and uses Dyson Brownian motion to accumulate subspace error as a sum-of-squares process (rather than worst-case additive Davis\u2013Kahan bounds). Provide regret-style guarantees (in the style of private FPL) where the dependence on time horizon improves when eigenvalue gaps persist over time.\n\n5. **Private Spectral Gap Estimation with Minimal Additional Privacy Cost**\n   Design a mechanism that privately estimates the top-(k+1) eigenvalue gap (or a lower bound on it) of \\(A^\\top A\\) using carefully calibrated noise and stability of eigenvalues under perturbation (Kato). Use this private gap estimate to tune subsequent private PCA/low-rank approximation, and prove an overall improvement in utility compared to gap-agnostic tuning.\n\n6. **Robust (Row-Outlier) Differentially Private PCA Using Perturbation-Stable Trimming**\n   Propose a DP-PCA pipeline that first performs a private, perturbation-stable trimming/winsorization step on rows (bounded influence) and then runs a gap-adaptive private PCA. Analyze how trimming changes the effective spectrum and eigen-gaps, and prove improved subspace recovery when a small fraction of rows are adversarial outliers.\n\n7. **Spectrum-Constrained Private Matrix Release with Optimal Frobenius Error**\n   Generalize \u201capproximate \\(M\\) by a matrix with prescribed spectrum \\(\\lambda\\)\u201d to allow releasing a spectrum-constrained low-rank approximation under DP, with \\(\\lambda\\) chosen to optimize a downstream objective (e.g., explained variance at rank k). Use Dyson-Brownian analysis to characterize how the chosen \\(\\lambda\\) interacts with eigen-gaps, yielding an algorithm that selects \\(\\lambda\\) privately and near-optimally.\n\n8. **Certified Private Factor Models: From Eckart\u2013Young to DP Factor Analysis**\n   Translate the Eckart\u2013Young canonical low-rank approximation view into a DP factor-analysis procedure that releases factors/loadings with guarantees on reconstruction error and identifiability up to rotation. Use invariant-subspace perturbation theory to quantify how much the latent-factor subspace can rotate under privacy noise, and provide conditions (gap/separation) under which factors are interpretable and stable.\n\n9. **Private Block-Schur/Parlett Methods for Structured Covariances**\n   Develop DP mechanisms tailored to structured covariance matrices (e.g., block-diagonal, banded, or Kronecker-structured) by combining (i) private estimation of the structured parameters and (ii) stable computation via Schur decomposition and block Parlett recurrences (Higham). Prove tighter utility bounds than unstructured Gaussian mechanisms by exploiting reduced degrees of freedom and larger effective spectral gaps in structured settings.\n\n10. **Joint DP Guarantees for Eigenvalues and Eigenvectors via Resolvent-Based Sensitivity**\n   Create a unified analysis that simultaneously bounds the private release error of eigenvalues and the rotation of eigenvectors using resolvent techniques from Kato (rather than treating them separately). Derive sensitivity/condition-number measures that predict when eigenvectors are stable enough to release accurately under DP, and validate the theory by matching it against Dyson-Brownian predictions on synthetic and real covariance datasets.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Principal-Angle Privacy Accounting for Subspace Release\nIntroduce a new utility metric for private PCA based on the distribution of principal angles (Davis\u2019 rotation framework) rather than variance lo",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Private Low-Rank Approximation for Matrix Functions via f(A)b\nExtend private low-rank approximation beyond covariance/PCA to enable private evaluation of matrix-function queries of the form \\(f(M)b\\) ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Dyson-Brownian Bounds for Streaming/Online Private PCA\nBuild an online private PCA method that models each update as a small perturbation and uses Dyson Brownian motion to accumulate subspace error as",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Robust (Row-Outlier) Differentially Private PCA Using Perturbation-Stable Trimming\nPropose a DP-PCA pipeline that first performs a private, perturbation-stable trimming/winsorization step on rows (bou",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Spectrum-Constrained Private Matrix Release with Optimal Frobenius Error\nGeneralize \u201capproximate \\(M\\) by a matrix with prescribed spectrum \\(\\lambda\\)\u201d to allow releasing a spectrum-constrained low-r",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Certified Private Factor Models: From Eckart\u2013Young to DP Factor Analysis\nTranslate the Eckart\u2013Young canonical low-rank approximation view into a DP factor-analysis procedure that releases factors/load",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Private Block-Schur/Parlett Methods for Structured Covariances\nDevelop DP mechanisms tailored to structured covariance matrices (e.g., block-diagonal, banded, or Kronecker-structured) by combining (i)",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Joint DP Guarantees for Eigenvalues and Eigenvectors via Resolvent-Based Sensitivity\nCreate a unified analysis that simultaneously bounds the private release error of eigenvalues and the rotation of e",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 28,
      "paper_title": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization",
      "contribution": "By analyzing algorithm-dependent sample concentration and GP sample-path properties to refine information-gain estimates, the paper proves improved high-probability regret bounds for GP-UCB\u2014eO(\u221aT) under certain Mat\u00e9rn kernels and O(\u221a(T ln^2 T)) for the squared-exponential kernel.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11910,
      "output_tokens": 1136,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Gaussian Process Optimization in the Bandit Setting: No Regret and ...",
          "url": "https://icml.cc/Conferences/2010/papers/422.pdf",
          "content": "Gaussian Process Optimization in the Bandit Setting:\nNo Regret and Experimental Design\nNiranjan Srinivas niranjan@caltech.edu\nAndreas Krause krausea@caltech.edu\nCalifornia Institute of Technology, Pasadena, CA, USA\nSham Kakade skakade@wharton.upenn.edu\nUniversity of Pennsylvania, Philadelphia, PA, USA\nMatthias Seeger mseeger@mmci.uni-saarland.de\nSaarland University, Saarbr\u00a8ucken, Germany\nAbstract\nMany applications require optimizing an un\u0002known, noisy function that is expensive to\nevaluate. We formalize this task as a multi\u0002armed bandit problem, where the payoff function\nis either sampled from a Gaussian process (GP)\nor has low RKHS norm. We resolve the impor\u0002tant open problem of deriving regret bounds for\nthis setting, which imply novel convergence rates\nfor GP optimization. We analyze GP-UCB, an\nintuitive upper-confidence based algorithm, and\nbound its cumulative regret in terms of maximal\ninformation gain, establishing a novel connection\nbetween GP optimization and experimental de\u0002sign. Moreover, by bounding the latter in terms\nof operator spectra, we obtain explicit sublinear\nregret bounds for many commonly used covari\u0002ance functions. In some important cases, our\nbounds have surprisingly weak dependence on\nthe dimensionality. In our experiments on real\nsensor data, GP-UCB compares favorably with\nother heuristical GP optimization approaches.\n1. Introduction\nIn most stochastic optimization settings, evaluating\nthe unknown function is expensive, and sampling\nis to be minimized. Examples include choosing\nadvertisements in sponsored search to maximize\nprofit in a click-through model (Pandey & Olston,\n2007) or learning optimal control strategies for robots\n(Lizotte et al., 2007). Predominant approaches\nto this problem include the multi-armed bandit\nparadigm (Robbins, 1952), where the goal is to\nmaximize cumulative reward by optimally balancing\nexploration and exploitation, and experimental design\n(Chaloner & Verdinelli, 1995), where the function\nis to be explored globally with as few evaluations\nAppearing in Proceedings of the 27 th International Confer\u0002ence on Machine Learning, Haifa, Israel, 2010. Copyright\n2010 by the author(s)/owner(s).\nas possible, for example by maximizing information\ngain. The challenge in both approaches is twofold: we\nhave to estimate an unknown function f from noisy\nsamples, and we must optimize our estimate over some\nhigh-dimensional input space. For the former, much\nprogress has been made in machine learning through\nkernel methods and Gaussian process (GP) models\n(Rasmussen & Williams, 2006), where smoothness\nassumptions about f are encoded through the choice\nof kernel in a flexible nonparametric fashion. Beyond\nEuclidean spaces, kernels can be defined on diverse\ndomains such as spaces of graphs, sets, or lists.\nWe are concerned with GP optimization in the multi\u0002armed bandit setting, where f is sampled from a GP\ndistribution or has low \u201ccomplexity\u201d measured in\nterms of its RKHS norm under some kernel. We pro\u0002vide the first sublinear regret bounds in this nonpara\u0002metric setting, which imply convergence rates for GP\noptimization. In particular, we analyze the Gaussian\nProcess Upper Confidence Bound (GP-UCB) algo\u0002rithm, a simple and intuitive Bayesian method (Auer\net al., 2002; Auer, 2002; Dani et al., 2008). While\nobjectives are different in the multi-armed bandit\nand experimental design paradigm, our results draw\na close technical connection between them: our regret\nbounds come in terms of an information gain quantity,\nmeasuring how fast f can be learned in an information\ntheoretic sense. The submodularity of this function\nallows us to prove sharp regret bounds for particular\ncovariance functions, which we demonstrate for com\u0002monly used Squared Exponential and Mat\u00b4ern kernels.\nRelated Work. Our work generalizes stochastic\nlinear optimization in a bandit setting, where the un\u0002known function comes from a finite-dimensional linear\nspace. GPs are nonlinear random functions, which can\nbe represented in an infinite-dimensional linear space.\nFor the standard linear setting, Dani et al. (2008)\nGaussian Process Optimization in the Bandit Setting\nprovide a near-complete characterization explicitly\ndependent on the dimensionality. In the GP setting,\nthe challenge is to characterize complexity in a differ\u0002ent manner, through properties of the kernel function.\nOur technical contributions are twofold: first, we\nshow how to analyze the nonlinear setting by focusing\non the concept of information gain, and second, we\nexplicitly bound this information gain measure using\nthe concept of submodularity (Nemhauser et al.,\n1978) and knowledge about kernel operator spectra.\nKleinberg et al. (2008) provide regret bounds un\u0002der weaker and less configurable assumptions (only\nLipschitz-continuity w.r.t. a metric is assumed;\nBubeck et al. 2008 consider arbitrary topological\nspaces), which however degrade rapidly with the di\u0002mensionality of the problem (\u2126(T\nd+1\nd+2 )). In practice,\nlinearity w.r.t. a fixed basis is often too stringent\nan assumption, while Lipschitz-continuity can be too\ncoarse-grained, leading to poor rate bounds. Adopting\nGP assumptions, we can model levels of smoothness in\na fine-grained way. For example, our rates for the fre\u0002quently used Squared Exponential kernel, enforcing a\nhigh degree of smoothness, have weak dependence on\nthe dimensionality: O(\np\nT(log T)\nd+1) (see Fig. 1).\nThere is a large literature on GP (response surface)\noptimization. Several heuristics for trading off explo\u0002ration and exploitation in GP optimization have been\nproposed (such as Expected Improvement, Mockus\net al. 1978, and Most Probable Improvement, Mockus\n1989) and successfully applied in practice (c.f., Lizotte\net al. 2007). Brochu et al. (2009) provide a comprehen\u0002sive review of and motivation for Bayesian optimiza\u0002tion using GPs. The Efficient Global Optimization\n(EGO) algorithm for optimizing expensive black-box\nfunctions is proposed by Jones et al. (1998) and ex\u0002tended to GPs by Huang et al. (2006). Little is known\nabout theoretical performance of GP optimization.\nWhile convergence of EGO is established by Vazquez\n& Bect (2007), convergence rates have remained elu\u0002sive. Gr\u00a8unew\u00a8alder et al. (2010) consider the pure ex\u0002ploration problem for GPs, where the goal is to find the\noptimal decision over T rounds, rather than maximize\ncumulative reward (with no exploration/exploitation\ndilemma). They provide sharp bounds for this explo\u0002ration problem. Note that this methodology would not\nlead to bounds for minimizing the cumulative regret.\nOur cumulative regret bounds translate to the first\nperformance guarantees (rates) for GP optimization.\nSummary. Our main contributions are:\n\u2022 We analyze GP-UCB, an intuitive algorithm for\nGP optimization, when the function is either sam\u0002pled from a known GP, or has low RKHS norm.\nKernel Linear \nkernel\nRBF Mat\u00e9rn \nkernel Regret RT\n!\nT(log T)d+1 T\n\u03bd+d(d+1) d 2\u03bd+d(d+1) \u221a\nT\nFigure 1. Our regret bounds (up to polylog factors) for lin\u0002ear, radial basis, and Mat\u00b4ern kernels \u2014 d is the dimension,\nT is the time horizon, and \u03bd is a Mat\u00b4ern parameter.\n\u2022 We bound the cumulative regret for GP-UCB in\nterms of the information gain due to sampling,\nestablishing a novel connection between experi\u0002mental design and GP optimization.\n\u2022 By bounding the information gain for popular\nclasses of kernels, we establish sublinear regret\nbounds for GP optimization for the first time.\nOur bounds depend on kernel choice and param\u0002eters in a fine-grained fashion.\n\u2022 We evaluate GP-UCB on sensor network data,\ndemonstrating that it compares favorably to ex\u0002isting algorithms for GP optimization.\n2. Problem Statement and Background\nConsider the problem of sequentially optimizing an un\u0002known reward function f : D \u2192 R: in each round t, we\nchoose a point xt \u2208 D and get to see the function value\nthere, perturbed by noise: yt = f(xt) +\u000ft. Our goal is\nto maximize the sum of rewards PT\nt=1 f(xt), thus to\nperform essentially as well as x\n\u2217 = argmaxx\u2208D f(x)\n(as rapidly as possible). For example, we might want\nto find locations of highest temperature in a building\nby sequentially activating sensors in a spatial network\nand regressing on their measurements. D consists of\nall sensor locations, f(x) is the temperature at x, and\nsensor accuracy is quantified by the noise variance.\nEach activation draws battery power, so we want to\nsample from as few sensors as possible.\nRegret. A natural performance metric in this con\u0002text is cumulative regret, the loss in reward due to not\nknowing f\u2019s maximum points beforehand. Suppose\nthe unknown function is f, its maximum point1\nx\n\u2217 = argmaxx\u2208D f(x). For our choice xt in round\nt, we incur instantaneous regret rt = f(x\n\u2217\n) \u2212 f(xt).\nThe cumulative regret RT after T rounds is the sum\nof instantaneous regrets: RT =\nPT\nt=1 rt. A desirable\nasymptotic property of an algorithm is to be no-regret:\nlimT\u2192\u221e RT /T = 0. Note that neither rt nor RT are\never revealed to the algorithm. Bounds on the average\nregret RT /T translate to convergence rates for GP\noptimization: the maximum maxt\u2264T f(xt) in the first\nT rounds is no further from f(x\n\u2217\n) than the average.\n1 x\u2217 need not be unique; only f(x\u2217\n) occurs in the regret.\nGaussian Process Optimization in the Bandit Setting\n2.1. Gaussian Processes and RKHS\u2019s\nGaussian Processes. Some assumptions on f are\nrequired to guarantee no-regret. While rigid paramet\u0002ric assumptions such as linearity may not hold in prac\u0002tice, a certain degree of smoothness is often warranted.\nIn our sensor network, temperature readings at closeby\nlocations are highly correlated (see Figure 2(a)). We\ncan enforce implicit properties like smoothness with\u0002out relying on any parametric assumptions, modeling\nf as a sample from a Gaussian process (GP): a col\u0002lection of dependent random variables, one for each\nx \u2208 D, every finite subset of which is multivariate\nGaussian distributed in an overall consistent way (Ras\u0002mussen & Williams, 2006). A GP(\u00b5(x), k(x, x\n0\n)) is\nspecified by its mean function \u00b5(x) = ",
          "original_query": "Gaussian process optimization in the bandit setting: No regret and experimental design",
          "cleaned_query": "Gaussian process optimization in the bandit setting: No regret and experimental design",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Tight Regret Bounds for Bayesian Optimization in One Dimension",
          "url": "https://arxiv.org/html/1805.11792",
          "content": "Tight Regret Bounds for Bayesian Optimization in One Dimension\n# Tight Regret Bounds for Bayesian Optimization in One Dimension\nJonathan Scarlett\n###### Abstract\nWe consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to timeT\ud835\udc47Titalic\\_Tbehaves as\u03a9\u2062(T)\u03a9\ud835\udc47\\\\Omega(\\\\sqrt{T})roman\\_\u03a9 ( square-root start\\_ARG italic\\_T end\\_ARG )andO\u2062(T\u2062log\u2061T)\ud835\udc42\ud835\udc47\ud835\udc47O(\\\\sqrt{T\\\\log T})italic\\_O ( square-root start\\_ARG italic\\_T roman\\_log italic\\_T end\\_ARG ). This gives a tight characterization up to alog\u2061T\ud835\udc47\\\\sqrt{\\\\log T}square-root start\\_ARG roman\\_log italic\\_T end\\_ARGfactor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Mat\u00e9rn-\u03bd\ud835\udf08\\\\nuitalic\\_\u03bdkernels, with the latter requiring\u03bd&gt;2\ud835\udf082\\\\nu&gt;&gt;2italic\\_\u03bd &gt;&gt; 2. Our results certify the near-optimality of existing bounds (Srinivaset al., 2009) for the SE kernel, while proving them to be strictly suboptimal for the Mat\u00e9rn kernel with\u03bd&gt;2\ud835\udf082\\\\nu&gt;&gt;2italic\\_\u03bd &gt;&gt; 2.\nMachine Learning, ICML\n## 1Introduction\nBayesian optimization (BO)> (Shahriari et\u00a0al., [> 2016\n](https://arxiv.org/html/1805.11792v3#bib.bib21)> )\nis a powerful and versatile tool for black-box function optimization, with applications including parameter tuning, robotics, molecular design, sensor networks, and more. The idea is to model the unknown function as a Gaussian process with a givenkernel functiondictating the smoothness properties. This model is updated using (typically noisy) samples, which are selected to steer towards the function maximum.\nOne of the most attractive properties of BO is its efficiency in terms of the number of function samples used. Consequently, algorithms withrigorous guaranteeson the trade-off between samples and optimization performance are particularly valuable. Perhaps the most prominent work in the literature giving such guarantees is that of> (Srinivas et\u00a0al., [> 2010\n](https://arxiv.org/html/1805.11792v3#bib.bib23)> )\n, who consider thecumulative regret:\n|RT=\u2211t=1T(maxx\u2061f\u2062(x)\u2212f\u2062(xt)),subscript\ud835\udc45\ud835\udc47superscriptsubscript\ud835\udc611\ud835\udc47subscript\ud835\udc65\ud835\udc53\ud835\udc65\ud835\udc53subscript\ud835\udc65\ud835\udc61R\\_{T}=\\\\sum\\_{t=1}^{T}\\\\Big{(}\\\\max\\_{x}f(x)-f(x\\_{t})\\\\Big{)},italic\\_R start\\_POSTSUBSCRIPT italic\\_T end\\_POSTSUBSCRIPT = \u2211start\\_POSTSUBSCRIPT italic\\_t = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_T end\\_POSTSUPERSCRIPT ( roman\\_max start\\_POSTSUBSCRIPT italic\\_x end\\_POSTSUBSCRIPT italic\\_f ( italic\\_x ) - italic\\_f ( italic\\_x start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ) ) ,||(1)|\nwheref\ud835\udc53fitalic\\_fis the function being optimized, andxtsubscript\ud835\udc65\ud835\udc61x\\_{t}italic\\_x start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPTis the point chosen at timet\ud835\udc61titalic\\_t. Under a Gaussian process (GP) prior and Gaussian noise, it is shown in> (Srinivas et\u00a0al., [> 2010\n](https://arxiv.org/html/1805.11792v3#bib.bib23)> )\nthat an algorithm called Gaussian Process Upper Confidence Bound (GP-UCB) achieves a cumulative regret of the form\n|RT=O\u2217\u2062(T\u2062\u03b3T),subscript\ud835\udc45\ud835\udc47superscript\ud835\udc42\ud835\udc47subscript\ud835\udefe\ud835\udc47R\\_{T}=O^{\\*}(\\\\sqrt{T\\\\gamma\\_{T}}),italic\\_R start\\_POSTSUBSCRIPT italic\\_T end\\_POSTSUBSCRIPT = italic\\_O start\\_POSTSUPERSCRIPT \u2217end\\_POSTSUPERSCRIPT ( square-root start\\_ARG italic\\_T italic\\_\u03b3 start\\_POSTSUBSCRIPT italic\\_T end\\_POSTSUBSCRIPT end\\_ARG ) ,||(2)|\nwhere\u03b3T=maxx1,\u2026,xT\u2061I\u2062(\ud835\udc1f;\ud835\udc32)subscript\ud835\udefe\ud835\udc47subscriptsubscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc47\ud835\udc3c\ud835\udc1f\ud835\udc32\\\\gamma\\_{T}=\\\\max\\_{x\\_{1},\\\\dotsc,x\\_{T}}I(\\\\mathbf{f};\\\\mathbf{y})italic\\_\u03b3 start\\_POSTSUBSCRIPT italic\\_T end\\_POSTSUBSCRIPT = roman\\_max start\\_POSTSUBSCRIPT italic\\_x start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , \u2026, italic\\_x start\\_POSTSUBSCRIPT italic\\_T end\\_POSTSUBSCRIPT end\\_POSTSUBSCRIPT italic\\_I ( bold\\_f ; bold\\_y )(with function values\ud835\udc1f=(f\u2062(x1),\u2026,f\u2062(xT))\ud835\udc1f\ud835\udc53subscript\ud835\udc651\u2026\ud835\udc53subscript\ud835\udc65\ud835\udc47\\\\mathbf{f}=(f(x\\_{1}),\\\\dotsc,f(x\\_{T}))bold\\_f = ( italic\\_f ( italic\\_x start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ) , \u2026, italic\\_f ( italic\\_x start\\_POSTSUBSCRIPT italic\\_T end\\_POSTSUBSCRIPT ) )and noisy samples\ud835\udc32=(y1,\u2026,yT)\ud835\udc32subscript\ud835\udc661\u2026subscript\ud835\udc66\ud835\udc47\\\\mathbf{y}=(y\\_{1},\\\\dotsc,y\\_{T})bold\\_y = ( italic\\_y start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , \u2026, italic\\_y start\\_POSTSUBSCRIPT italic\\_T end\\_POSTSUBSCRIPT )) is known as themaximum information gain. HereI\u2062(\ud835\udc1f;\ud835\udc32)\ud835\udc3c\ud835\udc1f\ud835\udc32I(\\\\mathbf{f};\\\\mathbf{y})italic\\_I ( bold\\_f ; bold\\_y )denotes the mutual information> (Cover &amp; Thomas, [> 2001\n](https://arxiv.org/html/1805.11792v3#bib.bib6)> )\nbetween the function values and noisy samples, andO\u2217\u2062(\u22c5)superscript\ud835\udc42\u22c5O^{\\*}(\\\\cdot)italic\\_O start\\_POSTSUPERSCRIPT \u2217end\\_POSTSUPERSCRIPT ( \u22c5)denotes asymptotic notation up to logarithmic factors.\nThe guarantee ([2](https://arxiv.org/html/1805.11792v3#S1.E2)) ensures sub-linear cumulative regret for many kernels of interest. However, the literature is severely lacking inalgorithm-independent lower bounds, and without these, it is impossible to know to what extent the upper bounds, including ([2](https://arxiv.org/html/1805.11792v3#S1.E2)), can be improved. In this work, we address this gap in detail in the special case of a one-dimensional function. We show that the best possible cumulative regret behaves as\u0398\u2217\u2062(T)superscript\u0398\ud835\udc47\\\\Theta^{\\*}(\\\\sqrt{T})roman\\_\u0398 start\\_POSTSUPERSCRIPT \u2217end\\_POSTSUPERSCRIPT ( square-root start\\_ARG italic\\_T end\\_ARG )under mild assumptions on the kernel, thus identifying both cases where ([2](https://arxiv.org/html/1805.11792v3#S1.E2)) is near-optimal, and cases where it is strictly suboptimal.\n### 1.1Related Work\nAn extensive range of BO algorithms have been proposed in the literature, typically involving the maximization of an acquisition function> (Hennig &amp; Schuler, [> 2012\n](https://arxiv.org/html/1805.11792v3#bib.bib10)> ; Hern\u00e1ndez-Lobato et\u00a0al., [> 2014\n](https://arxiv.org/html/1805.11792v3#bib.bib11)> ; Russo &amp; Van\u00a0Roy, [> 2014\n](https://arxiv.org/html/1805.11792v3#bib.bib19)> ; Wang et\u00a0al., [> 2016\n](https://arxiv.org/html/1805.11792v3#bib.bib25)> )\n; see> (Shahriari et\u00a0al., [> 2016\n](https://arxiv.org/html/1805.11792v3#bib.bib21)> )\nfor a recent overview.\nAs mentioned above, the most relevant algorithm to this work for the noisy setting is GP-UCB> (Srinivas et\u00a0al., [> 2010\n](https://arxiv.org/html/1805.11792v3#bib.bib23)> )\n, which constructsconfidence boundsin which the function lies with high probability, and samples the point with the highest upper confidence bound. Several extensions to GP-UCB have also been proposed, including contextual> (Krause &amp; Ong, [> 2011\n](https://arxiv.org/html/1805.11792v3#bib.bib15)> ; Bogunovic et\u00a0al., [> 2016a\n](https://arxiv.org/html/1805.11792v3#bib.bib1)> )\n, batch> (Contal et\u00a0al., [> 2013\n](https://arxiv.org/html/1805.11792v3#bib.bib5)> ; Desautels et\u00a0al., [> 2014\n](https://arxiv.org/html/1805.11792v3#bib.bib8)> )\n, and high-dimensional> (Kandasamy et\u00a0al., [> 2015\n](https://arxiv.org/html/1805.11792v3#bib.bib12)> ; Rolland et\u00a0al., [> 2018\n](https://arxiv.org/html/1805.11792v3#bib.bib18)> )\nvariants.\nIn the noiseless setting, it has been shown that it is possible to achieveboundedcumulative regret> (de Freitas et\u00a0al., [> 2012\n](https://arxiv.org/html/1805.11792v3#bib.bib7)> ; Kawaguchi et\u00a0al., [> 2015\n](https://arxiv.org/html/1805.11792v3#bib.bib13)> )\nunder some technical assumptions. In> (de Freitas et\u00a0al., [> 2012\n](https://arxiv.org/html/1805.11792v3#bib.bib7)> )\n, this is done by keeping track of a set of potential maximizers, and sampling increasingly finely in order to shrink that set and \u201czoom in\u201d towards the optimal point. Similar ideas have also been used in the noisy setting for studying batch variants of GP-UCB> (Contal et\u00a0al., [> 2013\n](https://arxiv.org/html/1805.11792v3#bib.bib5)> )\n, simultaneous online optimization (SOO) methods> (Wang et\u00a0al., [> 2014\n](https://arxiv.org/html/1805.11792v3#bib.bib24)> )\n, and lookahead algorithms that use confidence bounds> (Bogunovic et\u00a0al., [> 2016b\n](https://arxiv.org/html/1805.11792v3#bib.bib2)> )\n. Returning to the noiseless setting, upper and lower bounds were given in> (Gr\u00fcnew\u00e4lder et\u00a0al., [> 2010\n](https://arxiv.org/html/1805.11792v3#bib.bib9)> )\nfor kernels satisfying certain smoothness assumptions, with the lower bounds showing that bounded cumulative regret is not always to be expected.\nAlongside the Bayesian view of the Gaussian process model, several works have also considered anon-Bayesiancounterpart assuming that the function has a bounded norm in the associated reproducing kernel Hilbert space (RKHS). Interestingly, GP-UCB still provides similar guarantees to ([2](https://arxiv.org/html/1805.11792v3#S1.E2)) in this setting> (Srinivas et\u00a0al., [> 2010\n](https://arxiv.org/html/1805.11792v3#bib.bib23)> )\n. Moreover, lower bounds have been proved; see> (Bull, [> 2011\n](https://arxiv.org/html/1805.11792v3#bib.bib4)> )\nfor the noiseless setting, and> (Scarlett et\u00a0al., [> 2017\n](https://arxiv.org/html/1805.11792v3#bib.bib20)> )\nfor the noisy setting. In the latter, the lower bounds nearly match the GP-UCB upper bound for the squared exponential (SE) kernel, but gaps remain for the Mat\u00e9rn kernel. For reference, we note that these kernels are defined as follows:\n|kSE\u2062(x,x\u2032)subscript\ud835\udc58SE\ud835\udc65superscript\ud835\udc65\u2032\\\\displaystyle k\\_{\\\\text{SE}}(x,x^{\\\\prime})italic\\_k start\\_POSTSUBSCRIPT SE end\\_POSTSUBSCRIPT ( italic\\_x , italic\\_x start\\_POSTSUPERSCRIPT \u2032end\\_POSTSUPERSCRIPT )|=exp\u2061(\u2212\u2016x\u2212x\u2032\u201622\u2062l2)absentsuperscriptnorm\ud835\udc65superscript\ud835\udc65\u203222superscript\ud835\udc592\\\\displaystyle=\\\\exp\\\\bigg{(}-\\\\dfrac{\\\\|x-x^{\\\\prime}\\\\|^{2}}{2l^{2}}\\\\bigg{)}= roman\\_exp ( - divide start\\_ARG \u2225italic\\_x - italic\\_x start\\_POSTSUPERSCRIPT \u2032end\\_POSTSUPERSCRIPT \u2225start\\_POSTSUPERSCRIPT 2 end\\_POSTSUPERSCRIPT end\\_ARG start\\_ARG 2 italic\\_l start\\_POSTSUPERSCRIPT 2 end\\_POSTSUPERSCRIPT end\\_ARG )||(3)|\n|kMat\u00e9rn\u2062(x,x\u2032)subscript\ud835\udc58Mat\u00e9rn\ud835\udc65superscript\ud835\udc65\u2032",
          "original_query": "Tight regret bounds for Bayesian optimization in one dimension",
          "cleaned_query": "Tight regret bounds for Bayesian optimization in one dimension",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "On Information Gain and Regret Bounds in Gaussian ...",
          "url": "https://arxiv.org/abs/2009.06966",
          "content": "# Statistics > Machine Learning\n\n**arXiv:2009.06966** (stat)\n\n\\[Submitted on 15 Sep 2020 ( [v1](https://arxiv.org/abs/2009.06966v1)), last revised 9 Mar 2021 (this version, v3)\\]\n\n# Title:On Information Gain and Regret Bounds in Gaussian Process Bandits\n\nAuthors: [Sattar Vakili](https://arxiv.org/search/stat?searchtype=author&query=Vakili,+S), [Kia Khezeli](https://arxiv.org/search/stat?searchtype=author&query=Khezeli,+K), [Victor Picheny](https://arxiv.org/search/stat?searchtype=author&query=Picheny,+V)\n\nView a PDF of the paper titled On Information Gain and Regret Bounds in Gaussian Process Bandits, by Sattar Vakili and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2009.06966)\n\n> Abstract:Consider the sequential optimization of an expensive to evaluate and possibly non-convex objective function $f$ from noisy feedback, that can be considered as a continuum-armed bandit problem. Upper bounds on the regret performance of several learning algorithms (GP-UCB, GP-TS, and their variants) are known under both a Bayesian (when $f$ is a sample from a Gaussian process (GP)) and a frequentist (when $f$ lives in a reproducing kernel Hilbert space) setting. The regret bounds often rely on the maximal information gain $\\\\gamma\\_T$ between $T$ observations and the underlying GP (surrogate) model. We provide general bounds on $\\\\gamma\\_T$ based on the decay rate of the eigenvalues of the GP kernel, whose specialisation for commonly used kernels, improves the existing bounds on $\\\\gamma\\_T$, and subsequently the regret bounds relying on $\\\\gamma\\_T$ under numerous settings. For the Mat\u00e9rn family of kernels, where the lower bounds on $\\\\gamma\\_T$, and regret under the frequentist setting, are known, our results close a huge polynomial in $T$ gap between the upper and lower bounds (up to logarithmic in $T$ factors).\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (stat.ML); Information Theory (cs.IT); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2009.06966](https://arxiv.org/abs/2009.06966) \\[stat.ML\\] |\n| | (or [arXiv:2009.06966v3](https://arxiv.org/abs/2009.06966v3) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2009.06966](https://doi.org/10.48550/arXiv.2009.06966) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Sattar Vakili \\[ [view email](https://arxiv.org/show-email/c6d9e213/2009.06966)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2009.06966v1)**\nTue, 15 Sep 2020 10:15:29 UTC (39 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2009.06966v2)**\nFri, 9 Oct 2020 14:28:46 UTC (40 KB)\n\n**\\[v3\\]**\nTue, 9 Mar 2021 22:46:52 UTC (41 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled On Information Gain and Regret Bounds in Gaussian Process Bandits, by Sattar Vakili and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2009.06966)\n- [TeX Source](https://arxiv.org/src/2009.06966)\n- [Other Formats](https://arxiv.org/format/2009.06966)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2009.06966&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2009.06966&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2020-09](https://arxiv.org/list/stat.ML/2020-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2009.06966?context=cs)\n\n[cs.IT](https://arxiv.org/abs/2009.06966?context=cs.IT)\n\n[cs.LG](https://arxiv.org/abs/2009.06966?context=cs.LG)\n\n[math](https://arxiv.org/abs/2009.06966?context=math)\n\n[math.IT](https://arxiv.org/abs/2009.06966?context=math.IT)\n\n[stat](https://arxiv.org/abs/2009.06966?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2009.06966)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2009.06966)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2009.06966)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2009.06966&description=On Information Gain and Regret Bounds in Gaussian Process Bandits) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2009.06966&title=On Information Gain and Regret Bounds in Gaussian Process Bandits)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2009.06966) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "On information gain and regret bounds in Gaussian process bandits",
          "cleaned_query": "On information gain and regret bounds in Gaussian process bandits",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Bandit optimisation of functions in the Mat\u00e9rn kernel RKHS",
          "url": "https://arxiv.org/abs/2001.10396",
          "content": "\n Download PDF \nAbstract: We consider the problem of optimising functions in the reproducing kernel\nHilbert space (RKHS) of a Mat\u00e9rn kernel with smoothness parameter $\\nu$ over\nthe domain $[0,1]^d$ under noisy bandit feedback. Our contribution, the\n$\\pi$-GP-UCB algorithm, is the first practical approach with guaranteed\nsublinear regret for all $\\nu&gt;1$ and $d \\geq 1$. Empirical validation suggests\nbetter performance and drastically improved computational scalablity compared\nwith its predecessor, Improved GP-UCB.\n \n \n Submission history From: David Janz [ view email]\n \n [v1] \nTue, 28 Jan 2020 15:09:21 UTC (189 KB) [v2] \nMon, 2 Mar 2020 14:50:32 UTC (383 KB) ||||I|||| Skip to main content\nWe gratefully acknowledge support from\nthe Simons Foundation and member institutions.\n> cs > arXiv:2001.10396\nHelp | Advanced Search\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\nGO\nquick links\n* Login\n* Help Pages\n* About\nComputer Science > Machine Learning\narXiv:2001.10396 (cs)\n[Submitted on 28 Jan 2020 (v1), last revised 2 Mar 2020 (this version, v2)]\nTitle: Bandit optimisation of functions in the Mat\u00e9rn kernel RKHS\nAuthors: David Janz, David R. Burt, Javier Gonz\u00e1lez\nDownload PDF\nAbstract: We consider the problem of optimising functions in the reproducing kernel Hilbert space (RKHS) of a Mat\u00e9rn kernel with smoothness parameter $\\nu$ over the domain $[0,1]^d$ under noisy bandit feedback. Our contribution, the $\\pi$-GP-UCB algorithm, is the first practical approach with guaranteed sublinear regret for all $\\nu>1$ and $d \\geq 1$. Empirical validation suggests better performance and drastically improved computational scalablity compared with its predecessor, Improved GP-UCB.\nComments: AISTATS 2020, camera ready\nSubjects: Machine Learning (cs.LG) ; Machine Learning (stat.ML)\nCite as: arXiv:2001.10396 [cs.LG]\n(or arXiv:2001.10396v2 [cs.LG] for this version)\nhttps://doi.org/10.48550/arXiv.2001.10396\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: David Janz [view email]\n[v1] Tue, 28 Jan 2020 15:09:21 UTC (189 KB)\n[v2] Mon, 2 Mar 2020 14:50:32 UTC (383 KB)\nFull-text links:\nDownload:\n* PDF\n* Other formats\n(license)\nCurrent browse context:\ncs.LG\n< prev | next >\nnew | recent | 2001\nChange to browse by:\ncs\nstat\nstat.ML\nReferences & Citations\n* NASA ADS\n* Google Scholar\n* Semantic Scholar\nDBLP - CS Bibliography\nlisting | bibtex\nDavid Janz\nDavid R. Burt\nJavier Gonz\u00e1lez\na export bibtex citation Loading...\nBibtex formatted citation\n\u00d7\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nLinks to Code Toggle\nPapers with Code (What is Papers with Code?)\nScienceCast Toggle\nScienceCast (What is ScienceCast?)\nDemos\nDemos\nReplicate Toggle\nReplicate (What is Replicate?)\nSpaces Toggle\nHugging Face Spaces (What is Spaces?)\nRelated Papers\nRecommenders and Search Tools\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nCore recommender toggle\nCORE Recommender (What is CORE?)\nIArxiv recommender toggle\nIArxiv Recommender (What is IArxiv?)\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n* About\n* Help\n* Click here to contact arXiv Contact\n* Click here to subscribe Subscribe\n* Copyright\n* Privacy Policy\n* Web Accessibility Assistance\n* arXiv Operational Status\nGet status notifications via email or slack",
          "original_query": "Bandit optimisation of functions in the Mat\u00e9rn kernel RKHS",
          "cleaned_query": "Bandit optimisation of functions in the Mat\u00e9rn kernel RKHS",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Lenient Regret and Good-Action Identification in Gaussian Process ...",
          "url": "https://arxiv.org/abs/2102.05793",
          "content": "# Statistics > Machine Learning\n\n**arXiv:2102.05793** (stat)\n\n\\[Submitted on 11 Feb 2021 ( [v1](https://arxiv.org/abs/2102.05793v1)), last revised 26 May 2021 (this version, v2)\\]\n\n# Title:Lenient Regret and Good-Action Identification in Gaussian Process Bandits\n\nAuthors: [Xu Cai](https://arxiv.org/search/stat?searchtype=author&query=Cai,+X), [Selwyn Gomes](https://arxiv.org/search/stat?searchtype=author&query=Gomes,+S), [Jonathan Scarlett](https://arxiv.org/search/stat?searchtype=author&query=Scarlett,+J)\n\nView a PDF of the paper titled Lenient Regret and Good-Action Identification in Gaussian Process Bandits, by Xu Cai and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2102.05793)\n\n> Abstract:In this paper, we study the problem of Gaussian process (GP) bandits under relaxed optimization criteria stating that any function value above a certain threshold is \"good enough\". On the theoretical side, we study various {\\\\em lenient regret} notions in which all near-optimal actions incur zero penalty, and provide upper bounds on the lenient regret for GP-UCB and an elimination algorithm, circumventing the usual $O(\\\\sqrt{T})$ term (with time horizon $T$) resulting from zooming extremely close towards the function maximum. In addition, we complement these upper bounds with algorithm-independent lower bounds. On the practical side, we consider the problem of finding a single \"good action\" according to a known pre-specified threshold, and introduce several good-action identification algorithms that exploit knowledge of the threshold. We experimentally find that such algorithms can often find a good action faster than standard optimization-based approaches.\n\n| | |\n| --- | --- |\n| Comments: | ICML 2021 |\n| Subjects: | Machine Learning (stat.ML); Information Theory (cs.IT); Machine Learning (cs.LG); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:2102.05793](https://arxiv.org/abs/2102.05793) \\[stat.ML\\] |\n| | (or [arXiv:2102.05793v2](https://arxiv.org/abs/2102.05793v2) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2102.05793](https://doi.org/10.48550/arXiv.2102.05793) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jonathan Scarlett \\[ [view email](https://arxiv.org/show-email/f3ef04ef/2102.05793)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2102.05793v1)**\nThu, 11 Feb 2021 01:16:58 UTC (275 KB)\n\n**\\[v2\\]**\nWed, 26 May 2021 06:46:03 UTC (443 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Lenient Regret and Good-Action Identification in Gaussian Process Bandits, by Xu Cai and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2102.05793)\n- [TeX Source](https://arxiv.org/src/2102.05793)\n- [Other Formats](https://arxiv.org/format/2102.05793)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2102.05793&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2102.05793&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2021-02](https://arxiv.org/list/stat.ML/2021-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2102.05793?context=cs)\n\n[cs.IT](https://arxiv.org/abs/2102.05793?context=cs.IT)\n\n[cs.LG](https://arxiv.org/abs/2102.05793?context=cs.LG)\n\n[math](https://arxiv.org/abs/2102.05793?context=math)\n\n[math.IT](https://arxiv.org/abs/2102.05793?context=math.IT)\n\n[math.OC](https://arxiv.org/abs/2102.05793?context=math.OC)\n\n[stat](https://arxiv.org/abs/2102.05793?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2102.05793)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2102.05793)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2102.05793)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2102.05793&description=Lenient Regret and Good-Action Identification in Gaussian Process Bandits) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2102.05793&title=Lenient Regret and Good-Action Identification in Gaussian Process Bandits)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2102.05793) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Lenient regret and good-action identification in Gaussian process bandits",
          "cleaned_query": "Lenient regret and good-action identification in Gaussian process bandits",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Posterior consistency of Gaussian process prior for nonparametric ...",
          "url": "https://projecteuclid.org/journals/annals-of-statistics/volume-34/issue-5/Posterior-consistency-of-Gaussian-process-prior-for-nonparametric-binary-regression/10.1214/009053606000000795.full",
          "content": "Email\n\nPassword [Forgot your password?](https://projecteuclid.org/)\n\nShow\n\nRemember Email on this computer\n\nRemember Password\n\n![](https://projecteuclid.org/Content/themes/SPIEImages/Loading.gif)Please wait...\n\nNo Project Euclid account? [Create an account](https://projecteuclid.org/)\n\nor [Sign in with your institutional credentials](https://projecteuclid.org/Account/institutionalsignin?redirect=https%3a%2f%2fprojecteuclid.org%2fjournals%2fannals-of-statistics%2fvolume-34%2fissue-5%2fPosterior-consistency-of-Gaussian-process-prior-for-nonparametric-binary-regression%2f10.1214%2f009053606000000795.full)\n\nWe can help you reset your password using the email address linked to your Project Euclid account.\n\nEmail\n\nRegistered users receive a variety of benefits including the ability to customize email alerts, create favorite journals list, and save searches.\nPlease note that a Project Euclid web account does not automatically grant access to full-text content. An institutional or society member subscription is required to view non-Open Access content.\nContact [customer\\_support@projecteuclid.org](mailto:customer_support@projecteuclid.org) with any questions.\n\n[View Project Euclid Privacy Policy](https://projecteuclid.org/policies)\n\nAll Fields are Required\n\n\\*First Name\n\n\\*Last/Family Name\n\n\\*Email\n\n\\*Password\n\nPassword Requirements: Minimum 8 characters, must include as least one uppercase, one lowercase letter, and one number or permitted symbol\nValid Symbols for password:\n\n~ Tilde\n\n! Exclamation Mark\n\n@ At sign\n\n$ Dollar sign\n\n^ Caret\n\n( Opening Parenthesis\n\n) Closing Parenthesis\n\n\\_ Underscore\n\n. Period\n![](https://projecteuclid.org/Content/themes/SPIEImages/InformationQuestionMark.png)\n\n\\*Confirm Password\n\n![](https://projecteuclid.org/Content/themes/SPIEImages/Loading.gif)Please wait...\n\nWeb Account created successfully\n\n![Open Access](https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg)\n\nOctober 2006Posterior consistency of Gaussian process prior for nonparametric binary regression\n\nSubhashis Ghosal,\nAnindya Roy\n\nAnn. Statist. 34(5): 2413-2429 (October 2006). DOI: 10.1214/009053606000000795\n\n- ABOUT\n- FIRST PAGE\n- CITED BY\n- REFERENCES\n- [DOWNLOAD PAPER](https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F009053606000000795) [SAVE TO MY LIBRARY](https://projecteuclid.org/)\n\n\nPERSONAL SIGN IN\n\nFull access may be available with your subscription\n\nEmail\n\nPasswordForgot your password?\n\nShow\n\nRemember Email on this computer\n\nRemember Password\n\nNo Project Euclid account? [Create an account](https://projecteuclid.org/)\n\nor [Sign in with your institutional credentials](https://projecteuclid.org/Account/institutionalsignin?redirect=https%3a%2f%2fprojecteuclid.org%2fjournals%2fannals-of-statistics%2fvolume-34%2fissue-5%2fPosterior-consistency-of-Gaussian-process-prior-for-nonparametric-binary-regression%2f10.1214%2f009053606000000795.full)\n\nPURCHASE SINGLE ARTICLE\n\nThis article is only available to [subscribers](https://projecteuclid.org/subscriptions-and-access). It is not available for individual sale.\n\nThis will count as one of your downloads.\n\nYou will have access to both the presentation and article (if available).\n\n[DOWNLOAD NOW](https://projecteuclid.org/)\n\nThis content is available for download via your institution's subscription. To access this item, please sign in to your personal account.\n\nEmail\n\nPasswordForgot your password?\n\nShow\n\nRemember Email on this computer\n\nRemember Password\n\nNo Project Euclid account? [Create an account](https://projecteuclid.org/)\n\nMy Library\n\nYou currently do not have any folders to save your paper to! Create a new folder below.\n\nCreate New Folder\n\nSAVE >\n\n## Abstract\n\nConsider binary observations whose response probability is an unknown smooth function of a set of covariates. Suppose that a prior on the response probability function is induced by a Gaussian process mapped to the unit interval through a link function. In this paper we study consistency of the resulting posterior distribution. If the covariance kernel has derivatives up to a desired order and the bandwidth parameter of the kernel is allowed to take arbitrarily small values, we show that the posterior distribution is consistent in the _L_ 1-distance. As an auxiliary result to our proofs, we show that, under certain conditions, a Gaussian process assigns positive probabilities to the uniform neighborhoods of a continuous function. This result may be of independent interest in the literature for small ball probabilities of Gaussian processes.\n\n## Citation\n\n[Download Citation](https://projecteuclid.org/)\n\nSubhashis Ghosal.Anindya Roy.\"Posterior consistency of Gaussian process prior for nonparametric binary regression.\"Ann. Statist.34(5)2413 - 2429,October 2006.https://doi.org/10.1214/009053606000000795\n\n## Information\n\nPublished: October 2006\n\nFirst available in Project Euclid: 23 January 2007\n\nzbMATH: [1106.62039](https://zbmath.org/?q=an:1106.62039)\n\nMathSciNet: [MR2291505](https://mathscinet.ams.org/mathscinet-getitem?mr=MR2291505)\n\nDigital Object Identifier: 10.1214/009053606000000795\n\nSubjects:\n\nPrimary:\n62G08\n, 62G20\n\nKeywords:\nbinary regression\n, Gaussian process\n, Karhunen\u2013Loeve expansion\n, maximal inequality\n, posterior consistency\n, \u200ereproducing kernel Hilbert \u200espace\n\nRights: Copyright \u00a9 2006 Institute of Mathematical Statistics\n\nJOURNAL ARTICLE\n\n17 PAGES\n\n* * *\n\n[DOWNLOAD PDF](https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2F009053606000000795)+SAVE TO MY LIBRARY\n\n* * *\n\n![](https://projecteuclid.org/content/themes/spieimages/GetCitation.png)\nGET CITATION\n\nMy Library\n\nYou currently do not have any folders to save your paper to! Create a new folder below.\n\nCreate New Folder\n\nSAVE >\n\nFolder Name\n\nFolder Description\n\nSAVE\n\n< [Previous Article](https://projecteuclid.org/journals/annals-of-statistics/volume-34/issue-5/A-multivariate-empirical-Bayes-statistic-for-replicated-microarray-time-course/10.1214/009053606000000759.full)\n\n\\|\n\n[Next Article](https://projecteuclid.org/journals/annals-of-statistics/volume-34/issue-5/The-shape-of-incomplete-preferences/10.1214/009053606000000740.full) >\n\n[**Ann. Statist.**](https://projecteuclid.org/journals/annals-of-statistics/volume-34/issue-5)\n\nVol.34 \u2022 No. 5 \u2022 October 2006\n\n![](https://projecteuclid.org/images/journals/cover_aos.jpg)\n\n[Institute of Mathematical Statistics](https://projecteuclid.org/publishers/Institute-of-Mathematical-Statistics)\n\n[![](https://projecteuclid.org/images/publishers/thumbnail/publisher_ims.png)](https://projecteuclid.org/publishers/Institute-of-Mathematical-Statistics)\n\n[![](https://projecteuclid.org/Content/themes/SPIEImages/Subscribe_Check.png)Subscribe to Project Euclid](https://projecteuclid.org/subscribe)\n\n[![](https://projecteuclid.org/Content/themes/SPIEImages/Alerts_Mark.png)Receive erratum alerts for this article](https://projecteuclid.org/)\n\nSubhashis Ghosal, Anindya Roy \"Posterior consistency of Gaussian process prior for nonparametric binary regression,\" The Annals of Statistics, Ann. Statist. 34(5), 2413-2429, (October 2006)\n\nInclude:\n\nCitation Only\n\nCitation & Abstract\n\nFormat:\n\nRIS\n\nEndNote\n\nBibTex\n\nPrint Friendly Version (PDF)\n\nDOWNLOAD CITATION\n\n[![Back to Top](https://projecteuclid.org/Images/Project%20Euclid/Back-Top_Icon.png)](https://projecteuclid.org/#top)\n\n## KEYWORDS/PHRASES\n\nKeywords\n\nin\n\nAll FieldsAbstractAuthor NameAffiliationDOI/ISSN/ISBNFigure & Table CaptionsKeywordsTitle\n\nRemove\n\nANDORNOT\n\nin\n\nAll FieldsAbstractAuthor NameAffiliationDOI/ISSN/ISBNFigure & Table CaptionsKeywordsTitle\n\nRemove\n\nANDORNOT\n\nin\n\nAll FieldsAbstractAuthor NameAffiliationDOI/ISSN/ISBNFigure & Table CaptionsKeywordsTitle\n\nRemove\n\n\\+ Add another field\n\n* * *\n\n## PUBLICATION TITLE:\n\nAll Titles\n\nChoose Title(s)\n\nSelect a title\n\n* * *\n\n## PUBLICATION YEARS\n\nRange\n\nSingle Year\n\n* * *\n\nClear Form",
          "original_query": "Posterior consistency of Gaussian process prior for nonparametric binary regression",
          "cleaned_query": "Posterior consistency of Gaussian process prior for nonparametric binary regression",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Kernel-Smoothness-Adaptive GP-UCB via Online Eigen-Decay Estimation\nDevelop a GP-UCB variant that does not assume a known Mat\u00e9rn smoothness parameter \\(\\nu\\), but instead estimates the kernel eigenvalue decay rate online from the posterior covariance operator. Use this estimate to adapt the exploration parameter \\(\\beta_t\\) (through a data-driven bound on \\(\\gamma_T\\)) and prove regret guarantees that match the best fixed-\\(\\nu\\) choice up to logarithmic factors.",
        "Threshold-Adaptive \u201cGood-Action\u201d Identification with Unknown Satisfaction Level\nExtend lenient regret / good-action identification to the realistic case where the \u201cgood enough\u201d threshold is unknown and must be learned (e.g., user-defined utility drift). Propose an algorithm that maintains a posterior over both \\(f\\) and the threshold \\(\\tau\\), alternating between probing to localize \\(\\tau\\) and sampling to find \\(x\\) with \\(f(x)\\ge \\tau\\); provide sample-complexity bounds in terms of information gain and a margin condition near \\(\\tau\\).",
        "Computationally-Scalable \\(\\pi\\)-GP-UCB with Provable Regret under Random Feature Approximations\nCombine the scalability goals of \\(\\pi\\)-GP-UCB with structured kernel approximations (Nystr\u00f6m / random Fourier features) and analyze how approximation error perturbs \\(\\gamma_T\\) and regret. Deliver an implementable algorithm with explicit trade-offs between runtime (feature dimension), approximation quality (operator-spectrum tail), and cumulative regret.",
        "Experimental-Design-Constrained GP Bandits: Regret with Submodular Budgeted Information Gain\nFormalize a setting where each query has a cost and the learner must respect a budget while optimizing reward (linking GP optimization to experimental design submodularity). Create a budget-feasible GP-UCB that selects points via a greedy approximation to submodular information gain per unit cost, and prove regret bounds in terms of cost-normalized \\(\\gamma_T\\) and knapsack-submodularity approximation factors.",
        "Misspecified-Kernel GP Bandits with Posterior-Consistency-Guided Regret Guarantees\nUse posterior consistency tools (small-ball probabilities / RKHS support arguments) to analyze GP bandits when the assumed kernel is misspecified but still \u201ccovers\u201d the true function class. Provide conditions under which the posterior concentrates fast enough to retain sublinear regret, and design a practical kernel-mixture GP-UCB that maintains a discrete set of kernels and achieves oracle-like regret relative to the best kernel in hindsight.",
        "GP Bandits for Non-Gaussian Feedback Using Binary-Regression Consistency Machinery\nReplace Gaussian noise with binary/categorical feedback (e.g., success/fail experiments), modeling the latent function with a GP passed through a link function (as in GP binary regression). Develop a \u201cGP-UCB with link\u201d acquisition based on posterior credible intervals in the latent space, and prove regret or good-action identification bounds using posterior consistency and information-gain-type measures for generalized likelihoods.",
        "Joint Optimization and Identification: Pareto-Optimal Trade-off Between Standard and Lenient Regret\nCreate a unified framework where the learner can choose a tunable parameter controlling how aggressively it zooms toward the maximizer vs. how quickly it finds any \\(\\epsilon\\)-good region. Propose an algorithm that interpolates between GP-UCB and elimination-style good-action identification, and prove a Pareto frontier of guarantees (e.g., bounds simultaneously on cumulative regret and lenient regret) in terms of \\(\\gamma_T\\) and kernel smoothness (Mat\u00e9rn \\(\\nu\\))."
      ],
      "generated_ideas_raw": "1. **Kernel-Smoothness-Adaptive GP-UCB via Online Eigen-Decay Estimation**  \n   Develop a GP-UCB variant that does not assume a known Mat\u00e9rn smoothness parameter \\(\\nu\\), but instead estimates the kernel eigenvalue decay rate online from the posterior covariance operator. Use this estimate to adapt the exploration parameter \\(\\beta_t\\) (through a data-driven bound on \\(\\gamma_T\\)) and prove regret guarantees that match the best fixed-\\(\\nu\\) choice up to logarithmic factors.\n\n2. **Threshold-Adaptive \u201cGood-Action\u201d Identification with Unknown Satisfaction Level**  \n   Extend lenient regret / good-action identification to the realistic case where the \u201cgood enough\u201d threshold is unknown and must be learned (e.g., user-defined utility drift). Propose an algorithm that maintains a posterior over both \\(f\\) and the threshold \\(\\tau\\), alternating between probing to localize \\(\\tau\\) and sampling to find \\(x\\) with \\(f(x)\\ge \\tau\\); provide sample-complexity bounds in terms of information gain and a margin condition near \\(\\tau\\).\n\n3. **Computationally-Scalable \\(\\pi\\)-GP-UCB with Provable Regret under Random Feature Approximations**  \n   Combine the scalability goals of \\(\\pi\\)-GP-UCB with structured kernel approximations (Nystr\u00f6m / random Fourier features) and analyze how approximation error perturbs \\(\\gamma_T\\) and regret. Deliver an implementable algorithm with explicit trade-offs between runtime (feature dimension), approximation quality (operator-spectrum tail), and cumulative regret.\n\n4. **Heteroscedastic-Noise GP Bandits with Information-Gain-Driven Exploration**  \n   Generalize the information-gain-based regret analysis to heteroscedastic observation noise \\(\\sigma^2(x)\\) that can vary across the domain (common in sensors/robotics). Propose a GP-UCB/TS rule that uses a noise-aware mutual information criterion (e.g., weighted log-determinant) and prove improved regret when the algorithm learns to sample low-noise regions for exploitation while selectively exploring high-noise regions only when informative.\n\n5. **One-Dimensional Tight Bounds Beyond Standard Cumulative Regret: Simple Regret and Lenient Regret**  \n   Building on the tight 1D cumulative regret characterization, derive tight (or near-tight) lower/upper bounds for 1D *simple regret* and *lenient regret* under SE and Mat\u00e9rn kernels. Design an algorithm that is provably optimal for these criteria by explicitly avoiding over-zooming near the maximizer (as suggested by lenient regret results) and quantify the exact improvement relative to GP-UCB.\n\n6. **Experimental-Design-Constrained GP Bandits: Regret with Submodular Budgeted Information Gain**  \n   Formalize a setting where each query has a cost and the learner must respect a budget while optimizing reward (linking GP optimization to experimental design submodularity). Create a budget-feasible GP-UCB that selects points via a greedy approximation to submodular information gain per unit cost, and prove regret bounds in terms of cost-normalized \\(\\gamma_T\\) and knapsack-submodularity approximation factors.\n\n7. **Misspecified-Kernel GP Bandits with Posterior-Consistency-Guided Regret Guarantees**  \n   Use posterior consistency tools (small-ball probabilities / RKHS support arguments) to analyze GP bandits when the assumed kernel is misspecified but still \u201ccovers\u201d the true function class. Provide conditions under which the posterior concentrates fast enough to retain sublinear regret, and design a practical kernel-mixture GP-UCB that maintains a discrete set of kernels and achieves oracle-like regret relative to the best kernel in hindsight.\n\n8. **Manifold/Intrinsic-Dimension GP-UCB via Spectrum-Aware Domain Adaptation**  \n   Extend eigenvalue-decay-based \\(\\gamma_T\\) bounds to domains where inputs lie on an unknown low-dimensional manifold embedded in \\([0,1]^d\\). Propose an algorithm that estimates intrinsic dimension from observed covariance spectra and adapts exploration accordingly, with regret scaling in intrinsic dimension rather than ambient dimension (making the \u201cweak dependence on dimensionality\u201d explicit and data-adaptive).\n\n9. **GP Bandits for Non-Gaussian Feedback Using Binary-Regression Consistency Machinery**  \n   Replace Gaussian noise with binary/categorical feedback (e.g., success/fail experiments), modeling the latent function with a GP passed through a link function (as in GP binary regression). Develop a \u201cGP-UCB with link\u201d acquisition based on posterior credible intervals in the latent space, and prove regret or good-action identification bounds using posterior consistency and information-gain-type measures for generalized likelihoods.\n\n10. **Joint Optimization and Identification: Pareto-Optimal Trade-off Between Standard and Lenient Regret**  \n   Create a unified framework where the learner can choose a tunable parameter controlling how aggressively it zooms toward the maximizer vs. how quickly it finds any \\(\\epsilon\\)-good region. Propose an algorithm that interpolates between GP-UCB and elimination-style good-action identification, and prove a Pareto frontier of guarantees (e.g., bounds simultaneously on cumulative regret and lenient regret) in terms of \\(\\gamma_T\\) and kernel smoothness (Mat\u00e9rn \\(\\nu\\)).",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Kernel-Smoothness-Adaptive GP-UCB via Online Eigen-Decay Estimation\nDevelop a GP-UCB variant that does not assume a known Mat\u00e9rn smoothness parameter \\(\\nu\\), but instead estimates the kernel eigenval",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Threshold-Adaptive \u201cGood-Action\u201d Identification with Unknown Satisfaction Level\nExtend lenient regret / good-action identification to the realistic case where the \u201cgood enough\u201d threshold is unknown an",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Computationally-Scalable \\(\\pi\\)-GP-UCB with Provable Regret under Random Feature Approximations\nCombine the scalability goals of \\(\\pi\\)-GP-UCB with structured kernel approximations (Nystr\u00f6m / random",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Experimental-Design-Constrained GP Bandits: Regret with Submodular Budgeted Information Gain\nFormalize a setting where each query has a cost and the learner must respect a budget while optimizing rewa",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Misspecified-Kernel GP Bandits with Posterior-Consistency-Guided Regret Guarantees\nUse posterior consistency tools (small-ball probabilities / RKHS support arguments) to analyze GP bandits when the as",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "GP Bandits for Non-Gaussian Feedback Using Binary-Regression Consistency Machinery\nReplace Gaussian noise with binary/categorical feedback (e.g., success/fail experiments), modeling the latent functio",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Joint Optimization and Identification: Pareto-Optimal Trade-off Between Standard and Lenient Regret\nCreate a unified framework where the learner can choose a tunable parameter controlling how aggressi",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 29,
      "paper_title": "Auto-Compressing Networks",
      "contribution": "Introduce a novel architecture (ACN) that replaces short residual connections with long additive feedforward connections to the output, inducing an architectural auto-compression dynamic that concentrates useful information into earlier layers during training and yields compact, more robust representations without sacrificing accuracy.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10828,
      "output_tokens": 901,
      "predecessor_details": [
        {
          "success": true,
          "title": "Deep Residual Learning for Image Recognition",
          "url": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf",
          "content": "Deep Residual Learning for Image Recognition\nKaiming He Xiangyu Zhang Shaoqing Ren Jian Sun\nMicrosoft Research\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\nAbstract\nDeeper neural networks are more difficult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn\u0002ing residual functions with reference to the layer inputs, in\u0002stead of learning unreferenced functions. We provide com\u0002prehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers\u20148\u00d7\ndeeper than VGG nets [40] but still having lower complex\u0002ity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex\u0002tremely deep representations, we obtain a 28% relative im\u0002provement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC\n& COCO 2015 competitions1\n, where we also won the 1st\nplaces on the tasks of ImageNet detection, ImageNet local\u0002ization, COCO detection, and COCO segmentation.\n1. Introduction\nDeep convolutional neural networks [22, 21] have led\nto a series of breakthroughs for image classification [21,\n49, 39]. Deep networks naturally integrate low/mid/high\u0002level features [49] and classifiers in an end-to-end multi\u0002layer fashion, and the \u201clevels\u201d of features can be enriched\nby the number of stacked layers (depth). Recent evidence\n[40, 43] reveals that network depth is of crucial importance,\nand the leading results [40, 43, 12, 16] on the challenging\nImageNet dataset [35] all exploit \u201cvery deep\u201d [40] models,\nwith a depth of sixteen [40] to thirty [16]. Many other non\u0002trivial visual recognition tasks [7, 11, 6, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/ and\nhttp://mscoco.org/dataset/#detections-challenge2015.\n0 1 2 3 4 5 6\n0\n10\n20\niter. (1e4)\ntraining error (%)\n0 1 2 3 4 5 6\n0\n10\n20\niter. (1e4)\ntest error (%)\n56-layer\n20-layer\n56-layer\n20-layer\nFigure 1. Training error (left) and test error (right) on CIFAR-10\nwith 20-layer and 56-layer \u201cplain\u201d networks. The deeper network\nhas higher training error, and thus test error. Similar phenomena\non ImageNet is presented in Fig. 4.\ngreatly benefited from very deep models.\nDriven by the significance of depth, a question arises: Is\nlearning better networks as easy as stacking more layers?\nAn obstacle to answering this question was the notorious\nproblem of vanishing/exploding gradients [14, 1, 8], which\nhamper convergence from the beginning. This problem,\nhowever, has been largely addressed by normalized initial\u0002ization [23, 8, 36, 12] and intermediate normalization layers\n[16], which enable networks with tens of layers to start con\u0002verging for stochastic gradient descent (SGD) with back\u0002propagation [22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly. Unexpectedly,\nsuch degradation is not caused by overfitting, and adding\nmore layers to a suitably deep model leads to higher train\u0002ing error, as reported in [10, 41] and thoroughly verified by\nour experiments. Fig. 1 shows a typical example.\nThe degradation (of training accuracy) indicates that not\nall systems are similarly easy to optimize. Let us consider a\nshallower architecture and its deeper counterpart that adds\nmore layers onto it. There exists a solution by construction\nto the deeper model: the added layers are identity mapping,\nand the other layers are copied from the learned shallower\nmodel. The existence of this constructed solution indicates\nthat a deeper model should produce no higher training error\nthan its shallower counterpart. But experiments show that\nour current solvers on hand are unable to find solutions that\n1770\nidentity\nweight layer\nweight layer\nrelu\nrelu\nF(x) + x\nx\nF(x)\nx\nFigure 2. Residual learning: a building block.\nare comparably good or better than the constructed solution\n(or unable to do so in feasible time).\nIn this paper, we address the degradation problem by\nintroducing a deep residual learning framework. In\u0002stead of hoping each few stacked layers directly fit a\ndesired underlying mapping, we explicitly let these lay\u0002ers fit a residual mapping. Formally, denoting the desired\nunderlying mapping as H(x), we let the stacked nonlinear\nlayers fit another mapping of F(x) := H(x)\u2212x. The orig\u0002inal mapping is recast into F(x)+x. We hypothesize that it\nis easier to optimize the residual mapping than to optimize\nthe original, unreferenced mapping. To the extreme, if an\nidentity mapping were optimal, it would be easier to push\nthe residual to zero than to fit an identity mapping by a stack\nof nonlinear layers.\nThe formulation of F(x) +x can be realized by feedfor\u0002ward neural networks with \u201cshortcut connections\u201d (Fig. 2).\nShortcut connections [2, 33, 48] are those skipping one or\nmore layers. In our case, the shortcut connections simply\nperform identity mapping, and their outputs are added to\nthe outputs of the stacked layers (Fig. 2). Identity short\u0002cut connections add neither extra parameter nor computa\u0002tional complexity. The entire network can still be trained\nend-to-end by SGD with backpropagation, and can be eas\u0002ily implemented using common libraries (e.g., Caffe [19])\nwithout modifying the solvers.\nWe present comprehensive experiments on ImageNet\n[35] to show the degradation problem and evaluate our\nmethod. We show that: 1) Our extremely deep residual nets\nare easy to optimize, but the counterpart \u201cplain\u201d nets (that\nsimply stack layers) exhibit higher training error when the\ndepth increases; 2) Our deep residual nets can easily enjoy\naccuracy gains from greatly increased depth, producing re\u0002sults substantially better than previous networks.\nSimilar phenomena are also shown on the CIFAR-10 set\n[20], suggesting that the optimization difficulties and the\neffects of our method are not just akin to a particular dataset.\nWe present successfully trained models on this dataset with\nover 100 layers, and explore models with over 1000 layers.\nOn the ImageNet classification dataset [35], we obtain\nexcellent results by extremely deep residual nets. Our 152-\nlayer residual net is the deepest network ever presented on\nImageNet, while still having lower complexity than VGG\nnets [40]. Our ensemble has 3.57% top-5 error on the\nImageNet test set, and won the 1st place in the ILSVRC\n2015 classification competition. The extremely deep rep\u0002resentations also have excellent generalization performance\non other recognition tasks, and lead us to further win the\n1st places on: ImageNet detection, ImageNet localization,\nCOCO detection, and COCO segmentation in ILSVRC &\nCOCO 2015 competitions. This strong evidence shows that\nthe residual learning principle is generic, and we expect that\nit is applicable in other vision and non-vision problems.\n2. Related Work\nResidual Representations. In image recognition, VLAD\n[18] is a representation that encodes by the residual vectors\nwith respect to a dictionary, and Fisher Vector [30] can be\nformulated as a probabilistic version [18] of VLAD. Both\nof them are powerful shallow representations for image re\u0002trieval and classification [4, 47]. For vector quantization,\nencoding residual vectors [17] is shown to be more effec\u0002tive than encoding original vectors.\nIn low-level vision and computer graphics, for solv\u0002ing Partial Differential Equations (PDEs), the widely used\nMultigrid method [3] reformulates the system as subprob\u0002lems at multiple scales, where each subproblem is respon\u0002sible for the residual solution between a coarser and a finer\nscale. An alternative to Multigrid is hierarchical basis pre\u0002conditioning [44, 45], which relies on variables that repre\u0002sent residual vectors between two scales. It has been shown\n[3, 44, 45] that these solvers converge much faster than stan\u0002dard solvers that are unaware of the residual nature of the\nsolutions. These methods suggest that a good reformulation\nor preconditioning can simplify the optimization.\nShortcut Connections. Practices and theories that lead to\nshortcut connections [2, 33, 48] have been studied for a long\ntime. An early practice of training multi-layer perceptrons\n(MLPs) is to add a linear layer connected from the network\ninput to the output [33, 48]. In [43, 24], a few interme\u0002diate layers are directly connected to auxiliary classifiers\nfor addressing vanishing/exploding gradients. The papers\nof [38, 37, 31, 46] propose methods for centering layer re\u0002sponses, gradients, and propagated errors, implemented by\nshortcut connections. In [43], an \u201cinception\u201d layer is com\u0002posed of a shortcut branch and a few deeper branches.\nConcurrent with our work, \u201chighway networks\u201d [41, 42]\npresent shortcut connections with gating functions [15].\nThese gates are data-dependent and have parameters, in\ncontrast to our identity shortcuts that are parameter-free.\nWhen a gated shortcut is \u201cclosed\u201d (approaching zero), the\nlayers in highway networks represent non-residual func\u0002tions. On the contrary, our formulation always learns\nresidual functions; our identity shortcuts are never closed,\nand all information is always passed through, with addi\u0002tional residual functions to be learned. In addition, high\u00022771\nway networks have not demonstrated accuracy gains with\nextremely increased depth (e.g., over 100 layers).\n3. Deep Residual Learning\n3.1. Residual Learning\nLet us consider H(x) as an underlying mapping to be\nfit by a few stacked layers (not necessarily the entire net),\nwith x denoting the inputs to the first of thes",
          "original_query": "Deep residual learning for image recognition",
          "cleaned_query": "Deep residual learning for image recognition",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning",
          "url": "https://arxiv.org/abs/2511.12507",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2511.12507** (cs)\n\n\\[Submitted on 16 Nov 2025\\]\n\n# Title:Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning\n\nAuthors: [Jingtian Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+J), [Jingyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J), [Leong Hou U](https://arxiv.org/search/cs?searchtype=author&query=U,+L+H)\n\nView a PDF of the paper titled Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning, by Jingtian Ma and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2511.12507) [HTML (experimental)](https://arxiv.org/html/2511.12507v1)\n\n> Abstract:Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Graphics (cs.GR) |\n| Cite as: | [arXiv:2511.12507](https://arxiv.org/abs/2511.12507) \\[cs.LG\\] |\n| (or [arXiv:2511.12507v1](https://arxiv.org/abs/2511.12507v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2511.12507](https://doi.org/10.48550/arXiv.2511.12507) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Jingtian Ma \\[ [view email](https://arxiv.org/show-email/cfac4e77/2511.12507)\\] **\\[v1\\]**\nSun, 16 Nov 2025 08:48:02 UTC (3,544 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning, by Jingtian Ma and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2511.12507)\n- [HTML (experimental)](https://arxiv.org/html/2511.12507v1)\n- [TeX Source](https://arxiv.org/src/2511.12507)\n\n[view license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2511.12507&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2511.12507&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-11](https://arxiv.org/list/cs.LG/2025-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2511.12507?context=cs) [cs.GR](https://arxiv.org/abs/2511.12507?context=cs.GR)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2511.12507)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2511.12507)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2511.12507)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2511.12507) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Highway Networks",
          "cleaned_query": "Highway Networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "(PDF) Residual Networks Behave Like Ensembles of Relatively ...",
          "url": "https://www.researchgate.net/publication/303409435_Residual_Networks_Behave_Like_Ensembles_of_Relatively_Shallow_Networks",
          "content": "ArticlePDF Available\n\n# Residual Networks Behave Like Ensembles of Relatively Shallow Networks\n\n- May 2016\n- [Advances in Neural Information Processing Systems](https://www.researchgate.net/journal/Advances-in-Neural-Information-Processing-Systems-1049-5258)\n\nDOI: [10.48550/arXiv.1605.06431](https://doi.org/10.48550/arXiv.1605.06431)\n\nAuthors:\n\n[Andreas Veit](https://www.researchgate.net/profile/Andreas-Veit)\n\n- [Google Inc.](https://www.researchgate.net/institution/Google_Inc)\n\n[Michael Wilber](https://www.researchgate.net/scientific-contributions/Michael-J-Wilber-2046208080)\n\n[Michael Wilber](https://www.researchgate.net/scientific-contributions/Michael-J-Wilber-2046208080)\n\n- This person is not on ResearchGate, or hasn't claimed this research yet.\n\n\n[Serge Belongie](https://www.researchgate.net/scientific-contributions/Serge-Belongie-2082762757)\n\n[Serge Belongie](https://www.researchgate.net/scientific-contributions/Serge-Belongie-2082762757)\n\n- This person is not on ResearchGate, or hasn't claimed this research yet.\n\n\n[Download full-text PDF](https://www.researchgate.net/profile/Andreas-Veit/publication/303409435_Residual_Networks_Behave_Like_Ensembles_of_Relatively_Shallow_Networks/links/5a2f58ddaca2726d0bd6c9b6/Residual-Networks-Behave-Like-Ensembles-of-Relatively-Shallow-Networks.pdf)\n\n[Read full-text](https://www.researchgate.net/publication/303409435_Residual_Networks_Behave_Like_Ensembles_of_Relatively_Shallow_Networks#read)\n\n[Download citation](https://www.researchgate.net/publication/303409435_Residual_Networks_Behave_Like_Ensembles_of_Relatively_Shallow_Networks/citation/download)\n\nCopy link Link copied\n\n[Read full-text](https://www.researchgate.net/publication/303409435_Residual_Networks_Behave_Like_Ensembles_of_Relatively_Shallow_Networks#read) [Download citation](https://www.researchgate.net/publication/303409435_Residual_Networks_Behave_Like_Ensembles_of_Relatively_Shallow_Networks/citation/download)\nCopy link Link copied\n\n## Abstract\n\nIn this work, we introduce a novel interpretation of residual networks showing they are exponential ensembles. This observation is supported by a large-scale lesion study that demonstrates they behave just like ensembles at test time. Subsequently, we perform an analysis showing these ensembles mostly consist of networks that are each relatively shallow. For example, contrary to our expectations, most of the gradient in a residual network with 110 layers comes from an ensemble of very short networks, i.e., only 10-34 layers deep. This suggests that in addition to describing neural networks in terms of width and depth, there is a third dimension: multiplicity, the size of the implicit ensemble. Ultimately, residual networks do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network - rather, they avoid the problem simply by ensembling many short networks together. This insight reveals that depth is still an open research question and invites the exploration of the related notion of multiplicity.\n\n**Discover the world's research**\n\n- 25+ million members\n- 160+ million publication pages\n- 2.3+ billion citations\n\n[Join for free](https://www.researchgate.net/signup.SignUp.html)\n\nContent uploaded by [Andreas Veit](https://www.researchgate.net/profile/Andreas-Veit)\n\nAuthor content\n\nAll content in this area was uploaded by Andreas Veit on Dec 12, 2017\n\nContent may be subject to copyright.\n\nResidual Networks Behave Like Ensembles of\n\nRelatively Shallow Networks\n\nAndreas VeitMichael WilberSerge Belongie\n\nDepartment of Computer Science & Cornell Tech\n\nCornell University\n\n{av443, mjw285, sjb344}@cornell.edu\n\nAbstract\n\nIn this work we propose a novel interpretation of residual networks showing that\n\nthey can be seen as a collection of many paths of differing length.Moreover,\n\nresidual networks seem to enable very deep networks by leveraging only the short\n\npaths during training. To support this observation, we rewrite residual networks as\n\nan explicit collection of paths. Unlike traditional models, paths through residual\n\nnetworks vary in length.Further, a lesion study reveals that these paths show\n\nensemble-like behavior in the sense that they do not strongly depend on each other.\n\nFinally, and most surprising, most paths are shorter than one might expect, and\n\nonly the short paths are needed during training, as longer paths do not contribute\n\nany gradient.For example, most of the gradient in a residual network with 110\n\nlayers comes from paths that are only 10-34 layers deep.Our results reveal one\n\nof the key characteristics that seem to enable the training of very deep networks:\n\nResidual networks avoid the vanishing gradient problem by introducing short paths\n\nwhich can carry gradient throughout the extent of very deep networks.\n\n1 Introduction\n\nMost modern computer vision systems follow a familiar architecture, processing inputs from low-\n\nlevel features up to task speci\ufb01c high-level features.Recently proposed residual networks \\[\n\n5\n\n,\n\n6\n\n\\]\n\nchallenge this conventional view in three ways. First, they introduce identity skip-connections that\n\nbypass residual layers, allowing data to \ufb02ow from any layers directly to any subsequent layers. This\n\nis in stark contrast to the traditional strictly sequential pipeline. Second, skip connections give rise to\n\nnetworks that are two orders of magnitude deeper than previous models, with as many as 1202 layers.\n\nThis is contrary to architectures like AlexNet \\[\n\n13\n\n\\] and even biological systems \\[\n\n17\n\n\\] that can capture\n\ncomplex concepts within half a dozen layers.\n\n1\n\nThird, in initial experiments, we observe that removing\n\nsingle layers from residual networks at test time does not noticeably affect their performance. This\n\nis surprising because removing a layer from a traditional architecture such as VGG \\[\n\n18\n\n\\] leads to a\n\ndramatic loss in performance.\n\nIn this work we investigate the impact of these differences. To address the in\ufb02uence of identity skip-\n\nconnections, we introduce the unraveled view. This novel representation shows residual networks\n\ncan be viewed as a collection of many paths instead of a single deep network. Further, the perceived\n\nresilience of residual networks raises the question whether the paths are dependent on each other or\n\nwhether they exhibit a degree of redundancy. To \ufb01nd out, we perform a lesion study. The results show\n\nensemble-like behavior in the sense that removing paths from residual networks by deleting layers or\n\ncorrupting paths by reordering layers only has a modest and smooth impact on performance. Finally,\n\nwe investigate the depth of residual networks.Unlike traditional models, paths through residual\n\nnetworks vary in length. The distribution of path lengths follows a binomial distribution, meaning\n\n1Making the common assumption that a layer in a neural network corresponds to a cortical area.\n\narXiv:1605.06431v2 \\[cs.CV\\] 27 Oct 2016\n\nthat the majority of paths in a network with 110 layers are only about 55 layers deep. Moreover, we\n\nshow most gradient during training comes from paths that are even shorter, i.e., 10-34 layers deep.\n\nThis reveals a tension. On the one hand, residual network performance improves with adding more\n\nand more layers \\[\n\n6\n\n\\].However, on the other hand, residual networks can be seen as collections of\n\nmany paths and the only effective paths are relatively shallow.Our results could provide a \ufb01rst\n\nexplanation: residual networks do not resolve the vanishing gradient problem by preserving gradient\n\n\ufb02ow throughout the entire depth of the network. Rather, they enable very deep networks by shortening\n\nthe effective paths. For now, short paths still seem necessary to train very deep networks.\n\nIn this paper we make the following contributions:\n\n\u2022\n\nWe introduce the unraveled view, which illustrates that residual networks can be viewed as\n\na collection of many paths, instead of a single ultra-deep network.\n\n\u2022\n\nWe perform a lesion study to show that these paths do not strongly depend on each other,\n\neven though they are trained jointly. Moreover, they exhibit ensemble-like behavior in the\n\nsense that their performance smoothly correlates with the number of valid paths.\n\n\u2022\n\nWe investigate the gradient \ufb02ow through residual networks, revealing that only the short\n\npaths contribute gradient during training. Deep paths are not required during training.\n\n2Related Work\n\nThe sequential and hierarchical computer vision pipeline\n\nVisual processing has long been un-\n\nderstood to follow a hierarchical process from the analysis of simple to complex features.This\n\nformalism is based on the discovery of the receptive \ufb01eld \\[\n\n10\n\n\\], which characterizes the visual system\n\nas a hierarchical and feedforward system. Neurons in early visual areas have small receptive \ufb01elds\n\nand are sensitive to basic visual features, e.g., edges and bars.Neurons in deeper layers of the\n\nhierarchy capture basic shapes, and even deeper neurons respond to full objects. This organization\n\nhas been widely adopted in the computer vision and machine learning literature, from early neural\n\nnetworks such as the Neocognitron \\[\n\n4\n\n\\] and the traditional hand-crafted feature pipeline of Malik and\n\nPerona \\[\n\n15\n\n\\] to convolutional neural networks \\[\n\n13\n\n,\n\n14\n\n\\]. The recent strong results of very deep neural\n\nnetworks \\[\n\n18\n\n,\n\n20\n\n\\] led to the general perception that it is the depth of neural networks that govern their\n\nexpressive power and performance. In this work, we show that residual networks do not necessarily\n\nfollow this tradition.\n\nResidual networks\n\n\\[\n\n5\n\n,\n\n6\n\n\\] are neural networks in which each layer consists of a residual module\n\nfi\n\nand a skip connection\n\n2\n\nbypassing\n\nfi\n\n.Since layers in residual networks can comprise multiple\n\nconvolutional layers, we refer to them as residual blocks in the remainder of this paper. For clarity of\n\nnotation, we omit the initial pre-processing and \ufb01nal classi\ufb01cation steps. With\n\nyi\u22121\n\nas is in",
          "original_query": "Residual networks behave like ensembles of relatively shallow networks",
          "cleaned_query": "Residual networks behave like ensembles of relatively shallow networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Understanding intermediate layers using linear classifier probes",
          "url": "https://openreview.net/forum?id=HJ4-rAVtl",
          "content": "Understanding intermediate layers using linear classifier probes | OpenReview\n[![back arrow](https://openreview.net/images/arrow_left.svg)Go to**ICLR 2017**homepage](https://openreview.net/group?id=ICLR.cc/2017/conference)\n## Understanding intermediate layers using linear classifier probes[![Download PDF](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=HJ4-rAVtl)\n### [Guillaume Alain](https://openreview.net/profile?email=guillaume.alain.umontreal@gmail.com),[Yoshua Bengio](https://openreview.net/profile?email=yoshua.umontreal@gmail.com)\n26 Dec 2025 (modified: 12 Oct 2025)Submitted to ICLR 2017Readers:Everyone\n**TL;DR:**Investigating deep learning models by proposing a different concept of information\n**Abstract:**Neural network models have a reputation for being black boxes.\nWe propose a new method to better understand the roles and dynamics\nof the intermediate layers.\nOur method uses linear classifiers, referred to as &quot;probes&quot;,\nwhere a probe can only use the hidden units of a given intermediate layer\nas discriminating features.\nMoreover, these probes cannot affect the training phase of a model,\nand they are generally added after training.\nWe demonstrate how this can be used to develop a better intuition\nabout models and to diagnose potential problems.\n**Keywords:**Deep learning, Supervised Learning, Theory\n**Conflicts:**umontreal.ca, google.com\n**Community Implementations:**[![CatalyzeX](/images/catalyzex\\_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/understanding-intermediate-layers-using/code)\n3 Replies\nLoading\n[OpenReview](https://openreview.net/about)is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the[OpenReview Sponsors](https://openreview.net/sponsors). \u00a92025OpenReview",
          "original_query": "Understanding intermediate layers using linear classifier probes",
          "cleaned_query": "Understanding intermediate layers using linear classifier probes",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "(PDF) Deep Networks with Stochastic Depth - ResearchGate",
          "url": "https://www.researchgate.net/publication/308278012_Deep_Networks_with_Stochastic_Depth",
          "content": "(PDF) Deep Networks with Stochastic Depth\n* [Home](directory/publications)\n* [Probability Theory](topic/Probability-Theory/publications)\n* [Stochastic](topic/Stochastic/publications)\nConference PaperPDF Available\n# Deep Networks with Stochastic Depth\n* October 2016\n* [Lecture Notes in Computer Science](journal/Lecture-Notes-in-Computer-Science-0302-9743)9908:646-661\nDOI:[10.1007/978-3-319-46493-0\\_39](https://doi.org/10.1007/978-3-319-46493-0_39)\n* Conference: European Conference on Computer Vision\nAuthors:\n[](profile/Gao-Huang)\n[Gao Huang](profile/Gao-Huang)\n* [Tsinghua University](https://www.researchgate.net/institution/Tsinghua-University)\n[](scientific-contributions/Yu-Sun-2108180850)\n[Yu Sun](scientific-contributions/Yu-Sun-2108180850)\n[Yu Sun](scientific-contributions/Yu-Sun-2108180850)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\n[](profile/Zhuang-Liu)\n[Zhuang Liu](profile/Zhuang-Liu)\n* [Tsinghua University](https://www.researchgate.net/institution/Tsinghua-University)\n[](scientific-contributions/Daniel-Sedra-2108159175)\n[Daniel Sedra](scientific-contributions/Daniel-Sedra-2108159175)\n[Daniel Sedra](scientific-contributions/Daniel-Sedra-2108159175)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\nShow all 5 authorsHide\n[Download full-text PDF](profile/Gao-Huang/publication/308278012_Deep_Networks_with_Stochastic_Depth/links/5a7470d8aca2720bc0ddd3df/Deep-Networks-with-Stochastic-Depth.pdf)[Read full-text](publication/308278012_Deep_Networks_with_Stochastic_Depth#read)\n[Download full-text PDF](https://www.researchgate.net/profile/Gao-Huang/publication/308278012_Deep_Networks_with_Stochastic_Depth/links/5a7470d8aca2720bc0ddd3df/Deep-Networks-with-Stochastic-Depth.pdf)\n[Read full-text](publication/308278012_Deep_Networks_with_Stochastic_Depth#read)\n[Download citation](https://www.researchgate.net/publication/308278012_Deep_Networks_with_Stochastic_Depth/citation/download)\nCopy linkLink copied\n[\nRead full-text\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#read)[\nDownload citation\n](https://www.researchgate.net/publication/308278012_Deep_Networks_with_Stochastic_Depth/citation/download)\nCopy linkLink copied\n## Abstract and Figures\nVery deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 % on CIFAR-10).\n[\n](https://www.researchgate.net/figure/The-first-convolutional-layers-mean-gradient-magnitude-for-each-epoch-during-training_fig7_308278012)\n[\nThe first convolutional layer\u2019s mean gradient magnitude for each epoch during training. The vertical dotted lines indicate scheduled reductions in learning rate by a factor of 10, which cause gradients to shrink.\n\u2026](https://www.researchgate.net/figure/The-first-convolutional-layers-mean-gradient-magnitude-for-each-epoch-during-training_fig7_308278012)\nFigures - uploaded by[Gao Huang](profile/Gao-Huang)\nAuthor content\nAll figure content in this area was uploaded by Gao Huang\nContent may be subject to copyright.\n**Discover the world's research**\n* 25+ million members\n* 160+ million publication pages\n* 2.3+ billion citations[Join for free](signup.SignUp.html)\n[](publication/308278012_Deep_Networks_with_Stochastic_Depth#read-preview)\nContent uploaded by[Gao Huang](profile/Gao-Huang)\nAuthor content\nAll content in this area was uploaded by Gao Huang on Feb 02, 2018\nContent may be subject to copyright.\nDeep Networks with Stochastic Depth\nGao Huang1(B\n), Yu Sun1,ZhuangLiu\n2, Daniel Sedra1,\nand Kilian Q. Weinberger1\n1Cornell University, Ithaca, USA\n{gh349,ys646,dms422,kqw4}@cornell.edu\n2Tsinghua University, Beijing, China\nliuzhuang13@mails.tsinghua.edu.cn\nAbstract.Verydeep convolutional networks with hundreds of layers\nhave led to signi\ufb01cant reductions in error on competitive benchmarks.\nAlthough the unmatched expressiveness of the many layers can be highly\ndesirable at test time, training very deep networks comes with its own\nset of challenges. The gradients can vanish, the forward \ufb02ow often dimin-\nishes, and the training time can be painfully slow. To addressthese prob-\nlems, we proposestochastic depth, a training procedure that enables the\nseemingly contradictory setup totrain shortnetworks anduse deepnet-\nworks at test time. We start with very deep networks but during train-\ning, for each mini-batch, randomly drop a subset of layers and bypass\nthem with the identity function. This simple approach complements the\nrecent success of residual networks. It reduces training time substantially\nand improves the test error signi\ufb01cantly on almost all data sets that we\nused for evaluation. With stochastic depth we can increase the depth\nof residual networks even beyond 1200 layers and still yield meaningful\nimprovements in test error (4.91% on CIFAR-10).\n1 Introduction\nConvolutional Neural Networks (CNNs) were arguably popularized within the\nvision community in 2009 through AlexNet [1] and its celebrated victory at the\nImageNet competition [2]. Since then there has been a notable shift towards\nCNNs in many areas of computer vision [3\u20138]. As this shift unfolds, a second\ntrend emerges; deeper and deeper CNN architectures are being developed and\ntrained. Whereas AlexNet had 5 convolutional layers [1], the VGG network and\nGoogLeNet in 2014 had 19 and 22 layers respectively [5,7], and most recently\nthe ResNet architecture featured 152 layers [8].\nNetwork depth is a major determinant of model expressiveness, both in the-\nory [9,10] and in practice [5,7,8]. However, very deep models also introduce new\nchallenges: vanishing gradients in backward propagation, diminishing feature\nreuse in forward propagation, and long training time.\nG. Huang and Y. Sun are contributed equally.\nc\n\ue002Springer International Publishing AG 2016\nB. Leibe et al. (Eds.): ECCV 2016, Part IV, LNCS 9908, pp. 646\u2013661, 2016.\nDOI: 10.1007/978-3-319-46493-039\n[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)\nDeep Networks with Stochastic Depth647\nVanishing Gradientsis a well known nuisance in neural networks with many\nlayers [11]. As the gradient information is back-propagated, repeated multipli-\ncation or convolution with small weights renders the gradient information inef-\nfectively small in earlier layers. Several approaches exist to reduce this e\ufb00ect in\npractice, for example through careful initialization [12], hidden layer supervision\n[13], or, recently, Batch Normalization [14].\nDiminishing feature reuseduring forward propagation (also known as loss in\ninformation \ufb02ow [15]) refers to the analogous problem to vanishing gradients in\nthe forward direction. The features of the input instance, or those computed by\nearlier layers, are \u201cwashed out\u201d through repeated multiplication or convolution\nwith (randomly initialized) weight matrices, making it hard for later layers to\nidentify and learn \u201cmeaningful\u201d gradient directions. Recently, several new archi-\ntectures attempt to circumvent this problem through direct identity mappings\nbetween layers, which allow the network to pass on features unimpededly from\nearlier layers to later layers [8,15].\nLong training timeis a serious concern as networks become very deep. The\nforward and backward passes scale linearly with the depth of the network. Even\non modern computers with multiple state-of-the-art GPUs, architectures like the\n152-layer ResNet require several weeks to converge on the ImageNet dataset [8].\nThe researcher is faced with an inherent dilemma: shorter networks have\nthe advantage that information \ufb02ows e\ufb03ciently forward and backward, and can\ntherefore be trained e\ufb00ectively and within a reasonable amount of time. How-\never, they are not expressive enough to represent the complex concepts that are\ncommonplace in computer vision applications. Very deep networks have much\ngreather model complexity, but are very di\ufb03cult to train in practice and require\na lot of time and patience.\nIn this paper, we proposedeep networks with stochastic depth, a novel train-\ning algorithm that is based on the seemingly contradictory insight that ideally\nwe would like to have adeepnetwork duringtestingbut ashortnetwork during\ntraining. We resolve this con\ufb02ict by creating deep Residual Network [8] archi-\ntectures (with hundre",
          "original_query": "Deep networks with stochastic depth",
          "cleaned_query": "Deep networks with stochastic depth",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Direct Feedback Alignment Provides Learning in Deep ...",
          "url": "https://arxiv.org/abs/1609.01596",
          "content": "# Statistics > Machine Learning\n\n**arXiv:1609.01596** (stat)\n\n\\[Submitted on 6 Sep 2016 ( [v1](https://arxiv.org/abs/1609.01596v1)), last revised 21 Dec 2016 (this version, v5)\\]\n\n# Title:Direct Feedback Alignment Provides Learning in Deep Neural Networks\n\nAuthors: [Arild N\u00f8kland](https://arxiv.org/search/stat?searchtype=author&query=N%C3%B8kland,+A)\n\nView a PDF of the paper titled Direct Feedback Alignment Provides Learning in Deep Neural Networks, by Arild N{\\\\o}kland\n\n[View PDF](https://arxiv.org/pdf/1609.01596)\n\n> Abstract:Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task.\n\n| | |\n| --- | --- |\n| Comments: | Accepted for publication at NIPS 2016. \\[v2\\] Corrected convolutional results for feedback-alignment. \\[v3,v4,v5\\] Corrected theorem and proof |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1609.01596](https://arxiv.org/abs/1609.01596) \\[stat.ML\\] |\n| | (or [arXiv:1609.01596v5](https://arxiv.org/abs/1609.01596v5) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1609.01596](https://doi.org/10.48550/arXiv.1609.01596) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Arild N\u00f8kland \\[ [view email](https://arxiv.org/show-email/dd9ff0c1/1609.01596)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1609.01596v1)**\nTue, 6 Sep 2016 15:07:32 UTC (1,691 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1609.01596v2)**\nSat, 24 Sep 2016 10:12:35 UTC (1,692 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/1609.01596v3)**\nThu, 6 Oct 2016 16:19:16 UTC (1,692 KB)\n\n**[\\[v4\\]](https://arxiv.org/abs/1609.01596v4)**\nSun, 23 Oct 2016 18:14:54 UTC (1,692 KB)\n\n**\\[v5\\]**\nWed, 21 Dec 2016 16:36:40 UTC (1,692 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Direct Feedback Alignment Provides Learning in Deep Neural Networks, by Arild N{\\\\o}kland\n\n- [View PDF](https://arxiv.org/pdf/1609.01596)\n- [TeX Source](https://arxiv.org/src/1609.01596)\n- [Other Formats](https://arxiv.org/format/1609.01596)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1609.01596&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1609.01596&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2016-09](https://arxiv.org/list/stat.ML/2016-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1609.01596?context=cs)\n\n[cs.LG](https://arxiv.org/abs/1609.01596?context=cs.LG)\n\n[stat](https://arxiv.org/abs/1609.01596?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1609.01596)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1609.01596)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1609.01596)\n\n### [1 blog link](https://arxiv.org/tb/1609.01596)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1609.01596&description=Direct Feedback Alignment Provides Learning in Deep Neural Networks) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1609.01596&title=Direct Feedback Alignment Provides Learning in Deep Neural Networks)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1609.01596) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Direct feedback alignment provides learning in deep neural networks",
          "cleaned_query": "Direct feedback alignment provides learning in deep neural networks",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Frequency-Gated Residual Graph Blocks for Road Networks\nBuild a GNN block that mirrors ResNet\u2019s identity shortcut but routes messages through separate low-/high-frequency branches (as in HiFiNet), then recombines via a learned gate. Evaluate whether frequency-gated skip connections reduce over-smoothing while preserving long-range structure on tasks like speed forecasting and incident detection.",
        "Multiplicity-Aware HiFiNet: Explicit Path Ensembling in Spatial\u2013Spectral GNNs\nExtend the \u201cResNets as ensembles\u201d view by explicitly enumerating and regularizing multiple computation paths across HiFiNet\u2019s hierarchy levels (e.g., different virtual-node granularities and frequency bands). Add a training objective that encourages diverse-but-consistent paths and measure gains in robustness and calibration under sensor dropout and topology perturbations.",
        "Stochastic Depth for Hierarchical Frequency Decomposition (HiFiNet-SD)\nIntroduce stochastic depth specifically at the decomposition\u2013updating\u2013reconstruction stages and across hierarchy levels, randomly bypassing (identity) certain frequency-update modules per mini-batch. This should shorten effective training paths (as in stochastic depth) while retaining deep inference capacity at test time; benchmark training time, stability, and generalization across multiple cities/datasets.",
        "Probe-Guided Frequency Alignment Diagnostics for Road GNNs\nAttach linear classifier probes to intermediate representations at each hierarchy level and each frequency branch (low/high) to quantify when/where predictive information emerges. Use probe trends to diagnose spatial\u2013spectral misalignment (e.g., \u201chigh-frequency branch learns global signals\u201d) and propose a corrective regularizer that nudges branches toward intended frequency roles.",
        "Direct Feedback Alignment Training for Residual Road GNNs\nAdapt Direct Feedback Alignment (DFA) to HiFiNet-style GNNs by sending task error to each hierarchy level and frequency branch via fixed random feedback matrices, avoiding full backprop through the entire stack. Compare DFA vs backprop in deep residual graph transformers for road networks, focusing on depth scaling, memory savings, and performance under limited compute.",
        "Learned Identity Shortcuts Conditioned on Road Topology Changes\nReplace fixed identity skips with \u201ctopology-conditioned identity\u201d mappings that adjust shortcut strength based on detected graph edits (closures, new roads) or edge attribute shifts. The key contribution is a residual mechanism that preserves stable representations under minor changes while allowing rapid adaptation when topology materially changes.",
        "Cross-Frequency Contrastive Pretraining with Virtual-Node Hierarchies\nUse HiFiNet\u2019s decomposition to create positive pairs across hierarchy levels (same region at different resolutions) and across frequency bands (same node\u2019s low/high components) with contrastive objectives. This yields a self-supervised pretraining pipeline tailored to road graphs, then fine-tune for downstream tasks to test improvements in low-label regimes.",
        "Gradient-Path Attribution for Spatial\u2013Spectral Residual Networks\nCombine lesion-style analyses (from the ResNet ensemble paper) with frequency-branch ablations to quantify which paths (short vs long, low vs high frequency, coarse vs fine hierarchy) contribute most to gradients and predictions. Use these attributions to design a pruning/distillation method that keeps only the most useful path families for efficient deployment.",
        "Residual Fusion of Multi-Modal Traffic Signals via Frequency Decomposition\nExtend the decomposition\u2013reconstruction framework to fuse multiple modalities (loop detectors, GPS traces, weather, events) by assigning each modality a frequency profile and combining them through residual fusion blocks. The actionable outcome is a concrete architecture and training recipe that tests whether residual multi-modal fusion improves incident prediction and demand forecasting versus late/early fusion baselines.",
        "Frequency-Adaptive Stochastic Routing: Dynamic Depth per Band\nCreate a controller that chooses per-sample effective depth separately for low- and high-frequency branches (e.g., deeper for smooth recurring patterns, shallower for noisy fluctuations), generalizing stochastic depth into conditional routing. Validate that adaptive multiplicity improves accuracy\u2013latency trade-offs and offers interpretable \u201cwhen the model goes deep\u201d behavior on challenging traffic regimes (rush hours, special events)."
      ],
      "generated_ideas_raw": "1. **Frequency-Gated Residual Graph Blocks for Road Networks**\n   Build a GNN block that mirrors ResNet\u2019s identity shortcut but routes messages through separate low-/high-frequency branches (as in HiFiNet), then recombines via a learned gate. Evaluate whether frequency-gated skip connections reduce over-smoothing while preserving long-range structure on tasks like speed forecasting and incident detection.\n\n2. **Multiplicity-Aware HiFiNet: Explicit Path Ensembling in Spatial\u2013Spectral GNNs**\n   Extend the \u201cResNets as ensembles\u201d view by explicitly enumerating and regularizing multiple computation paths across HiFiNet\u2019s hierarchy levels (e.g., different virtual-node granularities and frequency bands). Add a training objective that encourages diverse-but-consistent paths and measure gains in robustness and calibration under sensor dropout and topology perturbations.\n\n3. **Stochastic Depth for Hierarchical Frequency Decomposition (HiFiNet-SD)**\n   Introduce stochastic depth specifically at the decomposition\u2013updating\u2013reconstruction stages and across hierarchy levels, randomly bypassing (identity) certain frequency-update modules per mini-batch. This should shorten effective training paths (as in stochastic depth) while retaining deep inference capacity at test time; benchmark training time, stability, and generalization across multiple cities/datasets.\n\n4. **Probe-Guided Frequency Alignment Diagnostics for Road GNNs**\n   Attach linear classifier probes to intermediate representations at each hierarchy level and each frequency branch (low/high) to quantify when/where predictive information emerges. Use probe trends to diagnose spatial\u2013spectral misalignment (e.g., \u201chigh-frequency branch learns global signals\u201d) and propose a corrective regularizer that nudges branches toward intended frequency roles.\n\n5. **Direct Feedback Alignment Training for Residual Road GNNs**\n   Adapt Direct Feedback Alignment (DFA) to HiFiNet-style GNNs by sending task error to each hierarchy level and frequency branch via fixed random feedback matrices, avoiding full backprop through the entire stack. Compare DFA vs backprop in deep residual graph transformers for road networks, focusing on depth scaling, memory savings, and performance under limited compute.\n\n6. **Learned Identity Shortcuts Conditioned on Road Topology Changes**\n   Replace fixed identity skips with \u201ctopology-conditioned identity\u201d mappings that adjust shortcut strength based on detected graph edits (closures, new roads) or edge attribute shifts. The key contribution is a residual mechanism that preserves stable representations under minor changes while allowing rapid adaptation when topology materially changes.\n\n7. **Cross-Frequency Contrastive Pretraining with Virtual-Node Hierarchies**\n   Use HiFiNet\u2019s decomposition to create positive pairs across hierarchy levels (same region at different resolutions) and across frequency bands (same node\u2019s low/high components) with contrastive objectives. This yields a self-supervised pretraining pipeline tailored to road graphs, then fine-tune for downstream tasks to test improvements in low-label regimes.\n\n8. **Gradient-Path Attribution for Spatial\u2013Spectral Residual Networks**\n   Combine lesion-style analyses (from the ResNet ensemble paper) with frequency-branch ablations to quantify which paths (short vs long, low vs high frequency, coarse vs fine hierarchy) contribute most to gradients and predictions. Use these attributions to design a pruning/distillation method that keeps only the most useful path families for efficient deployment.\n\n9. **Residual Fusion of Multi-Modal Traffic Signals via Frequency Decomposition**\n   Extend the decomposition\u2013reconstruction framework to fuse multiple modalities (loop detectors, GPS traces, weather, events) by assigning each modality a frequency profile and combining them through residual fusion blocks. The actionable outcome is a concrete architecture and training recipe that tests whether residual multi-modal fusion improves incident prediction and demand forecasting versus late/early fusion baselines.\n\n10. **Frequency-Adaptive Stochastic Routing: Dynamic Depth per Band**\n   Create a controller that chooses per-sample effective depth separately for low- and high-frequency branches (e.g., deeper for smooth recurring patterns, shallower for noisy fluctuations), generalizing stochastic depth into conditional routing. Validate that adaptive multiplicity improves accuracy\u2013latency trade-offs and offers interpretable \u201cwhen the model goes deep\u201d behavior on challenging traffic regimes (rush hours, special events).",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Frequency-Gated Residual Graph Blocks for Road Networks\nBuild a GNN block that mirrors ResNet\u2019s identity shortcut but routes messages through separate low-/high-frequency branches (as in HiFiNet), the",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Multiplicity-Aware HiFiNet: Explicit Path Ensembling in Spatial\u2013Spectral GNNs\nExtend the \u201cResNets as ensembles\u201d view by explicitly enumerating and regularizing multiple computation paths across HiFiNe",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Stochastic Depth for Hierarchical Frequency Decomposition (HiFiNet-SD)\nIntroduce stochastic depth specifically at the decomposition\u2013updating\u2013reconstruction stages and across hierarchy levels, randomly",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Probe-Guided Frequency Alignment Diagnostics for Road GNNs\nAttach linear classifier probes to intermediate representations at each hierarchy level and each frequency branch (low/high) to quantify when",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Direct Feedback Alignment Training for Residual Road GNNs\nAdapt Direct Feedback Alignment (DFA) to HiFiNet-style GNNs by sending task error to each hierarchy level and frequency branch via fixed rando",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Learned Identity Shortcuts Conditioned on Road Topology Changes\nReplace fixed identity skips with \u201ctopology-conditioned identity\u201d mappings that adjust shortcut strength based on detected graph edits (",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Cross-Frequency Contrastive Pretraining with Virtual-Node Hierarchies\nUse HiFiNet\u2019s decomposition to create positive pairs across hierarchy levels (same region at different resolutions) and across fre",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Gradient-Path Attribution for Spatial\u2013Spectral Residual Networks\nCombine lesion-style analyses (from the ResNet ensemble paper) with frequency-branch ablations to quantify which paths (short vs long, ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Residual Fusion of Multi-Modal Traffic Signals via Frequency Decomposition\nExtend the decomposition\u2013reconstruction framework to fuse multiple modalities (loop detectors, GPS traces, weather, events) b",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Frequency-Adaptive Stochastic Routing: Dynamic Depth per Band\nCreate a controller that chooses per-sample effective depth separately for low- and high-frequency branches (e.g., deeper for smooth recur",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 35,
      "paper_title": "Interactive Cross-modal Learning for Text-3D Scene Retrieval",
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "num_predecessors": 6,
      "ideas_generated": 10,
      "input_tokens": 9190,
      "output_tokens": 1042,
      "contribution": "",
      "predecessor_details": [],
      "generated_ideas": [],
      "judgments": []
    },
    {
      "paper_idx": 36,
      "paper_title": "Rethinking Joint Maximum Mean Discrepancy for Visual Domain Adaptation",
      "contribution": "They derive a concise, representer-theorem based form of JMMD that (1) unifies marginal/conditional/weighted distances as special cases via label kernels, (2) explains why JMMD can hurt feature discrimination through a graph-embedding view, and (3) repairs this by jointly optimizing JMMD with HSIC (JMMD-HSIC) to produce a tractable, discrimination-preserving adaptation loss.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 10214,
      "output_tokens": 960,
      "predecessor_details": [
        {
          "success": true,
          "title": "[0805.2368] A Kernel Method for the Two-Sample Problem",
          "url": "https://arxiv.org/abs/0805.2368",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:0805.2368** (cs)\n\n\\[Submitted on 15 May 2008\\]\n\n# Title:A Kernel Method for the Two-Sample Problem\n\nAuthors: [Arthur Gretton](https://arxiv.org/search/cs?searchtype=author&query=Gretton,+A), [Karsten Borgwardt](https://arxiv.org/search/cs?searchtype=author&query=Borgwardt,+K), [Malte J. Rasch](https://arxiv.org/search/cs?searchtype=author&query=Rasch,+M+J), [Bernhard Scholkopf](https://arxiv.org/search/cs?searchtype=author&query=Scholkopf,+B), [Alexander J. Smola](https://arxiv.org/search/cs?searchtype=author&query=Smola,+A+J)\n\nView a PDF of the paper titled A Kernel Method for the Two-Sample Problem, by Arthur Gretton and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/0805.2368)\n\n> Abstract: We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| ACM\u00a0classes: | G.3; I.2.6 |\n| Cite as: | [arXiv:0805.2368](https://arxiv.org/abs/0805.2368) \\[cs.LG\\] |\n| (or [arXiv:0805.2368v1](https://arxiv.org/abs/0805.2368v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.0805.2368](https://doi.org/10.48550/arXiv.0805.2368) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Alex Smola J \\[ [view email](https://arxiv.org/show-email/6cd3ab59/0805.2368)\\] **\\[v1\\]**\nThu, 15 May 2008 17:46:53 UTC (102 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled A Kernel Method for the Two-Sample Problem, by Arthur Gretton and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/0805.2368)\n- [TeX Source](https://arxiv.org/src/0805.2368)\n- [Other Formats](https://arxiv.org/format/0805.2368)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=0805.2368&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=0805.2368&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2008-05](https://arxiv.org/list/cs.LG/2008-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/0805.2368?context=cs) [cs.AI](https://arxiv.org/abs/0805.2368?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:0805.2368)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=0805.2368)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:0805.2368)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr0805.html#abs-0805-2368) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-0805-2368)\n\n[Arthur Gretton](https://dblp.uni-trier.de/search/author?author=Arthur%20Gretton) [Karsten M. Borgwardt](https://dblp.uni-trier.de/search/author?author=Karsten%20M.%20Borgwardt) [Malte J. Rasch](https://dblp.uni-trier.de/search/author?author=Malte%20J.%20Rasch) [Bernhard Sch\u00f6lkopf](https://dblp.uni-trier.de/search/author?author=Bernhard%20Sch%C3%B6lkopf) [Alexander J. Smola](https://dblp.uni-trier.de/search/author?author=Alexander%20J.%20Smola)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/0805.2368) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "A kernel method for the two-sample-problem",
          "cleaned_query": "A kernel method for the two-sample-problem",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Measuring Statistical Dependence with Hilbert-Schmidt Norms",
          "url": "http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBouSmoSch05.pdf",
          "content": "Measuring Statistical Dependence with\nHilbert-Schmidt Norms\nArthur Gretton1, Olivier Bousquet2, Alex Smola3, and Bernhard Sch\u00a8olkopf1\n1 MPI for Biological Cybernetics, Spemannstr. 38, 72076 T\u00a8ubingen, Germany\n{arthur, bernhard.schoelkopf}@tuebingen.mpg.de 2 Pertinence, 32, Rue des Je\u02c6uneurs, 75002 Paris, France\nolivier.bousquet@pertinence.com 3 National ICT Australia, North Road, Canberra 0200 ACT, Australia\nalex.smola@nicta.com.au\nAbstract. We propose an independence criterion based on the eigen\u0002spectrum of covariance operators in reproducing kernel Hilbert spaces\n(RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt\nnorm of the cross-covariance operator (we term this a Hilbert-Schmidt In\u0002dependence Criterion, or HSIC). This approach has several advantages,\ncompared with previous kernel-based independence criteria. First, the\nempirical estimate is simpler than any other kernel dependence test, and\nrequires no user-defined regularisation. Second, there is a clearly defined\npopulation quantity which the empirical estimate approaches in the large\nsample limit, with exponential convergence guaranteed between the two:\nthis ensures that independence tests based on HSIC do not suffer from\nslow learning rates. Finally, we show in the context of independent com\u0002ponent analysis (ICA) that the performance of HSIC is competitive with\nthat of previously published kernel-based criteria, and of other recently\npublished ICA methods.\n1 Introduction\nMethods for detecting dependence using kernel-based approaches have recently\nfound application in a wide variety of areas. Examples include independent com\u0002ponent analysis [3, 10], gene selection [20], descriptions of gait in terms of hip and\nknee trajectories [15], feature selection [9], and dependence detection in fMRI\nsignals [11]. The principle underlying these algorithms is that we may define\ncovariance and cross-covariance operators in RKHSs, and derive statistics from\nthese operators suited to measuring the dependence between functions in these\nspaces.\nIn the method of Bach and Jordan [3], a regularised correlation operator\nwas derived from the covariance and cross-covariance operators, and its largest\nsingular value (the kernel canonical correlation, or KCC) was used as a statistic\nto test independence. The approach of Gretton et al. [11] was to use the largest\nsingular value of the cross-covariance operator, which behaves identically to the\nS. Jain, H.U. Simon, and E. Tomita (Eds.): ALT 2005, LNAI 3734, pp. 63\u201377, 2005.\n\u0001c Springer-Verlag Berlin Heidelberg 2005\n64 A. Gretton et al.\ncorrelation operator at independence, but is easier to define and requires no reg\u0002ularisation \u2014 the resulting test is called the constrained covariance (COCO).\nBoth these quantities fall within the framework set out by R\u00b4enyi [17], namely\nthat for sufficiently rich function classes, the functional correlation (or, alterna\u0002tively, the cross-covariance) serves as an independence test, being zero only when\nthe random variables tested are independent. Various empirical kernel quantities\n(derived from bounds on the mutual information that hold near independence)1\nwere also proposed based on the correlation and cross-covariance operators in\n[3, 10], however their connection to the population covariance operators remains\nto be established (indeed, the population quantities to which these approxima\u0002tions converge are not yet known). Gretton et al. [11] showed that these various\nquantities are guaranteed to be zero for independent random variables only when\nthe associated RKHSs are universal [19].\nThe present study extends the concept of COCO by using the entire spec\u0002trum of the cross-covariance operator to determine when all its singular values\nare zero, rather than looking only at the largest singular value; the idea being to\nobtain a more robust indication of independence. To this end, we use the sum\nof the squared singular values of the cross-covariance operator (i.e., its squared\nHilbert-Schmidt norm) to measure dependence \u2014 we call the resulting quan\u0002tity the Hilbert-Schmidt Independence Criterion (HSIC).2 It turns out that the\nempirical estimate of HSIC is identical to the quadratic dependence measure of\nAchard et al. [1], although we shall see that their derivation approaches this\ncriterion in a completely different way. Thus, the present work resolves the open\nquestion in [1] regarding the link between the quadratic dependence measure\nand kernel dependence measures based on RKHSs, and generalises this measure\nto metric spaces (as opposed to subsets of the reals). More importantly, however,\nwe believe our proof assures that HSIC is indeed a dependence criterion under\nall circumstances (i.e., HSIC is zero if and only if the random variables are in\u0002dependent), which is not necessarily guaranteed in [1]. We give a more detailed\nanalysis of Achard\u2019s proof in Appendix B.\nCompared with previous kernel independence measures, HSIC has several\nadvantages:\n\u2013 The empirical estimate is much simpler \u2014 just the trace of a product of\nGram matrices \u2014 and, unlike the canonical correlation or kernel generalised\nvariance [3], HSIC does not require extra regularisation terms for good finite\nsample behaviour.\n\u2013 The empirical estimate converges to the population estimate at rate 1/\n\u221am,\nwhere m is the sample size, and thus independence tests based on HSIC\ndo not suffer from slow learning rates [8]. In particular, as the sample size\nincreases, we are guaranteed to detect any existing dependence with high\n1 Respectively the Kernel Generalised Variance (KGV) and the Kernel Mutual Infor\u0002mation (KMI). 2 The possibility of using a Hilbert-Schmidt norm was suggested by Fukumizu et al. [9],\nalthough the idea was not pursued further in that work.\nMeasuring Statistical Dependence with Hilbert-Schmidt Norms 65\nprobability. Of the alternative kernel dependence tests, this result is proved\nonly for the constrained covariance [11].\n\u2013 The finite sample bias of the estimate is O(m\u22121), and is therefore negligible\ncompared to the finite sample fluctuations (which underly the convergence\nrate in the previous point). This is currently proved for no other kernel\ndependence test, including COCO.\n\u2013 Experimental results on an ICA problem show that the new independence\ntest is superior to the previous ones, and competitive with the best existing\nspecialised ICA methods. In particular, kernel methods are substantially\nmore resistant to outliers than other specialised ICA algorithms.\nWe begin our discussion in Section 2, in which we define the cross-covariance\noperator between RKHSs, and give its Hilbert-Schmidt (HS) norm (this being\nthe population HSIC). In Section 3, we given an empirical estimate of the HS\nnorm, and establish the link between the population and empirical HSIC by\ndetermining the bias of the finite sample estimate. In Section 4, we demonstrate\nexponential convergence between the population HSIC and empirical HSIC. As\na consequence of this fast convergence, we show in Section 5 that dependence\ntests formulated using HSIC do not suffer from slow learning rates. Also in this\nsection, we describe an efficient approximation to the empirical HSIC based on\nthe incomplete Cholesky decomposition. Finally, in Section 6, we apply HSIC to\nthe problem of independent component analysis (ICA).\n2 Cross-Covariance Operators\nIn this section, we provide the functional analytic background necessary in de\u0002scribing cross-covariance operators between RKHSs, and introduce the Hilbert\u0002Schmidt norm of these operators. Our presentation follows [21, 12], the main\ndifference being that we deal with cross-covariance operators rather than the\ncovariance operators.3 We also draw on [9], which uses covariance and cross\u0002covariance operators as a means of defining conditional covariance operators,\nbut does not investigate the Hilbert-Schmidt norm; and on [4], which charac\u0002terises the covariance and cross-covariance operators for general Hilbert spaces.\n2.1 RKHS Theory\nConsider a Hilbert space F of functions from X to R. Then F is a reproducing\nkernel Hilbert space if for each x \u2208 X, the Dirac evaluation operator \u03b4x : F \u2192 R,\nwhich maps f \u2208 F to f(x) \u2208 R, is a bounded linear functional. To each point x \u2208\nX, there corresponds an element \u03c6(x) \u2208 F such that \u0004\u03c6(x), \u03c6(x\u0002\n)\u0005F = k(x, x\u0002),\nwhere k : X \u00d7X \u2192 R is a unique positive definite kernel. We will require in\nparticular that F be separable (it must have a complete orthonormal system).\n3 Briefly, a cross-covariance operator maps from one space to another, whereas a\ncovariance operator maps from a space to itself. In the linear algebraic case,\nthe covariance is Cxx := Ex[xx\u0001] \u2212 Ex[x]Ex[x\u0001], while the cross-covariance is\nCxy := Ex,y[xy\u0001] \u2212 Ex[x]Ey[y\u0001].\n66 A. Gretton et al.\nAs pointed out in [12\u2013Theorem 7], any continuous kernel on a separable X (e.g.\nRn) induces a separable RKHS.4 We likewise define a second separable RKHS,\nG, with kernel l(\u00b7, \u00b7) and feature map \u03c8, on the separable space Y.\nHilbert-Schmidt Norm. Denote by C : G\u2192F a linear operator. Then provided\nthe sum converges, the Hilbert-Schmidt (HS) norm of C is defined as\n\u0006C\u00062\nHS := \u0001\ni,j\n\u0004Cvi, uj \u0005\n2\nF , (1)\nwhere ui and vj are orthonormal bases of F and G respectively. It is easy to see\nthat this generalises the Frobenius norm on matrices.\nHilbert-Schmidt Operator. A linear operator C : G\u2192F is called a Hilbert\u0002Schmidt operator if its HS norm exists. The set of Hilbert-Schmidt operators\nHS(G, F) : G\u2192F is a separable Hilbert space with inner product\n\u0004C, D\u0005HS := \u0001\ni,j\n\u0004Cvi, uj\u0005F \u0004Dvi, uj\u0005F .\nTensor Product. Let f \u2208 F and g \u2208 G. Then the tensor product operator\nf \u2297 g : G\u2192F is defined as\n(f \u2297 g)h := f\u0004g, h\u0005G for all h \u2208 G. (2)\nMoreover, by the definition of the HS norm, we can compute the HS norm of\nf \u2297 g via\n\u0006f \u2297 g\u00062\nHS = \u0004f \u2297 g, f \u2297 g\u0005HS = \u0004f,(f \u2297 g)g\u0005F\n= \u0004f,f\u0005F \u0004g, g\u0005G = \u0006f\u00062\nF \u0006g\u00062G (3)\n2.2 The Cross-Covariance Operator\nMean. We assume that (X, \u0393) and (Y, \u039b) are furnished with probability mea\u0002sures px, py respectively (\u0393 being the Borel sets on X,",
          "original_query": "Measuring statistical dependence with Hilbert\u2013Schmidt norms",
          "cleaned_query": "Measuring statistical dependence with Hilbert\u2013Schmidt norms",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning series) (Paperback)",
          "url": "https://www.harvard.com/book/9780262536578",
          "content": "[Skip to main content](https://www.harvard.com/book/9780262536578#main-content)\n**[Subscribe to Our Newsletter!](https://harvard.us7.list-manage.com/subscribe?u=4c9441ff8ea56271562808048&id=287f09065c)**\n- [![Facebook](https://www.harvard.com/sites/all/themes/contrib/flatland/images/facebook.png)](http://www.facebook.com/harvardbookstore)\n- [![Twitter](https://www.harvard.com/sites/all/themes/contrib/flatland/images/twitter.png)](http://www.twitter.com/harvardbooks)\n- [![Instagram](https://www.harvard.com/sites/all/themes/contrib/flatland/images/instagram.png)](http://www.instagram.com/harvardbookstore)\n- [![Email](https://www.harvard.com/sites/all/themes/contrib/flatland/images/email.png)](mailto:info@harvard.com)\n# Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning series) (Paperback)\n[![Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning series) By Bernhard Scholkopf, Alexander J. Smola Cover Image](https://images.booksense.com/images/578/536/9780262536578.jpg)](https://www.harvard.com/book/9780262536578)\n# Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning series) (Paperback)\nBy [Bernhard Scholkopf](https://www.harvard.com/search/author/%22Scholkopf%2C%20Bernhard%22), [Alexander J. Smola](https://www.harvard.com/search/author/%22Smola%2C%20Alexander%20J.%22)\n### $108.00\nAt Distributor - We Can Usually Get It in 3-8 Days!\n[Add to Wish List](https://www.harvard.com/multiple_wishlist/nojs/9780262536578)\n- [Description](https://www.harvard.com/book/9780262536578#desc)\n- [About the Author](https://www.harvard.com/book/9780262536578#about)\n- [Details](https://www.harvard.com/book/9780262536578#praise)\n- [Reviews & Media](https://www.harvard.com/book/9780262536578#reviews)\n**A comprehensive introduction to Support Vector Machines and related kernel methods.**\nIn the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs\u2014-kernels\u2014for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.\n_Learning with Kernels_ provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.\nBernhard Sch\u00f6lkopf is Director at the Max Planck Institute for Intelligent Systems in T\u00fcbingen, Germany. He is coauthor of _Learning with Kernels_ (2002) and is a coeditor of _Advances in Kernel Methods: Support Vector Learning_ (1998), _Advances in Large-Margin Classifiers_ (2000), and _Kernel Methods in Computational Biology_ (2004), all published by the MIT Press.\nAlexander J. Smola is Senior Principal Researcher and Machine Learning Program Leader at National ICT Australia/Australian National University, Canberra.\nCategories\n- [Computers / Computer Science](https://www.harvard.com/browse/book/COM014000)\nProduct Details **ISBN:** 9780262536578\n**ISBN-10:** 0262536579\n**Publisher:** The MIT Press\n**Publication Date:** June 5th, 2018\n**Pages:** 648\n**Language:** English\n**Series:** Adaptive Computation and Machine Learning series\nRecommended Reading Level **Minimum Age:** 18\n**Maximum Age:** UP\n**Minimum Grade Level:** UP\n**Maximum Grade Level:** UP",
          "original_query": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
          "cleaned_query": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Deep transfer learning with joint adaptation networks - Volume 70",
          "url": "https://dl.acm.org/doi/10.5555/3305890.3305909",
          "content": "Authors Info &amp; Claims Published: 06 August 2017 Publication History Deep transfer learning with joint adaptation networks Pages 2208 - 2217 Abstract References Information &amp; Contributors Bibliometrics &amp; Citations View Options References Figures Tables Media Share Abstract Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets. References [1] Arjovsky, Martin, Chintala, Soumith, and Bottou, L\u00e9on. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017. [2] Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. A theory of learning from different domains. Machine Learning, 79(1-2):151-175, 2010. [3] Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 35(8):1798-1828, 2013. [4] Bousmalis, Konstantinos, Trigeorgis, George, Silberman, Nathan, Krishnan, Dilip, and Erhan, Dumitru. Domain separation networks. In Advances in Neural Information Processing Systems (NIPS), pp. 343-351, 2016. [5] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. Journal of Machine Learning Research (JMLR), 12:2493-2537, 2011. [6] Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning (ICML), 2014. [7] Duan, L., Tsang, I. W., and Xu, D. Domain transfer multiple kernel learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 34(3):465-479, 2012. [8] Ganin, Y. and Lempitsky, V. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning (ICML), 2015. [9] Glorot, X., Bordes, A., and Bengio, Y. Domain adaptation for large-scale sentiment classification: A deep learning approach. In International Conference on Machine Learning (ICML), 2011. [10] Gong, B., Shi, Y., Sha, F., and Grauman, K. Geodesic flow kernel for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012. [11] Gong, B., Grauman, K., and Sha, F. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), 2013. [12] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), 2014. [13] Gopalan, R., Li, R., and Chellappa, R. Domain adaptation for object recognition: An unsupervised approach. In IEEE International Conference on Computer Vision (ICCV), 2011. [14] Gretton, A., Borgwardt, K., Rasch, M., Sch\u00f6lkopf, B., and Smola, A. A kernel two-sample test. Journal of Machine Learning Research (JMLR), 13:723-773, 2012. [15] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [16] Hoffman, J., Guadarrama, S., Tzeng, E., Hu, R., Donahue, J., Girshick, R., Darrell, T., and Saenko, K. LSDA: Large scale detection through adaptation. In Advances in Neural Information Processing Systems (NIPS), 2014. [17] Huang, J., Smola, A. J., Gretton, A., Borgwardt, K. M., and Sch\u00f6lkopf, B. Correcting sample selection bias by unlabeled data. In Advances in Neural Information Processing Systems (NIPS), 2006. [18] Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012. [19] Long, Mingsheng, Cao, Yue, Wang, Jianmin, and Jordan, Michael I. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning (ICML), 2015. [20] Long, Mingsheng, Zhu, Han, Wang, Jianmin, and Jordan, Michael I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems (NIPS), pp. 136-144, 2016. [21] Mansour, Y., Mohri, M., and Rostamizadeh, A. Domain adaptation: Learning bounds and algorithms. In Conference on Computational Learning Theory (COLT), 2009. [22] Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and transferring mid-level image representations using convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013. [23] Pan, S. J. and Yang, Q. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering (TKDE), 22(10):1345-1359, 2010. [24] Pan, S. J., Tsang, I. W., Kwok, J. T., and Yang, Q. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks (TNN), 22(2):199-210, 2011. [25] Quionero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N. D. Dataset shift in machine learning. The MIT Press, 2009. [26] Reddi, Sashank J, Ramdas, Aaditya, P\u00f3czos, Barnab\u00e1s, Singh, Aarti, and Wasserman, Larry A. On the high dimensional power of a linear-time two sample test under mean-shift alternatives. In Artificial Intelligence and Statistics Conference (AISTATS), 2015. [27] Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In European Conference on Computer Vision (ECCV), 2010. [28] Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015 (arXiv:1409.1556v6), 2015. [29] Smola, Alex, Gretton, Arthur, Song, Le, and Sch\u00f6lkopf, Bernhard. A hilbert space embedding for distributions. In International Conference on Algorithmic Learning Theory (ALT), pp. 13-31. Springer, 2007. [30] Song, L., Huang, J., Smola, A., and Fukumizu, K. Hilbert space embeddings of conditional distributions with applications to dynamical systems. In International Conference on Machine Learning (ICML), 2009. [31] Song, Le and Dai, Bo. Robust low rank kernel embeddings of multivariate distributions. In Advances in Neural Information Processing Systems (NIPS), pp. 3228-3236, 2013. [32] Song, Le, Boots, Byron, Siddiqi, Sajid M, Gordon, Geoffrey J, and Smola, Alex. Hilbert space embeddings of hidden markov models. In International Conference on Machine Learning (ICML), 2010. [33] Song, Le, Fukumizu, Kenji, and Gretton, Arthur. Kernel embeddings of conditional distributions: A unified kernel framework for nonparametric inference in graphical models. IEEE Signal Processing Magazine, 30(4):98-111, 2013. [34] Sriperumbudur, B. K., Fukumizu, K., Gretton, A., Lanckriet, G., and Sch\u00f6lkopf, B. Kernel choice and classifiability for rkhs embeddings of probability distributions. In Advances in Neural Information Processing Systems (NIPS), 2009. [35] Sriperumbudur, Bharath K, Gretton, Arthur, Fukumizu, Kenji, Sch\u00f6lkopf, Bernhard, and Lanckriet, Gert RG. Hilbert space embeddings and metrics on probability measures. Journal of Machine Learning Research (JMLR), 11(Apr):1517-1561, 2010. [36] Sugiyama, M., Nakajima, S., Kashima, H., Buenau, P. V., and Kawanabe, M. Direct importance estimation with model selection and its application to covariate shift adaptation. In Advances in Neural Information Processing Systems (NIPS), 2008. [37] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. [38] Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell, T. Deep domain confusion: Maximizing for domain invariance. CoRR, abs/1412.3474, 2014. [39] Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., and Darrell, T. Simultaneous deep transfer across domains and tasks. In IEEE International Conference on Computer Vision (ICCV), 2015. [40] Tzeng, Eric, Hoffman, Judy, Saenko, Kate, and Darrell, Trevor. Adversarial discriminative domain adaptation. arXiv preprint arXiv:1702.05464, 2017. [41] Wang, X. and Schneider, J. Flexible transfer learning under support and model shift. In Advances in Neural Information Processing Systems (NIPS), 2014. [42] Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems (NIPS), 2014. [43] Zhang, K., Sch\u00f6lkopf, B., Muandet, K., and Wang, Z. Domain adaptation under target and conditional shift. In International Conference on Machine Learning (ICML), 2013. [44] Zhong, E., Fan, W., Yang, Q., Verscheure, O., and Ren, J. Cross validation framework to choose amongst models and datasets for transfer learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML/PKDD), pp. 547-562. Springer, 2010. Information &amp; Contributors Information Published In \n \n ICML'17: Proceedings of the 34th International Conference on Machine Learning - Volume 70 August 2017 4208 pages \n Publisher Publication History Published: 06 August 2017 Qualifiers Article \n \n \n Contributors \n \n \n \n \n Other Metrics Bibliometrics &amp; Citations Bibliometrics \n \n \n Article Metrics\n \n \n View Citations Downloads (Last 12 months) 173 Downloads (Last 6 weeks) 23 Reflects downloads up to ",
          "original_query": "Deep transfer learning with joint adaptation networks",
          "cleaned_query": "Deep transfer learning with joint adaptation networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Transfer Feature Learning with Joint Distribution Adaptation",
          "url": "https://openaccess.thecvf.com/content_iccv_2013/papers/Long_Transfer_Feature_Learning_2013_ICCV_paper.pdf",
          "content": "Transfer Feature Learning with Joint Distribution Adaptation\nMingsheng Long\u2020\u2021, Jianmin Wang\u2020, Guiguang Ding\u2020, Jiaguang Sun\u2020, and Philip S. Yu\u00a7\n\u2020School of Software, TNLIST, Tsinghua University, Beijing, China\n\u2021\nDepartment of Computer Science, Tsinghua University, Beijing, China\n\u00a7Department of Computer Science, University of Illinois at Chicago, IL, USA\nlongmingsheng@gmail.com, {jimwang,dinggg,sunjg}@tsinghua.edu.cn, psyu@uic.edu\nAbstract\nTransfer learning is established as an effective technolo\u0002gy in computer vision for leveraging rich labeled data in the\nsource domain to build an accurate classifier for the target\ndomain. However, most prior methods have not simultane\u0002ously reduced the difference in both the marginal distribu\u0002tion and conditional distribution between domains. In this\npaper, we put forward a novel transfer learning approach,\nreferred to as Joint Distribution Adaptation (JDA). Specifi\u0002cally, JDA aims to jointly adapt both the marginal distribu\u0002tion and conditional distribution in a principled dimension\u0002ality reduction procedure, and construct new feature repre\u0002sentation that is effective and robust for substantial distribu\u0002tion difference. Extensive experiments verify that JDA can\nsignificantly outperform several state-of-the-art methods on\nfour types of cross-domain image classification problems.\n1. Introduction\nIn computer vision, labeled information is crucial for a\nvariety of recognition problems. For highly-evolving visual\ndomains where labeled data are very sparse, one may expect\nto leverage abundant labeled data readily available in some\nrelated source domains for training accurate classifiers to be\nreused in the target domain. Recently, the literature has wit\u0002nessed an increasing interest in developing transfer learn\u0002ing [16] algorithms for cross-domain knowledge adaptation\nproblems. Transfer learning has proven to be promising in\nimage classification [24, 12] and tagging [19, 25], object\nrecognition [14, 2, 7, 10], and feature learning [13, 11, 17].\nIn cross-domain problems, the source and target data\nare usually sampled from different probability distributions.\nTherefore, a major computational issue of transfer learning\nis to reduce the distribution difference between domains.\nRecent works aim to discover a shared feature representa\u0002tion which can reduce the distribution difference and pre\u0002serve the important properties of input data simultaneously\n0 5 10 0\n5\n10\nSource Domain\n0 5 10 0\n5\n10\nTarget Domain\nFigure 1. In our problem, labeled source and unlabeled target do\u0002mains are different in both marginal and conditional distributions.\n[21, 15, 18], or re-weight source data in order to minimize\nthe distribution difference and then learn a classifier on the\nre-weighted source data [3, 4]. Most of existing methods\nmeasure the distribution difference based on either marginal\ndistribution or conditional distribution. However, Figure 1\ndemonstrates the importance of matching both marginal and\nconditional distributions for robust transfer learning. Some\nvery recent works started to match both the marginal and\nconditional distributions using sample selection [26], ker\u0002nel density estimation [18], or two-stage re-weighting [23],\nbut they may require either some labeled data in the target\ndomain, or multiple source domains for consensus learning.\nIn this paper, we address a challenging scenario in which\nthe source and target domains are different in both marginal\nand conditional distributions, and the target domain has no\nlabeled data. We put forward a novel transfer learning so\u0002lution, referred to as Joint Distribution Adaptation (JDA),\nto jointly adapt both the marginal and conditional distri\u0002butions in a principled dimensionality reduction procedure.\nSpecifically, we extend the nonparametric Maximum Mean\nDiscrepancy (MMD) [8] to measure the difference in both\nmarginal and conditional distributions, and integrate it with\nPrincipal Component Analysis (PCA) to construct feature\nrepresentation that is effective and robust for substantial dis\u0002tribution difference. We present the underlying assumption\nand learning algorithm for the JDA optimization problem.\nWe perform comprehensive experiments on four types of\nreal-world datasets: digit (USPS, MNIST), face (PIE), and\nobject (COIL20, Office+Caltech [20]). From these datasets,\nwe construct 36 cross-domain image datasets, each under a\n2013 IEEE International Conference on Computer Vision\n1550-5499/13 $31.00 \u00a9 2013 IEEE\nDOI 10.1109/ICCV.2013.274\n2200\ndifferent difficulty in knowledge adaptation. Our empirical\nresults demonstrate a significant improvement of 7.57% in\nterms of classification accuracy, obtained by the proposed\nJDA approach over several state-of-the-art transfer learning\nmethods. Our results reveal substantial effects of matching\nboth marginal and conditional distributions across domains.\n2. Related Work\nIn this section, we discuss prior works on transfer learn\u0002ing that are related to ours, and highlight their differences.\nAccording to the literature survey [16], existing transfer\nlearning methods can be roughly organized into two cate\u0002gories: instance reweighting [3, 4] and feature extraction.\nOur work belongs to the feature extraction category, which\ncan be further reorganized into two rough subcategories.\n1) Property preservation, which shares latent factors\nacross domains by preserving important properties of data,\ne.g. statistical property [17, 11], geometric structure [19, 6].\n2) Distribution adaptation, which explicitly minimizes\npredefined distance measures to reduce the difference in the\nmarginal distribution [22, 15], conditional distribution [21],\nor both [26, 23, 18]. However, to match conditional dis\u0002tributions, these methods require either some labeled target\ndata, or multiple source domains for consensus learning.\nTo our knowledge, our work is among the first attempts\nto jointly adapt both marginal and conditional distributions\nbetween domains, and no labeled data are required in the\ntarget domain. Our work is a principled dimensionality re\u0002duction procedure with MMD-based distribution matching,\nwhich is different from feature re-weighting methods [1, 5].\n3. Joint Distribution Adaptation\nIn this section, we present in detail the Joint Distribution\nAdaptation (JDA) approach for effective transfer learning.\n3.1. Problem Definition\nWe begin with the definitions of terminologies. For clari\u0002ty, the frequently used notations are summarized in Table 1.\nDefinition 1 (Domain) A domain D is composed of an m\u0002dimensional feature space X and a marginal probability\ndistribution P(x), i.e., D = {X, P(x)}, where x \u2208 X.\nDefinition 2 (Task) Given domain D, a task T is com\u0002posed of a C-cardinality label set Y and a classifier f(x),\ni.e., T = {Y, f(x)}, where y \u2208 Y, and f(x) = Q(y|x) can\nbe interpreted as the conditional probability distribution.\nProblem 1 (Joint Distribution Adaptation) Given la\u0002beled source domain Ds = {(x1, y1),...,(xns , yns )} and\nunlabeled target domain Dt = {xns+1,..., xns+nt } under\nthe assumptions that Xs = Xt, Ys = Yt, Ps(xs) \u0003= Pt(xt),\nQs(ys|xs) \u0003= Qt(yt|xt), learn a feature representation in\nTable 1. Notations and descriptions used in this paper.\nNotation Description Notation Description\nDs, Dt source/target domain X input data matrix\nns, nt #source/target examples A adaptation matrix\nm, C #shared features/classes Z embedding matrix\nk #subspace bases H centering matrix\n\u03bb regularization parameter Mc MMD matrices, c \u2208 {0,...,C}\nwhich the distribution differences between 1) Ps(xs) and\nPt(xt), 2) Qs(ys|xs) and Qt(yt|xt) are explicitly reduced.\n3.2. Proposed Approach\nIn this paper, we propose to adapt the joint distributions\nby a feature transformation T so that the joint expectations\nof the features x and labels y are matched between domains:\nmin\nT\n\u0002\n\u0002EP (xs,ys) [T (xs), ys] \u2212 EP (xt,yt) [T (xt), yt]\n\u0002\n\u0002\n2\n\u2248 \u0002\n\u0002EPs(xs) [T (xs)] \u2212 EPt(xt) [T (xt)]\n\u0002\n\u0002\n2\n+ \u0002\n\u0002EQs(ys|xs) [ys|T (xs)] \u2212 EQt(yt|xt) [yt|T (xt)]\n\u0002\n\u0002\n2\n(1)\nThe problem is nontrivial, since there are no labeled data in\nthe target domain, and Qt(yt|xt) cannot be estimated exact\u0002ly. The best approximation is to assume that Qt(yt|xt) \u2248\nQs(yt|xt) [1]. This can be executed by applying a classifier\nf trained on the labeled source data to the unlabeled target\ndata. In order to achieve a more accurate approximation for\nQt(yt|xt), we propose an iterative pseudo label refinement\nstrategy to iteratively refine the transformation T and clas\u0002sifier f. The proposed approach is technically detailed later.\n3.2.1 Feature Transformation\nDimensionality reduction methods can learn a transformed\nfeature representation by minimizing the reconstruction er\u0002ror of the input data. For simplicity and generality, we will\nchoose Principal Component Analysis (PCA) for data re\u0002construction. Denote X = [x1,..., xn] \u2208 Rm\u00d7n the input\ndata matrix, and H = I \u2212 1\nn 1 the centering matrix, where\nn = ns + nt and 1 the n \u00d7 n matrix of ones, then the co\u0002variance matrix can be computed as XHXT. The learning\ngoal of PCA is to find an orthogonal transformation matrix\nA \u2208 Rm\u00d7k such that embedded data variance is maximized\nmax\nATA=I\ntr \u0003ATXHXTA\u0004 (2)\nwhere tr(\u00b7) denotes the trace of a matrix. This optimization\nproblem can be efficiently solved by eigendecomposition\nXHXTA = A\u03a6, where \u03a6 = diag(\u03c61,...,\u03c6k) \u2208 Rk\u00d7k\nare the k largest eigenvalues. Then we find the optimal k\u0002dimensional representation by Z = [z1,..., zn] = ATX.\n3.2.2 Marginal Distribution Adaptation\nHowever, even through the PCA-induced k-dimensional\nrepresentation, the distribution difference between domains\n2201\nwill still be significantly large. Thus a major computation\u0002al issue is to reduce the distribution difference by explicitly\nminimizing proper distance measures. Since parametrically\nestimating the probability density for a distribution is often\na nontrivial problem, we resort to explore the sufficient s\u0002tatistics instead. To reduce the difference between marginal\ndistributions Ps(xs) and Pt(xt), we follow [8, 15, 23] and\nadopt the empirical M",
          "original_query": "Transfer feature learning with joint distribution adaptation",
          "cleaned_query": "Transfer feature learning with joint distribution adaptation",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Adaptive-kernel MMD for conditional shift using pseudo-label uncertainty\nBuild a variant of JDA where the MMD term for conditional alignment is weighted by target pseudo-label confidence (e.g., entropy or margin), reducing negative transfer from early incorrect pseudo-labels. Learn kernel hyperparameters (or MKL weights) jointly to maximize two-sample test power between class-conditionals while regularizing against overfitting.",
        "HSIC-regularized feature learning to enforce class-conditional invariance\nAdd an HSIC penalty between learned features and domain indicators *conditioned on class* (using source labels and target pseudo-labels) to explicitly suppress domain information while preserving label information. Implement as a differentiable, minibatch HSIC estimator plugged into JAN/JDA-style networks and evaluate whether it reduces conditional shift beyond JMMD alone.",
        "Graph two-sample testing for domain adaptation in relational data\nExtend Gretton et al.\u2019s distribution tests over graphs to a domain adaptation setting where source/target are graph populations (e.g., molecule graphs, social subgraphs). Use graph kernels to compute MMD/JMMD at multiple \u201clayers\u201d (node, ego-net, whole-graph) and adapt a GNN by minimizing these discrepancies while tracking which structural scales drive transfer.",
        "Power-aware joint alignment: directly optimizing two-sample test power during training\nReplace (or augment) fixed MMD/JMMD losses with a training objective that maximizes estimated test power under plausible alternatives (e.g., mean-shift or covariance-shift in RKHS), using large-deviation or asymptotic approximations from the kernel two-sample test. This yields an alignment criterion that is explicitly tuned for detectability of residual shift, potentially improving robustness when batch sizes are small.",
        "Linear-time streaming MMD/HSIC for continual domain shift detection and adaptation\nDevelop a streaming version of JAN/JDA that uses linear-time MMD (random features or block estimates) to detect distribution change online and trigger adaptation updates only when shifts are statistically significant. Pair this with HSIC-based dependence monitoring to determine whether shift is primarily marginal (feature drift) or conditional (label shift), selecting the appropriate adaptation module.",
        "Multi-source JDA with automatic source selection via two-sample tests\nFor multiple candidate source domains, compute kernel two-sample statistics between each source and the target (marginal and class-conditional where possible) to rank and weight sources. Train a unified adaptation model where each source\u2019s contribution is weighted by an exponentiated negative MMD/JMMD, giving an actionable pipeline for source selection and mixture transfer.",
        "Causal feature screening for transfer via domain\u2013feature HSIC and label\u2013feature HSIC\nUse paired HSIC criteria to select features (or learned units) that are strongly dependent on labels but weakly dependent on domain, aiming to approximate \u201cinvariant\u201d predictors. Implement as a differentiable gating layer whose parameters are trained to maximize HSIC(feature, label) \u2212 \u03bb\u00b7HSIC(feature, domain), and test whether it improves transfer in settings with strong spurious correlations.",
        "Calibrated, test-based early stopping and kernel choice for deep adaptation\nIntroduce a validation protocol that uses held-out batches to run a calibrated two-sample test (MMD) between source and target representations at each layer, selecting kernels and stopping training when further alignment becomes statistically insignificant or begins to harm class separation. This makes kernel selection and training duration data-driven rather than heuristic, and can be integrated with JAN\u2019s multi-layer alignment.",
        "Class-conditional two-sample tests for diagnosing negative transfer\nBuild a diagnostic toolkit that runs per-class MMD tests between source and target feature distributions (using pseudo-labels for target) to identify which classes are misaligned and likely to cause negative transfer. Use the diagnostics to drive targeted alignment (only for failing classes) or to downweight problematic classes during adaptation.",
        "Unified objective combining SVM-style margin preservation with joint distribution alignment\nCombine the \u201cLearning with Kernels\u201d large-margin principle with JDA/JMMD by jointly optimizing a classifier margin (e.g., SVM hinge loss in RKHS or a margin surrogate in deep nets) and a joint distribution discrepancy penalty. Concretely, enforce that alignment does not collapse discriminative structure by adding explicit margin constraints/regularizers while minimizing marginal + conditional MMD across domains."
      ],
      "generated_ideas_raw": "1. **Adaptive-kernel MMD for conditional shift using pseudo-label uncertainty**\n   Build a variant of JDA where the MMD term for conditional alignment is weighted by target pseudo-label confidence (e.g., entropy or margin), reducing negative transfer from early incorrect pseudo-labels. Learn kernel hyperparameters (or MKL weights) jointly to maximize two-sample test power between class-conditionals while regularizing against overfitting.\n\n2. **HSIC-regularized feature learning to enforce class-conditional invariance**\n   Add an HSIC penalty between learned features and domain indicators *conditioned on class* (using source labels and target pseudo-labels) to explicitly suppress domain information while preserving label information. Implement as a differentiable, minibatch HSIC estimator plugged into JAN/JDA-style networks and evaluate whether it reduces conditional shift beyond JMMD alone.\n\n3. **Graph two-sample testing for domain adaptation in relational data**\n   Extend Gretton et al.\u2019s distribution tests over graphs to a domain adaptation setting where source/target are graph populations (e.g., molecule graphs, social subgraphs). Use graph kernels to compute MMD/JMMD at multiple \u201clayers\u201d (node, ego-net, whole-graph) and adapt a GNN by minimizing these discrepancies while tracking which structural scales drive transfer.\n\n4. **Power-aware joint alignment: directly optimizing two-sample test power during training**\n   Replace (or augment) fixed MMD/JMMD losses with a training objective that maximizes estimated test power under plausible alternatives (e.g., mean-shift or covariance-shift in RKHS), using large-deviation or asymptotic approximations from the kernel two-sample test. This yields an alignment criterion that is explicitly tuned for detectability of residual shift, potentially improving robustness when batch sizes are small.\n\n5. **Linear-time streaming MMD/HSIC for continual domain shift detection and adaptation**\n   Develop a streaming version of JAN/JDA that uses linear-time MMD (random features or block estimates) to detect distribution change online and trigger adaptation updates only when shifts are statistically significant. Pair this with HSIC-based dependence monitoring to determine whether shift is primarily marginal (feature drift) or conditional (label shift), selecting the appropriate adaptation module.\n\n6. **Multi-source JDA with automatic source selection via two-sample tests**\n   For multiple candidate source domains, compute kernel two-sample statistics between each source and the target (marginal and class-conditional where possible) to rank and weight sources. Train a unified adaptation model where each source\u2019s contribution is weighted by an exponentiated negative MMD/JMMD, giving an actionable pipeline for source selection and mixture transfer.\n\n7. **Causal feature screening for transfer via domain\u2013feature HSIC and label\u2013feature HSIC**\n   Use paired HSIC criteria to select features (or learned units) that are strongly dependent on labels but weakly dependent on domain, aiming to approximate \u201cinvariant\u201d predictors. Implement as a differentiable gating layer whose parameters are trained to maximize HSIC(feature, label) \u2212 \u03bb\u00b7HSIC(feature, domain), and test whether it improves transfer in settings with strong spurious correlations.\n\n8. **Calibrated, test-based early stopping and kernel choice for deep adaptation**\n   Introduce a validation protocol that uses held-out batches to run a calibrated two-sample test (MMD) between source and target representations at each layer, selecting kernels and stopping training when further alignment becomes statistically insignificant or begins to harm class separation. This makes kernel selection and training duration data-driven rather than heuristic, and can be integrated with JAN\u2019s multi-layer alignment.\n\n9. **Class-conditional two-sample tests for diagnosing negative transfer**\n   Build a diagnostic toolkit that runs per-class MMD tests between source and target feature distributions (using pseudo-labels for target) to identify which classes are misaligned and likely to cause negative transfer. Use the diagnostics to drive targeted alignment (only for failing classes) or to downweight problematic classes during adaptation.\n\n10. **Unified objective combining SVM-style margin preservation with joint distribution alignment**\n   Combine the \u201cLearning with Kernels\u201d large-margin principle with JDA/JMMD by jointly optimizing a classifier margin (e.g., SVM hinge loss in RKHS or a margin surrogate in deep nets) and a joint distribution discrepancy penalty. Concretely, enforce that alignment does not collapse discriminative structure by adding explicit margin constraints/regularizers while minimizing marginal + conditional MMD across domains.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive-kernel MMD for conditional shift using pseudo-label uncertainty\nBuild a variant of JDA where the MMD term for conditional alignment is weighted by target pseudo-label confidence (e.g., entrop",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "HSIC-regularized feature learning to enforce class-conditional invariance\nAdd an HSIC penalty between learned features and domain indicators *conditioned on class* (using source labels and target pseu",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Graph two-sample testing for domain adaptation in relational data\nExtend Gretton et al.\u2019s distribution tests over graphs to a domain adaptation setting where source/target are graph populations (e.g.,",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Power-aware joint alignment: directly optimizing two-sample test power during training\nReplace (or augment) fixed MMD/JMMD losses with a training objective that maximizes estimated test power under pl",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Linear-time streaming MMD/HSIC for continual domain shift detection and adaptation\nDevelop a streaming version of JAN/JDA that uses linear-time MMD (random features or block estimates) to detect distr",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Multi-source JDA with automatic source selection via two-sample tests\nFor multiple candidate source domains, compute kernel two-sample statistics between each source and the target (marginal and class",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Causal feature screening for transfer via domain\u2013feature HSIC and label\u2013feature HSIC\nUse paired HSIC criteria to select features (or learned units) that are strongly dependent on labels but weakly dep",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Calibrated, test-based early stopping and kernel choice for deep adaptation\nIntroduce a validation protocol that uses held-out batches to run a calibrated two-sample test (MMD) between source and targ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Class-conditional two-sample tests for diagnosing negative transfer\nBuild a diagnostic toolkit that runs per-class MMD tests between source and target feature distributions (using pseudo-labels for ta",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Unified objective combining SVM-style margin preservation with joint distribution alignment\nCombine the \u201cLearning with Kernels\u201d large-margin principle with JDA/JMMD by jointly optimizing a classifier ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 37,
      "paper_title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables",
      "contribution": "Introduce a lightweight, learnable look-up-table (LUT) framework (PGLUT, SDLUT, AOLUT) that replaces heavy CNN components to perform high-quality, extremely fast pan-sharpening capable of processing very large remote-sensing images on commodity GPUs/CPUs.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "input_tokens": 7872,
      "output_tokens": 915,
      "predecessor_details": [
        {
          "success": true,
          "title": "Pansharpening by Convolutional Neural Networks in the Full Resolution Framework",
          "url": "https://ieeexplore.ieee.org/document/9745494",
          "content": "Pansharpening by Convolutional Neural Networks in the Full Resolution Framework | IEEE Journals &amp; Magazine | IEEE Xplore\n[**]()\n[](https://ieeexplore.ieee.org/rest/api/hpdata)\n### IEEE Account\n* [Change Username/Password]()\n* [Update Address]()\n### Purchase Details\n* [Payment Options]()\n* [Order History]()\n* [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n### Profile Information\n* [Communications Preferences]()\n* [Profession and Education]()\n* [Technical Interests]()\n### Need Help?\n* **US &amp; Canada:**+1 800 678 4333\n* **Worldwide:**+1 732 981 0060\n* [Contact &amp; Support](https://ieeexplore.ieee.org/xpl/contact)\n* [About IEEE*Xplore*](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n* [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n* [Help](https://ieeexplore.ieee.org/Xplorehelp)\n* [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n* [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n* [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n* [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n* [Privacy &amp; Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n&copy; Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.\n**",
          "original_query": "Pansharpening by convolutional neural networks",
          "cleaned_query": "Pansharpening by convolutional neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] PanNet: A deep network architecture for pan-sharpening - Xueyang Fu",
          "url": "https://xueyangfu.github.io/paper/2017/iccv/YangFuetal2017.pdf",
          "content": "PanNet: A deep network architecture for pan-sharpening\nJunfeng Yang\u2020, Xueyang Fu\u2020, Yuwen Hu, Yue Huang, Xinghao Ding\u2217, John Paisley\u2021\nFujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, China\n\u2021Department of Electrical Engineering, Columbia University, USA\nAbstract\nWe propose a deep network architecture for the pan\u0002sharpening problem called PanNet. We incorporate\ndomain-specific knowledge to design our PanNet architec\u0002ture by focusing on the two aims of the pan-sharpening\nproblem: spectral and spatial preservation. For spectral\npreservation, we add up-sampled multispectral images to\nthe network output, which directly propagates the spectral\ninformation to the reconstructed image. To preserve spatial\nstructure, we train our network parameters in the high-pass\nfiltering domain rather than the image domain. We show\nthat the trained network generalizes well to images from\ndifferent satellites without needing retraining. Experiments\nshow significant improvement over state-of-the-art methods\nvisually and in terms of standard quality metrics.\n1. Introduction\nMultispectral images are widely used, for example in\nagriculture, mining and environmental monitoring applica\u0002tions. Due to physical constraints, satellites will often only\nmeasure a high resolution panchromatic (PAN) image (i.e.,\ngrayscale) and several low resolution multispectral (LRMS)\nimages. The goal of pan-sharpening is to fuse this spectral\nand spatial information to produce a high resolution multi\u0002spectral (HRMS) image of the same size as PAN.\nWith recent advances made by deep neural networks for\nimage processing applications, researchers have begun ex\u0002ploring this avenue for pan-sharpening. For example, one\ndeep pan-sharpening model assumes that the relationship\nbetween HR/LR multispectral image patches is the same\n\u2020\nco-first authors contributed equally, \u2217correspondence: dxh@xmu.edu.cn.\nThis work was supported in part by the National Natural Science Foun\u0002dation of China grants 61571382, 81671766, 61571005, 81671674,\nU1605252, 61671309 and 81301278, Guangdong Natural Science Founda\u0002tion grant 2015A030313007, Fundamental Research Funds for the Central\nUniversities grants 20720160075 and 20720150169, the CCF-Tencent re\u0002search fund, and the Science and Technology funds from the Fujian Provin\u0002cial Administration of Surveying, Mapping and Geoinformation. Xueyang\nFu conducted portions of this work at Columbia University under China\nScholarship Council grant No. [2016]3100.\nFigure 1. Examples from 425 satellite images used in experiments.\nas that between the corresponding HR/LR panochromatic\nimage patches, and uses this assumption to learn a map\u0002ping through a neural network [16]. The state-of-the-art\npan-sharpening model, based on the convolutional neural\nnetwork and called PNN [21], adopts an architecture previ\u0002ously proposed for image super-resolution [11].\nThese two methods regard the pan-sharpening problem\nas a simple image regression problem. That is, though\nthey are able to obtain good results, they do not exploit the\nparticular goals of the pan-sharpening problem\u2014spectral\nand spatial preservation\u2014but rather treat pan-sharpening\nas a black-box deep learning problem. However, for pan\u0002sharpening it is clear that preserving spatial and spectral\ninformation are the primary goals of fusion, and so deep\nlearning methods should explicitly focus on these aspects.\nThis motivates our proposed deep network called \u201cPanNet,\u201d\nwhich has the following features:\n1. We incorporate problem-specific knowledge about\npan-sharpening into the deep learning framework. Specif\u0002ically, we propagate spectral information through the net\u0002work using up-sampled multispectral images, a procedure\nwe\u2019ll refer to as \u201cspectra-mapping.\u201d To focus on the spa\u0002tial structure in the PAN image, we train the network in the\nhigh-pass domain rather than the image domain.\n2. Our approach is an end-to-end system which auto\u0002matically learns the mapping purely from the data. Convo\u0002lutions allow us to capture intra-correlation across different\nbands of the MS images and the PAN image, unlike pre\u0002vious (non-deep) methods. Experiments show that PanNet\nachieves state-of-the-art performance compared with sev\u00025449\nFigure 2. The deep neural network structure of the proposed pan-sharpening framework called PanNet.\neral standard approaches, as well as other deep models.\n3. Most conventional methods require parameter tuning\nfor different satellites because the range of imaging values\nare inconsistent. However, training in the high-pass domain\nremoves this factor, allowing for training on one satellite to\ngeneralize well to new satellites. This is not a feature of\nother deep approaches, which train on the image domain.\n1.1. Related work\nVarious pan-sharpening methods have emerged in recent\ndecades. Among these, the most popular are based on com\u0002ponent substitution, including the intensity hue-saturation\ntechnique (IHS) [5], principal component analysis (PCA)\n[20] and the Brovey transform [14]. These methods are\nstraightforward and fast, but they tend to succeed in approx\u0002imating the spatial resolution of the HRMS image contained\nin PAN at the expense of introducing spectral distortions. To\nfix this problem, more complex techniques have been pro\u0002posed, such as adaptive approaches (e.g., PRACS [8]) and\nband-dependent approaches (e.g., BDSD [13]). In multi\u0002resolution approaches [19, 22], the PAN image and LRMS\nimages are decomposed, e.g. using wavelets or Laplacian\npyramids, and then fused. Other model-based methods en\u0002code beliefs about the relationships between PAN, HRMS\nand LRMS images in a regularized objective function, and\nthen treat the fusion problem as an image restoration opti\u0002mization problem [3, 4, 7, 9, 12, 18]. Many of these algo\u0002rithms obtain excellent results. We choose the best among\nthese methods for comparison in our experiments.\n2. PanNet: A deep network for pan-sharpening\nFigure 2 shows a high-level outline of our proposed\ndeep learning approach to pan-sharpening called PanNet.\nWe motivate this structure by first reviewing common ap\u0002proaches to the pan-sharpening problem, and then dis\u0002cuss our approach in the context of the two goals of pan\u0002sharpening, which is to reconstruct high-resolution multi\u0002spectral images that contain the spatial content of PAN and\nthe spectral content of the low-resolution images.\n2.1. Background and motivation\nWe denote the set of desired HRMS images as X and let\nXb be the image of the bth band. For the observed data, P\ndenotes the PAN image and M denotes the LRMS images,\nwith Mb the bth band. Most state-of-the-art methods treat\nfusion as minimizing an objective of the form\nL = \u03bb1f1(X, P) + \u03bb2f2(X, M) + f3(X), (1)\nwhere the term f1(X, P) enforces structural consistency,\nf2(X, M) enforces spectral consistency, and f3(X) imposes\ndesired image constraints on X. For example the first varia\u0002tional method P+XS [4] lets\nf1(X, P) = k\nPB\nb=1 \u03c9bXb \u2212 Pk\n2\n2\n(2)\nwith \u03c9 a B-dimensional probability weight vector. Other\napproaches use a spatial difference operator G to focus on\nhigh-frequency content, for example kG(\nP\nb \u03c9bXb \u2212 P)k\n2\n2\nor P\nb \u03c9bkG(Xb \u2212 P)k\n2\n2\n[3, 9, 12]. A similar structural\npenalty uses a hyper-Laplacian kG(\nP\nb \u03c9bXb\u2212P)k1/2 [18].\nFor spectral consistency, many methods define\nf2(X, M) = PB\nb=1 kk \u2217 Xb\u2212 \u2191Mbk\n2\n2\n, (3)\nwhere \u2191 Mb indicates upsampling Mb to be the same size\nas Xb, which is smoothed by convolving with smoothing\nkernel k [3,4,12,18]. The term f3(X) is often total variation\npenalization.\nA straightforward deep learning approach to the pan\u0002sharpening problem can leverage a plain network architec\u0002ture to learn a nonlinear mapping relationship between the\ninputs (P, M) and the outputs X that minimizes\nL = kfW(P, M) \u2212 Xk\n2\nF . (4)\nHere, fW represents a neural network and W its parameters.\nThis idea is used by PNN [21], which directly inputs (P, M)\ninto a deep convolutional neural network to approximate X.\nAlthough this direct architecture gives excellent results, it\ndoes not exploit known image characteristics to define the\ninputs or network structure.\n5450\nFigure 3. Example of the three model structures we considered for pan-sharpening: (left-to-right) ResNet [15], ResNet+spectra-mapping,\nand the final proposed network, called PanNet. ResNet has been shown to improve CNN performance on image processing tasks, but\nhas drawbacks in the pan-sharpening framework. The second network captures the goal of spectral preservation, while the final proposed\nnetwork captures both spatial and spectral information. We experiment with all three, none of which have been applied to pan-sharpening.\n2.2. PanNet architecture\nWe are motivated to build on the success of PNN in\ndefining PanNet. As with PNN, we also use a convolu\u0002tion neural network (CNN), but our specific structure dif\u0002fers from PNN in using the recently proposed ResNet struc\u0002ture [15] as our neural network. Convolutional filters are\nparticularly useful for this problem, since they can exploit\nthe high correlation across different bands of the multispec\u0002tral images, something shown to be useful by the SIRF al\u0002gorithm [7]. As with other pan-sharpening approaches, our\ndeep network aims to preserve both spectral and spatial in\u0002formation. We discuss these separately below.\nThe high-level idea is represented in the sequence of\npotential network structures shown in Figure 3. The first\nis vanilla ResNet, while the second network only focuses\non spectral information. We propose the third network\ncalled PanNet, which performs the best. We experiment\nwith all three, none of which have been applied to the pan\u0002sharpening problem.\n2.2.1 Spectral preservation\nTo fuse spectral information, we up-sample M and add a\nskip connection to the deep network of the form\nLb = kfW(P, M) + \u2191M \u2212 Xk\n2\nF\n. (5)\n\u2191M represents the up-sampled LRMS image and fW repre\u0002sents ResNet, discussed later. This term is motivated by the\nsame goal as represented in Equation (3). As we will see,\nit enforces that X shares the spectral co",
          "original_query": "Pannet: A deep network architecture for pan-sharpening",
          "cleaned_query": "Pannet: A deep network architecture for pan-sharpening",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Learning Image-Adaptive 3D Lookup Tables for High Performance Photo Enhancement in Real-Time",
          "url": "https://ieeexplore.ieee.org/document/9206076/",
          "content": "\n \n \n \n \n \n IEEE Account \n \n Change Username/Password \n Update Address \n \n \n \n Purchase Details \n \n Payment Options \n Order History \n View Purchased Documents \n \n \n \n Profile Information \n \n Communications Preferences \n Profession and Education \n Technical Interests \n \n \n \n Need Help? \n \n US &amp; Canada: +1 800 678 4333 \n Worldwide: +1 732 981 0060 \n \n Contact &amp; Support \n \n \n \n \n \n About IEEE Xplore \n Contact Us \n Help \n Accessibility \n Terms of Use \n Nondiscrimination Policy \n Sitemap \n Privacy &amp; Opting Out of Cookies \n \n \nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. \u00a9 Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.\n \n \n \n",
          "original_query": "Learning image-adaptive 3d lookup tables for high performance photo enhancement in real-time",
          "cleaned_query": "Learning image-adaptive 3d lookup tables for high performance photo enhancement in real-time",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Real-time Image Enhancer via Learnable Spatial-aware ...",
          "url": "https://arxiv.org/abs/2108.08697",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2108.08697** (cs)\n\n\\[Submitted on 19 Aug 2021\\]\n\n# Title:Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables\n\nAuthors: [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+T), [Yong Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+Y), [Jingyang Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng,+J), [Yipeng Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+Y), [Xian Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+X), [Fenglong Song](https://arxiv.org/search/cs?searchtype=author&query=Song,+F), [Youliang Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan,+Y)\n\nView a PDF of the paper titled Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables, by Tao Wang and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2108.08697)\n\n> Abstract:Recently, deep learning-based image enhancement algorithms achieved state-of-the-art (SOTA) performance on several publicly available datasets. However, most existing methods fail to meet practical requirements either for visual perception or for computation efficiency, especially for high-resolution images. In this paper, we propose a novel real-time image enhancer via learnable spatial-aware 3-dimentional lookup tables(3D LUTs), which well considers global scenario and local spatial information. Specifically, we introduce a light weight two-head weight predictor that has two outputs. One is a 1D weight vector used for image-level scenario adaptation, the other is a 3D weight map aimed for pixel-wise category fusion. We learn the spatial-aware 3D LUTs and fuse them according to the aforementioned weights in an end-to-end manner. The fused LUT is then used to transform the source image into the target tone in an efficient way. Extensive results show that our model outperforms SOTA image enhancement methods on public datasets both subjectively and objectively, and that our model only takes about 4ms to process a 4K resolution image on one NVIDIA V100 GPU.\n\n| | |\n| --- | --- |\n| Comments: | Accepted to ICCV2021 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV) |\n| Cite as: | [arXiv:2108.08697](https://arxiv.org/abs/2108.08697) \\[cs.CV\\] |\n| | (or [arXiv:2108.08697v1](https://arxiv.org/abs/2108.08697v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2108.08697](https://doi.org/10.48550/arXiv.2108.08697) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Tao Wang \\[ [view email](https://arxiv.org/show-email/e9720b1c/2108.08697)\\]\n\n**\\[v1\\]**\nThu, 19 Aug 2021 14:04:59 UTC (7,777 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables, by Tao Wang and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2108.08697)\n- [TeX Source](https://arxiv.org/src/2108.08697)\n- [Other Formats](https://arxiv.org/format/2108.08697)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2108.08697&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2108.08697&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2021-08](https://arxiv.org/list/cs.CV/2021-08)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2108.08697?context=cs)\n\n[eess](https://arxiv.org/abs/2108.08697?context=eess)\n\n[eess.IV](https://arxiv.org/abs/2108.08697?context=eess.IV)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2108.08697)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2108.08697)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2108.08697)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2108.html#abs-2108-08697) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2108-08697)\n\n[Tao Wang](https://dblp.uni-trier.de/search/author?author=Tao%20Wang)\n\n[Yong Li](https://dblp.uni-trier.de/search/author?author=Yong%20Li)\n\n[Yipeng Ma](https://dblp.uni-trier.de/search/author?author=Yipeng%20Ma)\n\n[Xian Wang](https://dblp.uni-trier.de/search/author?author=Xian%20Wang)\n\n[Youliang Yan](https://dblp.uni-trier.de/search/author?author=Youliang%20Yan)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2108.08697&description=Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2108.08697&title=Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2108.08697) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Real-time image enhancer via learnable spatial-aware 3d lookup tables",
          "cleaned_query": "Real-time image enhancer via learnable spatial-aware 3d lookup tables",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution",
          "url": "https://arxiv.org/abs/2507.09923",
          "content": "[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)\n\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n# Electrical Engineering and Systems Science > Image and Video Processing\n\n**arXiv:2507.09923** (eess)\n\n\\[Submitted on 14 Jul 2025 ( [v1](https://arxiv.org/abs/2507.09923v1)), last revised 28 Jul 2025 (this version, v3)\\]\n\n# Title:IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution\n\nAuthors: [Sejin Park](https://arxiv.org/search/eess?searchtype=author&query=Park,+S), [Sangmin Lee](https://arxiv.org/search/eess?searchtype=author&query=Lee,+S), [Kyong Hwan Jin](https://arxiv.org/search/eess?searchtype=author&query=Jin,+K+H), [Seung-Won Jung](https://arxiv.org/search/eess?searchtype=author&query=Jung,+S)\n\nView a PDF of the paper titled IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution, by Sejin Park and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2507.09923)\n\n> Abstract:Super-resolution (SR) has been a pivotal task in image processing, aimed at enhancing image resolution across various applications. Recently, look-up table (LUT)-based approaches have attracted interest due to their efficiency and performance. However, these methods are typically designed for fixed scale factors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing ASISR techniques often employ implicit neural representations, which come with considerable computational cost and memory demands. To address these limitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework that operates ASISR by learning to blend multiple interpolation functions to maximize their representational capacity. Specifically, we introduce IM-Net, a network trained to predict mixing weights for interpolation functions based on local image patterns and the target scale factor. To enhance efficiency of interpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are employed to replace computationally expensive operations, enabling lightweight and fast inference on CPUs while preserving reconstruction quality. Experimental results on several benchmark datasets demonstrate that IM-LUT consistently achieves a superior balance between image quality and efficiency compared to existing methods, highlighting its potential as a promising solution for resource-constrained applications.\n\n| | |\n| --- | --- |\n| Comments: | ICCV 2025 |\n| Subjects: | Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2507.09923](https://arxiv.org/abs/2507.09923) \\[eess.IV\\] |\n| (or [arXiv:2507.09923v3](https://arxiv.org/abs/2507.09923v3) \\[eess.IV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.09923](https://doi.org/10.48550/arXiv.2507.09923) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Sejin Park \\[ [view email](https://arxiv.org/show-email/9a1ae4bd/2507.09923)\\] **[\\[v1\\]](https://arxiv.org/abs/2507.09923v1)**\nMon, 14 Jul 2025 05:02:57 UTC (24,652 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2507.09923v2)**\nTue, 15 Jul 2025 10:04:02 UTC (24,652 KB)\n**\\[v3\\]**\nMon, 28 Jul 2025 06:48:28 UTC (24,652 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution, by Sejin Park and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2507.09923)\n- [TeX Source](https://arxiv.org/src/2507.09923)\n- [Other Formats](https://arxiv.org/format/2507.09923)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\neess.IV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2507.09923&function=prev&context=eess.IV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2507.09923&function=next&context=eess.IV)\n\n[new](https://arxiv.org/list/eess.IV/new) \\| [recent](https://arxiv.org/list/eess.IV/recent) \\| [2025-07](https://arxiv.org/list/eess.IV/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.09923?context=cs) [cs.CV](https://arxiv.org/abs/2507.09923?context=cs.CV) [eess](https://arxiv.org/abs/2507.09923?context=eess)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.09923)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.09923)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.09923)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2507.09923&description=IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2507.09923&title=IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.09923) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Practical single-image super-resolution using look-up table",
          "cleaned_query": "Practical single-image super-resolution using look-up table",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Arbitrary-Scale Pansharpening via IM-LUT-Style Interpolation Mixing\nExtend IM-LUT to pansharpening by learning scale-conditioned mixing weights that fuse multiple interpolation kernels for LRMS upsampling and detail injection from PAN. This yields a single model that supports variable ground sampling distance ratios (non-integer and sensor-dependent) without retraining, while maintaining spectral fidelity.",
        "Cross-Sensor Generalization with High-Pass Domain + LUT Domain Adaptation\nBuild a two-stage pansharpening pipeline where a PanNet-like high-pass network predicts spatial detail and a lightweight LUT corrects sensor-specific radiometric response. Learn the LUT using a small calibration set (or self-supervised statistics matching) to adapt across satellites while keeping the core network frozen.",
        "Spatial-Aware Bandwise LUTs for Multispectral Spectral Preservation\nGeneralize spatial-aware LUT fusion to multispectral outputs by learning per-band (or grouped-band) LUT banks plus a pixel-wise weight map conditioned on PAN edges and local MS texture. Enforce cross-band consistency constraints (e.g., preserving band ratios/angles) so that spatial enhancement does not introduce band-dependent color shifts.",
        "Uncertainty-Aware Pansharpening with Mixture-of-LUTs and Confidence Maps\nUse multiple specialized LUTs (e.g., vegetation, water, urban) and predict both mixing weights and a confidence/uncertainty map indicating where fusion is reliable. Train with a heteroscedastic loss in the high-pass domain and validate that uncertainty correlates with spectral distortion metrics and downstream task failure modes.",
        "Physics-Constrained \u201cSpectral Response LUT\u201d for PAN-to-MS Detail Transfer\nIncorporate known/estimated sensor spectral response functions by constraining a LUT that maps PAN intensities into bandwise detail coefficients before injection into MS. Optimize jointly with a CNN that predicts residual corrections, explicitly reducing spectral distortion caused by PAN/MS spectral mismatch across sensors.",
        "Temporal-Consistent Video/Time-Series Pansharpening with Recurrent Weight Predictors\nExtend LUT weight prediction to exploit temporal context (previous frames or earlier acquisitions) using a lightweight recurrent module that stabilizes mixing weights over time. Add a temporal high-pass consistency loss to reduce flicker and ensure stable spatial detail injection for monitoring applications.",
        "Task-Driven Pansharpening: Optimize Fusion for Segmentation/Detection with LUT Efficiency\nTrain pansharpening to improve downstream remote-sensing tasks by combining reconstruction losses (spectral + spatial, high-pass) with task losses from a frozen or jointly trained segmentation/detector. Implement the final fusion stage as LUT-based for real-time inference, and quantify gains in mIoU/AP versus purely perceptual/PSNR-optimized fusion.",
        "Learned Degradation-to-Fusion Loop: Self-Supervised Pansharpening with LUT-Regularized Inversion\nDevelop a self-supervised training scheme that alternates between a learned degradation model (HRMS \u2192 PAN + LRMS) and an inverse pansharpening model constrained by LUT smoothness/monotonicity regularizers. This reduces reliance on perfectly paired HRMS ground truth and enables training on large unlabeled archives while maintaining controllable, efficient inference."
      ],
      "generated_ideas_raw": "1. **PanLUT: LUT-Parameterized Pansharpening for Real-Time Onboard Fusion**  \n   Replace (part of) the pansharpening CNN mapping with a learnable, image-adaptive 3D LUT module that injects PAN-guided spatial detail into upsampled MS while preserving spectra (as in PanNet\u2019s spectra-mapping). Train end-to-end with a high-pass-domain loss and evaluate latency/energy on CPU/edge GPUs for onboard satellite processing.\n\n2. **Arbitrary-Scale Pansharpening via IM-LUT-Style Interpolation Mixing**  \n   Extend IM-LUT to pansharpening by learning scale-conditioned mixing weights that fuse multiple interpolation kernels for LRMS upsampling and detail injection from PAN. This yields a single model that supports variable ground sampling distance ratios (non-integer and sensor-dependent) without retraining, while maintaining spectral fidelity.\n\n3. **Cross-Sensor Generalization with High-Pass Domain + LUT Domain Adaptation**  \n   Build a two-stage pansharpening pipeline where a PanNet-like high-pass network predicts spatial detail and a lightweight LUT corrects sensor-specific radiometric response. Learn the LUT using a small calibration set (or self-supervised statistics matching) to adapt across satellites while keeping the core network frozen.\n\n4. **Spatial-Aware Bandwise LUTs for Multispectral Spectral Preservation**  \n   Generalize spatial-aware LUT fusion to multispectral outputs by learning per-band (or grouped-band) LUT banks plus a pixel-wise weight map conditioned on PAN edges and local MS texture. Enforce cross-band consistency constraints (e.g., preserving band ratios/angles) so that spatial enhancement does not introduce band-dependent color shifts.\n\n5. **Uncertainty-Aware Pansharpening with Mixture-of-LUTs and Confidence Maps**  \n   Use multiple specialized LUTs (e.g., vegetation, water, urban) and predict both mixing weights and a confidence/uncertainty map indicating where fusion is reliable. Train with a heteroscedastic loss in the high-pass domain and validate that uncertainty correlates with spectral distortion metrics and downstream task failure modes.\n\n6. **Physics-Constrained \u201cSpectral Response LUT\u201d for PAN-to-MS Detail Transfer**  \n   Incorporate known/estimated sensor spectral response functions by constraining a LUT that maps PAN intensities into bandwise detail coefficients before injection into MS. Optimize jointly with a CNN that predicts residual corrections, explicitly reducing spectral distortion caused by PAN/MS spectral mismatch across sensors.\n\n7. **Joint Pansharpening + Tone/Color Harmonization Using Cascaded LUTs**  \n   Create a cascaded system: (i) a pansharpening backbone (full-resolution framework or PanNet) followed by (ii) a learnable spatial-aware 3D LUT that harmonizes tone/color to a target style or reference acquisition. This targets practical pipelines where fused products must match consistent visual/radiometric appearance across time or satellites.\n\n8. **Temporal-Consistent Video/Time-Series Pansharpening with Recurrent Weight Predictors**  \n   Extend LUT weight prediction to exploit temporal context (previous frames or earlier acquisitions) using a lightweight recurrent module that stabilizes mixing weights over time. Add a temporal high-pass consistency loss to reduce flicker and ensure stable spatial detail injection for monitoring applications.\n\n9. **Task-Driven Pansharpening: Optimize Fusion for Segmentation/Detection with LUT Efficiency**  \n   Train pansharpening to improve downstream remote-sensing tasks by combining reconstruction losses (spectral + spatial, high-pass) with task losses from a frozen or jointly trained segmentation/detector. Implement the final fusion stage as LUT-based for real-time inference, and quantify gains in mIoU/AP versus purely perceptual/PSNR-optimized fusion.\n\n10. **Learned Degradation-to-Fusion Loop: Self-Supervised Pansharpening with LUT-Regularized Inversion**  \n   Develop a self-supervised training scheme that alternates between a learned degradation model (HRMS \u2192 PAN + LRMS) and an inverse pansharpening model constrained by LUT smoothness/monotonicity regularizers. This reduces reliance on perfectly paired HRMS ground truth and enables training on large unlabeled archives while maintaining controllable, efficient inference.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Arbitrary-Scale Pansharpening via IM-LUT-Style Interpolation Mixing\nExtend IM-LUT to pansharpening by learning scale-conditioned mixing weights that fuse multiple interpolation kernels for LRMS upsamp",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Cross-Sensor Generalization with High-Pass Domain + LUT Domain Adaptation\nBuild a two-stage pansharpening pipeline where a PanNet-like high-pass network predicts spatial detail and a lightweight LUT c",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Spatial-Aware Bandwise LUTs for Multispectral Spectral Preservation\nGeneralize spatial-aware LUT fusion to multispectral outputs by learning per-band (or grouped-band) LUT banks plus a pixel-wise weig",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Uncertainty-Aware Pansharpening with Mixture-of-LUTs and Confidence Maps\nUse multiple specialized LUTs (e.g., vegetation, water, urban) and predict both mixing weights and a confidence/uncertainty map",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Physics-Constrained \u201cSpectral Response LUT\u201d for PAN-to-MS Detail Transfer\nIncorporate known/estimated sensor spectral response functions by constraining a LUT that maps PAN intensities into bandwise d",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Temporal-Consistent Video/Time-Series Pansharpening with Recurrent Weight Predictors\nExtend LUT weight prediction to exploit temporal context (previous frames or earlier acquisitions) using a lightwei",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Task-Driven Pansharpening: Optimize Fusion for Segmentation/Detection with LUT Efficiency\nTrain pansharpening to improve downstream remote-sensing tasks by combining reconstruction losses (spectral + ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Learned Degradation-to-Fusion Loop: Self-Supervised Pansharpening with LUT-Regularized Inversion\nDevelop a self-supervised training scheme that alternates between a learned degradation model (HRMS \u2192 P",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 38,
      "paper_title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
      "contribution": "Using dynamical mean field theory the authors show that, in the joint large-width and large-sample regime, training dynamics exhibits a separation of timescales that (i) produces slow growth of function complexity, (ii) yields an inductive bias toward low-complexity solutions determined by initialization, and (iii) dynamically decouples feature learning from overfitting \u2014 predicting nonmonotone test error and a late-time 'feature unlearning' regime.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13688,
      "output_tokens": 1032,
      "predecessor_details": [
        {
          "success": true,
          "title": "A Mean Field View of the Landscape of Two-Layers Neural Networks",
          "url": "https://arxiv.org/abs/1804.06561",
          "content": "# Statistics > Machine Learning\n\n**arXiv:1804.06561** (stat)\n\n\\[Submitted on 18 Apr 2018 ( [v1](https://arxiv.org/abs/1804.06561v1)), last revised 28 Aug 2018 (this version, v2)\\]\n\n# Title:A Mean Field View of the Landscape of Two-Layers Neural Networks\n\nAuthors: [Song Mei](https://arxiv.org/search/stat?searchtype=author&query=Mei,+S), [Andrea Montanari](https://arxiv.org/search/stat?searchtype=author&query=Montanari,+A), [Phan-Minh Nguyen](https://arxiv.org/search/stat?searchtype=author&query=Nguyen,+P)\n\nView a PDF of the paper titled A Mean Field View of the Landscape of Two-Layers Neural Networks, by Song Mei and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/1804.06561)\n\n> Abstract:Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties?\n>\n> In this paper we consider a simple case, namely two-layers neural networks, and prove that -in a suitable scaling limit- SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows to 'average-out' some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.\n\n| | |\n| --- | --- |\n| Comments: | 103 pages |\n| Subjects: | Machine Learning (stat.ML); Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG); Statistics Theory (math.ST) |\n| Cite as: | [arXiv:1804.06561](https://arxiv.org/abs/1804.06561) \\[stat.ML\\] |\n| | (or [arXiv:1804.06561v2](https://arxiv.org/abs/1804.06561v2) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1804.06561](https://doi.org/10.48550/arXiv.1804.06561) Focus to learn more arXiv-issued DOI via DataCite |\n| Related DOI: | [https://doi.org/10.1073/pnas.1806579115](https://doi.org/10.1073/pnas.1806579115) Focus to learn more DOI(s) linking to related resources |\n\n## Submission history\n\nFrom: Song Mei \\[ [view email](https://arxiv.org/show-email/6fcaee66/1804.06561)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1804.06561v1)**\nWed, 18 Apr 2018 05:31:45 UTC (1,533 KB)\n\n**\\[v2\\]**\nTue, 28 Aug 2018 06:21:23 UTC (1,535 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled A Mean Field View of the Landscape of Two-Layers Neural Networks, by Song Mei and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/1804.06561)\n- [TeX Source](https://arxiv.org/src/1804.06561)\n- [Other Formats](https://arxiv.org/format/1804.06561)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1804.06561&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1804.06561&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2018-04](https://arxiv.org/list/stat.ML/2018-04)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/1804.06561?context=cond-mat)\n\n[cond-mat.stat-mech](https://arxiv.org/abs/1804.06561?context=cond-mat.stat-mech)\n\n[cs](https://arxiv.org/abs/1804.06561?context=cs)\n\n[cs.LG](https://arxiv.org/abs/1804.06561?context=cs.LG)\n\n[math](https://arxiv.org/abs/1804.06561?context=math)\n\n[math.ST](https://arxiv.org/abs/1804.06561?context=math.ST)\n\n[stat](https://arxiv.org/abs/1804.06561?context=stat)\n\n[stat.TH](https://arxiv.org/abs/1804.06561?context=stat.TH)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1804.06561)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1804.06561)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1804.06561)\n\n### [1 blog link](https://arxiv.org/tb/1804.06561)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1804.06561&description=A Mean Field View of the Landscape of Two-Layers Neural Networks) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1804.06561&title=A Mean Field View of the Landscape of Two-Layers Neural Networks)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1804.06561) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "A mean field view of the landscape of two-layer neural networks",
          "cleaned_query": "A mean field view of the landscape of two-layer neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Neural Tangent Kernel: Convergence and Generalization in ... - arXiv",
          "url": "https://arxiv.org/abs/1806.07572",
          "content": "[1806.07572] Neural Tangent Kernel: Convergence and Generalization in Neural Networks[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1806.07572\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1806.07572**(cs)\n[Submitted on 20 Jun 2018 ([v1](https://arxiv.org/abs/1806.07572v1)), last revised 10 Feb 2020 (this version, v4)]\n# Title:Neural Tangent Kernel: Convergence and Generalization in Neural Networks\nAuthors:[Arthur Jacot](https://arxiv.org/search/cs?searchtype=author&amp;query=Jacot,+A),[Franck Gabriel](https://arxiv.org/search/cs?searchtype=author&amp;query=Gabriel,+F),[Cl\u00e9ment Hongler](https://arxiv.org/search/cs?searchtype=author&amp;query=Hongler,+C)\nView a PDF of the paper titled Neural Tangent Kernel: Convergence and Generalization in Neural Networks, by Arthur Jacot and 2 other authors\n[View PDF](https://arxiv.org/pdf/1806.07572)> > Abstract:\n> At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function $f_\\theta$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function $f_\\theta$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit. Subjects:|Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Probability (math.PR); Machine Learning (stat.ML)|\nCite as:|[arXiv:1806.07572](https://arxiv.org/abs/1806.07572)[cs.LG]|\n|(or[arXiv:1806.07572v4](https://arxiv.org/abs/1806.07572v4)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1806.07572](https://doi.org/10.48550/arXiv.1806.07572)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|In Advances in neural information processing systems (pp. 8571-8580) 2018|\n## Submission history\nFrom: Arthur Jacot [[view email](https://arxiv.org/show-email/cfefbf84/1806.07572)]\n**[[v1]](https://arxiv.org/abs/1806.07572v1)**Wed, 20 Jun 2018 06:35:46 UTC (211 KB)\n**[[v2]](https://arxiv.org/abs/1806.07572v2)**Mon, 12 Nov 2018 10:31:42 UTC (125 KB)\n**[[v3]](https://arxiv.org/abs/1806.07572v3)**Mon, 26 Nov 2018 15:42:05 UTC (127 KB)\n**[v4]**Mon, 10 Feb 2020 08:39:09 UTC (128 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Neural Tangent Kernel: Convergence and Generalization in Neural Networks, by Arthur Jacot and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1806.07572)\n* [TeX Source](https://arxiv.org/src/1806.07572)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1806.07572&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1806.07572&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2018-06](https://arxiv.org/list/cs.LG/2018-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1806.07572?context=cs)\n[cs.NE](https://arxiv.org/abs/1806.07572?context=cs.NE)\n[math](https://arxiv.org/abs/1806.07572?context=math)\n[math.PR](https://arxiv.org/abs/1806.07572?context=math.PR)\n[stat](https://arxiv.org/abs/1806.07572?context=stat)\n[stat.ML](https://arxiv.org/abs/1806.07572?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1806.07572)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1806.07572)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1806.07572)\n### [2 blog links](https://arxiv.org/tb/1806.07572)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1806.html#abs-1806-07572)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1806-07572)\n[Arthur Jacot]()\n[Franck Gabriel]()\n[Cl\u00e9ment Hongler]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1806.07572)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Neural tangent kernel: Convergence and generalization in neural networks",
          "cleaned_query": "Neural tangent kernel: Convergence and generalization in neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[1812.07956] On Lazy Training in Differentiable Programming - arXiv",
          "url": "https://arxiv.org/abs/1812.07956",
          "content": "# Mathematics > Optimization and Control\n\n**arXiv:1812.07956** (math)\n\n\\[Submitted on 19 Dec 2018 ( [v1](https://arxiv.org/abs/1812.07956v1)), last revised 7 Jan 2020 (this version, v5)\\]\n\n# Title:On Lazy Training in Differentiable Programming\n\nAuthors: [Lenaic Chizat](https://arxiv.org/search/math?searchtype=author&query=Chizat,+L) (CNRS, UP11), [Edouard Oyallon](https://arxiv.org/search/math?searchtype=author&query=Oyallon,+E), [Francis Bach](https://arxiv.org/search/math?searchtype=author&query=Bach,+F) (LIENS, SIERRA)\n\nView a PDF of the paper titled On Lazy Training in Differentiable Programming, by Lenaic Chizat (CNRS and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/1812.07956)\n\n> Abstract:In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this \"lazy training\" phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that \"lazy training\" is behind the many successes of neural networks in difficult high dimensional tasks.\n\n| | |\n| --- | --- |\n| Subjects: | Optimization and Control (math.OC); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1812.07956](https://arxiv.org/abs/1812.07956) \\[math.OC\\] |\n| | (or [arXiv:1812.07956v5](https://arxiv.org/abs/1812.07956v5) \\[math.OC\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1812.07956](https://doi.org/10.48550/arXiv.1812.07956) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Advances in Neural Information Processing Systems (NeurIPS), Dec 2019, Vancouver, Canada |\n\n## Submission history\n\nFrom: Lenaic Chizat \\[ [view email](https://arxiv.org/show-email/88471c15/1812.07956)\\]\u00a0\\[via CCSD proxy\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1812.07956v1)**\nWed, 19 Dec 2018 14:11:20 UTC (312 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1812.07956v2)**\nThu, 21 Feb 2019 10:33:25 UTC (312 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/1812.07956v3)**\nMon, 17 Jun 2019 14:39:39 UTC (360 KB)\n\n**[\\[v4\\]](https://arxiv.org/abs/1812.07956v4)**\nTue, 18 Jun 2019 14:36:18 UTC (355 KB)\n\n**\\[v5\\]**\nTue, 7 Jan 2020 16:11:56 UTC (357 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled On Lazy Training in Differentiable Programming, by Lenaic Chizat (CNRS and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/1812.07956)\n- [TeX Source](https://arxiv.org/src/1812.07956)\n- [Other Formats](https://arxiv.org/format/1812.07956)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.OC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1812.07956&function=prev&context=math.OC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1812.07956&function=next&context=math.OC)\n\n[new](https://arxiv.org/list/math.OC/new) \\| [recent](https://arxiv.org/list/math.OC/recent) \\| [2018-12](https://arxiv.org/list/math.OC/2018-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1812.07956?context=cs)\n\n[cs.LG](https://arxiv.org/abs/1812.07956?context=cs.LG)\n\n[math](https://arxiv.org/abs/1812.07956?context=math)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1812.07956)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1812.07956)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1812.07956)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1812.07956&description=On Lazy Training in Differentiable Programming) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1812.07956&title=On Lazy Training in Differentiable Programming)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1812.07956) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "On lazy training in differentiable programming",
          "cleaned_query": "On lazy training in differentiable programming",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "The high-dimensional asymptotics of first order methods ...",
          "url": "https://arxiv.org/abs/2112.07572",
          "content": "# Mathematics > Probability\n\n**arXiv:2112.07572** (math)\n\n\\[Submitted on 14 Dec 2021\\]\n\n# Title:The high-dimensional asymptotics of first order methods with random data\n\nAuthors: [Michael Celentano](https://arxiv.org/search/math?searchtype=author&query=Celentano,+M), [Chen Cheng](https://arxiv.org/search/math?searchtype=author&query=Cheng,+C), [Andrea Montanari](https://arxiv.org/search/math?searchtype=author&query=Montanari,+A)\n\nView a PDF of the paper titled The high-dimensional asymptotics of first order methods with random data, by Michael Celentano and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2112.07572)\n\n> Abstract:We study a class of deterministic flows in ${\\\\mathbb R}^{d\\\\times k}$, parametrized by a random matrix ${\\\\boldsymbol X}\\\\in {\\\\mathbb R}^{n\\\\times d}$ with i.i.d. centered subgaussian entries. We characterize the asymptotic behavior of these flows over bounded time horizons, in the high-dimensional limit in which $n,d\\\\to\\\\infty$ with $k$ fixed and converging aspect ratios $n/d\\\\to\\\\delta$. The asymptotic characterization we prove is in terms of a system of a nonlinear stochastic process in $k$ dimensions, whose parameters are determined by a fixed point condition. This type of characterization is known in physics as dynamical mean field theory. Rigorous results of this type have been obtained in the past for a few spin glass models. Our proof is based on time discretization and a reduction to certain iterative schemes known as approximate message passing (AMP) algorithms, as opposed to earlier work that was based on large deviations theory and stochastic processes theory. The new approach allows for a more elementary proof and implies that the high-dimensional behavior of the flow is universal with respect to the distribution of the entries of ${\\\\boldsymbol X}$. As specific applications, we obtain high-dimensional characterizations of gradient flow in some classical models from statistics and machine learning, under a random design assumption.\n\n| | |\n| --- | --- |\n| Comments: | 83 pages |\n| Subjects: | Probability (math.PR); Statistics Theory (math.ST) |\n| Cite as: | [arXiv:2112.07572](https://arxiv.org/abs/2112.07572) \\[math.PR\\] |\n| (or [arXiv:2112.07572v1](https://arxiv.org/abs/2112.07572v1) \\[math.PR\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2112.07572](https://doi.org/10.48550/arXiv.2112.07572) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Andrea Montanari \\[ [view email](https://arxiv.org/show-email/e0f68b39/2112.07572)\\] **\\[v1\\]**\nTue, 14 Dec 2021 17:29:32 UTC (73 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The high-dimensional asymptotics of first order methods with random data, by Michael Celentano and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2112.07572)\n- [TeX Source](https://arxiv.org/src/2112.07572)\n- [Other Formats](https://arxiv.org/format/2112.07572)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.PR\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2112.07572&function=prev&context=math.PR)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2112.07572&function=next&context=math.PR)\n\n[new](https://arxiv.org/list/math.PR/new) \\| [recent](https://arxiv.org/list/math.PR/recent) \\| [2021-12](https://arxiv.org/list/math.PR/2021-12)\n\nChange to browse by:\n\n[math](https://arxiv.org/abs/2112.07572?context=math) [math.ST](https://arxiv.org/abs/2112.07572?context=math.ST) [stat](https://arxiv.org/abs/2112.07572?context=stat) [stat.TH](https://arxiv.org/abs/2112.07572?context=stat.TH)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2112.07572)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2112.07572)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2112.07572)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2112.07572) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "The high-dimensional asymptotics of first order methods with random data",
          "cleaned_query": "The high-dimensional asymptotics of first order methods with random data",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Learning time-scales in two-layers neural networks - arXiv",
          "url": "https://arxiv.org/pdf/2303.00055",
          "content": "Learning time-scales in two-layers neural networks\nRapha\u00ebl Berthier\u2217, Andrea Montanari\u2020, Kangjie Zhou\u2021\nMarch 24, 2025\nAbstract\nGradient-based learning in multi-layer neural networks displays a number of striking\nfeatures. In particular, the decrease rate of empirical risk is non-monotone even after\naveraging over large batches. Long plateaus in which one observes barely any progress\nalternate with intervals of rapid decrease. These successive phases of learning often take\nplace on very different time scales. Finally, models learnt in an early phase are typically\n\u2018simpler\u2019 or \u2018easier to learn\u2019 although in a way that is difficult to formalize.\nAlthough theoretical explanations of these phenomena have been put forward, each of them\ncaptures at best certain specific regimes. In this paper, we study the gradient flow dynamics\nof a wide two-layer neural network in high-dimension, when data are distributed according\nto a single-index model (i.e., the target function depends on a one-dimensional projection\nof the covariates). Based on a mixture of new rigorous results, non-rigorous mathematical\nderivations, and numerical simulations, we propose a scenario for the learning dynamics\nin this setting. In particular, the proposed evolution exhibits separation of timescales and\nintermittency. These behaviors arise naturally because the population gradient flow can be\nrecast as a singularly perturbed dynamical system.\nKeywords: Deep learning, Neural network, Gradient flow, Dynamical system, Non-convex\noptimization, Incremental learning\nMathematics Subject Classification: 34E15, 37N40, 68T07\n\u2217EPFL, email address: raphael.berthier1@gmail.com\n\u2020Department of Electrical Engineering and Department of Statistics, Stanford University, email address:\nmontanar@stanford.edu\n\u2021Department of Statistics, Stanford University, email address: kangjie@stanford.edu\n1\narXiv:2303.00055v4 [cs.LG] 9 Mar 2025\nContents\n1 Introduction 3\n2 Setting and canonical learning order 4\n3 Further related work 8\n4 The large-network, high-dimensional limit 9\n4.1 Reduction to d-independent flow . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n4.2 Elimination of the products \u27e8ui, uj \u27e9 . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n4.3 Connection with mean field theory . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n4.4 A general formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n5 Numerical solution 14\n6 Timescales hierarchy in the gradient flow dynamics 17\n6.1 Matched asymptotic expansions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n6.2 First time scale: constant component . . . . . . . . . . . . . . . . . . . . . . . . . 19\n6.3 Second time scale: linear component I . . . . . . . . . . . . . . . . . . . . . . . . 20\n6.4 Third time scale: linear component II . . . . . . . . . . . . . . . . . . . . . . . . 23\n6.5 Conjectured behavior for larger time scales . . . . . . . . . . . . . . . . . . . . . 25\n7 Stochastic gradient descent and finite sample size 27\n8 Discussion 29\nA Proof of Proposition 1 36\nB Appendix to Section 4 37\nB.1 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nB.2 Proof of Corollary 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\nB.3 Proof of Proposition 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nB.4 Derivation of the mean field dynamics (29) . . . . . . . . . . . . . . . . . . . . . 43\nB.5 Details of the alternative mean field approach . . . . . . . . . . . . . . . . . . . . 43\nB.6 Proof of Proposition 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\nC Calculations for the analysis of mean-field gradient flow 46\nC.1 Solution of Eq. (89) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\nC.2 Induced approximation of the risk . . . . . . . . . . . . . . . . . . . . . . . . . . 48\nC.3 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\nD Proofs of Theorem 2 and 3: learning with projected SGD 53\nD.1 Difference between GF and GD . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\nD.2 Difference between GD and SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\nD.3 Difference between SGD and projected SGD . . . . . . . . . . . . . . . . . . . . . 59\nD.4 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\nE Counterexamples to the canonical learning order 63\nE.1 Case 1: \u03c3k = 0 for some k \u2208 N . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nE.2 Case 2: \u03c60 = \u00b7 \u00b7 \u00b7 = \u03c6k = 0 for some k \u2265 1 . . . . . . . . . . . . . . . . . . . . . . 63\nE.3 Case 3: \u03c6k = 0 for some k \u2265 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n2\n1 Introduction\nIt is a recurring empirical observation that the training dynamics of neural networks exhibits a\nwhole range of surprising behaviors:\n1. Plateaus. Plotting the training and test error as a function of SGD steps, using either\nsmall stepsize or large batches to average out stochasticity, reveals striking patterns. These\nerror curves display long plateaus where barely anything seems to be happening, which\nare followed by rapid drops [41, 48, 39].\n2. Time-scales separation. The time window for this rapid descent is much shorter than the\ntime spent in the plateaus. Additionally, subsequent phases of learning take increasingly\nlonger times [18, 8].\n3. Incremental learning. Models learnt in the first phases of learning appear to be simpler\nthan in later phases. Among others, [5] demonstrated that easier examples in a dataset\nare learned earlier; [28] showed that models learnt in the first phase of training correlate\nwell with linear models; [22] showed that, in many simplified models, the dynamics of\ngradient descent explores the solution space in an incremental order of complexity; [39]\ndemonstrated that, in certain settings, a function that approximates well the target is only\nlearnt past the point of overfitting.\nUnderstanding these phenomena is not a matter of intellectual curiosity. In particular, incremental\nlearning plays a key role in our understanding of generalization in deep learning. Indeed, in this\nscenario, stopping the learning at a certain time t amounts to controlling the complexity of the\nmodel learnt. The notion of complexity corresponds to the order in which the space of models is\nexplored.\nWhile a number of groups have developed models to explain these phenomena, it is fair to\nsay that a complete picture is still lacking. An exhaustive overview of these works is out of place\nhere. We will outline three possible explanations that have been developed in the past, and\nprovide more pointers in Section 3.\nTheory #1: Dynamics near singular points. Several early works [41, 17, 44] pointed out\nthat the parametrization of multi-layer neural networks presents symmetries and degeneracies.\nFor instance, the function represented by a multi-layer perceptron is invariant under permutations\nof the neurons in the same layer. As a consequence, the population risk has multiple local minima\nconnected through saddles or other singular sub-manifolds. Dynamics near these sub-manifolds\nnaturally exhibits plateaus. Further, random or agnostic initializations typically place the\nnetwork close to such submanifolds.\nTheory #2: Linear networks. Following the pioneering work of [7], a number of authors,\nmost notably [43, 30], studied the behavior of deep neural networks with linear activations. While\nsuch networks can only represent linear functions, the training dynamics is highly non-linear.\nAs demonstrated in [43], learning happens through stages that correspond to the singular value\ndecomposition of the input-output covariance. Time scales are determined by the singular values.\nTheory #3: Kernel regime. Following an initial insight of [26], a number of groups proved\nthat, for certain initializations, the training dynamics and model learnt by overparametrized\nneural networks is well approximated by certain linearly parametrized models. In the limit of very\nwide networks, the training dynamics of these models converges in turn to the training dynamics\nof kernel ridge(less) regression (KRR) with respect to a deterministic kernel (independent of the\nrandom initialization.) We refer to [9] for an overview and pointers to this literature. Recently\n3\n[21] show that, in high dimension, the learning dynamics of KRR also exhibits plateaus and\nwaterfalls, and learns functions of increasing complexity over a diverging sequence of timescales.\nWhile each of these theories offers useful insights, it is important to realize that they do not\nagree on the basic mechanism that explains plateaus, time-scales separation, and incremental\nlearning. In theory #1, plateaus are associated to singular manifolds and high-dimensional\nsaddles, while in theories #2 and #3 they are related to a hierarchy of singular values of a\ncertain matrix. In #2, the relevant singular values are the ones of the input-output covariance,\nand the fact that these singular values are well separated is postulated to be a property of the\ndata distribution. In contrast, in #3 the relevant singular values are the eigenvalues of the\nkernel operator, and hence completely independent of the output (the target function). In this\ncase, eigenvalues which are very different are proved to exist under natural high-dimensional\ndistributions.\nNot only these theories propose different explanations, but they are also motivated by very\ndifferent simplified models. Theory #1 has been developed only for networks with a small\nnumber of hidden units. Theory #2 only applies to networks with multiple output units, because\notherwise the input-output covariance is a d \u00d7 1 matrix and hence has only one non-trivial\nsingular value. Finally, theory #3 applies under the conditions of the linear (a.k.a. lazy) regime,\nnamely large overparametrization and suitable initialization (see, e.g., [9]).\nIn order to",
          "original_query": "Learning time-scales in two-layers neural networks",
          "cleaned_query": "Learning time-scales in two-layers neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "The Implicit Bias of Gradient Descent on Separable Data",
          "url": "https://www.jmlr.org/papers/volume19/18-188/18-188.pdf",
          "content": "Journal of Machine Learning Research 19 (2018) 1-57 Submitted 4/18; Published 11/18\nThe Implicit Bias of Gradient Descent on Separable Data\nDaniel Soudry DANIEL.SOUDRY@GMAIL.COM\nElad Hoffer ELAD.HOFFER@GMAIL.COM\nMor Shpigel Nacson MOR.SHPIGEL@GMAIL.COM\nDepartment of Electrical Engineering,Technion\nHaifa, 320003, Israel\nSuriya Gunasekar SURIYA@TTIC.EDU\nNathan Srebro NATI@TTIC.EDU\nToyota Technological Institute at Chicago\nChicago, Illinois 60637, USA\nEditor: Leon Bottou\nAbstract\nWe examine gradient descent on unregularized logistic regression problems, with homogeneous\nlinear predictors on linearly separable datasets. We show the predictor converges to the direction\nof the max-margin (hard margin SVM) solution. The result also generalizes to other monotone\ndecreasing loss functions with an infimum at infinity, to multi-class problems, and to training a\nweight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence\nis very slow, and only logarithmic in the convergence of the loss itself. This can help explain the\nbenefit of continuing to optimize the logistic or cross-entropy loss even after the training error is\nzero and the training loss is extremely small, and, as we show, even if the validation loss increases.\nOur methodology can also aid in understanding implicit regularization in more complex models and\nwith other optimization methods.\nKeywords: gradient descent, implicit regularization, generalization, margin, logistic regression\n1. Introduction\nIt is becoming increasingly clear that implicit biases introduced by the optimization algorithm play a\ncrucial role in deep learning and in the generalization ability of the learned models (Neyshabur et al.,\n2014, 2015; Zhang et al., 2017; Keskar et al., 2017; Neyshabur et al., 2017; Wilson et al., 2017).\nIn particular, minimizing the training error, without explicit regularization, over models with more\nparameters and capacity than the number of training examples, often yields good generalization. This\nis despite the fact that the empirical optimization problem being highly underdetermined. That is,\nthere are many global minima of the training objective, most of which will not generalize well, but\nthe optimization algorithm (e.g. gradient descent) biases us toward a particular minimum that does\ngeneralize well. Unfortunately, we still do not have a good understanding of the biases introduced by\ndifferent optimization algorithms in different situations.\nWe do have an understanding of the implicit regularization introduced by early stopping of\nstochastic methods or, at an extreme, of one-pass (no repetition) stochastic gradient descent (Hardt\net al., 2016). However, as discussed above, in deep learning we often benefit from implicit bias even\nwhen optimizing the training error to convergence (without early stopping) using stochastic or batch\nmethods. For loss functions with attainable, finite minimizers, such as the squared loss, we have some\n c 2018 Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at\nhttp://jmlr.org/papers/v19/18-188.html.\nSOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO\nunderstanding of this: in particular, when minimizing an underdetermined least squares problem\nusing gradient descent starting from the origin, it can be shown that we will converge to the minimum\nEuclidean norm solution. However, the logistic loss, and its generalization the cross-entropy loss\nwhich is often used in deep learning, do not admit finite minimizers on separable problems. Instead,\nto drive the loss toward zero and thus minimize it, the norm of the predictor must diverge toward\ninfinity.\nDo we still benefit from implicit regularization when minimizing the logistic loss on separable\ndata? Clearly the norm of the predictor itself is not minimized, since it grows to infinity. However,\nfor prediction, only the direction of the predictor, i.e. the normalized w(t)/ kw(t)k, is important.\nHow does w(t)/ kw(t)k behave as t \u2192 \u221e when we minimize the logistic (or similar) loss using\ngradient descent on separable data, i.e., when it is possible to get zero misclassification error and\nthus drive the loss to zero?\nIn this paper, we show that even without any explicit regularization, for all linearly separa\u0002ble datasets, when minimizing logistic regression problems using gradient descent, we have that\nw(t)/ kw(t)k converges to the L2 maximum margin separator, i.e. to the solution of the hard margin\nSVM for homogeneous linear predictors. This happens even though neither the norm kwk, nor the\nmargin constraint, are part of the objective or explicitly introduced into optimization. More generally,\nwe show the same behavior for generalized linear problems with any smooth, monotone strictly\ndecreasing, lower bounded loss with an exponential tail. Furthermore, we characterize the rate of\nthis convergence, and show that it is rather slow, wherein for almost all datasets, the distance to the\nmax-margin predictor decreasing only as O(1/ log(t)), and in some degenerate datasets, the rate\nfurther slows down to O(log log(t)/ log(t)). This explains why the predictor continues to improve\neven when the training loss is already extremely small. We emphasize that this bias is specific to\ngradient descent, and changing the optimization algorithm, e.g. using adaptive learning rate methods\nsuch as ADAM (Kingma and Ba, 2015), changes this implicit bias.\n2. Main Results\nConsider a dataset {xn, yn}\nN\nn=1, with xn \u2208 R\nd\nand binary labels yn \u2208 {\u22121, 1}. We analyze learning\nby minimizing an empirical loss of the form\nL(w) = X\nN\nn=1\n`\n\u0010\nynw>xn\n\u0011\n. (1)\nwhere w \u2208 R\nd\nis the weight vector. To simplify notation, we assume that all the labels are positive:\n\u2200n : yn = 1 \u2014 this is true without loss of generality, since we can always re-define ynxn as xn.\nWe are particularly interested in problems that are linearly separable, and the loss is smooth\nstrictly decreasing and non-negative:\nAssumption 1 The dataset is linearly separable: \u2203w\u2217 such that \u2200n : w>\n\u2217 xn > 0 .\nAssumption 2 ` (u) is a positive, differentiable, monotonically decreasing to zero1, (so \u2200u : ` (u) >\n0, `0(u) < 0, limu\u2192\u221e ` (u) = limu\u2192\u221e `\n0\n(u) = 0), a \u03b2-smooth function, i.e. its derivative is \u03b2\u0002Lipshitz and limu\u2192\u2212\u221e `\n0\n(u) 6= 0.\n1. The requirement of non-negativity and that the loss asymptotes to zero is purely for convenience. It is enough to\nrequire the loss is monotone decreasing and bounded from below. Any such loss asymptotes to some constant, and is\nthus equivalent to one that satisfies this assumption, up to a shift by that constant.\n2\nGRADIENT DESCENT ON SEPARABLE DATA\nAssumption 2 includes many common loss functions, including the logistic, exp-loss2and probit\nlosses. Assumption 2 implies that L(w) is a \u03b2\u03c32\nmax (X )-smooth function, where \u03c3max (X ) is the\nmaximal singular value of the data matrix X \u2208 R\nd\u00d7N .\nUnder these conditions, the infimum of the optimization problem is zero, but it is not attained\nat any finite w. Furthermore, no finite critical point w exists. We consider minimizing eq. 1 using\nGradient Descent (GD) with a fixed learning rate \u03b7, i.e., with steps of the form:\nw (t + 1) = w (t) \u2212 \u03b7\u2207L(w(t)) = w (t) \u2212 \u03b7\nX\nN\nn=1\n`\n0\n\u0010\nw (t)\n>\nxn\n\u0011\nxn. (2)\nWe do not require convexity. Under Assumptions 1 and 2, gradient descent converges to the global\nminimum (i.e. to zero loss) even without it:\nLemma 1 Let w (t) be the iterates of gradient descent (eq. 2) with \u03b7 < 2\u03b2\n\u22121\u03c3\u22122\nmax (X ) and\nany starting point w(0). Under Assumptions 1 and 2, we have: (1) limt\u2192\u221e L(w (t)) = 0, (2)\nlimt\u2192\u221e kw (t)k = \u221e, and (3) \u2200n : limt\u2192\u221e w (t)\n>\nxn = \u221e.\nProof Since the data is linearly separable, \u2203w\u2217 which linearly separates the data, and therefore\nw>\n\u2217 \u2207L(w) = X\nN\nn=1\n`\n0\n\u0010\nw>xn\n\u0011\nw>\n\u2217 xn.\nFor any finite w, this sum cannot be equal to zero, as a sum of negative terms, since \u2200n : w>\n\u2217 xn > 0\nand \u2200u : `\n0\n(u) < 0. Therefore, there are no finite critical points w, for which \u2207L(w) = 0. But\ngradient descent on a smooth loss with an appropriate stepsize is always guaranteed to converge to a\ncritical point: \u2207L(w (t)) \u2192 0 (see, e.g. Lemma 10 in Appendix A.4, slightly adapted from Ganti\n(2015), Theorem 2). This necessarily implies that kw (t)k \u2192 \u221e while \u2200n : w (t)\n>\nxn > 0 for large\nenough t\u2014since only then `\n0\n\u0010\nw (t)\n>\nxn\n\u0011\n\u2192 0. Therefore, L(w) \u2192 0, so GD converges to the\nglobal minimum.\nThe main question we ask is: can we characterize the direction in which w(t) diverges? That is, does\nthe limit limt\u2192\u221e w (t) / kw (t)k always exist, and if so, what is it?\nIn order to analyze this limit, we will need to make a further assumption on the tail of the loss\nfunction:\nDefinition 2 A function f (u) has a \u201ctight exponential tail\u201d, if there exist positive constants\nc, a, \u00b5+, \u00b5\u2212, u+ and u\u2212 such that\n\u2200u > u+ :f (u) \u2264 c (1 + exp (\u2212\u00b5+u)) e\n\u2212au\n\u2200u > u\u2212 :f (u) \u2265 c (1 \u2212 exp (\u2212\u00b5\u2212u)) e\n\u2212au\n.\nAssumption 3 The negative loss derivative \u2212`\n0\n(u) has a tight exponential tail (Definition 2).\nFor example, the exponential loss ` (u) = e\n\u2212u\nand the commonly used logistic loss ` (u) =\nlog (1 + e\n\u2212u\n) both follow this assumption with a = c = 1. We will assume a = c = 1 \u2014 without\nloss of generality, since these constants can be always absorbed by re-scaling xn and \u03b7.\nWe are now ready to state our main result:\n2. The exp-loss does not have a global \u03b2 smoothness parameter. However, if we initialize with \u03b7 < 1/L(w(0)) then it is\nstraightforward to show the gradient descent iterates maintain bounded local smoothness.\n3\nSOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO\nTheorem 3 For any dataset which is linearly separable (Assumption 1), any \u03b2-smooth decreas\u0002ing loss function (Assumption 2) with an exponential tail (Assumption 3), any stepsize \u03b7 <\n2\u03b2\n\u22121\u03c3\u22122\nmax (X ) and any starting point w(0), the gradient descent iterates (as in eq. 2) will be\u0002have as:\nw (t) = w\u02c6 log t + \u03c1 (t) , (3)\nwhere w\u02c6 is the L2 max margin vector (the solution to the",
          "original_query": "The implicit bias of gradient descent on separable data",
          "cleaned_query": "The implicit bias of gradient descent on separable data",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Unified PDE\u2013NTK bridge for two-layer networks across feature-learning regimes\nDevelop a formal connection between the distributional dynamics (mean-field PDE) and the NTK linearized dynamics by deriving a controlled expansion of the PDE around initialization that yields the NTK as a first-order term. Quantify the error as a function of width, step size/noise, and training time, and identify a sharp \u201ccrossover time\u201d when feature learning necessarily departs from the lazy regime.",
        "Predicting plateau lengths via singular perturbation invariants in mean-field gradient flow\nExtend the singularly perturbed dynamical system analysis of time-scale separation to a broader class of target functions beyond single-index (e.g., additive index models or sparse multi-index). Derive explicit asymptotic formulas for plateau durations and drop magnitudes in terms of identifiable quantities (e.g., Hermite/Fourier coefficients of the target and activation), then validate them by simulating wide two-layer networks.",
        "Implicit bias of mean-field SGD: max-margin directions in separable classification\nStudy two-layer networks trained with logistic/cross-entropy loss on separable data in the mean-field limit, aiming to characterize the limiting predictor direction (in function space) analogous to the max-margin bias for linear models. Prove (or refute) convergence to a margin-maximizing solution under a norm induced by the limiting kernel/feature map, and quantify how SGD noise affects the margin growth rate.",
        "Dynamical mean field theory for finite-width feature learning using AMP reductions\nAdapt the AMP-based proof approach for high-dimensional first-order methods with random data to analyze training of two-layer networks with finite width scaling (intermediate between NTK and mean-field). Produce a closed-form state evolution for key order parameters (e.g., alignment with teacher direction, feature covariance), and use it to predict generalization curves and intermittency under random-design assumptions.",
        "Optimal noise/temperature schedules to accelerate transitions between learning phases\nUse the distributional dynamics PDE with injected noise (as in noisy SGD) to design annealing schedules that intentionally shorten plateaus while preserving final generalization. Formulate an optimal control problem on the PDE (or reduced low-dimensional flow) to minimize time-to-accuracy, then implement practical SGD noise schedules and test whether they selectively speed up later \u201chard\u201d components.",
        "Activation design for controllable learning order and reduced intermittency\nLeverage the \u201ccanonical learning order\u201d idea (components learned in sequence) by constructing activations whose induced kernel/mean-field dynamics reshapes the spectral bias. Optimize activation families to either (i) enforce a desired curriculum (e.g., learn low-frequency then high-frequency components) or (ii) collapse time-scale separation to avoid long plateaus, and verify predictions via both NTK eigen-structure and mean-field simulations.",
        "Early stopping criteria from matched asymptotics: phase-aware stopping rules\nDerive stopping rules that detect regime transitions (plateau \u2192 rapid descent) using observable proxies (e.g., NTK trace drift, gradient norm scaling, margin growth rate) predicted by asymptotic expansions. Provide theory linking these proxies to generalization in both lazy and feature-learning regimes, and demonstrate that phase-aware stopping outperforms standard validation-based heuristics in controlled high-dim experiments.",
        "From random design to structured data: universality breaks and corrections\nTest and extend universality results (subgaussian i.i.d. design) to structured covariates (e.g., correlated Gaussian, low-rank plus noise, convolutional features). Derive correction terms to the dynamical mean field / PDE descriptions that capture non-i.i.d. structure, and identify which training phenomena (e.g., intermittency, implicit bias) persist or change under realistic data geometry.",
        "Coupled dynamics of representation and margin: a joint theory for classification generalization\nBuild a model that simultaneously tracks (a) feature evolution (departure from NTK constancy) and (b) margin evolution (implicit bias) during training on separable data. The contribution is a set of coupled evolution equations (PDE-reduced or state-evolution style) that predict when margin growth is dominated by kernel-like behavior vs. representation learning, and how that impacts test error.",
        "Projected/low-dimensional training as a controlled approximation to full SGD dynamics\nGeneralize \u201cprojected SGD\u201d ideas by designing adaptive low-dimensional subspaces (e.g., top evolving NTK eigen-directions or mean-field order-parameter directions) in which training is provably close to full SGD for long times. Provide approximation bounds on the learned function and generalization error, and develop an algorithm that updates the projection online to track regime changes without paying full training cost."
      ],
      "generated_ideas_raw": "1. **Unified PDE\u2013NTK bridge for two-layer networks across feature-learning regimes**  \nDevelop a formal connection between the distributional dynamics (mean-field PDE) and the NTK linearized dynamics by deriving a controlled expansion of the PDE around initialization that yields the NTK as a first-order term. Quantify the error as a function of width, step size/noise, and training time, and identify a sharp \u201ccrossover time\u201d when feature learning necessarily departs from the lazy regime.\n\n2. **Predicting plateau lengths via singular perturbation invariants in mean-field gradient flow**  \nExtend the singularly perturbed dynamical system analysis of time-scale separation to a broader class of target functions beyond single-index (e.g., additive index models or sparse multi-index). Derive explicit asymptotic formulas for plateau durations and drop magnitudes in terms of identifiable quantities (e.g., Hermite/Fourier coefficients of the target and activation), then validate them by simulating wide two-layer networks.\n\n3. **Implicit bias of mean-field SGD: max-margin directions in separable classification**  \nStudy two-layer networks trained with logistic/cross-entropy loss on separable data in the mean-field limit, aiming to characterize the limiting predictor direction (in function space) analogous to the max-margin bias for linear models. Prove (or refute) convergence to a margin-maximizing solution under a norm induced by the limiting kernel/feature map, and quantify how SGD noise affects the margin growth rate.\n\n4. **Dynamical mean field theory for finite-width feature learning using AMP reductions**  \nAdapt the AMP-based proof approach for high-dimensional first-order methods with random data to analyze training of two-layer networks with finite width scaling (intermediate between NTK and mean-field). Produce a closed-form state evolution for key order parameters (e.g., alignment with teacher direction, feature covariance), and use it to predict generalization curves and intermittency under random-design assumptions.\n\n5. **Optimal noise/temperature schedules to accelerate transitions between learning phases**  \nUse the distributional dynamics PDE with injected noise (as in noisy SGD) to design annealing schedules that intentionally shorten plateaus while preserving final generalization. Formulate an optimal control problem on the PDE (or reduced low-dimensional flow) to minimize time-to-accuracy, then implement practical SGD noise schedules and test whether they selectively speed up later \u201chard\u201d components.\n\n6. **Activation design for controllable learning order and reduced intermittency**  \nLeverage the \u201ccanonical learning order\u201d idea (components learned in sequence) by constructing activations whose induced kernel/mean-field dynamics reshapes the spectral bias. Optimize activation families to either (i) enforce a desired curriculum (e.g., learn low-frequency then high-frequency components) or (ii) collapse time-scale separation to avoid long plateaus, and verify predictions via both NTK eigen-structure and mean-field simulations.\n\n7. **Early stopping criteria from matched asymptotics: phase-aware stopping rules**  \nDerive stopping rules that detect regime transitions (plateau \u2192 rapid descent) using observable proxies (e.g., NTK trace drift, gradient norm scaling, margin growth rate) predicted by asymptotic expansions. Provide theory linking these proxies to generalization in both lazy and feature-learning regimes, and demonstrate that phase-aware stopping outperforms standard validation-based heuristics in controlled high-dim experiments.\n\n8. **From random design to structured data: universality breaks and corrections**  \nTest and extend universality results (subgaussian i.i.d. design) to structured covariates (e.g., correlated Gaussian, low-rank plus noise, convolutional features). Derive correction terms to the dynamical mean field / PDE descriptions that capture non-i.i.d. structure, and identify which training phenomena (e.g., intermittency, implicit bias) persist or change under realistic data geometry.\n\n9. **Coupled dynamics of representation and margin: a joint theory for classification generalization**  \nBuild a model that simultaneously tracks (a) feature evolution (departure from NTK constancy) and (b) margin evolution (implicit bias) during training on separable data. The contribution is a set of coupled evolution equations (PDE-reduced or state-evolution style) that predict when margin growth is dominated by kernel-like behavior vs. representation learning, and how that impacts test error.\n\n10. **Projected/low-dimensional training as a controlled approximation to full SGD dynamics**  \nGeneralize \u201cprojected SGD\u201d ideas by designing adaptive low-dimensional subspaces (e.g., top evolving NTK eigen-directions or mean-field order-parameter directions) in which training is provably close to full SGD for long times. Provide approximation bounds on the learned function and generalization error, and develop an algorithm that updates the projection online to track regime changes without paying full training cost.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Unified PDE\u2013NTK bridge for two-layer networks across feature-learning regimes\nDevelop a formal connection between the distributional dynamics (mean-field PDE) and the NTK linearized dynamics by derivi",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Predicting plateau lengths via singular perturbation invariants in mean-field gradient flow\nExtend the singularly perturbed dynamical system analysis of time-scale separation to a broader class of tar",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Implicit bias of mean-field SGD: max-margin directions in separable classification\nStudy two-layer networks trained with logistic/cross-entropy loss on separable data in the mean-field limit, aiming t",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Dynamical mean field theory for finite-width feature learning using AMP reductions\nAdapt the AMP-based proof approach for high-dimensional first-order methods with random data to analyze training of t",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Optimal noise/temperature schedules to accelerate transitions between learning phases\nUse the distributional dynamics PDE with injected noise (as in noisy SGD) to design annealing schedules that inten",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Activation design for controllable learning order and reduced intermittency\nLeverage the \u201ccanonical learning order\u201d idea (components learned in sequence) by constructing activations whose induced kern",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Early stopping criteria from matched asymptotics: phase-aware stopping rules\nDerive stopping rules that detect regime transitions (plateau \u2192 rapid descent) using observable proxies (e.g., NTK trace dr",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "From random design to structured data: universality breaks and corrections\nTest and extend universality results (subgaussian i.i.d. design) to structured covariates (e.g., correlated Gaussian, low-ran",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Coupled dynamics of representation and margin: a joint theory for classification generalization\nBuild a model that simultaneously tracks (a) feature evolution (departure from NTK constancy) and (b) ma",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Projected/low-dimensional training as a controlled approximation to full SGD dynamics\nGeneralize \u201cprojected SGD\u201d ideas by designing adaptive low-dimensional subspaces (e.g., top evolving NTK eigen-dir",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 39,
      "paper_title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
      "contribution": "Demonstrates that dramatically increasing network depth (up to 1024 layers) in a self-supervised, goal-conditioned contrastive RL setup yields large quantitative gains and qualitatively new goal-reaching behaviors that shallower agents cannot discover.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12744,
      "output_tokens": 1024,
      "predecessor_details": [
        {
          "success": true,
          "title": "CURL: Contrastive Unsupervised Representations for ...",
          "url": "https://arxiv.org/abs/2004.04136",
          "content": "[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)\n\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n# Computer Science > Machine Learning\n\n**arXiv:2004.04136** (cs)\n\n\\[Submitted on 8 Apr 2020 ( [v1](https://arxiv.org/abs/2004.04136v1)), last revised 21 Sep 2020 (this version, v4)\\]\n\n# Title:CURL: Contrastive Unsupervised Representations for Reinforcement Learning\n\nAuthors: [Aravind Srinivas](https://arxiv.org/search/cs?searchtype=author&query=Aravind), [Michael Laskin](https://arxiv.org/search/cs?searchtype=author&query=Laskin,+M), [Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&query=Abbeel,+P)\n\nView a PDF of the paper titled CURL: Contrastive Unsupervised Representations for Reinforcement Learning, by Aravind Srinivas and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2004.04136)\n\n> Abstract:We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at [this https URL](https://github.com/MishaLaskin/curl).\n\n| | |\n| --- | --- |\n| Comments: | First two authors contributed equally, website: [this https URL](https://mishalaskin.github.io/curl) code: [this https URL](https://github.com/MishaLaskin/curl) |\n| Subjects: | Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2004.04136](https://arxiv.org/abs/2004.04136) \\[cs.LG\\] |\n| (or [arXiv:2004.04136v4](https://arxiv.org/abs/2004.04136v4) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2004.04136](https://doi.org/10.48550/arXiv.2004.04136) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Michael Laskin \\[ [view email](https://arxiv.org/show-email/cb6b0ab7/2004.04136)\\] **[\\[v1\\]](https://arxiv.org/abs/2004.04136v1)**\nWed, 8 Apr 2020 17:40:43 UTC (4,829 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2004.04136v2)**\nTue, 28 Apr 2020 17:54:47 UTC (5,056 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2004.04136v3)**\nTue, 7 Jul 2020 16:37:04 UTC (10,150 KB)\n**\\[v4\\]**\nMon, 21 Sep 2020 15:34:30 UTC (5,115 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled CURL: Contrastive Unsupervised Representations for Reinforcement Learning, by Aravind Srinivas and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2004.04136)\n- [TeX Source](https://arxiv.org/src/2004.04136)\n- [Other Formats](https://arxiv.org/format/2004.04136)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2004.04136&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2004.04136&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2020-04](https://arxiv.org/list/cs.LG/2020-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2004.04136?context=cs) [cs.CV](https://arxiv.org/abs/2004.04136?context=cs.CV) [stat](https://arxiv.org/abs/2004.04136?context=stat) [stat.ML](https://arxiv.org/abs/2004.04136?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2004.04136)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2004.04136)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2004.04136)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2004.html#abs-2004-04136) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2004-04136)\n\n[Aravind Srinivas](https://dblp.uni-trier.de/search/author?author=Aravind%20Srinivas) [Pieter Abbeel](https://dblp.uni-trier.de/search/author?author=Pieter%20Abbeel)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2004.04136&description=CURL: Contrastive Unsupervised Representations for Reinforcement Learning) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2004.04136&title=CURL: Contrastive Unsupervised Representations for Reinforcement Learning)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2004.04136) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning (Srinivas et al., 2020)",
          "cleaned_query": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] A Simple Framework for Contrastive Learning of Visual ...",
          "url": "https://proceedings.mlr.press/v119/chen20j/chen20j.pdf",
          "content": "A Simple Framework for Contrastive Learning of Visual Representations\nTing Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton 1\nAbstract\nThis paper presents SimCLR: a simple framework\nfor contrastive learning of visual representations.\nWe simplify recently proposed contrastive self\u0002supervised learning algorithms without requiring\nspecialized architectures or a memory bank. In\norder to understand what enables the contrastive\nprediction tasks to learn useful representations,\nwe systematically study the major components of\nour framework. We show that (1) composition of\ndata augmentations plays a critical role in defining\neffective predictive tasks, (2) introducing a learn\u0002able nonlinear transformation between the repre\u0002sentation and the contrastive loss substantially im\u0002proves the quality of the learned representations,\nand (3) contrastive learning benefits from larger\nbatch sizes and more training steps compared to\nsupervised learning. By combining these findings,\nwe are able to considerably outperform previous\nmethods for self-supervised and semi-supervised\nlearning on ImageNet. A linear classifier trained\non self-supervised representations learned by Sim\u0002CLR achieves 76.5% top-1 accuracy, which is a\n7% relative improvement over previous state-of\u0002the-art, matching the performance of a supervised\nResNet-50. When fine-tuned on only 1% of the\nlabels, we achieve 85.8% top-5 accuracy, outper\u0002forming AlexNet with 100\u00d7 fewer labels. 1\n1. Introduction\nLearning effective visual representations without human\nsupervision is a long-standing problem. Most mainstream\napproaches fall into one of two classes: generative or dis\u0002criminative. Generative approaches learn to generate or\notherwise model pixels in the input space (Hinton et al.,\n2006; Kingma & Welling, 2013; Goodfellow et al., 2014).\n1Google Research, Brain Team. Correspondence to: Ting Chen\n.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\n1Code available at https://github.com/google-research/simclr.\nFigure 1. ImageNet Top-1 accuracy of linear classifiers trained\non representations learned with different self-supervised meth\u0002ods (pretrained on ImageNet). Gray cross indicates supervised\nResNet-50. Our method, SimCLR, is shown in bold.\nHowever, pixel-level generation is computationally expen\u0002sive and may not be necessary for representation learning.\nDiscriminative approaches learn representations using objec\u0002tive functions similar to those used for supervised learning,\nbut train networks to perform pretext tasks where both the in\u0002puts and labels are derived from an unlabeled dataset. Many\nsuch approaches have relied on heuristics to design pretext\ntasks (Doersch et al., 2015; Zhang et al., 2016; Noroozi &\nFavaro, 2016; Gidaris et al., 2018), which could limit the\ngenerality of the learned representations. Discriminative\napproaches based on contrastive learning in the latent space\nhave recently shown great promise, achieving state-of-the\u0002art results (Hadsell et al., 2006; Dosovitskiy et al., 2014;\nOord et al., 2018; Bachman et al., 2019).\nIn this work, we introduce a simple framework for con\u0002trastive learning of visual representations, which we call\nSimCLR. Not only does SimCLR outperform previous work\n(Figure 1), but it is also simpler, requiring neither special\u0002ized architectures (Bachman et al., 2019; H\u00e9naff et al., 2019)\nnor a memory bank (Wu et al., 2018; Tian et al., 2019; He\net al., 2019; Misra & van der Maaten, 2019).\nIn order to understand what enables good contrastive repre\u0002sentation learning, we systematically study the major com\u0002ponents of our framework and show that:\nA Simple Framework for Contrastive Learning of Visual Representations\n\u2022 Composition of multiple data augmentation operations\nis crucial in defining the contrastive prediction tasks that\nyield effective representations. In addition, unsupervised\ncontrastive learning benefits from stronger data augmen\u0002tation than supervised learning.\n\u2022 Introducing a learnable nonlinear transformation be\u0002tween the representation and the contrastive loss substan\u0002tially improves the quality of the learned representations.\n\u2022 Representation learning with contrastive cross entropy\nloss benefits from normalized embeddings and an appro\u0002priately adjusted temperature parameter.\n\u2022 Contrastive learning benefits from larger batch sizes and\nlonger training compared to its supervised counterpart.\nLike supervised learning, contrastive learning benefits\nfrom deeper and wider networks.\nWe combine these findings to achieve a new state-of-the-art\nin self-supervised and semi-supervised learning on Ima\u0002geNet ILSVRC-2012 (Russakovsky et al., 2015). Under the\nlinear evaluation protocol, SimCLR achieves 76.5% top-1\naccuracy, which is a 7% relative improvement over previous\nstate-of-the-art (H\u00e9naff et al., 2019). When fine-tuned with\nonly 1% of the ImageNet labels, SimCLR achieves 85.8%\ntop-5 accuracy, a relative improvement of 10% (H\u00e9naff et al.,\n2019). When fine-tuned on other natural image classifica\u0002tion datasets, SimCLR performs on par with or better than\na strong supervised baseline (Kornblith et al., 2019) on 10\nout of 12 datasets.\n2. Method\n2.1. The Contrastive Learning Framework\nInspired by recent contrastive learning algorithms (see Sec\u0002tion 7 for an overview), SimCLR learns representations\nby maximizing agreement between differently augmented\nviews of the same data example via a contrastive loss in\nthe latent space. As illustrated in Figure 2, this framework\ncomprises the following four major components.\n\u2022 A stochastic data augmentation module that transforms\nany given data example randomly resulting in two cor\u0002related views of the same example, denoted x\u02dci and x\u02dcj ,\nwhich we consider as a positive pair. In this work, we\nsequentially apply three simple augmentations: random\ncropping followed by resize back to the original size, ran\u0002dom color distortions, and random Gaussian blur. As\nshown in Section 3, the combination of random crop and\ncolor distortion is crucial to achieve a good performance.\n\u2022 A neural network base encoder f(\u00b7) that extracts repre\u0002sentation vectors from augmented data examples. Our\nframework allows various choices of the network archi\u0002tecture without any constraints. We opt for simplicity\nand adopt the commonly used ResNet (He et al., 2016)\n\u2190\u2212 Representation \u2212\u2192\nx\nx\u02dci x\u02dcj\nhi hj\nzi zj\nt \u223c T\nt\n0 \u223c T\nf(\u00b7) f(\u00b7)\ng(\u00b7) g(\u00b7)\nMaximize agreement\nFigure 2. A simple framework for contrastive learning of visual\nrepresentations. Two separate data augmentation operators are\nsampled from the same family of augmentations (t \u223c T and\nt\n0 \u223c T ) and applied to each data example to obtain two correlated\nviews. A base encoder network f(\u00b7) and a projection head g(\u00b7)\nare trained to maximize agreement using a contrastive loss. After\ntraining is completed, we throw away the projection head g(\u00b7) and\nuse encoder f(\u00b7) and representation h for downstream tasks.\nto obtain hi = f(x\u02dci) = ResNet(x\u02dci) where hi \u2208 R\nd\nis\nthe output after the average pooling layer.\n\u2022 A small neural network projection head g(\u00b7) that maps\nrepresentations to the space where contrastive loss is\napplied. We use a MLP with one hidden layer to obtain\nzi = g(hi) = W(2)\u03c3(W(1)hi) where \u03c3 is a ReLU non\u0002linearity. As shown in section 4, we find it beneficial to\ndefine the contrastive loss on zi\u2019s rather than hi\u2019s.\n\u2022 A contrastive loss function defined for a contrastive pre\u0002diction task. Given a set {x\u02dck} including a positive pair\nof examples x\u02dci and x\u02dcj , the contrastive prediction task\naims to identify x\u02dcj in {x\u02dck}k6=i for a given x\u02dci.\nWe randomly sample a minibatch of N examples and define\nthe contrastive prediction task on pairs of augmented exam\u0002ples derived from the minibatch, resulting in 2N data points.\nWe do not sample negative examples explicitly. Instead,\ngiven a positive pair, similar to (Chen et al., 2017), we treat\nthe other 2(N \u2212 1) augmented examples within a minibatch\nas negative examples. Let sim(u, v) = u\n>v/kukkvk de\u0002note the dot product between `2 normalized u and v (i.e.\ncosine similarity). Then the loss function for a positive pair\nof examples (i, j) is defined as\n`i,j = \u2212 log exp(sim(zi\n, zj )/\u03c4 )\nP2N\nk=1 1[k6=i] exp(sim(zi\n, zk)/\u03c4 )\n, (1)\nwhere 1[k6=i] \u2208 {0, 1} is an indicator function evaluating to\n1 iff k 6= i and \u03c4 denotes a temperature parameter. The fi\u0002nal loss is computed across all positive pairs, both (i, j)\nand (j, i), in a mini-batch. This loss has been used in\nprevious work (Sohn, 2016; Wu et al., 2018; Oord et al.,\n2018); for convenience, we term it NT-Xent (the normalized\ntemperature-scaled cross entropy loss).\nA Simple Framework for Contrastive Learning of Visual Representations\nAlgorithm 1 SimCLR\u2019s main learning algorithm.\ninput: batch size N, constant \u03c4 , structure of f, g, T .\nfor sampled minibatch {xk}\nN\nk=1 do\nfor all k \u2208 {1, . . . , N} do\ndraw two augmentation functions t\u223c T , t\n0 \u223c T\n# the first augmentation\nx\u02dc2k\u22121 = t(xk)\nh2k\u22121 = f(x\u02dc2k\u22121) # representation\nz2k\u22121 = g(h2k\u22121) # projection\n# the second augmentation\nx\u02dc2k = t\n0\n(xk)\nh2k = f(x\u02dc2k) # representation\nz2k = g(h2k) # projection\nend for\nfor all i \u2208 {1, . . . , 2N} and j \u2208 {1, . . . , 2N} do\nsi,j = z\n>\ni zj/(kzikkzjk) # pairwise similarity\nend for\ndefine `(i, j) as `(i, j)=\u2212 log P\nexp(si,j /\u03c4)\n2N\nk=1 1[k6=i] exp(si,k/\u03c4)\nL =\n1\n2N\nPN\nk=1 [`(2k\u22121, 2k) + `(2k, 2k\u22121)]\nupdate networks f and g to minimize L\nend for\nreturn encoder network f(\u00b7), and throw away g(\u00b7)\nAlgorithm 1 summarizes the proposed method.\n2.2. Training with Large Batch Size\nTo keep it simple, we do not train the model with a memory\nbank (Wu et al., 2018; He et al., 2019). Instead, we vary\nthe training batch size N from 256 to 8192. A batch size\nof 8192 gives us 16382 negative examples per positive pair\nfrom both augmentation views. Training with large batch\nsize may be unstable when using standard SGD/Momentum\nwith linear learning rate scaling (Goyal et al., 2017). To\nstabilize the training, we use the LARS optimizer (",
          "original_query": "A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) (Chen et al., 2020)",
          "cleaned_query": "A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[1707.01495] Hindsight Experience Replay",
          "url": "https://arxiv.org/abs/1707.01495",
          "content": "[1707.01495] Hindsight Experience Replay[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1707.01495\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1707.01495**(cs)\n[Submitted on 5 Jul 2017 ([v1](https://arxiv.org/abs/1707.01495v1)), last revised 23 Feb 2018 (this version, v3)]\n# Title:Hindsight Experience Replay\nAuthors:[Marcin Andrychowicz](https://arxiv.org/search/cs?searchtype=author&amp;query=Andrychowicz,+M),[Filip Wolski](https://arxiv.org/search/cs?searchtype=author&amp;query=Wolski,+F),[Alex Ray](https://arxiv.org/search/cs?searchtype=author&amp;query=Ray,+A),[Jonas Schneider](https://arxiv.org/search/cs?searchtype=author&amp;query=Schneider,+J),[Rachel Fong](https://arxiv.org/search/cs?searchtype=author&amp;query=Fong,+R),[Peter Welinder](https://arxiv.org/search/cs?searchtype=author&amp;query=Welinder,+P),[Bob McGrew](https://arxiv.org/search/cs?searchtype=author&amp;query=McGrew,+B),[Josh Tobin](https://arxiv.org/search/cs?searchtype=author&amp;query=Tobin,+J),[Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&amp;query=Abbeel,+P),[Wojciech Zaremba](https://arxiv.org/search/cs?searchtype=author&amp;query=Zaremba,+W)\nView a PDF of the paper titled Hindsight Experience Replay, by Marcin Andrychowicz and 9 other authors\n[View PDF](https://arxiv.org/pdf/1707.01495)> > Abstract:\n> Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.\n> We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Robotics (cs.RO)|\nCite as:|[arXiv:1707.01495](https://arxiv.org/abs/1707.01495)[cs.LG]|\n|(or[arXiv:1707.01495v3](https://arxiv.org/abs/1707.01495v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1707.01495](https://doi.org/10.48550/arXiv.1707.01495)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Marcin Andrychowicz [[view email](https://arxiv.org/show-email/c7a00ded/1707.01495)]\n**[[v1]](https://arxiv.org/abs/1707.01495v1)**Wed, 5 Jul 2017 17:55:53 UTC (1,023 KB)\n**[[v2]](https://arxiv.org/abs/1707.01495v2)**Mon, 10 Jul 2017 18:35:33 UTC (1,023 KB)\n**[v3]**Fri, 23 Feb 2018 10:04:20 UTC (1,238 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Hindsight Experience Replay, by Marcin Andrychowicz and 9 other authors\n* [View PDF](https://arxiv.org/pdf/1707.01495)\n* [TeX Source](https://arxiv.org/src/1707.01495)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1707.01495&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1707.01495&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2017-07](https://arxiv.org/list/cs.LG/2017-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/1707.01495?context=cs)\n[cs.AI](https://arxiv.org/abs/1707.01495?context=cs.AI)\n[cs.NE](https://arxiv.org/abs/1707.01495?context=cs.NE)\n[cs.RO](https://arxiv.org/abs/1707.01495?context=cs.RO)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1707.01495)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1707.01495)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1707.01495)\n### [4 blog links](https://arxiv.org/tb/1707.01495)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1707.html#AndrychowiczWRS17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/AndrychowiczWRS17)\n[Marcin Andrychowicz]()\n[Filip Wolski]()\n[Alex Ray]()\n[Jonas Schneider]()\n[Rachel Fong]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1707.01495)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Hindsight Experience Replay (HER) (Andrychowicz et al., 2017)",
          "cleaned_query": "Hindsight Experience Replay (HER)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Diversity is All You Need: Learning Skills without a Reward Function",
          "url": "https://arxiv.org/abs/1802.06070",
          "content": "# Computer Science > Artificial Intelligence\n\n**arXiv:1802.06070** (cs)\n\n\\[Submitted on 16 Feb 2018 ( [v1](https://arxiv.org/abs/1802.06070v1)), last revised 9 Oct 2018 (this version, v6)\\]\n\n# Title:Diversity is All You Need: Learning Skills without a Reward Function\n\nAuthors: [Benjamin Eysenbach](https://arxiv.org/search/cs?searchtype=author&query=Eysenbach,+B), [Abhishek Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta,+A), [Julian Ibarz](https://arxiv.org/search/cs?searchtype=author&query=Ibarz,+J), [Sergey Levine](https://arxiv.org/search/cs?searchtype=author&query=Levine,+S)\n\nView a PDF of the paper titled Diversity is All You Need: Learning Skills without a Reward Function, by Benjamin Eysenbach and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/1802.06070)\n\n> Abstract:Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.\n\n| | |\n| --- | --- |\n| Comments: | Videos and code for our experiments are available at: [this https URL](https://sites.google.com/view/diayn) |\n| Subjects: | Artificial Intelligence (cs.AI); Robotics (cs.RO) |\n| Cite as: | [arXiv:1802.06070](https://arxiv.org/abs/1802.06070) \\[cs.AI\\] |\n| (or [arXiv:1802.06070v6](https://arxiv.org/abs/1802.06070v6) \\[cs.AI\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1802.06070](https://doi.org/10.48550/arXiv.1802.06070) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Benjamin Eysenbach \\[ [view email](https://arxiv.org/show-email/11404bcf/1802.06070)\\] **[\\[v1\\]](https://arxiv.org/abs/1802.06070v1)**\nFri, 16 Feb 2018 18:57:57 UTC (6,821 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1802.06070v2)**\nTue, 20 Feb 2018 18:45:19 UTC (7,288 KB)\n**[\\[v3\\]](https://arxiv.org/abs/1802.06070v3)**\nFri, 23 Feb 2018 18:56:13 UTC (7,466 KB)\n**[\\[v4\\]](https://arxiv.org/abs/1802.06070v4)**\nThu, 1 Mar 2018 17:10:25 UTC (6,767 KB)\n**[\\[v5\\]](https://arxiv.org/abs/1802.06070v5)**\nWed, 6 Jun 2018 23:07:09 UTC (7,600 KB)\n**\\[v6\\]**\nTue, 9 Oct 2018 23:19:52 UTC (8,523 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Diversity is All You Need: Learning Skills without a Reward Function, by Benjamin Eysenbach and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/1802.06070)\n- [TeX Source](https://arxiv.org/src/1802.06070)\n- [Other Formats](https://arxiv.org/format/1802.06070)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.AI\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1802.06070&function=prev&context=cs.AI)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1802.06070&function=next&context=cs.AI)\n\n[new](https://arxiv.org/list/cs.AI/new) \\| [recent](https://arxiv.org/list/cs.AI/recent) \\| [2018-02](https://arxiv.org/list/cs.AI/2018-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1802.06070?context=cs) [cs.RO](https://arxiv.org/abs/1802.06070?context=cs.RO)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1802.06070)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1802.06070)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1802.06070)\n\n### [2 blog links](https://arxiv.org/tb/1802.06070)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1802.html#abs-1802-06070) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1802-06070)\n\n[Benjamin Eysenbach](https://dblp.uni-trier.de/search/author?author=Benjamin%20Eysenbach) [Abhishek Gupta](https://dblp.uni-trier.de/search/author?author=Abhishek%20Gupta) [Julian Ibarz](https://dblp.uni-trier.de/search/author?author=Julian%20Ibarz) [Sergey Levine](https://dblp.uni-trier.de/search/author?author=Sergey%20Levine)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1802.06070) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Diversity Is All You Need (DIAYN) (Eysenbach et al., 2019)",
          "cleaned_query": "Diversity Is All You Need (DIAYN)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Universal Value Function Approximators",
          "url": "https://proceedings.mlr.press/v37/schaul15.html",
          "content": "\\[ [edit](https://github.com/mlresearch/v37/edit/gh-pages/_posts/2015-06-01-schaul15.md)\\]\n\n# Universal Value Function Approximators\n\nTom Schaul,\u00a0Daniel Horgan,\u00a0Karol Gregor,\u00a0David Silver\n\n_Proceedings of the 32nd International Conference on Machine Learning_,\u00a0PMLR 37:1312-1320,\u00a02015.\n\n#### Abstract\n\nValue functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.\n\n#### Cite this Paper\n\n* * *\n\nBibTeX\n\n`@InProceedings{pmlr-v37-schaul15,\ntitle = {Universal Value Function Approximators},\nauthor = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},\nbooktitle = {Proceedings of the 32nd International Conference on Machine Learning},\npages = {1312--1320},\nyear = {2015},\neditor = {Bach, Francis and Blei, David},\nvolume = {37},\nseries = {Proceedings of Machine Learning Research},\naddress = {Lille, France},\nmonth = {07--09 Jul},\npublisher = {PMLR},\npdf = {http://proceedings.mlr.press/v37/schaul15.pdf},\nurl = {https://proceedings.mlr.press/v37/schaul15.html},\nabstract = {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}\n}`\n\nCopy to ClipboardDownload\n\nEndnote\n\n`%0 Conference Paper\n%T Universal Value Function Approximators\n%A Tom Schaul\n%A Daniel Horgan\n%A Karol Gregor\n%A David Silver\n%B Proceedings of the 32nd International Conference on Machine Learning\n%C Proceedings of Machine Learning Research\n%D 2015\n%E Francis Bach\n%E David Blei\n%F pmlr-v37-schaul15\n%I PMLR\n%P 1312--1320\n%U https://proceedings.mlr.press/v37/schaul15.html\n%V 37\n%X Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.`\n\nCopy to ClipboardDownload\n\nRIS\n\n`TY - CPAPER\nTI - Universal Value Function Approximators\nAU - Tom Schaul\nAU - Daniel Horgan\nAU - Karol Gregor\nAU - David Silver\nBT - Proceedings of the 32nd International Conference on Machine Learning\nDA - 2015/06/01\nED - Francis Bach\nED - David Blei\nID - pmlr-v37-schaul15\nPB - PMLR\nDP - Proceedings of Machine Learning Research\nVL - 37\nSP - 1312\nEP - 1320\nL1 - http://proceedings.mlr.press/v37/schaul15.pdf\nUR - https://proceedings.mlr.press/v37/schaul15.html\nAB - Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.\nER -`\n\nCopy to ClipboardDownload\n\nAPA\n\n`Schaul, T., Horgan, D., Gregor, K. & Silver, D.. (2015). Universal Value Function Approximators. Proceedings of the 32nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 37:1312-1320 Available from https://proceedings.mlr.press/v37/schaul15.html.`\n\nCopy to ClipboardDownload\n\n* * *\n\n#### Related Material\n\n- [Download PDF](http://proceedings.mlr.press/v37/schaul15.pdf)\n- [Supplementary Material](http://proceedings.mlr.press/v37/schaul15-supp.pdf)",
          "original_query": "Universal Value Function Approximators (UVFA) (Schaul et al., 2015)",
          "cleaned_query": "Universal Value Function Approximators (UVFA)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Relative Scaling Laws for LLMs",
          "url": "https://arxiv.org/html/2510.24626",
          "content": "Relative Scaling Laws for LLMs\n# Relative Scaling Laws for LLMs\nWilliam Held\u03c3,\u03b3,David Hall\u03bcPercy Liang\u03c3Diyi Yang\u03c3\u03c3Stanford University\u03bcOpenAthena\u03b3Georgia Institute of Technologyheld@stanford.eduContact: held@stanford.edu\n###### Abstract\nScaling laws describe how language models improve with additional data, parameters, and compute. While widely used, they are typically measured on aggregate test sets. Aggregate evaluations yield clean trends but average over heterogeneous subpopulations, obscuring performance disparities. We introducerelative scaling laws, which track how performance gaps between test distributions evolve with scale rather than focusing solely on absolute error. Using 255 decoder-only Transformers trained under matched-compute (*IsoFLOP*) budgets from101810^{18}\u2013102010^{20}FLOPs on standard pretraining datasets, we find diverse trajectories: academic domains on MMLU converge toward parity; regional English dialects shift depending on population size; and clusters of AI risk behaviours split, with capability- and influence-related risks increasing during pretraining while adversarial risks do not. These results show that although scaling improves overall performance, it is not a universal equalizer. To support further study, we release all model checkpoints from this work to enable practitioners to measure*relative*alongside traditional scaling laws, in order to better prioritize robustness challenges in light of the bitter lesson111All models trained and used in this work are available on[HuggingFace](https://huggingface.co/collections/marin-community/all-marin-isoflops). The experimental code to train and evaluate these models is available in the[Marin Github](https://github.com/marin-community/marin/blob/main/experiments/isoflop_sweep.py), while analysis and plotting code is available in a separate[project repository](https://github.com/Helw150/relative-scaling-laws). Experimental logs for all experiments are viewable on the[Marin data browser](https://marin.community/data-browser/experiment/?path=gs://marin-us-central1/experiments/isoflop_sweep-fd786f.json).\n![Refer to caption](x1.png)Figure 1:Relative scaling law case studies.Scaling compute has uneven effects (illustrated here with models trained on DCLM> (Li et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib27)> )\nfrom101810^{18}\u2013102010^{20}FLOPs): (left) knowledge domains, (center) English variation, and (right) AI risk behaviours. We propose relative scaling laws as a method to measure which gaps close with scale and which persist or widen.\n## 1Introduction\nNeural scaling laws show that language model error typically decreases as a power law with increases in model size, data, and compute> (Hestness et\u00a0al., [> 2017\n](https://arxiv.org/html/2510.24626v1#bib.bib18)> ; Kaplan et\u00a0al., [> 2020\n](https://arxiv.org/html/2510.24626v1#bib.bib24)> ; Hoffmann et\u00a0al., [> 2022\n](https://arxiv.org/html/2510.24626v1#bib.bib19)> )\n. These trends suggest that \u201cbigger is better\u201d, with only rare cases of inverse scaling> (McKenzie et\u00a0al., [> 2023\n](https://arxiv.org/html/2510.24626v1#bib.bib37)> ; Sharma et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib53)> )\n. However, because these laws average over heterogeneous test distributions, the*rate*of improvement may not be uniform across subdomains> (Magnusson et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib33)> )\n.\nIn practice, gains from scale may favor some areas more than others, much as economic growth can deliver uneven returns across groups and increase inequality> (Piketty, [> 2015\n](https://arxiv.org/html/2510.24626v1#bib.bib44)> )\n.\nWe introduce*relative scaling laws*to study this dimension of scaling effects. Whereas traditional scaling laws describe absolute improvements, relative scaling laws quantify how*performance gaps*between settings evolve with scale. This separates disparities at small scales\n\u2013often shaped by confounding factors such as inherent data entropy \u2014from differences in improvement rate, which more directly capture the response to scale. The relative law is fit directly as a power law by regressing the ratio of treatment to baseline error on compute. This procedure is no harder than fitting absolute laws, but indicates whether gaps persist, narrow, or widen as compute increases. This provides a concrete lens on distributional consequences of scaling model compute\n, with implications for robustness, fairness, and risk.\nTo support such analyses, we train 255 decoder-only Transformers under matched-compute (*IsoFLOP*) budgets from101810^{18}to102010^{20}FLOPs, consisting of 85 models on each of three pretraining datasets. Training under fixed compute ensures that comparisons reflect the tradeoff between model size and data size, avoiding confounds that otherwise complicate scaling-law studies> (Hoffmann et\u00a0al., [> 2022\n](https://arxiv.org/html/2510.24626v1#bib.bib19)> ; Besiroglu et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib2)> )\n. The datasets span three distinct design philosophies\u2014permissively licensed corpora, filtered web data, and hybrid web+synthetic mixtures\u2014so that we can test whether scaling trends generalize across training data sources. We release the full model suite, providing a resource analogous to> Biderman et\u00a0al. (\n[> 2023\n](https://arxiv.org/html/2510.24626v1#bib.bib4)> )\nfor downstream scaling-law evaluation> (Roberts et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.24626v1#bib.bib48)> ; Hu et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib21)> )\n.\nFinally, we demonstrate the scope of relative scaling laws in three case studies. First, we analyze MMLU> (Hendrycks et\u00a0al., [> 2021\n](https://arxiv.org/html/2510.24626v1#bib.bib17)> )\nsub-domains to measure how knowledge scales across academic disciplines. Second, we evaluate robustness to English variation, testing generalization across regional English using the International Corpus of English (ICE)> (Greenbaum, [> 1996\n](https://arxiv.org/html/2510.24626v1#bib.bib12)> )\n. Third, we assess how relative risks emerge during pretraining using Anthropic\u2019s AI risk evaluations from> Perez et\u00a0al. (\n[> 2023\n](https://arxiv.org/html/2510.24626v1#bib.bib43)> )\n. Across all these settings, we fit both traditional and relative scaling laws.\nContributions.Our contributions combine conceptual, resource, and empirical components:\n1. 1.\nRelative scaling framework.We formalize*relative scaling laws*, which separate initial disparities from differences in improvement rate. Formulated as a power law, relative scaling provides a clear diagnostic of which distributions benefit the most from scaling.\n2. 2.\nOpen-source scaling suite.We train and release 255 decoder-only Transformers under IsoFLOP budgets from101810^{18}\u2013102010^{20}FLOPs across three corpora\u2014CommonPile> (Kandpal et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.24626v1#bib.bib23)> )\n,DCLM Baseline> (Li et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib27)> )\n, andNemotron-CC> (Su et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.24626v1#bib.bib55)> )\n. The suite enables reproducible study of both traditional and relative scaling laws.\n3. 3.\nEmpirical case studies.We apply relative scaling laws to three domains: academic knowledge (Massively Multitask Language Understanding benchmark; MMLU), linguistic variation (International Corpus of English; ICE), and AI risk (Anthropic Advanced AI Risk). Together, these studies show a range of relative scaling effects highlighting the non-uniformity of scale\u2019s impacts on distributional robustness.\n## 2Relative Scaling Laws\nRelative scaling laws follow directly from the assumptions of classical scaling laws. Absolute errorEEis assumed to decrease as a power law in scaleFF(e.g., FLOPs, tokens, or parameters),\n|E\u200b(F)=\u03b1\u200bF\u2212\u03b2,E(F)=\\\\alpha F^{-\\\\beta},||\nwith\u03b1&gt;0\\\\alpha&gt;&gt;0as the initial error level and\u03b2\u22650\\\\beta\\\\geq 0as the rate of improvement with scale> (Kaplan et\u00a0al., [> 2020\n](https://arxiv.org/html/2510.24626v1#bib.bib24)> )\n. These constants are empirically fit based on sample populations of training runs.\nIn order to relativize performance gains, we compare two conditions: a*baseline*(the reference, here the most favored under current practice) and a*treatment*of interest. Their relative errorGGis\n|G\u200b(F)=Etreatment\u200b(F)Ebaseline\u200b(F)=\u03b3\u200bF\u0394\u200b\u03b2G(F)=\\\\frac{E\\_{\\\\text{treatment}}(F)}{E\\_{\\\\text{baseline}}(F)}=\\\\gamma F^{\\\\Delta\\\\beta}||\nwhere\u03b3=\u03b1treatment/\u03b1baseline\\\\gamma=\\\\alpha\\_{\\\\text{treatment}}/\\\\alpha\\_{\\\\text{baseline}}captures the initial disparity and\u0394\u200b\u03b2=\u03b2baseline\u2212\u03b2treatment\\\\Delta\\\\beta=\\\\beta\\_{\\\\text{baseline}}-\\\\beta\\_{\\\\text{treatment}}the difference in improvement rates. If\u0394\u200b\u03b2&lt;0\\\\Delta\\\\beta&lt;&lt;0, the treatment improves faster and the gap narrows; if\u0394\u200b\u03b2&gt;0\\\\Delta\\\\beta&gt;&gt;0, it improves more slowly and the gap widens; if\u0394\u200b\u03b2=0\\\\Delta\\\\beta=0, the gap remains constant222In this work, we only interpret the slope if the sign is significant atP&lt;0.05P&lt;0.05by a bootstrap significance test. We recommend this as a best practice for interpreting\u0394\u200b\u03b2\\\\Delta\\\\beta..\nThis form parallels the subgroup laws of> Rolf et\u00a0al. (\n[> 2021\n](https://arxiv.org/html/2510.24626v1#bib.bib49)> )\n, who model subgroup loss as a mixture of power-law terms for in-group and total data. Our formulation is looser \u2014we do not require subgroup allocations \u2014but the sign of\u0394\u200b\u03b2\\\\Delta\\\\betastill forecasts whether gaps shrink or persist. While relative loss can correspond to small absolute differences at low loss, small absolute loss gaps can lead to large differences in downstream utility for large scale models> (Wei et\u00a0al., [> 2022\n](https://arxiv.org/html/2510.24626v1#bib.bib59)> ; Du et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib8)> )\nwhich motivates this scale-invariant metric rather than absolute disparity> (Yeh et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib65)> )\n333Beyond test-distribution disparities, relative scaling can be used to compare mod",
          "original_query": "Scaling Laws for Neural Language Models (Kaplan et al., 2020)",
          "cleaned_query": "Scaling Laws for Neural Language Models",
          "content_quality": "good_from_html"
        }
      ],
      "generated_ideas": [
        "Goal-Conditioned CURL with UVFA-Aligned Contrastive Objectives\nLearn pixel embeddings with CURL while jointly training a UVFA \\(Q(s,g)\\) so that contrastive positives are not just augmented views of the same frame, but also states predicted to be valuable for the same goal. Evaluate whether value-aware representations improve generalization to unseen goals in goal-conditioned control benchmarks (e.g., DMControl with randomized goal locations).",
        "Hindsight-Contrastive Replay (HCR): Using HER Relabels as Contrastive Positives\nExtend HER by treating \\((s_t, g)\\) and \\((s_t, g')\\) where \\(g'\\) is a hindsight goal achieved later as a structured positive pair in a joint state\u2013goal embedding space. Optimize an InfoNCE loss that encourages invariance across goal relabelings that are behaviorally equivalent, and test whether this reduces sparse-reward sample complexity beyond HER alone.",
        "DIAYN-CURL: Pixel-Based Unsupervised Skill Discovery with Contrastive Skill Separation\nReplace DIAYN\u2019s discriminator features with CURL-style representations learned from raw pixels, and add an auxiliary contrastive loss where positives share the same skill id \\(z\\) and negatives come from different skills. This directly enforces skill separability in representation space while retaining DIAYN\u2019s maximum-entropy objective, enabling unsupervised skills in visual robotics tasks without privileged state.",
        "Augmentation Policy Search for RL Contrastive Learning (SimCLR\u2192CURL Transfer)\nSystematically search or learn augmentation compositions (crop, color, blur, camera jitter, distractors) that maximize downstream RL performance rather than ImageNet linear probing. Use SimCLR-style ablations (temperature, projection head depth, strength of augmentations) but optimize for control-suite returns at fixed interaction budgets to produce an \u201cRL-optimal\u201d augmentation recipe.",
        "Relative Scaling Laws for Representation Learning in RL (Compute vs. Gap Closing)\nTrain families of CURL-based agents across matched-compute (IsoFLOP-like) budgets while measuring *relative* performance gaps between task clusters (e.g., easy vs. hard exploration, clean vs. distractor backgrounds, seen vs. unseen goals). Fit relative scaling laws to identify which robustness gaps close with scale and which persist, guiding where algorithmic changes are needed beyond more compute.",
        "Compute-Optimal Tradeoffs: Representation Capacity vs. Replay Diversity under Fixed FLOPs\nUnder a fixed compute budget, vary encoder size/training steps (\u00e0 la SimCLR \u201cmore steps helps\u201d) versus replay diversity (more environments/augmentations/goal relabels). Quantify regimes where additional contrastive pretraining steps dominate versus regimes where broader replay distributions dominate, producing actionable budgeting rules for pixel-based off-policy RL.",
        "Value-Consistent Data Augmentations: Preventing Reward-Destructive Invariances\nDesign a criterion that penalizes augmentations that change predicted goal-conditioned values \\(Q(s,g)\\) while still encouraging invariance to nuisance factors (lighting, viewpoint). Implement as a constrained contrastive objective: maximize agreement across augmentations subject to small UVFA value deviation, and test on tasks where naive augmentations can remove reward-relevant cues.",
        "Hierarchical Skill Composition with UVFA-Conditioned Skill Selection (DIAYN\u2192Goal RL)\nPretrain a DIAYN skill library, then train a UVFA over skills \\(Q(s,g,z)\\) that selects skills to reach new goals with sparse rewards (option-level control). Compare to flat HER+UVFA baselines by measuring zero-shot and few-shot goal generalization, especially when goals are specified visually (goal images).",
        "Contrastive Credit Assignment Across Time: Using Future-State Positives for Control\nModify CURL so positives can be \\((s_t, s_{t+k})\\) within the same trajectory (temporal views) in addition to image augmentations, with \\(k\\) sampled from a goal-dependent distribution (larger \\(k\\) for long-horizon tasks). This aims to encode controllable aspects that persist and predict future outcomes, improving long-horizon planning without an explicit dynamics model.",
        "Distribution-Shift Diagnostics for RL via Relative Scaling Gaps (Distractors, Viewpoints, Dialects-of-World)\nBuild an evaluation suite analogous to \u201cregional dialects\u201d in LLMs but for RL observations: different render styles, camera lenses, textures, sensor noise, and UI overlays. Track relative scaling laws for each shift while training CURL+HER/UVFA agents, identifying which shifts worsen with scale (capability increases without robustness) and motivating targeted regularizers or data augmentation strategies."
      ],
      "generated_ideas_raw": "1. **Goal-Conditioned CURL with UVFA-Aligned Contrastive Objectives**  \n   Learn pixel embeddings with CURL while jointly training a UVFA \\(Q(s,g)\\) so that contrastive positives are not just augmented views of the same frame, but also states predicted to be valuable for the same goal. Evaluate whether value-aware representations improve generalization to unseen goals in goal-conditioned control benchmarks (e.g., DMControl with randomized goal locations).\n\n2. **Hindsight-Contrastive Replay (HCR): Using HER Relabels as Contrastive Positives**  \n   Extend HER by treating \\((s_t, g)\\) and \\((s_t, g')\\) where \\(g'\\) is a hindsight goal achieved later as a structured positive pair in a joint state\u2013goal embedding space. Optimize an InfoNCE loss that encourages invariance across goal relabelings that are behaviorally equivalent, and test whether this reduces sparse-reward sample complexity beyond HER alone.\n\n3. **DIAYN-CURL: Pixel-Based Unsupervised Skill Discovery with Contrastive Skill Separation**  \n   Replace DIAYN\u2019s discriminator features with CURL-style representations learned from raw pixels, and add an auxiliary contrastive loss where positives share the same skill id \\(z\\) and negatives come from different skills. This directly enforces skill separability in representation space while retaining DIAYN\u2019s maximum-entropy objective, enabling unsupervised skills in visual robotics tasks without privileged state.\n\n4. **Augmentation Policy Search for RL Contrastive Learning (SimCLR\u2192CURL Transfer)**  \n   Systematically search or learn augmentation compositions (crop, color, blur, camera jitter, distractors) that maximize downstream RL performance rather than ImageNet linear probing. Use SimCLR-style ablations (temperature, projection head depth, strength of augmentations) but optimize for control-suite returns at fixed interaction budgets to produce an \u201cRL-optimal\u201d augmentation recipe.\n\n5. **Relative Scaling Laws for Representation Learning in RL (Compute vs. Gap Closing)**  \n   Train families of CURL-based agents across matched-compute (IsoFLOP-like) budgets while measuring *relative* performance gaps between task clusters (e.g., easy vs. hard exploration, clean vs. distractor backgrounds, seen vs. unseen goals). Fit relative scaling laws to identify which robustness gaps close with scale and which persist, guiding where algorithmic changes are needed beyond more compute.\n\n6. **Compute-Optimal Tradeoffs: Representation Capacity vs. Replay Diversity under Fixed FLOPs**  \n   Under a fixed compute budget, vary encoder size/training steps (\u00e0 la SimCLR \u201cmore steps helps\u201d) versus replay diversity (more environments/augmentations/goal relabels). Quantify regimes where additional contrastive pretraining steps dominate versus regimes where broader replay distributions dominate, producing actionable budgeting rules for pixel-based off-policy RL.\n\n7. **Value-Consistent Data Augmentations: Preventing Reward-Destructive Invariances**  \n   Design a criterion that penalizes augmentations that change predicted goal-conditioned values \\(Q(s,g)\\) while still encouraging invariance to nuisance factors (lighting, viewpoint). Implement as a constrained contrastive objective: maximize agreement across augmentations subject to small UVFA value deviation, and test on tasks where naive augmentations can remove reward-relevant cues.\n\n8. **Hierarchical Skill Composition with UVFA-Conditioned Skill Selection (DIAYN\u2192Goal RL)**  \n   Pretrain a DIAYN skill library, then train a UVFA over skills \\(Q(s,g,z)\\) that selects skills to reach new goals with sparse rewards (option-level control). Compare to flat HER+UVFA baselines by measuring zero-shot and few-shot goal generalization, especially when goals are specified visually (goal images).\n\n9. **Contrastive Credit Assignment Across Time: Using Future-State Positives for Control**  \n   Modify CURL so positives can be \\((s_t, s_{t+k})\\) within the same trajectory (temporal views) in addition to image augmentations, with \\(k\\) sampled from a goal-dependent distribution (larger \\(k\\) for long-horizon tasks). This aims to encode controllable aspects that persist and predict future outcomes, improving long-horizon planning without an explicit dynamics model.\n\n10. **Distribution-Shift Diagnostics for RL via Relative Scaling Gaps (Distractors, Viewpoints, Dialects-of-World)**  \n   Build an evaluation suite analogous to \u201cregional dialects\u201d in LLMs but for RL observations: different render styles, camera lenses, textures, sensor noise, and UI overlays. Track relative scaling laws for each shift while training CURL+HER/UVFA agents, identifying which shifts worsen with scale (capability increases without robustness) and motivating targeted regularizers or data augmentation strategies.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Goal-Conditioned CURL with UVFA-Aligned Contrastive Objectives\nLearn pixel embeddings with CURL while jointly training a UVFA \\(Q(s,g)\\) so that contrastive positives are not just augmented views of t",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hindsight-Contrastive Replay (HCR): Using HER Relabels as Contrastive Positives\nExtend HER by treating \\((s_t, g)\\) and \\((s_t, g')\\) where \\(g'\\) is a hindsight goal achieved later as a structured po",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "DIAYN-CURL: Pixel-Based Unsupervised Skill Discovery with Contrastive Skill Separation\nReplace DIAYN\u2019s discriminator features with CURL-style representations learned from raw pixels, and add an auxili",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Augmentation Policy Search for RL Contrastive Learning (SimCLR\u2192CURL Transfer)\nSystematically search or learn augmentation compositions (crop, color, blur, camera jitter, distractors) that maximize dow",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Relative Scaling Laws for Representation Learning in RL (Compute vs. Gap Closing)\nTrain families of CURL-based agents across matched-compute (IsoFLOP-like) budgets while measuring *relative* performan",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Compute-Optimal Tradeoffs: Representation Capacity vs. Replay Diversity under Fixed FLOPs\nUnder a fixed compute budget, vary encoder size/training steps (\u00e0 la SimCLR \u201cmore steps helps\u201d) versus replay ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Value-Consistent Data Augmentations: Preventing Reward-Destructive Invariances\nDesign a criterion that penalizes augmentations that change predicted goal-conditioned values \\(Q(s,g)\\) while still enco",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hierarchical Skill Composition with UVFA-Conditioned Skill Selection (DIAYN\u2192Goal RL)\nPretrain a DIAYN skill library, then train a UVFA over skills \\(Q(s,g,z)\\) that selects skills to reach new goals w",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Contrastive Credit Assignment Across Time: Using Future-State Positives for Control\nModify CURL so positives can be \\((s_t, s_{t+k})\\) within the same trajectory (temporal views) in addition to image ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Distribution-Shift Diagnostics for RL via Relative Scaling Gaps (Distractors, Viewpoints, Dialects-of-World)\nBuild an evaluation suite analogous to \u201cregional dialects\u201d in LLMs but for RL observations:",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 40,
      "paper_title": "Depth-Bounds for Neural Networks via the Braid Arrangement",
      "contribution": "For ReLU (and related maxout) networks compatible with the braid fan, the paper proves a non-constant lower bound \u2126(log log d) on the number of hidden layers needed to compute the maximum of d numbers, gives a combinatorial proof that max of 5 numbers needs three hidden layers under the same compatibility assumption, and supplies a tighter constructive upper bound in the maxout setting (rank-3 followed by rank-2 suffices for max of 7).",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10964,
      "output_tokens": 917,
      "predecessor_details": [
        {
          "success": true,
          "title": "Adaptive control of the Wang-Sun four-scroll chaotic system with ...",
          "url": "https://ieeexplore.ieee.org/document/7238377/",
          "content": "Adaptive control of the Wang-Sun four-scroll chaotic system with unknown parameters \\| IEEE Conference Publication \\| IEEE Xplore\n\n### IEEE Account\n\n- [Change Username/Password](https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Update Address](https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Purchase Details\n\n- [Payment Options](https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Order History](https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n\n### Profile Information\n\n- [Communications Preferences](https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Profession and Education](https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Technical Interests](https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Need Help?\n\n- **US & Canada:** +1 800 678 4333\n- **Worldwide:** +1 732 981 0060\n\n- [Contact & Support](https://ieeexplore.ieee.org/xpl/contact)\n\n- [About IEEE _Xplore_](https://ieeexplore.ieee.org/Xplorehelp/about-ieee-xplore.html)\n- [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n- [Help](https://ieeexplore.ieee.org/Xplorehelp/Help_start.html)\n- [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/accessibility-statement.html)\n- [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/Help_Terms_of_Use.html)\n- [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n- [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n- [Privacy & Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\n\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n\n\u00a9 Copyright 2024 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.",
          "original_query": "Wang and Sun (2005) \u2014 reduction of CPWL representation to maxima of affine terms",
          "cleaned_query": "Wang and Sun",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] NBER WORKING PAPER SERIES THE CHANGING STRUCTURE ...",
          "url": "https://www.nber.org/system/files/working_papers/w25893/w25893.pdf",
          "content": "NBER WORKING PAPER SERIES\nTHE CHANGING STRUCTURE OF AMERICAN INNOVATION:\nSOME CAUTIONARY REMARKS FOR ECONOMIC GROWTH\nAshish Arora\nSharon Belenzon\nAndrea Patacconi\nJungkyu Suh\nWorking Paper 25893\nhttp://www.nber.org/papers/w25893\nNATIONAL BUREAU OF ECONOMIC RESEARCH\n1050 Massachusetts Avenue\nCambridge, MA 02138\nMay 2019\nThe views expressed herein are those of the authors and do not necessarily reflect the views of the\nNational Bureau of Economic Research.\nNBER working papers are circulated for discussion and comment purposes. They have not been peer\u0002reviewed or been subject to the review by the NBER Board of Directors that accompanies official\nNBER publications.\n\u00a9 2019 by Ashish Arora, Sharon Belenzon, Andrea Patacconi, and Jungkyu Suh. All rights reserved.\nShort sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided\nthat full credit, including \u00a9 notice, is given to the source.\nThe Changing Structure of American Innovation: Some Cautionary Remarks for Economic\nGrowth\nAshish Arora, Sharon Belenzon, Andrea Patacconi, and Jungkyu Suh\nNBER Working Paper No. 25893\nMay 2019, Revised August 2019\nJEL No. O3\nABSTRACT\nA defiining feature of modern economic growth is the systematic application of science to advance\ntechnology. However, despite sustained progress in scientific knowledge, recent productivity growth\nin the U.S. has been disappointing. We review major changes in the American innovation ecosystem\nover the past century. The past three decades have been marked by a growing division of labor between\nuniversities focusing on research and large corporations focusing on development. Knowledge produced\nby universities is not often in a form that can be readily digested and turned into new goods and services.\nSmall firms and university technology transfer offices cannot fully substitute for corporate research,\nwhich had integrated multiple disciplines at the scale required to solve significant technical problems.\nTherefore, whereas the division of innovative labor may have raised the volume of science by universities,\nit has also slowed, at least for a period of time, the transformation of that knowledge into novel products\nand processes.\nAshish Arora\nFuqua School of Business\nDuke University\nBox 90120\nDurham, NC 27708-0120\nand NBER\nashish.arora@duke.edu\nSharon Belenzon\nFuqua School of Business\nDuke University\n100 Fuqua Drive\nDurham, NC 27708\nand NBER\nsharon.belenzon@duke.edu\nAndrea Patacconi\nNorwich Business School\nUniversity of East Anglia\nNorwich, NR4 7TJ\nUnited Kingdom\nA.Patacconi@uea.ac.uk\nJungkyu Suh\nDuke University\nFuqua School of Business\njungkyu.suh@duke.edu\n1 Introduction\nA defining feature of modern economic growth is the systematic application of science to advance\ntechnology. Many innovations that spurred economic growth in the twentieth century, including syn\u0002thetic fibers, plastics, integrated circuits, and gene therapy, originated from advances in the natural\nsciences, engineering and medicine. Science, by producing \u201ca potential for technology far greater than\nexisted previously,\u201d clearly distinguishes modern economic growth from previous economic epochs\n(Kuznets, 1971).\nHowever, despite sustained increases in the quantity of scientific knowledge, productivity growth\nin most advanced economies has stagnated in recent decades in comparison to a \u201cgolden age\u201d in the\nmid-twentieth century. Using data from the United States, Gordon (2016) shows that real GDP per\nhour (i.e., labor productivity) grew substantially in the middle of the twentieth century, from 1.79\npercent per year between 1870 and 1920 to 2.82 percent per year between 1920 and 1970. However,\nin the most recent period (1970-2014), productivity grew by a modest 1.62 percent per year. Gordon\nconcludes that productivity rose between 1920 and 1970 largely because of significant technological\nprogress, but more recently technical advance has been much less potent in spurring growth. This\nslowdown is surprising given the sustained expansion of scientific input (measured in terms of research\ndollars spent) and output (measured by academic articles published) from American academia, as\nshown in figure 1.1\nGordon attributes the rapid pace of technological progress in 1920-1970 to the development and\nextension of earlier fundamental technologies, such as the internal combustion engine and electricity.\nThis process, which was often accompanied by important advances in science and engineering, was\nlargely carried out by researchers working in corporate labs, which, by the 1920s, had replaced\nindividual entrepreneurs as the primary source of American invention. As Gordon (2016, p.571-2)\n1\nIndeed, Bloom et al. (2017) present evidence across a number of sectors showing that research productivity in the\nU.S. has declined since the 1970s. For instance, maintaining the exponential growth in semiconductor performance\n(otherwise known as \u201cMoore\u2019s Law\u201d) in 2014 required around 18 times the number of researchers it used to take in\n1971. While growth rates for yields per acre for corns, soybeans, cotton, and wheat have averaged around 1.5 percent,\nthe number of researchers in the agriculture sector has grown by a factor between 3 (wheat) and 25 (soybeans), a\nresearch productivity decline of about 4 to 6 percent per year. In the life sciences, the number of researchers has\nbeen rising by 6 percent annually, while research productivity measured by the discovery of new molecular entities per\nnumber of researchers has been falling by 3.5 percent per year.\n1\nFigure 1: U.S. Scientific Investment and Output (1980-2013)\nNotes: Doctorates Awarded in S&E are calculated from the NSF\u2019s Survey of Earned Doctorates and excludes degrees in the Social\nsciences. Number of S&E Publications are from the Clarivate Web of Science and includes all scientific articles in the Science Citation\nIndex-Expanded (SCI-EXPANDED) and Conference Proceedings Citation Index-Science (CPCSI-S) with a U.S. author from 1980 to\n2013. U.S. Research Expenditure figures are calculated from the National Patterns of R&D Resources: 2014-15 Data update. NSF\n17-311. tables and includes both basic and applied research expenditure. Figures are adjusted to 2016 dollars using GDP deflator from\nthe World Bank National Accounts dataset.\nwrites:\n\u201cMuch of the early development of the automobile culminating in the powerful Chevrolets\nand Buicks of 1940-41 was achieved at the GM corporate research labs. Similarly, much of\nthe development of the electronic computer was carried out in the corporate laboratories\nof IBM, Bell Labs, and other large firms. The transistor, the fundamental building block\nof modern electronics and digital innovation, was invented by a team led by William\nShockley at Bell Labs in late 1947. The corporate R&D division of IBM pioneered most\nof the advances of the mainframe computer era from 1950 to 1980. Improvements in\nconsumer electric appliances occurred at large firms such as General Electric, General\nMotors and Whirlpool, while RCA led the early development of television.\u201d\nBy the 1980s, however, many corporations began to look to universities and small start-ups\nfor ideas and new products.2 Large corporations\u2019 reliance on externally sourced inventions grew,\nand many leading Western corporations began to withdraw from scientific research (Mowery, 2009;\n2A good example is IBM, which on November 6, 1980 signed a contract with a then small firm, Microsoft, for\nthe development of its operating systems. Microsoft itself developed its operating system (eventually named the IBM\nPC-DOS) building on the operating system of another small company, Seattle Computer Products.\n2\nFigure 2: Business funded and performed research in the United States (1953-2015)\nNotes: Data for this graph is sourced from the National Patterns of R&D Resources: 2014-15 Data update. NSF 17-311. from\nthe National Science Foundation, National Center for Science and Engineering Statistics. 2017. Arlington, VA. Available at\nhttps://www.nsf.gov/statistics/2017/nsf17311/.\nArora et al., 2018). Some corporate labs were shut down and others spun-off as independent en\u0002tities. Bell Labs had been separated from its parent company AT&T and placed under Lucent\nin 1996; Xerox PARC had also been spun off into a separate company in 2002. Others had been\ndownsized: IBM under Louis Gerstner re-directed research toward more commercial applications in\nthe mid-90s (Bhaskarabhatla and Hegde, 2014).3 A more recent example is DuPont\u2019s closing of its\nCentral Research & Development Lab in 2016. Established in 1903, DuPont research rivaled that\nof top academic chemistry departments. In the 1960s, DuPont\u2019s central R&D unit published more\narticles in the Journal of the American Chemical Society than MIT and Caltech combined. How\u0002ever, in the 1990s, DuPont\u2019s attitude toward research changed and after a gradual decline in scientific\npublications, the company\u2019s management closed its Central Research and Development Lab in 2016.4\nThese examples are backed by systematic evidence. NSF data indicate that share of research\n(both basic and applied) in total business R&D in the U.S. fell from about 30 percent in 1985 to\nbelow 20 percent in 2015 (figure 2). The figure also shows that the absolute amount of research in\nindustry, after increasing over the 1980s, barely grew over the 20 year period between 1990 to 2010.\nOther data show the same decline. Utilizing data on scientific publications, Arora et al. (2018) show\n3According to personal communications with Ralph Gomory (former research director and Senior Vice President for\nScience & Technology at IBM), IBM even downplayed to investors the discovery of the scanning tunneling microscope\n(which earned Gerd Binnig and Heinrich Rohrer of the IBM Zurich Research Laboratory the Nobel prize in physics in\n1986), for fear of a drop in share price.\n4https://cen.acs.org/articles/94/i1/DuPont-Shutting-Central-Research.html\n3\nthat the number of publications per firm fell at a rate of 20 per",
          "original_query": "Arora et al. (2018) \u2014 ReLU networks exactly represent CPWL functions and a log2(d+1) upper bound",
          "cleaned_query": "Arora et al.",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "A Study of Learnability in Software as Perceived by ...",
          "url": "https://digitalcommons.odu.edu/cgi/viewcontent.cgi?article=1147&context=stemps_etds",
          "content": "Old Dominion University \nODU Digital Commons \nSTEMPS Theses & Dissertations STEM Education & Professional Studies \nSummer 2024 \nA Study of Learnability in Softwar A Study of Learnability in Software as Perceived by Practitioners actitioners \nin User Experience and Learning Design Professions \nCourtney N. Miller \nOld Dominion University, cnmiller.phd@gmail.com \nFollow this and additional works at: https://digitalcommons.odu.edu/stemps_etds \nPart of the Adult and Continuing Education Commons, Educational Technology Commons, \nInstructional Media Design Commons, and the Organizational Behavior and Theory Commons \nRecommended Citation \nMiller, Courtney N.. \"A Study of Learnability in Software as Perceived by Practitioners in User Experience \nand Learning Design Professions\" (2024). Doctor of Philosophy (PhD), Dissertation, STEM Education & \nProfessional Studies, Old Dominion University, DOI: 10.25777/244n-r853 \nhttps://digitalcommons.odu.edu/stemps_etds/147 \nThis Dissertation is brought to you for free and open access by the STEM Education & Professional Studies at ODU \nDigital Commons. It has been accepted for inclusion in STEMPS Theses & Dissertations by an authorized \nadministrator of ODU Digital Commons. For more information, please contact digitalcommons@odu.edu. \nA STUDY OF LEARNABILITY IN SOFTWARE AS PERCEIVED BY \nPRACTITIONERS IN USER EXPERIENCE AND LEARNING DESIGN PROFESSIONS\nby\nCourtney N. Miller\nB.A. May 2009, University of South Florida\nM.L.I.S July 2013, San Jose State University\nA Dissertation Submitted to the Faculty of\nOld Dominion University in Partial Fulfillment of the\nRequirements for the Degree of\nDOCTOR OF PHILOSOPHY IN EDUCATION\nOCCUPATIONAL AND TECHNICAL STUDIES\nOLD DOMINION UNIVERSITY\nAugust 2024\nApproved by:\nMickey Kosloski (Director)\nShanan Chappell Moots (Member)\nVirginia Jones (Member)\nABSTRACT\nA STUDY OF LEARNABILITY IN SOFTWARE AS PERCEIVED BY PRACTITIONERS IN \nUSER EXPERIENCE AND LEARNING DESIGN PROFESSIONS\nCourtney N. Miller\nOld Dominion University, 2024\nDirector: Mickey Kosloski\nIn the technology space, there are many factors that contribute to the marketability of \nsoftware, including pricing and overall usefulness of the product (Jayathilaka, 2021). Many \nfactors contribute to how usable a software is, including satisfaction, error prevention, \nmemorability, efficiency, and learnability (Nielsen, 1994b). Learnability is one factor that may \nbe affected or addressed by both user experience (UX) and learning design (LD) professional \ngroups. While both fields address learnability, very few studies have been conducted to look at \nthe UX interpretation of learnability as it relates to the LD interpretation (Elliott et al., 2002; Li \net al., 2023). This study addressed the gap in understanding between UX and LD professionals \nregarding learnability in software by exploring the degree of consensus on the importance of \nvarious learnability factors.\nA survey was distributed to UX and LD professionals, comprising open-ended, multiple\u0002choice, categorical, and Likert-type questions about demographics, perceptions of usability and \nlearnability, and the importance of specific learnability attributes. Data were analyzed using \ndescriptive statistics, Kaiser-Meyer-Olkin (KMO) and exploratory factor analysis, \nKrippendorff\u2019s Alpha, independent samples t-tests, and chi-square analyses. The analysis \nshowed significant differences in how UX and LD professionals prioritize learnability factors, \nsuggesting potential for collaborative improvement. These findings highlight the need for a \nunified framework to define and assess learnability in software and lays the groundwork for \ndeveloping integrated assessment tools and methodologies applicable across both fields to \nsupport more effective software design and training.\niv\nCopyright, 2024, by Courtney N. Miller, All Rights Reserved.\nv\nThis dissertation is dedicated to my husband, Jacob, who inspires me daily, supports me \ntirelessly, and lifts me high enough to touch the stars. To my family, Gina, Tim, and Alex, who \nraised me to exist loudly, to care about people, and to always ask \u201cwhere\u2019s the pony?\u201d This is \nalso dedicated to my friends and chosen family who have taken this roller coaster with me: \nJanine, Ashley, Jeremy, Suzanne, Maria, and Anna. And, finally, to Lucia, my stalwart study \nbuddy from Lean Six Sigma and beyond.\nvi\nACKNOWLEDGMENTS\nThere are many people that have played a key role in the successful completion of my\ndissertation. I will be forever grateful to all who took part in the journey. First and foremost, \nthanks to my committee who have spent countless hours talking through my ideas, helping me \nnavigate Qualtrics, and reading and re-reading my paper. To my chair, Dr. Kosloski, I appreciate \nyour willingness to help me reset course and navigate back to shore. To Dr. Chappell Moots for \nensuring my methodology could withstand any storm (and so much more). Lastly, thanks to Dr. \nJones, for joining this motley crew at the eleventh hour.\nMany thanks also go to my academic mentors who have taught me how to conduct \nresearch and collaborate across institutions. To Jamie Price and Mona Thiss, thank you for \ngetting me involved in scholarship and filling my CV early in my career. To Dr. Rob Moore, \nthank you for your mentorship as my first graduate advisor and beyond. \nTo my thought partners and colleagues who spent way more time talking about \nlearnability and usability than they ever wanted, I thank you as well! Thanks to Dr. Frank Dane, \nDr. Yvonne Earnshaw, Dr. Colin Gray, Dr. Marisa Exter, and Dr. Matthew Schmidt for letting \nme bounce ideas off you during the initial formation of my dissertation topic. Finally, thank you \nto Asterisk Loftis, my UX Design guru who started me down this rabbit hole. \nvii\nTABLE OF CONTENTS\nPage\nLIST OF TABLES......................................................................................................................................... VIII\nLIST OF FIGURES............................................................................................................................................ X\nChapter\n1. INTRODUCTION........................................................................................................................................... 1\nBACKGROUND, SIGNIFICANCE, AND THEORETICAL FRAMEWORK.................................................................... 6\nPROCEDURES ................................................................................................................................................... 12\nDEFINITIONS OF TERMS................................................................................................................................... 14\nSUMMARY AND OVERVIEW OF CHAPTERS ...................................................................................................... 17\n2. REVIEW OF LITERAURE......................................................................................................................... 19\nUSABILITY....................................................................................................................................................... 20\nLEARNABILITY ................................................................................................................................................ 35\n3. METHODOLOGY........................................................................................................................................ 47\nPOPULATION.................................................................................................................................................... 48\nRESEARCH VARIABLES.................................................................................................................................... 50\nINSTRUMENT DESIGN ...................................................................................................................................... 51\nPROCEDURES ................................................................................................................................................... 56\nMETHODS OF DATA COLLECTION ................................................................................................................... 57\nSTATISTICAL ANALYSIS .................................................................................................................................. 57\nSUMMARY ....................................................................................................................................................... 60\n4. RESULTS....................................................................................................................................................... 62\n5. CONCLUSIONS AND RECOMMENDATIONS ...................................................................................... 76\nREFERENCES .................................................................................................................................................. 90\nAPPENDICES\nA. HUMAN SUBJECTS REVIEW APPROVAL ........................................................................................ 117\nB. MAJOR USABILITY ASSESSMENT METHODS AND CITATIONS................................................. 118\nC. SURVEY INSTRUMENT ....................................................................................................................... 121\nD. LEARNABILITY ATTRIBUTE OPERATIONAL DEFINITIONS SURVEY SUPPLEMENT............ 125\nE. CALLS FOR PARTICIPATION.............................................................................................................. 128\nF. ASYNCHRONOUS THINK ALOUD FEEDBACK................................................................................ 130\nVITA................................................................................................................................................................. 132\nviii\nLIST OF TABLES\nTable Page\n1. Comparison ",
          "original_query": "Bakaev et al. (2025b) \u2014 improved upper bound (\u2308log3(d\u22121)\u2309+1) for CPWL representation",
          "cleaned_query": "Bakaev et al.",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Ingo Hertrich",
          "url": "https://scholar.google.com/citations?user=ZRb5TmAAAAAJ&hl=en",
          "content": "Consensus paper: language and the cerebellum: an ongoing enigma P Mari\u00ebn, H Ackermann, M Adamaszek, CHS Barwood, A Beaton, ... The Cerebellum 13, 386-410, 2014 572 2014 Identification of emotional intonation evaluated by fMRI D Wildgruber, A Riecker, I Hertrich, M Erb, W Grodd, T Ethofer, ... Neuroimage 24 (4), 1233-1241, 2005 476 2005 fMRI reveals two distinct cerebral networks subserving speech motor control A Riecker, K Mathiak, D Wildgruber, M Erb, I Hertrich, W Grodd, ... Neurology 64 (4), 700-706, 2005 399 2005 The role of the supplementary motor area for speech and language processing I Hertrich, S Dietrich, H Ackermann Neuroscience &amp; Biobehavioral Reviews 68, 602-610, 2016 323 2016 Distinct frontal regions subserve evaluation of linguistic and emotional aspects of speech intonation D Wildgruber, I Hertrich, A Riecker, M Erb, S Anders, W Grodd, ... Cerebral cortex 14 (12), 1384-1389, 2004 254 2004 Oral diadochokinesis in neurological dysarthrias H Ackermann, I Hertrich, T Hehr Folia phoniatrica et logopaedica 47 (1), 15-23, 1995 243 1995 The temporal control of repetitive articulatory movements in Parkinson's disease H Ackermann, J Konczak, I Hertrich Brain and language 56 (2), 312-319, 1997 216 1997 The role of the dorsolateral prefrontal cortex for speech and language processing I Hertrich, S Dietrich, C Blum, H Ackermann Frontiers in human neuroscience 15, 645209, 2021 193 2021 The contribution of mesiofrontal cortex to the preparation and execution of repetitive syllable productions: An fMRI study B Brendel, I Hertrich, M Erb, A Lindner, A Riecker, W Grodd, H Ackermann Neuroimage 50 (3), 1219-1230, 2010 163 2010 The contribution of the cerebellum to speech processing H Ackermann, I Hertrich Journal of Neurolinguistics 13 (2-3), 95-116, 2000 160 2000 Categorical speech perception in cerebellar disorders H Ackermann, S Gr\u00e4ber, I Hertrich, I Daum Brain and language 60 (2), 323-331, 1997 160 1997 The margins of the language network in the brain I Hertrich, S Dietrich, H Ackermann Frontiers in Communication 5, 519955, 2020 149 2020 Cerebellum and speech perception: a functional magnetic resonance imaging study K Mathiak, I Hertrich, W Grodd, H Ackermann Journal of Cognitive Neuroscience 14 (6), 902-912, 2002 146 2002 Discrimination of temporal information at the cerebellum: functional magnetic resonance imaging of nonverbal auditory memory K Mathiak, I Hertrich, W Grodd, H Ackermann Neuroimage 21 (1), 154-162, 2004 138 2004 Speech rate and rhythm in cerebellar dysarthria: An acoustic analysis of syllabic timing H Ackermann, I Hertrich Folia phoniatrica et logopaedica 46 (2), 70-78, 1994 133 1994 Control of repetitive lip and finger movements in Parkinson's disease: influence of external timing signals and simultaneous execution on motor performance J Konczak, H Ackermann, I Hertrich, S Spieker, J Dichgans Movement Disorders: Official Journal of the Movement Disorder Society 12 (5\u00a0\u2026, 1997 125 1997 Gender-specific vocal dysfunctions in Parkinson's disease: electroglottographic and acoustic analyses I Hertrich, H Ackermann Annals of Otology, Rhinology &amp; Laryngology 104 (3), 197-202, 1995 122 1995 Hearing lips: gamma-band activity during audiovisual speech perception J Kaiser, I Hertrich, H Ackermann, K Mathiak, W Lutzenberger Cerebral Cortex 15 (5), 646-653, 2005 117 2005 Psycholinguistic evidence for presuppositions: On-line and off-line data S Tiemann, M Schmid, N Bade, B Rolke, I Hertrich, H Ackermann, ... Proceedings of Sinn und Bedeutung 15, 581-596, 2011 115 2011 Voice onset time in ataxic dysarthria H Ackermann, I Hertrich Brain and language 56 (3), 321-333, 1997 112 1997",
          "original_query": "Hertrich et al. (2023) \u2014 conjectures and reductions tying general CPWL depth to the max primitive",
          "cleaned_query": "Hertrich et al.",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Strategic Choices for Matching Platforms",
          "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950",
          "content": "[Skip to main content](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950#maincontent)\n\n[![PDF icon](https://static.ssrn.com/cfincludes/img/icons/icon-adobe-pdf.svg)Download This Paper](https://papers.ssrn.com/sol3/Delivery.cfm/5027950.pdf?abstractid=5027950&mirid=1)\n\n[Open PDF in Browser](https://papers.ssrn.com/sol3/Delivery.cfm/5027950.pdf?abstractid=5027950&mirid=1&type=2)\n\n[Add Paper to My Library](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950)\n\nShare:\n\nPermalink\n\nUsing these links will ensure access to this page indefinitely\n\n[Copy URL](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950)\n\n[Copy DOI](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950)\n\n# Strategic Choices for Matching Platforms\n\n[SMU Cox School of Business Research Paper No. 24-15](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950)\n\n9 PagesPosted: 11 Dec 2024Last revised: 23 Nov 2024\n\n[See all articles by Amit Basu](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=470814)\n\n## [Amit Basu](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=470814)\n\nSouthern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM)\n\n## [Sreekumar R. Bhaskaran](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=343405)\n\nSouthern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM)\n\n## [Rajiv Mukherjee](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=2490216)\n\nTexas A&M University - Mays Business School\n\nDate Written: November 20, 2024\n\n### Abstract\n\nOnline matching platforms dramatically enhance the ability of both individuals and organizations to find matches for their personal or business needs. At the same time, some of the key issues in the design of these platforms are not well-understood, leading to poor strategic choices by some early entrants. For matching platforms to succeed, three core services stand out as critical value drivers: search, authentication, and compatibility counseling. To maximize their value, executives planning matching platforms can learn from the strategies of online dating platforms, where quality matches and user experience are essential for success. Then, drawing upon our research on matching platforms, we lay out some of the key design choices, and provide guidance in the form of a decision framework.\n\n**Keywords:** search, compatibility, counseling, authentication, matching platforms, dating markets\n\n**Suggested Citation:** [Suggested Citation](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950)\n\nBasu, Amit and Bhaskaran, Sreekumar R. and Mukherjee, Rajiv, Strategic Choices for Matching Platforms (November 20, 2024). SMU Cox School of Business Research Paper No. 24-15, Available at SSRN: [https://ssrn.com/abstract=5027950](https://ssrn.com/abstract=5027950) or [http://dx.doi.org/10.2139/ssrn.5027950](https://dx.doi.org/10.2139/ssrn.5027950)\n\n### [Amit Basu](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=470814)\n\n[![Southern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM)](https://papers.ssrn.com/Organizations/OrgBrandings/17978_13002.gif)](http://www.smu.edu/businesslibrary)\n\n#### Southern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM) ( [email](javascript:void(0)) )\n\nDallas, TX 75275\n\nUnited States\n\n### [Sreekumar R. Bhaskaran (Contact Author)](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=343405)\n\n[![Southern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM)](https://papers.ssrn.com/Organizations/OrgBrandings/17978_13002.gif)](http://www.smu.edu/businesslibrary)\n\n#### Southern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM) ( [email](javascript:void(0)) )\n\nDallas, TX 75275\n\nUnited States\n\n### [Rajiv Mukherjee](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=2490216)\n\n[![Texas A&M University - Mays Business School](https://papers.ssrn.com/Organizations/OrgBrandings/193170_5868.gif)](http://mays.tamu.edu/)\n\n#### Texas A&M University - Mays Business School ( [email](javascript:void(0)) )\n\nWehner 401Q, MS 4353\n\nCollege Station, TX 77843-4218\n\nUnited States\n\n## Do you have a job opening that you would like to promote on SSRN?\n\n[Place Job Opening](https://www.ssrn.com/index.cfm/en/Announcements-Jobs/)\n\n## Paper statistics\n\nDownloads\n\n58\n\nAbstract Views\n\n333\n\nRank\n\n786,223\n\n[4 References](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950#paper-references-widget)\n\nPlumX Metrics\n\n## Related eJournals\n\n- [SMU Cox School of Business Research Paper Series](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=968283)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### SMU Cox School of Business Research Paper Series\n\n\n\nSubscribe to this free journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n5,157\n\n\n\n\n\n\n\nPAPERS\n\n\n\n514\n\n\n\n\n\n\n\n\n\nThis Journal is curated by:\n\n\n\n**Sandy Miller** at Southern Methodist University (SMU) - SMU Cox School of Business, **Melissa Johnson** at Southern Methodist University (SMU) - SMU Cox School of Business\n\n- [Microeconomics: Search; Learning; Information Costs & Specific Knowledge; Expectation & Speculation eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=1499582)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Microeconomics: Search; Learning; Information Costs & Specific Knowledge; Expectation & Speculation eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n775\n\n\n\n\n\n\n\nPAPERS\n\n\n\n8,463\n\n- [Operations Strategy eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=992373)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Operations Strategy eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n744\n\n\n\n\n\n\n\nPAPERS\n\n\n\n1,643\n\n- [Technology, Operations Management & Production eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=930066)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Technology, Operations Management & Production eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n734\n\n\n\n\n\n\n\nPAPERS\n\n\n\n2,149\n\n- [Sources of Innovation eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=2011220)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Sources of Innovation eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n280\n\n\n\n\n\n\n\nPAPERS\n\n\n\n3,339\n\n- [Information Systems & Economics eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=1475407)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Information Systems & Economics eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n224\n\n\n\n\n\n\n\nPAPERS\n\n\n\n15,486\n\n\n\n\n\n\n\n\n\nThis Journal is curated by:\n\n\n\n**Erik Brynjolfsson** at National Bureau of Economic Research (NBER)\n\n- [Resource Based Strategy & Policy eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=1358731)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Resource Based Strategy & Policy eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n215\n\n\n\n\n\n\n\nPAPERS\n\n\n\n1,346\n\n\nFeedback\n\nFeedback to SSRN\n\nFeedback\u00a0(required)\n\nEmail\u00a0(required)\n\nSubmit",
          "original_query": "Mukherjee and Basu (2017) \u2014 lower-bound example: max{0,x1,x2} not representable with one hidden layer",
          "cleaned_query": "Mukherjee and Basu",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "\u202aIan Goodfellow\u202c - \u202aGoogle Scholar\u202c",
          "url": "https://scholar.google.com/citations?user=iYN86KEAAAAJ&hl=en",
          "content": "Loading...\n\nThe system can't perform the operation now. Try again later.\n\n## Citations per year\n\n## Duplicate citations\n\nThe following articles are merged in Scholar. Their [combined citations](javascript:void(0)) are counted only for the first article.\n\n## Merged citations\n\nThis \"Cited by\" count includes citations to the following articles in Scholar. The ones marked \\* may be different from the article in the profile.\n\n## Add co-authorsCo-authors\n\n## Follow\n\n[New articles by this author](javascript:void(0))\n\n[New citations to this author](javascript:void(0))\n\n[New articles related to this author's research](javascript:void(0))\n\nEmail address for updates\n\nDone\n\n[My profile](https://scholar.google.com/citations?hl=en) [My library](https://scholar.google.com/scholar?scilib=1&hl=en) [Metrics](https://scholar.google.com/citations?view_op=metrics_intro&hl=en) [Alerts](https://scholar.google.com/scholar_alerts?view_op=list_alerts&hl=en)\n\n[Settings](https://scholar.google.com/scholar_settings?hl=en)\n\n[Sign in](https://accounts.google.com/Login?hl=en&continue=https://scholar.google.com/schhp%3Fhl%3Den)\n\n[Sign in](https://accounts.google.com/Login?hl=en&continue=https://scholar.google.com/schhp%3Fhl%3Den)\n\n[Get my own profile](https://scholar.google.com/citations?hl=en)\n\n### Cited byView all\n\n| All | Since 2020 |\n| --- | --- |\n| [Citations](javascript:void(0)) | 376268 | 301034 |\n| [h-index](javascript:void(0)) | 100 | 93 |\n| [i10-index](javascript:void(0)) | 178 | 170 |\n\n0\n\n59000\n\n29500\n\n14750\n\n44250\n\n20152016201720182019202020212022202320242025 [1241](javascript:void(0)) [3474](javascript:void(0)) [10195](javascript:void(0)) [22919](javascript:void(0)) [33961](javascript:void(0)) [43923](javascript:void(0)) [51928](javascript:void(0)) [53444](javascript:void(0)) [56569](javascript:void(0)) [58507](javascript:void(0)) [36661](javascript:void(0))\n\nPublic access\n\n[View all](https://scholar.google.com/citations?view_op=list_mandates&hl=en&user=iYN86KEAAAAJ)\n\nView all\n\n4 articles\n\n1 article\n\navailable\n\nnot available\n\nBased on funding mandates\n\n### Co-authorsView all\n\n- [Yoshua Bengio](https://scholar.google.com/citations?user=kukA0LcAAAAJ&hl=en) Professor of computer science, University of Montreal, Mila, IVADO, CIFARVerified email at umontreal.ca\n\n- [Aaron Courville](https://scholar.google.com/citations?user=km6CP8cAAAAJ&hl=en) Professor, DIRO, Universit\u00e9 de Montr\u00e9al, Mila, Cifar CAI chairVerified email at umontreal.ca\n\n- [Mehdi Mirza](https://scholar.google.com/citations?user=c646VbAAAAAJ&hl=en) DeepMindVerified email at google.com\n\n- [David Warde-Farley](https://scholar.google.com/citations?user=MOgfm8oAAAAJ&hl=en) Staff Research Scientist at Google DeepMindVerified email at google.com\n\n- [Bing Xu](https://scholar.google.com/citations?user=nHh9PSsAAAAJ&hl=en) Distinguished Engineer, NVIDIAVerified email at nvidia.com\n\n- [Jean Pouget-Abadie](https://scholar.google.com/citations?user=6F3ZIeEAAAAJ&hl=en) Google ResearchVerified email at google.com\n\n- [Nicolas Papernot](https://scholar.google.com/citations?user=cGxq0cMAAAAJ&hl=en) University of Toronto and Vector InstituteVerified email at utoronto.ca\n\n- [Alexey Kurakin](https://scholar.google.com/citations?user=nCh4qyMAAAAJ&hl=en) Research Software Engineer, Google BrainVerified email at google.com\n\n- [Wojciech Zaremba](https://scholar.google.com/citations?user=XCZpOcAAAAAJ&hl=en) Co-Founder of OpenAIVerified email at openai.com\n\n- [Patrick McDaniel](https://scholar.google.com/citations?user=AMGqrI0AAAAJ&hl=en) Tsun-Ming Shih Professor of Computer Sciences, University of Wisconsin-MadisonVerified email at cs.wisc.edu\n\n- [Samy Bengio](https://scholar.google.com/citations?user=Vs-MdPcAAAAJ&hl=en) Senior Director, AI and ML Research, AppleVerified email at apple.com\n\n- [Colin Raffel](https://scholar.google.com/citations?user=I66ZBYwAAAAJ&hl=en) University of Toronto, Vector Institute and Hugging FaceVerified email at cs.toronto.edu\n\n- [Martin Abadi](https://scholar.google.com/citations?user=vWTI60AAAAAJ&hl=en) Research Scientist, Google, and Professor Emeritus, UC Santa CruzVerified email at cs.ucsc.edu\n\n- [Rob Fergus](https://scholar.google.com/citations?user=GgQ9GEkAAAAJ&hl=en) Professor of Computer Science, New York UniversityVerified email at cs.nyu.edu\n\n- [Dumitru Erhan](https://scholar.google.com/citations?user=wfGiqXEAAAAJ&hl=en) Director of Research @ Google DeepMindVerified email at google.com\n\n- [Joan Bruna](https://scholar.google.com/citations?user=L4bNmsMAAAAJ&hl=en) Professor of Computer Science, Data Science & Mathematics (aff), Courant Institute and CDS, NYUVerified email at cims.nyu.edu\n\n- [Ilya Sutskever](https://scholar.google.com/citations?user=x04W_mMAAAAJ&hl=en) Co-Founder and Chief Scientist at Safe Superintelligence IncVerified email at ssi.inc\n\n- [Tim Salimans](https://scholar.google.com/citations?user=w68-7AYAAAAJ&hl=en) Google DeepMind AmsterdamVerified email at google.com\n\n- [Kunal Talwar](https://scholar.google.com/citations?user=XD_01h8AAAAJ&hl=en) Apple IncVerified email at apple.com\n\n- [(Peter) Xi Chen](https://scholar.google.com/citations?user=5tVuggUAAAAJ&hl=en) covariant.ai \\| UC BerkeleyVerified email at berkeley.edu\n\n\nView all\n\n[Follow](javascript:void(0))\n\nIan Goodfellow\n\nDeepMind\n\nVerified email at deepmind.com - [Homepage](http://www.iangoodfellow.com/)\n\n[Deep Learning](https://scholar.google.com/citations?view_op=search_authors&hl=en&mauthors=label:deep_learning)\n\n[Articles](javascript:void(0)) [Cited by](javascript:void(0)) [Public access](javascript:void(0)) [Co-authors](javascript:void(0))\n\n| [Title](https://scholar.google.com/citations?hl=en&user=iYN86KEAAAAJ&view_op=list_works&sortby=title) Sort [Sort by citations](https://scholar.google.com/citations?hl=en&user=iYN86KEAAAAJ&view_op=list_works) [Sort by year](https://scholar.google.com/citations?hl=en&user=iYN86KEAAAAJ&view_op=list_works&sortby=pubdate) [Sort by title](https://scholar.google.com/citations?hl=en&user=iYN86KEAAAAJ&view_op=list_works&sortby=title) | Cited by Cited by | [Year](https://scholar.google.com/citations?hl=en&user=iYN86KEAAAAJ&view_op=list_works&sortby=pubdate) |\n| --- | --- | --- |\n| [Generative adversarial networks](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iYN86KEAAAAJ&citation_for_view=iYN86KEAAAAJ:kNdYIx-mwKoC) I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, ... Advances in neural information processing systems 27, 2014 | [101326](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11977070277539609369,7652588910447996374,16890861223062024876,10913732071597931018,11362283349112095142,12025250469173508241,15544552152195978527,8618380841735941249)[\\*](javascript:void(0)) | 2014 |\n| [Deep learning](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iYN86KEAAAAJ&citation_for_view=iYN86KEAAAAJ:ZeXyd9-uunAC) I Goodfellow, Y Bengio, A Courville, Y Bengio MIT press 1 (2), 2016 | [83949](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16766804411681372720,2705125642663662569,16538979328513850542) | 2016 |\n| [Explaining and Harnessing Adversarial Examples](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iYN86KEAAAAJ&citation_for_view=iYN86KEAAAAJ:KxtntwgDAa4C) I Goodfellow, J Shlens, C Szegedy ICLR, 2014 | [26697](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14908107896544813002,17757247586479584164,6802210412227455718) | 2014 |\n| [TensorFlow: Large-scale machine learning on heterogeneous systems](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iYN86KEAAAAJ&citation_for_view=iYN86KEAAAAJ:1taIhTC69MYC) M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, GS Corrado, ... | [21593](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6781733040830078545) | 2015 |\n| [Intriguing properties of neural networks](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iYN86KEAAAAJ&citation_for_view=iYN86KEAAAAJ:MXK_kJrjxJIC) C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, ... arXiv preprint arXiv:1312.6199, 2013 | [20142](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2835128024326609853,14134151322722010398) | 2013 |\n| [Tensorflow: Large-scale machine learning on heterogeneous distributed systems](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iYN86KEAAAAJ&citation_for_view=iYN86KEAAAAJ:AXPGKjj_ei8C) M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, GS Corrado, ... arXiv preprint arXiv:1603.04467, 2016 | [13774](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4870469586968585222) | 2016 |\n| [Improved techniques for training gans](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iYN86KEAAAAJ&citation_for_view=iYN86KEAAAAJ:_Qo2XoVZTnwC) T Salimans, I Goodfellow, W Zaremba, V Cheung, A Radford, X Chen Advances in neural information processing systems 29, 2016 | [12523](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2151481962498772342) | 2016 |\n| [Deep learning with differential privacy](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iYN86KEAAAAJ&citation_for_view=iYN86KEAAAAJ:RHpTSmoSYBkC) M Abadi, A Chu, I Goodfellow, HB McMahan, I Mironov, K Talwar, L Zhang Proceedings of the 2016 ACM SIGSAC conference on computer and communications\u00a0\u2026, 2016 | [8574](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11431158613977668861) | 2016 |\n| [Adversarial examples in the physical world](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iYN86KEAAAAJ&citation_for_view=iYN86KEAAAAJ:4JMBOYKVnBMC) A Kurakin, IJ Goodfellow, S Bengio Artificial intelligence safety and security, 99-112, 2018 | [7771](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16342929405179799360) | 2018 |\n| [Self-attention generative adversarial networks](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iYN86KEAAAAJ&citation_for_view=iYN86KEAAAAJ:NhqRSupF_l8C) H Zhang, I Goodfellow, D Metaxas, A Ode",
          "original_query": "Goodfellow et al. (2013) \u2014 Maxout networks",
          "cleaned_query": "Goodfellow et al.",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Robust Adaptive Control of Multi-Scroll Chaotic Systems Under Sensor Noise and Actuator Saturation\nExtend the adaptive controller for the Wang\u2013Sun four-scroll chaotic system to explicitly handle measurement noise, unmodeled dynamics, and actuator saturation using a composite adaptation + disturbance observer design. Validate via Lyapunov guarantees and benchmark against baseline adaptive control on perturbed four-scroll and related multi-scroll systems.",
        "GAN-Augmented Parameter Identification for Chaotic Systems with Unknown Parameters\nUse GANs to synthesize realistic chaotic trajectories under varying hidden parameters, then train a parameter estimator that is robust to sparse/noisy observations. Evaluate whether GAN-based augmentation reduces data requirements and improves identification accuracy compared to classical adaptive laws and standard simulation augmentation.",
        "Learnability-Aware Interface Design for Matching Platforms: Unifying UX and Learning Design Metrics\nCreate a shared \u201clearnability rubric\u201d that maps UX-focused learnability attributes (e.g., error prevention, efficiency) to learning design constructs (e.g., scaffolding, feedback) and operationalize it into measurable UI instrumentation. Run A/B tests on a prototype matching platform to quantify how learnability improvements affect onboarding completion, search quality, and match follow-through.",
        "Compatibility Counseling as an Adaptive, Explainable Tutor in Matching Platforms\nImplement \u201ccompatibility counseling\u201d (as described in matching platform strategy) as an interactive tutoring system that adapts explanations and next-step guidance based on user behavior and comprehension signals. Test whether explainable, stepwise counseling improves decision quality (match satisfaction, reduced churn) relative to opaque recommendation-only designs.",
        "Authentication Friction vs. Trust: A Learnability-Optimized Verification Flow\nDesign and experimentally compare authentication flows that vary in complexity (multi-step, biometrics, document upload) while optimizing learnability (progressive disclosure, microlearning prompts). Quantify tradeoffs between user drop-off and downstream trust outcomes (fraud rates, successful match interactions), yielding actionable design rules for \u201clow-friction, high-assurance\u201d onboarding.",
        "Neuro-Informed Prosody Features for Emotion-Aware Matching and Messaging\nLeverage findings on distinct neural networks for linguistic vs. emotional prosody to develop speech/audio features that separate semantic intent from emotional intonation in short voice messages. Evaluate whether emotion-aware messaging tools (e.g., \u201ctone check,\u201d prosody summaries) reduce miscommunication and improve match outcomes, especially in cross-cultural or neurodiverse user groups.",
        "Cerebellar Timing-Inspired Models for Predicting Learnability Breakdowns in Complex Software\nTranslate cerebellum-linked temporal processing insights into computational predictors of \u201ctiming-related\u201d usability failures (e.g., rapid multi-step workflows, tight feedback loops, rhythmic interaction sequences). Use interaction logs to detect where users lose temporal coordination (hesitations, repeated corrections), then test targeted UI interventions (timing cues, pacing, chunking) to improve learnability.",
        "Platform Design for Translational Innovation: A Matching Market Between Universities and Corporate Developers\nBuild a two-sided matching platform that pairs university research outputs with corporate development needs, explicitly addressing the \u201cdigestibility\u201d gap highlighted in the innovation-structure paper. Prototype mechanisms for search, authentication (IP/provenance verification), and compatibility counseling (technical feasibility + integration cost), then measure impacts on time-to-licensing and downstream productization.",
        "AI \u201cDigestibility Translator\u201d: Generative Models to Convert Academic Results into Engineer-Ready Artifacts\nDevelop a generative system that transforms papers/patents into structured engineering packages (requirements, test plans, reference implementations, risk registers) aimed at reducing the university-to-industry handoff friction. Evaluate with industry practitioners on realism, completeness, and time saved, and quantify whether the generated artifacts increase adoption/implementation rates.",
        "Adversarial Robustness of Matching and Counseling Algorithms Under Strategic Manipulation\nInspired by Goodfellow-era adversarial thinking, model how users game matching systems (profile inflation, coordinated fraud, prompt-hacking of counseling chatbots) and build defenses via adversarial training and mechanism design constraints. Run red-team simulations to measure robustness of search, authentication, and compatibility counseling components, producing a practical threat model and mitigation toolkit for matching platforms."
      ],
      "generated_ideas_raw": "1. **Robust Adaptive Control of Multi-Scroll Chaotic Systems Under Sensor Noise and Actuator Saturation**  \nExtend the adaptive controller for the Wang\u2013Sun four-scroll chaotic system to explicitly handle measurement noise, unmodeled dynamics, and actuator saturation using a composite adaptation + disturbance observer design. Validate via Lyapunov guarantees and benchmark against baseline adaptive control on perturbed four-scroll and related multi-scroll systems.\n\n2. **GAN-Augmented Parameter Identification for Chaotic Systems with Unknown Parameters**  \nUse GANs to synthesize realistic chaotic trajectories under varying hidden parameters, then train a parameter estimator that is robust to sparse/noisy observations. Evaluate whether GAN-based augmentation reduces data requirements and improves identification accuracy compared to classical adaptive laws and standard simulation augmentation.\n\n3. **Learnability-Aware Interface Design for Matching Platforms: Unifying UX and Learning Design Metrics**  \nCreate a shared \u201clearnability rubric\u201d that maps UX-focused learnability attributes (e.g., error prevention, efficiency) to learning design constructs (e.g., scaffolding, feedback) and operationalize it into measurable UI instrumentation. Run A/B tests on a prototype matching platform to quantify how learnability improvements affect onboarding completion, search quality, and match follow-through.\n\n4. **Compatibility Counseling as an Adaptive, Explainable Tutor in Matching Platforms**  \nImplement \u201ccompatibility counseling\u201d (as described in matching platform strategy) as an interactive tutoring system that adapts explanations and next-step guidance based on user behavior and comprehension signals. Test whether explainable, stepwise counseling improves decision quality (match satisfaction, reduced churn) relative to opaque recommendation-only designs.\n\n5. **Authentication Friction vs. Trust: A Learnability-Optimized Verification Flow**  \nDesign and experimentally compare authentication flows that vary in complexity (multi-step, biometrics, document upload) while optimizing learnability (progressive disclosure, microlearning prompts). Quantify tradeoffs between user drop-off and downstream trust outcomes (fraud rates, successful match interactions), yielding actionable design rules for \u201clow-friction, high-assurance\u201d onboarding.\n\n6. **Neuro-Informed Prosody Features for Emotion-Aware Matching and Messaging**  \nLeverage findings on distinct neural networks for linguistic vs. emotional prosody to develop speech/audio features that separate semantic intent from emotional intonation in short voice messages. Evaluate whether emotion-aware messaging tools (e.g., \u201ctone check,\u201d prosody summaries) reduce miscommunication and improve match outcomes, especially in cross-cultural or neurodiverse user groups.\n\n7. **Cerebellar Timing-Inspired Models for Predicting Learnability Breakdowns in Complex Software**  \nTranslate cerebellum-linked temporal processing insights into computational predictors of \u201ctiming-related\u201d usability failures (e.g., rapid multi-step workflows, tight feedback loops, rhythmic interaction sequences). Use interaction logs to detect where users lose temporal coordination (hesitations, repeated corrections), then test targeted UI interventions (timing cues, pacing, chunking) to improve learnability.\n\n8. **Platform Design for Translational Innovation: A Matching Market Between Universities and Corporate Developers**  \nBuild a two-sided matching platform that pairs university research outputs with corporate development needs, explicitly addressing the \u201cdigestibility\u201d gap highlighted in the innovation-structure paper. Prototype mechanisms for search, authentication (IP/provenance verification), and compatibility counseling (technical feasibility + integration cost), then measure impacts on time-to-licensing and downstream productization.\n\n9. **AI \u201cDigestibility Translator\u201d: Generative Models to Convert Academic Results into Engineer-Ready Artifacts**  \nDevelop a generative system that transforms papers/patents into structured engineering packages (requirements, test plans, reference implementations, risk registers) aimed at reducing the university-to-industry handoff friction. Evaluate with industry practitioners on realism, completeness, and time saved, and quantify whether the generated artifacts increase adoption/implementation rates.\n\n10. **Adversarial Robustness of Matching and Counseling Algorithms Under Strategic Manipulation**  \nInspired by Goodfellow-era adversarial thinking, model how users game matching systems (profile inflation, coordinated fraud, prompt-hacking of counseling chatbots) and build defenses via adversarial training and mechanism design constraints. Run red-team simulations to measure robustness of search, authentication, and compatibility counseling components, producing a practical threat model and mitigation toolkit for matching platforms.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Robust Adaptive Control of Multi-Scroll Chaotic Systems Under Sensor Noise and Actuator Saturation\nExtend the adaptive controller for the Wang\u2013Sun four-scroll chaotic system to explicitly handle measu",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "GAN-Augmented Parameter Identification for Chaotic Systems with Unknown Parameters\nUse GANs to synthesize realistic chaotic trajectories under varying hidden parameters, then train a parameter estimat",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Learnability-Aware Interface Design for Matching Platforms: Unifying UX and Learning Design Metrics\nCreate a shared \u201clearnability rubric\u201d that maps UX-focused learnability attributes (e.g., error prev",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Compatibility Counseling as an Adaptive, Explainable Tutor in Matching Platforms\nImplement \u201ccompatibility counseling\u201d (as described in matching platform strategy) as an interactive tutoring system tha",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Authentication Friction vs. Trust: A Learnability-Optimized Verification Flow\nDesign and experimentally compare authentication flows that vary in complexity (multi-step, biometrics, document upload) w",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Neuro-Informed Prosody Features for Emotion-Aware Matching and Messaging\nLeverage findings on distinct neural networks for linguistic vs. emotional prosody to develop speech/audio features that separa",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Cerebellar Timing-Inspired Models for Predicting Learnability Breakdowns in Complex Software\nTranslate cerebellum-linked temporal processing insights into computational predictors of \u201ctiming-related\u201d ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Platform Design for Translational Innovation: A Matching Market Between Universities and Corporate Developers\nBuild a two-sided matching platform that pairs university research outputs with corporate ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "AI \u201cDigestibility Translator\u201d: Generative Models to Convert Academic Results into Engineer-Ready Artifacts\nDevelop a generative system that transforms papers/patents into structured engineering packag",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Adversarial Robustness of Matching and Counseling Algorithms Under Strategic Manipulation\nInspired by Goodfellow-era adversarial thinking, model how users game matching systems (profile inflation, coo",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 41,
      "paper_title": "Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization",
      "contribution": "Introduce a new CMI-style generalization bound that injects stochastic projection and lossy compression (quantization) into the CMI super-sample framework to obtain strictly tighter, non\u2011vacuous O(1/\u221an) guarantees on instances where prior MI/CMI bounds fail, and to argue that memorization is not necessary for good generalization.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 11863,
      "output_tokens": 1024,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Reasoning About Generalization via Conditional Mutual Information",
          "url": "http://proceedings.mlr.press/v125/steinke20a/steinke20a.pdf",
          "content": "Proceedings of Machine Learning Research vol 125:1\u201316, 2020 33rd Annual Conference on Learning Theory\nReasoning About Generalization via Conditional Mutual Information\nThomas Steinke CMI@THOMAS-STEINKE.NET\nIBM Research \u2013 Almaden\nLydia Zakynthinou ZAKYNTHINOU.L@NORTHEASTERN.EDU\nKhoury College of Computer Sciences, Northeastern University\nEditors: Jacob Abernethy and Shivani Agarwal\nAbstract\nWe provide an information-theoretic framework for studying the generalization properties of ma\u0002chine learning algorithms. Our framework ties together existing approaches, including uniform\nconvergence bounds and recent methods for adaptive data analysis.\nSpecifically, we use Conditional Mutual Information (CMI) to quantify how well the input (i.e.,\nthe training data) can be recognized given the output (i.e., the trained model) of the algorithm. We\nshow that bounds on CMI can be obtained from VC dimension, compression schemes, differential\nprivacy, and other methods. We then show that bounded CMI implies various forms of generalization.\nKeywords: Generalization, stability in data analysis, conditional mutual information\n1. Introduction\nHow can we ensure that a machine learning system produces an output that generalizes to the\nunderlying distribution, rather than overfitting its training data? That is, how can we ensure that the\nhypotheses or models that are produced are reflective of the underlying population the training data\nwas drawn from, rather than patterns that occur only by chance in the training data? This is perhaps\nthe fundamental question for the science of statistical machine learning.\nA vast array of methods have been proposed to answer this question. Most notably, the theory of\nuniform convergence shows that, if the output is sufficiently \u201csimple,\u201d then it cannot overfit too much.\nA more recent line of work has used distributional stability (in the form of differential privacy) to\nprovide generalization guarantees that compose adaptively \u2013 that is, statistical validity is preserved\neven when a dataset is reused multiple times with each analysis being influenced by prior outcomes.\nOther methods for proving generalization include compression schemes and uniform stability.\nUnfortunately, these different methods for providing generalization guarantees are largely dis\u0002connected from one another; it is, in general, not possible to compare or combine techniques. In this\npaper, we provide a framework to reason about many of these these differing approaches using the\nunifying language of information theory.\n1.1. Background: Generalization\nWe consider the standard setting of statistical learning (Valiant, 1984; Haussler, 1992; Kearns et al.,\n1994). There is an unknown probability distribution D over some known set Z. We have access to a\nExtended abstract. Full version appears as [Steinke and Zakynthinou 2020, v3].\n c 2020 T. Steinke & L. Zakynthinou.\nREASONING ABOUT GENERALIZATION VIA CONDITIONAL MUTUAL INFORMATION\nsample Z \u2208 Znconsisting of n independent draws from D. Informally, our goal is to learn something\nabout the underlying distribution D from the dataset Z. Formally, we have a function ` : W \u00d7Z \u2192 R\nand our goal is to find some w\u2217 \u2208 W that approximately minimizes `(w\u2217, D) := E\nZ0\u2190D\n[`(w\u2217, Z0)].\n1\nIntuitively, w\u2217 represents some hypothesis and `(w\u2217, D) measures the veracity or quality of\nw\u2217. In supervised machine learning, Z = X \u00d7 Y represents pairs of feature vectors and labels\nand w\u2217 represents a function fw\u2217: X \u2192 Y that predicts the label given the features. Then ` is\na \u201closs function.\u201d For example, the 0-1 loss measures the error rate of the predictor: `(w\u2217, D) =\nP\n(X,Y )\u2190D\n[fw\u2217(X) 6= Y ], so minimizing `(w\u2217, D) corresponds to finding the most accurate predictor.\nHowever, we cannot evaluate the true loss (a.k.a. \u201cpopulation loss\u201d or \u201crisk\u201d) `(w\u2217, D) since\nthe distribution D is unknown. Instead we can compute the empirical loss (a.k.a. \u201cempirical risk\u201d)\n`(w\u2217, Z) := 1\nn\nPn\ni=1 `(w\u2217, Zi) using the sample Z. A natural learning strategy is \u201cEmpirical Risk\nMinimization (ERM)\u201d \u2013 i.e., w\u2217 = arg minw\u2208W `(w, Z). The question of generalization is thus:\nHow can we ensure that `(w\u2217, Z) \u2248 `(w\u2217, D)?\nThe classical theory of uniform convergence (Vapnik and Chervonenkis, 1971) approaches\nthis problem by studying the class of functions F := {`(w, \u00b7) : w \u2208 W}. If we can show that\nsupw\u2208W |`(w, Z) \u2212 `(w, D)| is small with high probability for a random Z \u2190 Dn, then the question\nof generalization is answered. Such bounds can be obtained from combinatorial properties of F,\nsuch as its Vapnik-Chervonenkis (VC) dimension (Vapnik and Chervonenkis, 1971; Talagrand, 1994;\nAlon et al., 1997) or its fat-shattering dimension (Kearns and Schapire, 1994; Bartlett et al., 1996).\nUniform convergence makes no reference to the algorithm; it depends only on its range F.\nAn algorithm may generalize better than uniform convergence would suggest (Shalev-Shwartz\net al., 2009; Feldman, 2016). For example, it is common to add a regularizer to the ERM \u2013 that\nis, w\u2217 = arg minw\u2208W `(w, Z) + \u03bbkwk, where \u03bb > 0 is a parameter and kwk is a measure of the\ncomplexity of w. Thus we explicitly consider generalization to be a property of the learning algorithm\nA : Z\nn \u2192 W, which may or may not be randomized.\nThere are several ways to show that a specific algorithm A generalizes. Algorithms whose output\nessentially only depends on a few of the input data points, as formalized by compression schemes\n(Littlestone and Warmuth, 1986), can be shown to generalize. Uniform stability (Bousquet and\nElisseeff, 2002) entails strong generalization bounds for algorithms where changing a single input\ndatum does not change the loss of the algorithm too much. Similarly, differential privacy (Dwork\net al., 2006) \u2013 a distributional notion of stability \u2013 entails generalization bounds (Dwork et al., 2015b;\nBassily et al., 2016; Jung et al., 2019). In general, these various methods for proving generalization\nare incompatible and incomparable. This raises the question of whether it is possible to provide a\nunifying framework or language to study generalization.\n1.1.1. (UNCONDITIONAL) MUTUAL INFORMATION\nA recent line of work has studied generalization using mutual information and related quantities\n(Russo and Zou, 2016; Raginsky et al., 2016; Alabdulmohsin, 2016; Feldman and Steinke, 2018;\nBassily et al., 2018; Dwork et al., 2015a; Rogers et al., 2016; Smith, 2017; Xu and Raginsky, 2017;\nNachum and Yehudayoff, 2018; Nachum et al., 2018; Esposito et al., 2019; Bu et al., 2019, etc.). For\na (possibly randomized) algorithm A : Z\nn \u2192 W and a dataset Z \u2190 Dn\n, we consider the quantity\nI(A(Z);Z), which measures how much information the output A(Z) contains about its input Z.\n1\nFor simplicity, in this introduction we only consider ` to be a linear function (that is, only taking in a single element\nof Z). Our methods readily extend to the more general case where ` : W \u00d7 Zm \u2192 R.\n2\nREASONING ABOUT GENERALIZATION VIA CONDITIONAL MUTUAL INFORMATION\nBounded mutual information implies generalization: If ` : W \u00d7 Z \u2192 [0, 1], A : Z\nn \u2192 W\nand Z \u2190 Dn, then |E [`(A(Z), Z) \u2212 `(A(Z), D)]| \u2264 q\n2\nn\n\u00b7 I(A(Z);Z) (Russo and Zou, 2016; Xu\nand Raginsky, 2017). Bounds on mutual information can be obtained from differential privacy or\nfrom bounds on the entropy of the output of A. Specifically, if A is \u03b5-differentially private, then\nI(A(Z);Z) \u2264\n1\n2\n\u03b5\n2n (McGregor et al., 2010; Bun and Steinke, 2016). And we have the generic\nbound I(A(Z);Z) \u2264 H(A(Z)) \u2264 log |W|.\nUnfortunately, mutual information can easily be infinite even in settings where generalization\nis easy to prove. Bassily, Moran, Nachum, Shafer, and Yehudayoff (Bassily et al., 2018; Nachum\net al., 2018) showed that any proper and consistent learner A for threshold functions must have\nI(A(Z);Z) \u2265 \u2126\n\u0010\nlog log |Z|\nn2\n\u0011\nwhen Z \u2190 Dnfor some worst-case distribution D. The dependence\non the size of the domain Z \u2282 R is mild, but, if the domain is infinite, then the mutual information is\nunbounded. In contrast, the VC dimension of threshold functions is 1, which implies strong uniform\nconvergence bounds even for infinite domains.\nWe remark that thresholds can be \u201cembedded\u201d into larger classes, such as higher-dimensional\nlinear thresholds (halfspaces) or even neural networks. Thus these negative results for unconditional\nmutual information extend to those classes too. This strong negative result shows that any proper\nempirical risk minimizer for thresholds must have unbounded mutual information; it is easier to show\nthat many specific natural algorithms and natural distributions have unbounded mutual information:\nLinear regression has unbounded mutual information (even in dimension 0 with Gaussian data, which\nis simply outputting the mean (Bu et al., 2019)). The most natural algorithms for thresholds have\ninfinite mutual information for any continuous data distribution.\nThe fundamental issue with the mutual information approach is that even a single data point has\ninfinite information content if the distribution is continuous. Meanwhile, an algorithm revealing a\nsingle data point is not an issue for generalization.\nWe address the shortcomings of the mutual information approach by moving to conditional\nmutual information. Our conditioning approach can be viewed as \u201cnormalizing\u201d the information\ncontent of each data point to one bit. That is, an algorithm that reveals one data point only has\nconditional mutual information of one bit, even if the unconditional mutual information is infinite.\n1.2. Our Contributions: Conditional Mutual Information (CMI)\nWe introduce the conditional mutual information (CMI) framework for reasoning about the general\u0002ization properties of machine learning algorithms. CMI is a quantitative property of an algorithm A\nand a distribution D. (Note that it does not depend on the loss function of interest.)\nIntuitively, CMI measures how well we can \u201crecognize\u201d the input (i.e., training data) given the\noutput (i.e., trained model) of the algorithm. Recognizi",
          "original_query": "Reasoning about generalization via conditional mutual information",
          "cleaned_query": "Reasoning about generalization via conditional mutual information",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Information-theoretic analysis of generalization capability of learning ...",
          "url": "https://arxiv.org/abs/1705.07809",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1705.07809** (cs)\n\n\\[Submitted on 22 May 2017 ( [v1](https://arxiv.org/abs/1705.07809v1)), last revised 6 Nov 2017 (this version, v2)\\]\n\n# Title:Information-theoretic analysis of generalization capability of learning algorithms\n\nAuthors: [Aolin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+A), [Maxim Raginsky](https://arxiv.org/search/cs?searchtype=author&query=Raginsky,+M)\n\nView a PDF of the paper titled Information-theoretic analysis of generalization capability of learning algorithms, by Aolin Xu and Maxim Raginsky\n\n[View PDF](https://arxiv.org/pdf/1705.07809)\n\n> Abstract:We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems, and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information. We propose a number of methods for this purpose, among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.\n\n| | |\n| --- | --- |\n| Comments: | Final version, accepted to NIPS 2017 |\n| Subjects: | Machine Learning (cs.LG); Information Theory (cs.IT); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1705.07809](https://arxiv.org/abs/1705.07809) \\[cs.LG\\] |\n| | (or [arXiv:1705.07809v2](https://arxiv.org/abs/1705.07809v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1705.07809](https://doi.org/10.48550/arXiv.1705.07809) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Maxim Raginsky \\[ [view email](https://arxiv.org/show-email/53d0d258/1705.07809)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1705.07809v1)**\nMon, 22 May 2017 15:38:22 UTC (30 KB)\n\n**\\[v2\\]**\nMon, 6 Nov 2017 18:58:37 UTC (21 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Information-theoretic analysis of generalization capability of learning algorithms, by Aolin Xu and Maxim Raginsky\n\n- [View PDF](https://arxiv.org/pdf/1705.07809)\n- [TeX Source](https://arxiv.org/src/1705.07809)\n- [Other Formats](https://arxiv.org/format/1705.07809)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1705.07809&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1705.07809&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2017-05](https://arxiv.org/list/cs.LG/2017-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1705.07809?context=cs)\n\n[cs.IT](https://arxiv.org/abs/1705.07809?context=cs.IT)\n\n[math](https://arxiv.org/abs/1705.07809?context=math)\n\n[math.IT](https://arxiv.org/abs/1705.07809?context=math.IT)\n\n[stat](https://arxiv.org/abs/1705.07809?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1705.07809?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1705.07809)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1705.07809)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1705.07809)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1705.html#XuR17) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/XuR17)\n\n[Aolin Xu](https://dblp.uni-trier.de/search/author?author=Aolin%20Xu)\n\n[Maxim Raginsky](https://dblp.uni-trier.de/search/author?author=Maxim%20Raginsky)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1705.07809&description=Information-theoretic analysis of generalization capability of learning algorithms) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1705.07809&title=Information-theoretic analysis of generalization capability of learning algorithms)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1705.07809) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Information-theoretic analysis of generalization capability of learning algorithms",
          "cleaned_query": "Information-theoretic analysis of generalization capability of learning algorithms",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Idan Attias - Information Complexity of Stochastic Convex Optimization",
          "url": "https://www.youtube.com/watch?v=rA0zmqlQEcw",
          "content": "Presented on Thursday, May 16th, 2024, 10:30 AM, room B220\\n \\nSpeaker\\nIdan Attias (BGU)\\n \\nTitle\\nInformation Complexity of Stochastic Convex Optimization: Applications to Generalization, Memorization and Privacy\\n \\nAbstract:\\nWe investigate the interplay between memorization and learning in the context of stochasticconvex optimization (SCO). We define memorization via the information a learning algorithm revealsabout its training data points. We then quantify this information using the framework of conditionalmutual information (CMI) proposed by Steinke and Zakynthinou [SZ20]. Our main result is a precisecharacterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering anopen question posed by Livni [Liv23]. We show that, in the L2 Lipschitz\u2013bounded setting and understrong convexity, every learner with an excess error \u03b5 has CMI bounded below by \u2126(1/\u03b52) and \u2126(1/\u03b5),respectively. We further demonstrate the essential role of memorization in learning problems in SCO bydesigning an adversary capable of accurately identifying a significant fraction of the training samples inspecific SCO problems. Finally, we enumerate several implications of our results, such as a limitation ofgeneralization bounds based on CMI and the incompressibility of samples in SCO problems.\\n\\nBio: \\n\\nIdan is a PhD student advised by Aryeh Kontorovich and Yishay Mansour. He will start a postdoctoral position this fall at IDEAL (NSF program), hosted by Lev Reyzin and Avrim Blum. \\nHis primary research interests lie in the foundations of machine learning theory and data-driven sequential decision-making, and its intersection with game theory, optimization, statistics, private data analysis, causal inference, and information theory.\\n\\n \\n \\n \\nRelevant links:\\nYou can watch previous talks in Panopto and on our YouTube channel.\\nFuture lectures are published on our website and google calendar. \\nIf you want to get email updates about the next talks, please subscribe to our mailing list.\n| view_count: 609 views | short_view_count: 609 views | num_likes: 8 likes | num_subscribers: 350 | duration: 59 minutes",
          "original_query": "Information complexity of stochastic convex optimization: Applications to generalization, memorization, and tracing",
          "cleaned_query": "Information complexity of stochastic convex optimization: Applications to generalization, memorization, and tracing",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2302.04925] Information Theoretic Lower Bounds for ... - arXiv",
          "url": "https://arxiv.org/html/2302.04925",
          "content": "Information Theoretic Lower Bounds for Information Theoretic Upper Bounds\nLicense: arXiv.org perpetual non-exclusive license\narXiv:2302.04925v2 [cs.LG] 14 Jan 2024\n# Information Theoretic Lower Bounds for Information Theoretic Upper Bounds\nRoi Livni\nDepartment Electrical Engineering\nTel Aviv University\nrlivni@tauex.tau.ac.il\n###### Abstract\nWe examine the relationship between the mutual information between the output model and the empirical sample and the generalization of the algorithm in the context of stochastic convex optimization. Despite increasing interest in information-theoretic generalization bounds, it is uncertain if these bounds can provide insight into the exceptional performance of various learning algorithms. Our study of stochastic convex optimization reveals that, for true risk minimization, dimension-dependent mutual information is necessary. This indicates that existing information-theoretic generalization bounds fall short in capturing the generalization capabilities of algorithms like SGD and regularized ERM, which have dimension-independent sample complexity.\n## 1Introduction\nOne of the crucial challenges facing contemporary generalization theory is to understand and explain the behavior of overparameterized models. These models have a large number of parameters compared to the available training examples. But nonetheless, they tend to perform well on unseen test data. The significance of this issue has become more pronounced in recent years, as it has become evident that many state-of-the-art learning algorithms are highly overparameterized> [\n[> 28\n](#bib.bib28)> , [> 46\n](#bib.bib46)> ]\n. The classical generalization bounds, which are well designed to describe the learning behavior of underparameterized models, seem to fail to explain these algorithms.\nUnderstanding the success of overparameterized models seems challenging. Partly, due to the counter-intuitive nature of the process. Common wisdom suggests that, inorder to learn, one has to have certain good bias of the problem at hand, and that in learning we need to restrict ourselves to a class of models that cannot overfit the data. This intuition has been justified by classical learning models such as PAC learning> [\n[> 42\n](#bib.bib42)> ]\nas well as regression> [\n[> 2\n](#bib.bib2)> ]\n. In these classical models, it can be even demonstrated> [\n[> 43\n](#bib.bib43)> , [> 8\n](#bib.bib8)> ]\nthat learning requires more examples than the capacity of the class of model to be learnt, and that avoiding interpolation is necessary for generalization. These results, though, are obtained in distribution-independent setups where one assumes worst-cast distributions over the data.\nFor this reason, researchers have been searching for new, refined models, as well as improved generalization bounds that incorporate distributional as well as algorithmic assumptions. A promising approach, in this direction, tries to connect the generalization performance to the amount of information the learner holds regarding the data> [\n[> 34\n](#bib.bib34)> , [> 45\n](#bib.bib45)> , [> 6\n](#bib.bib6)> ]\n. For example,> Xu and Raginsky [\n[> 45\n](#bib.bib45)> ]\ndemonstrated an upper bound on the generalization gap which informally states:\n|generalization gap\u2062(wS)=O\u2062(I\u2062(wS,S)|S|)generalization gapsubscript\ud835\udc64\ud835\udc46\ud835\udc42\ud835\udc3csubscript\ud835\udc64\ud835\udc46\ud835\udc46\ud835\udc46\\\\textrm{generalization gap}(w\\_{S})=O\\\\left(\\\\sqrt{\\\\frac{I(w\\_{S},S)}{|S|}}\\\\right)generalization gap ( italic\\_w start\\_POSTSUBSCRIPT italic\\_S end\\_POSTSUBSCRIPT ) = italic\\_O ( square-root start\\_ARG divide start\\_ARG italic\\_I ( italic\\_w start\\_POSTSUBSCRIPT italic\\_S end\\_POSTSUBSCRIPT , italic\\_S ) end\\_ARG start\\_ARG | italic\\_S | end\\_ARG end\\_ARG )||(1)|\nNamely, given an empirical sampleS\ud835\udc46Sitalic\\_S, and an output modelwSsubscript\ud835\udc64\ud835\udc46w\\_{S}italic\\_w start\\_POSTSUBSCRIPT italic\\_S end\\_POSTSUBSCRIPT, the difference between its empirical error and its true error can be upper bounded byI\u2062(wS,S)\ud835\udc3csubscript\ud835\udc64\ud835\udc46\ud835\udc46I(w\\_{S},S)italic\\_I ( italic\\_w start\\_POSTSUBSCRIPT italic\\_S end\\_POSTSUBSCRIPT , italic\\_S ), the mutual information between these two random variables. Notice that[Eq.1](#S1.E1)does not depend on any trait of the class of feasible models to be considered. In particular, it does not depend, apriori, on number of \u201cfree parameters\", or a complexity measure such as VC dimension, the dimension ofwSsubscript\ud835\udc64\ud835\udc46w\\_{S}italic\\_w start\\_POSTSUBSCRIPT italic\\_S end\\_POSTSUBSCRIPT, and not even on some prior distribution.\nHowever, it remains a question whether this method and technique can be useful in analysing state-of-the-art learning algorithms. While there has been a lot of work trying to establish the success of learning algorithms in various setups> [\n[> 27\n](#bib.bib27)> , [> 45\n](#bib.bib45)> , [> 11\n](#bib.bib11)> , [> 3\n](#bib.bib3)> , [> 30\n](#bib.bib30)> ]\n, many of the established bounds are opaque, and often there is no comprehensive end-to-end analysis that effectively illustrates how generalization is to be bounded by[Eq.1](#S1.E1)and*simultaneously*obtain good empirical performance. In fact, there is also evidence> [\n[> 14\n](#bib.bib14)> , [> 17\n](#bib.bib17)> ]\nthat memorizing data is required, in some regimes, for effective learning.\nTowards better understanding, we will focus, in this work, on the setup of*Stochastic Convex Optimization*> [\n[> 37\n](#bib.bib37)> ]\n(SCO), and provide accompanying lower bounds to[Eq.1](#S1.E1)that will describe how much mutual information is*necessary*for learning.\n#### SCO as a case study for overparametrization:\nSCO is a very clean and simple setup where a learner observes noisy instances of (Lipschitz) convex functions, defined in\u211ddsuperscript\u211d\ud835\udc51\\\\mathbb{R}^{d}blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_d end\\_POSTSUPERSCRIPT, and is required to minimize their expectation. On the one hand, it provides simple, amenable to rigorous analysis, definitions of learnability and learning. On the other hand, this model is the cradle of prototypical algorithms such as Gradient Descent (GD) and Stochastic Gradient Descent (SGD), as well as accelerated methods, which are the workhorse behind state-of-the-art optimization methods.\nMoreover, SCO is an ideal model for understanding overparameterization. It is known> [\n[> 16\n](#bib.bib16)> ]\nthat in this setup,\u03a9\u2062(d)\u03a9\ud835\udc51\\\\Omega(d)roman\\_\u03a9 ( italic\\_d )examples are needed in order to avoid overfitting. In fact, even concrete algorithms such as GD and regularized-GD may overfit unless they observe dimension-dependent sample size> [\n[> 4\n](#bib.bib4)> , [> 5\n](#bib.bib5)> ]\n. In other words, the capacity of the model and its ability to overfit does indeed scale with the dimension. Nevertheless, it is also known that*some*algorithms do learn with far fewer examples. For example SGD> [\n[> 20\n](#bib.bib20)> ]\n, Regularized-ERM> [\n[> 37\n](#bib.bib37)> , [> 10\n](#bib.bib10)> ]\nand a stable variant of GD> [\n[> 7\n](#bib.bib7)> ]\nall learn withO\u2062(1/\u03b52)\ud835\udc421superscript\ud835\udf002O(1/\\\\varepsilon^{2})italic\\_O ( 1 / italic\\_\u03b5 start\\_POSTSUPERSCRIPT 2 end\\_POSTSUPERSCRIPT )examples, a dimension independent magnitude. To put it differently, learning in SCO is not just a question of finding the empirical risk minimizer, but also a question of*how*\u2013 what algorithm was used, and learning is not demonstrated by naive uniform convergence bounds that scale with the number of parameters in the model.\nTherefore, SCO is a natural candidate to study how information theoretic bounds play a role in learning. We might even hope that these bounds shed light on why some algorithms succeed to learn while others fail. Existing algorithms don\u2019t avoid memorizing the data, but it is unclear if holding information on the data is*necessary*. So we start here with the simplest question:\nWhat is the smallest amount of mutual information required for learning in SCO?\nOur main result shows that, in contrast with the dimension-independent learnability results in this setup, the information between the model and the sample has to be*dimension-dependent*. As such, the complexity of the class appears implicitly in[Eq.1](#S1.E1). As a result, carrying\u03a9\u2062(d)\u03a9\ud835\udc51\\\\Omega(d)roman\\_\u03a9 ( italic\\_d )bits of information over the sample is*necessary*for learning at optimal rates, and[Eq.1](#S1.E1)doesn\u2019t yield the optimal generalization performance of algorithms such as Regularized ERM, SGD and stable-GD.\n### 1.1Related Work\nInformation-theoretic generalization bounds have a long history of study in ML theory> [\n[> 24\n](#bib.bib24)> , [> 25\n](#bib.bib25)> , [> 22\n](#bib.bib22)> ]\n. Generalization bounds that directly relate to the information between output and input of the learner initiated in the works of> [\n[> 45\n](#bib.bib45)> , [> 6\n](#bib.bib6)> , [> 34\n](#bib.bib34)> ]\n,> Bassily et\u00a0al. [\n[> 6\n](#bib.bib6)> ]\ndemonstrated limitations for such generalization bounds, for proper ERM learners, and> Livni and Moran [\n[> 23\n](#bib.bib23)> ]\nshowed that any learner (proper or not), that learns the class of thresholds must leak unbounded amount of information. In this work we focus on stochastic optimization and on learning Lipschitz functions. In the setup of SCO the above is not true, and one can construct learners that leakO\\~\u2062(d)\\~\ud835\udc42\ud835\udc51\\\\tilde{O}(d)over\\~ start\\_ARG italic\\_O end\\_ARG ( italic\\_d )bits of information (see[Proposition1](#Thmproposition1)).\nBut we would like to know whether information-theoretic bounds behave like the uniform convergence bounds (dimension dependent) or capture the minmax learning rates (dimension independent).\nSeveral lines of works applied the analysis of> Xu and Raginsky [\n[> 45\n](#bib.bib45)> ]\nto provide algorithmic-dependent analysis in the context of stochastic optimization.> Pensia et\u00a0al. [\n[> 30\n](#bib.bib30)> ]\nand followup improvements> [\n[> 33\n](#bib.bib33)> , [> 26\n](#bib.bib26)> , [> 18\n](#bib.bib18)> ]\nprovided information-theoretic generalization bounds for Stochastic Gradient Langevine Dynamics (SGLD) and> Neu et\u00a0al. [\n[> 27\n](#bib.bib27)> ]\nextends the idea to an",
          "original_query": "Information theoretic lower bounds for information theoretic upper bounds",
          "cleaned_query": "Information theoretic lower bounds for information theoretic upper bounds",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "[PDF] Rate-Distortion Theoretic Generalization Bounds for Stochastic ...",
          "url": "https://proceedings.mlr.press/v178/sefidgaran22a/sefidgaran22a.pdf",
          "content": "Proceedings of Machine Learning Research vol 178:1\u201348, 2022 35th Annual Conference on Learning Theory\nRate-Distortion Theoretic Generalization Bounds\nfor Stochastic Learning Algorithms\nMilad Sefidgaran MILAD.SEFIDGARAN@TELECOM-PARIS.FR\nLTCI, Tel\u00b4 ecom Paris, Institut Polyetechnique de Paris \u00b4\nAmin Gohari AMIN.AMINZADEH@GMAIL.COM\nTehran Institute for Advanced Studies, Khatam University\nGael Richard \u00a8 GAEL.RICHARD@TELECOM-PARIS.FR\nLTCI, Tel\u00b4 ecom Paris, Institut Polyetechnique de Paris \u00b4\nUmut S\u00b8ims\u00b8ekli UMUT.SIMSEKLI@INRIA.FR\nINRIA & ENS \u2013 PSL Research University\nEditors: Po-Ling Loh and Maxim Raginsky\nAbstract\nUnderstanding generalization in modern machine learning settings has been one of the major\nchallenges in statistical learning theory. In this context, recent years have witnessed the develop\u0002ment of various generalization bounds suggesting different complexity notions such as the mutual\ninformation between the data sample and the algorithm output, compressibility of the hypothesis\nspace, and the fractal dimension of the hypothesis space. While these bounds have illuminated the\nproblem at hand from different angles, their suggested complexity notions might appear seemingly\nunrelated, thereby restricting their high-level impact. In this study, we prove novel generalization\nbounds through the lens of rate-distortion theory, and explicitly relate the concepts of mutual infor\u0002mation, compressibility, and fractal dimensions in a single mathematical framework. Our approach\nconsists of (i) defining a generalized notion of compressibility by using source coding concepts, and\n(ii) showing that the \u2018compression error rate\u2019 can be linked to the generalization error both in expec\u0002tation and with high probability. We show that in the \u2018lossless compression\u2019 setting, we recover and\nimprove existing mutual information-based bounds, whereas a \u2018lossy compression\u2019 scheme allows\nus to link generalization to the rate-distortion dimension \u2013 a particular notion of fractal dimension.\nOur results bring a more unified perspective on generalization and open up several future research\ndirections.\nKeywords: Generalization error, rate-distortion theory, source coding.\n1. Introduction\nMany important problems in statistical learning can be cast as the population risk minimization\nproblem, which is defined as follows (Shalev-Shwartz and Ben-David, 2014):\nmin\nwPW\n!\nLpwq \u2013 EZ\u201e\u00b5r`pZ, wqs)\n, (1)\nwhere W \u0102 R\nd denotes a parametric hypothesis class, Z P Z denotes the input data with Z being\nthe data space, \u00b5 denotes an unknown data distribution over Z, and ` : Z \u02c6 W \u00d1 R\n` is a loss\n\u00a9 2022 M. Sefidgaran, A. Gohari, G. Richard & U. S\u00b8ims\u00b8ekli.\nSEFIDGARAN GOHARI RICHARD S\u00b8IMS\u00b8EKLI\nfunction that measures the quality of a hypothesis w P W. As the data distribution \u00b5 is unknown in\npractice, we instead consider the empirical risk minimization problem, given as follows:\nmin\nwPW\n!\nL\u02c6pS, wq \u2013\n1\nn\n\u00ffn\ni\u201c1\n`pZi, wq\n)\n, (2)\nwhere S :\u201c tZ1, . . . , Znu denotes a training dataset with independent and identically distributed\n(i.i.d.) elements, i.e., each Zi \u201ei.i.d. \u00b5.\nTo attack the optimization problem (2), arguably, the most common approach is to utilize a\nstochastic optimization algorithm A : Z\nn \u00d1 W (e.g., stochastic gradient descent), such that the\nalgorithm outputs a random hypothesis, i.e., ApSq \u201c W P W. One of the main challenges in\nstatistical learning theory has been then to understand the behavior of the so-called generalization\nerror associated with the algorithm output, that is the difference between the population and em\u0002pirical risks induced by the algorithm output: genpS, ApSqq \u2013 LpApSqq \u00b4 L\u02c6pS, ApSqq. It has\nbeen illustrated that classical algorithm-independent generalization bounds fall short at explaining\nthe (perhaps unexpected) success of modern machine learning systems (Zhang et al., 2017). This\nhas motivated the development of algorithm-dependent generalization bounds, a field that has been\nevolving in different directions.\nAn important direction in this context, and the one that is closest to our study, is based on\nanalyzing the generalization error by using information-theoretic tools. Initiated by Russo and\nZou (2016) and Xu and Raginsky (2017), these approaches link the generalization error to the\nmutual information between the data sample S and the algorithm output W; suggesting that a\nlower statistical dependence between S and W implies better generalization. Their initial results\nwere later improved by using different conditional versions of the mutual information (Harutyunyan\net al., 2021; Haghifam et al., 2021; Negrea et al., 2020b; Steinke and Zakynthinou, 2020; Bu et al.,\n2020; Haghifam et al., 2020), and were further generalized to more general notions of the mutual\ninformation that are defined through f-divergences (rather than the Kullback-Leibler divergence)\n(Esposito et al., 2020; Hellstrom and Durisi, 2020; Masiha et al., 2021).\nA second approach has been based on the observation that the algorithm output W can be \u2018com\u0002pressible\u2019 in different senses. Littlestone and Warmuth (1986) in a pioneer work, considered a\ncompressibility framework for the binary classification problem, in which compressed hypothesis\nare chosen based on a subset of length k of S such that the picked hypothesis predicts correctly the\nlabel for all Zi P S. They showed that whenever such a compressing strategy exists, the algorithm\ngeneralizes well. The compressibility approach is later applied in different ways especially to over\u0002parametrized neural networks (Arora et al., 2018; Suzuki et al., 2020a,b; Negrea et al., 2020a; Hsu\net al., 2021; Barsbey et al., 2021; Baykal et al., 2019; Kuhn et al., 2021). Loosely speaking, under\ndifferent compressibility assumptions for W, these studies showed that a higher level of compress\u0002ibility indicates a lower generalization error since the hypothesis class W can be approximated by\na smaller, \u2018compressed\u2019 space, which intuitively induces a lower worst-case error.\nFinally, a recently initiated line of research has illustrated that when A is chosen as an iterative\noptimization algorithm, due to its recursive nature, A might generate a \u2018fractal structure\u2019, either\nin its optimization trajectories (S\u00b8ims\u00b8ekli et al., 2020; Birdal et al., 2021; Hodgkinson et al., 2021),\nor in the support of its stationary distribution (Camuto et al., 2021). These studies showed that the\ngeneralization error can be linked to the \u2018intrinsic dimension\u2019 of the fractal structure that is generated\nby the algorithm; suggesting that a smaller intrinsic dimension implies improved generalization.\n2\nRATE-DISTORTION THEORETIC GENERALIZATION BOUNDS\nEven though these three research directions have shed light on different fac\u00b8ades of the problem\nof understanding the generalization error, the mathematical frameworks that underlie their theoreti\u0002cal results and their implied take-home messages might be seemingly unrelated, thereby restricting\ntheir high-level impact. In this paper, we prove novel generalization bounds through the lens of\nrate-distortion theory (Berger, 1975), and explicitly relate the concepts of mutual information, com\u0002pressibility, and fractal dimensions in a single mathematical framework.\nTo achieve this goal, we first define a generalized notion of compressibility by using source\ncoding concepts from information theory, which then allows us to use \u2018information-theoretic cov\u0002erings\u2019 for W that we will detail in Section 3. Within this context, we show that the \u2018compression\nerror rate\u2019 of an algorithm A can be linked to its generalization error both in expectation and with\nhigh probability. Next, we show that the aforementioned information-theoretical frameworks can\nbe obtained as a special case of our setup, which is referred to as \u2018lossless compression\u2019. Thanks\nto this connection, the results of Xu and Raginsky (2017) can be re-derived. The bound in (Xu\nand Raginsky, 2017, Theorem 1) is in terms of the mutual information between S and W, denoted\nas IpS; Wq, which was previously viewed as the dataset dependency of the algorithm. However,\nour framework reveals that it is an upper-bound on the compression rate in terms of lossless algo\u0002rithm compressibility. This new perspective allows us to introduce the notion of lossy algorithm\ncompressibility to handle continuous or large alphabets where (Xu and Raginsky, 2017, Theorem 1)\ncan be vacuous as IpS; Wq can be very large; implying that a large IpS; Wq does not necessarily\nindicate the algorithm will not generalize as long as the algorithm is \u2018lossily\u2019 compressible. We fur\u0002ther established novel tail bounds suggesting that IpS; Wq (or its lossy version) needs to be small\nnot only for the underlying distribution of pS, Wq, but also for any distribution in its vicinity. This\nis in the spirit of stability: the algorithm should be compressible under any small perturbation of\nthe dataset and hypothesis. The new tail bounds are established using a new \u2018information-theoretic\ncovering\u2019 technique, highlighted in Section 4.\nSimilarly, we derive and improve the results based on \u2018conditional mutual information\u2019 (Steinke\nand Zakynthinou, 2020) in Appendix A. Thanks to this approach, we established tail and in expec\u0002tation bounds that recover the VC-dimension bounds (Corollaries 18 and 22); the recovery in terms\nof the tail bound is novel.\nBy exploiting the flexibility of our lossy compression framework, we further extend our results\nand obtain bounds in terms of the intrinsic dimension of the marginal distribution of W, namely\nthe rate-distortion dimension (Kawabata and Dembo, 1994). Our results bring a unified perspective\non mutual information, compressibility, and fractal dimensions, and open up several future research\ndirections as we will point out in Section 5.\n2. Preliminaries\n2.1. Notation and problem setup\nRandom variables, their realizations, and their domains are denoted by upper-case letters, lower-case\nletters, and calligraphy fonts, e.g. X, x, and X . We assume that all the domains are en",
          "original_query": "Rate-distortion theoretic generalization bounds for stochastic learning algorithms",
          "cleaned_query": "Rate-distortion theoretic generalization bounds for stochastic learning algorithms",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] JL-Johnson.pdf - Stanford University",
          "url": "http://stanford.edu/class/cs114/readings/JL-Johnson.pdf",
          "content": "Contemporary Mathematics \nVolume 26, 1984 \nEXTENSIONS OF LIPSCHITZ MAPPINGS INTO A HILBERT SPACE \n1 2 William B. Johnson and Joram Lindenstrauss \nINTRODUCTION \nIn this note we consider the following extension problem for Lipschitz \nfunctions: Given a metric space X and n = 2, 3, 4, ... ' estimate the \nsmallest constant L = L(X, n) so that every ~pping f from every n-element \nsubset of X into t 2 extends to a mapping f from X into t 2 with \n(Here ll&lltip is the Lipschitz constant of the function g.) A classical re\u0002sult of Kirszbraun's [14, p. 48] states that L(t2, n) = 1 for all n, but \nit is easy to see that L(X, n) ~ ~ as n ~ ~ for many metric spaces X. \nMarcus and Pisier [10] initiated the study of L(X, n) for X = Lp. (For \nbrevity, we will use hereafter the notation L(p, n) for L(Lp(O,l), n).) \nThey prove that for each 1 < p < 2 there is a constant C(p) so that for \nn = 2, 3, 4, , , , \n1/p - 1/2 L(p, n) ~ C(p) (Log n) \u2022 \nThe main result of this note is a verification of their conjecture that for \nsome constant C and all n = 2, 3, 4, , , , \nL(X, n) ~ C(Log n) l/2 \nfor all metric spaces X. While our proof is completely different from that \nof Marcus and Pisier, there is a common theme: Probabilistic techniques de\u0002veloped for linear theory are combiaed with Kirszbraun's theorem to yield ex\u0002tension theorems. \nThe main tool for proving Theorem 1 is a simply stated elementary geome\u0002tric lemma, which we now describe: Given n points in Euclidean space, what \n1 \n2 \nSupported in part by NSF MCS-7903042. \nSupported in part by NSF MCS-8102714. \n189 \n\u00a9 1984 American Mathematical Society \n0271-4132/84 $1.00 + $25 per page \nhttp://dx.doi.org/10.1090/conm/026/737400\nLicensed to Stanford Univ. Prepared on Wed Mar 30 15:53:46 EDT 2022for download from IP 171.64.66.240.\nLicense or copyright restrictions may apply to redistribution; see https://www.ams.org/publications/ebooks/terms\n190 JOHNSON AND LINDENSTRAUSS \nis the smallest k = k(n) so that these points can be moved into k-dimensional \nEuclidean space via a transformation which expands or contracts all pairwise \ndistances by a factor of at most 1 + e? The answer, that k ~ C(e) Log n, is \na simple consequence of the isoperimetric inequality for the n-sphere in the \nform studied in [2]. \nIt seems likely that the Marcus-Pisier result and Theorem 1 give the right \norder of growth for L(p, n). While we cannot verify this, in Theorem 3 we get \nthe estimate \nL(p, n) ::: 6 ( Log n ) 1/p - 1/2 \nLog Log n (1 ~ p < 2) \nfor some absolute constant 6 > 0. (Throughout this paper we use the conven\u0002tion that Log X denotes the maximum of 1 and the natural logarithm of x.) \nThis of course gives a lower estimate of \n( Log n \nn) \n1/2 \n6 Log Log \nfor L(~, n). That our approach cannot give a lower bound of \n6(Log n)l/p - l/2 for L(p, n) is shown by Theorem 2, which is an extension \ntheorem for mappings into e2 whose domains are e-separated. \nThe minimal notation we use is introduced as needed. Here we note only \nthat By(y, e) (respectively, by(y, e)) is the closed (respectively, open) \nball in Y about y of radius e. If y = 0, we use By(e) and by(e), \nand we drop the subscript Y when there is no ambiguity. S(Y) is the unit \nsphere of the normed space Y. For isomorphic normed spaces X and Y, we \nlet \nd(X, Y) ~ inf II Til !I T-111, \nwhere the inf is over all invertible linear operators from X onto Y. Given \na bounded Banach space valued function f on a set K, we set \n1. THE EXTENSION THEOREMS \nllfll~ = sup llf(x)[l. \nxEK \nWe begin with the geometrical lemma mentioned in the introduction. \nLEMMA 1. For each 1 > ~ > 0 there is a constant K = K(~) > 0 so that if \nn Act 2 , A= n for some n = 2, 3, \u2022\u2022. , then there is a mapping \nonto a subset of e~ (k _ [K log n]) which satisfies \nf from A \nLicensed to Stanford Univ. Prepared on Wed Mar 30 15:53:46 EDT 2022for download from IP 171.64.66.240.\nLicense or copyright restrictions may apply to redistribution; see https://www.ams.org/publications/ebooks/terms\nEXTENSIONS OF LIPSCHITZ MAPPINGS 191 \nPROOF. The proof will show that if one chooses at random a rank k orthogonal \nn projection on t 2 , then, with positive probability (which can be made arbitra\u0002rily close to one by adjusting k), the projection restricted to A will \nsatisfy the condition on f. To make this precise, we let Q be the projec\u0002tion onto the first k coordinates of l~ and let cr be normalized Haar \nn measure on O(n), the orthogonal group on t 2\u2022 Then the random variable \ndefined by \nn f: (O(n), cr) 4 L(t 2) \nf(u) U* QU \ndetermines the notion of \"random rank k projection.\" The applications of \nLevy's inequality in the first few self-contained pages of [2] make it easy to \ncheck that f(u) has the desired property. For the convenience of the reader, \nwe follow the notation of [2). \nLet I I 1\u20221 II denote the usual Euclidean norm on In and for 1 ~ k ~ n \nand x E In set \nr(x) \nwhich is equal to \nfur our eventual choice of k \nwhich satisfies \n= vn (. ~ x(i) 2\\ \n]. = 1 } \nvn IIIQxlll \n[K log n]. Thus r(\u2022) \nr(x) ~Vn lllxlll \n1/2 \nis a semi-norm on \n(In [2], r(\u2022) is assumed to be a norm, but inasmuch as the left estimate \na! I !xi I I ~ r(x) in formula (2.5) of [2] is not needed in the present situation, \nit is okay that r(\u2022) is only a semi-norm.) \nSetting \nB n-1 \ns ' \nwe want to select U E O(n) so that for some constant M, \nLicensed to Stanford Univ. Prepared on Wed Mar 30 15:53:46 EDT 2022for download from IP 171.64.66.240.\nLicense or copyright restrictions may apply to redistribution; see https://www.ams.org/publications/ebooks/terms\n192 JOHNSON AND LINDENSTBAUSS \nM(l ~ ~) ~ r(Ux) ~ M(l + ~) (x E B) \nLet Mr be the median of r(\u2022) n-1 \non S , so that \nand \n~ 1 [x E Sn-l : r(x) ~ M ] ~ 1/2 n- r \nn-1 where ~n-l is normalized rotationally invariant measure on S \u2022 \nn-1 We have from page 58 of [2] that for each y E S and e > 0, \na[U E O(n) : Mr - Vn e ~ r(Uy) ~ Mr + Vn e] :::: 1 - 4 exp ( -n~ 2 ). \nHence \n(1.1) a[U E O(n) M - Vn e ~ r(Uy)) ::: M + Vn e for all y E B) :::: r r \n:::: 1 - 2n(n+l) exp (-n~ 2 ). \nBy Lemma 1.7 of [2], there is a constant \nso that \n(1.2) \n.. c ~ 4 \nm = 1 \n(m+l) \n2 -m /2 e \nII r(x) d~n-l(x) - Mrl < C \u2022 \n8n-l \nWe now repeat a known argument for estimating I r(x) ~n-l(x) which uses \nonly Khintchine's inequality. 8n-l \nFor 1 ::: k ::: n we have: \nSetting \nk \nAv ! 1 I Z \u00b1 x(i) I d~ 1 (x) \u2022 \u00b1 sn- i = 1 n\u0002k \n= Av ! I< x, Z \u00b1 6i >I d~ _1 (x) \u00b1 sn-1 i \u2022 1 n \n= Yk /n-11< x, <\\>I ~n-l(x) \ns \nrby the rotational ] \nLinvariance of ~n-l \nLicensed to Stanford Univ. Prepared on Wed Mar 30 15:53:46 EDT 2022for download from IP 171.64.66.240.\nLicense or copyright restrictions may apply to redistribution; see https://www.ams.org/publications/ebooks/terms\nEXTENSIONS OF LIPSCHITZ 193 \nwe have from Khintchine 1 s inequality that for each 1 ~ k ~ n, \nVnk a ~ J 1 rk(x) c\\1. 1(x) ~ V2nk a n sn- n- n \n(l:e plugged in the exact constant of VZ in Khintchine's inequality calcu\u0002lated in [5] and [13], but of course any constant would serve as well.) \nSince obviously r (x) = y-n we conclude that for 1 ~ k ~ n n , \n(1. 3) Vk/ ~ J n-1 rk (x) ~n-1 (x) ~ Vk \u00b7 s \nSpecializing now to the case k \nthat \n[K log n], we have from (1.2) and (1.3) \nat least for K log n sufficiently large. Thus if we define \nwe get from (1.1) that \no [U E O(n) : (1 - -r)M ~ r(Uy) ~ (1 + 't\")M for all y E B] r r \n::: 1 - 2n(n + 1) exp (- ,.~sk) \n(- T \n2 K log ::: 1 - 2n(n + 1) exp n) 18 \nwhich is positive if, say, \nD \nIt is easily seen that the estimate K log n in Lemma 1 cannot be im\u0002proved. Indeed, in a ball of radius 2 in e~ there are at most 4k vectors \n{xi} so that llxi - xj II 0:: 1 for every i {< j (see the proof of Lemma 3 \nbelow). Hence for 't\" sufficiently small there is no map F which maps an \nk orthonormal set with more than 4 vectors into a k-dimensional subspace of \ne2 with \nWe can now verify the conjecture of Marcus and Pisier [~0]. \nLicensed to Stanford Univ. Prepared on Wed Mar 30 15:53:46 EDT 2022for download from IP 171.64.66.240.\nLicense or copyright restrictions may apply to redistribution; see https://www.ams.org/publications/ebooks/terms\n194 JOHNSON AND LINDENSTRAUSS \nTHEOREM 1. Sup (log n)- 112 L(~, n) < ~. In other words: there is a \nn '\"' 2, 3, \nconstant K so that for all metric spaces X and all finite subsets M of X \n(card M = n, say) every function f from M into t 2 has a Lipschitz exten\u0002sion f : X 4 t 2 which satisfies \nPROOF. Given X, M c X with card M = n, and f: M4 t 2 , \nWe apply Lemma 1 with ~ = 1/2 to get a one-to-one function \nset \n-1 \ng \n-1 k a subset g [A] of t 2 (where k ~ K log n) which satisfies \nBy Kirszbraun's theorem, we can extend g to a function g \na way that \nN \nIJgiiHp ~ 3 \u2022 \nLet I tk 4 tk denote the formal identity map, so that 2 ~ \nThen \n-l h : M 4 tk h : Ig f, ~ \nA = f [M]. \nfrom A onto \nhas Lipschitz norm at most \n(see, e.g., p. 48 of [14]), \nliflltip' so by the non-linear Hahn-Banach theorem \nh can be extended to a mapping \nwhich satisfies \nThen \nN -1 N \nf : g I h; f X 4 t 2 \nis an extension of f and satisfies \n[J \nLicensed to Stanford Univ. Prepared on Wed Mar 30 15:53:46 EDT 2022for download from IP 171.64.66.240.\nLicense or copyright restrictions may apply to redistribution; see https://www.ams.org/publications/ebooks/terms\nEXTENSIONS OF LIPSCHITZ MAPPINGS \nNext we outline our approach to the problem of obtaining \nf the inclusion mapping from an e-net for \n195 \na lower bound for \nSN-l into eN \n2' L(~,n). Take for \nand consider e~ isometrically embedded into L~. A Lipschitz extension of f \nto a mapping f : L -+ e .. 2 should act like the identity so the techniques \nof (8] should yield a linear projection from L.., onto whose norm is of \norder II fll e ip. Since is complemented in L.., only of order v-N and there \nare e-nets for of cardinality N n = [4/e] , we should get that \nL(00 ,n) ::: VN::: ( ) \n1/2 \n15 Log n \n- Loge \nIn Theorem 2 we make this approach work when e is of order \nL(oo ,n) ;:: 15' og n (\n. L )1/2 \n-2 N , so we get \nThat the difficulties we incur with the outlined approach ",
          "original_query": "Extensions of Lipschitz mappings into a Hilbert space (Johnson\u2013Lindenstrauss type results)",
          "cleaned_query": "Extensions of Lipschitz mappings into a Hilbert space (Johnson\u2013Lindenstrauss type results)",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "CMI Bounds for SGD via Rate\u2013Distortion Quantization of Trajectories\nDevelop a framework that upper-bounds conditional mutual information (CMI) of SGD by quantizing *the entire optimization trajectory* using rate\u2013distortion theory, rather than only the final iterate. Derive explicit gen bounds in terms of trajectory distortion (e.g., average gradient-noise energy) and validate by measuring empirical CMI proxies across step sizes, batch sizes, and noise levels.",
        "Johnson\u2013Lindenstrauss (JL) Compression as a Concrete Lossy Coding Scheme for Generalization\nUse JL random projections to construct an explicit lossy compression pipeline for learned parameters (or features) and plug it into rate\u2013distortion generalization bounds to obtain computable guarantees. Study when projecting to \\(k=\\tilde{O}(\\log n)\\) dimensions preserves both empirical risk and low information leakage, and provide an end-to-end bound linking \\(k\\) to CMI and excess risk.",
        "Tight CMI\u2013Accuracy Tradeoffs Beyond Strong Convexity in Stochastic Convex Optimization\nExtend Attias/Livni-style lower bounds from strongly convex objectives to merely convex, uniformly convex, or exp-concave losses under Lipschitz/bounded assumptions. Construct explicit hard distributions and prove matching upper bounds for specific algorithms (e.g., noisy mirror descent), yielding a sharp phase diagram of \u201chow much memorization is necessary\u201d across curvature regimes.",
        "Adaptive Data Reuse in Optimization: Compositional CMI Accounting Across Rounds\nBuild a composition theorem for CMI when the same dataset is reused across multiple optimization runs/hyperparameter trials (adaptive data analysis setting). Provide an accounting method analogous to privacy composition: track per-round conditional MI increments (estimated from algorithm noise/regularization) to certify validity under many sequential training decisions.",
        "An \u201cInformation Complexity\u201d Regularizer for ERM with Provable CMI Control\nDesign an ERM variant that directly penalizes an empirical upper bound on \\(I(W;S\\mid \\text{aux})\\) (or a tractable surrogate), combining Xu\u2013Raginsky\u2019s relative-entropy regularization with Steinke\u2013Zakynthinou\u2019s CMI framework. Prove that the resulting optimizer achieves a target excess risk with minimal necessary CMI in regimes where lower bounds suggest memorization is unavoidable.",
        "Sample-Reconstruction Attacks as a Benchmark for Memorization Predicted by CMI\nTurn the adversary constructions (identifying training points) into a standardized empirical protocol: given a trained model, attempt to reconstruct membership or actual data points and compare success to theoretical CMI lower bounds. The contribution is a calibrated \u201cmemorization audit\u201d that tests whether CMI-based bounds are tight predictors of real reconstruction risk across SCO and neural networks.",
        "Dimension-Free Generalization via Conditional MI with Data-Dependent Conditioning Variables\nInvestigate whether appropriate conditioning (e.g., on a random projection of data, a JL sketch, or a learned representation) yields CMI bounds that avoid the dimension dependence highlighted by Livni. Propose specific conditioning schemes and prove when \\(I(W;S\\mid U)\\) can be small even if \\(I(W;S)\\) is large, while still implying strong generalization.",
        "Rate\u2013Distortion Dimension for Neural Network Hypothesis Classes via Local Fractal Geometry\nExtend Sefidgaran et al.\u2019s rate\u2013distortion dimension approach by estimating *local* fractal/rate\u2013distortion dimensions along the optimization path of overparameterized networks (e.g., near minima). Provide algorithms to estimate these dimensions from checkpoints and connect them to observed generalization, giving a measurable complexity notion that unifies compressibility and MI in modern settings.",
        "Information Lower Bounds for Robust/Adversarial Generalization in SCO\nDefine a robust generalization gap (under distribution shift or adversarial perturbations) and derive MI/CMI-based lower bounds showing how robustness increases required information leakage about the sample. Construct SCO instances where achieving adversarial robustness forces higher CMI than standard accuracy, formalizing a memorization\u2013robustness tradeoff.",
        "JL-Enabled Lipschitz Extension Meets Generalization: Extending Predictors from Coresets\nCombine Lipschitz extension theory (Johnson\u2013Lindenstrauss/extension constants) with compression schemes: learn a predictor on a small coreset, embed it via JL to preserve distances, then extend it to the full space with controlled Lipschitz constant. Prove generalization bounds that depend on coreset size and extension distortion, yielding an actionable pipeline for provably generalizing models from tiny summaries."
      ],
      "generated_ideas_raw": "1. **CMI Bounds for SGD via Rate\u2013Distortion Quantization of Trajectories**  \n   Develop a framework that upper-bounds conditional mutual information (CMI) of SGD by quantizing *the entire optimization trajectory* using rate\u2013distortion theory, rather than only the final iterate. Derive explicit gen bounds in terms of trajectory distortion (e.g., average gradient-noise energy) and validate by measuring empirical CMI proxies across step sizes, batch sizes, and noise levels.\n\n2. **Johnson\u2013Lindenstrauss (JL) Compression as a Concrete Lossy Coding Scheme for Generalization**  \n   Use JL random projections to construct an explicit lossy compression pipeline for learned parameters (or features) and plug it into rate\u2013distortion generalization bounds to obtain computable guarantees. Study when projecting to \\(k=\\tilde{O}(\\log n)\\) dimensions preserves both empirical risk and low information leakage, and provide an end-to-end bound linking \\(k\\) to CMI and excess risk.\n\n3. **Tight CMI\u2013Accuracy Tradeoffs Beyond Strong Convexity in Stochastic Convex Optimization**  \n   Extend Attias/Livni-style lower bounds from strongly convex objectives to merely convex, uniformly convex, or exp-concave losses under Lipschitz/bounded assumptions. Construct explicit hard distributions and prove matching upper bounds for specific algorithms (e.g., noisy mirror descent), yielding a sharp phase diagram of \u201chow much memorization is necessary\u201d across curvature regimes.\n\n4. **Adaptive Data Reuse in Optimization: Compositional CMI Accounting Across Rounds**  \n   Build a composition theorem for CMI when the same dataset is reused across multiple optimization runs/hyperparameter trials (adaptive data analysis setting). Provide an accounting method analogous to privacy composition: track per-round conditional MI increments (estimated from algorithm noise/regularization) to certify validity under many sequential training decisions.\n\n5. **An \u201cInformation Complexity\u201d Regularizer for ERM with Provable CMI Control**  \n   Design an ERM variant that directly penalizes an empirical upper bound on \\(I(W;S\\mid \\text{aux})\\) (or a tractable surrogate), combining Xu\u2013Raginsky\u2019s relative-entropy regularization with Steinke\u2013Zakynthinou\u2019s CMI framework. Prove that the resulting optimizer achieves a target excess risk with minimal necessary CMI in regimes where lower bounds suggest memorization is unavoidable.\n\n6. **Sample-Reconstruction Attacks as a Benchmark for Memorization Predicted by CMI**  \n   Turn the adversary constructions (identifying training points) into a standardized empirical protocol: given a trained model, attempt to reconstruct membership or actual data points and compare success to theoretical CMI lower bounds. The contribution is a calibrated \u201cmemorization audit\u201d that tests whether CMI-based bounds are tight predictors of real reconstruction risk across SCO and neural networks.\n\n7. **Dimension-Free Generalization via Conditional MI with Data-Dependent Conditioning Variables**  \n   Investigate whether appropriate conditioning (e.g., on a random projection of data, a JL sketch, or a learned representation) yields CMI bounds that avoid the dimension dependence highlighted by Livni. Propose specific conditioning schemes and prove when \\(I(W;S\\mid U)\\) can be small even if \\(I(W;S)\\) is large, while still implying strong generalization.\n\n8. **Rate\u2013Distortion Dimension for Neural Network Hypothesis Classes via Local Fractal Geometry**  \n   Extend Sefidgaran et al.\u2019s rate\u2013distortion dimension approach by estimating *local* fractal/rate\u2013distortion dimensions along the optimization path of overparameterized networks (e.g., near minima). Provide algorithms to estimate these dimensions from checkpoints and connect them to observed generalization, giving a measurable complexity notion that unifies compressibility and MI in modern settings.\n\n9. **Information Lower Bounds for Robust/Adversarial Generalization in SCO**  \n   Define a robust generalization gap (under distribution shift or adversarial perturbations) and derive MI/CMI-based lower bounds showing how robustness increases required information leakage about the sample. Construct SCO instances where achieving adversarial robustness forces higher CMI than standard accuracy, formalizing a memorization\u2013robustness tradeoff.\n\n10. **JL-Enabled Lipschitz Extension Meets Generalization: Extending Predictors from Coresets**  \n   Combine Lipschitz extension theory (Johnson\u2013Lindenstrauss/extension constants) with compression schemes: learn a predictor on a small coreset, embed it via JL to preserve distances, then extend it to the full space with controlled Lipschitz constant. Prove generalization bounds that depend on coreset size and extension distortion, yielding an actionable pipeline for provably generalizing models from tiny summaries.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "CMI Bounds for SGD via Rate\u2013Distortion Quantization of Trajectories\nDevelop a framework that upper-bounds conditional mutual information (CMI) of SGD by quantizing *the entire optimization trajectory*",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Johnson\u2013Lindenstrauss (JL) Compression as a Concrete Lossy Coding Scheme for Generalization\nUse JL random projections to construct an explicit lossy compression pipeline for learned parameters (or fea",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Tight CMI\u2013Accuracy Tradeoffs Beyond Strong Convexity in Stochastic Convex Optimization\nExtend Attias/Livni-style lower bounds from strongly convex objectives to merely convex, uniformly convex, or exp",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Adaptive Data Reuse in Optimization: Compositional CMI Accounting Across Rounds\nBuild a composition theorem for CMI when the same dataset is reused across multiple optimization runs/hyperparameter tri",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "An \u201cInformation Complexity\u201d Regularizer for ERM with Provable CMI Control\nDesign an ERM variant that directly penalizes an empirical upper bound on \\(I(W;S\\mid \\text{aux})\\) (or a tractable surrogate)",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Sample-Reconstruction Attacks as a Benchmark for Memorization Predicted by CMI\nTurn the adversary constructions (identifying training points) into a standardized empirical protocol: given a trained mo",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Dimension-Free Generalization via Conditional MI with Data-Dependent Conditioning Variables\nInvestigate whether appropriate conditioning (e.g., on a random projection of data, a JL sketch, or a learne",
          "is_match": true
        },
        {
          "idea_idx": 7,
          "idea_text": "Rate\u2013Distortion Dimension for Neural Network Hypothesis Classes via Local Fractal Geometry\nExtend Sefidgaran et al.\u2019s rate\u2013distortion dimension approach by estimating *local* fractal/rate\u2013distortion d",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Information Lower Bounds for Robust/Adversarial Generalization in SCO\nDefine a robust generalization gap (under distribution shift or adversarial perturbations) and derive MI/CMI-based lower bounds sh",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "JL-Enabled Lipschitz Extension Meets Generalization: Extending Predictors from Coresets\nCombine Lipschitz extension theory (Johnson\u2013Lindenstrauss/extension constants) with compression schemes: learn a",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 42,
      "paper_title": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning",
      "contribution": "Introduce a local data-attribution framework for online RL (PPO) using gradient-similarity-based influence from recent buffers, and leverage it to diagnose learning and to iteratively filter experiences (IIF) to speed and stabilize training.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 10689,
      "output_tokens": 1028,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1707.06347] Proximal Policy Optimization Algorithms - arXiv",
          "url": "https://arxiv.org/abs/1707.06347",
          "content": "[1707.06347] Proximal Policy Optimization Algorithms[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1707.06347\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1707.06347**(cs)\n[Submitted on 20 Jul 2017 ([v1](https://arxiv.org/abs/1707.06347v1)), last revised 28 Aug 2017 (this version, v2)]\n# Title:Proximal Policy Optimization Algorithms\nAuthors:[John Schulman](https://arxiv.org/search/cs?searchtype=author&amp;query=Schulman,+J),[Filip Wolski](https://arxiv.org/search/cs?searchtype=author&amp;query=Wolski,+F),[Prafulla Dhariwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Dhariwal,+P),[Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford,+A),[Oleg Klimov](https://arxiv.org/search/cs?searchtype=author&amp;query=Klimov,+O)\nView a PDF of the paper titled Proximal Policy Optimization Algorithms, by John Schulman and 4 other authors\n[View PDF](https://arxiv.org/pdf/1707.06347)> > Abstract:\n> We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a &#34;surrogate&#34; objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. Subjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:1707.06347](https://arxiv.org/abs/1707.06347)[cs.LG]|\n|(or[arXiv:1707.06347v2](https://arxiv.org/abs/1707.06347v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1707.06347](https://doi.org/10.48550/arXiv.1707.06347)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: John Schulman [[view email](https://arxiv.org/show-email/b7d77275/1707.06347)]\n**[[v1]](https://arxiv.org/abs/1707.06347v1)**Thu, 20 Jul 2017 02:32:33 UTC (2,178 KB)\n**[v2]**Mon, 28 Aug 2017 09:20:06 UTC (2,537 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Proximal Policy Optimization Algorithms, by John Schulman and 4 other authors\n* [View PDF](https://arxiv.org/pdf/1707.06347)\n* [TeX Source](https://arxiv.org/src/1707.06347)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1707.06347&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1707.06347&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2017-07](https://arxiv.org/list/cs.LG/2017-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/1707.06347?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1707.06347)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1707.06347)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1707.06347)\n### [18 blog links](https://arxiv.org/tb/1707.06347)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/SchulmanWDRK17)\n[John Schulman]()\n[Filip Wolski]()\n[Prafulla Dhariwal]()\n[Alec Radford]()\n[Oleg Klimov]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1707.06347)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Proximal Policy Optimization Algorithms (PPO)",
          "cleaned_query": "Proximal Policy Optimization Algorithms (PPO)",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Understanding Black-box Predictions via Influence Functions - arXiv",
          "url": "https://arxiv.org/abs/1703.04730",
          "content": "# Statistics > Machine Learning\n\n**arXiv:1703.04730** (stat)\n\n\\[Submitted on 14 Mar 2017 ( [v1](https://arxiv.org/abs/1703.04730v1)), last revised 29 Dec 2020 (this version, v3)\\]\n\n# Title:Understanding Black-box Predictions via Influence Functions\n\nAuthors: [Pang Wei Koh](https://arxiv.org/search/stat?searchtype=author&query=Koh,+P+W), [Percy Liang](https://arxiv.org/search/stat?searchtype=author&query=Liang,+P)\n\nView a PDF of the paper titled Understanding Black-box Predictions via Influence Functions, by Pang Wei Koh and Percy Liang\n\n[View PDF](https://arxiv.org/pdf/1703.04730)\n\n> Abstract:How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.\n\n| | |\n| --- | --- |\n| Comments: | International Conference on Machine Learning, 2017. (This version adds more historical references and fixes typos.) |\n| Subjects: | Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1703.04730](https://arxiv.org/abs/1703.04730) \\[stat.ML\\] |\n| (or [arXiv:1703.04730v3](https://arxiv.org/abs/1703.04730v3) \\[stat.ML\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1703.04730](https://doi.org/10.48550/arXiv.1703.04730) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Pang Wei Koh \\[ [view email](https://arxiv.org/show-email/ed31f4a2/1703.04730)\\] **[\\[v1\\]](https://arxiv.org/abs/1703.04730v1)**\nTue, 14 Mar 2017 21:07:01 UTC (4,753 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1703.04730v2)**\nMon, 10 Jul 2017 02:31:54 UTC (4,538 KB)\n**\\[v3\\]**\nTue, 29 Dec 2020 22:40:43 UTC (4,831 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Understanding Black-box Predictions via Influence Functions, by Pang Wei Koh and Percy Liang\n\n- [View PDF](https://arxiv.org/pdf/1703.04730)\n- [TeX Source](https://arxiv.org/src/1703.04730)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1703.04730&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1703.04730&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2017-03](https://arxiv.org/list/stat.ML/2017-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1703.04730?context=cs) [cs.AI](https://arxiv.org/abs/1703.04730?context=cs.AI) [cs.LG](https://arxiv.org/abs/1703.04730?context=cs.LG) [stat](https://arxiv.org/abs/1703.04730?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1703.04730)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1703.04730)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1703.04730)\n\n### [3 blog links](https://arxiv.org/tb/1703.04730)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1703.04730) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Understanding Black-box Predictions via Influence Functions",
          "cleaned_query": "Understanding Black-box Predictions via Influence Functions",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Estimating Training Data Influence by Tracing Gradient Descent",
          "url": "https://proceedings.neurips.cc/paper/2020/file/e6385d39ec9394f2f3a354d9d2b88eec-Paper.pdf",
          "content": "Estimating Training Data Influence\nby Tracing Gradient Descent\nGarima\u21e4\nGoogle\npruthi@google.com\nFrederick Liu\u21e4\nGoogle\nfrederickliu@google.com\nSatyen Kale\nGoogle\nsatyenkale@google.com\nMukund Sundararajan \u2020\nGoogle\nmukunds@google.com\nAbstract\nWe introduce a method called TracIn that computes the influence of a training\nexample on a prediction made by the model. The idea is to trace how the loss on\nthe test point changes during the training process whenever the training example of\ninterest was utilized. We provide a scalable implementation of TracIn via: (a) a\nfirst-order gradient approximation to the exact computation, (b) saved checkpoints\nof standard training procedures, and (c) cherry-picking layers of a deep neural\nnetwork. In contrast with previously proposed methods, TracIn is simple to\nimplement; all it needs is the ability to work with gradients, checkpoints, and loss\nfunctions. The method is general. It applies to any machine learning model trained\nusing stochastic gradient descent or a variant of it, agnostic of architecture, domain\nand task. We expect the method to be widely useful within processes that study\nand improve training data. Code is available at [1].\n1 Motivation\nDeep learning has been used to solve a variety of real-world problems. A common form of machine\nlearning is supervised learning, where the model is trained on labelled data. Controlling the training\ndata input to the model is one of the main quality knobs to improve the quality of the deep learning\nmodel. For instance, such a technique could be used to identify and fix mislabelled data using the\nworkflow described in Section 4.1. Our main motivation is to identify practical techniques to improve\nthe analysis of the training data. Specifically, we study the problem of identifying the influence\nof training examples on the prediction of a test example. We propose a method called TracIn for\ncomputing this influence, provide a scalable implementation, and evaluate the method experimentally.\n2 Related Work\n[2, 3] tackle influential training examples in the context of deep learning. We discuss these methods\nin detail in Section 4.4.\nThere are related notions of influence used to explain deep learning models that differ in either the\ntarget of the explanation or the choice of influencer or both. For instance, [4, 5, 6] identify the\ninfluence of features on an individual prediction. [7, 8] identify the influence of features on the\n\u21e4Equal contribution.\n\u2020Corresponding author.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\noverall accuracy (loss) of the model. [9, 10] identify the influence of training examples on the overall\naccuracy of the model. [11], a technique closely related to TracIn, identifies the influence of training\nexamples on the overall loss by tracing the training process while TracIn identifies the influence on\nthe loss of a test point. The key trick in [11] is to use a certain hessian of the model parameters to\ntrace the influence of a training point through minibatches in which it is absent. This trick is also\npotentially useful in implementing idealized version of TracIn. However, idealized TracIn requires\nthe test/inference point to be known at training time and is therefore impractical, making the trick less\nrelevant to the problem we study. TracInCP, a practical implementation, leverages checkpoints to\nreplay the training process. Checkpoint ensembling is a widely used technique in machine translation\n[12], semi-supervised learning [13] and knowledge distillation [14] which provide intuition on why\nTracIn performs better than other methods.\n3 The Method\nIn this section we define TracIn. TracIn is inspired by the fundamental theorem of calculus. The\nfundamental theorem of calculus decomposes the difference between a function at two points using\nthe gradients along the path between the two points. Analogously, TracIn decomposes the difference\nbetween the loss of the test point at the end of training versus at the beginning of training along the\npath taken by the training process.3\nWe start with an idealized definition to clarify the idea, but this definition will be impractical because\nit would require that the test examples (the ones to be explained) to be specified at training time. We\nwill then develop practical approximations that resolve this constraint.\n3.1 Idealized Notion of Influence\nLet Z represent the space of examples, and we represent training or test examples in Z by the notation\nz, z0 etc. We train predictors parameterized by a weight vector w 2 Rp. We measure the performance\nof a predictor via a loss function ` : Rp \u21e5 Z ! R; thus, the loss of a predictor parameterized by w\non an example z is given by `(w, z).\nGiven a set of n training points S = {z1, z2,...,zn 2 Z}, we train the predictor by finding\nparameters w that minimize the training loss Pn\ni=1 `(w, zi), via an iterative optimization procedure\n(such as stochastic gradient descent) which utilizes one training example zt 2 S in iteration t,\nupdating the parameter vector from wt to wt+1. Then the idealized notion of influence of a particular\ntraining example z 2 S on a given test example4 z0 2 Z is defined as the total reduction in loss on\nthe test example z0 that is induced by the training process whenever the training example z is utilized,\ni.e.TracInIdeal(z, z0) = P\nt: zt=z `(wt, z0\n) `(wt+1, z0)\nRecall that TracIn was inspired by the fundamental theorem of calculus, which has the property that\nthe integration of the gradients of a function between two points is equal to the difference between\nfunction values between the two points. Analogously, idealized influence has the appealing property\nthat the sum of the influences of all training examples on a fixed test point z0 is exactly the total\nreduction in loss on z0 in the training process:\nLemma 3.1 Suppose the initial parameter vector before starting the training process is w0, and the\nfinal parameter vector is wT . Then Pn\ni=1 TracInIdeal(zi, z0\n) = `(w0, z0) `(wT , z0)\nOur treatment above assumes that the iterative optimization technique operates on one training\nexample at a time. Practical gradient descent algorithms almost always operate with a group of\ntraining examples, i.e., a minibatch. We cannot extend the definition of idealized influence to this\nsetting, because there is no obvious way to redistribute the loss change across members of the\nminibatch. In Section 3.2, we will define an approximate version for minibatches.\nRemark 3.2 (Proponents and Opponents) We will term training examples that have a positive\nvalue of influence score as proponents, because they serve to reduce loss, and examples that have\n3\nWith the minor difference that the training process is a discrete process, whereas the path used within the\nfundamental theorem is continuous. 4\nBy test example, we simply mean an example whose prediction is being explained. It doesn\u2019t have to be in\nthe test set.\n2\na negative value of influence score as opponents, because they increase loss. In [2], proponents\nare called \u2019helpful\u2019 examples, and opponents called \u2019harmful\u2019 examples. We chose more neutral\nterms to make the discussions around mislabelled test examples more natural. [3] uses the terms\n\u2019excitory\u2019 and \u2019inhibitory\u2019, which can be interpreted as proponents and opponents for test examples\nthat are correctly classified, and the reverse if they are misclassified. The distinction arises because\nthe representer approach explains the prediction score and not the loss.\n3.2 First-order Approximation to Idealized Influence, and Extension to Minibatches\nSince the step-sizes used in updating the parameters in the training process are typically quite small,\nwe can approximate the change in the loss of a test example in a given iteration t via a simple\nfirst-order approximation: `(wt+1, z0) = `(wt, z0) + r`(wt, z0) \u00b7 (wt+1 wt) + O(kwt+1 wtk2).\nHere, the gradient is with respect to the parameters and is evaluated at wt. Now, if stochastic gradient\ndescent is utilized in training the model, using the training point zt at iteration t, then the change in\nparameters is wt+1 wt = \u2318tr`(wt, zt), where \u2318t is the step size in iteration t. Note that this\nformula should be changed appropriately if other optimization methods (such as AdaGrad, Adam, or\nNewton\u2019s method) are used to update the parameters. The first-order approximation remains valid,\nhowever, as long as a small step-size is used in the update.\nFor the rest of this section we restrict to gradient descent for concreteness. Substituting the change\nin parameters formula in the first-order approximation, and ignoring the higher-order term (which\nis of the order of O(\u23182\nt )), we arrive at the following first-order approximation for the change in the\nloss `(wt, z0) `(wt+1, z0) \u21e1 \u2318tr`(wt, z0)\u00b7 r`(wt, zt). For a particular training example z, we can\napproximate the idealized influence by summing up this approximation in all the iterations in which\nz was used to update the parameters. We call this first-order approximation TracIn, our primary\nnotion of influence: TracIn(z, z0) = P\nt: zt=z \u2318tr`(wt, z0\n) \u00b7 r`(wt, z).\nTo handle minibatches of size b 1, we compute the influence of a minibatch on the test\npoint z0, mimicking the derivation in Section 3.1, and then take its first-order approximation:\nFirst-Order Approximation(Bt, z0) = 1\nb\nP\nz2Bt \u2318tr`(wt, z0\n) \u00b7 r`(wt, z), because the gradient for\nthe minibatch Bt is 1\nb\nP\nz2Bt r`(wt, z). Then, for each training point z 2 Bt, we attribute the\n1\nb \u00b7 \u2318tr`(wt, z0\n) \u00b7 r`(wt, z) portion of the influence of Bt on the test point z0. Summing up over\nall iterations t in which a particular training point z was chosen in Bt, we arrive at the following\ndefinition of TracIn with minibatches: TracIn(z, z0) = 1\nb\nP\nt: z2Bt \u2318tr`(wt, z0\n) \u00b7 r`(wt, z).\nRemark 3.3 The derivation suggests a way to measure the goodness of the approximation for a\ngiven step: We can check that the change in loss for a step `(wt, z0) `(wt+1, z0) is approx",
          "original_query": "TracIn: Estimating Training Data Influence by Tracking Gradients",
          "cleaned_query": "TracIn: Estimating Training Data Influence by Tracking Gradients",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Uncertainty Prioritized Experience Replay",
          "url": "https://rlj.cs.umass.edu/2025/papers/RLJ_RLC_2025_45.pdf",
          "content": "Reinforcement Learning Journal 2025\n\f\n\f Cover Page\nUncertainty Prioritized Experience Replay\nRodrigo Carrasco-Davis, Sebastian Lee, Claudia Clopath, Will Dabney\nKeywords: Experience Replay, Uncertainty Estimation, Information Gain\nSummary\nPrioritized experience replay, which improves sample efficiency by selecting relevant tran\u0002sitions to update parameter estimates, is a crucial component of contemporary value-based deep\nreinforcement learning models. Typically, transitions are prioritized based on their temporal\ndifference error. However, this approach is prone to favoring noisy transitions, even when\nthe value estimation closely approximates the target mean. This phenomenon resembles the\nnoisy TV problem postulated in the exploration literature, in which exploration-guided agents\nget stuck by mistaking noise for novelty. To mitigate the disruptive effects of noise in value\nestimation, we propose using epistemic uncertainty to guide the prioritization of transitions\nfrom the replay buffer. Epistemic uncertainty quantifies the uncertainty that can be reduced by\nlearning, hence reducing transitions sampled from the buffer generated by unpredictable ran\u0002dom processes. We first illustrate the benefits of epistemic uncertainty prioritized replay in two\ntabular toy models: a simple multi-arm bandit task, and a noisy gridworld. Subsequently, we\nevaluate our prioritization scheme on the Atari suite, outperforming quantile regression deep\nQ-learning benchmarks; thus forging a path for the use of epistemic uncertainty prioritized\nreplay in reinforcement learning agents.\nContribution(s)\n1. We introduce a new decomposition of uncertainties in reinforcement learning extending\nprevious formulations of epistemic and aleatoric uncertainty estimators (Clements et al.,\n2020) to include a distance-to-target term. This decomposition better accounts for bias\u0002variance trade-offs in the underlying estimator.\nContext: While Clements et al. (2020) start by defining total uncertainty estimator as the\nvariance over distributional and ensemble dimensions of the value estimate, we start instead\nfrom the average square error to the target over distributional and ensemble dimensions.\nUnder the definitions given by Lahlou et al. (2022) in their Direct Epistemic Uncertainty\nPrediction (DEUP) framework, this yields a modified epistemic uncertainty estimator that\nwe term the target epistemic uncertainty.\n2. We propose using these measures of epistemic and aleatoric uncertainty in an information\ngain criterion to prioritize experience replay in reinforcement learning. We call this priori\u0002tization scheme Uncertainty Prioritized Experience Replay (UPER).\nContext: The de facto method for prioritizing replay in reinforcement learning has been the\nabsolute value of the temporal difference error since its introduction by Schaul et al. (2016).\nHowever we argue that this can lead to sub-optimal behavior in noisy environments. We go\non to derive the information gain prioritization criterion from principled treatment of a toy\nBayesian problem.\n3. We demonstrate the effectiveness of this prioritization scheme in two toy models (a bandit\nand gridworld), as well as in a deep learning model on the Atari test suite. In the latter we\nuse an ensemble of distributional QR agents (Dabney et al., 2017) to estimate the relevant\nuncertainty quantities.\nContext: We provide a series of ablation studies in Atari that isolate the effect of the\nprioritization variable (from architectural changes such as adding an ensemble), showing\nthat UPER could be a promising alternative to PER and other uncertainty measures like\nplain ensemble disagreement.\nUncertainty Prioritized Experience Replay\nUncertainty Prioritized Experience Replay\nRodrigo Carrasco-Davis1,\u2020, Sebastian Lee1,2,4,\u2020, Claudia Clopath3,4, Will\nDabney5\nrodrigo.cd.20@ucl.ac.uk,\nsebastianlee@flatironinstitute.org\n1Gatsby Computational Neuroscience Unit, University College London\n2Center for Computational Neuroscience, Flatiron Institute\n3Sainsbury Wellcome Centre for Neural Circuits and Behaviour, University College London\n4Bioengineering Department, Imperial College London\n5Google DeepMind\n\u2020 Equal contribution\nAbstract\nPrioritized experience replay, which improves sample efficiency by selecting relevant\ntransitions to update parameter estimates, is a crucial component of contemporary\nvalue-based deep reinforcement learning models. Typically, transitions are prioritized\nbased on their temporal difference error. However, this approach is prone to favoring\nnoisy transitions, even when the value estimation closely approximates the target mean.\nThis phenomenon resembles the noisy TV problem postulated in the exploration lit\u0002erature, in which exploration-guided agents get stuck by mistaking noise for novelty.\nTo mitigate the disruptive effects of noise in value estimation, we propose using epis\u0002temic uncertainty estimation to guide the prioritization of transitions from the replay\nbuffer. Epistemic uncertainty quantifies the uncertainty that can be reduced by learning,\nhence reducing transitions sampled from the buffer generated by unpredictable random\nprocesses. We first illustrate the benefits of epistemic uncertainty prioritized replay in\ntwo tabular toy models: a simple multi-arm bandit task, and a noisy gridworld. Subse\u0002quently, we evaluate our prioritization scheme on the Atari suite, outperforming quantile\nregression deep Q-learning benchmarks; thus forging a path for the use of uncertainty\nprioritized replay in reinforcement learning agents.\n1 Introduction\nDeep Reinforcement Learning (DRL) has proven highly effective across a diverse array of problems,\nconsistently yielding state-of-the-art results in control of dynamical systems (Nian et al., 2020; De\u0002grave et al., 2022; Weinberg et al., 2023), abstract strategy games (Mnih et al., 2015; Silver et al.,\n2016), continual learning (Khetarpal et al., 2022; Team et al., 2021), and multi-agent learning (Ope\u0002nAI et al., 2019; Baker et al., 2020). It has also been established as a foundational theory for\nexplaining phenomena in cognitive neuroscience (Botvinick et al., 2020; Subramanian et al., 2022).\nNonetheless, a significant drawback of these methods pertains to their inherent sample inefficiency\nwhereby accurate estimations of value and policy necessitate a substantial demand for interactions\nwith the environment.\nSample inefficiency has been mitigated through the use of, among other methods, Prioritized Expe\u0002rience Replay (PER) (Schaul et al., 2016). PER is an extension of Experience Replay (Lin, 1992),\nwhich uses a memory buffer populated with past agent transitions to improve training stability\nthrough the temporal de-correlation of data used in parameter updates. Subsequently, PER extends\nReinforcement Learning Journal 2025\nthis approach by sampling transitions from the buffer with probabilities proportional to their abso\u0002lute Temporal Difference (TD) error, thereby allowing agents to prioritize learning from pertinent\ndata. PER has been widely adopted as a standard technique in DRL; however, despite significantly\nbetter performance over uniform sampling in most cases, it is worth noting that PER can encounter\nlimitations under specific task conditions and agent designs. The most prominent example of such a\nlimitation is related to the so-called noisy TV problem (Burda et al., 2018), a thought experiment at\nthe heart of the literature around exploration in RL. Just as novelty-based exploration bonuses can\ntrap agents in noisy states, PER is susceptible to frequently replaying transitions involving high lev\u0002els of randomness (e.g. in reward or transition dynamics) even if they do not translate to meaningful\nlearning and thus are not useful for solving the task.\nTo combat this issue, we propose combining epistemic and aleatoric uncertainty estima\u0002tors (Clements et al., 2020; Alverio et al., 2022; Lahlou et al., 2022; Liu et al., 2023; Jiang et al.,\n2023), originally used to promote exploration, under an information gain criterion for use in replay\nprioritization. Epistemic uncertainty, the uncertainty reducible through learning, is the key quantity\nof interest. However this need to be appropriately \u2018calibrated\u2019, which we show-both empirically, and\nwith justification from Bayesian inference-can be done effectively by dividing the epistemic uncer\u0002tainty estimate by an aleatoric uncertainty estimate (and taking the logarithm, i.e. the information\ngain). Intuitively the need for this kind of calibration can be seen by considering the following game:\nthe aim is to estimate the mean of two distributions; the ground truth is that both distributions have\nidentical mean but different variance, and your current estimates for both distributions are the same\ni.e. your epistemic uncertainty on the mean is the same for both distributions. However if I offer\nyou a new sample from either distribution to refine your estimate you would choose to sample the\ndistribution with lower variance since this is more likely to be informative. In addition to arguing\nfor this novel prioritization variable, we also provide candidate methods involving distributions of\nensembles (in the vein of Clements et al. (2020)) to estimate these quantities. A comprehensive\nreview of related literature is provided in App. B, with further details in SM 1.\nOur primary contributions are as follows: (1) In Sec. 3, we present a novel approach for estimating\nepistemic uncertainty, building upon an existing uncertainty formalization introduced by Clements\net al. (2020) & Jiang et al. (2023). This extension incorporates information about the target value that\nthe model aims to estimate thereby accounting for bias in the estimator; (2) We derive a prioritiza\u0002tion variable using estimated uncertainty quantities, finding a specific functional form derived from\na concept called information gain, showing that both, epistemic and aleatoric uncertainty should be\nconsidered for prioritization; (3) In Sec. 4, we illustrate t",
          "original_query": "Prioritized Experience Replay",
          "cleaned_query": "Prioritized Experience Replay",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Data Shapley: Equitable Valuation of Data for Machine Learning",
          "url": "https://arxiv.org/abs/1904.02868",
          "content": "# Statistics > Machine Learning\n\n**arXiv:1904.02868** (stat)\n\n\\[Submitted on 5 Apr 2019 ( [v1](https://arxiv.org/abs/1904.02868v1)), last revised 10 Jun 2019 (this version, v2)\\]\n\n# Title:Data Shapley: Equitable Valuation of Data for Machine Learning\n\nAuthors: [Amirata Ghorbani](https://arxiv.org/search/stat?searchtype=author&query=Ghorbani,+A), [James Zou](https://arxiv.org/search/stat?searchtype=author&query=Zou,+J)\n\nView a PDF of the paper titled Data Shapley: Equitable Valuation of Data for Machine Learning, by Amirata Ghorbani and James Zou\n\n[View PDF](https://arxiv.org/pdf/1904.02868)\n\n> Abstract:As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1904.02868](https://arxiv.org/abs/1904.02868) \\[stat.ML\\] |\n| (or [arXiv:1904.02868v2](https://arxiv.org/abs/1904.02868v2) \\[stat.ML\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1904.02868](https://doi.org/10.48550/arXiv.1904.02868) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Amirata Ghorbani \\[ [view email](https://arxiv.org/show-email/b338fc99/1904.02868)\\] **[\\[v1\\]](https://arxiv.org/abs/1904.02868v1)**\nFri, 5 Apr 2019 04:54:10 UTC (2,772 KB)\n**\\[v2\\]**\nMon, 10 Jun 2019 08:10:40 UTC (3,761 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Data Shapley: Equitable Valuation of Data for Machine Learning, by Amirata Ghorbani and James Zou\n\n- [View PDF](https://arxiv.org/pdf/1904.02868)\n- [TeX Source](https://arxiv.org/src/1904.02868)\n- [Other Formats](https://arxiv.org/format/1904.02868)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1904.02868&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1904.02868&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2019-04](https://arxiv.org/list/stat.ML/2019-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1904.02868?context=cs) [cs.AI](https://arxiv.org/abs/1904.02868?context=cs.AI) [cs.LG](https://arxiv.org/abs/1904.02868?context=cs.LG) [stat](https://arxiv.org/abs/1904.02868?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1904.02868)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1904.02868)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1904.02868)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1904.02868) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Data Shapley: Equitable Valuation of Data for Machine Learning",
          "cleaned_query": "Data Shapley: Equitable Valuation of Data for Machine Learning",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Influence-Regularized PPO for Data-Efficient Policy Learning\nAugment PPO\u2019s clipped surrogate objective with a regularizer that downweights (or constrains) updates driven by trajectories with high harmful influence on a held-out validation return, estimated via TracIn-style checkpoint tracing. This yields a practical algorithm that explicitly reduces overfitting to \u201cspurious\u201d rollouts and can be evaluated on Atari and locomotion with controlled noise/partial observability.",
        "Trajectory Shapley: Equitable Valuation of Episodes in Reinforcement Learning\nDefine an RL analogue of Data Shapley where \u201cplayers\u201d are trajectories (or sub-trajectories) and the utility is final policy performance (average return) after training (e.g., PPO). Develop a scalable approximation using checkpointed training plus TracIn-like influence estimates to compute per-episode value, enabling principled dataset curation for offline RL and for filtering poor-quality on-policy rollouts.",
        "Uncertainty-Prioritized Rollout Collection for On-Policy PPO\nAdapt UPER\u2019s information-gain criterion to on-policy methods by prioritizing *which environments/seeds/state initializations* to sample next, rather than which replay transitions to revisit. Use ensemble-based epistemic uncertainty over advantage/value predictions to schedule rollouts, and test whether the same interaction budget achieves higher returns than uniform sampling across parallel actors.",
        "Counterfactual Data Debugging in RL via Influence Functions on Reward and Termination Signals\nUse influence functions (Hessian-vector products over supervised components like value loss or reward model loss) to attribute specific reward/termination events to downstream policy failures. Concretely, identify mislabeled terminal flags, reward glitches, or simulator bugs by ranking transitions whose removal most improves validation return, then iteratively \u201cclean\u201d the RL dataset and quantify performance recovery.",
        "Noise-Aware Prioritized Experience Replay with Target-Epistemic Filtering for Off-Policy Distillation\nCombine UPER\u2019s target-epistemic uncertainty with TracIn-style influence to build a replay sampler that suppresses transitions that are (a) high TD-error but mostly aleatoric/noise-driven and (b) empirically non-helpful for improving a student policy/value network. Evaluate in Atari-style off-policy settings and in \u201cnoisy TV\u201d synthetic benchmarks where conventional PER collapses.",
        "Influence-Guided Robustness Audits for RL Training Set Attacks\nExtend the \u201ctraining-set attack\u201d perspective from influence functions to RL by crafting small sets of poisoned trajectories that maximally shift a policy\u2019s behavior (e.g., induce unsafe actions) under PPO training. Then develop defenses that detect such trajectories using a joint metric: high influence on specific undesirable behaviors + high epistemic uncertainty, with ablations on how many poisoned episodes evade detection.",
        "Layer-Selective TracIn for Fast Attribution in Large Actor-Critic Models\nGeneralize TracIn\u2019s \u201ccherry-picking layers\u201d idea to actor-critic networks by tracing influence through only the policy head, value head, or shared trunk, and measuring how attribution quality trades off with compute. Deliver an actionable toolkit: per-test-state explanations of which training states/episodes most shaped the chosen action, suitable for debugging large-scale PPO agents.",
        "KL-Constraint Scheduling in PPO Using Epistemic Uncertainty\nReplace fixed KL targets (or fixed clip ranges) with an uncertainty-adaptive schedule: when epistemic uncertainty over advantages/values is high, tighten the trust region (smaller clip/stronger KL penalty), and relax it when uncertainty is low. This directly links UPER-style epistemic estimates to PPO stability, and can be benchmarked for reducing catastrophic updates in sparse-reward tasks.",
        "Unified Influence-Information Gain Metric for Data Acquisition in Supervised + RL Pipelines\nCreate a single acquisition score that combines (i) estimated marginal utility from Data Shapley (supervised components) or trajectory Shapley (RL components) and (ii) UPER-style expected information gain from epistemic uncertainty. Apply to pipelines like \u201clearn perception supervised, then control with PPO,\u201d selecting new labeled images or new robot rollouts to collect under a fixed budget and measuring end-to-end return improvements.",
        "Test-Point\u2013Conditioned Replay: Personalizing Experience Selection to a Deployment Distribution\nGiven a target deployment set of states (or tasks), use influence estimates to prioritize training experiences that most improve performance on that specific test distribution\u2014analogous to influence functions tracing a particular prediction to training points. Implement as a replay buffer sampler (offline or off-policy) that conditions priorities on a small batch of \u201cdeployment states,\u201d and evaluate domain-shift robustness (e.g., altered textures/dynamics) versus standard PER/UPER."
      ],
      "generated_ideas_raw": "1. **Influence-Regularized PPO for Data-Efficient Policy Learning**  \nAugment PPO\u2019s clipped surrogate objective with a regularizer that downweights (or constrains) updates driven by trajectories with high harmful influence on a held-out validation return, estimated via TracIn-style checkpoint tracing. This yields a practical algorithm that explicitly reduces overfitting to \u201cspurious\u201d rollouts and can be evaluated on Atari and locomotion with controlled noise/partial observability.\n\n2. **Trajectory Shapley: Equitable Valuation of Episodes in Reinforcement Learning**  \nDefine an RL analogue of Data Shapley where \u201cplayers\u201d are trajectories (or sub-trajectories) and the utility is final policy performance (average return) after training (e.g., PPO). Develop a scalable approximation using checkpointed training plus TracIn-like influence estimates to compute per-episode value, enabling principled dataset curation for offline RL and for filtering poor-quality on-policy rollouts.\n\n3. **Uncertainty-Prioritized Rollout Collection for On-Policy PPO**  \nAdapt UPER\u2019s information-gain criterion to on-policy methods by prioritizing *which environments/seeds/state initializations* to sample next, rather than which replay transitions to revisit. Use ensemble-based epistemic uncertainty over advantage/value predictions to schedule rollouts, and test whether the same interaction budget achieves higher returns than uniform sampling across parallel actors.\n\n4. **Counterfactual Data Debugging in RL via Influence Functions on Reward and Termination Signals**  \nUse influence functions (Hessian-vector products over supervised components like value loss or reward model loss) to attribute specific reward/termination events to downstream policy failures. Concretely, identify mislabeled terminal flags, reward glitches, or simulator bugs by ranking transitions whose removal most improves validation return, then iteratively \u201cclean\u201d the RL dataset and quantify performance recovery.\n\n5. **Noise-Aware Prioritized Experience Replay with Target-Epistemic Filtering for Off-Policy Distillation**  \nCombine UPER\u2019s target-epistemic uncertainty with TracIn-style influence to build a replay sampler that suppresses transitions that are (a) high TD-error but mostly aleatoric/noise-driven and (b) empirically non-helpful for improving a student policy/value network. Evaluate in Atari-style off-policy settings and in \u201cnoisy TV\u201d synthetic benchmarks where conventional PER collapses.\n\n6. **Influence-Guided Robustness Audits for RL Training Set Attacks**  \nExtend the \u201ctraining-set attack\u201d perspective from influence functions to RL by crafting small sets of poisoned trajectories that maximally shift a policy\u2019s behavior (e.g., induce unsafe actions) under PPO training. Then develop defenses that detect such trajectories using a joint metric: high influence on specific undesirable behaviors + high epistemic uncertainty, with ablations on how many poisoned episodes evade detection.\n\n7. **Layer-Selective TracIn for Fast Attribution in Large Actor-Critic Models**  \nGeneralize TracIn\u2019s \u201ccherry-picking layers\u201d idea to actor-critic networks by tracing influence through only the policy head, value head, or shared trunk, and measuring how attribution quality trades off with compute. Deliver an actionable toolkit: per-test-state explanations of which training states/episodes most shaped the chosen action, suitable for debugging large-scale PPO agents.\n\n8. **KL-Constraint Scheduling in PPO Using Epistemic Uncertainty**  \nReplace fixed KL targets (or fixed clip ranges) with an uncertainty-adaptive schedule: when epistemic uncertainty over advantages/values is high, tighten the trust region (smaller clip/stronger KL penalty), and relax it when uncertainty is low. This directly links UPER-style epistemic estimates to PPO stability, and can be benchmarked for reducing catastrophic updates in sparse-reward tasks.\n\n9. **Unified Influence-Information Gain Metric for Data Acquisition in Supervised + RL Pipelines**  \nCreate a single acquisition score that combines (i) estimated marginal utility from Data Shapley (supervised components) or trajectory Shapley (RL components) and (ii) UPER-style expected information gain from epistemic uncertainty. Apply to pipelines like \u201clearn perception supervised, then control with PPO,\u201d selecting new labeled images or new robot rollouts to collect under a fixed budget and measuring end-to-end return improvements.\n\n10. **Test-Point\u2013Conditioned Replay: Personalizing Experience Selection to a Deployment Distribution**  \nGiven a target deployment set of states (or tasks), use influence estimates to prioritize training experiences that most improve performance on that specific test distribution\u2014analogous to influence functions tracing a particular prediction to training points. Implement as a replay buffer sampler (offline or off-policy) that conditions priorities on a small batch of \u201cdeployment states,\u201d and evaluate domain-shift robustness (e.g., altered textures/dynamics) versus standard PER/UPER.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Influence-Regularized PPO for Data-Efficient Policy Learning\nAugment PPO\u2019s clipped surrogate objective with a regularizer that downweights (or constrains) updates driven by trajectories with high harm",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Trajectory Shapley: Equitable Valuation of Episodes in Reinforcement Learning\nDefine an RL analogue of Data Shapley where \u201cplayers\u201d are trajectories (or sub-trajectories) and the utility is final poli",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Uncertainty-Prioritized Rollout Collection for On-Policy PPO\nAdapt UPER\u2019s information-gain criterion to on-policy methods by prioritizing *which environments/seeds/state initializations* to sample nex",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Counterfactual Data Debugging in RL via Influence Functions on Reward and Termination Signals\nUse influence functions (Hessian-vector products over supervised components like value loss or reward mode",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Noise-Aware Prioritized Experience Replay with Target-Epistemic Filtering for Off-Policy Distillation\nCombine UPER\u2019s target-epistemic uncertainty with TracIn-style influence to build a replay sampler ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Influence-Guided Robustness Audits for RL Training Set Attacks\nExtend the \u201ctraining-set attack\u201d perspective from influence functions to RL by crafting small sets of poisoned trajectories that maximall",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Layer-Selective TracIn for Fast Attribution in Large Actor-Critic Models\nGeneralize TracIn\u2019s \u201ccherry-picking layers\u201d idea to actor-critic networks by tracing influence through only the policy head, va",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "KL-Constraint Scheduling in PPO Using Epistemic Uncertainty\nReplace fixed KL targets (or fixed clip ranges) with an uncertainty-adaptive schedule: when epistemic uncertainty over advantages/values is ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Unified Influence-Information Gain Metric for Data Acquisition in Supervised + RL Pipelines\nCreate a single acquisition score that combines (i) estimated marginal utility from Data Shapley (supervised",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Test-Point\u2013Conditioned Replay: Personalizing Experience Selection to a Deployment Distribution\nGiven a target deployment set of states (or tasks), use influence estimates to prioritize training experi",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 43,
      "paper_title": "High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model",
      "contribution": "Shows analytically and empirically that low-dimensional recurrent latent dynamics can produce high-dimensional observed neural activity (after neuronal nonlinearities), and introduces a provably interpretable latent\u2011variable method (NCE) to recover the latent dimensionality from recordings.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 4,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9286,
      "output_tokens": 1064,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] High-dimensional geometry of population responses in visual cortex",
          "url": "https://discovery.ucl.ac.uk/10076947/1/HighDim%20preprint%20190421.pdf",
          "content": "High-dimensional geometry of population responses in visual cortex\nCarsen Stringer*1,2, Marius Pachitariu*1,3, Nicholas Steinmetz3,\u2020, Matteo Carandini\u20214and Kenneth D. Harris\u20213\n1 HHMI Janelia Research Campus, Ashburn, Virginia, 20147, USA. 2 Gatsby Computational Neuroscience Unit, UCL, London W1T 4JG, UK.\n3 UCL Institute of Neurology, London WC1E 6DE, UK. 4 UCL Institute of Ophthalmology, London EC1V 9EL, UK.\n\u2020 Present address: Department of Biological Structure, University of Washington, Seattle, WA 98195, USA.\n* equal first authors, \u2021 equal senior authors.\nA neuronal population encodes information most efficiently when its stimulus responses are high-dimensional and uncor\u0002related, and most robustly when they are correlated and lower-dimensional. Here, we analyzed the dimensionality of the\nencoding of natural images by large visual cortical populations recorded from awake mice. Evoked population activity\nwas high dimensional, with correlations obeying an unexpected power-law: the n\nth principal component variance scaled as\n1/n. This scaling was not inherited from the 1/f spectrum of natural images, because it persisted after stimulus whitening.\nWe proved mathematically that if the variance spectrum decayed any slower, the population code could not be smooth,\nallowing small changes in input to dominate population activity. The theory also predicts larger power-law exponents for\nlower-dimensional stimulus ensembles, which we validated experimentally. These results suggest that coding smoothness\nmay represent a fundamental constraint governing correlations in neural population codes.\nIntroduction\nThe visual cortex contains millions of neurons, and the\npatterns of activity that images evoke in these neurons\nform a \"population code\". The structure of this code is\nlargely unknown, due to the lack of techniques able to\nrecord from large populations. Nonetheless, the popula\u0002tion code is the subject of long-standing theories.\nOne such theory is the \"efficient coding hypothesis\"\n[1, 2, 3], which maintains that the neural code maximizes\ninformation transmission by eliminating correlations in\nnatural image inputs. Such codes are high-dimensional\nand sparse, which can allow complex features to be read\nout by simple downstream networks [4, 5, 6].\nHowever, several studies have suggested that neural codes\nare confined to low-dimensional subspaces (\"planes\")\n[7, 8, 9, 10, 11, 12, 13, 14, 15]. Codes of low planar di\u0002mension are correlated and redundant, allowing for robust\ncomputations of stimuli in the face of noise [16, 17]. Nev\u0002ertheless, low planar dimension is inevitable given stimuli\nor tasks of limited complexity [18]: the responses to a set\nof n stimuli, for example, have to lie in an n dimensional\nsubspace. The planar dimension of the cortical code thus\nremains an open question, which can only be answered\nby recording the responses of large numbers of neurons to\nlarge numbers of stimuli.\nHere, we recorded the simultaneous activity of \u223c10,000\nneurons in mouse visual cortex, in response to thousands\nof natural images. We found that stimulus responses\nwere neither uncorrelated (\"efficient coding\") nor low\u0002dimensional. Instead, responses occupied a multidimen\u0002sional space with the variance in the n\nth dimension scal\u0002ing as a power law n\n\u2212\u03b1, where \u03b1 \u2248 1. We showed math\u0002ematically that if variances decay slower than a power law\nwith exponent \u03b1 = 1 + 2/d, where d is the dimension of\nthe input ensemble, then the space of neural activity must\nbe non-differentiable \u2013 i.e. not smooth. We varied the\ndimensionality of the stimuli d and found that the neural\nresponses respected this lower bound. These findings sug\u0002gest that the population responses are constrained by effi\u0002ciency, to make best use of limited numbers of neurons,\nand smoothness, which allows similar images to evoke\nsimilar responses.\nSimultaneous recordings of \u223c10,000 neurons\nTo obtain simultaneous recordings of \u223c10,000 cells from\nmouse V1, we employed resonance-scanning two-photon\ncalcium microscopy, using 11 imaging planes spaced at\n35\u00b5m (Fig. 1a). The slow timecourse of the GCaMP6s\nsensor allowed activity to be detected at a 2.5 Hz scan\nrate, and an efficient data processing pipeline [19] allowed\nlarge numbers of cells to be detected accurately (Fig. 1b).\nNatural image scenes (Imagenet database [20]) were pre\u0002sented on an array of 3 monitors surrounding the mouse\n(Fig. 1c), at an average of 1 image/s. Cells were tuned to\nthese natural image stimuli: in experiments in which\n1\nplane 5/11\n-90 0 90 \nhorizontal angle\n30 \n0 \n-30\nvertical angle neurons 65 / 12578\nstimuli\nfirst data half\nstimuli\nsecond data half\n-5\n0\n5\nrepeat 1\nrepeat 2\n... \n... \nx2800\nx2800\n0 0.2 0.4 0.6\ntuning SNR\n0\n200\n400\n600\nnumber of cells\nN = 11449 neurons N = 14062 N = 9410 N = 8122 N = 8704 N = 10145 N = 10103\n101 102 103 104\nnumber of neurons\n10-4\n10-3\n10-2\n10-1\n100\nfraction correct\nchance level\nlinear RF model\n-5\n0\n5 Gabor model\n-90 0 90\nhorizontal angle\n-30\n30\nvertical angle\na b\nc Example stimulus\nd Example data (trial-averaged)\ne Stimulus sequence\nf Neural stimulus tuning\ng Decoding 2800 stimuli\nh Best single-neuron RFs\ni RF locations\nFigure 1: Population coding of visual stimuli. a,\nSimultaneous recording of \u223c10,000 neurons using 11-\nplane two-photon calcium imaging. b, Randomly\u0002pseudocolored cells in an example imaging plane. c,\nExample stimulus spans three screens surrounding the\nmouse\u2019s head. d, Mean responses of 65 randomly-chosen\nneurons to 32 image stimuli (96 repeats, z-scored, scale\nbar represents standard deviations, one recording out of\nfour shown). e, A sequence of 2800 stimuli was repeated\ntwice during the recording. f, Distribution of single-cell\nsignal-to-noise ratios (SNR) (2800 stimuli, two repeats).\nColors denote recordings; arrows represent means. g,\nStimulus decoding accuracy as a function of neuron count\nfor each recording. h, Example receptive fields (RFs) fit\nusing reduced-rank regression or Gabor models (z-scored)\n(one recording shown, out of 7). i, Distribution of the re\u0002ceptive field centers, plotted on the left and center screens\n(line denotes screen boundary). Each cross represents\na different recording, with 95% of neuron\u2019s RF centers\nwithin error bars.\nresponses to 32 images were averaged over 96 repeats\n(Fig. 1d), stimulus responses accounted for 55.4\u00b13.3%\n(SE, n=4 recordings) of the trial-averaged variance. Con\u0002sistent with prior reports [21, 22, 23], neuronal responses\nwere sparse: only a small fraction of cells (13.4\u00b11.0%\nSE, n=4 recordings) were driven more than two standard\ndeviations above their baseline firing rate by any particu\u0002lar stimulus.\nFor our main experiments, we assembled a sequence of\n2,800 image stimuli, and presented these stimuli twice\nin the same order, to maximize the number of images\npresented while still allowing analyses based on cross\u0002validation (Fig. 1e). Most neurons (81.4\u00b15.1% SE, n=7\nrecordings) showed correlation between repeats at p <\n0.05 (Extended Data Fig. 1a,b). Nevertheless, consis\u0002tent with previous reports [24], responses showed substan\u0002tial trial-to-trial variability. Cross-validation showed that\nstimulus responses accounted for on average 13.2\u00b11.5%\nof the single-trial variance (Extended Data Fig. 1c), and\nthe average signal-to-noise ratio was 17.3\u00b12.4% (Fig.\n1f). This level of trial-to-trial variability was not due to\nour particular recording method: measuring responses to\nthe same stimuli electrophysiologically yielded a similar\nsignal-to-noise ratio (Extended Data Fig. 2). Despite trial\u0002to-trial variability, however, population activity recorded\non a single trial contained substantial information about\nthe sensory stimuli. A simple nearest-neighbor decoder,\ntrained on one repeat and tested on the other, was able to\nidentify the presented stimulus with up to 75.5% accuracy\n(Fig. 1g; range 25.4%-75.5%; median 41.7% compared to\nchance level of 0.036%, n=7 recordings). Decoding accu\u0002racy did not fully saturate at a population size of 10,000,\nsuggesting that performance would increase further with\nmore neurons.\nNeurons had similar visual properties to previous reports\n[23, 25], and their responses were only partially captured\nby classical linear-nonlinear models, consistent with pre\u0002vious visual cortex studies [26, 27, 28, 29, 30]. We calcu\u0002lated a receptive field (RF) for each cell from its responses\nto natural images in two ways: by fitting linear RFs regu\u0002larized with a reduced rank method; or by searching for an\noptimal Gabor filter that was rectified/quadrature filtered\nto simulate classical simple/complex cell responses. As\nexpected from retinotopy, the RF locations of simultane\u0002ously recorded neurons overlapped but there was a high\ndiversity of receptive field sizes and shapes (Fig. 1h; Ex\u0002tended Data Fig. 3). Both RF models, however, explained\nless than 20% of the stimulus-related variance (the linear\nmodel explained 11.4\u00b10.7% SE, and the Gabor model ex\u0002plained 18.5\u00b11.0% SE, n=7 recordings each).\n2\nPC\ntrain\nstimuli\nPC\ntest\na\n100 101 102 103\nPC dimension\n0\n0.2\n0.4\n0.6\n0.8\n1\nvariance\n(cumulative)\n2800\nimages\nb 32 images\n100 101 102 103\nPC dimension\n10-5\n10-4\n10-3\n10-2\n10-1\nvariance\n=1.04\nc\n100 101 102 103\nPC dimension\n10-5\n10-4\n10-3\n10-2\n10-1\nvariance\n(all recordings)\nd\n0.9 1 1.1\n0\n2\n4\n# of recordings\npower law exponent\ne\n100 101 102 103\nPC dimension\n0\n0.2\n0.4\n0.6\n0.8\n1\nvariance\n(cumulative)\nclassical RF model\nneural\ndata\nf\n100 101 102 103\ndimension\n10-5\n10-4\n10-3\n10-2\n10-1\nvariance\n1.000\n0.500\n0.250\n0.125\n0.062\n0.031\n0.016\nfraction of\nall neurons: g\n100 101 102 103\ndimension\n10-5\n10-4\n10-3\n10-2\n10-1\nvariance\n1.000\n0.500\n0.250\n0.125\n0.062\n0.031\n0.016\nfraction of\nall stimuli: h\n10-1 100\nfraction of\nneurons/stimuli\n0.4\n0.6\n0.8\n1\ncorrelation\ncoefficient\ni\n10-1 100\nfraction of\nneurons/stimuli\n0.5\n1\n1.5\npower law\nexponent\nj\nFigure 2: Visual cortical responses are high\u0002dimensional with power-law eigenspectrum. a, The\neigenspectrum of visual stimulus responses was esti\u0002mated by cross-validated principal component analysis\n(cv",
          "original_query": "High-dimensional geometry of population responses in visual cortex",
          "cleaned_query": "High-dimensional geometry of population responses in visual cortex",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Neural Manifolds for the Control of Movement - ScienceDirect.com",
          "url": "https://www.sciencedirect.com/science/article/pii/S0896627317304634",
          "content": "Perspective Neural Manifolds for the Control of Movement The analysis of neural dynamics in several brain cortices has consistently uncovered low-dimensional manifolds that capture a significant fraction of neural variability. These neural manifolds are spanned by specific patterns of correlated neural activity, the \u201cneural modes.\u201d We discuss a model for neural control of movement in which the time-dependent activation of these neural modes is the generator of motor behavior. This manifold-based view of motor cortex may lead to a better understanding of how the brain controls movement. Previous Next Main Text Since the work of Herbert Jasper ( Jasper et\u00a0al., 1958) and Ed Evarts ( Evarts, 1968), cortical function has been studied by recording single-neuron activity while animals perform a variety of behaviors, including decision making ( Newsome et\u00a0al., 1989), sensation ( Wurtz, 1969), and movement ( Georgopoulos et\u00a0al., 1982, Humphrey et\u00a0al., 1970). In the motor system, the main focus of this article, single neuron studies typically involved recordings during repeated, stereotypical movements. Many of these experiments sought explicit representations that relate single-neuron activity to specific movement covariates, including but not limited to target position, endpoint and joint kinematics, endpoint forces, and muscle activity ( Evarts, 1968, Georgopoulos et\u00a0al., 1982, Humphrey et\u00a0al., 1970, Morrow et\u00a0al., 2007, Thach, 1978). Although some of these efforts involved the decoding of population activity ( Georgopoulos et\u00a0al., 1982), they were restricted to models of non-interacting neurons whose individual activity was associated with specific movement covariates. However, some of these studies also identified single neurons whose activity did not represent movement parameters ( Churchland and Shenoy, 2007, Fetz, 1992, Scott, 2008). If neurons in primary motor cortex (M1) were to represent movement parameters, those representations ought to be most evident in corticomotoneuronal (CM) cells, which make direct connections onto\u00a0spinal motoneurons ( Fetz, 1992). Yet, many of these CM cells do not represent any specific movement covariate ( Fetz et\u00a0al., 1989). The ultimate role of M1 is to generate movement, not to represent it ( Churchland et\u00a0al., 2012, Cisek, 2006, Scott, 2004); thus, it is not surprising that many M1 neurons do not relate to any single movement covariate. The search for representations at the single-neuron level might actually divert us from understanding the neural control of movement. Early neural network simulations indicated that individual neurons need not explicitly encode movement covariates when the goal of M1 population activity is to generate realistic muscle activation patterns ( Fetz, 1992). The role of neurons that do not explicitly represent any movement covariate can be explained by recent work based on optimal feedback control theory, which postulates that the goal of motor cortex is to produce a desired movement and force, taking into account the state of the muscles. This hypothesis avoids the need for explicit representation of movement covariates by single neurons, though some neurons may still represent\u00a0movement covariates or high-level task characteristics as a byproduct of the necessary control signals ( Scott, 2008, Todorov, 2000). Recent and accelerating technical developments provide the experimental tools for monitoring the activity of large numbers of neurons, as well as the statistical and modeling tools for analyzing how these neural populations perform the computations necessary to plan and execute movement ( Gao and Ganguli, 2015). The challenge of understanding the neural control of movement by analyzing neural population activity is formidable, as population activity in any specific area not only reflects its intrinsic dynamics, but must also respond to its inputs and generate output projections based on the computations being performed ( Sussillo et\u00a0al., 2015). A simplification arises from the fact that neural computations are based on the joint activity of interconnected neurons ( Fetz, 1992, Hatsopoulos et\u00a0al., 1998, Shenoy et\u00a0al., 2013); the resulting population activity is thus likely constrained by the connectivity of the underlying network. Here we argue that the underlying network connectivity constrains these possible patterns of population activity ( Okun et\u00a0al., 2015, Sadtler et\u00a0al., 2014, Tsodyks et\u00a0al., 1999) and that the possible patterns are confined to a low-dimensional manifold ( Stopfer et\u00a0al., 2003, Yu et\u00a0al., 2009) spanned by a few independent patterns that we call \u201cneural modes.\u201d These neural modes capture a significant fraction of population covariance. It is the activation of these neural modes, rather than the activity of single neurons, that provides the basic building blocks of neural dynamics and function ( Luczak et\u00a0al., 2015, Sadtler et\u00a0al., 2014, Shenoy et\u00a0al., 2013). We thus propose a generative model of the activity of individual neurons based on the activation of neural modes, and explain how the parameters of the model can be identified using dimensionality reduction methods. We then review work showing that\u00a0these neural modes span task-specific neural manifolds in premotor and motor cortices. We propose that neural manifolds spanned by a surprisingly small number of neural modes are likely to simplify the neural control of movement and speculate on the potential learning mechanisms underlying the emergence of this low-dimensional organization. From Single Neurons to Neural Manifolds Current multi-electrode arrays (MEAs) allow for the simultaneous recording of about a hundred neurons. This is much more than the small numbers recorded with single electrodes but still a tiny fraction of the total number of neurons involved in movement generation. Despite this limitation, brain-machine interfaces (BMIs) based on these MEAs are able to predict reasonably well many behavioral variables ( Carmena et\u00a0al., 2003, Ethier et\u00a0al., 2012, Serruya et\u00a0al., 2002). What is the underlying reason for this success? Intuitively, it is\u00a0the high degree of correlation and redundancy across individual neural activity. This intuition has been recently made precise in elegant arguments on the low dimensionality of the stereotypical motor behaviors used in most motor control studies ( Gao\u00a0and Ganguli, 2015). The relatively small number of independent signals needed to control behavior during the execution\u00a0of such tasks only requires a small number of independent neural signals. These neural signals are the \u201clatent variables\u201d ( Cunningham and Yu, 2014) that describe the dynamics of the \u201cneural modes.\u201d The participation of individual neurons in neural modes is illustrated in Figure\u00a01 A. Note that each neural mode includes a large fraction of the neurons in the population and that a given neuron can participate in several neural modes. In this view, the time-dependent activity of individual neurons is simply a reflection of the latent variables ( Figure\u00a01 B) ( Kaufman et\u00a0al., 2016, Kobak et\u00a0al., 2016, Macke et\u00a0al., 2011). Consider the \u201cneural space\u201d in\u00a0 Figure\u00a01 C; each axis represents the activity of one of the N \u00a0recorded neurons (here, N = 3). Assuming that network connectivity constrains the possible patterns of population activity ( Okun et\u00a0al., 2015, Sadtler et\u00a0al., 2014, Tsodyks et\u00a0al., 1999), the population dynamics will not explore the full high-dimensional neural space but will instead remain confined to a low-dimensional surface within the full space, the \u201cneural manifold.\u201d In the simplest linear case, the neural manifold is flat, as is the hyperplane in Figure\u00a01 C, spanned by the two neural modes, u1 and u2. Download : Download high-res image (319KB) Download : Download full-size image Figure\u00a01. The Neural Manifold Hypothesis (A) Latent variables as a generative model for population activity. The relative area of the blue/green regions in each neuron represents the relative magnitude of the contribution of each latent variable to the neuron\u2019s activity. (B) Spikes from three recorded neurons during task execution as a linear combination of two latent variables. (C) Trajectory of time-varying population activity in the neural space of the three recorded neurons (black). The trajectory is mostly confined to the neural manifold, a plane shown in gray and spanned by the neural modes u 1 and u 2. (D) A curved, nonlinear neural manifold, shown in blue. Linear methods would capture a flat, local approximation to a small task-specific region of the manifold. (E) Linear manifolds for two different tasks shown as gray and purple planes. Are these two planes local linear approximations to different regions within a large, continuous manifold (transparent surface with blue contour), or are they distinct task-specific manifolds that may or not share neural modes? This geometrical picture illustrates a possible generative model for the dynamics of individual neurons: the activity ni(t) of the i th neuron, 1\u2264i\u2264N, results from a linear combination of latent variables Lj(t) plus additive noise \u03b5i:Equation 1ni(t)=\u2211juijLj(t)+\u03b5i. Here, Lj(t) is the j th latent variable, the time-dependent activation of the j th neural mode. Each latent variable results from projecting the neural population activity onto the corresponding neural mode. The coefficient uij in the linear combination quantifies the contribution of the j th latent variable to the activity of the i th neuron. These \u201cparticipation weights\u201d relate to the internal connectivity of the network ( Okun et\u00a0al., 2015). The noise term \u03b5i represents intrinsic neural noise, and potentially other processes not accounted for in the model. By construction, neural population activity remains within the neural manifold except for small fluctuations (see how close the actual black trajectory is to the gray trajectory projected into the manifold in Figure\u00a01 C). Dimensionality reduction techniques allow us to study neural popula",
          "original_query": "Neural manifolds for the control of movement",
          "cleaned_query": "Neural manifolds for the control of movement",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Shaping dynamics with multiple populations in low-rank recurrent ...",
          "url": "https://arxiv.org/abs/2007.02062",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: An emerging paradigm proposes that neural computations can be understood at\nthe level of dynamical systems that govern low-dimensional trajectories of\ncollective neural activity. How the connectivity structure of a network\ndetermines the emergent dynamical system however remains to be clarified. Here\nwe consider a novel class of models, Gaussian-mixture low-rank recurrent\nnetworks, in which the rank of the connectivity matrix and the number of\nstatistically-defined populations are independent hyper-parameters. We show\nthat the resulting collective dynamics form a dynamical system, where the rank\nsets the dimensionality and the population structure shapes the dynamics. In\nparticular, the collective dynamics can be described in terms of a simplified\neffective circuit of interacting latent variables. While having a single,\nglobal population strongly restricts the possible dynamics, we demonstrate that\nif the number of populations is large enough, a rank-R network can approximate\nany R-dimensional dynamical system.\n \n \n \n \n Submission history From: Manuel Beiran [ view email]\n \n [v1] \n Sat, 4 Jul 2020 10:13:04 UTC (4,568 KB) [v2] \nTue, 17 Nov 2020 08:40:09 UTC (4,620 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > q-bio > arXiv:2007.02062\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Quantitative Biology > Neurons and Cognition\n\n arXiv:2007.02062 (q-bio)\n [Submitted on 4 Jul 2020 (v1), last revised 17 Nov 2020 (this version, v2)]\n\n Title: Shaping dynamics with multiple populations in low-rank recurrent networks\n\n Authors: Manuel Beiran, Alexis Dubreuil, Adrian Valente, Francesca Mastrogiuseppe, Srdjan Ostojic\n Download PDF\n Abstract: An emerging paradigm proposes that neural computations can be understood at the level of dynamical systems that govern low-dimensional trajectories of collective neural activity. How the connectivity structure of a network determines the emergent dynamical system however remains to be clarified. Here we consider a novel class of models, Gaussian-mixture low-rank recurrent networks, in which the rank of the connectivity matrix and the number of statistically-defined populations are independent hyper-parameters. We show that the resulting collective dynamics form a dynamical system, where the rank sets the dimensionality and the population structure shapes the dynamics. In particular, the collective dynamics can be described in terms of a simplified effective circuit of interacting latent variables. While having a single, global population strongly restricts the possible dynamics, we demonstrate that if the number of populations is large enough, a rank-R network can approximate any R-dimensional dynamical system.\n Comments: 29 pages, 7 figures \n Subjects: Neurons and Cognition (q-bio.NC) \n Cite as: arXiv:2007.02062 [q-bio.NC] \n (or arXiv:2007.02062v2 [q-bio.NC] for this version)\n https://doi.org/10.48550/arXiv.2007.02062 \n Focus to learn more \n arXiv-issued DOI via DataCite \n https://doi.org/10.1162/neco_a_01381 \n Related DOI: Focus to learn more \n DOI(s) linking to related resources \n \n\n Submission history\n\n From: Manuel Beiran [view email]\n [v1] Sat, 4 Jul 2020 10:13:04 UTC (4,568 KB)\n [v2] Tue, 17 Nov 2020 08:40:09 UTC (4,620 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n (license)\n Current browse context:\n q-bio.NC\n < prev | next >\n new | recent | 2007\n Change to browse by:\n q-bio\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Shaping dynamics with multiple populations in low-rank recurrent networks",
          "cleaned_query": "Shaping dynamics with multiple populations in low-rank recurrent networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Kernel Methods for Deep Learning",
          "url": "http://papers.neurips.cc/paper/3628-kernel-methods-for-deep-learning.pdf",
          "content": "Kernel Methods for Deep Learning\nYoungmin Cho and Lawrence K. Saul\nDepartment of Computer Science and Engineering\nUniversity of California, San Diego\n9500 Gilman Drive, Mail Code 0404\nLa Jolla, CA 92093-0404\n{yoc002,saul}@cs.ucsd.edu\nAbstract\nWe introduce a new family of positive-definite kernel functions that mimic the\ncomputation in large, multilayer neural nets. These kernel functions can be used\nin shallow architectures, such as support vector machines (SVMs), or in deep\nkernel-based architectures that we call multilayer kernel machines (MKMs). We\nevaluate SVMs and MKMs with these kernel functions on problems designed to\nillustrate the advantages of deep architectures. On several problems, we obtain\nbetter results than previous, leading benchmarks from both SVMs with Gaussian\nkernels as well as deep belief nets.\n1 Introduction\nRecent work in machine learning has highlighted the circumstances that appear to favor deep archi\u0002tectures, such as multilayer neural nets, over shallow architectures, such as support vector machines\n(SVMs) [1]. Deep architectures learn complex mappings by transforming their inputs through mul\u0002tiple layers of nonlinear processing [2]. Researchers have advanced several motivations for deep\narchitectures: the wide range of functions that can be parameterized by composing weakly non\u0002linear transformations, the appeal of hierarchical distributed representations, and the potential for\ncombining unsupervised and supervised methods. Experiments have also shown the benefits of\ndeep learning in several interesting applications [3, 4, 5].\nMany issues surround the ongoing debate over deep versus shallow architectures [1, 6]. Deep ar\u0002chitectures are generally more difficult to train than shallow ones. They involve difficult nonlinear\noptimizations and many heuristics. The challenges of deep learning explain the early and continued\nappeal of SVMs, which learn nonlinear classifiers via the \u201ckernel trick\u201d. Unlike deep architectures,\nSVMs are trained by solving a simple problem in quadratic programming. However, SVMs cannot\nseemingly benefit from the advantages of deep learning.\nLike many, we are intrigued by the successes of deep architectures yet drawn to the elegance of ker\u0002nel methods. In this paper, we explore the possibility of deep learning in kernel machines. Though\nwe share a similar motivation as previous authors [7], our approach is very different. Our paper\nmakes two main contributions. First, we develop a new family of kernel functions that mimic the\ncomputation in large neural nets. Second, using these kernel functions, we show how to train multi\u0002layer kernel machines (MKMs) that benefit from many advantages of deep learning.\nThe organization of this paper is as follows. In section 2, we describe a new family of kernel\nfunctions and experiment with their use in SVMs. Our results on SVMs are interesting in their own\nright; they also foreshadow certain trends that we observe (and certain choices that we make) for the\nMKMs introduced in section 3. In this section, we describe a kernel-based architecture with multiple\nlayers of nonlinear transformation. The different layers are trained using a simple combination of\nsupervised and unsupervised methods. Finally, we conclude in section 4 by evaluating the strengths\nand weaknesses of our approach.\n1\n2 Arc-cosine kernels\nIn this section, we develop a new family of kernel functions for computing the similarity of vector\ninputs x, y \u2208 0, the angular dependence is more complicated. The first few expressions are:\nJ0(\u03b8) = \u03c0 \u2212 \u03b8 (5)\nJ1(\u03b8) = sin \u03b8 + (\u03c0 \u2212 \u03b8) cos \u03b8 (6)\nJ2(\u03b8) = 3 sin \u03b8 cos \u03b8 + (\u03c0 \u2212 \u03b8)(1 + 2 cos2\u03b8) (7)\nWe describe eq. (3) as an arc-cosine kernel because for n = 0, it takes the simple form\nk0(x, y) = 1\u2212\n1\n\u03c0\ncos\u22121 x\u00b7y\nkxkkyk\n. In fact, the zeroth and first order kernels in this family are strongly\nmotivated by previous work in neural computation. We explore these connections in the next section.\nArc-cosine kernels have other intriguing properties. From the magnitude dependence in eq. (3),\nwe observe the following: (i) the n = 0 arc-cosine kernel maps inputs x to the unit hypersphere\nin feature space, with k0(x, x) = 1; (ii) the n = 1 arc-cosine kernel preserves the norm of inputs,\nwith k1(x, x) = kxk\n2\n; (iii) higher order (n>1) arc-cosine kernels expand the dynamic range of the\ninputs, with kn(x, x) \u223c kxk\n2n. Properties (i)\u2013(iii) are shared respectively by radial basis function\n(RBF), linear, and polynomial kernels. Interestingly, though, the n = 1 arc-cosine kernel is highly\nnonlinear, also satisfying k1(x, \u2212x) = 0 for all inputs x. As a practical matter, we note that arc\u0002cosine kernels do not have any continuous tuning parameters (such as the kernel width in RBF\nkernels), which can be laborious to set by cross-validation.\n2.2 Computation in single-layer threshold networks\nConsider the single-layer network shown in Fig. 1 (left) whose weights Wij connect the jth input\nunit to the ith output unit. The network maps inputs x to outputs f(x) by applying an elementwise\nnonlinearity to the matrix-vector product of the inputs and the weight matrix: f(x) = g(Wx). The\nnonlinearity is described by the network\u2019s so-called activation function. Here we consider the family\nof one-sided polynomial activation functions gn(z) = \u0398(z)z\nn illustrated in the right panel of Fig. 1.\n2\nf2 f3 fi\nx1 x2 xj\n. . . \n. . . \nf1 fm\nxd\nW\n. . . \n. . . \u22121 0 1\n0\n0.5\n1\nStep (n=0)\n\u22121 0 1\n0\n0.5\n1\nRamp (n=1)\n\u22121 0 1\n0\n0.5\n1\nQuarter\u2212pipe (n=2)\nFigure 1: Single layer network and activation functions\nFor n= 0, the activation function is a step function, and the network is an array of perceptrons. For\nn= 1, the activation function is a ramp function (or rectification nonlinearity [9]), and the mapping\nf(x) is piecewise linear. More generally, the nonlinear (non-polynomial) behavior of these networks\nis induced by thresholding on weighted sums. We refer to networks with these activation functions\nas single-layer threshold networks of degree n.\nComputation in these networks is closely connected to computation with the arc-cosine kernel func\u0002tion in eq. (1). To see the connection, consider how inner products are transformed by the mapping\nin single-layer threshold networks. As notation, let the vector wi denote ith row of the weight\nmatrix W. Then we can express the inner product between different outputs of the network as:\nf(x) \u00b7 f(y) = Xm\ni=1\n\u0398(wi\u00b7 x)\u0398(wi\u00b7 y)(wi\u00b7 x)\nn\n(wi\u00b7 y)\nn\n, (8)\nwhere m is the number of output units. The connection with the arc-cosine kernel function emerges\nin the limit of very large networks [10, 8]. Imagine that the network has an infinite number of\noutput units, and that the weights Wij are Gaussian distributed with zero mean and unit vari\u0002ance. In this limit, we see that eq. (8) reduces to eq. (1) up to a trivial multiplicative factor:\nlimm\u2192\u221e\n2\nm f(x) \u00b7 f(y) = kn(x, y). Thus the arc-cosine kernel function in eq. (1) can be viewed\nas the inner product between feature vectors derived from the mapping of an infinite single-layer\nthreshold network [8].\nMany researchers have noted the general connection between kernel machines and neural networks\nwith one layer of hidden units [1]. The n = 0 arc-cosine kernel in eq. (1) can also be derived from\nan earlier result obtained in the context of Gaussian processes [8]. However, we are unaware of any\nprevious theoretical or empirical work on the general family of these kernels for degrees n\u22650.\nArc-cosine kernels differ from polynomial and RBF kernels in one especially interesting respect.\nAs highlighted by the integral representation in eq. (1), arc-cosine kernels induce feature spaces\nthat mimic the sparse, nonnegative, distributed representations of single-layer threshold networks.\nPolynomial and RBF kernels do not encode their inputs in this way. In particular, the feature vector\ninduced by polynomial kernels is neither sparse nor nonnegative, while the feature vector induced\nby RBF kernels resembles the localized output of a soft vector quantizer. Further implications of\nthis difference are explored in the next section.\n2.3 Computation in multilayer threshold networks\nA kernel function can be viewed as inducing a nonlinear mapping from inputs x to fea\u0002ture vectors \u03a6(x). The kernel computes the inner product in the induced feature space:\nk(x, y) = \u03a6(x)\u00b7\u03a6(y). In this section, we consider how to compose the nonlinear mappings in\u0002duced by kernel functions. Specifically, we show how to derive new kernel functions\nk\n(`)\n(x, y) = \u03a6(\u03a6(...\u03a6\n| {z }\n` times\n(x))) \u00b7 \u03a6(\u03a6(...\u03a6\n| {z }\n` times\n(y))) (9)\nwhich compute the inner product after ` successive applications of the nonlinear mapping \u03a6(\u00b7). Our\nmotivation is the following: intuitively, if the base kernel function k(x, y) = \u03a6(x) \u00b7 \u03a6(y) mimics\nthe computation in a single-layer network, then the iterated mapping in eq. (9) should mimic the\ncomputation in a multilayer network.\n3\n22\n24\n26\nDBN\u22123\nSVM\u2212RBF\nTest error rate (%)\n1 2 3 4 5 6\n Step (n=0)\n1 2 3 4 5 6\n Ramp (n=1)\n1 2 3 4 5 6\nQuarter\u2212pipe (n=2)\ngfpggg\ntest set. SVMs with arc cosine kernels have error rates from 22.36\u201325.64%. Results are s\nkernels of varying degree (n) and levels of recursion (!). The best previous results are 24\nSVMs with RBF kernels and 22.50% for deep belief nets [2]. See text for details.\n17\n18\n19\n20\n21\nTest error rate (%)\n1 2 3 4 5 6\n Step (n=0)\n1 2 3 4 5 6\n Ramp (n=1)\n1 2 3 4 5 6\nQuarter!pipe (n=2)\nFigure 3: Left: examples from the convex data set. Right: classification error rates on th\nSVMs with arc cosine kernels have error rates from 17.15\u201320.51%. Results are shown fo\nof varying degree (n) and levels of recursion (!). The best previous results are 19.13% f\nwith RBF kernels and 18.63% for deep belief nets [2]. See text for details.\n2000 training examples as a validation set to choose the margin penalty parameter; after \nthis parameter by cross-validation, we then retrained each SVM using all the training exam\nreference, we also report the best results obtained previously from three layer deep belief n",
          "original_query": "Kernel methods for deep learning (random feature / arc-cosine kernel literature)",
          "cleaned_query": "Kernel methods for deep learning (random feature",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Learning identifiable and interpretable latent models of high ...",
          "url": "https://proceedings.neurips.cc/paper_files/paper/2020/hash/510f2318f324cf07fce24c3a4b89c771-Abstract.html",
          "content": "Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE\n#### Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE\nPart of[Advances in Neural Information Processing Systems 33 (NeurIPS 2020)](https://proceedings.neurips.cc/paper_files/paper/2020)\n[AuthorFeedback](https://proceedings.neurips.cc/paper_files/paper/2020/file/510f2318f324cf07fce24c3a4b89c771-AuthorFeedback.pdf)[Bibtex](https://proceedings.neurips.cc/paper_files/paper/10331-/bibtex)[MetaReview](https://proceedings.neurips.cc/paper_files/paper/2020/file/510f2318f324cf07fce24c3a4b89c771-MetaReview.html)[Paper](https://proceedings.neurips.cc/paper_files/paper/2020/file/510f2318f324cf07fce24c3a4b89c771-Paper.pdf)[Review](https://proceedings.neurips.cc/paper_files/paper/2020/file/510f2318f324cf07fce24c3a4b89c771-Review.html)[Supplemental](https://proceedings.neurips.cc/paper_files/paper/2020/file/510f2318f324cf07fce24c3a4b89c771-Supplemental.pdf)\n#### Authors\n*Ding Zhou, Xue-Xin Wei*\n#### Abstract\nThe ability to record activities from hundreds of neurons simultaneously in the brain has placed an increasing demand for developing appropriate statistical techniques to analyze such data. Recently, deep generative models have been proposed to fit neural population responses. While these methods are flexible and expressive, the downside is that they can be difficult to interpret and identify. To address this problem, we propose a method that integrates key ingredients from latent models and traditional neural encoding models. Our method, pi-VAE, is inspired by recent progress on identifiable variational auto-encoder, which we adapt to make appropriate for neuroscience applications. Specifically, we propose to construct latent variable models of neural activity while simultaneously modeling the relation between the latent and task variables (non-neural variables, e.g. sensory, motor, and other externally observable states). The incorporation of task variables results in models that are not only more constrained, but also show qualitative improvements in interpretability and identifiability. We validate pi-VAE using synthetic data, and apply it to analyze neurophysiological datasets from rat hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits the data better, but also provides unexpected novel insights into the structure of the neural codes.\n#### Name Change Policy\n&times;\nRequests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.\nUse the \"Report an Issue\" link to request a name change.\nDo not remove: This comment is monitored to verify that the site is working properly",
          "original_query": "Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-vae",
          "cleaned_query": "Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-vae",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Deep Equals Shallow for ReLU Networks in Kernel Regimes - arXiv",
          "url": "https://arxiv.org/abs/2009.14397",
          "content": "# Statistics > Machine Learning\n\n**arXiv:2009.14397** (stat)\n\n\\[Submitted on 30 Sep 2020 ( [v1](https://arxiv.org/abs/2009.14397v1)), last revised 26 Aug 2021 (this version, v4)\\]\n\n# Title:Deep Equals Shallow for ReLU Networks in Kernel Regimes\n\nAuthors: [Alberto Bietti](https://arxiv.org/search/stat?searchtype=author&query=Bietti,+A), [Francis Bach](https://arxiv.org/search/stat?searchtype=author&query=Bach,+F)\n\nView a PDF of the paper titled Deep Equals Shallow for ReLU Networks in Kernel Regimes, by Alberto Bietti and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2009.14397)\n\n> Abstract:Deep networks are often considered to be more expressive than shallow ones in terms of approximation. Indeed, certain functions can be approximated by deep networks provably more efficiently than by shallow ones, however, no tractable algorithms are known for learning such deep models. Separately, a recent line of work has shown that deep networks trained with gradient descent may behave like (tractable) kernel methods in a certain over-parameterized regime, where the kernel is determined by the architecture and initialization, and this paper focuses on approximation for such kernels. We show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their shallow two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. Our main theoretical result relies on characterizing such eigenvalue decays through differentiability properties of the kernel function, which also easily applies to the study of other kernels defined on the sphere.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2009.14397](https://arxiv.org/abs/2009.14397) \\[stat.ML\\] |\n| | (or [arXiv:2009.14397v4](https://arxiv.org/abs/2009.14397v4) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2009.14397](https://doi.org/10.48550/arXiv.2009.14397) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Alberto Bietti \\[ [view email](https://arxiv.org/show-email/9f59015f/2009.14397)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2009.14397v1)**\nWed, 30 Sep 2020 02:37:43 UTC (47 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2009.14397v2)**\nFri, 9 Oct 2020 16:54:31 UTC (48 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2009.14397v3)**\nWed, 17 Mar 2021 22:25:45 UTC (60 KB)\n\n**\\[v4\\]**\nThu, 26 Aug 2021 15:49:40 UTC (61 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Deep Equals Shallow for ReLU Networks in Kernel Regimes, by Alberto Bietti and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2009.14397)\n- [TeX Source](https://arxiv.org/src/2009.14397)\n- [Other Formats](https://arxiv.org/format/2009.14397)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2009.14397&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2009.14397&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2020-09](https://arxiv.org/list/stat.ML/2020-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2009.14397?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2009.14397?context=cs.LG)\n\n[stat](https://arxiv.org/abs/2009.14397?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2009.14397)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2009.14397)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2009.14397)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2009.14397&description=Deep Equals Shallow for ReLU Networks in Kernel Regimes) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2009.14397&title=Deep Equals Shallow for ReLU Networks in Kernel Regimes)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2009.14397) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Deep equals shallow for ReLU networks in kernel regimes",
          "cleaned_query": "Deep equals shallow for ReLU networks in kernel regimes",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Linking Kernel Eigenvalue Decay to Neural PCA Spectra via Stimulus Manifold Dimension\nDerive and test a quantitative relationship between (i) eigenvalue decay of kernels induced by ReLU/arc-cosine architectures (per \u201cDeep Equals Shallow\u201d results) and (ii) the neural population variance spectrum when stimuli lie on manifolds of dimension \\(d\\). Experimentally vary \\(d\\) with synthetic image manifolds and check whether predicted changes in kernel/operator spectra track changes in neural PCA exponents.",
        "Identifiable Mode Decompositions that Map Neural Modes to Effective Low-Rank Circuits\nCombine pi-VAE identifiability with Beiran et al.\u2019s \u201ceffective circuit of interacting latent variables\u201d by constraining the latent space to correspond to rank-\\(R\\) dynamical variables with population-structured couplings. Fit to motor cortex datasets and explicitly recover which statistically-defined populations implement each latent interaction term, yielding an interpretable bridge from recorded neurons \u2192 modes \u2192 effective dynamical system.",
        "Controllability-Optimized Low-Rank Motor Networks with Population Mixtures\nUse the Gaussian-mixture low-rank framework to fit motor behavior by optimizing not just trajectory reconstruction but also control-theoretic metrics (e.g., minimal energy to reach targets, robustness to noise) over the latent dynamics. Predict how increasing the number of populations \\(P\\) changes the set of learnable motor primitives (neural modes) and test these predictions against adaptation/learning experiments that constrain animals to new manifolds.",
        "Task-Conditional Power-Law Exponents: Context Gates Dimensionality\nBuild a task-variable-conditioned pi-VAE where context (attention, reward, locomotion state) modulates the latent covariance spectrum, and test whether the PCA exponent \\(\\alpha\\) shifts systematically with behavioral state even for the same stimulus ensemble. This would directly assess whether the power-law geometry is a fixed property of circuitry or a state-dependent coding strategy that trades off smoothness, robustness, and efficiency."
      ],
      "generated_ideas_raw": "1. **Smoothness-Constrained Latent Manifolds in Sensory Cortex**\n   Build a latent-variable model (e.g., pi-VAE) with an explicit regularizer that enforces the Stringer et al. smoothness bound by penalizing variance spectra that decay slower than \\(n^{-\\alpha}\\) (with \\(\\alpha \\ge 1+2/d\\)). Fit it to large-scale V1 responses under controlled stimulus ensemble dimension \\(d\\) (natural images vs. low-d manifolds) and test whether the learned latent geometry improves prediction of responses to small stimulus perturbations.\n\n2. **Population-Structured Low-Rank RNNs that Reproduce 1/n Variance Laws**\n   Extend Gaussian-mixture low-rank recurrent networks by adding an objective to match an empirical PCA spectrum (e.g., \\( \\lambda_n \\propto 1/n \\)) for evoked activity while preserving task performance. Systematically vary rank \\(R\\) and number of populations \\(P\\) to identify minimal circuit structure that yields both smooth stimulus-to-activity mappings and the observed power-law correlations.\n\n3. **Deep Arc-Cosine Kernel Readouts for Neural Population Decoding Under Whitening**\n   Use arc-cosine kernel machines (including multilayer kernel machines) as decoders of V1 population activity, explicitly comparing performance and robustness on raw vs. whitened stimuli. Measure how kernel order/layering changes the effective eigenvalue decay of the stimulus-response mapping and whether certain kernels better respect the smoothness constraint implied by the 1/n spectrum.\n\n4. **Linking Kernel Eigenvalue Decay to Neural PCA Spectra via Stimulus Manifold Dimension**\n   Derive and test a quantitative relationship between (i) eigenvalue decay of kernels induced by ReLU/arc-cosine architectures (per \u201cDeep Equals Shallow\u201d results) and (ii) the neural population variance spectrum when stimuli lie on manifolds of dimension \\(d\\). Experimentally vary \\(d\\) with synthetic image manifolds and check whether predicted changes in kernel/operator spectra track changes in neural PCA exponents.\n\n5. **Cross-Area Comparison: High-Dimensional Sensory Codes vs. Low-D Motor Manifolds**\n   Run a matched analysis pipeline on large-population recordings from V1 (sensory) and M1/PMd (motor) to quantify when population activity is power-law high-dimensional versus low-dimensional manifold-structured. Test the hypothesis that sensory areas optimize smoothness of input mapping (power-law constraints), while motor areas optimize controllability/effector constraints (manifold modes), using common metrics (spectral decay, curvature, local linearity).\n\n6. **Identifiable Mode Decompositions that Map Neural Modes to Effective Low-Rank Circuits**\n   Combine pi-VAE identifiability with Beiran et al.\u2019s \u201ceffective circuit of interacting latent variables\u201d by constraining the latent space to correspond to rank-\\(R\\) dynamical variables with population-structured couplings. Fit to motor cortex datasets and explicitly recover which statistically-defined populations implement each latent interaction term, yielding an interpretable bridge from recorded neurons \u2192 modes \u2192 effective dynamical system.\n\n7. **Perturbation-Response Tests of Smoothness: Closed-Loop Micro-Stimulus Jittering**\n   Design a stimulus perturbation experiment where images are jittered along controlled directions in pixel space vs. along low-d generative model directions, while recording large V1 populations. Quantify whether neural response differences scale smoothly with perturbation magnitude in a way predicted by the power-law bound, and identify which principal components violate smoothness first under adversarial (non-natural) perturbations.\n\n8. **Learning Rules that Produce Power-Law Correlations in Population Codes**\n   Propose and simulate biologically plausible plasticity (Hebbian + homeostatic + decorrelation terms) that provably drives variance spectra toward \\(1/n^\\alpha\\) while maintaining differentiable stimulus mappings. Validate by training recurrent or feedforward networks on naturalistic inputs (including whitened controls) and comparing emergent spectra and decoding robustness to the mouse V1 benchmarks.\n\n9. **Controllability-Optimized Low-Rank Motor Networks with Population Mixtures**\n   Use the Gaussian-mixture low-rank framework to fit motor behavior by optimizing not just trajectory reconstruction but also control-theoretic metrics (e.g., minimal energy to reach targets, robustness to noise) over the latent dynamics. Predict how increasing the number of populations \\(P\\) changes the set of learnable motor primitives (neural modes) and test these predictions against adaptation/learning experiments that constrain animals to new manifolds.\n\n10. **Task-Conditional Power-Law Exponents: Context Gates Dimensionality**\n   Build a task-variable-conditioned pi-VAE where context (attention, reward, locomotion state) modulates the latent covariance spectrum, and test whether the PCA exponent \\(\\alpha\\) shifts systematically with behavioral state even for the same stimulus ensemble. This would directly assess whether the power-law geometry is a fixed property of circuitry or a state-dependent coding strategy that trades off smoothness, robustness, and efficiency.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Linking Kernel Eigenvalue Decay to Neural PCA Spectra via Stimulus Manifold Dimension\nDerive and test a quantitative relationship between (i) eigenvalue decay of kernels induced by ReLU/arc-cosine arc",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Identifiable Mode Decompositions that Map Neural Modes to Effective Low-Rank Circuits\nCombine pi-VAE identifiability with Beiran et al.\u2019s \u201ceffective circuit of interacting latent variables\u201d by constra",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Controllability-Optimized Low-Rank Motor Networks with Population Mixtures\nUse the Gaussian-mixture low-rank framework to fit motor behavior by optimizing not just trajectory reconstruction but also c",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Task-Conditional Power-Law Exponents: Context Gates Dimensionality\nBuild a task-variable-conditioned pi-VAE where context (attention, reward, locomotion state) modulates the latent covariance spectrum",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 44,
      "paper_title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks",
      "contribution": "The paper presents a novel training approach for Spiking Neural Networks that utilizes adaptive surrogate gradients and a guiding policy to enhance performance in sequential reinforcement learning tasks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10513,
      "output_tokens": 1065,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Networks of Spiking Neurons: The Third Generation of Neural ...",
          "url": "https://igi-web.tugraz.at/PDF/85a.pdf",
          "content": "Pergamon \nPII: S0893-6080(97)00011-7 \nNeuralNetworks, Vol. 10, No. 9, pp. 1659-1671, 1997 \n\u00a9 1997 Elsevier Science Ltd. All rights reserved \nPrinted in Great Britain \n0893-6080/97 $17.00+.00 \nCONTRIBUTED ARTICLE \nNetworks of Spiking Neurons: The Third Generation of \nNeural Network Models \nWOLFGANG MAASS \nInstitute for Theoretical Computer Science, Technische Universit~it Graz \n(Received 27 March 1996; accepted 10 November 1996) \nAbstract--The computational power of formal models for networks of spiking neurons is compared with that of other \nneural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In \nparticular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, \ncomputationally more powerful than these other neural network models. A concrete biologically relevant function is \nexhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but \nwhich requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that \ncan be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This \narticle does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the \ncurrently available literature on computations in networks of spiking neurons and relevant results from neurobiology. \n\u00a9 1997 Elsevier Science Ltd. All rights reserved. \nKeywords--Spiking neuron, Integrate-and-fire neutron, Computational complexity, Sigmoidal neural nets, Lower \nbounds. \n1. DEFINITIONS AND MOTIVATIONS \nIf one classifies neural network models according to their \ncomputational units, one can distinguish three different \ngenerations. The first generation is based on \nMcCulloch-Pitts neurons as computational units. \nThese are also referred to as perceptrons or threshold \ngates. They give rise to a variety of neural network mod\u0002els such as multilayer perceptrons (also called threshold \ncircuits), Hopfield nets, and Boltzmann machines. A \ncharacteristic feature of these models is that they can \nonly give digital output. In fact they are universal for \ncomputations with digital input and output, and every \nboolean function can be computed by some multilayer \nperceptron with a single hidden layer. \nThe second generation is based on computational units \nthat apply an \"activation function\" with a continuous set \nof possible output values to a weighted sum (or poly\u0002nomial) of the inputs. Common activation functions are \nthe sigmoid function a(y) = 1/(1 + e -y) and the linear \nAcknowledgements: I would like to thank Eduardo Sontag and an \nanonymous referee for their helpful comments. Written under partial \nsupport by the Austrian Science Fund. \nRequests for reprints should be sent to W. Maass, Institute for The\u0002oretical Computer Science, Technische Universit~it Graz, Klosterwies\u0002gasse 32/2, A-8010, Graz, Austria; tel. +43 316 873-5822; fax: +43 316 \n873-5805; e-mail: maass@igi,tu-graz.ac.at \nsaturated function 7r with 7r(y) = y for 0 --< y --< 1, 7r(y) = \n0 for y < 0, lr(y) = 1 for y > 1. Besides piecewise \npolynomial activation functions we consider in this \npaper also \"piecewise exponential\" activation func\u0002tions, whose pieces can be defined by expressions invol\u0002ving exponentiation (such as the definition of a). Typical \nexamples for networks from this second generation are \nfeedforward and recurrent sigmoidal neural nets, as well \nas networks of radial basis function units. These nets are \nalso able to compute (with the help of thresholding at the \nnetwork output) arbitrary boolean functions. Actually it \nhas been shown that neural nets from the second genera\u0002tion can compute certain boolean functions with fewer \ngates than neural nets from the first generation (Maass, \nSchnitger, & Sontag, 1991; DasGupta & Schnitger, \n1993). In addition, neural nets from the second genera\u0002tion are able to compute functions with analog input and \noutput. In fact they are universal for analog computations \nin the sense that any continuous function with a compact \ndomain and range can be approximated arbitrarily well \n(with regard to uniform convergence, i.e., the L= - \nnorm) by a network of this type with a single hidden \nlayer. Another characteristic feature of this second \ngeneration of neural network models is that they support \nlearning algorithms that are based on gradient descent \nsuch as backprop. \n1659 \n1660 W. Maass \nAI \nA2 \nA3 \nA4 \nA5 \nA6 \nB1 \nB2 \nB3 \nB4 \nB5 \nB6 \nCI \nC2 \nC3 \nC4 \nC5 \nC6 \nDII \nD2 \nD3 \nD4 \nD5 \nD6 \nEl \nE2 \nE3 \nE4 \nE5 \nE6 \nI I l L \nI I It \n\u2022 I II I I I \nI I I I I i \nIll III ] Itlla III li II II \n\u2022 mill t I II II I \nI I \nII II I I I I II i \nI \nla II I I I I I I \n20O0 \nI J \u2022 \nI \nIII \nI III \nI It i \nII I I I II I! I !111 II \nII I I I [ I I I I \nI \n1 I!1 I II \nI I I II i \nI I \nI II l, I I \nIB II II I II I~ IIIInl I I II U \nI I1 I \nI I \nI I I! II \ni \nI I I \nl i i t i al i \u2022 it m It Ill i \nt I I \nU I I I I I I IIIIIII I I I I II I II U ! I II I II! ! III I \nI I \nI I I I I \nII111 I \u2022 Im II I ILl I II \u2022 I I If i Ill I I [ ] \nI II I I HI ~ n Ii iii I i i I I I III I I I I III i I | \nI I II I lib \nII ! I I Ill II I nl Illllll | \n! II I II II II i Ill | \n|1 I II |1 I I t I I I I t \nFIGURE 1. Simultaneous recordings (over 4 sec) of the firing times of 30 neurons from monkey striate cortex by Kriiger & Aiple (1988). \nEach firing is denoted by a short vertical bar, with a separate row for each neuron. For comparison we have marked the length of an \ninterval of 100 msec by two vertical lines. This time span is known to suffice for the completion of some complex multilayer cortical \ncomputations. \nFor a biological interpretation of neural nets from the \nsecond generation one views the output of a sigmoidal \nunit as a representation of the current firing rate of a \nbiological neuron. Since biological neurons, especially \nin higher cortical areas, are known to fire at various \nintermediate frequencies between their minimum and \nmaximum frequency, neural nets from the second gen\u0002eration are, with regard to this \"firing rate interpreta\u0002tion\", biologically more realistic than models from the \nfirst generation. \nHowever, at least with regard to fast analog computa\u0002tions by networks of neurons in the cortex, the \"firing \nrate interpretation\" itself has become questionable. Per\u0002rett, Rolls, and Caan (1982) and Thorpe and Imbert \n(1989) have demonstrated that visual pattern analysis \nand pattern classification can be carried out by humans \nin just 100 msec, in spite of the fact that it involves a \nminimum of 10 synaptic stages from the retina to the \ntemporal lobe (see Figure 1.) The same speed of visual \nprocessing has been measured by Rolls and Tovee (1994) \nin macaque monkeys. Furthermore, they have shown that \na single cortical area involved in visual processing can \ncomplete its computation in just 20-30 msec (Rolls, \n1994; Rolls & Tovee, 1994). On the other hand, the \nfiring rates of neurons involved in these computations \nare usually below 100Hz, and hence at least 20- \n30 msec would be needed just to sample the current \nfiring rate of a neuron. Thus a coding of analog variables \nby firing rates seems quite dubious in the context of fast \ncortical computations. \nOn the other hand, experimental evidence has accu\u0002mulated during the last few years which indicates that \nmany biological neural systems use the timing of single \naction potentials (or \"spikes\") to encode information \n(Abeles, 1991; Abeles, Bergman, Margalit, & Vaadia, \n1993; Aertsen, 1993; Arbib, 1995; Bair, Koch, News\u0002ome, & Britten, 1994; Bialek & Rieke, 1992; Ferster & \nSpruston, 1995; Hopfield, 1995; Kempter, Gerstner, van \nHemmen, & Wagner, 1996; Lestienne, 1996; Rieke, \nWarland, van Stevenick, & Bialek, 1996; Sejnowski, \n1995; Singer, 1995; Softky, 1994; Thorpe & Imbert, \n1989). \nThese experimental results from neurobiology have \nlead to the investigation of a third generation of neural \nnetwork models which employ spiking neurons (or \n\"integrate-and-fire neurons\") as computational units. \nRecently, one has also started to carry out experiments \nwith related new types of electronic hardware such as \npulse stream VLSI (see, e.g., DeYong, Findley, & Fields, \n1992; Douglas, Koch, Mahowald, Martin, & Suarez, \n1995; Horinchi, Lazzaro, Moore, & Koch, 1991; \nJahnke, Roth, & Klar, 1996; Jiu & Leong, 1996; \nMahowald, 1992, 1994; Mead, 1989; Meador, Wu, \nCole, Nintunze, & Chintrakulchai, 1991; Murray & \nNetworks of Spiking Neurons: The Third Generation of Neural Network Models 1661 \nV \n0 \n: \ns t \nV \nI \ns ~~oe~,o(t-s) \nFIGURE 2. Typical shape of response functions (EPSP and IPSP) \nof a biological neuron. \nTarassenko, 1994; Northmore & Elias, 1996; Pratt, 1989; \nZaghloul, Meador, & Newcomb, 1994). In these new \nchips one can encode analog variables by time differ\u0002ences between pulses, which has practical advantages \nover other encoding methods. The goal of understanding \nthe capabilities and limitations of this new type of analog \nneural hardware provides additional motivation for \ntheoretical investigation of the third generation of \nneural network models. \nOne may also view threshold circuits (i.e., neural nets \nfrom the first generation) as abstract models for digital \ncomputation on networks of spiking neurons, where the \nbit 1 is coded by the firing of a neuron within a certain \nshort time window, and 0 by the non-firing of this neuron \nwithin this time window (see e.g., Valiant, 1994). How\u0002ever, under this coding scheme a threshold circuit pro\u0002vides a reasonably good model for a network of spiking \nneurons only if the firing times of all neurons that provide \nthe input bits for another spiking neuron are synchronized \n(up to a few msec). Apparently such strongly synchro\u0002nized activity does occur in biological neural systems \n(see Abeles et al., 1993; Bair et al., 1994) but many \nargue that it is not their typical",
          "original_query": "Networks of spiking neurons: The third generation of neural network models",
          "cleaned_query": "Networks of spiking neurons: The third generation of neural network models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization",
          "url": "https://arxiv.org/abs/2511.12199",
          "content": "\n View PDF \n HTML (experimental) \n Abstract: The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.\n \n \n Submission history From: Runhao Jiang [ view email] [v1] \nSat, 15 Nov 2025 13:12:20 UTC (464 KB) \n",
          "original_query": "Surrogate gradient learning in spiking neural networks",
          "cleaned_query": "Surrogate gradient learning in spiking neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Surrogate Gradient Learning in Spiking Neural Networks: Theory",
          "url": "https://zenkelab.org/2024/04/elucidating-the-theoretical-underpinnings-of-surrogate-gradient-learning-in-spiking-neural-networks/",
          "content": "Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks &#8211; Zenke Lab\n[Skip to content](#content)\n# [Zenke Lab](https://zenkelab.org/)\n## Computational Neuroscience at the FMI\n![](https://zenkelab.org/wp-content/uploads/2024/04/sg_theory_tree.png)\n# Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks\n[April 25, 2024April 25, 2024](https://zenkelab.org/2024/04/elucidating-the-theoretical-underpinnings-of-surrogate-gradient-learning-in-spiking-neural-networks/)[fzenke](https://zenkelab.org/author/fzenke/)\nSurrogate gradients (SGs) are empirically successful at training spiking neural networks (SNNs). But why do they work so well, and what is their theoretical basis? In our new preprint led by Julia, we answer these questions.\nPaper:[arxiv.org/abs/2404.14964](http://arxiv.org/abs/2404.14964)\nWe studied the relation of SGs with two theoretical frameworks: 1) Smoothed probabilistic models, which provide exact gradients in stochastic neurons. 2) Stochastic autodifferentiation, which deals with derivatives of discrete random variables but hasn&#8217;&#8217;tbeen appliedto SNNs.\n[![](https://zenkelab.org/wp-content/uploads/2024/04/image4-1024x557.png)](https://zenkelab.org/wp-content/uploads/2024/04/image4.png)\nWhile SGs are equal to the gradient of the expected output in single neurons, this equivalence breaks in deep nets. Here, SGs can be understood as smoothed stochastic derivatives, and the form of the surrogate derivative is linked to the escape noise function of the neurons.\nThe stochastic motivation makes SGs ideal for training stochastic SNNs. In our experiments, they achieved comparable performance to their deterministic counterparts, albeit with biologically plausible levels of trial-to-trial variability.\n[![](https://zenkelab.org/wp-content/uploads/2024/04/image-2-1024x476.png)](https://zenkelab.org/wp-content/uploads/2024/04/image-2.png)\nCuriously, we also find that we cannot interpret SGs as gradients of a surrogate loss. They do not result in an integral of the closed loop of zero and, therefore, do not correspond to a conservative field.\n[![](https://zenkelab.org/wp-content/uploads/2024/04/image-1-1024x355.png)](https://zenkelab.org/wp-content/uploads/2024/04/image-1.png)\nWhen comparing SGs to actual gradients in differentiable sigmoid networks, we found that the sign of the SG doesn&#8217;t always match that of the exact gradient. Consequently, SG descent is not guaranteed to find a local minimum of the loss.\n[![](https://zenkelab.org/wp-content/uploads/2024/04/image-1024x438.png)](https://zenkelab.org/wp-content/uploads/2024/04/image.png)\n**Search for:#### Links\n* [Basel Neurocircuit Community](https://cncb.ch)\n* [Swiss Comp Neuro Network](https://www.swisscompneuro.org/)\n* [PhD program](https://www.fmi.ch/education-careers/programs/#phd-program)\n* [FMI website](https://www.fmi.ch/research-groups/groupleader.html?group=142)\n* [Privacy policy](https://zenkelab.org/privacy-policy/)\n#### Latest news\n* [Understanding cortical computation through the lens of joint-embedding predictive architectures](https://zenkelab.org/2025/11/understanding-cortical-computation-through-the-lens-of-joint-embedding-predictive-architectures/)\n* [New mates on our crew](https://zenkelab.org/2025/11/new-mates-on-our-crew/)\n* [SNUFA 2025](https://zenkelab.org/2025/10/snufa-2025/)\n* [Our work featured by The Transmitter](https://zenkelab.org/2025/09/our-work-featured-by-the-transmitter/)\n* [Breaking Balance: Encoding local error signals in perturbations of excitation-inhibition balance](https://zenkelab.org/2025/07/breaking-balance-encoding-local-error-signals-in-perturbations-of-excitation-inhibition-balance/)[![](https://zenkelab.org/wp-content/uploads/2023/12/2023-coat_of_arms_white_tentacles-236x300.png)](https://zenkelab.org/wp-content/uploads/2023/12/2023-coat_of_arms_white_tentacles.png)\n[![](https://zenkelab.org/wp-content/uploads/2023/05/FMI_logo_extended_white.png)](http://www.fmi.ch)\n[![](https://zenkelab.org/wp-content/uploads/2023/04/University_Basel_2018_logo_white.png)](https://www.unibas.ch)\n[![](https://zenkelab.org/wp-content/uploads/2023/09/NNB_Logo.png)](https://www.neuronetwork.unibas.ch/)",
          "original_query": "Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks",
          "cleaned_query": "Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Spiking Neural Network Actor\u2013Critic Reinforcement Learning with Temporal Coding and Reward-Modulated Plasticity",
          "url": "https://link.springer.com/article/10.3103/S0027134924702400?error=cookies_not_supported&code=20757227-5dd3-4b44-8a76-2a80bd079586",
          "content": "# Spiking Neural Network Actor\u2013Critic Reinforcement Learning with Temporal Coding and Reward-Modulated Plasticity\n\n- Published: 22 March 2025\n\n- Volume\u00a079,\u00a0pages S944\u2013S952, (2024)\n- [Cite this article](https://link.springer.com/article/10.3103/S0027134924702400?error=cookies_not_supported&code=20757227-5dd3-4b44-8a76-2a80bd079586#citeas)\n\n[![](https://media.springernature.com/w144/springer-static/cover-hires/journal/11972?as=webp)Moscow University Physics Bulletin](https://link.springer.com/journal/11972) [Aims and scope](https://link.springer.com/journal/11972/aims-and-scope)\n\n### Abstract\n\nThe article presents an algorithm for adjusting the weights of the spike neural network of the actor\u2013critic architecture. A feature of the algorithm is the use of time coding of input data. The critic neuron is applied to calculate the change in the expected value of the action performed based on the difference in spike times received by the critic when processing the previous and current states. The change in the weights of the synaptic connections of the actor and critic neurons is carried out under the influence of local plasticity (spike\u2013timing-dependent plasticity), in which the change in weight depends on the received value of the expected reward. The proposed learning algorithm was tested to solve the problem of holding a cart pole, in which it demonstrated its effectiveness. The proposed algorithm is an important step towards the implementation of reinforcement learning algorithms for spiking neural networks on neuromorphic computing devices.\n\nThis is a preview of subscription content, [log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Farticle%2F10.3103%2FS0027134924702400%3Ferror%3Dcookies_not_supported%26code%3D20757227-5dd3-4b44-8a76-2a80bd079586) to check access.\n\n## Access this article\n\n[Log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Farticle%2F10.3103%2FS0027134924702400%3Ferror%3Dcookies_not_supported%26code%3D20757227-5dd3-4b44-8a76-2a80bd079586)\n\n## Subscribe and save\n\nSpringer+ Basic\n\n\u20ac32.70 /Month\n\n- Get 10 units per month\n- Download Article/Chapter or eBook\n- 1 Unit = 1 Article or 1 Chapter\n- Cancel anytime\n\n[Subscribe now](https://link.springer.com/product/springer-plus)\n\n## Buy Now\n\nBuy article PDF 39,95 \u20ac\n\nPrice includes VAT (Australia)\n\nInstant access to the full article PDF.\n\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/journals)\n\n**Fig. 1**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig1_HTML.png)\n\n**Fig. 2**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig2_HTML.png)\n\n**Fig. 3**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig3_HTML.png)\n\n**Fig. 4**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig4_HTML.png)\n\n**Fig. 5**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig5_HTML.png)\n\n**Fig. 6**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig6_HTML.png)\n\n**Fig. 7**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig7_HTML.png)\n\n**Fig. 8**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig8_HTML.png)\n\n**Fig. 9**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig9_HTML.png)\n\n### Explore related subjects\n\nDiscover the latest articles and news from researchers in related subjects, suggested using machine learning.\n\n- [Spike-timing-dependent plasticity](https://link.springer.com/subjects/spike-timing-dependent-plasticity)\n- [Neural encoding](https://link.springer.com/subjects/neural-encoding)\n- [Neural decoding](https://link.springer.com/subjects/neural-decoding)\n- [Synaptic plasticity](https://link.springer.com/subjects/synaptic-plasticity)\n- [Computational Neuroscience](https://link.springer.com/subjects/computational-neuroscience)\n- [Stochastic Learning and Adaptive Control](https://link.springer.com/subjects/stochastic-learning-and-adaptive-control)\n\n## Notes\n\n1. https://motivnt.ru/neurochip-altai.\n\n2. https://scikit-learn.org/stable/modules/preprocessing.html.\n\n\n## REFERENCES\n\n01. J. Zhu, T. Zhang, Yu. Yang, and R. Huang, Appl. Phys. Rev. **7**, 11312 (2020). [https://doi.org/10.1063/1.5118217](https://doi.org/10.1063/1.5118217)\n\n [Article](https://doi.org/10.1063%2F1.5118217) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=&journal=Appl.%20Phys.%20Rev.&doi=10.1063%2F1.5118217&volume=7&publication_year=2020&author=Zhu%2CJ.&author=Zhang%2CT.&author=Yang%2CYu.&author=Huang%2CR.)\n\n02. D. Ielmini and S. Menzel, in _Resistive Switching_, Ed. by D. Ielmini and R. Waser (Wiley, 2016), p. 317. [https://doi.org/10.1002/9783527680870.ch11](https://doi.org/10.1002/9783527680870.ch11)\n\n [Book](https://doi.org/10.1002%2F9783527680870.ch11) [MATH](http://www.emis.de/MATH-item?1416.65577) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Resistive%20Switching&doi=10.1002%2F9783527680870.ch11&publication_year=2016&author=Ielmini%2CD.&author=Menzel%2CS.)\n\n03. Yu. V. Pershin and M. Di Ventra, Neural Networks **23**, 881 (2010). [https://doi.org/10.1016/j.neunet.2010.05.001](https://doi.org/10.1016/j.neunet.2010.05.001)\n\n [Article](https://doi.org/10.1016%2Fj.neunet.2010.05.001) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=&journal=Neural%20Networks&doi=10.1016%2Fj.neunet.2010.05.001&volume=23&publication_year=2010&author=Pershin%2CYu.%20V.&author=Di%20Ventra%2CM.)\n\n04. K. Berggren, Q. Xia, K. K. Likharev, D. B. Strukov, H. Jiang, T. Mikolajick, D. Querlioz, M. Salinga, J. R. Erickson, Sh. Pi, F. Xiong, P. Lin, C. Li, Yu. Chen, Sh. Xiong, B. D. Hoskins, M. W. Daniels, A. Madhavan, J. A. Liddle, J. J. Mcclelland, Yu. Yang, J. Rupp, S. S. Nonnenmann, K.-T. Cheng, N. Gong, M. A. Lastras-Monta\u00f1o, A. A. Talin, A. Salleo, B. J. Shastri, T. F. De Lima, P. Prucnal, A. N. Tait, Yi. Shen, H. Meng, Ch. Roques-Carmes, Z. Cheng, H. Bhaskaran, D. Jariwala, H. Wang, J. M. Shainline, K. Segall, J. J. Yang, K. Roy, S. Datta, and A. Raychowdhury, Nanotechnology **32**, 012002 (2020). [https://doi.org/10.1088/1361-6528/aba70f](https://doi.org/10.1088/1361-6528/aba70f)\n\n05. B. Rajendran, A. Sebastian, M. Schmuker, N. Srinivasa, and E. Eleftheriou, IEEE Signal Process. Mag. **36**, 97 (2019). [https://doi.org/10.1109/msp.2019.2933719](https://doi.org/10.1109/msp.2019.2933719)\n\n [Article](https://doi.org/10.1109%2Fmsp.2019.2933719) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=&journal=IEEE%20Signal%20Process.%20Mag.&doi=10.1109%2Fmsp.2019.2933719&volume=36&publication_year=2019&author=Rajendran%2CB.&author=Sebastian%2CA.&author=Schmuker%2CM.&author=Srinivasa%2CN.&author=Eleftheriou%2CE.)\n\n06. P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy, J. Sawada, F. Akopyan, B. L. Jackson, N. Imam, Ch. Guo, Yu. Nakamura, B. Brezzo, I. Vo, S. K. Esser, R. Appuswamy, B. Taba, A. Amir, M. D. Flickner, W. P. Risk, R. Manohar, and D. S. Modha, Science **345**, 668 (2014). [https://doi.org/10.1126/science.1254642](https://doi.org/10.1126/science.1254642)\n\n [Article](https://doi.org/10.1126%2Fscience.1254642) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&bibcode=2014Sci...345..668M) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=&journal=Science&doi=10.1126%2Fscience.1254642&volume=345&publication_year=2014&author=Merolla%2CP.%20A.&author=Arthur%2CJ.%20V.&author=Alvarez-Icaza%2CR.&author=Cassidy%2CA.%20S.&author=Sawada%2CJ.&author=Akopyan%2CF.&author=Jackson%2CB.%20L.&author=Imam%2CN.&author=Guo%2CCh.&author=Nakamura%2CYu.&author=Brezzo%2CB.&author=Vo%2CI.&author=Esser%2CS.%20K.&author=Appuswamy%2CR.&author=Taba%2CB.&author=Amir%2CA.&author=Flickner%2CM.%20D.&author=Risk%2CW.%20P.&author=Manohar%2CR.&author=Modha%2CD.%20S.)\n\n07. M. Davies, N. Srinivasa, T.-H. Lin, G. Chinya, Yo. Cao, S. H. Choday, G. Dimou, P. Joshi, N. Imam, Sh. Jain, Yu. Liao, C.-K. Lin, A. Lines, R. Liu, D. Mathaikutty, S. Mccoy, A. Paul, J. Tse, G. Venkataramanan, Y.-H. Weng, A. Wild, Yo. Yang, and H. Wang, IEEE Micro **38**, 82 (2018). [https://doi.org/10.1109/mm.2018.112130359](https://doi.org/10.1109/mm.2018.112130359)\n\n [Article](https://doi.org/10.1109%2Fmm.2018.112130359) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=&journal=IEEE%20Micro&doi=10.1109%2Fmm.2018.112130359&volume=38&publication_year=2018&author=Davies%2CM.&author=Srinivasa%2CN.&author=Lin%2CT.-H.&author=Chinya%2CG.&author=Cao%2CYo.&author=Choday%2CS.%20H.&author=Dimou%2CG.&author=Joshi%2CP.&author=Imam%2CN.&author=Jain%2CSh.&author=Liao%2CYu.&author=Lin%2CC.-K.&author=Lines%2CA.&author=Liu%2CR.&author=Mathaikutty%2CD.&author=Mccoy%2CS.&author=Paul%2CA.&author=Tse%2CJ.&author=Venkataramanan%2CG.&author=Weng%2CY.-H.&author=Wild%2CA.&author=Yang%2CYo.&author=Wang%2CH.)\n\n08. J. Pei, L. Deng, S. Song, M. Zhao, Yo. Zhang, Sh. Wu, G. Wang, Zh. Zou, Zh. Wu, W. He, F. Chen, N. Deng, S. Wu, Yu. Wang, Yu. Wu, Zh. Yang, Ch. Ma, G. Li, W. Han, H. Li, H. Wu, R. Zhao, Yu. Xie, and L. Shi, Nature **572**, 106 (2019). [https://doi.org/10.1038/s41586-019-1424-8](https://doi.org/10.1038/s41586-019-1424-8)\n\n [Article](https://doi.org/10.1038%2Fs41586-019-1424-8) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&bibcode=2019Natur.572..106P) [Google Scholar](http://scholar.google.com/sc",
          "original_query": "Reinforcement learning using a continuous time actor-critic framework with spiking neurons",
          "cleaned_query": "Reinforcement learning using a continuous time actor-critic framework with spiking neurons",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Jump-Start RL",
          "url": "https://jumpstart-rl.github.io/",
          "content": "Jump-Start RL\n## **Jump-Start Reinforcement Learning**\n## ICML 2023\n* [Ikechukwu Uchendu](https://ikeuchendu.com/)\n* [Ted Xiao](https://tedxiao.me/)\n* [Yao Lu](https://scholar.google.com/citations?hl=en&user=OI7zFmwAAAAJ&)\n* [Banghua Zhu](https://people.eecs.berkeley.edu/~banghua/)\n* [Mengyuan Yan](https://scholar.google.com/citations?user=-S_9ZRcAAAAJ&hl=en)\n* [Jos&eacute;phine Simon]()\n* [Matthew Bennice]()\n* [Chuyuan Fu]()\n* [Cong Ma](https://congma1028.github.io/)\n* [Jiantao Jiao](https://people.eecs.berkeley.edu/~jiantao/)\n* [Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/)\n* [Karol Hausman](https://karolhausman.github.io/)\n[![](img/robotics-at-google.png)Robotics at Google](http://g.co/robotics)[![](img/EverydayRobots2.gif)Everyday Robots](https://everydayrobots.com/)\n* [![](img/mip_paper_image.jpg)#### **Paper**\n](https://arxiv.org/abs/2204.02372)\n* [![](img/google-ai-blog-small.png)#### **Blogpost**\n](https://ai.googleblog.com/2022/04/efficiently-initializing-reinforcement.html)\n### Abstract\nReinforcement learning (RL) provides a theoretical framework for continuously improving an\nagent's behavior via trial and error. However, efficiently learning policies from scratch\ncan be very difficult, particularly for tasks with exploration challenges. In such settings,\nit might be desirable to initialize RL with an existing policy, offline data, or\ndemonstrations. However, naively performing such initialization in RL often works poorly,\nespecially for value-based methods.\nIn this paper, we present a meta algorithm that can use offline data, demonstrations, or a\npre-existing policy to initialize an RL policy, and is compatible with any RL approach.\nIn particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that\nemploys two policies to solve tasks: a guide-policy, and an exploration-policy.\nBy using the guide-policy to form a curriculum of starting states for the\nexploration-policy, we are able to efficiently improve performance on a set of simulated\nrobotic tasks.\nWe show via experiments that JSRL is able to significantly outperform existing imitation and\nreinforcement learning algorithms, particularly in the small-data regime.\nIn addition, we provide an upper bound on the sample complexity of JSRL and show that with\nthe help of a guide-policy, one can improve the sample complexity for non-optimism\nexploration methods from exponential in horizon to polynomial.\n### Approach\nWe\u2019re introducing a meta-algorithm called Jump-Start Reinforcement Learning (JSRL) that can\nuse a pre-existing policy of any form to initialize any type of RL algorithm. JSRL uses two\npolicies to learn tasks: a guide-policy, and an exploration-policy. The exploration-policy\nis an RL policy that is trained online with new experience, and the guide-policy is a fixed,\npre-existing policy of any form. In this work, we focus on scenarios where the guide-policy\nis learned from demonstrations, but many other kinds of guide-policies can be used. It could\nbe a scripted policy, a policy trained with RL, or even a live human demonstrator. The only\nrequirements are that the guide-policy is reasonable (i.e., better than random exploration),\nand it can select actions based on observations of the environment.\nAt the beginning of training, we roll out the guide-policy for a fixed number of steps so\nthat the agent is closer to goal states. The exploration-policy then takes over and\ncontinues acting in the environment to reach these goals. As the performance of the\nexploration-policy improves, we gradually reduce the number of steps that the guide-policy\ntakes, until the exploration-policy takes over completely. This process creates a curriculum\nof starting states for the exploration-policy such that in each curriculum stage, it only\nneeds to learn to reach the initial states of prior curriculum stages.\n### Results\n**Comparison to IL+RL Baselines**: Since JSRL can use a prior policy to initialize RL,\na natural comparison would be to\nimitation and reinforcement learning (IL+RL) methods that train on offline datasets, then\nfine-tune. We show how JSRL compares to competitive IL+RL methods on the D4RL benchmark\ntasks, which vary in complexity and offline dataset quality. Out of the D4RL tasks, we\nfocus on the difficult ant maze and adroit dexterous manipulation environments.\nFor each experiment, we train on an offline dataset then run online fine-tuning. We\ncompare against algorithms designed specifically for this setting, which include AWAC,\nIQL, CQL, and behavioral cloning (BC). While JSRL can be used in combination with any\ninitial guide-policy or fine-tuning algorithm, we use a pre-trained IQL policy as the\nguide and also use IQL for fine-tuning. We find that JSRL performs well even with limited\naccess to demonstrations:\n**Vision-Based Robotic Tasks**: Utilizing offline data is especially challenging in\ncomplex tasks such as vision-based robotic manipulation. The high dimensionality of both\nthe continuous-control action spaces as well as the pixel-based state space present unique\nscaling challenges for IL+RL methods. To study how JSRL scales to such settings, we focus\non two challenging simulated robotic manipulation tasks: indiscriminate grasping and\ninstance grasping.\nWe compare our algorithm against methods that are able to scale to complex vision-based\nrobotics settings such as Qt-Opt and AW-Opt. Each method has access to the same offline\ndataset of successful demonstrations and is allowed to run online fine-tuning for\nup to 100,000 steps.\nIn these experiments, we use BC as a guide-policy and combine JSRL with Qt-Opt for\nfine-tuning. The combination of Qt-Opt+JSRL significantly outperforms the other methods in\nboth sample efficiency and final performance.\n### Citation\n[[arxiv version]](https://arxiv.org/abs/2204.02372)\n@inproceedings{jsrl2022arxiv,\ntitle={Jump-Start Reinforcement Learning},\nauthor={Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Jos\u00e9phine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, Sergey Levine, and Karol Hausman},\nbooktitle={arXiv preprint arXiv:2204.02372},\nyear={2022}\n}\n### Acknowledgements\nThe authors would like to thank Kanishka Rao, Nikhil Joshi, and Alex Irpan for their insightful\ndiscussions and feedback on our work,\nRosario Jauregui Ruano for performing physical robot experiments with JSRL, and Tom Small for\ncreating the animations for this website. Jiantao Jiao and Banghua Zhu were partially supported by\nNSF Grants IIS-1901252 and CCF-1909499.\nThe website template was borrowed from[Jon Barron](http://jonbarron.info/).",
          "original_query": "Jump-start reinforcement learning",
          "cleaned_query": "Jump-start reinforcement learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Recurrent Experience Replay in Distributed Reinforcement ...",
          "url": "https://openreview.net/forum?id=r1lyTjAqYX",
          "content": "[![back arrow](https://openreview.net/images/arrow_left.svg)Go to **ICLR 2019 Conference** homepage](https://openreview.net/group?id=ICLR.cc/2019/Conference)\n\n\u00d7\n\n## Recurrent Experience Replay in Distributed Reinforcement Learning [![](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=r1lyTjAqYX)\n\n## Blind Submission by Conference \u2022 Recurrent Experience Replay in Distributed Reinforcement Learning\n\n[Steven Kapturowski](https://openreview.net/profile?email=skapturowski%40google.com), [Georg Ostrovski](https://openreview.net/profile?email=ostrovski%40google.com), [John Quan](https://openreview.net/profile?email=johnquan%40google.com), [Remi Munos](https://openreview.net/profile?email=munos%40google.com), [Will Dabney](https://openreview.net/profile?email=wdabney%40google.com)\n\nPublished: 20 Dec 2018, Last Modified: 05 May 2023ICLR 2019 Conference Blind SubmissionReaders: Everyone[Show Bibtex](https://openreview.net/forum?id=r1lyTjAqYX) [Show Revisions](https://openreview.net/revisions?id=r1lyTjAqYX)\n\nKeywords: RNN, LSTM, experience replay, distributed training, reinforcement learning\n\nTL;DR: Investigation on combining recurrent neural networks and experience replay leading to state-of-the-art agent on both Atari-57 and DMLab-30 using single set of hyper-parameters.\n\nAbstract: Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyper-parameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the first agent to exceed human-level performance in 52 of the 57 Atari games.\n\nCode: [![Papers with Code](https://openreview.net/images/pwc_icon.svg) 3 community implementations](https://paperswithcode.com/paper/?openreview=r1lyTjAqYX)\n\nData: [Arcade Learning Environment](https://paperswithcode.com/dataset/arcade-learning-environment), [DQN Replay Dataset](https://paperswithcode.com/dataset/dqn-replay-dataset)\n\n* * *\n\nReply Type:\n\nall\n\n- Select All\n- Paper765 Meta Review\n- Paper765 Official Review\n- Paper765 Official Comment\n- Paper765 Public Comment\n\nAuthor:\n\neverybody\n\n- Select All\n- Paper765 Authors\n- Paper765 AnonReviewer1\n- Paper765 AnonReviewer2\n- Paper765 AnonReviewer3\n- Paper765 Area Chair1\n- (anonymous)\n\nVisible To:\n\nall readers\n\n- Select All\n- everyone\n\nHidden From:\n\nnobody\n\n- Select All\n- everyone\n\n22 Replies\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## Valuable insights on training reinforcement learning with recurrent neural networks at scale\n\n## Meta Review of Paper765 by Area Chair1 \u2022 Valuable insights on training reinforcement learning with recurrent neural networks at scale\n\nICLR 2019 Conference Paper765 Area Chair1\n\n13 Dec 2018, 16:33 (modified: 20 Dec 2018, 20:08)ICLR 2019 Conference Paper765 Meta ReviewReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=BkeeQlUelV)\n\nMetareview: The paper proposes a new distributed DQN algorithm that combines recurrent neural networks with distributed prioritized replay memory. The authors systematically compare three types of initialization strategies for training the recurrent models. The thorough investigation is cited as a valuable contribution by all reviewers, with reviewer 1 noting that the study would be of interest to \"anyone using recurrent networks on RL tasks\". Empirical results on Atari and DMLab are impressive.\nThe reviewers noted several weaknesses in their original reviews. These included issues of clarity, a need for more detailed ablation studies, and need to more carefully document the empirical setup. A further question was raised on whether the empirical results could be complemented with theoretical or conceptual insights.\nThe authors carefully addressed all concerns raised during the reviewing and rebuttal period. They took exceptional care to clarify their writing, document experiment details, and ran a large set of additional experiments as suggested by the reviewers. The AC feels that the review period for the paper was particularly productive and would like to thank the reviewers and authors.\nThe reviewers and AC agree that the paper makes a significant contribution to the field and should be accepted.\n\nRecommendation: Accept (Poster)\n\nConfidence: 4: The area chair is confident but not absolutely certain\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## All Reviewers\n\n## Official Comment by Paper765 Authors \u2022 All Reviewers\n\nICLR 2019 Conference Paper765 Authors\n\n20 Nov 2018, 12:41ICLR 2019 Conference Paper765 Official CommentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=SkxyvPTZ0X)\n\nComment: We first want to thank all the reviewers and commenters for their close reading and constructive feedback. We have revised or expanded most of our experiments and attempted to clarify the text in various locations. Additionally, as requested we are including (in the appendix) a figure comparing the sample efficiency of R2D2 with Rainbow, Ape-X, and Reactor on Atari. As detailed in our individual responses we have extended the ablations and rerun our experiments to address some concerns. This resulted in slightly improved performance, which is now averaged over three seeds.\nWe intend to add further ablation results (on life-loss signal) and our own rerun of IMPALA with matched action-set before the revision period ends.\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## All Reviewers\n\n## Official Comment by Paper765 Authors \u2022 All Reviewers\n\nICLR 2019 Conference Paper765 Authors\n\n27 Nov 2018, 10:31ICLR 2019 Conference Paper765 Official CommentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=ryx6VXJoCm)\n\nComment: We thank all reviewers once again for the careful reading of the paper and the helpful comments.\nWe have updated our ablations, now including two life-loss ablations, and provided complete details on the feed-forward ablation including human-normalized scores and sample efficiency data.\nFinally, we have updated the paper with our own re-run of the IMPALA agent on DMLab with the new action-set and longer training time, for a fairer comparison with R2D2. To explore the potential of our agent further, we also added a version of R2D2 more closely matching the Deep IMPALA architecture (deep ResNet + asymmetric reward clipping). Both our re-run of IMPALA and R2D2 achieve new SOTA scores on DMLab-30. We intend to report all DMLab-30 scores at 10B environment frames, but have restricted to 5B frames for this revision as these runs have not all completed at the time of the revision deadline.\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## Re: All Reviewers\n\n## Official Comment by Paper765 AnonReviewer1 \u2022 Re: All Reviewers\n\nICLR 2019 Conference Paper765 AnonReviewer1\n\n27 Nov 2018, 17:02ICLR 2019 Conference Paper765 Official CommentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=S1xbJJBj0Q)\n\nComment: Thanks, very interesting! Could you please just add a short description of R2D2+ in Table 1's caption, for the final version?\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## Re: All Reviewers\n\n## Official Comment by Paper765 Authors \u2022 Re: All Reviewers\n\nICLR 2019 Conference Paper765 Authors\n\n29 Nov 2018, 08:55ICLR 2019 Conference Paper765 Official CommentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=S1x50y_aRm)\n\nComment: Absolutely, thanks!\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## Rrecurrent NNs in distributted RL settings as a clear improvement of the feed-forward NN variations in partially observed environments\n\n## Official Review of Paper765 by AnonReviewer2 \u2022 Rrecurrent NNs in distributted RL settings as a clear improvement of the feed-forward NN variations in partially observed environments\n\nICLR 2019 Conference Paper765 AnonReviewer2\n\n05 Nov 2018, 04:05 (modified: 06 Nov 2018, 14:48)ICLR 2019 Conference Paper765 Official ReviewReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=Ske9lOYp27)\n\nReview: This paper investigates the use of recurrent NNs in distributted RL settings as a clear improvement of the feed-forward NN variations in partially observed environments. The authors present \"R2DR\" algorithm as a A+B approach from previous works (actually, R2D2 is an Ape-X-like agent using LTSM), as well as an empirical study of a number of ways for training RNN from replay in terms of the effects of parameter lag (and potential alleviating actions) and sample-afficiency. The results presented show impressive performance in Atari-57 and DMLab-30 benchmarks.\nIn summary, this is a very nice paper in which the authors attack a challenging task and empirically confirm that RNN agents generalise far better when scaling up through parallelisation and distributed training allows them to benefit from huge experience. The results obtained in ALE and DMLab improves significantly upon the SOTA works, showing that the trend-line in those benchmarks seem to have been broken.\nFurthermore, the paper presents their approach/analyses in a well-structured manner and sufficient clarity to retrace the essential contribution. The background and results are well-contextualised with relevant related work.\nMy only major comments are that I\u2019m a bit skeptical about the lack of a more thorough (theoretical) analysis suppor",
          "original_query": "Recurrent experience replay in distributed reinforcement learning",
          "cleaned_query": "Recurrent experience replay in distributed reinforcement learning",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "MPD-Aware Adversarial Training for Deep SNNs (Match MPD Targets Across Layers)\nDevelop an adversarial training objective that *explicitly matches* each layer\u2019s membrane potential distribution (MPD) to a robustness-favoring target (low mass in the surrogate-gradient active window), extending MPD-SGR from regularization to full min\u2013max training. Implement layerwise MPD constraints (e.g., via moment matching or histogram KL) and evaluate robustness/accuracy trade-offs across different surrogate functions and coding schemes.",
        "Curriculum on \u201cGradient-Available Mass\u201d: A Robustness Scheduler for Surrogate Gradients\nCreate a training curriculum that gradually shrinks the surrogate-gradient active range (or equivalently drives MPD mass out of it) as learning progresses, inspired by the MPD\u2013SG interaction analysis. Concretely, schedule surrogate slope/width and MPD-SGR strength over epochs and test whether this yields better convergence than static surrogates while improving adversarial robustness.",
        "Nonconservative Surrogate Gradients: Closed-Loop Integral Minimization as a Stability Regularizer\nBuilding on the theory result that SG updates are not gradients of any surrogate loss (nonconservative field), add a regularizer that penalizes measured \u201cloop inconsistency\u201d of SG vector fields in parameter subspaces. Implement by sampling small parameter loops during training and minimizing the circulation estimate; test whether this improves optimization stability and reduces brittleness without sacrificing SNN accuracy.",
        "Escape-Noise-Calibrated Surrogates for Robust Stochastic SNNs\nUse the Zenke-lab link between surrogate derivatives and escape-noise functions to fit neuron-specific surrogate shapes from a chosen stochastic spiking model (or from recorded trial-to-trial variability). Train stochastic SNNs with these calibrated surrogates and quantify whether matching the implied escape noise improves robustness to both adversarial perturbations and synaptic/device noise (important for neuromorphic deployment).",
        "Spike-Time Actor\u2013Critic with Jump-Start State Curriculum (JSRL-SNN)\nCombine the temporal-coding actor\u2013critic with Jump-Start RL by using a guide-policy to generate a curriculum of *starting states* while the spiking actor\u2013critic learns with reward-modulated STDP. Concretely, implement guide roll-in lengths that decay over training and measure sample-efficiency gains on control tasks (CartPole \u2192 harder MuJoCo-style tasks) under tight spike/energy budgets.",
        "Replay for Spiking RL: \u201cSpike-Sequence Replay\u201d with Hidden-State Refresh\nAdapt Recurrent Experience Replay concepts to SNN-based RL by storing *spike trains plus membrane state summaries* and designing a replay procedure that mitigates state staleness (e.g., periodic membrane-state recomputation, burn-in via stored spikes). Benchmark against standard replay in spiking actor\u2013critic to quantify improvements in stability and data efficiency for partially observable tasks.",
        "Complexity-to-Robustness Bridge: Functions Where Few Spiking Neurons Are Also More Adversarially Stable\nInspired by Maass\u2019 neuron-efficiency separations, identify tasks/functions that admit compact spiking implementations but require large sigmoidal networks, then test whether the compact spiking solution exhibits intrinsic robustness due to sparse gradient-available MPD mass. Provide a concrete suite of \u201cseparation tasks\u201d plus robustness/gradient-norm analyses to connect expressivity advantages with adversarial resilience.",
        "Hardware-Aware MPD Control Under Memristive Variability for Neuromorphic RL\nExtend MPD-SGR to explicitly handle analog non-idealities (e.g., memristor drift/noise) by adding distributional constraints on membrane potentials that are robust under perturbed synaptic weights. Implement a co-design loop: simulate device noise models during training, optimize MPD targets, and deploy to spiking actor\u2013critic control where reward-modulated plasticity must tolerate on-chip variability.",
        "Adaptive Coding to Shape MPD: Learnable Temporal Encoders for Robust SNNs\nInstead of only regularizing internal MPD, make the *input spike encoder* (latency/temporal coding parameters) learnable with an objective that shapes downstream MPDs to reduce surrogate-gradient sensitivity. Implement encoder parameters optimized jointly with network weights (via surrogate gradients) and evaluate whether learned encodings improve adversarial robustness more than fixed rate/latency codes.",
        "Layerwise \u201cMPD Diagnostics\u201d for Debugging SG Training and Preventing Vanishing/Exploding Surrogate Gradients\nBuild a practical toolkit that monitors per-layer MPD overlap with each surrogate\u2019s gradient-available window and automatically triggers interventions (adjust thresholds, leak, surrogate width, or MPD-SGR strength). Validate by inducing known failure modes (gradient starvation vs oversensitivity) and showing the diagnostics can predict training collapse and prescribe corrective parameter updates in deep SNNs and spiking RL agents."
      ],
      "generated_ideas_raw": "1. **MPD-Aware Adversarial Training for Deep SNNs (Match MPD Targets Across Layers)**\n   Develop an adversarial training objective that *explicitly matches* each layer\u2019s membrane potential distribution (MPD) to a robustness-favoring target (low mass in the surrogate-gradient active window), extending MPD-SGR from regularization to full min\u2013max training. Implement layerwise MPD constraints (e.g., via moment matching or histogram KL) and evaluate robustness/accuracy trade-offs across different surrogate functions and coding schemes.\n\n2. **Curriculum on \u201cGradient-Available Mass\u201d: A Robustness Scheduler for Surrogate Gradients**\n   Create a training curriculum that gradually shrinks the surrogate-gradient active range (or equivalently drives MPD mass out of it) as learning progresses, inspired by the MPD\u2013SG interaction analysis. Concretely, schedule surrogate slope/width and MPD-SGR strength over epochs and test whether this yields better convergence than static surrogates while improving adversarial robustness.\n\n3. **Nonconservative Surrogate Gradients: Closed-Loop Integral Minimization as a Stability Regularizer**\n   Building on the theory result that SG updates are not gradients of any surrogate loss (nonconservative field), add a regularizer that penalizes measured \u201cloop inconsistency\u201d of SG vector fields in parameter subspaces. Implement by sampling small parameter loops during training and minimizing the circulation estimate; test whether this improves optimization stability and reduces brittleness without sacrificing SNN accuracy.\n\n4. **Escape-Noise-Calibrated Surrogates for Robust Stochastic SNNs**\n   Use the Zenke-lab link between surrogate derivatives and escape-noise functions to fit neuron-specific surrogate shapes from a chosen stochastic spiking model (or from recorded trial-to-trial variability). Train stochastic SNNs with these calibrated surrogates and quantify whether matching the implied escape noise improves robustness to both adversarial perturbations and synaptic/device noise (important for neuromorphic deployment).\n\n5. **Spike-Time Actor\u2013Critic with Jump-Start State Curriculum (JSRL-SNN)**\n   Combine the temporal-coding actor\u2013critic with Jump-Start RL by using a guide-policy to generate a curriculum of *starting states* while the spiking actor\u2013critic learns with reward-modulated STDP. Concretely, implement guide roll-in lengths that decay over training and measure sample-efficiency gains on control tasks (CartPole \u2192 harder MuJoCo-style tasks) under tight spike/energy budgets.\n\n6. **Replay for Spiking RL: \u201cSpike-Sequence Replay\u201d with Hidden-State Refresh**\n   Adapt Recurrent Experience Replay concepts to SNN-based RL by storing *spike trains plus membrane state summaries* and designing a replay procedure that mitigates state staleness (e.g., periodic membrane-state recomputation, burn-in via stored spikes). Benchmark against standard replay in spiking actor\u2013critic to quantify improvements in stability and data efficiency for partially observable tasks.\n\n7. **Complexity-to-Robustness Bridge: Functions Where Few Spiking Neurons Are Also More Adversarially Stable**\n   Inspired by Maass\u2019 neuron-efficiency separations, identify tasks/functions that admit compact spiking implementations but require large sigmoidal networks, then test whether the compact spiking solution exhibits intrinsic robustness due to sparse gradient-available MPD mass. Provide a concrete suite of \u201cseparation tasks\u201d plus robustness/gradient-norm analyses to connect expressivity advantages with adversarial resilience.\n\n8. **Hardware-Aware MPD Control Under Memristive Variability for Neuromorphic RL**\n   Extend MPD-SGR to explicitly handle analog non-idealities (e.g., memristor drift/noise) by adding distributional constraints on membrane potentials that are robust under perturbed synaptic weights. Implement a co-design loop: simulate device noise models during training, optimize MPD targets, and deploy to spiking actor\u2013critic control where reward-modulated plasticity must tolerate on-chip variability.\n\n9. **Adaptive Coding to Shape MPD: Learnable Temporal Encoders for Robust SNNs**\n   Instead of only regularizing internal MPD, make the *input spike encoder* (latency/temporal coding parameters) learnable with an objective that shapes downstream MPDs to reduce surrogate-gradient sensitivity. Implement encoder parameters optimized jointly with network weights (via surrogate gradients) and evaluate whether learned encodings improve adversarial robustness more than fixed rate/latency codes.\n\n10. **Layerwise \u201cMPD Diagnostics\u201d for Debugging SG Training and Preventing Vanishing/Exploding Surrogate Gradients**\n   Build a practical toolkit that monitors per-layer MPD overlap with each surrogate\u2019s gradient-available window and automatically triggers interventions (adjust thresholds, leak, surrogate width, or MPD-SGR strength). Validate by inducing known failure modes (gradient starvation vs oversensitivity) and showing the diagnostics can predict training collapse and prescribe corrective parameter updates in deep SNNs and spiking RL agents.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "MPD-Aware Adversarial Training for Deep SNNs (Match MPD Targets Across Layers)\nDevelop an adversarial training objective that *explicitly matches* each layer\u2019s membrane potential distribution (MPD) to",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Curriculum on \u201cGradient-Available Mass\u201d: A Robustness Scheduler for Surrogate Gradients\nCreate a training curriculum that gradually shrinks the surrogate-gradient active range (or equivalently drives ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Nonconservative Surrogate Gradients: Closed-Loop Integral Minimization as a Stability Regularizer\nBuilding on the theory result that SG updates are not gradients of any surrogate loss (nonconservative",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Escape-Noise-Calibrated Surrogates for Robust Stochastic SNNs\nUse the Zenke-lab link between surrogate derivatives and escape-noise functions to fit neuron-specific surrogate shapes from a chosen stoc",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Spike-Time Actor\u2013Critic with Jump-Start State Curriculum (JSRL-SNN)\nCombine the temporal-coding actor\u2013critic with Jump-Start RL by using a guide-policy to generate a curriculum of *starting states* wh",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Replay for Spiking RL: \u201cSpike-Sequence Replay\u201d with Hidden-State Refresh\nAdapt Recurrent Experience Replay concepts to SNN-based RL by storing *spike trains plus membrane state summaries* and designin",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Complexity-to-Robustness Bridge: Functions Where Few Spiking Neurons Are Also More Adversarially Stable\nInspired by Maass\u2019 neuron-efficiency separations, identify tasks/functions that admit compact sp",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hardware-Aware MPD Control Under Memristive Variability for Neuromorphic RL\nExtend MPD-SGR to explicitly handle analog non-idealities (e.g., memristor drift/noise) by adding distributional constraints",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Adaptive Coding to Shape MPD: Learnable Temporal Encoders for Robust SNNs\nInstead of only regularizing internal MPD, make the *input spike encoder* (latency/temporal coding parameters) learnable with ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Layerwise \u201cMPD Diagnostics\u201d for Debugging SG Training and Preventing Vanishing/Exploding Surrogate Gradients\nBuild a practical toolkit that monitors per-layer MPD overlap with each surrogate\u2019s gradien",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 45,
      "paper_title": "Class-wise Balancing Data Replay for Federated Class-Incremental Learning",
      "contribution": "Introduce FedCBDR, a privacy-preserving, global-perspective replay pipeline that (1) reconstructs class-level pseudo features for coordinated, class-balanced exemplar sampling across heterogeneous clients and (2) applies task-aware temperature scaling to mitigate class imbalance and overconfidence between replayed and new classes.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 9827,
      "output_tokens": 973,
      "predecessor_details": [
        {
          "success": true,
          "title": "iCaRL: Incremental Classifier and Representation Learning - arXiv",
          "url": "https://arxiv.org/abs/1611.07725",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:1611.07725** (cs)\n\n\\[Submitted on 23 Nov 2016 ( [v1](https://arxiv.org/abs/1611.07725v1)), last revised 14 Apr 2017 (this version, v2)\\]\n\n# Title:iCaRL: Incremental Classifier and Representation Learning\n\nAuthors: [Sylvestre-Alvise Rebuffi](https://arxiv.org/search/cs?searchtype=author&query=Rebuffi,+S), [Alexander Kolesnikov](https://arxiv.org/search/cs?searchtype=author&query=Kolesnikov,+A), [Georg Sperl](https://arxiv.org/search/cs?searchtype=author&query=Sperl,+G), [Christoph H. Lampert](https://arxiv.org/search/cs?searchtype=author&query=Lampert,+C+H)\n\nView a PDF of the paper titled iCaRL: Incremental Classifier and Representation Learning, by Sylvestre-Alvise Rebuffi and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/1611.07725)\n\n> Abstract:A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.\n\n| | |\n| --- | --- |\n| Comments: | Accepted paper at CVPR 2017 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1611.07725](https://arxiv.org/abs/1611.07725) \\[cs.CV\\] |\n| | (or [arXiv:1611.07725v2](https://arxiv.org/abs/1611.07725v2) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1611.07725](https://doi.org/10.48550/arXiv.1611.07725) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Christoph H. Lampert \\[ [view email](https://arxiv.org/show-email/c441c561/1611.07725)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1611.07725v1)**\nWed, 23 Nov 2016 10:24:11 UTC (1,155 KB)\n\n**\\[v2\\]**\nFri, 14 Apr 2017 16:41:02 UTC (1,165 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled iCaRL: Incremental Classifier and Representation Learning, by Sylvestre-Alvise Rebuffi and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/1611.07725)\n- [TeX Source](https://arxiv.org/src/1611.07725)\n- [Other Formats](https://arxiv.org/format/1611.07725)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1611.07725&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1611.07725&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2016-11](https://arxiv.org/list/cs.CV/2016-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1611.07725?context=cs)\n\n[cs.LG](https://arxiv.org/abs/1611.07725?context=cs.LG)\n\n[stat](https://arxiv.org/abs/1611.07725?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1611.07725?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1611.07725)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1611.07725)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1611.07725)\n\n### [1 blog link](https://arxiv.org/tb/1611.07725)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1611.html#RebuffiKL16) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/RebuffiKL16)\n\n[Sylvestre-Alvise Rebuffi](https://dblp.uni-trier.de/search/author?author=Sylvestre-Alvise%20Rebuffi)\n\n[Alexander Kolesnikov](https://dblp.uni-trier.de/search/author?author=Alexander%20Kolesnikov)\n\n[Christoph H. Lampert](https://dblp.uni-trier.de/search/author?author=Christoph%20H.%20Lampert)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1611.07725&description=iCaRL: Incremental Classifier and Representation Learning) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1611.07725&title=iCaRL: Incremental Classifier and Representation Learning)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1611.07725) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "icarl: Incremental classifier and representation learning",
          "cleaned_query": "icarl: Incremental classifier and representation learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Federated Class-Incremental Learning with Hierarchical Generative Prototypes",
          "url": "https://arxiv.org/html/2406.02447v4",
          "content": "# Federated Class-Incremental Learning with Hierarchical Generative Prototypes\n\nRiccardo Salami\u2217 \u00a0\u00a0 Pietro Buzzega\u2217 \u00a0\u00a0 Matteo Mosconi \u00a0\u00a0 Mattia Verasani \u00a0\u00a0 Simone Calderara\nAImageLab - University of Modena and Reggio Emilia, Modena, Italy\nname.surname@unimore.it\n\n###### Abstract\n\nFederated Learning (FL) aims at unburdening the training of deep models by distributing computation across multiple devices (clients) while safeguarding data privacy. On top of that, Federated Continual Learning (FCL) also accounts for data distribution evolving over time, mirroring the dynamic nature of real-world environments. While previous studies have identified Catastrophic Forgetting and Client Drift as primary causes of performance degradation in FCL, we shed light on the importance of Incremental Bias and Federated Bias, which cause models to prioritize classes that are recently introduced or locally predominant, respectively. Our proposal constrains both biases in the last layer by efficiently fine-tuning a pre-trained backbone using learnable prompts, resulting in clients that produce less biased representations and more biased classifiers. Therefore, instead of solely relying on parameter aggregation, we leverage generative prototypes to effectively balance the predictions of the global model. Our proposed methodology significantly improves the current State Of The Art across six datasets, each including three different scenarios.\n\n\\*\\*footnotetext: Equal contribution\n\n## 1 Introduction\n\nThe traditional paradigm in Deep Learning necessitates accessing large-scale datasets all at once, which hinders scalability and raises significant privacy concerns, especially when sensitive data is involved. Although distributing training across many devices could be an effective solution, there is still no effective mechanism for blending the resulting trained models into a single unified one. Federated Learning (FL)\u00a0[mcmahan2017communication](https://arxiv.org/html/2406.02447v4#bib.bib37)addresses this challenge through a centralized server that coordinates distributed devices, aiming to create a single unified model while minimizing communication costs.\n\nFederated Class-Incremental Learning (FCIL)\u00a0[yoon2021federated](https://arxiv.org/html/2406.02447v4#bib.bib52); [dong2022federated](https://arxiv.org/html/2406.02447v4#bib.bib7); [zhang2023target](https://arxiv.org/html/2406.02447v4#bib.bib56)takes a step further and couples distributed training with Online Learning, tolerating distribution shifts in the data over time. This presents new challenges, as deep models learning online (without relying on old examples) experience severe performance degradation due to Catastrophic Forgetting\u00a0[mccloskey1989catastrophic](https://arxiv.org/html/2406.02447v4#bib.bib35). In FCIL, the training process unfolds in tasks, each of which shifts the data distribution by introducing new categories. Each task is divided into communication rounds, wherein the local models train on their private data distribution. After local training, each client may transmit information to the orchestrator (server), which creates a global model and redistributes it to all clients. In the literature, some methodologies account for architectural heterogeneity ( _i.e_., heterogeneous FL\u00a0[diao2021heterofl](https://arxiv.org/html/2406.02447v4#bib.bib5); [kim2022depthfl](https://arxiv.org/html/2406.02447v4#bib.bib19); [ilhan2023scalefl](https://arxiv.org/html/2406.02447v4#bib.bib15)), while others aim to enhance the performance of local models without necessarily converging to a global one ( _i.e_., personalized FL\u00a0[collins2021exploiting](https://arxiv.org/html/2406.02447v4#bib.bib4); [ma2022layer](https://arxiv.org/html/2406.02447v4#bib.bib32); [oh2022fedbabu](https://arxiv.org/html/2406.02447v4#bib.bib38)). Instead, we follow the original FCIL setting as presented in\u00a0[dong2022federated](https://arxiv.org/html/2406.02447v4#bib.bib7), with the goal of training a single global model in a distributed way.\n\nWhen learning on a sequence of tasks, the model struggles the most at differentiating classes from distinct tasks, whereas it works well at separating those within the same one. Albeit one would intuitively link such behavior to Catastrophic Forgetting, it primarily occurs because tasks are learned separately, and some classes are never seen simultaneously\u00a0[kim2022theoretical](https://arxiv.org/html/2406.02447v4#bib.bib18). This results in a phenomenon known in the Incremental Learning literature as bias, which favors recently introduced classes\u00a0[wu2019large](https://arxiv.org/html/2406.02447v4#bib.bib51). We refer to this as Incremental Bias (IB). IB emerges because new classification heads are optimized independently, without concurrent access to previous classes. As a result, gradient updates are disproportionately influenced by the new classes, causing imbalance in the classifier\u2019s output.\n\nFigure 1: Federated bias. Histogram of the clients\u2019 responses computed on the global test set (\u03b2=0.05\ud835\udefd0.05\\\\beta=0.05italic\\_\u03b2 = 0.05). Class indexes have been rearranged for improved visualization (left). The entropy of the response histograms, averaged on all clients, compared with FL performance (right).\n\nThe authors of\u00a0[luo2021no](https://arxiv.org/html/2406.02447v4#bib.bib31)observe a similar tendency in the Federated Learning scenario: since clients train exclusively on their local datasets, they exhibit a bias towards their local label distribution. We refer to this effect as Federated Bias (FB). In contrast to the well-known Client Drift\u00a0[gao2022feddc](https://arxiv.org/html/2406.02447v4#bib.bib9); [karimireddy2020scaffold](https://arxiv.org/html/2406.02447v4#bib.bib17); [zhao2018federated](https://arxiv.org/html/2406.02447v4#bib.bib59), which causes misalignment between the clients\u2019 learned parameters, Federated Bias affects the clients\u2019 responses. Specifically, FB induces clients\u2019 outputs to diverge in different directions, mirroring the patterns of their local label distributions. Also, the strength of FB increases with growing heterogeneity, suggesting a correlation with declining performance (see [Section2](https://arxiv.org/html/2406.02447v4#S2)). To relieve such an effect, we constrain FB to the last layer by leveraging a frozen pre-trained backbone and efficiently fine-tuning it via prompt learning\u00a0[li2021prefix](https://arxiv.org/html/2406.02447v4#bib.bib26). Ideally, prompting keeps the clients\u2019 representations close to the pre-training optimum (hence, close to each other), thus minimizing their Federated Bias. In [Section4.3](https://arxiv.org/html/2406.02447v4#S4.SS3), we experimentally verify that prompting leads to reduced bias in the feature space w.r.t. fine-tuning all parameters. This confines the impact of FB to the last layer, providing the centralized server with less biased representations. On top of that, prompt-based methodologies i) have demonstrated SOTA results in Class-Incremental Learning\u00a0[wang2022learning](https://arxiv.org/html/2406.02447v4#bib.bib50); [wang2022dualprompt](https://arxiv.org/html/2406.02447v4#bib.bib49); [smith2023coda](https://arxiv.org/html/2406.02447v4#bib.bib44)and ii) adapt only a small portion of the clients\u2019 parameters, improving communication efficiency in distributed scenarios\u00a0[zhao2023fedprompt](https://arxiv.org/html/2406.02447v4#bib.bib57); [liu2023fedet](https://arxiv.org/html/2406.02447v4#bib.bib29).\n\nThe authors of\u00a0[wu2019large](https://arxiv.org/html/2406.02447v4#bib.bib51); [zhang2023slca](https://arxiv.org/html/2406.02447v4#bib.bib55); [luo2021no](https://arxiv.org/html/2406.02447v4#bib.bib31)address either IB or FB by fine-tuning the classification layer (where such biases are the most evident) on IID data samples. To meet the privacy requirements of FL, which prohibit transferring real data, we follow recent studies\u00a0[zhang2023slca](https://arxiv.org/html/2406.02447v4#bib.bib55); [luo2021no](https://arxiv.org/html/2406.02447v4#bib.bib31)and leverage latent generative replay. Specifically, at each communication round, we alleviate both biases \u2013 previously enforced to the last layer by the adopted prompt-based fine-tuning \u2013 by rebalancing the global classifier on a dataset of generated representations. In contrast to other approaches relying on prototypes ( _i.e_., the average feature vectors) to regularize clients\u2019 training procedures\u00a0[tan2022fedproto](https://arxiv.org/html/2406.02447v4#bib.bib45); [guo2024federated](https://arxiv.org/html/2406.02447v4#bib.bib10), we propose to compute their covariance matrix and parameterize a Multivariate Gaussian distribution for each class-client combination. This forms a grid of n\u2062u\u2062m\u2062\\_\u2062c\u2062l\u2062a\u2062s\u2062s\u2062e\u2062s\u00d7n\u2062u\u2062m\u2062\\_\u2062c\u2062l\u2062i\u2062e\u2062n\u2062t\u2062s\ud835\udc5b\ud835\udc62\ud835\udc5a\\_\ud835\udc50\ud835\udc59\ud835\udc4e\ud835\udc60\ud835\udc60\ud835\udc52\ud835\udc60\ud835\udc5b\ud835\udc62\ud835\udc5a\\_\ud835\udc50\ud835\udc59\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc60num\\\\\\_classes\\\\times num\\\\\\_clientsitalic\\_n italic\\_u italic\\_m \\_ italic\\_c italic\\_l italic\\_a italic\\_s italic\\_s italic\\_e italic\\_s \u00d7 italic\\_n italic\\_u italic\\_m \\_ italic\\_c italic\\_l italic\\_i italic\\_e italic\\_n italic\\_t italic\\_sgenerative prototypes, which are sampled hierarchically (first by class, then by client) to generate new data points.\n\nSummarizing, this work:\n\n- \u2022\n\n\nsheds light on the relation between prompt learning and the aforementioned biases, identifying the latter as the primary cause of performance degradation in FCIL;\n\n- \u2022\n\n\nproposes a novel methodology that confines (with prompting) and mitigates (by rebalancing) such biases in the final classification layer;\n\n- \u2022\n\n\nprovides a comprehensive evaluation of the proposed approach, demonstrating state-of-the-art performance on standard benchmarks while maintaining minimal communication costs.\n\n\n## 2 On Federated Bias\n\nThis section investigates how Federated Bias in clients\u2019 responses is associated with performance degradation in Federated Learning. All experiments are conducted on the CIFAR-100\u00a0[krizhevsky2009cifar](https://arxiv.org/html/2406.02447v4#bib.bib23)dataset, where the data is heterogeneously distributed across 10101010 clien",
          "original_query": "Federated class-incremental learning",
          "cleaned_query": "Federated class-incremental learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "FedProK: Trustworthy Federated Class-Incremental Learning",
          "url": "https://arxiv.org/abs/2405.02685",
          "content": "\n View PDF \n HTML (experimental) \nFederated Class-Incremental Learning (FCIL) focuses on continually transferring the previous knowledge to learn new classes in dynamic Federated Learning (FL). However, existing methods do not consider the trustworthiness of FCIL, i.e., improving continual utility, privacy, and efficiency simultaneously, which is greatly influenced by catastrophic forgetting and data heterogeneity among clients. To address this issue, we propose FedProK (Federated Prototypical Feature Knowledge Transfer), leveraging prototypical feature as a novel representation of knowledge to perform spatial-temporal knowledge transfer. Specifically, FedProK consists of two components: (1) feature translation procedure on the client side by temporal knowledge transfer from the learned classes and (2) prototypical knowledge fusion on the server side by spatial knowledge transfer among clients. Extensive experiments conducted in both synchronous and asynchronous settings demonstrate that our FedProK outperforms the other state-of-the-art methods in three perspectives of trustworthiness, validating its effectiveness in selectively transferring spatial-temporal knowledge.\n \n \n Submission history From: Xin Gao [ view email] [v1] \n Sat, 4 May 2024 14:57:09 UTC (3,661 KB) \n ||||I|||| Skip to main content\n We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\n > cs > arXiv:2405.02685\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:2405.02685 (cs)\n [Submitted on 4 May 2024]\n\n Title: FedProK: Trustworthy Federated Class-Incremental Learning via Prototypical Feature Knowledge Transfer\n\n Authors: Xin Gao, Xin Yang, Hao Yu, Yan Kang, Tianrui Li\n View a PDF of the paper titled FedProK: Trustworthy Federated Class-Incremental Learning via Prototypical Feature Knowledge Transfer, by Xin Gao and 4 other authors\n View PDF HTML (experimental)\n Abstract: Federated Class-Incremental Learning (FCIL) focuses on continually transferring the previous knowledge to learn new classes in dynamic Federated Learning (FL). However, existing methods do not consider the trustworthiness of FCIL, i.e., improving continual utility, privacy, and efficiency simultaneously, which is greatly influenced by catastrophic forgetting and data heterogeneity among clients. To address this issue, we propose FedProK (Federated Prototypical Feature Knowledge Transfer), leveraging prototypical feature as a novel representation of knowledge to perform spatial-temporal knowledge transfer. Specifically, FedProK consists of two components: (1) feature translation procedure on the client side by temporal knowledge transfer from the learned classes and (2) prototypical knowledge fusion on the server side by spatial knowledge transfer among clients. Extensive experiments conducted in both synchronous and asynchronous settings demonstrate that our FedProK outperforms the other state-of-the-art methods in three perspectives of trustworthiness, validating its effectiveness in selectively transferring spatial-temporal knowledge.\n Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)\n Cite as: arXiv:2405.02685 [cs.LG] \n (or arXiv:2405.02685v1 [cs.LG] for this version) \n \n\n Submission history\n\n From: Xin Gao [view email]\n [v1] Sat, 4 May 2024 14:57:09 UTC (3,661 KB)\n Full-text links:\n\n Access Paper:\n\n View a PDF of the paper titled FedProK: Trustworthy Federated Class-Incremental Learning via Prototypical Feature Knowledge Transfer, by Xin Gao and 4 other authors\n * View PDF\n * HTML (experimental)\n * TeX Source\n * Other Formats\n view license\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 2405\n Change to browse by:\n cs\n cs.AI\n cs.NE\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export BibTeX citation Loading...\n\n BibTeX formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n DagsHub Toggle\n DagsHub (What is DagsHub?)\n GotitPub Toggle\n Gotit.pub (What is GotitPub?)\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Spaces Toggle\n TXYZ.AI (What is TXYZ.AI?)\n Related Papers\n\n Recommenders and Search Tools\n\n Link to Influence Flower\n Influence Flower (What are Influence Flowers?)\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Fedprok: Trustworthy federated class-incremental learning via prototypical feature knowledge transfer",
          "cleaned_query": "Fedprok: Trustworthy federated class-incremental learning via prototypical feature knowledge transfer",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Federated Class Incremental Learning: A Pseudo Feature ...",
          "url": "https://openaccess.thecvf.com/content/ACCV2024/papers/Yoo_Federated_Class_Incremental_Learning_A_Pseudo_Feature_Based_Approach_Without_ACCV_2024_paper.pdf",
          "content": "Federated Class Incremental Learning: A Pseudo\nFeature Based Approach Without Exemplars\nMin Kyoon Yoo1 and Yu Rang Park1\nDepartment of Biomedical Systems Informatics, Yonsei University College of\nMedicine, Seoul, Republic of Korea\nminkyoon_da@yonsei.ac.kr, yurangpark@yuhs.ac\nAbstract. Federated learning often assumes that data is fixed in ad\u0002vance, which is unrealistic in many real-world scenarios where new data\ncontinuously arrives, causing catastrophic forgetting. To address this\nchallenge, we propose FCLPF (Federated Class Incremental Learning\nwith Pseudo Features), a method that uses pseudo features generated\nfrom prototypes to mitigate catastrophic forgetting. Our approach re\u0002duces communication costs and improves efficiency by eliminating the\nneed for past data and avoiding computationally heavy models like GANs.\nExperimental results on CIFAR-100 show that FCLPF achieves an aver\u0002age accuracy of 51.87% and an average forgetting of 9.62%, significantly\noutperforming existing baselines with an average accuracy of 47.72%\nand forgetting of 20.46%. On TinyImageNet, FCLPF achieves 37.56%\naccuracy and 3.14% forgetting, compared to the baselines\u2019 27.69% accu\u0002racy and 24.46% forgetting, demonstrating the superior performance of\nFCLPF.\nKeywords: Federated class incremental learning \u00b7 Continual learning \u00b7\nIncremental learning \u00b7 Catastrophic forgetting \u00b7 Federated learning\n1 Introduction\nFederated learning [9] is decentralized machine learning that does not require\ndata centralization . The absence of centralized data collection allows for the\npreservation of privacy. It has recently attracted considerable attention in both\nresearch and industry. It is employed in fields such as healthcare [18] and the\ndevelopment of local devices [8].\nIn conventional federated learning, it is assumed that data remains fixed over\ntime. This assumption is not valid in the real world. For example, in healthcare,\nA disease classification model may require incorporating new disease data over\ntime. Therefore, the model must be capable of learning new data without for\u0002getting previously learned information.\nCatastrophic forgetting [14] occurs when the addition of new data causes the\nmodel to forget previously learned information due to class imbalance. In the\ncentralized setting, this issue is studied under incremental learning [23]. Several\nmethods have been proposed to mitigate catastrophic forgetting [2, 7, 21, 28].\nThis ACCV 2024 paper, provided here by the Computer Vision Foundation, is the author-created version.\nThe content of this paper is identical to the content of the officially published ACCV 2024\nLNCS version of the paper as available on SpringerLink: https://link.springer.com/conference/accv\n488\n2 M. K. Yoo and Y. R. Park\nFederated class incremental learning (FCIL) [4] addresses the aforementioned\nchallenges by preventing catastrophic forgetting when learning new data in a\nfederated learning environment. A significant proportion of the existing proposed\nmethods make use of rehearsal memory [3,4,17]. Nevertheless, past data retention\ncan prove challenging for devices with limited storage [20], potentially raising\nconcerns about data privacy.\nTo address these issues, we propose FCLPF (Federated Class Incremental\nLearning with Pseudo Features), a novel approach that generates pseudo features\nwithout relying on rehearsal memory. The proposed FCLPF generates pseudo\nfeatures by utilizing only prototypes and data in the client. To create a pseudo\nfeature, only prototype types need to be exchanged between the client and server,\nreducing communication costs and preserving privacy. This method prevents\ncatastrophic forgetting and is efficient in federated learning environments. The\ncode is available on GitHub1\n.\n2 Related Work\nFederated Class Incremental Learning. The objectives of Federated Class\nIncremental Learning (FCIL) is to preserve previously learned knowledge while\nincorporating new data into the global model.\nThe studies GLFC [4] and LGA [3] employ a proxy server to mitigate catas\u0002trophic forgetting. However, having a proxy server raises possible privacy issues\nand increases communication costs. Furthermore, contrary to common federated\nlearning assumptions, both studies allow clients to share data. Moreover, re\u0002hearsal memory is necessary for both methods, which is impractical for devices\nwith little storage. Additionally, FedRCIL [17] employs self-supervised learning\n(SSL) and knowledged distillation to FCIL, yet it also leverages rehearsal mem\u0002ory.\nIn certain instances, past data is not accessible, and the retention of such data\nis challenging for devices with limited memory. Several methods of using GAN\n[5] to replace past data have been proposed. FedCIL [19] generates synthetic\ndata for global model consolidation by using locally trained generative models.\nMFCL [1] and TARGET [27] also demonstrate the feasibility of FCIL without the\nuse of rehearsal memory. However, MFCL and TARGET employ a centralized\nserver to train GAN, which incurs a significant computational cost and a high\ncommunication cost for sending and receiving models.\nExemplar-Free Class Incremental Learning Class incremental learning\n(CIL) algorithms can be employed effectively when new data is introduced.\nCatastrophic forgetting occurs when learning new data and is a significant chal\u0002lenge in machine learning. This is due to the lack of past data, a crucial factor\nin the learning process. Several CIL methods have recently been developed to\ncombat this problem [2, 7, 21, 28]. Numerous exemplar-based approaches have\n1\nhttps://github.com/DigitalHealthcareLab/24FCLPF.git\n489\nFCLPF 3\nbeen proposed. However, these methods present challenges in low-memory envi\u0002ronments and may raise privacy concerns.\nTo address these challenges, Exemplar-Free CIL (EFCIL) has been proposed\nas a potential solution [24, 26, 30]. These methods do not retain past data. A\nsignificant number of EFCIL methods employ regularization techniques to facil\u0002itate incremental updates to the deep model, and distillation is used to preserve\npast knowledge [13].\nSome techniques, such as PASS [30], employ prototype augmentation to pre\u0002vent catastrophic forgetting across incremental states; other techniques, such as\nABD [24], use picture inversion to produce pseudo-samples of previous classes.\nIt then uses linear head fine-tuning and importance-weighted feature distilla\u0002tion to distinguish between task features from the past and present without the\nrequirement for rehearsal memory.\nFor IL2A [29], the features are generated using the information from the\nclass distribution to create the features of previous classes. In FeTrIL [16], the\nfeature extractor is fixed, and pseudo-features are generated straightforwardly.\nOur method fixes the feature extractor like FeTrIL and generates pseudo features.\nBy fixing the feature extractor, only the fully connected (FC) layer needs to be\nexchanged, which significantly reduces the communication cost, making this an\nefficient and suitable method in federated class incremental learning. To the best\nof our knowledge, our study is the first to apply pseudo features to FCIL.\n3 Method\nWe propose FCLPF, a federated class incremental learning that uses pseudo\nfeatures to replace rehearsal memory. Previous studies are mostly with rehearsal\nmemory, and those without rehearsal memory create and use GANs, which leads\nto very high computational and communication costs. Therefore, we created\npseudo features to replace rehearsal memory. This study is the first to apply\npseudo features to FCIL. We propose a new way to create pseudo features using\nPrincipal Component Analysis (PCA) [25]. Our method is illustrated in Figure\n1.\nInitially, the client trains the feature extractor and the FC layer. At the end\nof the initial task, the server aggregates the class-specific prototypes from all\nclients, performing a sample-weighted prototype averaging based on the number\nof samples per class for each client. This ensures that the class distribution of\nall clients is reflected in the global model.\nFollowing the initial task, the feature extractor is frozen. The server sends\nonly the FC layers and global prototypes to the clients. The clients use the re\u0002ceived prototypes to generate pseudo features, replacing rehearsal memory. This\napproach reduces the parameters that need to be shared with the central server\nby sharing only FC layers and prototypes, significantly reducing communication\ncosts in a federated learning environment.\n490\n4 M. K. Yoo and Y. R. Park\nFig. 1: Overview of our method. ( P_n ) is the prototype of the nth class, and FC de\u0002notes the fully connected (FC) lyer. F(x_n) represents thefeature of\ndenotes the pseudo feature for the past class. Only prototypes and FC layers are ex\u0002changed between the client and server. The server aggregates the prototype and FC\nlayer.\nPrototype. We don\u2019t save the past data. We create a pseudo feature. To create\nthe pseudo feature, we used the prototype. A prototype was created using the\nfollowing formula:\n \\b\n,where f_c(\\bm {x}) denotes feature vector of sample x belongig to cl\nprototype for class c . We extracted the features for each class and averaged th\nWe sent the pseudo features created by each client in the first round of the task\nto the center and averaged them to create a single prototype. The prototype\ncreated by the center was sent to each client so that they could create pseudo\nfeatures within the client.\nClosest prototype. We compute the new class that is most similar to the old\nclass we want to create. The closest prototype was calculated using the formula\nas follows:\n C_\n, where _{\\text {new}} represents the set of ne clases, and \\\nthe prototype of the old class and the prototype of the new class, respectively.\n\\protect \\text {dist}(\\cdot , \\cdot ) denotes the distance metric. In this study, we employed the cosine sim\u0002ilarity as the distance metric. We then cal\nprototype \\bm {\\mu }_{c_{\\text {old}}} and thec\n491\nFCLPF 5\n \\D\nPCA Proj",
          "original_query": "Federated class incremental learning: A pseudo feature based approach without exemplars",
          "cleaned_query": "Federated class incremental learning: A pseudo feature based approach without exemplars",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Towards Efficient Replay in Federated Incremental Learning - arXiv",
          "url": "https://arxiv.org/abs/2403.05890",
          "content": "\n Download PDF \n HTML (experimental) \nIn Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we empirically show that Re-Fed achieves competitive performance compared to state-of-the-art methods.\n \n \n Submission history From: Yichen Li [ view email] [v1] \n Sat, 9 Mar 2024 12:04:56 UTC (1,838 KB) \n ||||I|||| Skip to main content\n We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\n > cs > arXiv:2403.05890\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:2403.05890 (cs)\n [Submitted on 9 Mar 2024]\n\n Title: Towards Efficient Replay in Federated Incremental Learning\n\n Authors: Yichen Li, Qunwei Li, Haozhao Wang, Ruixuan Li, Wenliang Zhong, Guannan Zhang\n Download a PDF of the paper titled Towards Efficient Replay in Federated Incremental Learning, by Yichen Li and 5 other authors\n Download PDF HTML (experimental)\n Abstract: In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we empirically show that Re-Fed achieves competitive performance compared to state-of-the-art methods.\n Subjects: Machine Learning (cs.LG) ; Distributed, Parallel, and Cluster Computing (cs.DC)\n Cite as: arXiv:2403.05890 [cs.LG] \n (or arXiv:2403.05890v1 [cs.LG] for this version) \n \n\n Submission history\n\n From: Yichen Li [view email]\n [v1] Sat, 9 Mar 2024 12:04:56 UTC (1,838 KB)\n Full-text links:\n\n Access Paper:\n\n Download a PDF of the paper titled Towards Efficient Replay in Federated Incremental Learning, by Yichen Li and 5 other authors\n * Download PDF\n * HTML (experimental)\n * TeX Source\n * Other Formats\n view license\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 2403\n Change to browse by:\n cs\n cs.DC\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export BibTeX citation Loading...\n\n BibTeX formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n DagsHub Toggle\n DagsHub (What is DagsHub?)\n GotitPub Toggle\n Gotit.pub (What is GotitPub?)\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Spaces Toggle\n TXYZ.AI (What is TXYZ.AI?)\n Related Papers\n\n Recommenders and Search Tools\n\n Link to Influence Flower\n Influence Flower (What are Influence Flowers?)\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Towards efficient replay in federated incremental learning",
          "cleaned_query": "Towards efficient replay in federated incremental learning",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Bias-Calibrated Prototype Aggregation for FCIL\nDevelop a server-side aggregation rule that explicitly estimates Incremental Bias (toward recent tasks) and Federated Bias (toward local label skew) from clients\u2019 logit histograms, then calibrates global predictions using class-conditional generative prototypes. Implement as a lightweight \u201cprototype temperature + prior correction\u201d layer trained on synthetic/pseudo features, and evaluate improvements in cross-task confusion and long-tail class recall.",
        "Prompt-Conditioned iCaRL for Federated Backbones\nExtend hierarchical generative prototypes\u2019 \u201cprompt-tuning of a frozen backbone\u201d idea to an iCaRL-style representation learning objective in FCIL: clients learn task/class prompts while a shared backbone is largely frozen. Use nearest-mean-of-exemplars (or prototype classifier) in the prompt-induced embedding space, and study whether prompt locality reduces client drift without sacrificing class separation.",
        "Asynchronous Prototype Fusion with Staleness-Aware Weighting\nBuild on FedProK\u2019s synchronous/asynchronous setting by introducing a principled staleness model for prototypical knowledge fusion: down-weight or transform prototypes from delayed clients using an estimated representation shift (measured via prototype drift across rounds). Implement a server-side Kalman-filter-like update for class prototypes and test under realistic heterogeneous participation and sporadic connectivity.",
        "Differentially Private Prototypical Knowledge Transfer with Utility Guarantees\nDesign a DP mechanism specifically for prototype/pseudo-feature sharing (e.g., per-class prototype clipping + Gaussian noise + secure aggregation) and quantify its impact on continual utility and forgetting. Provide an actionable recipe to choose noise levels based on prototype sensitivity and class frequency, then benchmark privacy\u2013accuracy trade-offs against parameter-DP baselines in FCIL.",
        "Prototype-Aware Replay Selection: Re-Fed Meets Class-Incremental Confusability\nImprove Re-Fed\u2019s \u201cglobal/local importance\u201d caching by defining importance as expected reduction in cross-task confusion measured in prototype space (e.g., samples near decision boundaries between old/new class prototypes). Each client selects replay samples that maximize margin preservation between historical prototypes and incoming-class prototypes, aiming to directly reduce incremental bias rather than only general forgetting.",
        "Hierarchical Prototypes for Open-World FCIL (New-Class Detection + Integration)\nExtend hierarchical generative prototypes to support open-world operation: clients detect out-of-distribution/new classes using distance-to-prototype statistics and submit \u201ccandidate prototypes\u201d to the server for consolidation. The server clusters candidate prototypes across clients, assigns new global class IDs, and then triggers an incremental learning step\u2014enabling realistic deployments where class sets are not pre-scheduled.",
        "Cross-Client Prototype Transport via Feature Translation Networks\nGeneralize FedProK\u2019s feature translation by learning small client-specific transport maps that align local prototype manifolds to a shared global manifold (e.g., affine or low-rank adapters trained with prototype matching losses). This targets heterogeneity-induced representation mismatch while keeping communication low (send only adapter parameters and prototypes), and can be evaluated via improved prototype separability across clients.",
        "Uncertainty-Weighted Generative Prototypes for Long-Tail and Non-IID FCIL\nAdd uncertainty estimates to prototypes (covariance in feature space or ensemble variance from prompt heads) and use them to weight both pseudo-feature generation (more diverse sampling for uncertain classes) and server fusion (down-weight uncertain/low-support prototypes). This directly addresses federated bias where dominant local classes otherwise overwhelm rare classes in aggregation.",
        "A Unified Benchmark Suite for Incremental Bias vs Federated Bias Decomposition\nCreate an evaluation protocol and metrics that disentangle catastrophic forgetting, incremental bias (task-recency), and federated bias (client label skew) using controlled FCIL scenarios (varying class order, Dirichlet label skew, and client participation). Implement diagnostic tools (per-task calibration error, cross-task confusion matrices, client-response entropy) to standardize claims and guide algorithm design across prototype-, replay-, and prompt-based methods."
      ],
      "generated_ideas_raw": "1. **Bias-Calibrated Prototype Aggregation for FCIL**\n   Develop a server-side aggregation rule that explicitly estimates Incremental Bias (toward recent tasks) and Federated Bias (toward local label skew) from clients\u2019 logit histograms, then calibrates global predictions using class-conditional generative prototypes. Implement as a lightweight \u201cprototype temperature + prior correction\u201d layer trained on synthetic/pseudo features, and evaluate improvements in cross-task confusion and long-tail class recall.\n\n2. **Dual-Memory FCIL: Tiny Exemplar Cache + Pseudo-Feature Generator**\n   Combine Re-Fed\u2019s importance-based sample caching with FCLPF-style prototype-to-pseudo-feature generation to create a hybrid replay buffer that uses a few real samples only for \u201cprototype anchoring.\u201d Concretely, store 1\u20135 exemplars per class (or per client-class) and generate most replay as pseudo features, optimizing the trade-off between privacy, memory, and forgetting.\n\n3. **Prompt-Conditioned iCaRL for Federated Backbones**\n   Extend hierarchical generative prototypes\u2019 \u201cprompt-tuning of a frozen backbone\u201d idea to an iCaRL-style representation learning objective in FCIL: clients learn task/class prompts while a shared backbone is largely frozen. Use nearest-mean-of-exemplars (or prototype classifier) in the prompt-induced embedding space, and study whether prompt locality reduces client drift without sacrificing class separation.\n\n4. **Asynchronous Prototype Fusion with Staleness-Aware Weighting**\n   Build on FedProK\u2019s synchronous/asynchronous setting by introducing a principled staleness model for prototypical knowledge fusion: down-weight or transform prototypes from delayed clients using an estimated representation shift (measured via prototype drift across rounds). Implement a server-side Kalman-filter-like update for class prototypes and test under realistic heterogeneous participation and sporadic connectivity.\n\n5. **Differentially Private Prototypical Knowledge Transfer with Utility Guarantees**\n   Design a DP mechanism specifically for prototype/pseudo-feature sharing (e.g., per-class prototype clipping + Gaussian noise + secure aggregation) and quantify its impact on continual utility and forgetting. Provide an actionable recipe to choose noise levels based on prototype sensitivity and class frequency, then benchmark privacy\u2013accuracy trade-offs against parameter-DP baselines in FCIL.\n\n6. **Prototype-Aware Replay Selection: Re-Fed Meets Class-Incremental Confusability**\n   Improve Re-Fed\u2019s \u201cglobal/local importance\u201d caching by defining importance as expected reduction in cross-task confusion measured in prototype space (e.g., samples near decision boundaries between old/new class prototypes). Each client selects replay samples that maximize margin preservation between historical prototypes and incoming-class prototypes, aiming to directly reduce incremental bias rather than only general forgetting.\n\n7. **Hierarchical Prototypes for Open-World FCIL (New-Class Detection + Integration)**\n   Extend hierarchical generative prototypes to support open-world operation: clients detect out-of-distribution/new classes using distance-to-prototype statistics and submit \u201ccandidate prototypes\u201d to the server for consolidation. The server clusters candidate prototypes across clients, assigns new global class IDs, and then triggers an incremental learning step\u2014enabling realistic deployments where class sets are not pre-scheduled.\n\n8. **Cross-Client Prototype Transport via Feature Translation Networks**\n   Generalize FedProK\u2019s feature translation by learning small client-specific transport maps that align local prototype manifolds to a shared global manifold (e.g., affine or low-rank adapters trained with prototype matching losses). This targets heterogeneity-induced representation mismatch while keeping communication low (send only adapter parameters and prototypes), and can be evaluated via improved prototype separability across clients.\n\n9. **Uncertainty-Weighted Generative Prototypes for Long-Tail and Non-IID FCIL**\n   Add uncertainty estimates to prototypes (covariance in feature space or ensemble variance from prompt heads) and use them to weight both pseudo-feature generation (more diverse sampling for uncertain classes) and server fusion (down-weight uncertain/low-support prototypes). This directly addresses federated bias where dominant local classes otherwise overwhelm rare classes in aggregation.\n\n10. **A Unified Benchmark Suite for Incremental Bias vs Federated Bias Decomposition**\n   Create an evaluation protocol and metrics that disentangle catastrophic forgetting, incremental bias (task-recency), and federated bias (client label skew) using controlled FCIL scenarios (varying class order, Dirichlet label skew, and client participation). Implement diagnostic tools (per-task calibration error, cross-task confusion matrices, client-response entropy) to standardize claims and guide algorithm design across prototype-, replay-, and prompt-based methods.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Bias-Calibrated Prototype Aggregation for FCIL\nDevelop a server-side aggregation rule that explicitly estimates Incremental Bias (toward recent tasks) and Federated Bias (toward local label skew) from",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Prompt-Conditioned iCaRL for Federated Backbones\nExtend hierarchical generative prototypes\u2019 \u201cprompt-tuning of a frozen backbone\u201d idea to an iCaRL-style representation learning objective in FCIL: clien",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Asynchronous Prototype Fusion with Staleness-Aware Weighting\nBuild on FedProK\u2019s synchronous/asynchronous setting by introducing a principled staleness model for prototypical knowledge fusion: down-wei",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Differentially Private Prototypical Knowledge Transfer with Utility Guarantees\nDesign a DP mechanism specifically for prototype/pseudo-feature sharing (e.g., per-class prototype clipping + Gaussian no",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Prototype-Aware Replay Selection: Re-Fed Meets Class-Incremental Confusability\nImprove Re-Fed\u2019s \u201cglobal/local importance\u201d caching by defining importance as expected reduction in cross-task confusion m",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Hierarchical Prototypes for Open-World FCIL (New-Class Detection + Integration)\nExtend hierarchical generative prototypes to support open-world operation: clients detect out-of-distribution/new classe",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Cross-Client Prototype Transport via Feature Translation Networks\nGeneralize FedProK\u2019s feature translation by learning small client-specific transport maps that align local prototype manifolds to a sh",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Uncertainty-Weighted Generative Prototypes for Long-Tail and Non-IID FCIL\nAdd uncertainty estimates to prototypes (covariance in feature space or ensemble variance from prompt heads) and use them to w",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "A Unified Benchmark Suite for Incremental Bias vs Federated Bias Decomposition\nCreate an evaluation protocol and metrics that disentangle catastrophic forgetting, incremental bias (task-recency), and ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 46,
      "paper_title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain",
      "contribution": "The authors show that convolutional recurrent encoders trained on realistic, temporally-structured whisker simulator data \u2014 using both supervised and tactile-specific contrastive self-supervision \u2014 produce internal representations that closely match neural activity in rodent somatosensory cortex, and that recurrence and task performance predict neural alignment.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 11395,
      "output_tokens": 991,
      "predecessor_details": [
        {
          "success": true,
          "title": "Using goal-driven deep learning models to understand ...",
          "url": "https://pubmed.ncbi.nlm.nih.gov/26906502/",
          "content": "Clipboard, Search History, and several other advanced features are temporarily unavailable.\n\n [Skip to main page content](https://pubmed.ncbi.nlm.nih.gov/26906502/#article-details)\n\n![Dot gov](https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg)\n\n**The .gov means it\u2019s official.**\n\nFederal government websites often end in .gov or .mil. Before\nsharing sensitive information, make sure you\u2019re on a federal\ngovernment site.\n\n![Https](https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg)\n\n**The site is secure.**\n\nThe **https://** ensures that you are connecting to the\nofficial website and that any information you provide is encrypted\nand transmitted securely.\n\n[Access keys](https://www.ncbi.nlm.nih.gov/guide/browsers/#ncbi_accesskeys) [NCBI Homepage](https://www.ncbi.nlm.nih.gov) [MyNCBI Homepage](https://pubmed.ncbi.nlm.nih.gov/myncbi/) [Main Content](https://pubmed.ncbi.nlm.nih.gov/26906502/#maincontent) [Main Navigation](https://pubmed.ncbi.nlm.nih.gov/26906502/)\n\n[![pubmed logo](https://cdn.ncbi.nlm.nih.gov/pubmed/de3675f9-e3b3-4656-8215-6fc84d4c88ab/core/images/pubmed-logo-blue.svg)](https://pubmed.ncbi.nlm.nih.gov/)\n\nSearch:\nSearch\n\n[Advanced](https://pubmed.ncbi.nlm.nih.gov/advanced/) [Clipboard](https://pubmed.ncbi.nlm.nih.gov/clipboard/)\n\n[User Guide](https://pubmed.ncbi.nlm.nih.gov/help/)\n\nSave\n\nEmail\n\nSend to\n\n- [Clipboard](https://pubmed.ncbi.nlm.nih.gov/26906502/)\n- [My Bibliography](https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpubmed.ncbi.nlm.nih.gov%2F26906502%2F%23open-bibliography-panel)\n- [Collections](https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpubmed.ncbi.nlm.nih.gov%2F26906502%2F%23open-collections-panel)\n- [Citation manager](https://pubmed.ncbi.nlm.nih.gov/26906502/)\n\nDisplay options\n\nDisplay options\n\nFormat\nAbstractPubMedPMID\n\n## Save citation to file\n\nFormat:\nSummary (text)PubMedPMIDAbstract (text)CSV\n\nCreate file\n\nCancel\n\n## Email citation\n\nSubject:\n1 selected item: 26906502 - PubMed\n\nTo:\n\nFrom:\n\nFormat:\nSummarySummary (text)AbstractAbstract (text)\n\nMeSH and other data\n\nSend email\n\nCancel\n\n### Add to Collections\n\n- Create a new collection\n- Add to an existing collection\n\nName your collection:\n\nName must be less than 100 characters\n\nChoose a collection:\n\nUnable to load your collection due to an error\n\n[Please try again](https://pubmed.ncbi.nlm.nih.gov/26906502/)\n\nAdd\n\nCancel\n\n### Add to My Bibliography\n\n- My Bibliography\n\nUnable to load your delegates due to an error\n\n[Please try again](https://pubmed.ncbi.nlm.nih.gov/26906502/)\n\nAdd\n\nCancel\n\n## Your saved search\n\nName of saved search:\n\nSearch terms:\n\n[Test search terms](https://pubmed.ncbi.nlm.nih.gov/26906502/)\n\nWould you like email updates of new search results?Saved Search Alert Radio Buttons\n\n- Yes\n- No\n\nEmail:\n( [change](https://www.ncbi.nlm.nih.gov/account/settings/))\n\nFrequency:\nMonthlyWeeklyDaily\n\nWhich day?\nThe first SundayThe first MondayThe first TuesdayThe first WednesdayThe first ThursdayThe first FridayThe first SaturdayThe first dayThe first weekday\n\nWhich day?\nSundayMondayTuesdayWednesdayThursdayFridaySaturday\n\nReport format:\nSummarySummary (text)AbstractAbstract (text)PubMed\n\nSend at most:\n1 item5 items10 items20 items50 items100 items200 items\n\nSend even when there aren't any new results\n\nOptional text in email:\n\nSave\n\nCancel\n\n## Create a file for external citation management software\n\nCreate file\n\nCancel\n\n## Your RSS Feed\n\nName of RSS Feed:\n\nNumber of items displayed:\n510152050100\n\nCreate RSS\n\nCancel\n\nRSS Link\nCopy\n\nFull text links\nCite\n\nDisplay options\n\nDisplay options\n\nFormat\nAbstractPubMedPMID\n\n## Abstract\n\nFueled by innovation in the computer vision and artificial intelligence communities, recent developments in computational neuroscience have used goal-driven hierarchical convolutional neural networks (HCNNs) to make strides in modeling neural single-unit and population responses in higher visual cortical areas. In this Perspective, we review the recent progress in a broader modeling context and describe some of the key technical innovations that have supported it. We then outline how the goal-driven HCNN approach can be used to delve even more deeply into understanding the development and organization of sensory cortical processing.\n\n[PubMed Disclaimer](https://pubmed.ncbi.nlm.nih.gov/disclaimer/)\n\n## Comment in\n\n- [Commentary: Using goal-driven deep learning models to understand sensory cortex.](https://pubmed.ncbi.nlm.nih.gov/29403369/)\n\nDong Q, Wang H, Hu Z.Dong Q, et al.Front Comput Neurosci. 2018 Jan 19;12:4. doi: 10.3389/fncom.2018.00004. eCollection 2018.Front Comput Neurosci. 2018.PMID: 29403369Free PMC article.No abstract available.\n\n\n## Similar articles\n\n- [Texture and art with deep neural networks.](https://pubmed.ncbi.nlm.nih.gov/28926765/)\n\nGatys LA, Ecker AS, Bethge M.Gatys LA, et al.Curr Opin Neurobiol. 2017 Oct;46:178-186. doi: 10.1016/j.conb.2017.08.019. Epub 2017 Sep 18.Curr Opin Neurobiol. 2017.PMID: 28926765Review.\n\n- [A spiking neuron model of the cortico-basal ganglia circuits for goal-directed and habitual action learning.](https://pubmed.ncbi.nlm.nih.gov/23266482/)\n\nChersi F, Mirolli M, Pezzulo G, Baldassarre G.Chersi F, et al.Neural Netw. 2013 May;41:212-24. doi: 10.1016/j.neunet.2012.11.009. Epub 2012 Dec 5.Neural Netw. 2013.PMID: 23266482\n\n- [Acquisition of nonlinear forward optics in generative models: two-stage \"downside-up\" learning for occluded vision.](https://pubmed.ncbi.nlm.nih.gov/21094592/)\n\nTajima S, Watanabe M.Tajima S, et al.Neural Netw. 2011 Mar;24(2):148-58. doi: 10.1016/j.neunet.2010.10.004. Epub 2010 Oct 27.Neural Netw. 2011.PMID: 21094592Review.\n\n- [Associative learning in early vision.](https://pubmed.ncbi.nlm.nih.gov/15288900/)\n\nTsodyks M, Adini Y, Sagi D.Tsodyks M, et al.Neural Netw. 2004 Jun-Jul;17(5-6):823-32. doi: 10.1016/j.neunet.2004.03.004.Neural Netw. 2004.PMID: 15288900\n\n- [Hierarchical organization of macaque and cat cortical sensory systems explored with a novel network processor.](https://pubmed.ncbi.nlm.nih.gov/10703045/)\n\nHilgetag CC, O'Neill MA, Young MP.Hilgetag CC, et al.Philos Trans R Soc Lond B Biol Sci. 2000 Jan 29;355(1393):71-89. doi: 10.1098/rstb.2000.0550.Philos Trans R Soc Lond B Biol Sci. 2000.PMID: 10703045Free PMC article.\n\n\nSee all similar articles\n\n## Cited by\n\n- [Visual looming is a primitive for human emotion.](https://pubmed.ncbi.nlm.nih.gov/38799577/)\n\nThieu MK, Ayzenberg V, Lourenco SF, Kragel PA.Thieu MK, et al.iScience. 2024 May 3;27(6):109886. doi: 10.1016/j.isci.2024.109886. eCollection 2024 Jun 21.iScience. 2024.PMID: 38799577Free PMC article.\n\n- [Diverse task-driven modeling of macaque V4 reveals functional specialization towards semantic tasks.](https://pubmed.ncbi.nlm.nih.gov/38781156/)\n\nCadena SA, Willeke KF, Restivo K, Denfield G, Sinz FH, Bethge M, Tolias AS, Ecker AS.Cadena SA, et al.PLoS Comput Biol. 2024 May 23;20(5):e1012056. doi: 10.1371/journal.pcbi.1012056. eCollection 2024 May.PLoS Comput Biol. 2024.PMID: 38781156Free PMC article.\n\n- [Mapping model units to visual neurons reveals population code for social behaviour.](https://pubmed.ncbi.nlm.nih.gov/38778103/)\n\nCowley BR, Calhoun AJ, Rangarajan N, Ireland E, Turner MH, Pillow JW, Murthy M.Cowley BR, et al.Nature. 2024 May;629(8014):1100-1108. doi: 10.1038/s41586-024-07451-8. Epub 2024 May 22.Nature. 2024.PMID: 38778103Free PMC article.\n\n- [Computational reconstruction of mental representations using human behavior.](https://pubmed.ncbi.nlm.nih.gov/38760341/)\n\nCaplette L, Turk-Browne NB.Caplette L, et al.Nat Commun. 2024 May 17;15(1):4183. doi: 10.1038/s41467-024-48114-6.Nat Commun. 2024.PMID: 38760341Free PMC article.\n\n- [Brain2GAN: Feature-disentangled neural encoding and decoding of visual perception in the primate brain.](https://pubmed.ncbi.nlm.nih.gov/38709818/)\n\nDado T, Papale P, Lozano A, Le L, Wang F, van Gerven M, Roelfsema P, G\u00fc\u00e7l\u00fct\u00fcrk Y, G\u00fc\u00e7l\u00fc U.Dado T, et al.PLoS Comput Biol. 2024 May 6;20(5):e1012058. doi: 10.1371/journal.pcbi.1012058. eCollection 2024 May.PLoS Comput Biol. 2024.PMID: 38709818Free PMC article.\n\n\nSee all \"Cited by\" articles\n\n## References\n\n1. Proc Natl Acad Sci U S A. 2014 Jun 10;111(23):8619-24\n -\n[PubMed](https://pubmed.ncbi.nlm.nih.gov/24812127/)\n1. J Neurosci. 2015 Sep 30;35(39):13402-18\n -\n[PubMed](https://pubmed.ncbi.nlm.nih.gov/26424887/)\n1. Nat Rev Neurosci. 2011 Nov 23;13(1):51-62\n -\n[PubMed](https://pubmed.ncbi.nlm.nih.gov/22108672/)\n1. Cereb Cortex. 1991 Jan-Feb;1(1):1-47\n -\n[PubMed](https://pubmed.ncbi.nlm.nih.gov/1822724/)\n1. Annu Rev Neurosci. 1995;18:555-86\n -\n[PubMed](https://pubmed.ncbi.nlm.nih.gov/7605074/)\n\nShow all 48 references\n\n## Publication types\n\n- Review\n\n\n\nActions\n\n\n\n\n\n- [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Review%22%5Bpt%5D&sort=date&sort_order=desc)\n- [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Review)\n- [Add to Search](https://pubmed.ncbi.nlm.nih.gov/26906502/)\n\n## MeSH terms\n\n- Animals\n\n\n\nActions\n\n\n\n\n\n- [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Animals%22%5BMeSH%5D&sort=date&sort_order=desc)\n- [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Animals)\n- [Add to Search](https://pubmed.ncbi.nlm.nih.gov/26906502/)\n\n- Goals\\*\n\n\n\nActions\n\n\n\n\n\n- [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Goals%22%5BMAJR%5D&sort=date&sort_order=desc)\n- [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Goals)\n- [Add to Search](https://pubmed.ncbi.nlm.nih.gov/26906502/)\n\n- Humans\n\n\n\nActions\n\n\n\n\n\n- [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Humans%22%5BMeSH%5D&sort=date&sort_order=desc)\n- [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Humans)\n- [Add to Search](https://pubmed.ncbi.nlm.nih.gov/26906502/)\n\n- Learning / physiology\\*\n\n\n\nActions\n\n\n\n\n\n- [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Learning%2Fphysiology%22%5BMAJR%5D&sort=date&sort_order=desc)\n- [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Learning)\n- [Add to Search](https://pubmed.ncbi.nlm.nih.gov",
          "original_query": "Using goal-driven deep learning models to understand sensory cortex",
          "cleaned_query": "Using goal-driven deep learning models to understand sensory cortex",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Deep spatio-temporal dependent convolutional LSTM network for traffic flow prediction",
          "url": "https://www.nature.com/articles/s41598-025-95711-6",
          "content": "Deep spatio-temporal dependent convolutional LSTM network for traffic flow prediction\n\n[Download PDF](https://www.nature.com/articles/s41598-025-95711-6.pdf)\n\n[Download PDF](https://www.nature.com/articles/s41598-025-95711-6.pdf)\n\n### Subjects\n\n- [Computer science](https://www.nature.com/subjects/computer-science)\n- [Mathematics and computing](https://www.nature.com/subjects/mathematics-and-computing)\n\n## Abstract\n\nWith the rapid development of economy, the concept of intelligent transportation system (ITS) and smart city has been mentioned. The most important part of building them is whether they can accurately predict traffic flow. An accurate traffic flow forecast can help manage traffic, plan travel paths in advance, and rationally allocate public resources such as shared bicycles. The biggest difficulty in this task is how to solve the problem of spatial imbalance and the problem of temporal imbalance. In this paper, we propose a deep learning algorithm STDConvLSTM. Firstly, for spatial features, most scholars use convolutional neural networks (with fixed kernel size) to capture. However, this does not solve the problem of spatial imbalance, i.e. each region has a different size of correlated regions (e.g., the busy area has a wider range of correlated regions). In this paper, we design a space-dependent attention mechanism, which assigns a convolutional neural network with a different kernel size to each region through attention weights. Secondly, for time features, most scholars use time series prediction models, such as recurrent neural networks and their variants. However, in the actual forecasting process, the importance of historical data in different time steps is not the same. In this paper, we design a time-dependent attention mechanism that assigns different weights to historical data to solve the time imbalance. In the end, we ran experiments on two real-world data sets and achieve good performance.\n\n### Similar content being viewed by others\n\n### [Spatial\u2013temporal combination and multi-head flow-attention network for traffic flow prediction](https://www.nature.com/articles/s41598-024-60337-7?fromPaywallRec=false)\n\nArticleOpen access26 April 2024\n\n### [Graph convolution networks based on adaptive spatiotemporal attention for traffic flow forecasting](https://www.nature.com/articles/s41598-025-88706-w?fromPaywallRec=false)\n\nArticleOpen access15 March 2025\n\n### [Spatial linear transformer and temporal convolution network for traffic flow prediction](https://www.nature.com/articles/s41598-024-54114-9?fromPaywallRec=false)\n\nArticleOpen access19 February 2024\n\n## Introduction\n\nIn recent years, with the rapid development of the economy, people\u2019s demand for the construction of smart cities and intelligent transportation systems has become more and more intense. The most important point is to solve the problem of traffic flow prediction. By mining and analyzing traffic flow data, we can find out some potential traffic patterns and predict the future traffic flow. In simple terms, traffic flow forecasting predicts the future traffic flow based on the historical data of the city. It can provide reference value for mass travel and avoid congested areas. Moreover, it can provide a basis for traffic management and prevention by letting traffic managers know the traffic situation in advance.\n\nHowever, since the problem of traffic flow prediction has become a hot topic, the biggest difficulties include two aspects, spatial dependence and time dependence. (i) Spatial dependence. We know that the crowd flow prediction of a certain area in a city will be affected by the surrounding area. Therefore, most scholars use convolutional neural network (CNN) to obtain spatial features. However, they ignore the problem that the extent to which each area is affected by the surrounding area is not fixed. For example, some shopping malls can be influenced by a larger area around them. (ii) Time dependence. Traffic flow data has the characteristics of closeness, period and trend. Closeness means that the closer the data is to the predicted data, the greater the impact. Period refers to the hidden patterns of traffic data for each day. Trend refer to the hidden patterns of traffic data for each week. These characteristics indicate that there are some data in the historical data that have a greater impact on the predicted results.\n\nEarly methods can be roughly divided into three categories. In the first category, prediction models were established based on spatial correlation. For example, Zhang et al.[1](https://www.nature.com/articles/s41598-025-95711-6#ref-CR1) and Thaduri et al.[2](https://www.nature.com/articles/s41598-025-95711-6#ref-CR2) used CNN for traffic flow prediction. In the second category, a prediction model is established based on time correlation. For example, Yang et al.[3](https://www.nature.com/articles/s41598-025-95711-6#ref-CR3) and Tian et al.[4](https://www.nature.com/articles/s41598-025-95711-6#ref-CR4) used LSTM for flow prediction. The third type is to build prediction models by combining temporal correlation and spatial correlation. For example, Narmadha et al.[5](https://www.nature.com/articles/s41598-025-95711-6#ref-CR5) and Zhao et al.[6](https://www.nature.com/articles/s41598-025-95711-6#ref-CR6) use convolutional neural network and long short-term memory network to model spatial and temporal features respectively. However, the first and second types of methods only consider spatial or temporal dependencies. Although the third kind of method considers the time dependence and space dependence, it does not consider the problem of temporal imbalance and spatial imbalance.\n\nIn this paper, we propose a spatio-temporal dependent attention convolutional LSTM network for traffic flow prediction, which uses the time-dependent attention mechanism and the space-dependent attention mechanism to solve the problem of temporal imbalance and spatial imbalance respectively. The main contributions of this paper are as follows:\n\n- We design a selective kernel attention method called the space-dependent attention mechanism to address the spatial imbalance problem. Firstly, we feed the data into convolutional neural network of different kernel sizes. Secondly, we fuse all the output results. Finally, a multi-layer fully connected network is used to obtain multiple sets of attention vectors. Then, the output of the space-dependent attention mechanism is obtained by weighted summation of the attention vector and the output of the convolutional neural network.\n\n\n- We designed a time-dependent attention mechanism to solve the problem of temporal imbalance. Specifically, firstly, the data is fed into the spatio-temporal feature extraction module. Then, each historical data is given a different weight by using a time-dependent attention mechanism on the previous time step of the predicted data and the historical data.\n\n\n- The STDConvLSTM model was evaluated on two real-world data sets, and the experimental results show that the algorithm has competitive performance compared with the baseline algorithm.\n\n\nThe overall structure of the subsequent paper is as follows. Section\u00a0\u201c [Related works](https://www.nature.com/articles/s41598-025-95711-6#Sec2)\u201d is related work. Section\u00a0\u201c [STDConvLSTM framework](https://www.nature.com/articles/s41598-025-95711-6#Sec8)\u201d is the STDConvLSTM model. The experimental process is described in section\u00a0\u201c [Experiments](https://www.nature.com/articles/s41598-025-95711-6#Sec12)\u201d. Finally, the conclusion is presented in section\u00a0\u201c [Conclusion](https://www.nature.com/articles/s41598-025-95711-6#Sec20)\u201d.\n\n## Related works\n\nThe research of traffic flow prediction problem goes through the following processes. In the first stage, most scholars use time series prediction models to model traffic data, such as HA algorithm[7](https://www.nature.com/articles/s41598-025-95711-6#ref-CR7), ARMA algorithm[8](https://www.nature.com/articles/s41598-025-95711-6#ref-CR8), ARIMA algorithm and its variants[9](https://www.nature.com/www.nature.com#ref-CR9), [10](https://www.nature.com/www.nature.com#ref-CR10), [11](https://www.nature.com/www.nature.com#ref-CR11), [12](https://www.nature.com/articles/s41598-025-95711-6#ref-CR12), VAR algorithm[13](https://www.nature.com/articles/s41598-025-95711-6#ref-CR13), etc. These algorithms cannot obtain nonlinear characteristics in traffic data. Kumar et al.[14](https://www.nature.com/articles/s41598-025-95711-6#ref-CR14) designed a seasonal ARIMA algorithm for traffic prediction. Shahriari et al.[10](https://www.nature.com/articles/s41598-025-95711-6#ref-CR10) proposed a joint model (E-ARIMA) for traffic prediction, and the results show that the prediction results are improved.\n\nIn the second stage, with the popularity of machine learning algorithms, SVM algorithm[15](https://www.nature.com/articles/s41598-025-95711-6#ref-CR15), [16](https://www.nature.com/articles/s41598-025-95711-6#ref-CR16), Bayesian algorithm[17](https://www.nature.com/articles/s41598-025-95711-6#ref-CR17), KNN algorithm[18](https://www.nature.com/articles/s41598-025-95711-6#ref-CR18), SVR algorithm[19](https://www.nature.com/articles/s41598-025-95711-6#ref-CR19), XGBoost algorithm[20](https://www.nature.com/articles/s41598-025-95711-6#ref-CR20) and other algorithms have been applied to the field of traffic prediction by many scholars. Compared with the first stage, the prediction accuracy is improved, but due to the limit of model capacity, only shallow features of traffic data can be captured. Luo et al.[21](https://www.nature.com/articles/s41598-025-95711-6#ref-CR21) proposed a traffic prediction method combining KNN and LSTM network. Feng et al.[22](https://www.nature.com/articles/s41598-025-95711-6#ref-CR22) proposed a multi-kernel SVM model based on spatio-temporal dependence for traffic prediction. In addition, some scholars have tried to model by combining time series prediction methods and machine le",
          "original_query": "Convolutional LSTM network: A machine learning approach for spatio-temporal data",
          "cleaned_query": "Convolutional LSTM network: A machine learning approach for spatio-temporal data",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Representation Learning with Contrastive Predictive Coding",
          "url": "https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205",
          "content": "[Skip to search form](https://www.semanticscholar.org/www.semanticscholar.org#search-form) [Skip to main content](https://www.semanticscholar.org/www.semanticscholar.org#main-content) [Skip to account menu](https://www.semanticscholar.org/www.semanticscholar.org#account-menu)\n\n- Corpus ID: 49670925\n\n# Representation Learning with Contrastive Predictive Coding\n\n```\n@article{Oord2018RepresentationLW,\n title={Representation Learning with Contrastive Predictive Coding},\n author={A{\\\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},\n journal={ArXiv},\n year={2018},\n volume={abs/1807.03748},\n url={https://api.semanticscholar.org/CorpusID:49670925}\n}\n```\n\n- [A\u00e4ron van den Oord](https://www.semanticscholar.org/author/A%C3%A4ron-van-den-Oord/3422336), [Yazhe Li](https://www.semanticscholar.org/author/Yazhe-Li/2144417088), [O. Vinyals](https://www.semanticscholar.org/author/O.-Vinyals/1689108)\n- Published in [arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)10 July 2018\n- Computer Science, Mathematics\n\nTLDR\n\nThis work proposes a universal unsupervised learning approach to extract useful representations from high-dimensional data, which it calls Contrastive Predictive Coding, and demonstrates that the approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.Expand\n\n\\[PDF\\] Semantic Reader\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n10,782 Citations\n\n[Highly Influential Citations](https://www.semanticscholar.org/www.semanticscholar.org#citing-papers)\n\n1,350\n\n[Background Citations](https://www.semanticscholar.org/www.semanticscholar.org#citing-papers)\n\n4,105\n\n[Methods Citations](https://www.semanticscholar.org/www.semanticscholar.org#citing-papers)\n\n4,170\n\n[Results Citations](https://www.semanticscholar.org/www.semanticscholar.org#citing-papers)\n\n97\n\n[View All](https://www.semanticscholar.org/www.semanticscholar.org#citing-papers)\n\n## Figures and Tables from this paper\n\n- [figure 1](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205/figure/0)\n- [figure 2](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205/figure/1)\n- [table 2](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205/figure/2)\n- [figure 3](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205/figure/3)\n- [table 3](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205/figure/4)\n- [figure 4](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205/figure/5)\n- [table 4](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205/figure/6)\n- [figure 5](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205/figure/7)\n- [table 5](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205/figure/8)\n- [figure 6](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205/figure/9)\n\nView All 10 Figures & Tables\n\n## Topics\n\nAI-Generated\n\n[Contrastive Predictive Coding (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/15568599102?corpusId=49670925) [Probabilistic Contrastive Loss (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/46769459779?corpusId=49670925) [CPC Models (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/37524218988?corpusId=49670925) [Information Noise Contrastive Estimation (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/64628414620?corpusId=49670925) [Representation Learning (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/68048676001?corpusId=49670925) [Contrastive Loss (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/39867589880?corpusId=49670925) [Noise-contrastive Estimation (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/19972362814?corpusId=49670925) [Phone Classification (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/25444281847?corpusId=49670925) [Density Ratio (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/53881763085?corpusId=49670925) [Mutual Information (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/3418020660?corpusId=49670925)\n\n## 10,782 Citations\n\nCitation Type\n\nHas PDF\n\nAuthor\n\nMore Filters\n\nMore Filters\n\nFilters\n\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n[**Hybrid Generative-Contrastive Representation Learning**](https://www.semanticscholar.org/paper/Hybrid-Generative-Contrastive-Representation-Kim-Kim/63f2644515fe67fbe66df1100091099114e62f8f)\n\n[Saehoon Kim](https://www.semanticscholar.org/author/Saehoon-Kim/1898376)[Sungwoong Kim](https://www.semanticscholar.org/author/Sungwoong-Kim/2155640909)[Juho Lee](https://www.semanticscholar.org/author/Juho-Lee/2108550899)\n\nComputer Science\n\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n\n- 2021\n\nTLDR\n\nIt is demonstrated that a transformer-based encoder-decoder architecture trained with both contrastive and generative losses can learn highly discriminative and robust representations without hurting the generative performance.Expand\n\n- [11](https://www.semanticscholar.org/paper/63f2644515fe67fbe66df1100091099114e62f8f#citing-papers)\\[PDF\\]\n\n- 3 Excerpts\n\nSave\n\n[**Unsupervised State Representation Learning in Atari**](https://www.semanticscholar.org/paper/Unsupervised-State-Representation-Learning-in-Atari-Anand-Racah/2e73ba5c098e5207aaecf159b6e6620b3f2ea56e)\n\n[Ankesh Anand](https://www.semanticscholar.org/author/Ankesh-Anand/12679121)[Evan Racah](https://www.semanticscholar.org/author/Evan-Racah/3159503)[Sherjil Ozair](https://www.semanticscholar.org/author/Sherjil-Ozair/1955694)[Yoshua Bengio](https://www.semanticscholar.org/author/Yoshua-Bengio/1751762)[Marc-Alexandre C\u00f4t\u00e9](https://www.semanticscholar.org/author/Marc-Alexandre-C%C3%B4t%C3%A9/40638665)[R. Devon Hjelm](https://www.semanticscholar.org/author/R.-Devon-Hjelm/40482726)\n\nComputer Science\n\n[NeurIPS](https://www.semanticscholar.org/venue?name=NeurIPS)\n\n- 2019\n\nTLDR\n\nA method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations is introduced, and a new benchmark based on Atari 2600 games is introduced.Expand\n\n- [259](https://www.semanticscholar.org/paper/2e73ba5c098e5207aaecf159b6e6620b3f2ea56e#citing-papers)\n- [Highly Influenced](https://www.semanticscholar.org/paper/2e73ba5c098e5207aaecf159b6e6620b3f2ea56e?sort=is-influential#citing-papers)\n\\[PDF\\]\n\n- 10 Excerpts\n\nSave\n\n[**On the Importance of Feature Decorrelation for Unsupervised Representation Learning in Reinforcement Learning**](https://www.semanticscholar.org/paper/On-the-Importance-of-Feature-Decorrelation-for-in-Lee-Lee/1c73d712dd614d2eebf06f55229348f8e8b46b47)\n\n[Hojoon Lee](https://www.semanticscholar.org/author/Hojoon-Lee/2163406260)[Ko-tik Lee](https://www.semanticscholar.org/author/Ko-tik-Lee/2110019663)[Dongyoon Hwang](https://www.semanticscholar.org/author/Dongyoon-Hwang/2140537743)[Hyunho Lee](https://www.semanticscholar.org/author/Hyunho-Lee/2162205515)[ByungKun Lee](https://www.semanticscholar.org/author/ByungKun-Lee/7613239)[J. Choo](https://www.semanticscholar.org/author/J.-Choo/1795455)\n\nComputer Science\n\n[ICML](https://www.semanticscholar.org/venue?name=ICML)\n\n- 2023\n\nTLDR\n\nThis work proposes a novel URL framework that causally predicts future states while increasing the dimension of the latent manifold by decorrelating the features in the latent space and demonstrates that this framework effectively learns predictive representations without collapse.Expand\n\n- [7](https://www.semanticscholar.org/paper/1c73d712dd614d2eebf06f55229348f8e8b46b47#citing-papers)\n- [Highly Influenced](https://www.semanticscholar.org/paper/1c73d712dd614d2eebf06f55229348f8e8b46b47?sort=is-influential#citing-papers)\n\\[PDF\\]\n\n- 8 Excerpts\n\nSave\n\n[**Improving VAE-based Representation Learning**](https://www.semanticscholar.org/paper/Improving-VAE-based-Representation-Learning-Zhang-Xiao/06ac4906df932e34d279beed896b97b091822093)\n\n[Mingtian Zhang](https://www.semanticscholar.org/author/Mingtian-Zhang/2108795448)[Tim Z. Xiao](https://www.semanticscholar.org/author/Tim-Z.-Xiao/96397443)[Brooks Paige](https://www.semanticscholar.org/author/Brooks-Paige/2885717)[D. Barber](https://www.semanticscholar.org/author/D.-Barber/2056214907)\n\nComputer Science\n\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n\n- 2022\n\nTLDR\n\nIt is shown that by using a decoder that prefers to learn local features, the remaining global features can be well captured by the latent, which significantly improves performance of a downstream classification task.Expand\n\n- [10](https://www.semanticscholar.org/paper/06ac4906df932e34d279beed896b97b091822093#citing-papers)\\[PDF\\]\n\n- 1 Excerpt\n\nSave\n\n[**Function Contrastive Learning of Transferable Representations**](https://www.semanticscholar.org/paper/Function-Contrastive-Learning-of-Transferable-Gondal-Joshi/73e9ead9fd6911cb8fdce90f6613a3abda7525e0)\n\n[Muhammad Waleed Gondal](https://www.semanticscholar.org/author/Muhammad-Waleed-Gondal/51214165)[S. Joshi](https://www.semanticscholar.org/author/S.-Joshi/2114034081)[Nasim Ra",
          "original_query": "Representation Learning with Contrastive Predictive Coding",
          "cleaned_query": "Representation Learning with Contrastive Predictive Coding",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "A dynamical model for generating synthetic data to quantify active ...",
          "url": "https://www.pnas.org/doi/abs/10.1073/pnas.2011905118",
          "content": "- Contents\n- - [Information & Authors](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-info)\n- [Metrics & Citations](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-metrics)\n- [View Options](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-fulltext-options)\n- [References](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-references)\n- [Media](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-media)\n- [Share](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-share)\n\n## Significance\n\nIn recent years, increasing effort has been made to study sensorimotor integration within the context of the animal\u2019s body and the environment. However, it is challenging to collect neurophysiological data under naturalistic conditions, and simulations have become an important part of neuroscience research. We developed a simulation framework, _WHISKiT Physics_, to model the dynamics of a complete sensorimotor system\u2014the rodent vibrissal array\u2014operating under ethologically relevant conditions. The tight link between sensing and motor control and the discrete somatotopy throughout the sensory pathway make the vibrissal array ideal to study sensorimotor circuits. _WHISKiT Physics_ makes this widely used model system accessible to sensory research as well as to other disciplines including information theory, reinforcement learning, and artificial intelligence.\n\n## Abstract\n\nAs it becomes possible to simulate increasingly complex neural networks, it becomes correspondingly important to model the sensory information that animals actively acquire: the biomechanics of sensory acquisition directly determines the sensory input and therefore neural processing. Here, we exploit the tractable mechanics of the well-studied rodent vibrissal (\u201cwhisker\u201d) system to present a model that can simulate the signals acquired by a full sensor array actively sampling the environment. Rodents actively \u201cwhisk\u201d \u223c60 vibrissae (whiskers) to obtain tactile information, and this system is therefore ideal to study closed-loop sensorimotor processing. The simulation framework presented here, _WHISKiT Physics_, incorporates realistic morphology of the rat whisker array to predict the time-varying mechanical signals generated at each whisker base during sensory acquisition. Single-whisker dynamics were optimized based on experimental data and then validated against free tip oscillations and dynamic responses to collisions. The model is then extrapolated to include all whiskers in the array, incorporating each whisker\u2019s individual geometry. Simulation examples in laboratory and natural environments demonstrate that _WHISKiT Physics_ can predict input signals during various behaviors, currently impossible in the biological animal. In one exemplary use of the model, the results suggest that active whisking increases in-plane whisker bending compared to passive stimulation and that principal component analysis can reveal the relative contributions of whisker identity and mechanics at each whisker base to the vibrissotactile response. These results highlight how interactions between array morphology and individual whisker geometry and dynamics shape the signals that the brain must process.\n\n## Continue Reading\n\n[VIEW PDF](https://www.pnas.org/doi/reader/10.1073/pnas.2011905118) [FULL TEXT](https://www.pnas.org/doi/full/10.1073/pnas.2011905118)\n\n## Data Availability\n\nAll study data are included in the article and/or supporting information.\n\n## Acknowledgments\n\nWe thank Dr. Sara A. Solla (Northwestern University) for comments and insightful discussions that greatly improved the final version of the manuscript.\n\n## Supporting Information\n\nAppendix (PDF)\n\n- [Download](https://www.pnas.org/doi/suppl/10.1073/pnas.2011905118/suppl_file/pnas.2011905118.sapp.pdf)\n- 970.50 KB\n\nDataset\\_S01 (CSV)\n\n- [Download](https://www.pnas.org/doi/suppl/10.1073/pnas.2011905118/suppl_file/pnas.2011905118.sd01.csv)\n- 1.96 KB\n\n[iframe](https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiJhYmFjOTA0MDg1MTI2OWFlNjZiN2Q1MjMyMDI5Y2IzNCIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.xVELhuc0uYshjEqzh6s_kauQ8_anKXlEWaUPFLJ98gsrCXF7UBpxk5qfT3YdDScp3D-zQ1panpvUJMO54ceXrlZww-1D1hi6Y82m8vvVMvdx96qUp-HrvMqu97qF4rkzMz4_8LcsK2Tjv-f8ecAuHZGBPrHY7o_jX870WD3gF8TjId3Yew6rrx-Fp6LP71AXaYbPuX1Yfy_LLkgIzOpJIkPV_o_2PR-vAs30xhPIShFJVLHYbW4SHFbJBB3KNONwoFcewR3QgnmGkaIspd8ir7ZCaocDivnQ9MijT6rWokHtq4wKye4ju3hPdag6WiDw_88WLXtcYgKbTdaivqPSkg?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiJhYmFjOTA0MDg1MTI2OWFlNjZiN2Q1MjMyMDI5Y2IzNCIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.xVELhuc0uYshjEqzh6s_kauQ8_anKXlEWaUPFLJ98gsrCXF7UBpxk5qfT3YdDScp3D-zQ1panpvUJMO54ceXrlZww-1D1hi6Y82m8vvVMvdx96qUp-HrvMqu97qF4rkzMz4_8LcsK2Tjv-f8ecAuHZGBPrHY7o_jX870WD3gF8TjId3Yew6rrx-Fp6LP71AXaYbPuX1Yfy_LLkgIzOpJIkPV_o_2PR-vAs30xhPIShFJVLHYbW4SHFbJBB3KNONwoFcewR3QgnmGkaIspd8ir7ZCaocDivnQ9MijT6rWokHtq4wKye4ju3hPdag6WiDw_88WLXtcYgKbTdaivqPSkg%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D4.0635s)\n\nMovie S1.\n\n**Visualization of the passive stimulation experiment (Scenario 1).** A vertical peg was simulated to move from rostral to caudal through the middle of the immobile right array. See SI Appendix for the full caption.\n\n- [Download](https://www.pnas.org/doi/suppl/10.1073/pnas.2011905118/suppl_file/pnas.2011905118.sm01.mp4)\n- 2.20 MB\n\n[iframe](https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI3YjdlMzc4M2NkYWQyNjE0Njc1M2JlNjEzMTIwODlkZCIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.XG_xOHHz3AacyMcF_iMkQHXow0Q76AsvNCipkw2aXMmpitTqa8nOCXs0Af132jcEOpq-_VwtDvbaNA2CCoLXOKsIBVeq4NqkWtiNaReRsCB0NjBfF_S-K4LRL2mdhMg9dS7ZWwHeVSeNqa5-MaZ_CuKmbGmIHZXgiwRVJZFOUaHfdpWaSccMFljnEogd3MqS-GyGrquFctFE-BhqXFF2_Room7cBefw4Vv_W64e50MaWTo22LclSk4CaVqDY9nSbgw3iOa9P3V-FJk3KjWJII-4xhqJ3mZLtCGuW8ZomGr95vGvaietn82C_YvRCSw0unEllSM72oC1jd5HXb8E7MA?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI3YjdlMzc4M2NkYWQyNjE0Njc1M2JlNjEzMTIwODlkZCIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.XG_xOHHz3AacyMcF_iMkQHXow0Q76AsvNCipkw2aXMmpitTqa8nOCXs0Af132jcEOpq-_VwtDvbaNA2CCoLXOKsIBVeq4NqkWtiNaReRsCB0NjBfF_S-K4LRL2mdhMg9dS7ZWwHeVSeNqa5-MaZ_CuKmbGmIHZXgiwRVJZFOUaHfdpWaSccMFljnEogd3MqS-GyGrquFctFE-BhqXFF2_Room7cBefw4Vv_W64e50MaWTo22LclSk4CaVqDY9nSbgw3iOa9P3V-FJk3KjWJII-4xhqJ3mZLtCGuW8ZomGr95vGvaietn82C_YvRCSw0unEllSM72oC1jd5HXb8E7MA%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D10.0s)\n\nMovie S2.\n\n**Visualization of active whisking against two vertical pegs (Scenario 2).** Same as in Movie S1, but instead of a single sweep through the entire array, the peg oscillates back and forth between its start and end position (in the middle of the array) to repeatedly stimulate the array eight times per second (8Hz). This scenario was carefully designed to replicate as closely as possible the stimulation distances, velocities, and frequencies associated with active whisking (Scenario 3).\n\n- [Download](https://www.pnas.org/doi/suppl/10.1073/pnas.2011905118/suppl_file/pnas.2011905118.sm02.mp4)\n- 2.82 MB\n\n[iframe](https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI4YWMxZGNjOWFlMWU2NDZhMzZjZmU5NmMxNDk2ZjljYSIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.VwAVxl9lx1AisXXMYwAU3BIm68me_k3Zpo01fs8NPN_wm5Ygg-tf8Y93QtLzf4N2PIzir9dKJQ8QjelfycFsp3IcpCQkfh1J1TarvTjTH8cdxhtNziC5II2TAKyMKoVWMfrgvaK-JFJ3dJa1w6a9dYvkA9N57k729R4Ue1wncls75bzUrecd6jHjy0VSW5R06FyeKY37hS4rtPJhl0nlj0bKVDegjNu-CdLNRLzPZ3zqhKPx6JLPqQS2LyOGE-3J8FNnTsd15amf-NHppVLe4Aa8MRf81Lrqgir1yYl4TSpFyJWY16a-u7S0NY4FNfX7k6Z5JSTWH_RaLbvAm6GsGw?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI4YWMxZGNjOWFlMWU2NDZhMzZjZmU5NmMxNDk2ZjljYSIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.VwAVxl9lx1AisXXMYwAU3BIm68me_k3Zpo01fs8NPN_wm5Ygg-tf8Y93QtLzf4N2PIzir9dKJQ8QjelfycFsp3IcpCQkfh1J1TarvTjTH8cdxhtNziC5II2TAKyMKoVWMfrgvaK-JFJ3dJa1w6a9dYvkA9N57k729R4Ue1wncls75bzUrecd6jHjy0VSW5R06FyeKY37hS4rtPJhl0nlj0bKVDegjNu-CdLNRLzPZ3zqhKPx6JLPqQS2LyOGE-3J8FNnTsd15amf-NHppVLe4Aa8MRf81Lrqgir1yYl4TSpFyJWY16a-u7S0NY4FNfX7k6Z5JSTWH_RaLbvAm6GsGw%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D10.0s)\n\nMovie S3.\n\n**Visualization of active whisking against two vertical pegs (Scenario 3).** Simulation of active whisking against a fixed, vertical peg. Each whisker is driven at its base according to established kinematic equations for whisking motion (19). One cycle of protraction and retraction of the array lasts 125 ms, equivalent to a whisking frequency of 8 Hz. The peg is positioned laterally, 20 mm from the midline of the head with an offset of 10 mm from the nose tip.\n\n- [Download](https://www.pnas.org/doi/suppl/10.1073/pnas.2011905118/suppl_file/pnas.2011905118.sm03.mp4)\n- 8.39 MB\n\n[iframe](https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI1OWQyMDE1NjJlNTg4ZmQ1MDZjMTBiMWIyMWJhNTZmZSIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.C99ZYvftYx8zFja5xhJAZJnT79MJmFkdn4z0wdCOi5Osm-1-p4HbGos5gSgWot-mOzTaB23-oYKkVCdNrQV7HMLMJXhl-vE6_CO0nB6cvJOSi5a6aq1ge3HAMV35aHeYisl5XwcWr_oMk1ELvMRabfw5MEc7Deo3KfkNi0KofG58VSp_xr0NPuNUVzFfPIBrZAzP75N2AtXChsRN27yrVmZGmJg4IXe1ufrN0_GtirD60hyArVkGN0id050dOvFFkW3LiHz0V8E7LDuoGB1hpKG74xLYJXaX2lIK-btq8KcbDaBq-bBqI3S-v9bY-C5YzC7FjbChSJESPEn07hhoFQ?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6I",
          "original_query": "WHISKiT-Physics / physics-based whisker simulator (Cheung et al., 2019)",
          "cleaned_query": "WHISKiT-Physics",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Young Children's Haptic Exploratory Procedures - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3166994/",
          "content": "\n \n \n \n \n. Author manuscript; available in PMC: 2012 Dec 1. \n Published in final edited form as: J Exp Child Psychol. 2011 Jul 23;110(4):592\u2013602. doi: 10.1016/j.jecp.2011.06.007 \n \n Abstract \n Adults vary their haptic exploratory behavior reliably with variation both in the sensory input and in the task goals. Little is known about the development of these connections between perceptual goals and exploratory behaviors. Thirty-six children 3, 4, and 5 years of age and 20 adults completed a haptic intra-modal match-to-sample task. Participants were instructed to feel the shape, texture, rigidity, or weight of a sample object, and then asked to find which of three test objects matched the sample on that specific property. Hand movements were examined to determine whether children produced the same exploratory procedures while gathering perceptual information about each property as adults who searched for the same kind of information. Children demonstrated that they had good haptic abilities in two ways: they matched the sample objects on the specified perceptual dimension at near ceiling levels, and they produced the same hand movement patterns to find the same properties as adults. \n The human hand is a powerful tool. The sensory and motor subsystems that serve the hand allow for quick and efficient interactions with the world. The hand\u2019s sensory system is designed to learn about the perceptual features of objects and is made up of cutaneous, thermal, and kinesthetic sensors. The hand\u2019s motor system is designed to interact with and manipulate objects ( Klatzky &amp; Lederman, 2002; Lederman &amp; Klatzky, 2009). The sensory and motor systems are not independent. How the hands are moved determines what sensory information is obtained. The sensory input obtained guides further hand movements and importantly, constrains what can be perceived. \n When adults search for perceptual information, their hand movement patterns (i.e., the haptic exploratory behaviors) are efficient, rapid, and systematic (e.g., Klatzky, Lederman, &amp; Metzger, 1985). Lederman, Klatzky, and their colleagues have extensively studied the haptic exploratory abilities of adults. Lederman and Klatzky (1987) recorded the hand and finger movements of adults attempting to extract perceptual information about specific object properties \u2013 shape, texture, hardness, weight, temperature, volume, part motion, and specific function \u2013 using haptics alone. Subjects were instructed to find which of three test objects was the best match for a standard object on one of these eight specific dimensions. None of the test objects was identical to the standard and participants were told to ignore all other properties of the objects. The participants\u2019 object matching choices and the hand movements that preceded them were recorded. The researchers identified eight stereotyped hand movement patterns or \u201cexploratory procedures,\u201d each associated with a specific task goal. For example, adults asked to match objects by texture generally produced \u201clateral motion\u201d \u2013 movements of the fingers lateral to the object\u2019s surface. In contrast, adults asked to match objects by shape produced \u201ccontour following\u201d \u2013 tracing the object\u2019s contours with their fingertips. Hand movements during haptic exploration varied reliably both with the sensory input and with the kind of information participants had been asked to seek. The consistent relations between particular hand movements and particular goals suggested that observing how participants moved their hands during haptic tasks could reveal the goals behind their exploratory behavior ( Lederman &amp; Klatzky, 1987). \n Little is known about the development of the connections between particular perceptual goals and particular exploratory manual behaviors observed in adults. In the present study, we ask whether young children perform the same hand movements as adults perform to achieve the same perceptual goals. There are very few previous studies on this question. Bushnell and Boudreau (1991, 1993, 1998) developed (but did not test) a set of hypotheses about when in development the exploratory procedures in Lederman and Klatzky\u2019s (1987) taxonomy might become available to infants and young children due to age-typical advances in motor behavior and attention. For example, these authors suggested that infants should not be able to produce the contour-following exploratory procedure associated with haptic perception of shape until 9 or 10 months of age, because it is not until this age that infants acquire the ability to move their two hands independently. Bushnell and Boudreau (1991, 1993, 1998) suggested that children might have the capacity to display mature haptic exploratory movements when they reached the preschool period. \n However, there are few data on whether preschool-aged children do employ adult-like exploratory procedures. In fact, there are few data on children\u2019s hand movements of any kind during haptic object exploration. Instead, research on the development of haptic perception has largely focused on children\u2019s ability to use haptic information for object recognition either in haptics or in vision. A number of studies have concluded that young children have poor haptic perception. However, this conclusion has been based on findings of deficits in young children\u2019s ability to transfer information between haptics and vision in inter-modal object recognition tasks. These studies have not reported how children moved their hands during haptic exploration (e.g., Milner &amp; Bryant, 1970; Rose, Blank, &amp; Bridger, 1972). \n In contrast, a number of studies have looked at what children do with their hands during visual object exploration. Ruff (1982, 1984, 1986, 1989; Ruff &amp; Kohler, 1978) carried out a number of studies documenting how infants handle objects during visual exploration, but there is no evidence that the set of hand movements produced by infants in the service of visual exploration overlaps with the set of hand movements that serve haptic perception. Klatzky, Lederman, and Mankinen (2005) asked preschool-aged children to make decisions about the appropriateness of a tool to perform a certain task \u2013for example, could a spoon be used to carry a piece of candy. Children were visually presented with the tools and also allowed to handle them. The children were more likely to haptically explore objects when the important object property was rigidity than when it was shape. Moreover, when the children chose to explore haptically, they tested the objects for rigidity using the same exploratory procedure as adults use for assessing rigidity (i.e., pressure; Lederman &amp; Klatzky, 1987). \n Only three previous studies have documented what children do with their hands during haptic exploration. Schwarzer, Kufer, and Wilkening (1999) reported that children aged 3 to 9 years produced adult-like exploratory procedures. Following haptic exploration, children in this study haptically grouped novel objects that simultaneously varied in shape, texture, weight, and size. Almost all participants categorized the objects analytically - that is, on a single perceptual dimension \u2013 as opposed to holistically. In at least 60% of their trials, 3- to 5-year-old children produced enclosure, lateral motion, and contour following \u2013 three adult-like exploratory procedures. However, whereas adults used enclosure and contour following to find shape information, the children in this study produced these two movements at high frequencies before categorizing objects by texture. It is possible that the children in Schwarzer et al. (1999) produced something similar to enclosure and contour following but that these movements were not really the same as those produced by adults; or that children obtained shape information from these two procedures but failed to use it during categorization. It is also possible that exploratory procedures and perceptual goals are not as strongly and distinctively linked in children as they are in adults. In any case, Schwarzer et al.\u2019s (1999) report of exploratory procedure use by children indicates that at least three of the movement patterns reliably found in adult haptic exploration were in the children\u2019s repertoires. \n More recently, Scofield, Hernandez-Reif ,and Keith (2009) noted the hand and finger movements of children in a word-learning study. Children aged 2, 3, 4, and 5 were taught the names of novel objects experienced haptically. While still holding each novel object out of sight, the children were asked to indicate which of two objects was a visual match for the object that they were holding. Video recordings were coded for how children manipulated the exemplar object while it was being labeled. The taxonomy of exploratory procedures developed by Lederman and Klatzky (1987) was not used. Instead, hand movements were examined for instances of the following three categories: \u201ctouching\u201d (using two hands to explore the object); \u201cenclosing\u201d (using one or two hands to surround the object); and \u201cexploring\u201d (active handling of the object). Children received one point for each category of behavior observed in each of two test trials (maximum score = 6). Scofield et al. (2009) were interested in whether higher manipulation scores would be associated with better word-learning performance. Their results indicated that manipulation scores both increased with age and were significantly and positively correlated with better object name learning. \n Kalagher and Jones (2011) compared visual object recognition following visual or haptic exploration in children 2 \u00bd to 5 years of age and in adults. The hand movements produced by participants at each age level during exploration in the two modalities were also compared. A novel name extension paradigm was used. On each trial, participants explored an exemplar object either haptically or visually and were told its novel name (e.g., \u201cThat\u2019s a dax\u201d). After the exemplar object was r",
          "original_query": "Haptic exploratory procedures and active touch (Lederman & Klatzky, 2009)",
          "cleaned_query": "Haptic exploratory procedures and active touch",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "The functional organization of the barrel cortex",
          "url": "https://infoscience.epfl.ch/entities/publication/8f925c98-686d-43c0-abbe-ed8ad80e34f9",
          "content": "Skip to main content\nLoading...Loading...\nreview article\nThe functional organization of the barrel cortex\n[Petersen, C. C.](https://infoscience.epfl.ch/items/d190dfa9-090c-4ced-9d70-999a51fe0ab2)\n2007\n[Neuron](https://infoscience.epfl.ch/items/8075d08a-452f-4a1f-ac30-852988aae9bf)\nThe tactile somatosensory pathway from whisker to cortex in rodents provides a well-defined system for exploring the link between molecular mechanisms, synaptic circuits, and behavior. The primary somatosensory cortex has an exquisite somatotopic map where each individual whisker is represented in a discrete anatomical unit, the \"barrel,\" allowing precise delineation of functional organization, development, and plasticity. Sensory information is actively acquired in awake behaving rodents and processed differently within the barrel map depending upon whisker-related behavior. The prominence of state-dependent cortical sensory processing is likely to be crucial in our understanding of active sensory perception, experience-dependent plasticity and learning.\nShow more\nType\nreview article\nDOI\n[10.1016/j.neuron.2007.09.017](https://doi.org/10.1016/j.neuron.2007.09.017)\nWeb of Science ID\nWOS:000250740700011\nAuthor(s)\n[Petersen, C. C.](https://infoscience.epfl.ch/items/d190dfa9-090c-4ced-9d70-999a51fe0ab2)\nDate Issued\n2007\nPublished in\n[Neuron](https://infoscience.epfl.ch/items/8075d08a-452f-4a1f-ac30-852988aae9bf)\nVolume\n56\nIssue\n2\nStart page\n339\nEnd page\n355\nPeer reviewed\nREVIEWED\nWritten at\nEPFL\nEPFL units\n[LSENS](https://infoscience.epfl.ch/items/4ea2e933-4d08-42c1-a598-a9bbd568f48e)\nAvailable on Infoscience\nDecember 18, 2007\nUse this identifier to reference this record\n[https://infoscience.epfl.ch/handle/20.500.14299/15887](https://infoscience.epfl.ch/handle/20.500.14299/15887)",
          "original_query": "The functional organization of barrel cortex / reviews of rodent somatosensory processing",
          "cleaned_query": "The functional organization of barrel cortex",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "CPC-Pretrained Spatiotemporal Encoders for Low-Label Traffic Prediction\nPretrain a traffic encoder using Contrastive Predictive Coding on large unlabeled sensor streams (predict future latent segments from past context), then fine-tune with limited labeled forecasting data. Quantify gains under realistic missing-data and sensor-sparsity regimes, and compare against supervised-only STDConvLSTM and standard masked-autoencoding baselines.",
        "Goal-Driven Models of Barrel Cortex with Task-Conditional Readouts\nTrain hierarchical CNN/RNN models on whisker-relevant goals (object localization, texture classification, gap crossing) and test whether intermediate layers best predict barrel cortex responses under different behavioral states. Add task-conditional readout heads to model state-dependent processing highlighted in barrel cortex organization, then validate against neural data across whisking vs. quiet wakefulness.",
        "Closed-Loop Active Whisking via Reinforcement Learning in WHISKiT Physics\nUse WHISKiT Physics as an environment to train policies that control whisking amplitude/frequency/phase to maximize information about object identity or location under time/energy constraints. The key contribution is a quantitative link between optimal control and the statistics of whisker-base signals, yielding testable predictions about how behavior should change with task demands.",
        "Contrastive Predictive Coding for Whisker-Base Mechanics to Discover \u201cPrimitive\u201d Features\nApply CPC directly to multichannel time series of simulated whisker-base forces/moments from WHISKiT Physics to learn representations that predict future contact dynamics. Test whether learned latents align with interpretable mechanical factors (contact onset, slip, resonance modes) and whether they transfer to downstream tasks (texture vs. shape) with minimal supervision.",
        "Developmental Modeling of Haptic Exploratory Procedures with Goal-Conditioned Policies\nBuild a model that selects exploratory procedures (lateral motion, contour following, pressure, unsupported holding) as discrete actions conditioned on perceptual goals, trained in simulation or with instrumented-hand data. Fit age-dependent constraints (motor noise, limited bimanual coordination, shorter planning horizon) to reproduce children\u2019s near-ceiling matching yet evolving movement efficiency.",
        "From Whisker Mechanics to Cortical Maps: Morphology-Driven Somatotopy in Silico\nSystematically vary whisker array morphology in WHISKiT Physics (length/curvature distributions, spacing) and train goal-driven models to solve tactile tasks, then test whether barrel-like topographic organization emerges as an efficient coding solution. This would generate falsifiable predictions about how altered morphology (genetic or trimming manipulations) should reshape functional maps and tuning.",
        "Adaptive Temporal Attention with Event-Gated Memory for Incident-Aware Traffic Forecasting\nExtend time-dependent attention by adding an event-gating module that detects regime shifts (accidents, weather shocks) and dynamically changes the memory update rule of the ConvLSTM (e.g., fast vs. slow timescales). Evaluate on datasets with incident logs to measure not only average error but also time-to-recover and peak-error during disruptions.",
        "Unified Spatiotemporal \u201cImbalance\u201d Theory: Variable Receptive Fields in Brains and Cities\nFormalize spatial/temporal imbalance as a learnable receptive-field selection problem and compare solutions in two domains: STDConvLSTM for traffic and goal-driven sensory models for barrel/visual cortex. Implement a shared mechanism (mixtures over scales with sparsity/energy penalties) and test whether similar scale-selection statistics emerge (e.g., broader integration in \u201chigh-importance\u201d nodes/regions or behavioral states).",
        "Self-Supervised Sensorimotor Prediction Linking Whisking Commands to Sensory Consequences\nTrain a model that jointly encodes whisking motor commands and whisker-base sensory streams to predict future sensory input (CPC-style) under closed-loop control in WHISKiT Physics. The contribution is a mechanistic account of sensorimotor integration: representations should separate reafference (self-generated) from exafference (object-driven), offering concrete neural predictions for state-dependent barrel cortex responses."
      ],
      "generated_ideas_raw": "1. **Space-Dependent Graph ConvLSTM for Road-Network Traffic Forecasting**  \n   Replace STDConvLSTM\u2019s grid CNN with a graph convolution operating on the true road network, but keep the *space-dependent receptive field* idea by learning node-wise mixtures over multiple hop-radius neighborhoods (e.g., 1\u2013K hops). Evaluate whether adaptive neighborhood size reduces error specifically in heterogeneous areas (CBD vs. suburbs) and during incidents.\n\n2. **CPC-Pretrained Spatiotemporal Encoders for Low-Label Traffic Prediction**  \n   Pretrain a traffic encoder using Contrastive Predictive Coding on large unlabeled sensor streams (predict future latent segments from past context), then fine-tune with limited labeled forecasting data. Quantify gains under realistic missing-data and sensor-sparsity regimes, and compare against supervised-only STDConvLSTM and standard masked-autoencoding baselines.\n\n3. **Goal-Driven Models of Barrel Cortex with Task-Conditional Readouts**  \n   Train hierarchical CNN/RNN models on whisker-relevant goals (object localization, texture classification, gap crossing) and test whether intermediate layers best predict barrel cortex responses under different behavioral states. Add task-conditional readout heads to model state-dependent processing highlighted in barrel cortex organization, then validate against neural data across whisking vs. quiet wakefulness.\n\n4. **Closed-Loop Active Whisking via Reinforcement Learning in WHISKiT Physics**  \n   Use WHISKiT Physics as an environment to train policies that control whisking amplitude/frequency/phase to maximize information about object identity or location under time/energy constraints. The key contribution is a quantitative link between optimal control and the statistics of whisker-base signals, yielding testable predictions about how behavior should change with task demands.\n\n5. **Contrastive Predictive Coding for Whisker-Base Mechanics to Discover \u201cPrimitive\u201d Features**  \n   Apply CPC directly to multichannel time series of simulated whisker-base forces/moments from WHISKiT Physics to learn representations that predict future contact dynamics. Test whether learned latents align with interpretable mechanical factors (contact onset, slip, resonance modes) and whether they transfer to downstream tasks (texture vs. shape) with minimal supervision.\n\n6. **Developmental Modeling of Haptic Exploratory Procedures with Goal-Conditioned Policies**  \n   Build a model that selects exploratory procedures (lateral motion, contour following, pressure, unsupported holding) as discrete actions conditioned on perceptual goals, trained in simulation or with instrumented-hand data. Fit age-dependent constraints (motor noise, limited bimanual coordination, shorter planning horizon) to reproduce children\u2019s near-ceiling matching yet evolving movement efficiency.\n\n7. **From Whisker Mechanics to Cortical Maps: Morphology-Driven Somatotopy in Silico**  \n   Systematically vary whisker array morphology in WHISKiT Physics (length/curvature distributions, spacing) and train goal-driven models to solve tactile tasks, then test whether barrel-like topographic organization emerges as an efficient coding solution. This would generate falsifiable predictions about how altered morphology (genetic or trimming manipulations) should reshape functional maps and tuning.\n\n8. **Adaptive Temporal Attention with Event-Gated Memory for Incident-Aware Traffic Forecasting**  \n   Extend time-dependent attention by adding an event-gating module that detects regime shifts (accidents, weather shocks) and dynamically changes the memory update rule of the ConvLSTM (e.g., fast vs. slow timescales). Evaluate on datasets with incident logs to measure not only average error but also time-to-recover and peak-error during disruptions.\n\n9. **Unified Spatiotemporal \u201cImbalance\u201d Theory: Variable Receptive Fields in Brains and Cities**  \n   Formalize spatial/temporal imbalance as a learnable receptive-field selection problem and compare solutions in two domains: STDConvLSTM for traffic and goal-driven sensory models for barrel/visual cortex. Implement a shared mechanism (mixtures over scales with sparsity/energy penalties) and test whether similar scale-selection statistics emerge (e.g., broader integration in \u201chigh-importance\u201d nodes/regions or behavioral states).\n\n10. **Self-Supervised Sensorimotor Prediction Linking Whisking Commands to Sensory Consequences**  \n   Train a model that jointly encodes whisking motor commands and whisker-base sensory streams to predict future sensory input (CPC-style) under closed-loop control in WHISKiT Physics. The contribution is a mechanistic account of sensorimotor integration: representations should separate reafference (self-generated) from exafference (object-driven), offering concrete neural predictions for state-dependent barrel cortex responses.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "CPC-Pretrained Spatiotemporal Encoders for Low-Label Traffic Prediction\nPretrain a traffic encoder using Contrastive Predictive Coding on large unlabeled sensor streams (predict future latent segments",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Goal-Driven Models of Barrel Cortex with Task-Conditional Readouts\nTrain hierarchical CNN/RNN models on whisker-relevant goals (object localization, texture classification, gap crossing) and test whet",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Closed-Loop Active Whisking via Reinforcement Learning in WHISKiT Physics\nUse WHISKiT Physics as an environment to train policies that control whisking amplitude/frequency/phase to maximize informatio",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Contrastive Predictive Coding for Whisker-Base Mechanics to Discover \u201cPrimitive\u201d Features\nApply CPC directly to multichannel time series of simulated whisker-base forces/moments from WHISKiT Physics t",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Developmental Modeling of Haptic Exploratory Procedures with Goal-Conditioned Policies\nBuild a model that selects exploratory procedures (lateral motion, contour following, pressure, unsupported holdi",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "From Whisker Mechanics to Cortical Maps: Morphology-Driven Somatotopy in Silico\nSystematically vary whisker array morphology in WHISKiT Physics (length/curvature distributions, spacing) and train goal",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Adaptive Temporal Attention with Event-Gated Memory for Incident-Aware Traffic Forecasting\nExtend time-dependent attention by adding an event-gating module that detects regime shifts (accidents, weath",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Unified Spatiotemporal \u201cImbalance\u201d Theory: Variable Receptive Fields in Brains and Cities\nFormalize spatial/temporal imbalance as a learnable receptive-field selection problem and compare solutions in",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Self-Supervised Sensorimotor Prediction Linking Whisking Commands to Sensory Consequences\nTrain a model that jointly encodes whisking motor commands and whisker-base sensory streams to predict future ",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 47,
      "paper_title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
      "contribution": "Introduce Elastic Multimodal Parallelism (EMP) and ElasticMM, a serving system that decouples multimodal inference stages, performs modality-aware load balancing, and elastically adjusts per-stage parallelism and caching to greatly reduce TTFT and improve throughput for MLLMs.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "input_tokens": 9023,
      "output_tokens": 1077,
      "predecessor_details": [
        {
          "success": true,
          "title": "DistServe: Disaggregating Prefill and Decoding for ...",
          "url": "https://arxiv.org/html/2401.09670",
          "content": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving\n# DistServe: Disaggregating Prefill and Decoding for Goodput-optimized\nLarge Language Model Serving\nYinmin Zhong11{}^{\\\\text{1}}start\\_FLOATSUPERSCRIPT 1 end\\_FLOATSUPERSCRIPTShengyu Liu11{}^{\\\\text{1}}start\\_FLOATSUPERSCRIPT 1 end\\_FLOATSUPERSCRIPTJunda Chen33{}^{\\\\text{3}}start\\_FLOATSUPERSCRIPT 3 end\\_FLOATSUPERSCRIPTJianbo Hu11{}^{\\\\text{1}}start\\_FLOATSUPERSCRIPT 1 end\\_FLOATSUPERSCRIPTYibo Zhu22{}^{\\\\text{2}}start\\_FLOATSUPERSCRIPT 2 end\\_FLOATSUPERSCRIPTXuanzhe Liu11{}^{\\\\text{1}}start\\_FLOATSUPERSCRIPT 1 end\\_FLOATSUPERSCRIPT\nXin Jin11{}^{\\\\text{1}}start\\_FLOATSUPERSCRIPT 1 end\\_FLOATSUPERSCRIPTHao Zhang33{}^{\\\\text{3}}start\\_FLOATSUPERSCRIPT 3 end\\_FLOATSUPERSCRIPT\n11{}^{\\\\text{1}}start\\_FLOATSUPERSCRIPT 1 end\\_FLOATSUPERSCRIPTSchool of Computer Science, Peking University22{}^{\\\\text{2}}start\\_FLOATSUPERSCRIPT 2 end\\_FLOATSUPERSCRIPTStepFun33{}^{\\\\text{3}}start\\_FLOATSUPERSCRIPT 3 end\\_FLOATSUPERSCRIPTUC San Diego\n###### Abstract\nDistServe improves the performance of large language models (LLMs) serving by disaggregating the prefill and decoding computation. Existing LLM serving systems colocate the two phases and batch the computation of prefill and decoding across all users and requests.\nWe find that this strategy not only leads to strong prefill-decoding interferences but also couples the resource allocation and parallelism plans for both phases. LLM applications often emphasize individual latency for each phase: time to first token (TTFT) for the prefill phase and time per output token (TPOT) of each request for the decoding phase.\nIn the presence of stringent latency requirements, existing systems have to prioritize one latency over the other, or over-provision compute resources to meet both.\nDistServe assigns prefill and decoding computation to different GPUs, hence eliminating prefill-decoding interferences. Given the application\u2019s TTFT and TPOT requirements, DistServe co-optimizes the resource allocation and parallelism strategy*tailored*for each phase. DistServe also places the two phases according to the serving cluster\u2019s bandwidth to minimize the communication caused by disaggregation. As a result, DistServe significantly improves LLM serving performance in terms of the maximum rate that can be served within both TTFT and TPOT constraints on each GPU.\nOur evaluations show that on various popular LLMs, applications, and latency requirements, DistServe can serve 7.4\u00d7\\\\times\u00d7more requests or 12.6\u00d7\\\\times\u00d7tighter SLO, compared to state-of-the-art systems, while staying within latency constraints for&gt;90%absentpercent90&gt;&gt;90\\\\%&gt; 90 %of requests.\n## 1Introduction\nLarge language models (LLMs), such as GPT-4> [\n[> 37\n](https://arxiv.org/html/2401.09670v3#bib.bib37)> ]\n, Bard> [\n[> 2\n](https://arxiv.org/html/2401.09670v3#bib.bib2)> ]\n, and LLaMA> [\n[> 51\n](https://arxiv.org/html/2401.09670v3#bib.bib51)> ]\n, represent a groundbreaking shift in generative AI. They start to reshape existing Internet services, ranging from search engines to personal assistants> [\n[> 4\n](https://arxiv.org/html/2401.09670v3#bib.bib4)> ]\n, and enable fundamentally new applications, like universal chatbots> [\n[> 1\n](https://arxiv.org/html/2401.09670v3#bib.bib1)> , [> 16\n](https://arxiv.org/html/2401.09670v3#bib.bib16)> ]\nand programming assistants> [\n[> 15\n](https://arxiv.org/html/2401.09670v3#bib.bib15)> , [> 42\n](https://arxiv.org/html/2401.09670v3#bib.bib42)> ]\n. Yet, these advances come with a significant challenge: processing an end-to-end LLM query can be substantially slower than a standard search query> [\n[> 41\n](https://arxiv.org/html/2401.09670v3#bib.bib41)> ]\n. In order to meet the stringent latency requirements of various applications, service providers need to over-provision compute resources, particularly many GPUs, leading to a shortfall in cost efficiency. Therefore, optimizing the cost per LLM query while adhering to highSLO attainment(the proportion of requests that meet the SLOs) is becoming increasingly essential for all LLM services.\n![Refer to caption](x1.png)Figure 1:Performance when serving an LLM with 13B parameters under a synthetic workload with input length = 512 and output length = 64 on one NVIDIA 80GB A100.Upper: The P90 time-to-first-token (TTFT) latency comparing existing systems vs. a system serving only the prefill phase.Down: The P90 time-per-output-token (TPOT) latency comparing existing systems vs. a system serving only the decoding phase.\nAn LLM service responds to a user query in two phases. The*prefill phase*processes a user\u2019s prompt, composed of a sequence of tokens, to generate the first token of the response*in one step*. Following it, the*decoding phase*sequentially generates subsequent tokens*in multiple steps*; each decoding step generates a new token based on tokens generated in previous steps, until reaching a termination token.\nThis dual-phase process distinguishes LLM services from traditional services\n\u2013an LLM service\u2019s latency is uniquely measured by two key metrics: the*time to first token*(TTFT), which is the duration of the prefill phase, and the*time per output token*(TPOT), which represents the average time taken to generate a token for each request (except for the first token)111The overall request latency equals TTFT plus TPOT times the number of generated tokens in the decoding phase..\nDifferent applications place varying demands on each metric. For example, real-time chatbots> [\n[> 1\n](https://arxiv.org/html/2401.09670v3#bib.bib1)> ]\nprioritize low TTFT for response promptness, while TPOT only remains important until it is faster than human reading speed (i.e., 250 words/min).\nConversely, document summarization emphasizes low TPOT for faster generation of the summary.\nHence, given the application\u2019s TTFT and TPOT requirements, an effective LLM serving system should balance these needs and maximize*per-GPU goodput*, defined as the maximum request rate that can be served adhering to the SLO attainment goal (say, 90%) for each GPU provisioned \u2013higher per-GPU goodput directly translates into lower cost per query.\nAs the prefill and decoding phases share the LLM weights and working memory,\nexisting LLM serving systems typically colocate both phases on GPUs and maximize the overall system throughput \u2013tokens generated per second across all users and requests \u2013by batching the prefill and decoding steps across requests> [\n[> 54\n](https://arxiv.org/html/2401.09670v3#bib.bib54)> , [> 31\n](https://arxiv.org/html/2401.09670v3#bib.bib31)> ]\n. However, to meet latency requirements, we find these systems must over-provision compute resources. To see this, Figure[1](https://arxiv.org/html/2401.09670v3#S1.F1)illustrates how the P90 TTFT and TPOT shift with increasing request rates when serving a 13B LLM using existing systems> [\n[> 32\n](https://arxiv.org/html/2401.09670v3#bib.bib32)> ]\n, with workload pattern and two latency constraints set to emulate using LLM to generate a short summary for an article. Under the SLO attainment of 90%, the maximum achievable goodput on a single A100 GPU, which is constrained by the more stringent one of TTFT and TPOT requirements, is about 1.6 requests per second (rps).\nThe performance contrasts sharply when each phase is served independently on a separate GPU, shown by the orange and green curves, which achieve per-GPU goodput of 5.6 rps for the prefill phase and 10 rps for decoding. Ideally, by allocating 2 GPUs for prefill and 1 GPU for decoding, we can effectively serve the model with an overall goodput of 10 rps, or equally 3.3 rps per GPU, which is 2.1x higher than existing systems.\nThe gap in goodput primarily stems from the colocation of the prefill and decoding \u2013two phases with very distinct computational characteristics and latency requirements (\u00a7[2.1](https://arxiv.org/html/2401.09670v3#S2.SS1)).\nFirst, colocation leads to strong*prefill-decoding interference*.\nA prefill step often takes much longer than a decoding step. When batched together, decoding steps in the batch are delayed by the prefill steps, significantly elongating their TPOT; similarly, the inclusion of decoding steps contributes to a non-trivial increase in TTFT, as evidenced in Figure[2](https://arxiv.org/html/2401.09670v3#S2.F2).\nEven if we schedule them separately, issues persist as they begin to compete for resources. Decoding tasks awaiting GPU execution are subject to increased queuing delays due to ongoing prefill tasks, and vice versa. Prioritized scheduling of one phase risks failing the latency requirements of the other.\nSecond, the prefill and decoding computation differ in latency requirements and preference for different forms of parallelism (\u00a7[3](https://arxiv.org/html/2401.09670v3#S3)). Colocating prefill and decoding, however, couples their resource allocation, and prevents implementing different parallelism strategies more suited to meeting the specific latency requirements of each phase.\nTo overcome these challenges, we propose to disaggregate the prefill and decoding phases of LLM inference, assigning them to separate GPUs. Our approach has two benefits.\nFirst, operating each phase independently on different GPUs eliminates prefill-decoding interference. Second, it allows to scale each phase independently with tailored resource allocation and model parallelism strategies to meet their specific latency requirements.\nAlthough disaggregation causes communication of intermediate states between GPUs, we show that the communication overhead is insubstantial (\u00a7[3.3](https://arxiv.org/html/2401.09670v3#S3.SS3)) in modern GPU clusters, and when managed appropriately, disaggregation significantly improves per-GPU goodput.\nBased on the above insights, in this work, we build DistServe222[https://github.com/LLMServe/DistServe](https://github.com/LLMServe/DistServe), a goodput-optimized LLM serving system by disaggregating the prefill and decoding phases. Give",
          "original_query": "DistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving",
          "cleaned_query": "DistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "[2404.09526] LoongServe: Efficiently Serving Long-Context Large ...",
          "url": "https://arxiv.org/abs/2404.09526",
          "content": "# Computer Science > Distributed, Parallel, and Cluster Computing\n\n**arXiv:2404.09526** (cs)\n\n\\[Submitted on 15 Apr 2024 ( [v1](https://arxiv.org/abs/2404.09526v1)), last revised 29 Oct 2024 (this version, v2)\\]\n\n# Title:LoongServe: Efficiently Serving Long-Context Large Language Models with Elastic Sequence Parallelism\n\nAuthors: [Bingyang Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+B), [Shengyu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+S), [Yinmin Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong,+Y), [Peng Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+P), [Xuanzhe Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+X), [Xin Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin,+X)\n\nView a PDF of the paper titled LoongServe: Efficiently Serving Long-Context Large Language Models with Elastic Sequence Parallelism, by Bingyang Wu and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2404.09526) [HTML (experimental)](https://arxiv.org/html/2404.09526v2)\n\n> Abstract:The context window of large language models (LLMs) is rapidly increasing, leading to a huge variance in resource usage between different requests as well as between different phases of the same request. Restricted by static parallelism strategies, existing LLM serving systems cannot efficiently utilize the underlying resources to serve variable-length requests in different phases. To address this problem, we propose a new parallelism paradigm, elastic sequence parallelism (ESP), to elastically adapt to the variance between different requests and phases. Based on ESP, we design and build LoongServe, an LLM serving system that (1) improves computation efficiency by elastically adjusting the degree of parallelism in real-time, (2) improves communication efficiency by reducing key-value cache migration overhead and overlapping partial decoding communication with computation, and (3) improves GPU memory efficiency by reducing key-value cache fragmentation across instances. Our evaluation under diverse real-world datasets shows that LoongServe improves the maximum throughput by up to 3.85$\\\\times$ compared to the chunked prefill and 5.81$\\\\times$ compared to the prefill-decoding disaggregation.\n\n| | |\n| --- | --- |\n| Subjects: | Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2404.09526](https://arxiv.org/abs/2404.09526) \\[cs.DC\\] |\n| (or [arXiv:2404.09526v2](https://arxiv.org/abs/2404.09526v2) \\[cs.DC\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2404.09526](https://doi.org/10.48550/arXiv.2404.09526) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Bingyang Wu \\[ [view email](https://arxiv.org/show-email/6157a82c/2404.09526)\\] **[\\[v1\\]](https://arxiv.org/abs/2404.09526v1)**\nMon, 15 Apr 2024 07:45:04 UTC (399 KB)\n**\\[v2\\]**\nTue, 29 Oct 2024 13:04:42 UTC (440 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled LoongServe: Efficiently Serving Long-Context Large Language Models with Elastic Sequence Parallelism, by Bingyang Wu and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2404.09526)\n- [HTML (experimental)](https://arxiv.org/html/2404.09526v2)\n- [TeX Source](https://arxiv.org/src/2404.09526)\n- [Other Formats](https://arxiv.org/format/2404.09526)\n\n[view license](http://creativecommons.org/licenses/by-sa/4.0/)\n\nCurrent browse context:\n\ncs.DC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2404.09526&function=prev&context=cs.DC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2404.09526&function=next&context=cs.DC)\n\n[new](https://arxiv.org/list/cs.DC/new) \\| [recent](https://arxiv.org/list/cs.DC/recent) \\| [2024-04](https://arxiv.org/list/cs.DC/2024-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2404.09526?context=cs) [cs.LG](https://arxiv.org/abs/2404.09526?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2404.09526)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2404.09526)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2404.09526)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2404.09526) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "LoongServe: Efficiently serving long-context large language models with elastic sequence parallelism",
          "cleaned_query": "LoongServe: Efficiently serving long-context large language models with elastic sequence parallelism",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "ModServe: Scalable and Resource-Efficient Large ...",
          "url": "https://arxiv.org/html/2502.00937v2",
          "content": "\\\\newmdenv\n\n\\[\nbackgroundcolor=mygray,\nlinecolor=black,\nlinewidth=1pt,\nroundcorner=3pt,\nnobreak=true,\n\\]keyfinding\n\n# ModServe: Scalable and Resource-Efficient Large Multimodal Model Serving\n\nHaoran Qiu\nMicrosoft Azure Research\n[haoran.qiu@microsoft.com](mailto:haoran.qiu@microsoft.com),\u00a0Anish Biswas\nMicrosoft Research India\n[t-anibiswas@microsoft.com](mailto:t-anibiswas@microsoft.com),\u00a0Zihan Zhao\nUniversity of Virginia\n[rxy6cc@virginia.edu](mailto:rxy6cc@virginia.edu),\u00a0Jayashree Mohan\nMicrosoft Research India\n[jamohan@microsoft.com](mailto:jamohan@microsoft.com),\u00a0Alind Khare\nMicrosoft M365 Research\n[alindkhare@microsoft.com](mailto:alindkhare@microsoft.com),\u00a0Esha Choukse\nMicrosoft Azure Research\n[esha.choukse@microsoft.com](mailto:esha.choukse@microsoft.com),\u00a0\u00cd\u00f1igo Goiri\nMicrosoft Azure Research\n[inigog@microsoft.com](mailto:inigog@microsoft.com),\u00a0Zeyu Zhang\nUniversity of Virginia\n[qxc4fh@virginia.edu](mailto:qxc4fh@virginia.edu),\u00a0Haiying Shen\nUniversity of Virginia\n[hs6ms@virginia.edu](mailto:hs6ms@virginia.edu),\u00a0Chetan Bansal\nMicrosoft M365 Research\n[chetanb@microsoft.com](mailto:chetanb@microsoft.com),\u00a0Ramachandran Ramjee\nMicrosoft Research India\n[ramjee@microsoft.com](mailto:ramjee@microsoft.com)\u00a0and\u00a0Rodrigo Fonseca\nMicrosoft Azure Research\n[fonseca.rodrigo@microsoft.com](mailto:fonseca.rodrigo@microsoft.com)\n\n###### Abstract.\n\nLarge multimodal models (LMMs) demonstrate impressive capabilities in understanding images, videos, and audio beyond text.\nHowever, efficiently serving LMMs in production environments poses significant challenges due to their complex architectures and heterogeneous characteristics across their multi-stage inference pipelines.\n\nWe present the first comprehensive systems analysis of two prominent LMM architectures, decoder-only and cross-attention, across six representative open-source models, revealing key systems design implications.\nWe also present an in-depth analysis of production LMM inference traces, uncovering unique workload characteristics, including variable, heavy-tailed request distributions and bursty traffic patterns.\n\nBased on these insights, we propose ModServe, a modular LMM serving system that decouples stages for independent optimization and adaptive scaling. ModServe dynamically reconfigures stages and handles bursty traffic with modality-aware scheduling and autoscaling to meet tail latency SLOs while minimizing costs.\nModServe achieves 3.3\u20135.5\u00d7\\\\times\u00d7 higher throughput (leading to 25\u201341.3% cost saving) while meeting SLOs on a 128-GPU cluster with production traces.\n\n## 1\\. Introduction\n\nThe rapid advancement in generative AI has led to the development of large multimodal models (LMMs) capable of processing inputs across various modalities such as text, image, video, and audio.\nThese models have demonstrated remarkable capabilities in tasks like image captioning\u00a0(Chen et\u00a0al., [2022](https://arxiv.org/html/2502.00937v2#bib.bib6); Mokady et\u00a0al., [2021](https://arxiv.org/html/2502.00937v2#bib.bib36); Hu et\u00a0al., [2023](https://arxiv.org/html/2502.00937v2#bib.bib18)), visual question answering\u00a0(Schwenk et\u00a0al., [2022](https://arxiv.org/html/2502.00937v2#bib.bib47); Shao et\u00a0al., [2023](https://arxiv.org/html/2502.00937v2#bib.bib48)), and multimodal dialogue systems\u00a0(Li et\u00a0al., [2024b](https://arxiv.org/html/2502.00937v2#bib.bib26); Chen et\u00a0al., [2024b](https://arxiv.org/html/2502.00937v2#bib.bib9); Team et\u00a0al., [2024](https://arxiv.org/html/2502.00937v2#bib.bib53)).\nThis has led to a rapid adoption of LMMs in production services, including online applications where latency service-level objectives (SLOs) are critical.\n\nUnlike traditional large language models (LLMs) that process purely textual inputs using a single component, a decoder-based transformer architecture\u00a0(Waswani et\u00a0al., [2017](https://arxiv.org/html/2502.00937v2#bib.bib56)), LMMs handle fundamentally different types of inputs, each requiring distinct processing approaches.\nThis heterogeneity introduces unique serving complexities that demand novel analysis and serving strategies.\nFor Image-Text-to-Text models\u00a0(HuggingFace, [2024c](https://arxiv.org/html/2502.00937v2#bib.bib22)), the inference pipeline consists of multiple specialized stages:\nimage preprocessing to transform raw images into tensor representations, image encoding to convert these tensors into image tokens, and a language model backend that combines text prompts with image tokens to generate text outputs.\nCurrently, these stages are typically served as a monolithic system\u00a0(Kwon et\u00a0al., [2023](https://arxiv.org/html/2502.00937v2#bib.bib24); Wolf et\u00a0al., [2020](https://arxiv.org/html/2502.00937v2#bib.bib57); Aminabadi et\u00a0al., [2022](https://arxiv.org/html/2502.00937v2#bib.bib5)), where all components are integrated within a single serving instance and scaled together as a unified entity.\n\nFigure 1. Impact of image workload on LMM inference TTFT for state-of-the-art implementation of Llama3.2-11B on vLLM vs. ModServe with an 8-A100 GPU server.\nThe \u201cMonolith\u201d setup deploys the full model using 8 GPUs while the \u201cDecoupled\u201d setup deploys the LLM backend on 4 GPUs and four image encoders on the other 4 GPUs.\n\nUnfortunately, existing monolithic inference serving systems fail to account for multimodality, making them unable to scale effectively while meeting time-to-first-token (TTFT) SLOs, which now include image processing and encoding times.\n[Figure1](https://arxiv.org/html/2502.00937v2#S1.F1) shows how a monolithic deployment struggles to scale as the number of images per request increases (a common scenario in multi-image or video workloads) resulting in sharp TTFT degradation.\nAs a result, image-heavy requests can result in head-of-line (HoL) blocking, reducing system responsiveness and causing overprovisioning.\n\nOur Work.\nIn this paper, we present the first comprehensive systems analysis of two leading LMM architectures:\ncross-attention ( _CroAttn_) and decoder-only ( _DecOnly_), on both open-source LMMs and novel production LMM inference traces in Azure datacenters.\nWe analyze their multi-stage inference pipelines, performance-resource tradeoffs, and production workload patterns, including variable request rates, diverse multimodal inputs, and bursty traffic.\nWe focus on Image-Text-to-Text but our insights extend to other multimodal scenarios, such as Video-Text-to-Text, where videos are processed as image frame sequences\u00a0(Li et\u00a0al., [2024c](https://arxiv.org/html/2502.00937v2#bib.bib27)), and Audio-Text-to-Text tasks\u00a0(HuggingFace, [2024a](https://arxiv.org/html/2502.00937v2#bib.bib20)), which share similar model architectures with the models we study.\n\nOur analysis identifies three key insights for optimizing LMM inference.\nFirst, different LMM inference stages exhibit diverse performance characteristics and varying sensitivity to resource and model configurations ( _e.g._, batching and model sharding), necessitating _decoupled execution_.\nSecond, image encoding is a major bottleneck for TTFT, requiring efficient _encoder parallelization_ to reduce both latency and HoL blocking.\nFinally, production multimodal traffic exhibits distinct bursty patterns driven by increased images per request, highlighting the need for _modality-aware routing_ strategies to manage bursts and mitigate tail latency spikes.\n\nBased on these insights, we propose ModServe, a novel modular architecture for scalable and resource-efficient LMM serving which directly addresses the challenges identified in our analysis.\nModServe separates image- and text-specific inference stages into distinct instances for decoupled execution.\nIn ModServe, _Image Instances_ handle image preprocessing and encoding, while _Text Instances_ manage LLM prefill and decoding ( [Figure1](https://arxiv.org/html/2502.00937v2#S1.F1)).\nText-only requests are served by _Text Instances_, whereas image-text requests go through _Image Instances_ where images are converted to tokens before being forwarded to _Text Instances_ for text generation.\n\nModServe\u2019s modular architecture unlocks stage-specific optimizations. ModServe manages _Image_ and _Text Instances_ independently with stage-aware autoscaling, model sharding, and batching.\nBy autoscaling the stages separately, it minimizes resource overprovisioning.\nFor example, during image-driven bursts observed in production traffic, _Image Instances_ can scale out independently, making ModServe more resource-efficient than monolithic inference systems.\nTo navigate the image encoding bottleneck, ModServe parallelizes encoding of a single request across multiple _Image Instances_ ( [Figure1](https://arxiv.org/html/2502.00937v2#S1.F1)), leveraging our finding that the images within a request do not attend to each other during encoding, and hence the requests can be parallelized at the image level.\n\nFurther, to manage image-driven bursts, ModServe implements modality-aware routing for _Image and Text Instances_.\nFor example, images from image-text requests are routed to _Image Instances_ with the fewest pending image tokens to encode, reducing HoL blocking and tail latency spikes.\n\nWe implement ModServe on top of a high-performance inference system, vLLM\u00a0(Kwon et\u00a0al., [2023](https://arxiv.org/html/2502.00937v2#bib.bib24)), and demonstrate the effectiveness of ModServe through extensive evaluations on a 16-server (128 GPUs) cluster running production Azure LMM inference traces.\nCompared to state-of-the-art baselines, ModServe achieves 3.3\u20135.5\u00d7\\\\times\u00d7 higher throughput under static allocation and reduces LMM serving cost by 25\u201341.3% while meeting the P99 TTFT SLOs.\n\nSummary.\nThis paper makes the following contributions:\n\n- \u2022\n\n\nA comprehensive system characterization on LMM serving, examining performance profiles and resource utilization patterns across diverse workloads in both open-source LMM deployments and production environment.\n\n- \u2022\n\n\nA large open-source dataset containing sampled production Azure LMM inference traces.\n\n- \u2022\n\n\nDesign and implementation of Mo",
          "original_query": "ModServe: Scalable and resource-efficient large multimodal model serving",
          "cleaned_query": "ModServe: Scalable and resource-efficient large multimodal model serving",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "High-Throughput LLM Inference with Shared Prefixes",
          "url": "https://openreview.net/forum?id=Ye3ia34Fpp",
          "content": "[Go to **ICML 2024 Workshop ES-FoMo-II** homepage](https://openreview.net/group?id=ICML.cc/2024/Workshop/ES-FoMo-II)\n\n## Hydragen: High-Throughput LLM Inference with Shared Prefixes\n\n### [Jordan Juravsky](https://openreview.net/profile?id=~Jordan_Juravsky1), [Bradley Brown](https://openreview.net/profile?id=~Bradley_Brown1), [Ryan Saul Ehrlich](https://openreview.net/profile?id=~Ryan_Saul_Ehrlich1), [Daniel Y Fu](https://openreview.net/profile?id=~Daniel_Y_Fu1), [Christopher Re](https://openreview.net/profile?id=~Christopher_Re1), [Azalia Mirhoseini](https://openreview.net/profile?id=~Azalia_Mirhoseini3)\n\nPublished: 21 Jun 2024, Last Modified: 24 Jul 2024ES-FoMo-II 2024 PosterEveryone[Revisions](https://openreview.net/revisions?id=Ye3ia34Fpp)[BibTeX](https://openreview.net/openreview.net)[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)\n\n**Keywords:** Large language models, high throughput, hardware-awareness\n\n**TL;DR:** We present an efficient implementation of attention specialized for batches of sequences that share a prefix.\n\n**Abstract:** As large language models (LLMs) are deployed more broadly, reducing the cost of inference has become increasingly important. A common inference use case involves a batch of sequences that share a prefix, such as when reusing few-shot examples or sampling many completions from a single prompt. In a large-batch setting, transformer decoding can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention specialized for shared prefixes. Hydragen computes attention separately over the shared prefix and unique suffixes. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and replacing matrix-vector products with hardware-friendly matrix-matrix products. In a high-throughput setting (batch size 1K, tensor parallelism across eight A100s), our method can improve end-to-end CodeLlama-13b throughput by over 3x with a prefix length of 1K, and by over 30x with a prefix length of 16K. Hydragen\u2019s efficient processing of long shared contexts lead to only a 15\\\\% drop in throughput as the sequence length grows by 16x. We extend Hydragen beyond simple prefix-suffix decomposition and apply it to hierarchical sharing patterns, which allows us to further reduce inference time on competitive programming problems by a further 55\\\\%.\n\n**Submission Number:** 71\n\nLoading\n\n[OpenReview](https://openreview.net/about) is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the [OpenReview Sponsors](https://openreview.net/sponsors). \u00a9 2025 OpenReview",
          "original_query": "Hydragen: High-throughput LLM inference with shared prefixes",
          "cleaned_query": "Hydragen: High-throughput LLM inference with shared prefixes",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Orca: A Distributed Serving System for Transformer-Based ... - USENIX",
          "url": "https://www.usenix.org/system/files/osdi22-yu.pdf",
          "content": "This paper is included in the Proceedings of the\n16th USENIX Symposium on Operating Systems \nDesign and Implementation.\nJuly 11\u201313, 2022 \u2022 Carlsbad, CA, USA\n978-1-939133-28-1\nOpen access to the Proceedings of the \n16th USENIX Symposium on Operating \nSystems Design and Implementation \nis sponsored by\nOrca: A Distributed Serving System for \nTransformer-Based Generative Models\nGyeong-In Yu and Joo Seong Jeong, Seoul National University; \nGeon-Woo Kim, FriendliAI and Seoul National University; Soojeong Kim, FriendliAI; \nByung-Gon Chun, FriendliAI and Seoul National University\nhttps://www.usenix.org/conference/osdi22/presentation/yu\nORCA: A Distributed Serving System for\nTransformer-Based Generative Models\nGyeong-In Yu\nSeoul National University\nJoo Seong Jeong\nSeoul National University\nGeon-Woo Kim\nFriendliAI\nSeoul National University\nSoojeong Kim\nFriendliAI\nByung-Gon Chun\u2217\nFriendliAI\nSeoul National University\nAbstract\nLarge-scale Transformer-based models trained for generation\ntasks (e.g., GPT-3) have recently attracted huge interest, em\u0002phasizing the need for system support for serving models in\nthis family. Since these models generate a next token in an au\u0002toregressive manner, one has to run the model multiple times\nto process an inference request where each iteration of the\nmodel generates a single output token for the request. How\u0002ever, existing systems for inference serving do not perform\nwell on this type of workload that has a multi-iteration char\u0002acteristic, due to their inflexible scheduling mechanism that\ncannot change the current batch of requests being processed;\nrequests that have finished earlier than other requests in a\nbatch cannot return to the client, while newly arrived requests\nhave to wait until the current batch completely finishes.\nIn this paper, we propose iteration-level scheduling, a new\nscheduling mechanism that schedules execution at the gran\u0002ularity of iteration (instead of request) where the scheduler\ninvokes the execution engine to run only a single iteration of\nthe model on the batch. In addition, to apply batching and\niteration-level scheduling to a Transformer model at the same\ntime, we suggest selective batching, which applies batching\nonly to a selected set of operations. Based on these two tech\u0002niques, we have implemented a distributed serving system\ncalled ORCA, with additional designs for scalability to models\nwith hundreds of billions of parameters. Our evaluation on a\nGPT-3 175B model shows that ORCA can significantly out\u0002perform NVIDIA FasterTransformer in terms of both latency\nand throughput: 36.9\u00d7 throughput improvement at the same\nlevel of latency.\n1 Introduction\nLanguage generation tasks are becoming increasingly\nparamount to many types of applications, such as chatbot [9,\n52], summarization [41,45,54], code generation [13], and cap\u0002tion generation [65,66]. Moreover, recent works published by\n\u2217Corresponding author.\nAI21 Labs [37], DeepMind [26,48], Google [15,21,63], Meta\nPlatforms [10,67], Microsoft [50], Microsoft & NVIDIA [59],\nand OpenAI [12] have reported that every language process\u0002ing task, including translation [11, 17], classification [20, 53],\nquestion-answering [32, 33, 40] and more, can be cast as a\nlanguage generation problem and have shown great improve\u0002ments along this direction. The rise of generative models is\nnot limited to the language domain; the AI community has\nalso given growing interest to generation problems in other do\u0002mains such as image, video, speech, or a mixture of multiple\ndomains [19,38,51,62]. At the heart of generative models lies\nthe Transformer architecture [60] and its variants [15, 47\u201349].\nBy relying on the attention mechanism [60], Transformer\nmodels can learn better representations where each element\nof the sequence may have a direct connection with every other\nelement, which was not possible in recurrent models [25].\nTo use generative models in real-world applications, we\noften delegate the inference procedure to a separate service\nresponsible for ML inference serving. The growing demands\nfor this service, which should provide inference results for\nclient requests at low latency and high throughput, have fa\u0002cilitated the development of inference serving systems such\nas Triton Inference Server [7] and TensorFlow Serving [42].\nThese systems can use a separately-developed DNN execution\nengine to perform the actual tensor operations. For example,\nwe can deploy a service for language generation tasks by\nusing a combination of Triton and FasterTransformer [4], an\nexecution engine optimized for the inference of Transformer\u0002based models. In this case, Triton is mainly responsible for\ngrouping multiple client requests into a batch, while Faster\u0002Transformer receives the batch from Triton and conducts the\ninference procedure in the batched manner.\nUnfortunately, we notice that the existing inference sys\u0002tems, including both the serving system layer and the execu\u0002tion engine layer, have limitations in handling requests for\nTransformer-based generative models. Since these models are\ntrained to generate a next token in an autoregressive manner,\none should run the model as many times as the number of to\u0002kens to generate, while for other models like ResNet [24] and\nUSENIX Association 16th USENIX Symposium on Operating Systems Design and Implementation 521\nBERT [18] a request can be processed by running the model\nonce. That is, in order to process a request to the generative\nmodel, we have to run multiple iterations of the model; each\niteration generates a single output token, which is used as\nan input in the following iteration. Such multi-iteration char\u0002acteristic calls into question the current design of inference\nsystems, where the serving system schedules the execution\nof the engine at the granularity of request. Under this design,\nwhen the serving system dispatches a batch of requests to\nthe engine, the engine returns inference results for the entire\nbatch at once after processing all requests within the batch.\nAs different client requests may require different numbers of\niterations for processing, requests that have finished earlier\nthan others in the batch cannot return to the client, resulting\nin an increased latency. Requests arrived after dispatching the\nbatch also should wait for processing the batch, which can\nsignificantly increase the requests\u2019 queueing time.\nIn this paper, we propose to schedule the execution of the\nengine at the granularity of iteration instead of request. In\nparticular, the serving system invokes the engine to run only a\nsingle iteration of the model on the batch. As a result, a newly\narrived request can be considered for processing after waiting\nfor only a single iteration of the model. The serving system\nchecks whether a request has finished processing after every\nreturn from the engine \u2013 hence the finished requests can also\nbe returned to the clients immediately.\nNevertheless, a noticeable challenge arises when we at\u0002tempt to apply batching and the iteration-level scheduling at\nthe same time. Unlike the canonical request-level scheduling,\nthe proposed scheduling can issue a batch of requests where\neach request has so far processed a different number of tokens.\nIn such a case, the requests to the Transformer model cannot\nbe processed in the batched manner because the attention\nmechanism calls for non-batchable tensor operations whose\ninput tensors have variable shapes depending on the number\nof processed tokens.\nTo address this challenge, we suggest to apply batching\nonly to a selected set of operations, which we call selective\nbatching. By taking different characteristics of operations into\naccount, selective batching splits the batch and processes each\nrequest individually for the Attention1 operation while apply\u0002ing batching to other operations of the Transformer model.\nWe observe that the decision not to batch the executions of\nthe Attention operation has only a small impact on efficiency.\nSince the Attention operation is not associated with any model\nparameters, applying batching to Attention has no benefit of\nreducing the amount of GPU memory reads by reusing the\nloaded parameters across multiple requests.\nBased on these techniques, we design and implement\nORCA, a distributed serving system for Transformer-based\ngenerative models. In order to handle large-scale models,\n1\nIn some literature the Attention operation has an extended definition that\nincludes linear layers (QKV Linear and Attn Out Linear; Figure 1b). On the\nother hand, we use a narrow definition as described in Figure 1b.\nORCA adopts parallelization strategies including intra-layer\nand inter-layer model parallelism, which were originally de\u0002veloped by training systems [55, 58] for Transformer models.\nWe also devise a new scheduling algorithm for the proposed\niteration-level scheduling, with additional considerations for\nmemory management and pipelined execution across work\u0002ers.\nWe evaluate ORCA using OpenAI GPT-3 [12] models with\nvarious configurations, scaling up to 341B of parameters. The\nresults show that ORCA significantly outperforms FasterTrans\u0002former [4], showing 36.9\u00d7 throughput improvement at the\nsame level of latency. While we use a language model as\na driving example throughout the paper and conduct experi\u0002ments only on language models, generative models in other\ndomains can benefit from our approach as long as the mod\u0002els are based on the Transformer architecture and use the\nautoregressive generation procedure [19, 38, 51, 62].\n2 Background\nWe provide background on the inference procedure of\nGPT [12, 47], a representative example of Transformer-based\ngenerative models that we use throughout this paper, and ML\ninference serving systems.\nInference procedure of GPT. GPT is an autoregressive\nlanguage model based on one of architectural variants of\nTransformer [60]. It takes text as input and produces new text\nas output. In particular, the model receives a sequence of input\ntokens and then completes the sequence by generating subse\u0002q",
          "original_query": "Orca: A distributed serving system for Transformer-Based generative models",
          "cleaned_query": "Orca: A distributed serving system for Transformer-Based generative models",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Unified Phase-Elastic Serving: Combining Prefill/Decode Disaggregation with Elastic Sequence Parallelism\n- Design a serving runtime that simultaneously (a) disaggregates prefill vs. decoding across GPU pools (DistServe) and (b) elastically changes sequence-parallel degree per request and per phase (LoongServe). The key contribution is an online optimizer that chooses *both* phase placement and ESP degree to satisfy joint TTFT/TPOT SLOs under bandwidth constraints, and a prototype showing when hybridization outperforms either approach alone.",
        "Prefix-Aware Prefill Offload: Hydragen-Style Shared-Prefix Attention as a \u201cPrefill Accelerator\u201d Service\n- Build a specialized prefill service that detects shared prefixes across requests (few-shot templates, system prompts, RAG scaffolds) and executes prefix attention using Hydragen\u2019s decomposition, then hands off to standard decoding (possibly disaggregated). The key contribution is an end-to-end pipeline and cache management policy that decides when to route to prefix-accelerated prefill, quantifying TTFT gains and GPU-memory savings under real multi-tenant prompt reuse.",
        "KV Cache Mobility with Fragmentation Guarantees Across Elastic Parallelism\n- Develop a KV-cache layout and migration protocol that supports LoongServe\u2019s ESP resizing while bounding fragmentation and transfer overhead over time. The contribution is an allocator that provides provable (or empirically tight) fragmentation limits plus a \u201cmigration budget\u201d mechanism that throttles ESP changes when network/memory pressure would otherwise spike tail latency.",
        "Modality-Aware Prefill/Decode Disaggregation for Multimodal Models\n- Extend ModServe\u2019s stage decoupling by additionally separating *language prefill* and *language decoding* (DistServe) while keeping vision/audio encoders as independent stages. The contribution is a modality- and phase-aware placement algorithm that co-optimizes (image/video encoding latency + TTFT + TPOT) and reduces head-of-line blocking from image-heavy requests without overprovisioning the LLM backend.",
        "Hierarchical Shared-Context Serving for Long-Context Requests with Partial Reuse\n- Generalize Hydragen\u2019s hierarchical sharing patterns to long-context serving (LoongServe) where requests share some sections (retrieved documents, policy blocks) but diverge elsewhere. The contribution is an automatic \u201ccontext factorization\u201d module that segments prompts into reusable blocks, builds a block-level KV reuse graph, and schedules attention kernels to maximize reuse while remaining exact (no approximation).",
        "Bandwidth-Constrained Placement for Disaggregated Pipelines with Predictive Traffic Bursts\n- Create a predictive controller (using trace-driven features like ModServe\u2019s burstiness findings) that proactively re-places prefill/decoding/stages across racks or GPU islands to avoid network bottlenecks highlighted in DistServe. The key contribution is a burst-aware optimizer that plans phase/stage placement ahead of time (minutes-to-seconds horizon) and a measurable reduction in SLO violations during traffic spikes.",
        "Online Cost-Optimal Kernel Selection: Switching Between Standard Attention and Shared-Prefix Attention per Microbatch\n- Build a runtime that, at each iteration (ORCA-style), chooses attention implementations (standard vs. Hydragen shared-prefix kernels) and parallelism degree (LoongServe ESP) based on observed prefix-sharing, sequence lengths, and GPU utilization. The contribution is a low-overhead decision policy plus instrumentation that shows end-to-end throughput gains on real workloads where prefix sharing and context length vary over time."
      ],
      "generated_ideas_raw": "1. **Unified Phase-Elastic Serving: Combining Prefill/Decode Disaggregation with Elastic Sequence Parallelism**\n   - Design a serving runtime that simultaneously (a) disaggregates prefill vs. decoding across GPU pools (DistServe) and (b) elastically changes sequence-parallel degree per request and per phase (LoongServe). The key contribution is an online optimizer that chooses *both* phase placement and ESP degree to satisfy joint TTFT/TPOT SLOs under bandwidth constraints, and a prototype showing when hybridization outperforms either approach alone.\n\n2. **SLO-Aware Iteration-Level Scheduling for Disaggregated Decoding**\n   - Extend ORCA\u2019s iteration-level scheduling to a DistServe-like architecture where decoding runs on a dedicated GPU pool. The contribution is a decoding scheduler that dynamically reshapes micro-batches every iteration to minimize TPOT tail latency while accounting for KV-cache transfer/availability from prefill GPUs, and an evaluation on mixed workloads (chat + summarization) with strict P90/P99 SLOs.\n\n3. **Prefix-Aware Prefill Offload: Hydragen-Style Shared-Prefix Attention as a \u201cPrefill Accelerator\u201d Service**\n   - Build a specialized prefill service that detects shared prefixes across requests (few-shot templates, system prompts, RAG scaffolds) and executes prefix attention using Hydragen\u2019s decomposition, then hands off to standard decoding (possibly disaggregated). The key contribution is an end-to-end pipeline and cache management policy that decides when to route to prefix-accelerated prefill, quantifying TTFT gains and GPU-memory savings under real multi-tenant prompt reuse.\n\n4. **KV Cache Mobility with Fragmentation Guarantees Across Elastic Parallelism**\n   - Develop a KV-cache layout and migration protocol that supports LoongServe\u2019s ESP resizing while bounding fragmentation and transfer overhead over time. The contribution is an allocator that provides provable (or empirically tight) fragmentation limits plus a \u201cmigration budget\u201d mechanism that throttles ESP changes when network/memory pressure would otherwise spike tail latency.\n\n5. **Modality-Aware Prefill/Decode Disaggregation for Multimodal Models**\n   - Extend ModServe\u2019s stage decoupling by additionally separating *language prefill* and *language decoding* (DistServe) while keeping vision/audio encoders as independent stages. The contribution is a modality- and phase-aware placement algorithm that co-optimizes (image/video encoding latency + TTFT + TPOT) and reduces head-of-line blocking from image-heavy requests without overprovisioning the LLM backend.\n\n6. **Hierarchical Shared-Context Serving for Long-Context Requests with Partial Reuse**\n   - Generalize Hydragen\u2019s hierarchical sharing patterns to long-context serving (LoongServe) where requests share some sections (retrieved documents, policy blocks) but diverge elsewhere. The contribution is an automatic \u201ccontext factorization\u201d module that segments prompts into reusable blocks, builds a block-level KV reuse graph, and schedules attention kernels to maximize reuse while remaining exact (no approximation).\n\n7. **Bandwidth-Constrained Placement for Disaggregated Pipelines with Predictive Traffic Bursts**\n   - Create a predictive controller (using trace-driven features like ModServe\u2019s burstiness findings) that proactively re-places prefill/decoding/stages across racks or GPU islands to avoid network bottlenecks highlighted in DistServe. The key contribution is a burst-aware optimizer that plans phase/stage placement ahead of time (minutes-to-seconds horizon) and a measurable reduction in SLO violations during traffic spikes.\n\n8. **Tail-Latency Fairness in Multi-Tenant Serving via Phase-Specific Admission Control**\n   - Design admission control that treats TTFT and TPOT as separate fairness objectives across tenants, leveraging DistServe\u2019s phase separation and ORCA\u2019s iteration granularity. The contribution is a two-level policy: prefill admission to protect TTFT (interactive users) and decode iteration throttling to protect TPOT (batch users), with formal fairness metrics and experiments showing reduced P99 regressions under adversarial mixes.\n\n9. **Cross-Stage Autoscaling with \u201cToken-Rate Contracts\u201d Between Prefill and Decode Pools**\n   - Introduce an interface where prefill produces KV caches at a negotiated token-rate and decoding consumes them under TPOT constraints, enabling stable autoscaling in disaggregated systems (DistServe/ModServe). The contribution is a control-theoretic or queueing-based autoscaler that avoids oscillations by enforcing contracts (e.g., max KV production rate, max decode backlog) and demonstrates cost reduction while maintaining >90\u201399% SLO attainment.\n\n10. **Online Cost-Optimal Kernel Selection: Switching Between Standard Attention and Shared-Prefix Attention per Microbatch**\n   - Build a runtime that, at each iteration (ORCA-style), chooses attention implementations (standard vs. Hydragen shared-prefix kernels) and parallelism degree (LoongServe ESP) based on observed prefix-sharing, sequence lengths, and GPU utilization. The contribution is a low-overhead decision policy plus instrumentation that shows end-to-end throughput gains on real workloads where prefix sharing and context length vary over time.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Unified Phase-Elastic Serving: Combining Prefill/Decode Disaggregation with Elastic Sequence Parallelism\n- Design a serving runtime that simultaneously (a) disaggregates prefill vs. decoding across GP",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Prefix-Aware Prefill Offload: Hydragen-Style Shared-Prefix Attention as a \u201cPrefill Accelerator\u201d Service\n- Build a specialized prefill service that detects shared prefixes across requests (few-shot tem",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "KV Cache Mobility with Fragmentation Guarantees Across Elastic Parallelism\n- Develop a KV-cache layout and migration protocol that supports LoongServe\u2019s ESP resizing while bounding fragmentation and t",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Modality-Aware Prefill/Decode Disaggregation for Multimodal Models\n- Extend ModServe\u2019s stage decoupling by additionally separating *language prefill* and *language decoding* (DistServe) while keeping ",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Hierarchical Shared-Context Serving for Long-Context Requests with Partial Reuse\n- Generalize Hydragen\u2019s hierarchical sharing patterns to long-context serving (LoongServe) where requests share some se",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Bandwidth-Constrained Placement for Disaggregated Pipelines with Predictive Traffic Bursts\n- Create a predictive controller (using trace-driven features like ModServe\u2019s burstiness findings) that proac",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Online Cost-Optimal Kernel Selection: Switching Between Standard Attention and Shared-Prefix Attention per Microbatch\n- Build a runtime that, at each iteration (ORCA-style), chooses attention implemen",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 48,
      "paper_title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
      "contribution": "The paper introduces a dynamical low-rank training scheme with a novel spectral regularizer that enhances adversarial robustness while achieving significant compression in neural networks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 11158,
      "output_tokens": 995,
      "predecessor_details": [
        {
          "success": true,
          "title": "finding efficient low-rank neural networks via matrix differential ...",
          "url": "https://arxiv.org/abs/2205.13571",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2205.13571** (cs)\n\n\\[Submitted on 26 May 2022 ( [v1](https://arxiv.org/abs/2205.13571v1)), last revised 18 Oct 2022 (this version, v2)\\]\n\n# Title:Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations\n\nAuthors: [Steffen Schotth\u00f6fer](https://arxiv.org/search/cs?searchtype=author&query=Schotth%C3%B6fer,+S), [Emanuele Zangrando](https://arxiv.org/search/cs?searchtype=author&query=Zangrando,+E), [Jonas Kusch](https://arxiv.org/search/cs?searchtype=author&query=Kusch,+J), [Gianluca Ceruti](https://arxiv.org/search/cs?searchtype=author&query=Ceruti,+G), [Francesco Tudisco](https://arxiv.org/search/cs?searchtype=author&query=Tudisco,+F)\n\nView a PDF of the paper titled Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations, by Steffen Schotth\\\\\"ofer and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2205.13571)\n\n> Abstract:Neural networks have achieved tremendous success in a large variety of applications. However, their memory footprint and computational demand can render them impractical in application settings with limited hardware or energy resources. In this work, we propose a novel algorithm to find efficient low-rank subnetworks. Remarkably, these subnetworks are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are significantly reduced. The main idea is to restrict the weight matrices to a low-rank manifold and to update the low-rank factors rather than the full matrix during training. To derive training updates that are restricted to the prescribed manifold, we employ techniques from dynamic model order reduction for matrix differential equations. This allows us to provide approximation, stability, and descent guarantees. Moreover, our method automatically and dynamically adapts the ranks during training to achieve the desired approximation accuracy. The efficiency of the proposed method is demonstrated through a variety of numerical experiments on fully-connected and convolutional networks.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Numerical Analysis (math.NA); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2205.13571](https://arxiv.org/abs/2205.13571) \\[cs.LG\\] |\n| | (or [arXiv:2205.13571v2](https://arxiv.org/abs/2205.13571v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2205.13571](https://doi.org/10.48550/arXiv.2205.13571) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Proceedings NeurIPS 2022 |\n\n## Submission history\n\nFrom: Francesco Tudisco \\[ [view email](https://arxiv.org/show-email/3954d9c7/2205.13571)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2205.13571v1)**\nThu, 26 May 2022 18:18:12 UTC (3,764 KB)\n\n**\\[v2\\]**\nTue, 18 Oct 2022 12:22:36 UTC (2,118 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations, by Steffen Schotth\\\\\"ofer and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2205.13571)\n- [TeX Source](https://arxiv.org/src/2205.13571)\n- [Other Formats](https://arxiv.org/format/2205.13571)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2205.13571&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2205.13571&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-05](https://arxiv.org/list/cs.LG/2022-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2205.13571?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2205.13571?context=cs.AI)\n\n[cs.NA](https://arxiv.org/abs/2205.13571?context=cs.NA)\n\n[math](https://arxiv.org/abs/2205.13571?context=math)\n\n[math.NA](https://arxiv.org/abs/2205.13571?context=math.NA)\n\n[stat](https://arxiv.org/abs/2205.13571?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2205.13571?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2205.13571)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2205.13571)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2205.13571)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2205.13571&description=Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2205.13571&title=Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2205.13571) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations",
          "cleaned_query": "Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Theoretically Principled Trade-off between Robustness and Accuracy",
          "url": "https://arxiv.org/abs/1901.08573",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1901.08573** (cs)\n\n\\[Submitted on 24 Jan 2019 ( [v1](https://arxiv.org/abs/1901.08573v1)), last revised 24 Jun 2019 (this version, v3)\\]\n\n# Title:Theoretically Principled Trade-off between Robustness and Accuracy\n\nAuthors: [Hongyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+H), [Yaodong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+Y), [Jiantao Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao,+J), [Eric P. Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing,+E+P), [Laurent El Ghaoui](https://arxiv.org/search/cs?searchtype=author&query=Ghaoui,+L+E), [Michael I. Jordan](https://arxiv.org/search/cs?searchtype=author&query=Jordan,+M+I)\n\nView a PDF of the paper titled Theoretically Principled Trade-off between Robustness and Accuracy, by Hongyang Zhang and Yaodong Yu and Jiantao Jiao and Eric P. Xing and Laurent El Ghaoui and Michael I. Jordan\n\n[View PDF](https://arxiv.org/pdf/1901.08573)\n\n> Abstract:We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\\\\%$ in terms of mean $\\\\ell\\_2$ perturbation distance.\n\n| | |\n| --- | --- |\n| Comments: | Appeared in ICML 2019; the winning methodology of the NeurIPS 2018 Adversarial Vision Challenge |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1901.08573](https://arxiv.org/abs/1901.08573) \\[cs.LG\\] |\n| | (or [arXiv:1901.08573v3](https://arxiv.org/abs/1901.08573v3) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1901.08573](https://doi.org/10.48550/arXiv.1901.08573) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hongyang Zhang \\[ [view email](https://arxiv.org/show-email/9640e8c4/1901.08573)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1901.08573v1)**\nThu, 24 Jan 2019 18:43:57 UTC (1,310 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1901.08573v2)**\nThu, 23 May 2019 22:04:23 UTC (1,312 KB)\n\n**\\[v3\\]**\nMon, 24 Jun 2019 07:04:11 UTC (1,313 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Theoretically Principled Trade-off between Robustness and Accuracy, by Hongyang Zhang and Yaodong Yu and Jiantao Jiao and Eric P. Xing and Laurent El Ghaoui and Michael I. Jordan\n\n- [View PDF](https://arxiv.org/pdf/1901.08573)\n- [TeX Source](https://arxiv.org/src/1901.08573)\n- [Other Formats](https://arxiv.org/format/1901.08573)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1901.08573&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1901.08573&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2019-01](https://arxiv.org/list/cs.LG/2019-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1901.08573?context=cs)\n\n[stat](https://arxiv.org/abs/1901.08573?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1901.08573?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1901.08573)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1901.08573)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1901.08573)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1901.html#abs-1901-08573) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1901-08573)\n\n[Hongyang Zhang](https://dblp.uni-trier.de/search/author?author=Hongyang%20Zhang)\n\n[Yaodong Yu](https://dblp.uni-trier.de/search/author?author=Yaodong%20Yu)\n\n[Jiantao Jiao](https://dblp.uni-trier.de/search/author?author=Jiantao%20Jiao)\n\n[Eric P. Xing](https://dblp.uni-trier.de/search/author?author=Eric%20P.%20Xing)\n\n[Laurent El Ghaoui](https://dblp.uni-trier.de/search/author?author=Laurent%20El%20Ghaoui)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1901.08573&description=Theoretically Principled Trade-off between Robustness and Accuracy) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1901.08573&title=Theoretically Principled Trade-off between Robustness and Accuracy)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1901.08573) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Theoretically principled trade-off between robustness and accuracy",
          "cleaned_query": "Theoretically principled trade-off between robustness and accuracy",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "A Defense Framework for Remote Sensing Image Scene Classification",
          "url": "https://ieeexplore.ieee.org/document/9442932/",
          "content": "Perturbation-Seeking Generative Adversarial Networks: A Defense Framework for Remote Sensing Image Scene Classification | IEEE Journals &amp; Magazine | IEEE Xplore\n[**]()\n[](https://ieeexplore.ieee.org/rest/api/hpdata)\n### IEEE Account\n* [Change Username/Password]()\n* [Update Address]()\n### Purchase Details\n* [Payment Options]()\n* [Order History]()\n* [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n### Profile Information\n* [Communications Preferences]()\n* [Profession and Education]()\n* [Technical Interests]()\n### Need Help?\n* **US &amp; Canada:**+1 800 678 4333\n* **Worldwide:**+1 732 981 0060\n* [Contact &amp; Support](https://ieeexplore.ieee.org/xpl/contact)\n* [About IEEE*Xplore*](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n* [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n* [Help](https://ieeexplore.ieee.org/Xplorehelp)\n* [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n* [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n* [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n* [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n* [Privacy &amp; Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n&copy; Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.\n**",
          "original_query": "Perturbation-seeking generative adversarial networks: A defense framework for remote sensing image scene classification",
          "cleaned_query": "Perturbation-seeking generative adversarial networks: A defense framework for remote sensing image scene classification",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Explaining and Harnessing Adversarial Ex...",
          "url": "https://axi.lims.ac.uk/paper/1412.6572",
          "content": "Toggle theme\n\nID: 1412.6572\n\nSearch\n\n# Explaining and Harnessing Adversarial Examples\n\nDecember 20, 2014\n\n[View on ArXiv](https://arxiv.org/abs/1412.6572)\n\nIan J. Goodfellow, Jonathon Shlens, Christian Szegedy\n\nStatistics\n\nComputer Science\n\nMachine Learning\n\nMachine Learning\n\nSeveral machine learning models, including neural networks, consistently\nmisclassify adversarial examples---inputs formed by applying small but\nintentionally worst-case perturbations to examples from the dataset, such that\nthe perturbed input results in the model outputting an incorrect answer with\nhigh confidence. Early attempts at explaining this phenomenon focused on\nnonlinearity and overfitting. We argue instead that the primary cause of neural\nnetworks' vulnerability to adversarial perturbation is their linear nature.\nThis explanation is supported by new quantitative results while giving the\nfirst explanation of the most intriguing fact about them: their generalization\nacross architectures and training sets. Moreover, this view yields a simple and\nfast method of generating adversarial examples. Using this approach to provide\nexamples for adversarial training, we reduce the test set error of a maxout\nnetwork on the MNIST dataset.\n\nSimilar papers1\n\n## A Unified Gradient Regularization Family for Adversarial Examples\n\nNovember 19, 2015\n\n92% Match\n\nChunchuan Lyu, Kaizhu Huang, Hai-Ning Liang\n\nMachine Learning\n\nMachine Learning\n\nAdversarial examples are augmented data points generated by imperceptible\nperturbation of input samples. They have recently drawn much attention with the\nmachine learning and data mining community. Being difficult to distinguish from\nreal examples, such adversarial examples could change the prediction of many of\nthe best learning models including the state-of-the-art deep learning models.\nRecent attempts have been made to build robust models that take into account\nadversarial...\n\nFind Similar [View on arXiv](https://arxiv.org/abs/1511.06385)\n\n## How adversarial attacks can disrupt seemingly stable accurate classifiers\n\nSeptember 7, 2023\n\n91% Match\n\nOliver J. Sutton, Qinghua Zhou, Ivan Y. Tyukin, Alexander N. Gorban, ... , Higham Desmond J.\n\nMachine Learning\n\nArtificial Intelligence\n\nAdversarial attacks dramatically change the output of an otherwise accurate\nlearning system using a seemingly inconsequential modification to a piece of\ninput data. Paradoxically, empirical evidence indicates that even systems which\nare robust to large random perturbations of the input data remain susceptible\nto small, easily constructed, adversarial perturbations of their inputs. Here,\nwe show that this may be seen as a fundamental feature of classifiers working\nwith high di...\n\nFind Similar [View on arXiv](https://arxiv.org/abs/2309.03665)\n\n## Verifying the Causes of Adversarial Examples\n\nOctober 19, 2020\n\n91% Match\n\nHonglin Li, Yifei Fan, Frieder Ganz, ... , Barnaghi Payam\n\nMachine Learning\n\nThe robustness of neural networks is challenged by adversarial examples that\ncontain almost imperceptible perturbations to inputs, which mislead a\nclassifier to incorrect outputs in high confidence. Limited by the extreme\ndifficulty in examining a high-dimensional image space thoroughly, research on\nexplaining and justifying the causes of adversarial examples falls behind\nstudies on attacks and defenses. In this paper, we present a collection of\npotential causes of adversaria...\n\nFind Similar [View on arXiv](https://arxiv.org/abs/2010.09633)\n\n## The Limitations of Deep Learning in Adversarial Settings\n\nNovember 24, 2015\n\n91% Match\n\nNicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, ... , Swami Ananthram\n\nCryptography and Security\n\nMachine Learning\n\nNeural and Evolutionary Comp...\n\nMachine Learning\n\nDeep learning takes advantage of large datasets and computationally efficient\ntraining algorithms to outperform other approaches at various machine learning\ntasks. However, imperfections in the training phase of deep neural networks\nmake them vulnerable to adversarial samples: inputs crafted by adversaries with\nthe intent of causing deep neural networks to misclassify. In this work, we\nformalize the space of adversaries against deep neural networks (DNNs) and\nintroduce a nove...\n\nFind Similar [View on arXiv](https://arxiv.org/abs/1511.07528)\n\n## Improving Back-Propagation by Adding an Adversarial Gradient\n\nOctober 14, 2015\n\n91% Match\n\nArild N\u00f8kland\n\nMachine Learning\n\nMachine Learning\n\nThe back-propagation algorithm is widely used for learning in artificial\nneural networks. A challenge in machine learning is to create models that\ngeneralize to new data samples not seen in the training data. Recently, a\ncommon flaw in several machine learning algorithms was discovered: small\nperturbations added to the input data lead to consistent misclassification of\ndata samples. Samples that easily mislead the model are called adversarial\nexamples. Training a \"maxout\" net...\n\nFind Similar [View on arXiv](https://arxiv.org/abs/1510.04189)\n\n## Adversarial Margin Maximization Networks\n\nNovember 14, 2019\n\n91% Match\n\nZiang Yan, Yiwen Guo, Changshui Zhang\n\nMachine Learning\n\nComputer Vision and Pattern ...\n\nNeural and Evolutionary Comp...\n\nMachine Learning\n\nThe tremendous recent success of deep neural networks (DNNs) has sparked a\nsurge of interest in understanding their predictive ability. Unlike the human\nvisual system which is able to generalize robustly and learn with little\nsupervision, DNNs normally require a massive amount of data to learn new\nconcepts. In addition, research works also show that DNNs are vulnerable to\nadversarial examples-maliciously generated images which seem perceptually\nsimilar to the natural ones but...\n\nFind Similar [View on arXiv](https://arxiv.org/abs/1911.05916)\n\n## Exploring the Space of Adversarial Images\n\nOctober 19, 2015\n\n91% Match\n\nPedro Tabacof, Eduardo Valle\n\nNeural and Evolutionary Comp...\n\nAdversarial examples have raised questions regarding the robustness and\nsecurity of deep neural networks. In this work we formalize the problem of\nadversarial images given a pretrained classifier, showing that even in the\nlinear case the resulting optimization problem is nonconvex. We generate\nadversarial images using shallow and deep classifiers on the MNIST and ImageNet\ndatasets. We probe the pixel space of adversarial images using noise of varying\nintensity and distributio...\n\nFind Similar [View on arXiv](https://arxiv.org/abs/1510.05328)\n\n## Adversarial Examples - A Complete Characterisation of the Phenomenon\n\nOctober 2, 2018\n\n90% Match\n\nAlexandru Constantin Serban, Erik Poll, Joost Visser\n\nComputer Vision and Pattern ...\n\nCryptography and Security\n\nMachine Learning\n\nNeural and Evolutionary Comp...\n\nWe provide a complete characterisation of the phenomenon of adversarial\nexamples - inputs intentionally crafted to fool machine learning models. We aim\nto cover all the important concerns in this field of study: (1) the conjectures\non the existence of adversarial examples, (2) the security, safety and\nrobustness implications, (3) the methods used to generate and (4) protect\nagainst adversarial examples and (5) the ability of adversarial examples to\ntransfer between different ...\n\nFind Similar [View on arXiv](https://arxiv.org/abs/1810.01185)\n\n## How the Softmax Output is Misleading for Evaluating the Strength of Adversarial Examples\n\nNovember 21, 2018\n\n90% Match\n\nUtku Ozbulak, Neve Wesley De, Messem Arnout Van\n\nMachine Learning\n\nMachine Learning\n\nEven before deep learning architectures became the de facto models for\ncomplex computer vision tasks, the softmax function was, given its elegant\nproperties, already used to analyze the predictions of feedforward neural\nnetworks. Nowadays, the output of the softmax function is also commonly used to\nassess the strength of adversarial examples: malicious data points designed to\nfail machine learning models during the testing phase. However, in this paper,\nwe show that it is pos...\n\nFind Similar [View on arXiv](https://arxiv.org/abs/1811.08577)\n\n## Provable Robustness of ReLU networks via Maximization of Linear Regions\n\nOctober 17, 2018\n\n90% Match\n\nFrancesco University of T\u00fcbingen Croce, Maksym Saarland University Andriushchenko, Matthias University of T\u00fcbingen Hein\n\nMachine Learning\n\nMachine Learning\n\nIt has been shown that neural network classifiers are not robust. This raises\nconcerns about their usage in safety-critical systems. We propose in this paper\na regularization scheme for ReLU networks which provably improves the\nrobustness of the classifier by maximizing the linear regions of the classifier\nas well as the distance to the decision boundary. Our techniques allow even to\nfind the minimal adversarial perturbation for a fraction of test points for\nlarge networks. I...\n\nFind Similar [View on arXiv](https://arxiv.org/abs/1810.07481)",
          "original_query": "Explaining and harnessing adversarial examples",
          "cleaned_query": "Explaining and harnessing adversarial examples",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Theoretically Principled Trade-off between Robustness and Accuracy",
          "url": "https://arxiv.org/abs/1901.08573",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1901.08573** (cs)\n\n\\[Submitted on 24 Jan 2019 ( [v1](https://arxiv.org/abs/1901.08573v1)), last revised 24 Jun 2019 (this version, v3)\\]\n\n# Title:Theoretically Principled Trade-off between Robustness and Accuracy\n\nAuthors: [Hongyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+H), [Yaodong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+Y), [Jiantao Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao,+J), [Eric P. Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing,+E+P), [Laurent El Ghaoui](https://arxiv.org/search/cs?searchtype=author&query=Ghaoui,+L+E), [Michael I. Jordan](https://arxiv.org/search/cs?searchtype=author&query=Jordan,+M+I)\n\nView a PDF of the paper titled Theoretically Principled Trade-off between Robustness and Accuracy, by Hongyang Zhang and Yaodong Yu and Jiantao Jiao and Eric P. Xing and Laurent El Ghaoui and Michael I. Jordan\n\n[View PDF](https://arxiv.org/pdf/1901.08573)\n\n> Abstract:We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\\\\%$ in terms of mean $\\\\ell\\_2$ perturbation distance.\n\n| | |\n| --- | --- |\n| Comments: | Appeared in ICML 2019; the winning methodology of the NeurIPS 2018 Adversarial Vision Challenge |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1901.08573](https://arxiv.org/abs/1901.08573) \\[cs.LG\\] |\n| | (or [arXiv:1901.08573v3](https://arxiv.org/abs/1901.08573v3) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1901.08573](https://doi.org/10.48550/arXiv.1901.08573) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hongyang Zhang \\[ [view email](https://arxiv.org/show-email/9640e8c4/1901.08573)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1901.08573v1)**\nThu, 24 Jan 2019 18:43:57 UTC (1,310 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1901.08573v2)**\nThu, 23 May 2019 22:04:23 UTC (1,312 KB)\n\n**\\[v3\\]**\nMon, 24 Jun 2019 07:04:11 UTC (1,313 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Theoretically Principled Trade-off between Robustness and Accuracy, by Hongyang Zhang and Yaodong Yu and Jiantao Jiao and Eric P. Xing and Laurent El Ghaoui and Michael I. Jordan\n\n- [View PDF](https://arxiv.org/pdf/1901.08573)\n- [TeX Source](https://arxiv.org/src/1901.08573)\n- [Other Formats](https://arxiv.org/format/1901.08573)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1901.08573&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1901.08573&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2019-01](https://arxiv.org/list/cs.LG/2019-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1901.08573?context=cs)\n\n[stat](https://arxiv.org/abs/1901.08573?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1901.08573?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1901.08573)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1901.08573)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1901.08573)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1901.html#abs-1901-08573) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1901-08573)\n\n[Hongyang Zhang](https://dblp.uni-trier.de/search/author?author=Hongyang%20Zhang)\n\n[Yaodong Yu](https://dblp.uni-trier.de/search/author?author=Yaodong%20Yu)\n\n[Jiantao Jiao](https://dblp.uni-trier.de/search/author?author=Jiantao%20Jiao)\n\n[Eric P. Xing](https://dblp.uni-trier.de/search/author?author=Eric%20P.%20Xing)\n\n[Laurent El Ghaoui](https://dblp.uni-trier.de/search/author?author=Laurent%20El%20Ghaoui)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1901.08573&description=Theoretically Principled Trade-off between Robustness and Accuracy) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1901.08573&title=Theoretically Principled Trade-off between Robustness and Accuracy)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1901.08573) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Theoretically principled trade-off between robustness and accuracy",
          "cleaned_query": "Theoretically principled trade-off between robustness and accuracy",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Defense to Adversarial Perturbations with GAN",
          "url": "https://arxiv.org/abs/1705.03387",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: We propose a novel technique to make neural network robust to adversarial\nexamples using a generative adversarial network. We alternately train both\nclassifier and generator networks. The generator network generates an\nadversarial perturbation that can easily fool the classifier network by using a\ngradient of each image. Simultaneously, the classifier network is trained to\nclassify correctly both original and adversarial images generated by the\ngenerator. These procedures help the classifier network to become more robust\nto adversarial perturbations. Furthermore, our adversarial training framework\nefficiently reduces overfitting and outperforms other regularization methods\nsuch as Dropout. We applied our method to supervised learning for CIFAR\ndatasets, and experimantal results show that our method significantly lowers\nthe generalization error of the network. To the best of our knowledge, this is\nthe first method which uses GAN to improve supervised learning.\n \n \n \n \n Submission history From: Hyeungill Lee [ view email]\n \n [v1] \n Tue, 9 May 2017 15:30:58 UTC (405 KB) [v2] \nFri, 26 May 2017 21:44:32 UTC (405 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > cs > arXiv:1705.03387\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:1705.03387 (cs)\n [Submitted on 9 May 2017 (v1), last revised 26 May 2017 (this version, v2)]\n\n Title: Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN\n\n Authors: Hyeungill Lee, Sungyeob Han, Jungwoo Lee\n Download PDF\n Abstract: We propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network. We alternately train both classifier and generator networks. The generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image. Simultaneously, the classifier network is trained to classify correctly both original and adversarial images generated by the generator. These procedures help the classifier network to become more robust to adversarial perturbations. Furthermore, our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as Dropout. We applied our method to supervised learning for CIFAR datasets, and experimantal results show that our method significantly lowers the generalization error of the network. To the best of our knowledge, this is the first method which uses GAN to improve supervised learning.\n Subjects: Machine Learning (cs.LG) ; Machine Learning (stat.ML)\n Cite as: arXiv:1705.03387 [cs.LG] \n (or arXiv:1705.03387v2 [cs.LG] for this version) \n https://doi.org/10.48550/arXiv.1705.03387 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Hyeungill Lee [view email]\n [v1] Tue, 9 May 2017 15:30:58 UTC (405 KB)\n [v2] Fri, 26 May 2017 21:44:32 UTC (405 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n (license)\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 1705\n Change to browse by:\n cs\n stat\n stat.ML\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n\n DBLP - CS Bibliography\n\n listing | bibtex\n Hyeungill Lee\n Sungyeob Han\n Jungwoo Lee\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Generative adversarial trainer: Defense to adversarial perturbations with GAN",
          "cleaned_query": "Generative adversarial trainer: Defense to adversarial perturbations with GAN",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "TRADES-Consistent Dynamic Rank Adaptation for Robust Low-Rank Networks\nCombine the low-rank manifold training with TRADES by adding a robustness-aware criterion to the rank-adaptation rule (e.g., increase rank where boundary error concentrates). Implement an algorithm that adjusts per-layer rank based on robust loss curvature or boundary error estimates during training, and benchmark robust accuracy vs. FLOPs against fixed-rank and full-rank TRADES baselines.",
        "Boundary-Error\u2013Driven Layerwise Low-Rank Allocation\nUse TRADES\u2019 decomposition (natural error + boundary error) to define a per-layer \u201cboundary sensitivity\u201d score (e.g., layer contribution to input-gradient norm or adversarial logit margin). Allocate a global rank budget across layers to minimize boundary error under a compute constraint, yielding a principled alternative to heuristic compression schedules for adversarially trained models.",
        "Low-Rank Adversarial Example Generator (Manifold-Constrained GAN Attacker)\nModify GAN-based adversarial trainers so the generator produces perturbations constrained to a low-dimensional subspace (low-rank in patch/tile space or in a learned feature basis), matching realistic structured distortions. Evaluate whether training against such structured perturbations improves robustness to distribution-relevant attacks (e.g., cloud streaks, compression artifacts) while reducing adversarial training cost.",
        "Rank-Regularized TRADES via Matrix Differential Equation Stability Bounds\nLeverage the stability/descent guarantees from matrix differential equation\u2013based low-rank updates to derive a rank-regularized TRADES objective with explicit stability constraints (e.g., controlling spectral norms of low-rank factors). Provide both an implementable optimizer and a theoretical bound linking rank, spectral control, and boundary error, then test whether it reduces the robust-accuracy drop observed in standard TRADES.",
        "Robustness\u2013Efficiency Pareto Fronts via Joint Rank and TRADES Hyperparameter Continuation\nCreate a continuation method that jointly anneals TRADES\u2019 trade-off parameter and the network\u2019s effective rank during training (starting high-rank/high-accuracy, moving toward low-rank/robust). The contribution is a practical procedure to trace the Pareto frontier of (robust accuracy, natural accuracy, compute) in a single training run rather than many separate sweeps.",
        "Adversarially Robust Low-Rank Tickets for Transfer and Fine-Tuning\nExtend \u201clow-rank lottery tickets\u201d to the adversarial setting by finding subnetworks (low-rank factors + rank schedules) that retain robustness when transferred across datasets or domains. Concretely: identify robust low-rank tickets on a source dataset with TRADES or GAN-based adversarial training, then evaluate whether freezing or lightly adapting the low-rank factors preserves robustness under target-domain shifts.",
        "Remote Sensing\u2013Specific Robust Low-Rank CNNs with Physically Plausible Perturbations\nBuild a defense pipeline for remote sensing scene classification where adversarial examples are generated using physically motivated constraints (illumination changes, haze, sensor noise), and the classifier is trained on a low-rank manifold for efficiency. The key deliverable is an end-to-end method that couples structured attack generation (GAN or constrained gradients) with dynamic rank adaptation to fit edge/onboard compute limits.",
        "Certified Robustness Bounds for Low-Rank Factorized Networks\nDevelop Lipschitz/spectral-based robustness certificates tailored to low-rank factorized layers (e.g., bounding operator norms using factor norms and ranks). Provide an implementable certification routine that is faster than full-matrix bounds, and study how dynamic rank adaptation affects certifiable radii compared to standard (full-rank) certified training.",
        "Adversarial Training with Rank-Sparse Updates (Compute-Optimal Robust Optimization)\nDuring adversarial training, restrict not only weights but also *updates* to be low-rank (or rank-sparse) using matrix ODE-inspired projection steps. This targets the high cost of adversarial training by reducing backward-pass memory/compute, and enables controlled experiments on whether robustness emerges from a few dominant update directions.",
        "Geometry of Adversarial Vulnerability Under Low-Rank Manifold Constraints\nEmpirically and theoretically analyze how constraining weights to a low-rank manifold changes the linearity-based explanation of adversarial examples (e.g., logit sensitivity directions, gradient alignment across architectures). Produce measurable predictions\u2014such as changes in gradient concentration or transferability\u2014that can be validated on standard datasets, connecting Goodfellow-style linearity arguments with manifold-constrained training dynamics."
      ],
      "generated_ideas_raw": "1. **TRADES-Consistent Dynamic Rank Adaptation for Robust Low-Rank Networks**  \n   Combine the low-rank manifold training with TRADES by adding a robustness-aware criterion to the rank-adaptation rule (e.g., increase rank where boundary error concentrates). Implement an algorithm that adjusts per-layer rank based on robust loss curvature or boundary error estimates during training, and benchmark robust accuracy vs. FLOPs against fixed-rank and full-rank TRADES baselines.\n\n2. **Boundary-Error\u2013Driven Layerwise Low-Rank Allocation**  \n   Use TRADES\u2019 decomposition (natural error + boundary error) to define a per-layer \u201cboundary sensitivity\u201d score (e.g., layer contribution to input-gradient norm or adversarial logit margin). Allocate a global rank budget across layers to minimize boundary error under a compute constraint, yielding a principled alternative to heuristic compression schedules for adversarially trained models.\n\n3. **Low-Rank Adversarial Example Generator (Manifold-Constrained GAN Attacker)**  \n   Modify GAN-based adversarial trainers so the generator produces perturbations constrained to a low-dimensional subspace (low-rank in patch/tile space or in a learned feature basis), matching realistic structured distortions. Evaluate whether training against such structured perturbations improves robustness to distribution-relevant attacks (e.g., cloud streaks, compression artifacts) while reducing adversarial training cost.\n\n4. **Rank-Regularized TRADES via Matrix Differential Equation Stability Bounds**  \n   Leverage the stability/descent guarantees from matrix differential equation\u2013based low-rank updates to derive a rank-regularized TRADES objective with explicit stability constraints (e.g., controlling spectral norms of low-rank factors). Provide both an implementable optimizer and a theoretical bound linking rank, spectral control, and boundary error, then test whether it reduces the robust-accuracy drop observed in standard TRADES.\n\n5. **Robustness\u2013Efficiency Pareto Fronts via Joint Rank and TRADES Hyperparameter Continuation**  \n   Create a continuation method that jointly anneals TRADES\u2019 trade-off parameter and the network\u2019s effective rank during training (starting high-rank/high-accuracy, moving toward low-rank/robust). The contribution is a practical procedure to trace the Pareto frontier of (robust accuracy, natural accuracy, compute) in a single training run rather than many separate sweeps.\n\n6. **Adversarially Robust Low-Rank Tickets for Transfer and Fine-Tuning**  \n   Extend \u201clow-rank lottery tickets\u201d to the adversarial setting by finding subnetworks (low-rank factors + rank schedules) that retain robustness when transferred across datasets or domains. Concretely: identify robust low-rank tickets on a source dataset with TRADES or GAN-based adversarial training, then evaluate whether freezing or lightly adapting the low-rank factors preserves robustness under target-domain shifts.\n\n7. **Remote Sensing\u2013Specific Robust Low-Rank CNNs with Physically Plausible Perturbations**  \n   Build a defense pipeline for remote sensing scene classification where adversarial examples are generated using physically motivated constraints (illumination changes, haze, sensor noise), and the classifier is trained on a low-rank manifold for efficiency. The key deliverable is an end-to-end method that couples structured attack generation (GAN or constrained gradients) with dynamic rank adaptation to fit edge/onboard compute limits.\n\n8. **Certified Robustness Bounds for Low-Rank Factorized Networks**  \n   Develop Lipschitz/spectral-based robustness certificates tailored to low-rank factorized layers (e.g., bounding operator norms using factor norms and ranks). Provide an implementable certification routine that is faster than full-matrix bounds, and study how dynamic rank adaptation affects certifiable radii compared to standard (full-rank) certified training.\n\n9. **Adversarial Training with Rank-Sparse Updates (Compute-Optimal Robust Optimization)**  \n   During adversarial training, restrict not only weights but also *updates* to be low-rank (or rank-sparse) using matrix ODE-inspired projection steps. This targets the high cost of adversarial training by reducing backward-pass memory/compute, and enables controlled experiments on whether robustness emerges from a few dominant update directions.\n\n10. **Geometry of Adversarial Vulnerability Under Low-Rank Manifold Constraints**  \n   Empirically and theoretically analyze how constraining weights to a low-rank manifold changes the linearity-based explanation of adversarial examples (e.g., logit sensitivity directions, gradient alignment across architectures). Produce measurable predictions\u2014such as changes in gradient concentration or transferability\u2014that can be validated on standard datasets, connecting Goodfellow-style linearity arguments with manifold-constrained training dynamics.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "TRADES-Consistent Dynamic Rank Adaptation for Robust Low-Rank Networks\nCombine the low-rank manifold training with TRADES by adding a robustness-aware criterion to the rank-adaptation rule (e.g., incr",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Boundary-Error\u2013Driven Layerwise Low-Rank Allocation\nUse TRADES\u2019 decomposition (natural error + boundary error) to define a per-layer \u201cboundary sensitivity\u201d score (e.g., layer contribution to input-gra",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Low-Rank Adversarial Example Generator (Manifold-Constrained GAN Attacker)\nModify GAN-based adversarial trainers so the generator produces perturbations constrained to a low-dimensional subspace (low-",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Rank-Regularized TRADES via Matrix Differential Equation Stability Bounds\nLeverage the stability/descent guarantees from matrix differential equation\u2013based low-rank updates to derive a rank-regularize",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Robustness\u2013Efficiency Pareto Fronts via Joint Rank and TRADES Hyperparameter Continuation\nCreate a continuation method that jointly anneals TRADES\u2019 trade-off parameter and the network\u2019s effective rank",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Adversarially Robust Low-Rank Tickets for Transfer and Fine-Tuning\nExtend \u201clow-rank lottery tickets\u201d to the adversarial setting by finding subnetworks (low-rank factors + rank schedules) that retain r",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Remote Sensing\u2013Specific Robust Low-Rank CNNs with Physically Plausible Perturbations\nBuild a defense pipeline for remote sensing scene classification where adversarial examples are generated using phy",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Certified Robustness Bounds for Low-Rank Factorized Networks\nDevelop Lipschitz/spectral-based robustness certificates tailored to low-rank factorized layers (e.g., bounding operator norms using factor",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Adversarial Training with Rank-Sparse Updates (Compute-Optimal Robust Optimization)\nDuring adversarial training, restrict not only weights but also *updates* to be low-rank (or rank-sparse) using matr",
          "is_match": true
        },
        {
          "idea_idx": 9,
          "idea_text": "Geometry of Adversarial Vulnerability Under Low-Rank Manifold Constraints\nEmpirically and theoretically analyze how constraining weights to a low-rank manifold changes the linearity-based explanation ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 49,
      "paper_title": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training",
      "contribution": "QoQ-Med is the first open generalist clinical foundation model that effectively reasons across heterogeneous clinical data types.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10482,
      "output_tokens": 1105,
      "predecessor_details": [
        {
          "success": true,
          "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via ...",
          "url": "https://arxiv.org/abs/2501.12948",
          "content": "[2501.12948] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2501.12948\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2501.12948**(cs)\n[Submitted on 22 Jan 2025]\n# Title:DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\nAuthors:[DeepSeek-AI](https://arxiv.org/search/cs?searchtype=author&amp;query=DeepSeek-AI),[Daya Guo](https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+D),[Dejian Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+D),[Haowei Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H),[Junxiao Song](https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+J),[Ruoyu Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+R),[Runxin Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+R),[Qihao Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+Q),[Shirong Ma](https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+S),[Peiyi Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+P),[Xiao Bi](https://arxiv.org/search/cs?searchtype=author&amp;query=Bi,+X),[Xiaokang Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X),[Xingkai Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+X),[Yu Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Y),[Z.F. Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Z),[Zhibin Gou](https://arxiv.org/search/cs?searchtype=author&amp;query=Gou,+Z),[Zhihong Shao](https://arxiv.org/search/cs?searchtype=author&amp;query=Shao,+Z),[Zhuoshu Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Z),[Ziyi Gao](https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+Z),[Aixin Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+A),[Bing Xue](https://arxiv.org/search/cs?searchtype=author&amp;query=Xue,+B),[Bingxuan Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+B),[Bochao Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+B),[Bei Feng](https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+B),[Chengda Lu](https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+C),[Chenggang Zhao](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+C),[Chengqi Deng](https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+C),[Chenyu Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+C),[Chong Ruan](https://arxiv.org/search/cs?searchtype=author&amp;query=Ruan,+C),[Damai Dai](https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+D),[Deli Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+D),[Dongjie Ji](https://arxiv.org/search/cs?searchtype=author&amp;query=Ji,+D),[Erhang Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+E),[Fangyun Lin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+F),[Fucong Dai](https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+F),[Fuli Luo](https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+F),[Guangbo Hao](https://arxiv.org/search/cs?searchtype=author&amp;query=Hao,+G),[Guanting Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+G),[Guowei Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+G),[H. Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H),[Han Bao](https://arxiv.org/search/cs?searchtype=author&amp;query=Bao,+H),[Hanwei Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+H),[Haocheng Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+H),[Honghui Ding](https://arxiv.org/search/cs?searchtype=author&amp;query=Ding,+H),[Huajian Xin](https://arxiv.org/search/cs?searchtype=author&amp;query=Xin,+H),[Huazuo Gao](https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+H),[Hui Qu](https://arxiv.org/search/cs?searchtype=author&amp;query=Qu,+H),[Hui Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+H),[Jianzhong Guo](https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+J),[Jiashi Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J),[Jiawei Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J),[Jingchang Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J),[Jingyang Yuan](https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+J),[Junjie Qiu](https://arxiv.org/search/cs?searchtype=author&amp;query=Qiu,+J),[Junlong Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J),[J.L. Cai](https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+J),[Jiaqi Ni](https://arxiv.org/search/cs?searchtype=author&amp;query=Ni,+J),[Jian Liang](https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+J),[Jin Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J),[Kai Dong](https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+K),[Kai Hu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+K),[Kaige Gao](https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+K),[Kang Guan](https://arxiv.org/search/cs?searchtype=author&amp;query=Guan,+K),[Kexin Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+K),[Kuai Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+K),[Lean Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L),[Lecong Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+L),[Liang Zhao](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+L),[Litong Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L),[Liyue Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+L),[Lei Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+L),[Leyi Xia](https://arxiv.org/search/cs?searchtype=author&amp;query=Xia,+L),[Mingchuan Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+M),[Minghua Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+M),[Minghui Tang](https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+M),[Meng Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+M),[Miaojun Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+M),[Mingming Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+M),[Ning Tian](https://arxiv.org/search/cs?searchtype=author&amp;query=Tian,+N),[Panpan Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+P),[Peng Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+P),[Qiancheng Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Q),[Qinyu Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Q),[Qiushi Du](https://arxiv.org/search/cs?searchtype=author&amp;query=Du,+Q),[Ruiqi Ge](https://arxiv.org/search/cs?searchtype=author&amp;query=Ge,+R),[Ruisong Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+R),[Ruizhe Pan](https://arxiv.org/search/cs?searchtype=author&amp;query=Pan,+R),[Runji Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+R),[R.J. Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+R),[R.L. Jin](https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+R),[Ruyi Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+R),[Shanghao Lu](https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+S),[Shangyan Zhou](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+S),[Shanhuang Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+S),[Shengfeng Ye](https://arxiv.org/search/cs?searchtype=author&amp;query=Ye,+S),[Shiyu Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+S),[Shuiping Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+S),[Shunfeng Zhou](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+S),[Shuting Pan](https://arxiv.org/search/cs?searchtype=author&amp;query=Pan,+S),[S.S. Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+S)\n,[Shuang Zhou](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+S),[Shaoqing Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+S),[Shengfeng Ye](https://arxiv.org/search/cs?searchtype=author&amp;query=Ye,+S),[Tao Yun](https://arxiv.org/search/cs?searchtype=author&amp;query=Yun,+T),[Tian Pei](https://arxiv.org/search/cs?searchtype=author&amp;query=Pei,+T),[Tianyu Sun](https://arxiv.org/sea",
          "original_query": "Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning",
          "cleaned_query": "Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Med-Flamingo: a Multimodal Medical Few-shot Learner",
          "url": "https://proceedings.mlr.press/v225/moor23a.html",
          "content": "\\[ [edit](https://github.com/mlresearch/v225/edit/gh-pages/_posts/2023-12-04-moor23a.md)\\]\n\n# Med-Flamingo: a Multimodal Medical Few-shot Learner\n\nMichael Moor,\u00a0Qian Huang,\u00a0Shirley Wu,\u00a0Michihiro Yasunaga,\u00a0Yash Dalmia,\u00a0Jure Leskovec,\u00a0Cyril Zakka,\u00a0Eduardo Pontes Reis,\u00a0Pranav Rajpurkar\n\n_Proceedings of the 3rd Machine Learning for Health Symposium_,\u00a0PMLR 225:353-367,\u00a02023.\n\n#### Abstract\n\nMedicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models{~}(VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering{~}(VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app. Med-Flamingo improves performance in generative medical VQA by up to 20 {\\\\}% in clinician\u2019s rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation. We release our model, code, and evaluation app. %under{~}{\\\\}url\\\\{https://github.com/snap-stanford/med-flamingo\\\\}.\n\n#### Cite this Paper\n\n* * *\n\nBibTeX\n\n`@InProceedings{pmlr-v225-moor23a,\ntitle = {Med-Flamingo: a Multimodal Medical Few-shot Learner},\nauthor = {Moor, Michael and Huang, Qian and Wu, Shirley and Yasunaga, Michihiro and Dalmia, Yash and Leskovec, Jure and Zakka, Cyril and Reis, Eduardo Pontes and Rajpurkar, Pranav},\nbooktitle = {Proceedings of the 3rd Machine Learning for Health Symposium},\npages = {353--367},\nyear = {2023},\neditor = {Hegselmann, Stefan and Parziale, Antonio and Shanmugam, Divya and Tang, Shengpu and Asiedu, Mercy Nyamewaa and Chang, Serina and Hartvigsen, Tom and Singh, Harvineet},\nvolume = {225},\nseries = {Proceedings of Machine Learning Research},\nmonth = {10 Dec},\npublisher = {PMLR},\npdf = {https://proceedings.mlr.press/v225/moor23a/moor23a.pdf},\nurl = {https://proceedings.mlr.press/v225/moor23a.html},\nabstract = {Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models{~}(VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering{~}(VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app. Med-Flamingo improves performance in generative medical VQA by up to 20 {\\}% in clinician\u2019s rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation. We release our model, code, and evaluation app. %under{~}{\\}url\\{https://github.com/snap-stanford/med-flamingo\\}.}\n}`\n\nCopy to ClipboardDownload\n\nEndnote\n\n`%0 Conference Paper\n%T Med-Flamingo: a Multimodal Medical Few-shot Learner\n%A Michael Moor\n%A Qian Huang\n%A Shirley Wu\n%A Michihiro Yasunaga\n%A Yash Dalmia\n%A Jure Leskovec\n%A Cyril Zakka\n%A Eduardo Pontes Reis\n%A Pranav Rajpurkar\n%B Proceedings of the 3rd Machine Learning for Health Symposium\n%C Proceedings of Machine Learning Research\n%D 2023\n%E Stefan Hegselmann\n%E Antonio Parziale\n%E Divya Shanmugam\n%E Shengpu Tang\n%E Mercy Nyamewaa Asiedu\n%E Serina Chang\n%E Tom Hartvigsen\n%E Harvineet Singh\n%F pmlr-v225-moor23a\n%I PMLR\n%P 353--367\n%U https://proceedings.mlr.press/v225/moor23a.html\n%V 225\n%X Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models{~}(VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering{~}(VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app. Med-Flamingo improves performance in generative medical VQA by up to 20 {\\}% in clinician\u2019s rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation. We release our model, code, and evaluation app. %under{~}{\\}url\\{https://github.com/snap-stanford/med-flamingo\\}.`\n\nCopy to ClipboardDownload\n\nAPA\n\n`Moor, M., Huang, Q., Wu, S., Yasunaga, M., Dalmia, Y., Leskovec, J., Zakka, C., Reis, E.P. & Rajpurkar, P.. (2023). Med-Flamingo: a Multimodal Medical Few-shot Learner. Proceedings of the 3rd Machine Learning for Health Symposium, in Proceedings of Machine Learning Research 225:353-367 Available from https://proceedings.mlr.press/v225/moor23a.html.`\n\nCopy to ClipboardDownload\n\n* * *\n\n#### Related Material\n\n- [Download PDF](https://proceedings.mlr.press/v225/moor23a/moor23a.pdf)\n- [Software](https://github.com/snap-stanford/med-flamingo)",
          "original_query": "Med-flamingo: a multimodal medical few-shot learner",
          "cleaned_query": "Med-flamingo: a multimodal medical few-shot learner",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Biomedical Visual Instruction Tuning with Clinician Preference ...",
          "url": "https://arxiv.org/abs/2406.13173",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2406.13173** (cs)\n\n\\[Submitted on 19 Jun 2024 ( [v1](https://arxiv.org/abs/2406.13173v1)), last revised 16 Jul 2024 (this version, v3)\\]\n\n# Title:Biomedical Visual Instruction Tuning with Clinician Preference Alignment\n\nAuthors: [Hejie Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui,+H), [Lingjun Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao,+L), [Xin Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang,+X), [Jieyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J), [Hui Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren,+H), [Quanzheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+Q), [Xiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+X), [Carl Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+C)\n\nView a PDF of the paper titled Biomedical Visual Instruction Tuning with Clinician Preference Alignment, by Hejie Cui and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2406.13173) [HTML (experimental)](https://arxiv.org/html/2406.13173v3)\n\n> Abstract:Recent advancements in multimodal foundation models have showcased impressive capabilities in understanding and reasoning with visual and textual information. Adapting these foundation models trained for general usage to specialized domains like biomedicine requires large-scale domain-specific instruction datasets. While existing works have explored curating such datasets automatically, the resultant datasets are not explicitly aligned with domain expertise. In this work, we propose a data-centric framework, Biomedical Visual Instruction Tuning with Clinician Preference Alignment (BioMed-VITAL), that incorporates clinician preferences into both stages of generating and selecting instruction data for tuning biomedical multimodal foundation models. First, during the generation stage, we prompt the GPT-4V generator with a diverse set of clinician-selected demonstrations for preference-aligned data candidate generation. Then, during the selection phase, we train a separate selection model, which explicitly distills clinician and policy-guided model preferences into a rating function to select high-quality data for medical instruction tuning. Results show that the model tuned with the instruction-following data from our method demonstrates a significant improvement in open visual chat (18.5% relatively) and medical VQA (win rate up to 81.73%). Our instruction-following data and models are available at [this http URL](http://BioMed-VITAL.github.io).\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |\n| MSC classes: | 68T50, 68T45, 68T37, 68T05, 68T07, 68T09, |\n| ACM\u00a0classes: | I.2.7; I.2.6; I.2.10 |\n| Cite as: | [arXiv:2406.13173](https://arxiv.org/abs/2406.13173) \\[cs.CV\\] |\n| (or [arXiv:2406.13173v3](https://arxiv.org/abs/2406.13173v3) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2406.13173](https://doi.org/10.48550/arXiv.2406.13173) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hejie Cui \\[ [view email](https://arxiv.org/show-email/de4eeb23/2406.13173)\\] **[\\[v1\\]](https://arxiv.org/abs/2406.13173v1)**\nWed, 19 Jun 2024 03:07:33 UTC (4,981 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2406.13173v2)**\nSun, 30 Jun 2024 01:22:09 UTC (4,981 KB)\n**\\[v3\\]**\nTue, 16 Jul 2024 05:56:05 UTC (4,981 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Biomedical Visual Instruction Tuning with Clinician Preference Alignment, by Hejie Cui and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2406.13173)\n- [HTML (experimental)](https://arxiv.org/html/2406.13173v3)\n- [TeX Source](https://arxiv.org/src/2406.13173)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2406.13173&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2406.13173&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-06](https://arxiv.org/list/cs.CV/2024-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2406.13173?context=cs) [cs.AI](https://arxiv.org/abs/2406.13173?context=cs.AI) [cs.CL](https://arxiv.org/abs/2406.13173?context=cs.CL) [cs.LG](https://arxiv.org/abs/2406.13173?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2406.13173)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2406.13173)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2406.13173)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2406.13173) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Biomedical visual instruction tuning with clinician preference alignment",
          "cleaned_query": "Biomedical visual instruction tuning with clinician preference alignment",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "a Dataset for Sequential Sentence Classification in Medical Abstracts",
          "url": "https://aclanthology.org/I17-2052/",
          "content": "## [PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts](https://aclanthology.org/I17-2052.pdf)\n\n[Franck Dernoncourt](https://aclanthology.org/people/f/franck-dernoncourt/),\n[Ji Young Lee](https://aclanthology.org/people/j/ji-young-lee/)\n\n* * *\n\n##### Abstract\n\nWe present PubMed 200k RCT, a new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: we hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field.\n\nAnthology ID:I17-2052Volume:[Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)](https://aclanthology.org/volumes/I17-2/)Month:NovemberYear:2017Address:Taipei, TaiwanEditors:[Greg Kondrak](https://aclanthology.org/people/g/greg-kondrak/),\n[Taro Watanabe](https://aclanthology.org/people/t/taro-watanabe/)Venue:[IJCNLP](https://aclanthology.org/venues/ijcnlp/)SIG:Publisher:Asian Federation of Natural Language ProcessingNote:Pages:308\u2013313Language:URL:[https://aclanthology.org/I17-2052](https://aclanthology.org/I17-2052)DOI:Bibkey:dernoncourt-lee-2017-pubmedCite (ACL):Franck Dernoncourt and Ji Young Lee. 2017. [PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts](https://aclanthology.org/I17-2052). In _Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)_, pages 308\u2013313, Taipei, Taiwan. Asian Federation of Natural Language Processing.Cite (Informal):[PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts](https://aclanthology.org/I17-2052) (Dernoncourt & Lee, IJCNLP 2017)Copy Citation:BibTeXMarkdownMODS XMLEndnoteMore options\u2026PDF:[https://aclanthology.org/I17-2052.pdf](https://aclanthology.org/I17-2052.pdf)Code[Franck-Dernoncourt/pubmed-rct](https://github.com/Franck-Dernoncourt/pubmed-rct)\n+\n[additional community code](https://paperswithcode.com/paper/?acl=I17-2052)Data[PubMed RCT](https://paperswithcode.com/dataset/pubmed-rct)\n\n[PDF](https://aclanthology.org/I17-2052.pdf) [Cite](https://aclanthology.org/I17-2052/) [Search](https://www.semanticscholar.org/search?q=PubMed+200k+RCT%3A+a+Dataset+for+Sequential+Sentence+Classification+in+Medical+Abstracts) [Code](https://paperswithcode.com/paper/?acl=I17-2052)\n\n* * *",
          "original_query": "Pubmed 200k RCT: a dataset for sequential sentence classification in medical abstracts",
          "cleaned_query": "Pubmed 200k RCT: a dataset for sequential sentence classification in medical abstracts",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Towards Reasoning in Large Language Models: A Survey",
          "url": "https://aclanthology.org/2023.findings-acl.67.pdf",
          "content": "Findings of the Association for Computational Linguistics: ACL 2023, pages 1049\u20131065\nJuly 9-14, 2023 \u00a92023 Association for Computational Linguistics\nTowards Reasoning in Large Language Models: A Survey\nJie Huang Kevin Chen-Chuan Chang\nDepartment of Computer Science, University of Illinois at Urbana-Champaign\n{jeffhj, kcchang}@illinois.edu\nAbstract\nReasoning is a fundamental aspect of human\nintelligence that plays a crucial role in activi\u0002ties such as problem solving, decision making,\nand critical thinking. In recent years, large\nlanguage models (LLMs) have made signifi\u0002cant progress in natural language processing,\nand there is observation that these models may\nexhibit reasoning abilities when they are suf\u0002ficiently large. However, it is not yet clear to\nwhat extent LLMs are capable of reasoning.\nThis paper provides a comprehensive overview\nof the current state of knowledge on reasoning\nin LLMs, including techniques for improving\nand eliciting reasoning in these models, meth\u0002ods and benchmarks for evaluating reasoning\nabilities, findings and implications of previous\nresearch in this field, and suggestions on future\ndirections. Our aim is to provide a detailed and\nup-to-date review of this topic and stimulate\nmeaningful discussion and future work.1\n1 Introduction\nReasoning is a cognitive process that involves using\nevidence, arguments, and logic to arrive at conclu\u0002sions or make judgments. It plays a central role in\nmany intellectual activities, such as problem solv\u0002ing, decision making, and critical thinking. The\nstudy of reasoning is important in fields like psy\u0002chology (Wason and Johnson-Laird, 1972), philoso\u0002phy (Passmore, 1961), and computer science (Huth\nand Ryan, 2004), as it helps individuals make deci\u0002sions, solve problems, and think critically.\nRecently, large language models (LLMs)\n(Brown et al., 2020; Chowdhery et al., 2022; Chung\net al., 2022; OpenAI, 2022, inter alia) such as Chat\u0002GPT have made significant advancements in natu\u0002ral language processing and related fields. It has\nbeen shown that these models exhibit emergent be\u0002haviors, including the ability to \u201creason\u201d, when\n1\nPaperlist can be found at https://github.com/\njeffhj/LM-reasoning.\nthey are large enough (Wei et al., 2022a). For ex\u0002ample, by providing the models with \u201cchain of\nthoughts\u201d, i.e., reasoning exemplars, or a simple\nprompt \u201cLet\u2019s think step by step\u201d, these models\nare able to answer questions with explicit reason\u0002ing steps (Wei et al., 2022b; Kojima et al., 2022),\ne.g., \u201call whales are mammals, all mammals have\nkidneys; therefore, all whales have kidneys.\u201d This\nhas sparked considerable interest in the commu\u0002nity since reasoning ability is a hallmark of human\nintelligence that is frequently considered missed\nin current artificial intelligence systems (Marcus,\n2020; Russin et al., 2020; Mitchell, 2021; Bom\u0002masani et al., 2021).\nHowever, despite the strong performance of\nLLMs on certain reasoning tasks, it remains unclear\nwhether LLMs are actually reasoning and to what\nextent they are capable of reasoning. For exam\u0002ple, Kojima et al. (2022) claim that \u201cLLMs are de\u0002cent zero-shot reasoners (p. 1)\u201d, while Valmeekam\net al. (2022) conclude that \u201cLLMs are still far\nfrom achieving acceptable performance on com\u0002mon planning/reasoning tasks which pose no issues\nfor humans to do (p. 2).\u201d This limitation is also\nstated by Wei et al. (2022b):\n\u201cwe qualify that although chain of thought emu\u0002lates the thought processes of human reasoners,\nthis does not answer whether the neural network\nis actually reasoning (p. 9).\u201d\nTherefore, in this paper, we aim to provide a\ncomprehensive overview and engage in an insight\u0002ful discussion on the current state of knowledge on\nthis fast-evolving topic. We initiate our exploration\nwith a clarification of the concept of reasoning (\u00a72).\nSubsequently, we turn our attention to the tech\u0002niques for enhancing/eliciting reasoning in LLMs\n(\u00a73), the methods and benchmarks for evaluating\nreasoning in LLMs (\u00a74), and the key findings and\nimplications in this field (\u00a75). Finally, we reflect\non and discuss the current state of the field (\u00a76).\n1049\nReasoning in LLMs\nTechniques (\u00a73)\nFully Supervised Finetuning (\u00a73.1)\nPrompting & In-Context Learning (\u00a73.2)\nChain of Thought and Its Variants (\u00a73.2.1)\nRationale Engineering (\u00a73.2.2)\nProblem Decomposition (\u00a73.2.3)\nOthers (\u00a73.2.4)\nHybrid Method (\u00a73.3)\nReasoning-Enhanced Training & Prompting (\u00a73.3.1)\nBootstrapping & Self-Improving (\u00a73.3.2)\nEvaluation & Analysis (\u00a74)\nEnd Task Performance (\u00a74.1)\nAnalysis on Reasoning (\u00a74.2)\nFindings & Implications (\u00a75)\nReflection, Discussion & Future Directions (\u00a76)\nFigure 1: The structure of the paper.\n2 What is Reasoning?\nReasoning is the process of thinking about some\u0002thing in a logical and systematic way, using evi\u0002dence and past experiences to reach a conclusion or\nmake a decision (Wason and Johnson-Laird, 1972;\nWason, 1968; Galotti, 1989; Fagin et al., 2004;\nMcHugh and Way, 2018). Reasoning involves mak\u0002ing inferences, evaluating arguments, and drawing\nlogical conclusions based on available information.\nAlthough \u201creasoning\u201d is a term that is commonly\nused in literature and daily life, it is also an abstract\nconcept that can refer to many things. To help the\nreader better understand this concept, we summa\u0002rize several main categories of reasoning that are\ncommonly recognized:\nDeductive reasoning. Deductive reasoning is a\ntype of reasoning in which a conclusion is drawn\nbased on the truth of the premises. In deductive\nreasoning, the conclusion must necessarily follow\nfrom the premises, meaning that if the premises are\ntrue, the conclusion must also be true. For example:\n\u2022 Premise: All mammals have kidneys.\n\u2022 Premise: All whales are mammals.\n\u2022 Conclusion: All whales have kidneys.\nInductive reasoning. Inductive reasoning is a type\nof reasoning in which a conclusion is drawn based\non observations or evidence. The conclusion is\nlikely to be true based on the available evidence,\nbut it is not necessarily certain. For example:\n\u2022 Observation: Every time we see a creature\nwith wings, it is a bird.\n\u2022 Observation: We see a creature with wings.\n\u2022 Conclusion: The creature is likely to be a bird.\nAbductive reasoning. Abductive reasoning is a\ntype of reasoning in which a conclusion is drawn\nbased on the best explanation for a given set of\nobservations. The conclusion is the most likely\nexplanation based on the available evidence, but it\nis not necessarily certain. For example:\n\u2022 Observation: The car cannot start and there is\na puddle of liquid under the engine.\n\u2022 Conclusion: The most likely explanation is\nthat the car has a leak in the radiator.\nOther types of reasoning include analogical reason\u0002ing, which involves making comparisons between\ntwo or more things in order to make inferences\nor arrive at conclusions; causal reasoning, which\ninvolves identifying and understanding the causes\nand effects of events or phenomena; and probabilis\u0002tic reasoning, which involves making decisions or\narriving at conclusions based on the likelihood or\nprobability of certain outcomes.\nFormal Reasoning vs Informal Reasoning. For\u0002mal reasoning is a systematic and logical process\nthat follows a set of rules and principles, often used\nin mathematics and logic. Informal reasoning is a\nless structured approach that relies on intuition, ex\u0002perience, and common sense to draw conclusions\nand solve problems, and is often used in everyday\nlife. Formal reasoning is more structured and reli\u0002able, while informal reasoning is more adaptable\nand open-ended, but may also be less reliable. We\nrefer the reader to Galotti (1989); Bronkhorst et al.\n(2020) for a detailed distinction between them.\nReasoning in Language Models. The concept of\nreasoning in language models has been around for\nsome time, but there is not a clear definition of\nwhat it entails. In the literature, the term \u201creason\u0002ing\u201d is often used to refer to informal reasoning,\nalthough it is not always explicitly stated that it\nis informal (Cobbe et al., 2021; Wei et al., 2022b,\n1050\ninter alia). Different forms of reasoning may be\nused depending on the task, benchmark, or method\nbeing used, e.g., deductive reasoning (Cobbe et al.,\n2021; Creswell et al., 2022; Han et al., 2022b, in\u0002ter alia), inductive reasoning (Yang et al., 2022;\nMisra et al., 2022, inter alia) or abductive reason\u0002ing (Wiegreffe et al., 2022; Lampinen et al., 2022;\nJung et al., 2022, inter alia). In this paper, we\nencompass various forms of reasoning, with a par\u0002ticular focus on \u201cinformal deductive reasoning\u201d in\nlarge language models since it is a widely used\nform in which the conclusion is guaranteed to be\ntrue as long as the premises are true.\n3 Towards Reasoning in Large Language\nModels\nReasoning, particularly multi-step reasoning, is of\u0002ten seen as a weakness in language models and\nother NLP models (Bommasani et al., 2021; Rae\net al., 2021; Valmeekam et al., 2022). Recent re\u0002search has suggested that reasoning ability may\nemerge in language models at a certain scale, such\nas models with over 100 billion parameters (Wei\net al., 2022a,b; Cobbe et al., 2021). In this paper,\nwe follow Wei et al. (2022a) in considering rea\u0002soning as an ability that is rarely present in small\u0002scale models like GPT-2 (Radford et al., 2019) and\nBERT (Devlin et al., 2019), and therefore focus\non techniques applicable to improving or eliciting\n\u201creasoning\u201d2in LLMs such as GPT-3 (Brown et al.,\n2020) and PaLM (Chowdhery et al., 2022).\n3.1 Fully Supervised Finetuning\nBefore discussing reasoning in large language mod\u0002els, it is worth mentioning there is research work\u0002ing on eliciting/improving reasoning in small lan\u0002guage models through fully supervised finetuning\non specific datasets. For example, Rajani et al.\n(2019) finetune a pretrained GPT model (Radford\net al., 2018) to generate rationales that explain\nmodel predictions with the built CoS-E dataset,\nand find that models trained with explanations\nperform better on commonsense question answer\u0002ing tasks (Talmor et al., 2019). Talmor et al.\n(2020) trai",
          "original_query": "Towards reasoning in large language models: A survey",
          "cleaned_query": "Towards reasoning in large language models: A survey",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Applying generative AI with retrieval augmented ... - ResearchGate",
          "url": "https://www.researchgate.net/publication/381436243_Applying_generative_AI_with_retrieval_augmented_generation_to_summarize_and_extract_key_clinical_information_from_electronic_health_records",
          "content": "Figures - uploaded by Ping Yu Author content All figure content in this area was uploaded by Ping Yu Content may be subject to copyright. Discover the world's research 25+ million members 160+ million publication pages 2.3+ billion citations Join for free \n \n \n \n \n \n \n Journal of Biomedical Informatics 156 (2024) 104662 Available online 14 June 2024 1532-0464/\u00a9 2024 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/). Original Research Applying generative AI with retrieval augmented generation to summarize and extract key clinical information from electronic health records Mohammad Alkhalaf a, b, Ping Yu a, *, Mengyang Yin c, Chao Deng d a School of Computing and Information Technology, University of Wollongong, Wollongong, NSW 2522, Australia b School of Computer Science, Qassim University, Qassim 51452, Saudi Arabia c Opal Healthcare, Level 11/420 George St, Sydney NSW 2000, Australia d School of Medical, Indigenous and Health Sciences, University of Wollongong, Wollongong, NSW 2522, Australia ARTICLE INFO Keywords: Generative AI Nursing notes LLAMA Malnutrition Summarization RAG ABSTRACT Background: Malnutrition is a prevalent issue in aged care facilities (RACFs), leading to adverse health outcomes. The ability to ef\ue630ciently extract key clinical information from a large volume of data in electronic health records (EHR) can improve understanding about the extent of the problem and developing effective interventions. This research aimed to test the ef\ue630cacy of zero-shot prompt engineering applied to generative arti\ue630cial intelligence (AI) models on their own and in combination with retrieval augmented generation (RAG), for the automating tasks of summarizing both structured and unstructured data in EHR and extracting important malnutrition information. Methodology: We utilized Llama 2 13B model with zero-shot prompting. The dataset comprises unstructured and structured EHRs related to malnutrition management in 40 Australian RACFs. We employed zero-shot learning to the model alone \ue630rst, then combined it with RAG to accomplish two tasks: generate structured summaries about the nutritional status of a client and extract key information about malnutrition risk factors. We utilized 25 notes in the \ue630rst task and 1,399 in the second task. We evaluated the model \u2019 s output of each task manually against a gold standard dataset. Result: The evaluation outcomes indicated that zero-shot learning applied to generative AI model is highly effective in summarizing and extracting information about nutritional status of RACFs \u2019 clients. The generated summaries provided concise and accurate representation of the original data with an overall accuracy of 93.25%. The addition of RAG improved the summarization process, leading to a 6% increase and achieving an accuracy of 99.25%. The model also proved its capability in extracting risk factors with an accuracy of 90%. However, adding RAG did not further improve accuracy in this task. Overall, the model has shown a robust performance when information was explicitly stated in the notes; however, it could encounter hallucination limitations, particularly when details were not explicitly provided. Conclusion: This study demonstrates the high performance and limitations of applying zero-shot learning to generative AI models to automatic generation of structured summarization of EHRs data and extracting key clinical information. The inclusion of the RAG approach improved the model performance and mitigated the hallucination problem. 1. Introduction Malnutrition is a signi\ue630cant health concern, which can lead to weight and muscle loss, and affect a person \u2019 s physical health, cognitive func- tions, immune function, and overall quality of life. It is prevalent and extremely harmful for the frail older people living in RACFs, and can expose these older people to higher risk of developing chronic diseases, experiencing functional decline, increasing rate of falls, hospitalization and mortality [1,2]. In older people, malnutrition could be caused by a variety of factors, including physiological changes associated with aging, chronic diseases, social isolation, and poor appetite [3,4]. The complex nature of malnutrition demands attention to its risk factors and * Corresponding author at: Centre for Digital Transformation, School of Computing and Information Technology, Faculty of Engineering and Information Sciences, North\ue630eld Ave, University of Wollongong, Wollongong, NSW 2522, Australia. E-mail address: ping@uow.edu.au (P. Yu). Contents lists available at ScienceDirect Journal of Biomedical Informatics journal homepage: www.elsevier.com/locate/yjbin https://doi.org/10.1016/j.jbi.2024.104662 Received 3 February 2024; Received in revised form 25 May 2024; Accepted 28 May 2024 \n \n \n Journal of Biomedical Informatics 156 (2024) 104662 2 implement targeted prevention and management strategies. To date, the main methods for nutrition prevention and management are regular in- person screening for malnutrition risks, regular nutritional assessment, and speci\ue630c interventions to address the risk factors [5,6]. To date, different data analytics have been applied to uncover malnutrition information from the structured EHR [7 \u2013 9]. However, these methods are inadequate, often missing crucial health information about the nutritional health status of older people that are recorded in narrative text-based notes instead of structured tables in health services [10,11]. It has been found that approximately 80 % to 90 % of health data is recorded in unstructured format [12 \u2013 14]. Consequently, it is imperative to develop an effective machine learning approach to retrieve key health information from the large volume of unstructured clinical data to understand a person \u2019 s health status. An effective method to accomplish this is to transform unstructured clinical notes to struc- tured summarization, which could turn patient information into clinical insights. [15,16]. Summarization of the important health information recorded in the free text notes of EHR could potentially enhance health data accessi- bility and analytics accuracy. This will assist nurses and doctors in ef\ue630ciently retrieving clinical information to provide timely and informed treatment decisions [17]. In addition to summarization, extracting information about risk factors for a certain disease is also essential for providing timely support and preventive intervention to stop the progression of disease-related complications. Furthermore, understanding an individual \u2019 s speci\ue630c risk factors allows for the development of personalized, tailored strate- gies to address the person \u2019 s disease-related issues [18 \u2013 20], and is important for improving client \u2019 s health outcomes. At the population level, knowledge about the prevalence of risk factors is important for government health departments and health organizations for assessing the health needs of a population, developing prevention programs, prioritizing resources to improve public health and planning policy changes, and creating the right health services [21,22]. Transforming unstructured nursing notes in EHR into structured summarization with key information has been a technical challenge for a variety of reasons, including format inconsistencies, length variations, typos, and specialized medical terms. Extracting risk factors is also a challenge because they are embedded into the clinical notes and may be subjective or depend on individual health history [23,24]. Another challenge is the high demand for domain expertise and time to develop large, annotated corpora [25]. These challenges have made automated extraction of health information through human or basic natural lan- guage processing (NLP) techniques a time-consuming and labor- intensive task [26 \u2013 29]. The latest development in NLP and more speci\ue630cally, the generative AI and large language models (LLMs) are revolutionizing the way humans interact with text data. These LLMs have the ability to comprehend content of a given text and perform different NLP tasks without the need for large amounts of data for \ue630ne-tuning [30]. These advancements can signi\ue630cantly enhance summarization and health risk factor extraction techniques. They can effectively address certain limi- tations of the aforementioned NLP technologies [27]. Furthermore, they bring in technical breakthroughs in NLP with ability to accurately comprehend content of a given text and perform different tasks without the need for extensive data for \ue630ne-tuning; even exceed human capa- bilities in certain tasks [30 \u2013 32]. Since GPT-3.5 stormed the world in November 2022, there has been an ongoing surge of interest in exploring the application of the decoder- based generative AI and LLMs in healthcare. To date, the majority of GPT models for medicine are applied to medical questions and answering tasks [30]. Hu et al. applied GPT-3.5 and GPT-4 on clinical named entity recognition tasks on public data sets [25]. However, GPT- 3.5 and GPT-4 are commercial cloud-based solutions with data hosted in the US. This has prohibited the researchers in other countries to apply them for clinical text processing due to violation of many countries \u2019 information privacy laws. To the best of our knowledge, there is no attempt in applying the open-source LLM to text summarization and health risk factor extraction tasks from real-world EHR in after-hospital care setting. This motivates us to pilot the implementation of prompt engineering techniques in the open-source Llama model for clinical text summarization and health risk factor extraction tasks. Llama 2 is a state-of-the-art (SOTA) LLM model developed by Meta AI. It is an autoregressive language model that uses optimized trans- former architecture. It has been trained on a vast amount of text data using supervised \ue630ne-tuning (SFT) and reinforcement learning wit",
          "original_query": "Applying generative AI with retrieval augmented generation to summarize and extract key clinical information from electronic health records",
          "cleaned_query": "Applying generative AI with retrieval augmented generation to summarize and extract key clinical information from electronic health records",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "RAG-Guided \u201cEvidence-First\u201d Medical VQA with Hallucination Penalties\n- Extend EHR RAG summarization to multimodal VQA by forcing every claim in an answer/rationale to be grounded in retrieved snippets (guidelines, textbook passages, or local EHR context). Train with an explicit hallucination penalty: any unsupported span triggers negative reward or filtering during instruction selection (BioMed-VITAL selection-model style). Measure reductions in unsupported statements and improved clinician trust ratings.",
        "Few-Shot \u201cOn-the-Fly\u201d Hospital Adaptation via Preference-Aligned Data Selection\n- Create a deployable workflow where a model adapts to a new hospital/unit using only a handful of clinician-reviewed examples, leveraging BioMed-VITAL\u2019s preference-guided candidate generation and selection. The key contribution is a concrete algorithm for rapidly curating a small, high-leverage instruction set (and optional lightweight fine-tuning or in-context \u201cmemory\u201d) that measurably improves local note summarization and VQA. Evaluate cross-site generalization and how many clinician ratings are needed for stable gains.",
        "Interleaved Multimodal \u201cCase Timeline\u201d Modeling for Aged-Care Malnutrition\n- Extend the malnutrition EHR summarization setting to an interleaved timeline format (structured vitals/labs + sequential notes + relevant images like wound photos or meal tray images where available). Train a model to generate (i) a structured state summary and (ii) a temporally grounded rationale citing changes over time, using Med-Flamingo\u2019s interleaved pretraining idea. Evaluate on longitudinal consistency and early-risk detection accuracy.",
        "Clinician-Calibrated Uncertainty and Abstention for Medical Generative Models\n- Add a calibrated uncertainty head and an abstention policy trained from clinician preferences: models are rewarded for saying \u201cinsufficient information\u201d when evidence is missing (a key failure mode noted in EHR summarization hallucinations). Combine RAG coverage signals (retrieval confidence/overlap) with RL rewards to tune when to abstain vs. answer. Validate with physician review on ambiguous VQA and incomplete EHR cases.",
        "Counterfactual Visual QA for Robust Biomedical Instruction Tuning\n- Generate counterfactual image-text instruction pairs (e.g., \u201cif the opacity were absent, what diagnosis changes?\u201d) to stress-test and train robust visual reasoning rather than superficial pattern matching. Use clinician preference alignment to filter counterfactuals that are clinically plausible and educational, then instruction-tune a biomedical VLM. Evaluate robustness under controlled perturbations and reduced shortcut reliance.",
        "Cross-Modal Literature-to-EHR Knowledge Transfer via Structured Summary Distillation\n- Distill evidence from RCT abstracts into structured templates (population, intervention, comparator, outcomes) using sentence-role labels and then teach the model to map EHR patient contexts to the most relevant evidence-backed recommendations via RAG. The contribution is a concrete distillation-and-linking pipeline connecting PubMed-scale literature structure to bedside summarization/decision support. Evaluate on recommendation faithfulness (to retrieved evidence) and clinician-rated usefulness in real EHR summarization tasks."
      ],
      "generated_ideas_raw": "1. **Clinician-Preference RL for Medical Multimodal Reasoning (R1-MedRL)**\n   - Build a reinforcement-learning pipeline that optimizes Med-Flamingo/BioMed-VITAL-style VLMs directly for clinician-ranked responses, using preference rewards akin to DeepSeek-R1\u2019s reasoning-focused RL. Compare reward designs that separately score (i) final answer correctness and (ii) reasoning quality/clinical safety. Evaluate on Visual USMLE-style VQA plus physician interactive review to quantify \u201creasoning helpfulness\u201d gains beyond accuracy.\n\n2. **RAG-Guided \u201cEvidence-First\u201d Medical VQA with Hallucination Penalties**\n   - Extend EHR RAG summarization to multimodal VQA by forcing every claim in an answer/rationale to be grounded in retrieved snippets (guidelines, textbook passages, or local EHR context). Train with an explicit hallucination penalty: any unsupported span triggers negative reward or filtering during instruction selection (BioMed-VITAL selection-model style). Measure reductions in unsupported statements and improved clinician trust ratings.\n\n3. **PubMed-RCT-Structured Abstract Retrieval to Improve Clinical Summaries**\n   - Use PubMed 200k RCT sentence-role labels to create retrieval indices that prioritize *Methods/Results/Conclusions* when generating EHR summaries and medical answers. Implement a role-aware retriever that conditions on the user goal (e.g., \u201crisk factors\u201d retrieves Methods/Results; \u201crecommended action\u201d retrieves Conclusions). Test whether role-aware retrieval improves factuality and actionability vs. standard RAG.\n\n4. **Few-Shot \u201cOn-the-Fly\u201d Hospital Adaptation via Preference-Aligned Data Selection**\n   - Create a deployable workflow where a model adapts to a new hospital/unit using only a handful of clinician-reviewed examples, leveraging BioMed-VITAL\u2019s preference-guided candidate generation and selection. The key contribution is a concrete algorithm for rapidly curating a small, high-leverage instruction set (and optional lightweight fine-tuning or in-context \u201cmemory\u201d) that measurably improves local note summarization and VQA. Evaluate cross-site generalization and how many clinician ratings are needed for stable gains.\n\n5. **Reasoning-Type Attribution Benchmarks for Medical LLM/VLM Outputs**\n   - Using the reasoning taxonomy from the LLM reasoning survey, build a benchmark that tags medical questions and model rationales by reasoning type (deductive, abductive/diagnostic, causal, temporal, guideline-based, etc.). Then analyze whether RL-for-reasoning (DeepSeek-R1-style) or clinician preference tuning (BioMed-VITAL) disproportionately improves certain reasoning types. This yields an actionable diagnostic tool for targeting future training data and rewards.\n\n6. **Interleaved Multimodal \u201cCase Timeline\u201d Modeling for Aged-Care Malnutrition**\n   - Extend the malnutrition EHR summarization setting to an interleaved timeline format (structured vitals/labs + sequential notes + relevant images like wound photos or meal tray images where available). Train a model to generate (i) a structured state summary and (ii) a temporally grounded rationale citing changes over time, using Med-Flamingo\u2019s interleaved pretraining idea. Evaluate on longitudinal consistency and early-risk detection accuracy.\n\n7. **Clinician-Calibrated Uncertainty and Abstention for Medical Generative Models**\n   - Add a calibrated uncertainty head and an abstention policy trained from clinician preferences: models are rewarded for saying \u201cinsufficient information\u201d when evidence is missing (a key failure mode noted in EHR summarization hallucinations). Combine RAG coverage signals (retrieval confidence/overlap) with RL rewards to tune when to abstain vs. answer. Validate with physician review on ambiguous VQA and incomplete EHR cases.\n\n8. **Rationale Quality Metrics Grounded in Clinical Evidence and Abstract Structure**\n   - Develop automatic metrics for medical rationales that score (a) evidence citation completeness (from RAG), and (b) rhetorical structure alignment (Background\u2192Objective\u2192Method\u2192Result\u2192Conclusion) using PubMed-RCT-style role classifiers. Use these metrics both for training-time selection (BioMed-VITAL selector) and for RL rewards (DeepSeek-R1-style). Show improved correlation with clinician ratings compared to generic text similarity metrics.\n\n9. **Counterfactual Visual QA for Robust Biomedical Instruction Tuning**\n   - Generate counterfactual image-text instruction pairs (e.g., \u201cif the opacity were absent, what diagnosis changes?\u201d) to stress-test and train robust visual reasoning rather than superficial pattern matching. Use clinician preference alignment to filter counterfactuals that are clinically plausible and educational, then instruction-tune a biomedical VLM. Evaluate robustness under controlled perturbations and reduced shortcut reliance.\n\n10. **Cross-Modal Literature-to-EHR Knowledge Transfer via Structured Summary Distillation**\n   - Distill evidence from RCT abstracts into structured templates (population, intervention, comparator, outcomes) using sentence-role labels and then teach the model to map EHR patient contexts to the most relevant evidence-backed recommendations via RAG. The contribution is a concrete distillation-and-linking pipeline connecting PubMed-scale literature structure to bedside summarization/decision support. Evaluate on recommendation faithfulness (to retrieved evidence) and clinician-rated usefulness in real EHR summarization tasks.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "RAG-Guided \u201cEvidence-First\u201d Medical VQA with Hallucination Penalties\n- Extend EHR RAG summarization to multimodal VQA by forcing every claim in an answer/rationale to be grounded in retrieved snippets",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Few-Shot \u201cOn-the-Fly\u201d Hospital Adaptation via Preference-Aligned Data Selection\n- Create a deployable workflow where a model adapts to a new hospital/unit using only a handful of clinician-reviewed ex",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Interleaved Multimodal \u201cCase Timeline\u201d Modeling for Aged-Care Malnutrition\n- Extend the malnutrition EHR summarization setting to an interleaved timeline format (structured vitals/labs + sequential no",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Clinician-Calibrated Uncertainty and Abstention for Medical Generative Models\n- Add a calibrated uncertainty head and an abstention policy trained from clinician preferences: models are rewarded for s",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Counterfactual Visual QA for Robust Biomedical Instruction Tuning\n- Generate counterfactual image-text instruction pairs (e.g., \u201cif the opacity were absent, what diagnosis changes?\u201d) to stress-test an",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Cross-Modal Literature-to-EHR Knowledge Transfer via Structured Summary Distillation\n- Distill evidence from RCT abstracts into structured templates (population, intervention, comparator, outcomes) us",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 50,
      "paper_title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free",
      "contribution": "This paper presents a systematic investigation of gating mechanisms in softmax attention variants, demonstrating that a simple head-specific sigmoid gate can enhance performance in large language models significantly.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10264,
      "output_tokens": 1033,
      "predecessor_details": [
        {
          "success": true,
          "title": "Long Short-Term Memory Networks: A Comprehensive ...",
          "url": "https://www.mdpi.com/2673-2688/6/9/215",
          "content": "Long Short-Term Memory Networks: A Comprehensive Survey\n**\nNext Article in Journal\n[Large Language Models in Cybersecurity: A Survey of Applications, Vulnerabilities, and Defense Techniques](https://www.mdpi.com/2673-2688/6/9/216)\nNext Article in Special Issue\n[Advances and Optimization Trends in Photovoltaic Systems: A Systematic Review](https://www.mdpi.com/2673-2688/6/9/225)\n**\nPrevious Article in Journal\n[From Detection to Decision: Transforming Cybersecurity with Deep Learning and Visual Analytics](https://www.mdpi.com/2673-2688/6/9/214)\nPrevious Article in Special Issue\n[A GPT-Based Approach for Cyber Threat Assessment](https://www.mdpi.com/2673-2688/6/5/99)\n## Journals\n[Active Journals](https://www.mdpi.com/about/journals)[Find a Journal](https://www.mdpi.com/about/journalfinder)[Journal Proposal](https://www.mdpi.com/about/journals/proposal)[Proceedings Series](https://www.mdpi.com/about/proceedings)\n[## Topics\n](https://www.mdpi.com/topics)\n## Information\n[For Authors](https://www.mdpi.com/authors)[For Reviewers](https://www.mdpi.com/reviewers)[For Editors](https://www.mdpi.com/editors)[For Librarians](https://www.mdpi.com/librarians)[For Publishers](https://www.mdpi.com/publishing_services)[For Societies](https://www.mdpi.com/societies)[For Conference Organizers](https://www.mdpi.com/conference_organizers)\n[Open Access Policy](https://www.mdpi.com/openaccess)[Institutional Open Access Program](https://www.mdpi.com/ioap)[Special Issues Guidelines](https://www.mdpi.com/special_issues_guidelines)[Editorial Process](https://www.mdpi.com/editorial_process)[Research and Publication Ethics](https://www.mdpi.com/ethics)[Article Processing Charges](https://www.mdpi.com/apc)[Awards](https://www.mdpi.com/awards)[Testimonials](https://www.mdpi.com/testimonials)\n[## Author Services\n](https://www.mdpi.com/authors/english)\n## Initiatives\n[Sciforum](https://sciforum.net)[MDPI Books](https://www.mdpi.com/books)[Preprints.org](https://www.preprints.org)[Scilit](https://www.scilit.com)[SciProfiles](https://sciprofiles.com)[Encyclopedia](https://encyclopedia.pub)[JAMS](https://jams.pub)[Proceedings Series](https://www.mdpi.com/about/proceedings)\n## About\n[Overview](https://www.mdpi.com/about)[Contact](https://www.mdpi.com/about/contact)[Careers](https://careers.mdpi.com)[News](https://www.mdpi.com/about/announcements)[Press](https://www.mdpi.com/about/press)[Blog](http://blog.mdpi.com/)\n[Sign In / Sign Up](https://www.mdpi.com/user/login)\n## Notice\nYou can make submissions to other journals[here](https://susy.mdpi.com/user/manuscripts/upload).\n*clear*\n## Notice\nYou are accessing a machine-readable page. In order to be human-readable, please install an RSS reader.\nContinueCancel\n*clear*\nAll articles published by MDPI are made immediately available worldwide under an open access license. No special permission is required to reuse all or part of the article published by MDPI, including figures and tables. For articles published under an open access Creative Common CC BY license, any part of the article may be reused without permission provided that the original article is clearly cited. For more information, please refer to[https://www.mdpi.com/openaccess](https://www.mdpi.com/openaccess).\nFeature papers represent the most advanced research with significant potential for high impact in the field. A Feature Paper should be a substantial original Article that involves several techniques or approaches, provides an outlook for future research directions and describes possible research applications.\nFeature papers are submitted upon individual invitation or recommendation by the scientific editors and must receive positive feedback from the reviewers.\nEditor\u2019s Choice articles are based on recommendations by the scientific editors of MDPI journals from around the world. Editors select a small number of articles recently published in the journal that they believe will be particularly interesting to readers, or important in the respective research area. The aim is to provide a snapshot of some of the most exciting work published in the various research areas of the journal.\nOriginal Submission Date Received:.\n[![](https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1765786175 \"MDPI Open Access Journals\")](https://www.mdpi.com/)\n* [Journals](https://www.mdpi.com/about/journals)\n* * [Active Journals](https://www.mdpi.com/about/journals)\n* [Find a Journal](https://www.mdpi.com/about/journalfinder)\n* [Journal Proposal](https://www.mdpi.com/about/journals/proposal)\n* [Proceedings Series](https://www.mdpi.com/about/proceedings)\n* [Topics](https://www.mdpi.com/topics)\n* [Information](https://www.mdpi.com/authors)\n* * [For Authors](https://www.mdpi.com/authors)\n* [For Reviewers](https://www.mdpi.com/reviewers)\n* [For Editors](https://www.mdpi.com/editors)\n* [For Librarians](https://www.mdpi.com/librarians)\n* [For Publishers](https://www.mdpi.com/publishing_services)\n* [For Societies](https://www.mdpi.com/societies)\n* [For Conference Organizers](https://www.mdpi.com/conference_organizers)\n* [Open Access Policy](https://www.mdpi.com/openaccess)\n* [Institutional Open Access Program](https://www.mdpi.com/ioap)\n* [Special Issues Guidelines](https://www.mdpi.com/special_issues_guidelines)\n* [Editorial Process](https://www.mdpi.com/editorial_process)\n* [Research and Publication Ethics](https://www.mdpi.com/ethics)\n* [Article Processing Charges](https://www.mdpi.com/apc)\n* [Awards](https://www.mdpi.com/awards)\n* [Testimonials](https://www.mdpi.com/testimonials)\n* [Author Services](https://www.mdpi.com/authors/english)\n* [Initiatives](https://www.mdpi.com/about/initiatives)\n* * [Sciforum](https://sciforum.net)\n* [MDPI Books](https://www.mdpi.com/books)\n* [Preprints.org](https://www.preprints.org)\n* [Scilit](https://www.scilit.com)\n* [SciProfiles](https://sciprofiles.com)\n* [Encyclopedia](https://encyclopedia.pub)\n* [JAMS](https://jams.pub)\n* [Proceedings Series](https://www.mdpi.com/about/proceedings)\n* [About](https://www.mdpi.com/about)\n* * [Overview](https://www.mdpi.com/about)\n* [Contact](https://www.mdpi.com/about/contact)\n* [Careers](https://careers.mdpi.com)\n* [News](https://www.mdpi.com/about/announcements)\n* [Press](https://www.mdpi.com/about/press)\n* [Blog](http://blog.mdpi.com/)\n[Sign In / Sign Up](https://www.mdpi.com/user/login)[Submit](< https://susy.mdpi.com/user/manuscripts/upload?journal=ai\n>)\nSearchfor Articles:\nTitle / Keyword\nAuthor / Affiliation / Email\nJournal\nAll JournalsAccounting and AuditingAcousticsActa Microbiologica Hellenica (AMH)ActuatorsAdhesivesAdministrative SciencesAdolescentsAdvances in Respiratory Medicine (ARM)AerobiologyAerospaceAgricultureAgriEngineeringAgrochemicalsAgronomyAIAI ChemistryAI for EngineeringAI in EducationAI in MedicineAI MaterialsAI SensorsAirAlgorithmsAllergiesAlloysAnalogAnalyticaAnalyticsAnatomiaAnesthesia ResearchAnimalsAntibioticsAntibodiesAntioxidantsApplied BiosciencesApplied MechanicsApplied MicrobiologyApplied NanoApplied SciencesApplied System Innovation (ASI)AppliedChemAppliedMathAppliedPhysAquaculture JournalArchitectureArthropodaArtsAstronauticsAstronomyAtmosphereAtomsAudiology ResearchAutomationAxiomsBacteriaBatteriesBehavioral SciencesBeveragesBig Data and Cognitive Computing (BDCC)BioChemBioengineeringBiologicsBiologyBiology and Life Sciences ForumBiomassBiomechanicsBioMedBiomedicinesBioMedInformaticsBiomimeticsBiomoleculesBiophysicaBioresources and BioproductsBiosensorsBiosphereBioTechBirdsBlockchainsBrain SciencesBuildingsBusinessesC (Journal of Carbon Research)CancersCardiogeneticsCardiovascular MedicineCatalystsCellsCeramicsChallengesChemEngineeringChemistryChemistry ProceedingsChemosensorsChildrenChipsCivilEngClean Technologies (Clean Technol.)ClimateClinical and Translational Neuroscience (CTN)Clinical BioenergeticsClinics and PracticeClocks &amp; SleepCoastsCoatingsColloids and InterfacesColorantsCommoditiesComplexitiesComplicationsCompoundsComputationComputer Sciences &amp; Mathematics ForumComputersCondensed MatterConservationConstruction MaterialsCorrosion and Materials Degradation (CMD)CosmeticsCOVIDCraniomaxillofacial Trauma &amp; Reconstruction (CMTR)CropsCryoCryptographyCrystalsCultureCurrent Issues in Molecular Biology (CIMB)Current OncologyDairyDataDentistry JournalDermatoDermatopathologyDesignsDiabetologyDiagnosticsDieteticsDigitalDisabilitiesDiseasesDiversityDNADronesDrugs and Drug Candidates (DDC)DynamicsEarthEcologiesEconometricsEconomiesEducation SciencesElectricityElectrochemElectronic MaterialsElectronicsEmergency Care and MedicineEncyclopediaEndocrinesEnergiesEnergy Storage and Applications (ESA)EngEngineering ProceedingsEntropic and Disordered Matter (EDM)EntropyEnvironmental and Earth Sciences ProceedingsEnvironmentsEpidemiologiaEpigenomesEuropean Burn Journal (EBJ)European Journal of Investigation in Health, Psychology and Education (EJIHPE)Family SciencesFermentationFibersFinTechFireFishesFluidsFoodsForecastingForensic SciencesForestsFossil StudiesFoundationsFractal and Fractional (Fractal Fract)FuelsFutureFuture InternetFuture PharmacologyFuture TransportationGalaxiesGamesGasesGastroenterology InsightsGastrointestinal DisordersGastronomyGelsGenealogyGenesGeographiesGeoHazardsGeomaticsGeometryGeosciencesGeotechnicsGeriatricsGermsGlaciesGout, Urate, and Crystal Deposition Disease (GUCDD)GrassesGreen HealthHardwareHealthcareHeartsHematoHematology ReportsHeritageHistoriesHorticulturaeHospitalsHumanitiesHumansHydrobiologyHydrogenHydrologyHydropowerHygieneImmunoInfectious Disease ReportsInformaticsInformationInfrastructuresInorganicsInsectsInstrumentsIntelligent Infrastructure and ConstructionInternational Journal of Cognitive Sciences (IJCS)International Journal of Environmental Medicine (IJEM)International Journal of Environmental Research and Public Health (IJERPH)International Journal of Financial Studies (IJFS)International Journal of Molecular Sciences (IJMS)International Journal of Neonatal Screening (IJNS)Internat",
          "original_query": "Long short-term memory",
          "cleaned_query": "Long short-term memory",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Department of Transportation - National Highway Freight Network (NHFN)",
          "url": "https://catalog.data.gov/dataset/national-highway-freight-network-nhfn1",
          "content": "[Skip to main content](https://catalog.data.gov/catalog.data.gov#content)\n\n**Official websites use .gov** A\n**.gov** website belongs to an official government\norganization in the United States.\n\n**Secure .gov websites use HTTPS** A\n**lock** (\nA locked padlock) or **https://** means you\u2019ve safely connected to\nthe .gov website. Share sensitive information only on official,\nsecure websites.\n\n [Skip to content](https://catalog.data.gov/catalog.data.gov#content)\n\nMenu\n\n[User Guide](https://data.gov/user-guide/)\n\n1. [Home](https://catalog.data.gov/dataset/)\n2. [Department of Transportation](https://catalog.data.gov/organization/dot-gov)\n\n[Feedback](https://catalog.data.gov/catalog.data.gov)\n\n# National Highway Freight Network (NHFN)\n\nMetadata Updated:[July 17, 2025](https://catalog.data.gov/catalog.data.gov#sec-dates)\n\nThe National Highway Freight Network (NHFN) dataset was compiled on January 27, 2023 from the Federal Highway Administration (FHWA) and is part of the U.S. Department of Transportation (USDOT)/Bureau of Transportation Statistics (BTS) National Transportation Atlas Database (NTAD). Congress established a new National Highway Freight Program (NHFP) in 23 U.S.C. 167 to improve the efficient movement of freight on the National Highway Freight Network (NHFN) and support several goals. The law required the FHWA Administrator to strategically direct Federal resources and policies toward improved performance of the network. The NHFP provides formula funding apportioned annually to States, for use on the NHFN. The definition of the NHFN is established under 23 U.S.C. 167(c) and consists of four separate highway network components: the PHFS; Critical Rural Freight Corridors (CRFCs); Critical Urban Freight Corridors (CUFCs); and those portions of the Interstate System that are not part of the PHFS.\n\nPrimary Highway Freight System (PHFS): This is a network of highways identified as the most critical highway portions of the U.S. freight transportation system determined by measurable and objective national data. The network consists of 41,800 centerlines miles, including 38,014 centerline miles of Interstate and 3,785 centerline miles of non-Interstate roads.\nOther Interstate portions not on the PHFS: These highways consist of the remaining portion of Interstate roads not included in the PHFS. These routes provide important continuity and access to freight transportation facilities. These portions amount to an estimated 10,265 centerline miles of Interstate, nationwide, and will fluctuate with additions and deletions to the Interstate Highway System.\nCritical Rural Freight Corridors (CRFCs): These are public roads not in an urbanized area which provide access and connection to the PHFS and the Interstate with other important ports, public transportation facilities, or other intermodal freight facilities. Nationwide, there are 5,389 centerline miles designated as CRFCs as of January 27, 2023. CRFCs are not included in GIS data base.\nCritical Urban Freight Corridors (CUFCs): These are public roads in urbanized areas which provide access and connection to the PHFS and the Interstate with other ports, public transportation facilities, or other intermodal transportation facilities. Nationwide, there are 2,656 centerline miles designated as CUFC as of January 27, 2023. CUFCs are not included in GIS data base. A data dictionary, or other source of attribute information, is accessible at [https://doi.org/10.21949/1529856](https://doi.org/10.21949/1529856)\n\n### Access & Use Information\n\n**License:** No license information was provided. If this work was prepared by an officer or employee of the United States government as part of that person's official duties it is considered a [U.S. Government Work](http://www.usa.gov/publicdomain/label/1.0/).\n\n### Downloads & Resources\n\n- [Open Data Catalog](https://catalog.data.gov/dataset/national-highway-freight-network-nhfn1/resource/db169ca6-8402-4612-bc30-385a4d91b0ee)\nAn ESRI Geodatabase, ESRI shapefile, or spreadsheet.\n\n[Visit page](https://doi.org/10.21949/1530050)\n\n- [ArcGIS REST Services Directory Feature Server (WFS)](https://catalog.data.gov/dataset/national-highway-freight-network-nhfn1/resource/7db812c7-28fe-41b4-bd0a-ad99bbabe4c8)\ncomplies with\u00c2\u00a0OGC WFS Map Service for visualizing geospatial information\n\n[Visit page](https://doi.org/10.21949/1530051)\n\n\n### Dates\n\n| Metadata Date | July 16, 2025 |\n| Metadata Created Date | August 3, 2024 |\n| Metadata Updated Date | July 17, 2025 |\n| Reference Date(s) | January 23, 2024 (creation) |\n| Frequency Of Update | asNeeded |\n\n### Metadata Source\n\n- **ISO-19139**\n**ISO-19139 Metadata**\n[Download Metadata](https://catalog.data.gov/harvest/object/93e9416f-c0a2-44a0-a688-5f94cd27b7fc)\n\n\nHarvested from [National Transportation Atlas Database (NTAD) Metadata](https://catalog.data.gov/harvest/ee45e034-47c1-4a61-a3ec-de75031e6865)\n\n### Other Data Resources\n\n- **Geoplatform Metadata Information**\nView this on Geoplatform\n\n\n- [analysis](https://catalog.data.gov/dataset/?tags=analysis)\n- [atlas](https://catalog.data.gov/dataset/?tags=atlas)\n- [database](https://catalog.data.gov/dataset/?tags=database)\n- [freight](https://catalog.data.gov/dataset/?tags=freight)\n- [highways](https://catalog.data.gov/dataset/?tags=highways)\n- [line](https://catalog.data.gov/dataset/?tags=line)\n- [linear](https://catalog.data.gov/dataset/?tags=linear)\n- [national](https://catalog.data.gov/dataset/?tags=national)\n- [national...](https://catalog.data.gov/dataset/?tags=national+transportation+atlas+database)\n- [network](https://catalog.data.gov/dataset/?tags=network)\n- [nhfn](https://catalog.data.gov/dataset/?tags=nhfn)\n- [ntad](https://catalog.data.gov/dataset/?tags=ntad)\n- [roads](https://catalog.data.gov/dataset/?tags=roads)\n- [transportation](https://catalog.data.gov/dataset/?tags=transportation)\n- [usa](https://catalog.data.gov/dataset/?tags=usa)\n- [vector](https://catalog.data.gov/dataset/?tags=vector)\n\n### Additional Metadata\n\n| Resource Type | Dataset |\n| Metadata Date | July 16, 2025 |\n| Metadata Created Date | August 3, 2024 |\n| Metadata Updated Date | July 17, 2025 |\n| Reference Date(s) | January 23, 2024 (creation) |\n| Responsible Party | Federal Highway Administration (FHWA) (Point of Contact) |\n| Contact Email | [birat.pandey@dot.gov](mailto:birat.pandey@dot.gov) |\n| Guid | National Highway Freight Network (NHFN) |\n| Access Constraints | Access Constraints: unrestricted |\n| Bbox East Long | -65.633044999999996 |\n| Bbox North Lat | 64.860119999999995 |\n| Bbox South Lat | 17.969393 |\n| Bbox West Long | -159.39578499999999 |\n| Coupled Resource |\n| Frequency Of Update | asNeeded |\n| Harvest Object Id | 93e9416f-c0a2-44a0-a688-5f94cd27b7fc |\n| Harvest Source Id | ee45e034-47c1-4a61-a3ec-de75031e6865 |\n| Harvest Source Title | National Transportation Atlas Database (NTAD) Metadata |\n| Licence | The NHFN Version 2023.02.08 database, or any portion thereof, can be freely distributed as long as this metadata entry is included with each distribution. The original metadata entry cannot be modified or deleted from any data transfer. This NTAD dataset is a work of the United States government as defined in 17 U.S.C. \u00c2\u00a7 101 and as such are not protected by any U.S. copyrights. This work is available for unrestricted public use. |\n| Lineage |\n| Metadata Language | eng; USA |\n| Metadata Type | geospatial |\n| Old Spatial | {\"type\": \"Polygon\", \"coordinates\": \\[\\[\\[-159.395785, 17.969393\\], \\[-65.633045, 17.969393\\], \\[-65.633045, 64.86012\\], \\[-159.395785, 64.86012\\], \\[-159.395785, 17.969393\\]\\]\\]} |\n| Progress | onGoing |\n| Spatial Data Service Type |\n| Spatial Reference System |\n| Spatial Harvester | True |\n\nDidn't find what you're looking for? Suggest a dataset [here](https://www.data.gov/contact).",
          "original_query": "Highway networks",
          "cleaned_query": "Highway networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Forgetting Transformer: Softmax Attention with a Forget Gate - arXiv",
          "url": "https://arxiv.org/html/2503.02130",
          "content": "Forgetting Transformer: Softmax Attention with a Forget Gate\n\\\\doparttoc\\\\faketableofcontents\n# Forgetting Transformer: Softmax Attention with a Forget Gate\nZhixuan Lin\nMila &amp; Universit\u00e9 de Montr\u00e9al\nzxlin.cs@gmail.com&amp;Evgenii Nikishin\nMila &amp; Universit\u00e9 de Montr\u00e9al\nevgenii.nikishin@mila.quebec&amp;Xu Owen He\nMakerMaker AI\nowen.hexu@gmail.com&amp;Aaron Courville\nMila &amp; Universit\u00e9 de Montr\u00e9al\ncourvila@mila.quebecCorrespondence to Zhixuan Lin.Work done while at Google DeepMind.\n###### Abstract\nAn essential component of modern recurrent sequence models is the*forget gate*. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism*Forgetting Attention*and the resulting model the*Forgetting Transformer (FoX)*. We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer\u2019s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a \u201cPro\u201d block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer.\nOur code is available at[https://github.com/zhixuan-lin/forgetting-transformer](https://github.com/zhixuan-lin/forgetting-transformer).\n### 1Introduction\nDespite the growing interest in reviving recurrent sequence models> (Gu et\u00a0al., [> 2021\n](https://arxiv.org/html/2503.02130v2#bib.bib25)> ; Peng et\u00a0al., [> 2021\n](https://arxiv.org/html/2503.02130v2#bib.bib50)> ; Yang et\u00a0al., [> 2023\n](https://arxiv.org/html/2503.02130v2#bib.bib72)> ; Gu &amp; Dao, [> 2023\n](https://arxiv.org/html/2503.02130v2#bib.bib24)> ; Sun et\u00a0al., [> 2023\n](https://arxiv.org/html/2503.02130v2#bib.bib62)> ; De et\u00a0al., [> 2024\n](https://arxiv.org/html/2503.02130v2#bib.bib16)> ; Qin et\u00a0al., [> 2024b\n](https://arxiv.org/html/2503.02130v2#bib.bib53)> ; Dao &amp; Gu, [> 2024\n](https://arxiv.org/html/2503.02130v2#bib.bib14)> ; Peng et\u00a0al., [> 2024\n](https://arxiv.org/html/2503.02130v2#bib.bib49)> ; Beck et\u00a0al., [> 2024\n](https://arxiv.org/html/2503.02130v2#bib.bib4)> ; Zhang et\u00a0al., [> 2024\n](https://arxiv.org/html/2503.02130v2#bib.bib76)> )\n, these models still underperform the Transformer> (Vaswani et\u00a0al., [> 2017\n](https://arxiv.org/html/2503.02130v2#bib.bib68)> )\nin terms of*long-context capabilities*> (Hsieh et\u00a0al., [> 2024\n](https://arxiv.org/html/2503.02130v2#bib.bib28)> ; Waleffe et\u00a0al., [> 2024\n](https://arxiv.org/html/2503.02130v2#bib.bib69)> ; Shen et\u00a0al., [> 2024\n](https://arxiv.org/html/2503.02130v2#bib.bib58)> ; Qin et\u00a0al., [> 2024a\n](https://arxiv.org/html/2503.02130v2#bib.bib52)> )\n, likely due to their relatively small fixed-sized hidden states> (Jelassi et\u00a0al., [> 2024\n](https://arxiv.org/html/2503.02130v2#bib.bib30)> )\n. While the Transformer excels in handling long-context information, it lacks an explicit mechanism for forgetting past information in a*data-dependent*way. Such a mechanism \u2013often implemented as some form of the*forget gate*> (Gers et\u00a0al., [> 2000\n](https://arxiv.org/html/2503.02130v2#bib.bib21)> )\n\u2013is ubiquitous in recurrent sequence models and has proven critical in their success in short-context tasks> (Greff et\u00a0al., [> 2016\n](https://arxiv.org/html/2503.02130v2#bib.bib23)> ; Van Der\u00a0Westhuizen &amp; Lasenby, [> 2018\n](https://arxiv.org/html/2503.02130v2#bib.bib67)> ; Peng et\u00a0al., [> 2021\n](https://arxiv.org/html/2503.02130v2#bib.bib50)> ; Yang et\u00a0al., [> 2023\n](https://arxiv.org/html/2503.02130v2#bib.bib72)> ; Gu &amp; Dao, [> 2023\n](https://arxiv.org/html/2503.02130v2#bib.bib24)> )\n. A natural question to ask is then: can we have a forget gate in Transformers?\nTo address this question, we leverage an important fact: many recurrent sequence models with a forget gate can be written in a parallel linear attention form> (Katharopoulos et\u00a0al., [> 2020\n](https://arxiv.org/html/2503.02130v2#bib.bib33)> )\nanalogous to softmax attention> (Yang et\u00a0al., [> 2023\n](https://arxiv.org/html/2503.02130v2#bib.bib72)> ; Dao &amp; Gu, [> 2024\n](https://arxiv.org/html/2503.02130v2#bib.bib14)> )\n. In this parallel form, the forget gate mechanism translates into down-weighing the unnormalized attention scores in a data-dependent way. Our key insight is that this*exact*mechanism is also applicable to softmax attention. We name this attention mechanism*Forgetting Attention*and the resulting model the*ForgettingTransformer (FoX)*.\nWe show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Notably, it does not require any positional embeddings. It also retains Transformers\u2019 long-context retrieval abilities and achieves near-perfect accuracy in the needle-in-the-haystack test> (Kamradt, [> 2023\n](https://arxiv.org/html/2503.02130v2#bib.bib32)> )\nwithin the training context length. In contrast, all the tested recurrent sequence models fail. We also introduce a \u201cPro\u201d block design that integrates several architectural components commonly used in recurrent sequence models, which significantly improves the performance of FoX and the baseline Transformer. Finally, we show that FoX can be implemented in a hardware-aware way with a simple modification to the FlashAttention> (Dao, [> 2023\n](https://arxiv.org/html/2503.02130v2#bib.bib13)> )\nalgorithm.\n### 2Background: Linear attention with a forget gate\nThis section introduces the notation used in this work and gives a brief background on linear attention. We also introduce a gated variant of linear attention and discuss its parallel form, which naturally leads to FoX. Throughout this work, we only consider causal sequence modeling.\n#### 2.1Linear Attention\nStandard causal softmax attention takes a sequence of input vectors(\ud835\udc99i)i=1Lsuperscriptsubscriptsubscript\ud835\udc99\ud835\udc56\ud835\udc561\ud835\udc3f({\\\\bm{x}}\\_{i})\\_{i=1}^{L}( bold\\_italic\\_x start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ) start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_L end\\_POSTSUPERSCRIPTand produces a sequence of output vectors(\ud835\udc90i)i=1Lsuperscriptsubscriptsubscript\ud835\udc90\ud835\udc56\ud835\udc561\ud835\udc3f({\\\\bm{o}}\\_{i})\\_{i=1}^{L}( bold\\_italic\\_o start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ) start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_L end\\_POSTSUPERSCRIPT, where\ud835\udc99i,\ud835\udc90i\u2208\u211dd,i\u2208{1,\u2026,L}formulae-sequencesubscript\ud835\udc99\ud835\udc56subscript\ud835\udc90\ud835\udc56superscript\u211d\ud835\udc51\ud835\udc561\u2026\ud835\udc3f{\\\\bm{x}}\\_{i},{\\\\bm{o}}\\_{i}\\\\in\\\\mathbb{R}^{d},i\\\\in\\\\{1,\\\\ldots,L\\\\}bold\\_italic\\_x start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , bold\\_italic\\_o start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT \u2208blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_d end\\_POSTSUPERSCRIPT , italic\\_i \u2208{ 1 , \u2026, italic\\_L }. Each\ud835\udc90isubscript\ud835\udc90\ud835\udc56{\\\\bm{o}}\\_{i}bold\\_italic\\_o start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPTis computed as follows:\n|\ud835\udc92i,\ud835\udc8ci,\ud835\udc97i=\ud835\udc7eq\u2062\ud835\udc99i,subscript\ud835\udc92\ud835\udc56subscript\ud835\udc8c\ud835\udc56subscript\ud835\udc97\ud835\udc56subscript\ud835\udc7e\ud835\udc5esubscript\ud835\udc99\ud835\udc56\\\\displaystyle{\\\\bm{q}}\\_{i},{\\\\bm{k}}\\_{i},{\\\\bm{v}}\\_{i}={\\\\bm{W}}\\_{q}{\\\\bm{x}}\\_{i},bold\\_italic\\_q start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , bold\\_italic\\_k start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , bold\\_italic\\_v start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT = bold\\_italic\\_W start\\_POSTSUBSCRIPT italic\\_q end\\_POSTSUBSCRIPT bold\\_italic\\_x start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ,|\ud835\udc7ek\u2062\ud835\udc99i,\ud835\udc7ev\u2062\ud835\udc99i\u2208\u211dd,subscript\ud835\udc7e\ud835\udc58subscript\ud835\udc99\ud835\udc56subscript\ud835\udc7e\ud835\udc63subscript\ud835\udc99\ud835\udc56superscript\u211d\ud835\udc51\\\\displaystyle{\\\\bm{W}}\\_{k}{\\\\bm{x}}\\_{i},{\\\\bm{W}}\\_{v}{\\\\bm{x}}\\_{i}\\\\in{\\\\mathbb{R}}^%\n{d},bold\\_italic\\_W start\\_POSTSUBSCRIPT italic\\_k end\\_POSTSUBSCRIPT bold\\_italic\\_x start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , bold\\_italic\\_W start\\_POSTSUBSCRIPT italic\\_v end\\_POSTSUBSCRIPT bold\\_italic\\_x start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT \u2208blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_d end\\_POSTSUPERSCRIPT ,||(1)|\n|\ud835\udc90i=\u2211j=1ikexp\u2062(\ud835\udc92i,\ud835\udc8cj)\u2062\ud835\udc97j\u2211j=1ikexp\u2062(\ud835\udc92i,\ud835\udc8cj)subscript\ud835\udc90\ud835\udc56superscriptsubscript\ud835\udc571\ud835\udc56subscript\ud835\udc58subscript\ud835\udc92\ud835\udc56subscript\ud835\udc8c\ud835\udc57subscript\ud835\udc97\ud835\udc57superscriptsubscript\ud835\udc571\ud835\udc56subscript\ud835\udc58subscript\ud835\udc92\ud835\udc56subscript\ud835\udc8c\ud835\udc57\\\\displaystyle{\\\\bm{o}}\\_{i}=\\\\frac{\\\\sum\\_{j=1}^{i}k\\_{\\\\exp}({\\\\bm{q}}\\_{i},{\\\\bm{k}}\\_{%\nj}){\\\\bm{v}}\\_{j}}{\\\\sum\\_{j=1}^{i}k\\_{\\\\exp}({\\\\bm{q}}\\_{i},{\\\\bm{k}}\\_{j})}bold\\_italic\\_o start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT = divide start\\_ARG \u2211start\\_POSTSUBSCRIPT italic\\_j = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT italic\\_k start\\_POSTSUBSCRIPT roman\\_exp end\\_POSTSUBSCRIPT ( bold\\_italic\\_q start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , bold\\_italic\\_k start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ) bold\\_italic\\_v start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT end\\_ARG start\\_ARG \u2211start\\_POSTSUBSCRIPT italic\\_j = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_i end\\_POSTSUPERSCRIPT italic\\_k start\\_POSTSUBSCRIPT roman\\_exp end\\_POSTSUBSCRIPT ( bold\\_italic\\_q start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , bold\\_italic\\_k start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ) end\\_ARG|=\u2211j=1iexp\u2061(\ud835\udc92i\u22a4\u2062\ud835\udc8cj)\u2062\ud835\udc97j\u2211j=1iexp\u2061(\ud835\udc92i\u22a4\u2062\ud835\udc8cj),absentsuperscriptsubscript\ud835\udc571\ud835\udc56superscriptsubscript\ud835\udc92\ud835\udc56topsubscript\ud835\udc8c\ud835\udc57subscript\ud835\udc97\ud835\udc57superscriptsubscript\ud835\udc571\ud835\udc56superscriptsubscript\ud835\udc92\ud835\udc56topsubscript\ud835\udc8c\ud835\udc57\\\\displaystyle=\\\\frac{\\\\sum\\_{j=1}^{i}\\\\exp({\\\\bm{q}}\\_{i}^{\\\\top}{\\\\bm{k}}\\_{j}){\\\\bm{v}%\n}\\_{j}}{\\\\sum\\_{j=1}^{i}\\\\exp({\\\\bm{q}}\\_{i}^{\\\\top}{\\\\bm{k}}\\_{j})},= divide start\\_ARG \u2211start\\_POSTSUBSCRIPT i",
          "original_query": "Forgetting transformer: Softmax attention with a forget gate",
          "cleaned_query": "Forgetting transformer: Softmax attention with a forget gate",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "[PDF] Transformer Quality in Linear Time",
          "url": "https://icml.cc/media/icml-2022/Slides/17274.pdf",
          "content": "Transformer Quality in Linear Time\nWeizhe Hua*\n12, Zihang Dai*2\n, Hanxiao Liu*\n2\n, Quoc V. Le2\n*Equal contribution\nCornell Univeristy1; Google Research, Brain Team2\nSession 8 Track 3 @ICML 2022\nMotivation\nEmerging applications need to handle long contexts.\nTransformers: O(T2) over context length T.\nO(T) variants exist, but are rarely deployed:\n\u25cf Inferior Quality\n\u25cf Overhead in Practice\n\u25cb Memory re-formatting ops [Tay ICLR\u201921]\n\u25cf Inefficient Auto-regressive Training\n\u25cb Sequential state updates in linear attention Q(\u03d5\n(K)^T\u03d5(V)) [Choromanski ICLR\u201921]\nGoogle LaMDA\ncredit: ai.googleblog.com\nContribution\nKey Ideas\n\u25cf Simplify the Transformer layer to make it more suitable for approximation\n\u25cf An accelerator-friendly approximation of attention\nOur model \u2014 FLASH\nFast Linear Attention with a Single Head:\n\u25cf Quality: ~= Augmented Transformer\n\u25cf Complexity: O(T2) \u21d2 O(T)\n\u25cf Speed: Fast on TPUs & GPUs\nReplace Transformer with Gated Attention Unit\nMost ops in MHSA\n(purple) are removed\n\u25cf Compute & Memory Savings:\n\u25cb Shared Q and K with small hidden dimension (e.g., 128)\n\u25cb Single head attention reduces memory complexity from O(HT2) to O(T2)\n\u25cf Friendly to further approximation of the attention:\n\u25cb Enable deeper attention stacks: Transformer layer \u21d2 2 GAUs\nGAU vs. Transformers (short seqs)\nGAUs are at least competitive with Transformers across different model sizes even\nwhen the context length is relatively short (512).\nReduced Compute:\nglobal linear ( )\nPseudocode ( n:# of chunks, c:chunk size)\nlin_kv = enisum(\u2018bncd,bnce->bnde\u2018, k, v)\nlin_kv = cumsum(lin_kv, axis=1, exclusive=True)\nlin = enisum(\u2018bncd,bnde->bnce\u2018,q, lin_kv)\nMixed Chunk Attention (ours)\noi = qi ( \u2211 j \u2208 global kj vj\nT\n)\nlinear\nquadratic linear\nOurs\nReduced Compute:\nglobal linear ( )\nPseudocode ( n:# of chunks, c:chunk size)\nlin_kv = enisum(\u2018bncd,bnce->bnde\u2018, k, v)\nlin_kv = cumsum(lin_kv, axis=1, exclusive=True)\nlin = enisum(\u2018bncd,bnde->bnce\u2018,q, lin_kv)\nReduced RNN-like steps/states ( )\n# of tokens (e.g., 8K) \u21d2 # of states (e.g., 16)\nMixed Chunk Attention (ours)\nquadratic linear\nOurs\noi = qi ( \u2211 j \u2208 global kj vj\nT\n)\nlinear\nReduced Compute:\nglobal linear ( ) + Local quadratic ( )\nPseudocode ( n:# of chunks, c:chunk size)\nlin_kv = enisum(\u2018bncd,bnce->bnde\u2018, k, v)\nlin_kv = cumsum(lin_kv, axis=1, exclusive=True)\nlin = enisum(\u2018bncd,bnde->bnce\u2018,q, lin_kv)\nReduced RNN-like steps/states ( )\n# of tokens (e.g., 8K) \u21d2 # of states (e.g., 16)\nMixed Chunk Attention (ours)\nquadratic linear\nOurs\noi= qi ( \u2211 j \u2208 global kj vj\nT\n) + \u2211 j \u2208 local relu2( qi\nT\nkj) vj\nlinear quadratic\nMasked Language Modeling on C4\nFLASH reduces the training cost of Transformer++ by 1.0\u00d7 (T=512) \u2013 4.8\u00d7 (T=8K).\nT = 512 T = 2048 T = 8192\nLanguage Modeling on Wiki-40B\nFLASH reduces the training cost of Transformer++ by 1.2\u00d7 (T=512) \u2013 4.9\u00d7 (T=8K).\nT = 512 T = 2048 T = 8192\nLanguage Modeling on PG-19\nFLASH outperforms the quality of Transformer+ while being up to 12.1x faster!\nPG-19 [Rae ICLR\u201920]: books extracted from Project Gutenberg\n\u25cf Avg. length: 69K words\nFine-tuning on TriviaQA\nFLASH reduces the pretraining/fine-tuning cost of Transformer+ by 2.8\u00d7 and 2.7\u00d7.\nTriviaQA [Joshi ACL\u201917]: Passages can span multiple documents\n\u25cf Pretraining and fine-tuning with 4K context length\nSummary\nMore in the paper:\n\u25cf Experimental setups & implementation details\n\u25cf Ablation studies\nConclusion:\n\u25cf We propose a high-quality and high-performance sequence model, called\nFLASH, which achieves the transformer quality in linear time.\nTransformer Quality in Linear Time\nWeizhe Hua*\n12, Zihang Dai*2\n, Hanxiao Liu*\n2\n, Quoc Le2\n*Equal contribution\nCornell Univeristy1\n; Google Research, Brain Team2\nSession 8 Track 3 @ICML 2022",
          "original_query": "Transformer quality in linear time",
          "cleaned_query": "Transformer quality in linear time",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2405.16039] MoEUT: Mixture-of-Experts Universal Transformers",
          "url": "https://arxiv.org/html/2405.16039",
          "content": "MoEUT: Mixture-of-Experts Universal Transformers\n# MoEUT: Mixture-of-Experts Universal Transformers\nR\u00f3bert Csord\u00e1s1,2\u2020Kazuki Irie3J\u00fcrgen Schmidhuber2,4\nChristopher Potts1Christopher D.\u00a0Manning1\n1Stanford University, Stanford, CA, USA\n2The Swiss AI Lab IDSIA, USI &amp; SUPSI, Lugano, Switzerland\n3Center for Brain Science, Harvard University, Cambridge, MA, USA\n4AI Initiative, KAUST, Thuwal, Saudi Arabia\n{rcsordas,cgpotts,manning}@stanford.edu\nkirie@fas.harvard.edu,juergen@idsia.ch\n###### Abstract\n22footnotetext:Work started at IDSIA.\nPrevious work on Universal Transformers (UTs) has demonstrated the importance of parameter sharing across layers. By allowing recurrence in depth, UTs have advantages over standard Transformers in learning compositional generalizations, but layer-sharing comes with a practical limitation of parameter-compute ratio:\nit drastically reduces the parameter count compared to the non-shared model with the same dimensionality. Naively scaling up the layer size to compensate for the loss of parameters makes its computational resource requirements prohibitive. In practice, no previous work has succeeded in proposing a shared-layer Transformer design that is competitive in parameter count-dominated tasks such as language modeling. Here we propose MoEUT (pronounced \u201cmoot\u201d), an effective mixture-of-experts (MoE)-based shared-layer Transformer architecture, which combines several recent advances in MoEs for both feedforward and attention layers of standard Transformers together with novel layer-normalization and grouping schemes that are specific and crucial to UTs. The resulting UT model, for the first time, slightly outperforms standard Transformers on language modeling tasks such as BLiMP and PIQA, while using significantly less compute and memory.111Our code is public:[https://github.com/robertcsordas/moeut](https://github.com/robertcsordas/moeut)\n## 1Introduction\nTransformers> [\n[> 1\n](https://arxiv.org/html/2405.16039v2#bib.bib1)> , [> 2\n](https://arxiv.org/html/2405.16039v2#bib.bib2)> ]\nare ubiquitous neural architectures in modern machine learning. They power large language models> [\n[> 3\n](https://arxiv.org/html/2405.16039v2#bib.bib3)> , [> 4\n](https://arxiv.org/html/2405.16039v2#bib.bib4)> , [> 5\n](https://arxiv.org/html/2405.16039v2#bib.bib5)> , [> 6\n](https://arxiv.org/html/2405.16039v2#bib.bib6)> , [> 7\n](https://arxiv.org/html/2405.16039v2#bib.bib7)> ]\n, modern image processors> [\n[> 8\n](https://arxiv.org/html/2405.16039v2#bib.bib8)> ]\n, offline reinforcement learning agents> [\n[> 9\n](https://arxiv.org/html/2405.16039v2#bib.bib9)> ]\n, and many others.\nDespite these successes, we should ask whether more optimal architectures exist.\nOne important candidate is the Universal Transformer (UT,> [\n[> 10\n](https://arxiv.org/html/2405.16039v2#bib.bib10)> ]\n).\nThe core characteristic of UTs is*recurrence in depth*via*sharing parameters across layers*. This reintroduces the expressive power of recurrence provided by recurrent neural networks (RNNs,> [\n[> 11\n](https://arxiv.org/html/2405.16039v2#bib.bib11)> , [> 12\n](https://arxiv.org/html/2405.16039v2#bib.bib12)> , [> 13\n](https://arxiv.org/html/2405.16039v2#bib.bib13)> ]\n).\nLayer sharing allows UTs to outperform regular Transformers on compositional problems such as logical inference tasks, while also yielding improvements onsmall-scalelanguage modeling and translation tasks.\nIn particular, UTs have been shown to have better compositional generalization properties> [\n[> 14\n](https://arxiv.org/html/2405.16039v2#bib.bib14)> , [> 15\n](https://arxiv.org/html/2405.16039v2#bib.bib15)> ]\nby being able to decompose structured problems without supervision and generalize to longer sequences> [\n[> 16\n](https://arxiv.org/html/2405.16039v2#bib.bib16)> ]\n.222> Dehghani et\u00a0al. [\n[> 10\n](https://arxiv.org/html/2405.16039v2#bib.bib10)> ]\nalso augment UTs with an additional adaptive computation time (ACT,> [\n[> 17\n](https://arxiv.org/html/2405.16039v2#bib.bib17)> , [> 18\n](https://arxiv.org/html/2405.16039v2#bib.bib18)> ]\n) mechanism.\nHowever, the benefits of UTs we discuss here are purely due to layer-sharing, which, in consequence, is the focus of this work. Our models could also optionally be augmented with ACT but this is out of scope here.These empirical findings confirm that UTs are more general architectures with superior generalization properties compared to standard Transformers, in principle.\nHowever, UTs suffer from a fundamental problem ofparameter\u2013compute ratio: sharing the parameters amongL\ud835\udc3fLitalic\\_Llayers of anL\ud835\udc3fLitalic\\_L-layer Transformer\u2014while keeping the same model dimensionalities\u2014results in a model withL\ud835\udc3fLitalic\\_Ltimes fewer parameters (ignoring the input/output layers to simplify the discussion). Upscaling the size of the layer to compensate for the loss of parameters (essentially by making itL\ud835\udc3fLitalic\\_Ltimes wider) usually yields a very big layer whose computational requirements in terms of compute and memory are prohibitive in practice> [\n[> 19\n](https://arxiv.org/html/2405.16039v2#bib.bib19)> , [> 20\n](https://arxiv.org/html/2405.16039v2#bib.bib20)> ]\n.\nIn sum, despite their potential, UTs are much less compute-efficient than standard Transformers, and thus, they are not popular for parameter-dominated tasks such as modern language modeling.\nIndeed, we are not aware of any previous work that has succeeded in developing compute-efficient UT models that yield competitive performance compared to standard Transformers on such tasks.\nHere we bring new perspectives and a solution to UTs\u2019 fundamental compute\u2013parameter ratio problem.\nWe present Mixture-of-Experts Universal Transformers (MoEUTs, pronounced \u201cmoot\u201d), a mixture-of-experts (MoE) architecture> [\n[> 21\n](https://arxiv.org/html/2405.16039v2#bib.bib21)> , [> 22\n](https://arxiv.org/html/2405.16039v2#bib.bib22)> , [> 23\n](https://arxiv.org/html/2405.16039v2#bib.bib23)> ]\nfor UTs enabling them to scale in a computationally and memory efficient way.\nWe leverage various recent advances in MoEs for both feedforward and self-attention layers (Sec.[2.1](https://arxiv.org/html/2405.16039v2#S2.SS1)and[2.2](https://arxiv.org/html/2405.16039v2#S2.SS2)),\nand combine them with two new innovations: (1)*layer grouping*, in which we recurrently stack groups of MoE-based layers, and (2) a*peri-layernorm*scheme (which is \u201cin-between\u201d the standard pre- and post-layernorm), in which we apply layer norm only before linear layers that immediately precede sigmoid or softmax activations. Both are specifically designed for shared-layer MoE architectures, and strongly supported by empirical evidence.\nMoEUTs allow us to build parameter- and resource-efficient UT language models outperforming standard Transformers with less compute and memory requirements on all scales on which we can afford to test (up to 1B parameters). We demonstrate their capabilities on the C4, SlimPajama, and peS2o language modeling datasets, as well as on The Stack code generation.\nOur experiments show that recurrence is essential for our models to achieve competitive performance.\nWe also demonstrate good zero-shot performance on downstream tasks like BLiMP and Children\u2019s Book Test, Lambada, HellaSwag, PIQA and ARC-E.\n## 2The MoEUT Architecture\nOur MoEUT architecture is a Transformer architecture with shared layer parameters,\nin which we address the parameter-compute ratio problem by using mixture-of-experts.\nWhile there are many recent works on MoE methods for Transformer language models (e.g.,> [\n[> 24\n](https://arxiv.org/html/2405.16039v2#bib.bib24)> , [> 25\n](https://arxiv.org/html/2405.16039v2#bib.bib25)> , [> 26\n](https://arxiv.org/html/2405.16039v2#bib.bib26)> , [> 27\n](https://arxiv.org/html/2405.16039v2#bib.bib27)> , [> 28\n](https://arxiv.org/html/2405.16039v2#bib.bib28)> ]\n), making them competitive against their dense counterparts inparameter-equalcomparisons is known to be challenging> [\n[> 28\n](https://arxiv.org/html/2405.16039v2#bib.bib28)> ]\n.\nHere we leverage recent advances in MoE methods for both the feedforward network block (FFN, or simply MLP layer or feedforward layer; Sec.[2.1](https://arxiv.org/html/2405.16039v2#S2.SS1)) and the self-attention layer (Sec.[2.2](https://arxiv.org/html/2405.16039v2#S2.SS2)) together with two novel methods that take into account the specific properties of shared-layer models, namely: layer grouping (Sec.[2.3](https://arxiv.org/html/2405.16039v2#S2.SS3)) and signal propagation (Sec.[2.4](https://arxiv.org/html/2405.16039v2#S2.SS4)), which, taken together,\nare crucial for achieving effective shared-layer MoE Transformers.\n### 2.1MoE Feedforward Blocks\nTo parameterize the feedforward blocks of our shared-layer Transformers by an MoE,\nwe use\u03c3\ud835\udf0e\\\\sigmaitalic\\_\u03c3-MoE> [\n[> 28\n](https://arxiv.org/html/2405.16039v2#bib.bib28)> ]\nwith a few modifications.\u03c3\ud835\udf0e\\\\sigmaitalic\\_\u03c3-MoE divides the feedforward block intoNEsubscript\ud835\udc41\ud835\udc38N\\_{E}italic\\_N start\\_POSTSUBSCRIPT italic\\_E end\\_POSTSUBSCRIPTslices, calledexperts. Each expert has two sets of weights,\ud835\udc7e1e\u2208\u211ddmodel\u00d7dexpertsuperscriptsubscript\ud835\udc7e1\ud835\udc52superscript\u211dsubscript\ud835\udc51modelsubscript\ud835\udc51expert{\\\\bm{W}}\\_{1}^{e}\\\\in\\\\mathbb{R}^{d\\_{\\\\text{model}}\\\\times d\\_{\\\\text{expert}}}bold\\_italic\\_W start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_e end\\_POSTSUPERSCRIPT \u2208blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_d start\\_POSTSUBSCRIPT model end\\_POSTSUBSCRIPT \u00d7italic\\_d start\\_POSTSUBSCRIPT expert end\\_POSTSUBSCRIPT end\\_POSTSUPERSCRIPTand\ud835\udc7e2e\u2208\u211ddexpert\u00d7dmodelsuperscriptsubscript\ud835\udc7e2\ud835\udc52superscript\u211dsubscript\ud835\udc51expertsubscript\ud835\udc51model{\\\\bm{W}}\\_{2}^{e}\\\\in\\\\mathbb{R}^{d\\_{\\\\text{expert}}\\\\times d\\_{\\\\text{model}}}bold\\_italic\\_W start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_e end\\_POSTSUPERSCRIPT \u2208blackboard\\_R start\\_POSTSUPERSCRIPT italic\\_d start\\_POSTSUBSCRIPT expert end\\_POSTSUBSCRIPT \u00d7italic\\_d start\\_POSTSUBSCRIPT model end\\_POSTSUBSCRIPT end\\_POSTSUPERSCRIPT, wheree\u2208{1,\u2026,NE}\ud835\udc521\u2026subscript\ud835\udc41\ud835\udc38e\\\\in\\\\{1,\\\\",
          "original_query": "Mixture-of-experts universal transformers",
          "cleaned_query": "Mixture-of-experts universal transformers",
          "content_quality": "good_from_html"
        }
      ],
      "generated_ideas": [
        "FoX-Flash: Forget-Gated Mixed-Chunk Linear Attention\nIntegrate the Forgetting Transformer\u2019s data-dependent forget gate into FLASH\u2019s mixed chunk attention by applying the forget modulation to both (i) global linear attention score accumulation and (ii) local quadratic windows. Evaluate long-context language modeling and length extrapolation, while measuring real hardware throughput (FlashAttention compatibility, TPU/GPU kernel efficiency) versus standard FLASH and FoX.",
        "Adaptive Forget Schedules for Universal Transformers (UT)\nAdd a per-step (depth-iteration) forget gate to UT recurrence so that each recurrent refinement can selectively down-weight earlier token interactions rather than uniformly reprocessing context. Train with a stability-focused regularizer (e.g., penalize oscillatory gate dynamics across depth) and test compositional generalization tasks plus long-context retrieval to see when adaptive forgetting improves UT convergence and extrapolation.",
        "MoE-Routed Forget Gates in MoEUT for Compute-Aware Context Management\nExtend MoEUT by routing not only feedforward/attention experts but also specialized \u201cforget-gate experts\u201d that learn distinct forgetting behaviors (e.g., rapid decay vs. long retention) conditioned on token type or uncertainty. Implement a lightweight gate-router and compare against static forget gates on parameter-dominated LMs, reporting quality/compute trade-offs and expert specialization metrics.",
        "Positional-Embedding-Free GIS Sequence Modeling on NHFN with Forgetting Attention\nTreat NHFN road segments as sequences (e.g., paths, corridors, or learned traversals) and model freight-relevant attributes using FoX\u2019s no-positional-embedding property to reduce reliance on handcrafted spatial encodings. Construct tasks like segment-level classification (freight-critical vs. non-critical proxies), missing-attribute imputation, or corridor anomaly detection, and test whether forgetting attention improves generalization across states/regions.",
        "Trajectory-to-Corridor Mapping with UT Recurrence and Forget Gates\nBuild a model that maps noisy vehicle/freight trajectories to most-likely NHFN corridors using a UT-style iterative refinement: each depth step updates the path alignment while a forget gate suppresses inconsistent historical matches. Evaluate against HMM/CRF and transformer baselines on synthetic noise regimes and (if available) real GPS traces, focusing on robustness to long trips and detours.",
        "Needle-in-the-Haystack for Infrastructure: Long-Context Event Retrieval on Freight Networks\nCreate a benchmark where the \u201cneedle\u201d is a rare but important event (e.g., a critical link closure indicator embedded in long multi-source logs: weather, incidents, restrictions) tied to NHFN segments. Compare FoX, standard Transformers, and linear-time models (FLASH) on retrieving the relevant event and predicting downstream impacts (e.g., affected corridors), emphasizing length extrapolation and retrieval accuracy.",
        "State-Space vs. Forget-Gated Attention: Controlled Study of Long-Range Dependencies\nDesign synthetic and semi-synthetic tasks that isolate specific dependency types relevant to routing and language (e.g., periodic constraints, delayed triggers, hierarchical composition) and evaluate FoX/Transformer/FLASH/MoEUT under matched compute. Provide a taxonomy of which dependency structures benefit from explicit forgetting in attention versus recurrence-in-depth or linear approximations, with ablations on gate placement and granularity.",
        "Kernel-Level Co-Design: FlashAttention-Compatible Forget Gates with Quantization\nDevelop a fused kernel that combines FlashAttention-style tiling with forget-gate modulation and low-precision quantization of attention logits/activations, targeting minimal overhead versus baseline FlashAttention. Report end-to-end training speedups and quality impacts on long-context LM and one non-text domain (e.g., NHFN-based sequences), including sensitivity to quantization scale under dynamic forgetting.",
        "Expert-Conditional Chunking: MoE-Driven Dynamic Local/Global Attention Allocation\nExtend FLASH\u2019s mixed chunk attention by letting an MoE router choose chunk sizes and the proportion of local quadratic vs. global linear attention per token or per layer. Train with a budget constraint (fixed FLOPs) and evaluate whether dynamic chunking improves both short-context tasks (where local detail matters) and long-context retrieval (where global memory matters), compared to static chunking.",
        "Interpretable Forgetting Maps for Auditing Long-Context Models\nCreate tools that convert learned forget-gate values (FoX or forget-augmented UT/MoEUT) into \u201cforgetting maps\u201d over tokens/segments, analogous to attention maps but explicitly showing what the model suppresses over time/depth. Validate interpretability by correlating gate dynamics with known distractors (in text) or irrelevant road segments (in NHFN tasks), and test whether gate-based explanations better predict failure cases than raw attention visualization."
      ],
      "generated_ideas_raw": "1. **FoX-Flash: Forget-Gated Mixed-Chunk Linear Attention**\n   Integrate the Forgetting Transformer\u2019s data-dependent forget gate into FLASH\u2019s mixed chunk attention by applying the forget modulation to both (i) global linear attention score accumulation and (ii) local quadratic windows. Evaluate long-context language modeling and length extrapolation, while measuring real hardware throughput (FlashAttention compatibility, TPU/GPU kernel efficiency) versus standard FLASH and FoX.\n\n2. **Adaptive Forget Schedules for Universal Transformers (UT)**\n   Add a per-step (depth-iteration) forget gate to UT recurrence so that each recurrent refinement can selectively down-weight earlier token interactions rather than uniformly reprocessing context. Train with a stability-focused regularizer (e.g., penalize oscillatory gate dynamics across depth) and test compositional generalization tasks plus long-context retrieval to see when adaptive forgetting improves UT convergence and extrapolation.\n\n3. **MoE-Routed Forget Gates in MoEUT for Compute-Aware Context Management**\n   Extend MoEUT by routing not only feedforward/attention experts but also specialized \u201cforget-gate experts\u201d that learn distinct forgetting behaviors (e.g., rapid decay vs. long retention) conditioned on token type or uncertainty. Implement a lightweight gate-router and compare against static forget gates on parameter-dominated LMs, reporting quality/compute trade-offs and expert specialization metrics.\n\n4. **Positional-Embedding-Free GIS Sequence Modeling on NHFN with Forgetting Attention**\n   Treat NHFN road segments as sequences (e.g., paths, corridors, or learned traversals) and model freight-relevant attributes using FoX\u2019s no-positional-embedding property to reduce reliance on handcrafted spatial encodings. Construct tasks like segment-level classification (freight-critical vs. non-critical proxies), missing-attribute imputation, or corridor anomaly detection, and test whether forgetting attention improves generalization across states/regions.\n\n5. **Trajectory-to-Corridor Mapping with UT Recurrence and Forget Gates**\n   Build a model that maps noisy vehicle/freight trajectories to most-likely NHFN corridors using a UT-style iterative refinement: each depth step updates the path alignment while a forget gate suppresses inconsistent historical matches. Evaluate against HMM/CRF and transformer baselines on synthetic noise regimes and (if available) real GPS traces, focusing on robustness to long trips and detours.\n\n6. **Needle-in-the-Haystack for Infrastructure: Long-Context Event Retrieval on Freight Networks**\n   Create a benchmark where the \u201cneedle\u201d is a rare but important event (e.g., a critical link closure indicator embedded in long multi-source logs: weather, incidents, restrictions) tied to NHFN segments. Compare FoX, standard Transformers, and linear-time models (FLASH) on retrieving the relevant event and predicting downstream impacts (e.g., affected corridors), emphasizing length extrapolation and retrieval accuracy.\n\n7. **State-Space vs. Forget-Gated Attention: Controlled Study of Long-Range Dependencies**\n   Design synthetic and semi-synthetic tasks that isolate specific dependency types relevant to routing and language (e.g., periodic constraints, delayed triggers, hierarchical composition) and evaluate FoX/Transformer/FLASH/MoEUT under matched compute. Provide a taxonomy of which dependency structures benefit from explicit forgetting in attention versus recurrence-in-depth or linear approximations, with ablations on gate placement and granularity.\n\n8. **Kernel-Level Co-Design: FlashAttention-Compatible Forget Gates with Quantization**\n   Develop a fused kernel that combines FlashAttention-style tiling with forget-gate modulation and low-precision quantization of attention logits/activations, targeting minimal overhead versus baseline FlashAttention. Report end-to-end training speedups and quality impacts on long-context LM and one non-text domain (e.g., NHFN-based sequences), including sensitivity to quantization scale under dynamic forgetting.\n\n9. **Expert-Conditional Chunking: MoE-Driven Dynamic Local/Global Attention Allocation**\n   Extend FLASH\u2019s mixed chunk attention by letting an MoE router choose chunk sizes and the proportion of local quadratic vs. global linear attention per token or per layer. Train with a budget constraint (fixed FLOPs) and evaluate whether dynamic chunking improves both short-context tasks (where local detail matters) and long-context retrieval (where global memory matters), compared to static chunking.\n\n10. **Interpretable Forgetting Maps for Auditing Long-Context Models**\n   Create tools that convert learned forget-gate values (FoX or forget-augmented UT/MoEUT) into \u201cforgetting maps\u201d over tokens/segments, analogous to attention maps but explicitly showing what the model suppresses over time/depth. Validate interpretability by correlating gate dynamics with known distractors (in text) or irrelevant road segments (in NHFN tasks), and test whether gate-based explanations better predict failure cases than raw attention visualization.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "FoX-Flash: Forget-Gated Mixed-Chunk Linear Attention\nIntegrate the Forgetting Transformer\u2019s data-dependent forget gate into FLASH\u2019s mixed chunk attention by applying the forget modulation to both (i) ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Adaptive Forget Schedules for Universal Transformers (UT)\nAdd a per-step (depth-iteration) forget gate to UT recurrence so that each recurrent refinement can selectively down-weight earlier token inte",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "MoE-Routed Forget Gates in MoEUT for Compute-Aware Context Management\nExtend MoEUT by routing not only feedforward/attention experts but also specialized \u201cforget-gate experts\u201d that learn distinct forg",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Positional-Embedding-Free GIS Sequence Modeling on NHFN with Forgetting Attention\nTreat NHFN road segments as sequences (e.g., paths, corridors, or learned traversals) and model freight-relevant attri",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Trajectory-to-Corridor Mapping with UT Recurrence and Forget Gates\nBuild a model that maps noisy vehicle/freight trajectories to most-likely NHFN corridors using a UT-style iterative refinement: each ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Needle-in-the-Haystack for Infrastructure: Long-Context Event Retrieval on Freight Networks\nCreate a benchmark where the \u201cneedle\u201d is a rare but important event (e.g., a critical link closure indicator",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "State-Space vs. Forget-Gated Attention: Controlled Study of Long-Range Dependencies\nDesign synthetic and semi-synthetic tasks that isolate specific dependency types relevant to routing and language (e",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Kernel-Level Co-Design: FlashAttention-Compatible Forget Gates with Quantization\nDevelop a fused kernel that combines FlashAttention-style tiling with forget-gate modulation and low-precision quantiza",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Expert-Conditional Chunking: MoE-Driven Dynamic Local/Global Attention Allocation\nExtend FLASH\u2019s mixed chunk attention by letting an MoE router choose chunk sizes and the proportion of local quadratic",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Interpretable Forgetting Maps for Auditing Long-Context Models\nCreate tools that convert learned forget-gate values (FoX or forget-augmented UT/MoEUT) into \u201cforgetting maps\u201d over tokens/segments, anal",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 51,
      "paper_title": "Learning long range dependencies through time reversal symmetry breaking",
      "contribution": "Introducing Recurrent Hamiltonian Echo Learning (RHEL) for training state space models using Hamiltonian dynamics that efficiently compute gradients without backward pass or Jacobian calculations.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 9720,
      "output_tokens": 995,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] A Learning Algorithm for Continually Running Fully Recurrent ...",
          "url": "https://gwern.net/doc/ai/nn/rnn/1989-williams-2.pdf",
          "content": "A Learning Algorithm for Continually Running Fully\nRecurrent Neural Networks\nRonald J. Williams\nCollege of Computer Science\nNortheastern University\nBoston, Massachusetts 02115\nand\nDavid Zipser\nInstitute for Cognitive Science\nUniversity of California, San Diego\nLa Jolla, California 92093\nAppears in Neural Computation, 1, pp. 270-280, 1989.\nAbstract\nThe exact form of a gradient-following learning algorithm for completely recurrent net\u0002works running in continually sampled time is derived and used as the basis for practical\nalgorithms for temporal supervised learning tasks. These algorithms have: (1) the advantage\nthat they do not require a precisely de\fned training interval, operating while the network\nruns; and (2) the disadvantage that they require nonlocal communication in the network be\u0002ing trained and are computationally expensive. These algorithms are shown to allow networks\nhaving recurrent connections to learn complex tasks requiring the retention of information\nover time periods having either \fxed or inde\fnite length.\n1 Introduction\nA major problem in connectionist theory is to develop learning algorithms that can tap the full\ncomputational power of neural networks. Much progress has been made with feedforward net\u0002works, and attention has recently turned to developing algorithms for networks with recurrent\nconnections, which have important capabilities not found in feedforward networks, including at\u0002tractor dynamics and the ability to store information for later use. Of particular interest is their\nability to deal with time-varying input or output through their own natural temporal operation.\nA variety of approaches to learning in networks with recurrent connections have been proposed.\nAlgorithms for the special case of networks that settle to stable states, often regarded as associative\n1\nmemory networks, have been proposed by Hop\feld (1982), Lapedes and Farber (1986), Almeida\n(1987), Pineda (1988), and Rohwer and Forrest (1987).\nOther researchers have focused on learning algorithms for more general networks that use\nrecurrent connections to deal with time-varying input and/or output in nontrivial ways. A general\nframework for such problems was laid out by Rumelhart, Hinton, and Williams (1986), who\nunfolded the recurrent network into a multilayer feedforward network that grows by one layer\non each time step. We will call this approach backpropagation through time. One of its primary\nstrengths is its generality, but a corresponding weakness is its growing memory requirement when\ngiven an arbitrarily long training sequence.\nOther approaches to training recurrent nets to handle time-varying input or output have been\nsuggested or investigated by Jordan (1986), Bachrach (1988), Mozer (1988), Elman (1988), Servan\u0002Schreiber, Cleeremans, and McClelland (1988), Robinson and Fallside (1987), Stornetta, Hogg, and\nHuberman (1987), Gallant and King (1988), and Pearlmutter (1988). Many of these approaches\nuse restricted architectures or are based on more computationally limited approximations to the\nfull backpropagation-through-time computation.\nThe approach we propose here enjoys the generality of the backpropagation-through-time ap\u0002proach while not su\u000bering from its growing memory requirement in arbitrarily long training se\u0002quences. It coincides with an approach suggested in the system identi\fcation literature (McBride\n& Narendra, 1965) for tuning the parameters of general dynamical systems. The work of Bachrach\n(1988) and Mozer (1988) represents special cases of the algorithm presented here, and Robinson\nand Fallside (1987) have given an alternative description of the full algorithm as well. However,\nto the best of our knowledge, none of these investigators has published an account of the behavior\nof this algorithm in unrestricted architectures.\n2 The Learning Algorithm and Variations\n2.1 The Basic Algorithm\nLet the network have n units, with m external input lines. Let y(t) denote the n-tuple of outputs\nof the units in the network at time t, and let x(t) denote the m-tuple of external input signals\nto the network at time t. We concatenate y(t) and x(t) to form the (m + n)-tuple z(t), with U\ndenoting the set of indices k such that zk is the output of a unit in the network and I the set of\nindices k for which zk is an external input. The indices on y and x are chosen to correspond to\nthose of z, so that\nzk(t) =\n(\nxk(t) if k 2 I\nyk(t) if k 2 U.\n(1)\nLet W denote the weight matrix for the network, with a unique weight between every pair\nof units and also from each input line to each unit. By adopting the indexing convention just\ndescribed, we can incorporate all the weights into this single ntimes(m+n) matrix. To allow each\nunit to have a bias weight we simply include among the m input lines one input whose value is\nalways 1.\nIn what follows we use a discrete time formulation and we assume that the network consists\nentirely of semilinear units; it is straightforward to extend the approach to continuous time and\n2\nother forms of di\u000berentiable unit computation. We let\nsk(t) = X\nl2U[I\nwklzl(t) (2)\ndenote the net input to the kth unit at time t, for k 2 U, with its output at the next time step\nbeing\nyk(t + 1) = fk(sk(t)); (3)\nwhere fk is the unit's squashing function.\nThus the system of equations 2 and 3, where k ranges over U, constitute the entire dynamics\nof the network, where the zk values are de\fned by equation 1. Note that the external input at\ntime t does not in uence the output of any unit until time t + 1.\nWe now derive an algorithm for training this network in what we will call a temporal supervised\nlearning task, meaning that certain of the units' output values are to match speci\fed target values\nat speci\fed times. Let T (t) denote the set of indices k 2 U for which there exists a speci\fed target\nvalue dk(t) that the output of the kth unit should match at time t. Then de\fne a time-varying\nn-tuple e by\nek(t) =\n(\ndk(t) yk(t) if k 2 T (t)\n0 otherwise, (4)\nNote that this formulation allows for the possibility that target values are speci\fed for di\u000berent\nunits at di\u000berent times. The set of units considered to be \\visible\" can thus be time-varying. Now\nlet\nJ(t) = 1=2 X\nk2U\n[ek(t)]2(5)\ndenote the overall network error at time t. For the moment, assume that the network is run\nstarting at time t0 up to some \fnal time t1. We take as the objective the minimization of the total\nerror\nJtotal(t0; t1) = X\nt1\nt=t0+1\nJ(t) (6)\nover this trajectory. We do this by a gradient descent procedure, adjusting W along the negative\nof rWJtotal(t0; t + 1).\nSince the total error is just the sum of the errors at the individual time steps, one way to\ncompute this gradient is by accumulating the values of rWJ(t) for each time step along the\ntrajectory. The overall weight change for any particular weight wij in the network can thus be\nwritten as\n\u0001wij = X\nt1\nt=t0+1\n\u0001wij (t); (7)\nwhere\n\u0001wij (t) = \u000b\n@J(t)\n@wij\n(8)\nand \u000b is some \fxed positive learning rate.\nNow\n\n@J(t)\n@wij\n= X\nk2U\nek(t)\n@yk(t)\n@wij\n; (9)\n3\nwhere @yk(t)=@wij is easily computed by di\u000berentiating the network dynamics (equations 2 and\n3), yielding\n@yk(t + 1)\n@wij\n= fk\n0\n(sk(t))\n2\n4X\nl2U\nwkl\n@yl(t)\n@wij\n+ \u000eikzj (t)\n3\n5 ; (10)\nwhere \u000eik denotes the Kronecker delta. Because we assume that the initial state of the network\nhas no functional dependence on the weights, we also have\n@yk (t0)\n@wij\n= 0: (11)\nThese equations hold for all k 2 U, i 2 U, and j 2 U [ I .\nWe thus create a dynamical system with variables pk\nij for all k 2 U, i 2 U, and j 2 U [ I , and\ndynamics given by\np\nk\nij (t + 1) = fk\n0\n(sk(t))\n2\n4X\nl2U\nwklp\nl\nij (t) + \u000eikzj (t)\n3\n5 ; (12)\nwith initial conditions\np\nk\nij (t0) = 0; (13)\nand it follows that\np\nk\nij (t) =\n@yk(t)\n@wij\n(14)\nfor every time step t and all appropriate i, j, and k.\nThe precise algorithm then consists of computing, at each time step t from t0 to t1, the\nquantities pk\nij (t), using equations 12 and 13, and then using the discrepancies ek(t) between the\ndesired and actual outputs to compute the weight changes\n\u0001wij (t) = \u000b X\nk2U\nek(t)p\nk\nij (t): (15)\nThe overall correction to be applied to each weight wij in the net is then simply the sum of these\nindividual \u0001wij (t) values for each time step t along the trajectory.\nIn the case when each unit in the network uses the logistic squashing function we use\nfk\n0\n(sk(t)) = yk(t + 1)[1 yk(t + 1)] (16)\nin equation 12.\n2.2 Real-Time Recurrent Learning\nThe above algorithm was derived on the assumption that the weights remained \fxed throughout\nthe trajectory. In order to allow real-time training of behaviors of inde\fnite duration, however,\nit is useful to relax this assumption and actually make the weight changes while the network\nis running. This has the important advantage that no epoch boundaries need to be de\fned for\ntraining the network, leading to both a conceptual and an implementational simpli\fcation of the\nprocedure. For this algorithm, we simply increment each weight wij by the amount \u0001wij (t) given\n4\nby equation 15 at time step t, without accumulating the values elsewhere and making the weight\nchanges at some later time.\nA potential disadvantage of this real-time procedure is that it no longer follows the precise\nnegative gradient of the total error along a trajectory. However, this is exactly analogous to\nthe commonly used method of training a feedforward net by making weight changes after each\npattern presentation rather than accumulating them elsewhere and then making the net change\nafter the end of each complete cycle of pattern presentation. While the resulting algorithm is no\nlonger guaranteed to follow the gradient of total error, the practical di\u000berences are often slight,\nwith the two versions becoming more nearly identical as the learning rate is made smaller. The\nmost severe potential consequence of this departure from true gradient-following behavior for real\u0002time procedure for training the dynamics is that the observed trajectory may itself depend on\nthe variation in the weights caused by the learning",
          "original_query": "A learning algorithm for continually running fully recurrent neural networks",
          "cleaned_query": "A learning algorithm for continually running fully recurrent neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "A cookbook for hardware-friendly implicit learning on static data",
          "url": "https://openreview.net/forum?id=aL5AlzWrf0",
          "content": "A cookbook for hardware-friendly implicit learning on static data | OpenReview\n[![back arrow](https://openreview.net/images/arrow_left.svg)Go to**NeurIPS 2024 Workshop MLNCP**homepage](https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/MLNCP)\n## A cookbook for hardware-friendly implicit learning on static data\n[![Download PDF](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=aL5AlzWrf0)\n### [Maxence Ernoult](https://openreview.net/profile?id=~Maxence_Ernoult1),[Rasmus H\u00f8ier](https://openreview.net/profile?id=~Rasmus_H%C3%B8ier1),[Jack Kendall](https://openreview.net/profile?id=~Jack_Kendall1)\nPublished: 17 Oct 2024, Last Modified: 06 Dec 2024MLNCP PosterEveryone[Revisions](https://openreview.net/revisions?id=aL5AlzWrf0)[BibTeX](#)[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)\n**Keywords:**implicit differentiation, hardware, bilevel optimization, multilevel optimization, zeroth-order optimization, neuromorphic computing, energy-based models, Hopfield models, biologically plausible credit assignment\n**Abstract:**The following aims to be a pragmatic introduction to hardware-friendly learning of implicit models, which encompass a broad class of models from feedforward nets to physical systems, taking static data as inputs. Starting from first principles, we present a minimal hierarchy of independent concepts to circumvent some problems inherent to the hardware implementation of standard differentiation. This way, we avoid entangling essential ingredients with arbitrary design choices by naively listing existing algorithms and instead propose the draft of a \u201ccookbook\u201d to help the exploration of many possible combinations of these independent mechanisms.\n**Submission Number:**54\nLoading\n[OpenReview](https://openreview.net/about)is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the[OpenReview Sponsors](https://openreview.net/sponsors). \u00a92025OpenReview",
          "original_query": "A cookbook for hardware-friendly implicit learning on static data",
          "cleaned_query": "A cookbook for hardware-friendly implicit learning on static data",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Equilibrium Propagation Without Limits",
          "url": "https://arxiv.org/html/2511.22024v1",
          "content": "Equilibrium Propagation Without Limits\n\\\\correspondence\nelonlit@stanford.edu\\\\versionNovember 27, 2025\n# Equilibrium Propagation Without Limits\nElon Litman11Stanford University\n###### Abstract\nWe liberate Equilibrium Propagation (EP) from the limit of infinitesimal perturbations by establishing a finite-nudge foundation for local credit assignment. By modeling network states as Gibbs-Boltzmann distributions rather than deterministic points, we prove that the gradient of the difference in Helmholtz free energy between a nudged and free phase is exactly the difference in expected local energy derivatives. This validates the classic Contrastive Hebbian Learning update as an exact gradient estimator for arbitrary finite nudging, requiring neither infinitesimal approximations nor convexity. Furthermore, we derive a generalized EP algorithm based on the path integral of loss-energy covariances, enabling learning with strong error signals that standard infinitesimal approximations cannot support.\n## 1Introduction\nBackpropagation provides an efficient solution to the credit assignment problem in layered and recurrent networks> [\n[> 40\n](https://arxiv.org/html/2511.22024v1#bib.bib40)> , [> 33\n](https://arxiv.org/html/2511.22024v1#bib.bib33)> ]\nand underpins most contemporary applications of deep neural networks> [\n[> 22\n](https://arxiv.org/html/2511.22024v1#bib.bib22)> , [> 19\n](https://arxiv.org/html/2511.22024v1#bib.bib19)> , [> 14\n](https://arxiv.org/html/2511.22024v1#bib.bib14)> , [> 36\n](https://arxiv.org/html/2511.22024v1#bib.bib36)> ]\n. At the same time, the algorithm relies on a dedicated backward pass that transports errors through the network using the exact transpose of the forward weights. This weight transport requirement and the associated non-locality are widely regarded as biologically implausible and difficult to reconcile with the constraints of real neural circuits> [\n[> 7\n](https://arxiv.org/html/2511.22024v1#bib.bib7)> , [> 13\n](https://arxiv.org/html/2511.22024v1#bib.bib13)> , [> 3\n](https://arxiv.org/html/2511.22024v1#bib.bib3)> , [> 25\n](https://arxiv.org/html/2511.22024v1#bib.bib25)> , [> 42\n](https://arxiv.org/html/2511.22024v1#bib.bib42)> ]\n. Empirical and theoretical work in neuroscience instead points to synaptic plasticity rules that depend on locally available variables such as pre and postsynaptic activity> [\n[> 15\n](https://arxiv.org/html/2511.22024v1#bib.bib15)> ]\n, their timing> [\n[> 12\n](https://arxiv.org/html/2511.22024v1#bib.bib12)> , [> 6\n](https://arxiv.org/html/2511.22024v1#bib.bib6)> ]\n, and possibly a small number of modulatory signals> [\n[> 38\n](https://arxiv.org/html/2511.22024v1#bib.bib38)> ]\n.\nThis tension has motivated a broad search for learning rules that are both powerful and local. Proposals include multivariate Hebbian and three-factor rules, feedback alignment and its variants> [\n[> 24\n](https://arxiv.org/html/2511.22024v1#bib.bib24)> , [> 30\n](https://arxiv.org/html/2511.22024v1#bib.bib30)> ]\n, target propagation> [\n[> 23\n](https://arxiv.org/html/2511.22024v1#bib.bib23)> ]\n, and predictive coding style architectures that attempt to approximate error backpropagation with local computations> [\n[> 41\n](https://arxiv.org/html/2511.22024v1#bib.bib41)> , [> 27\n](https://arxiv.org/html/2511.22024v1#bib.bib27)> , [> 5\n](https://arxiv.org/html/2511.22024v1#bib.bib5)> ]\n. While these approaches relax strict weight symmetry and can achieve reasonable performance, they often lack a clean global objective, or they rely on auxiliary mechanisms whose physical or biological status is unclear> [\n[> 3\n](https://arxiv.org/html/2511.22024v1#bib.bib3)> , [> 25\n](https://arxiv.org/html/2511.22024v1#bib.bib25)> , [> 42\n](https://arxiv.org/html/2511.22024v1#bib.bib42)> ]\n.\nEnergy based models provide a natural framework in which to formulate local learning> [\n[> 17\n](https://arxiv.org/html/2511.22024v1#bib.bib17)> , [> 1\n](https://arxiv.org/html/2511.22024v1#bib.bib1)> , [> 21\n](https://arxiv.org/html/2511.22024v1#bib.bib21)> ]\n. In this setting a network is defined by an energy function over states and parameters, and its dynamics can be viewed as a relaxation process that lowers this energy> [\n[> 16\n](https://arxiv.org/html/2511.22024v1#bib.bib16)> , [> 8\n](https://arxiv.org/html/2511.22024v1#bib.bib8)> ]\n. Contrastive Hebbian Learning (CHL)> [\n[> 28\n](https://arxiv.org/html/2511.22024v1#bib.bib28)> , [> 32\n](https://arxiv.org/html/2511.22024v1#bib.bib32)> , [> 43\n](https://arxiv.org/html/2511.22024v1#bib.bib43)> ]\nand Equilibrium Propagation (EP)> [\n[> 34\n](https://arxiv.org/html/2511.22024v1#bib.bib34)> , [> 35\n](https://arxiv.org/html/2511.22024v1#bib.bib35)> ]\nare two influential schemes that exploit this structure. Both use a free phase, in which the network state is driven only by the input, and a nudged phase, in which a supervisory signal biases the system toward target configurations> [\n[> 10\n](https://arxiv.org/html/2511.22024v1#bib.bib10)> , [> 20\n](https://arxiv.org/html/2511.22024v1#bib.bib20)> , [> 31\n](https://arxiv.org/html/2511.22024v1#bib.bib31)> ]\n. Parameters are updated according to the difference between local statistics measured in the two phases, which yields a local two-phase learning rule.\nDespite their appeal, the theoretical status of these methods is incomplete. In its classical form, CHL is derived for architectures with symmetric weights and a single well defined energy function> [\n[> 28\n](https://arxiv.org/html/2511.22024v1#bib.bib28)> , [> 43\n](https://arxiv.org/html/2511.22024v1#bib.bib43)> ]\n. Under those assumptions, contrastive updates can be identified with gradients of a likelihood or related energy based objective> [\n[> 21\n](https://arxiv.org/html/2511.22024v1#bib.bib21)> ]\n. As soon as the infinitesimal limit is relaxed, which is required for noise tolerance in physical implementations and in biological circuits, the learning rule remains well defined but it is no longer obvious what global quantity it optimizes, if any> [\n[> 43\n](https://arxiv.org/html/2511.22024v1#bib.bib43)> , [> 24\n](https://arxiv.org/html/2511.22024v1#bib.bib24)> , [> 8\n](https://arxiv.org/html/2511.22024v1#bib.bib8)> ]\n. EP addresses the weight transport issue by avoiding an explicit backward pass. It couples the energy to a supervised loss via a nudging parameter and recovers the gradient of the supervised objective in the limit of an infinitesimal perturbation> [\n[> 34\n](https://arxiv.org/html/2511.22024v1#bib.bib34)> , [> 35\n](https://arxiv.org/html/2511.22024v1#bib.bib35)> , [> 10\n](https://arxiv.org/html/2511.22024v1#bib.bib10)> ]\n. In practice, however, finite nudging is required for stable learning, which introduces bias relative to the true gradient. Moreover, most analyses assume deterministic dynamics at zero temperature, in which the network state is identified with a single energy minimum> [\n[> 1\n](https://arxiv.org/html/2511.22024v1#bib.bib1)> ]\n, an idealization that is difficult to justify for complex nonconvex energy landscapes> [\n[> 26\n](https://arxiv.org/html/2511.22024v1#bib.bib26)> , [> 9\n](https://arxiv.org/html/2511.22024v1#bib.bib9)> , [> 29\n](https://arxiv.org/html/2511.22024v1#bib.bib29)> ]\n.\nThis paper develops a statistical mechanics foundation for contrastive learning that resolves these issues. Instead of treating the network as a deterministic energy minimizer, we model its state as a random variable distributed according to a Gibbs\u2013Boltzmann measure at finite temperature, defined by an energy function and a task-dependent loss. Within this framework we introduce the stochastic contrastive objective, defined as the difference in Helmholtz free energy between the nudged and free phases. This objective depends only on the underlying energy based model and the loss and is well defined for arbitrary nonlinear architectures, nonconvex energy landscapes, and finite temperature dynamics> [\n[> 26\n](https://arxiv.org/html/2511.22024v1#bib.bib26)> , [> 9\n](https://arxiv.org/html/2511.22024v1#bib.bib9)> , [> 29\n](https://arxiv.org/html/2511.22024v1#bib.bib29)> ]\n.\nWe prove that the stochastic contrastive objective admits two exact and complementary gradient representations. The first expresses the gradient as the difference between the expected local energy derivatives under the nudged and free Gibbs distributions. This shows that the familiar two-phase contrastive update implements exact gradient descent on a well defined free energy objective, rather than an approximation to some other quantity, and it does so without requiring symmetric weights or an infinitesimal nudging limit. The second representation expresses the same gradient as an integral, over the nudging strength, of the covariance between the loss and the local energy derivatives. This yields a finite nudging generalization of Equilibrium Propagation in which the classical EP rule appears as a first order approximation around the free phase> [\n[> 34\n](https://arxiv.org/html/2511.22024v1#bib.bib34)> , [> 35\n](https://arxiv.org/html/2511.22024v1#bib.bib35)> , [> 10\n](https://arxiv.org/html/2511.22024v1#bib.bib10)> , [> 20\n](https://arxiv.org/html/2511.22024v1#bib.bib20)> ]\n. Finally, using the Gibbs variational principle, we show that the stochastic contrastive objective is a regularized proxy objective of the expected supervised loss, as it decomposes into an accuracy term and an information term that penalizes the Kullback\u2013Leibler divergence between nudged and free distributions. This links our framework to variational inference> [\n[> 18\n](https://arxiv.org/html/2511.22024v1#bib.bib18)> , [> 39\n](https://arxiv.org/html/2511.22024v1#bib.bib39)> , [> 4\n](https://arxiv.org/html/2511.22024v1#bib.bib4)> ]\n, the information bottleneck> [\n[> 37\n](https://arxiv.org/html/2511.22024v1#bib.bib37)> , [> 2\n](https://arxiv.org/html/2511.22024v1#bib.bib2)> ]\n, and free energy based theories of brain function> [\n[> 11\n](https://arxiv.org/html/2511.22024v1#bib.bib11)> , [> 5\n](https://arxiv.org/html/2511.22",
          "original_query": "Equilibrium propagation: Bridging the gap between energy-based models and backpropagation",
          "cleaned_query": "Equilibrium propagation: Bridging the gap between energy-based models and backpropagation",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Self-Learning Machines Based on Hamiltonian Echo Backpropagation",
          "url": "https://ui.adsabs.harvard.edu/abs/2023PhRvX..13c1020L/abstract",
          "content": "Self-Learning Machines Based on Hamiltonian Echo Backpropagation - ADS\nNow on home page\n## ADS\n## Self-Learning Machines Based on Hamiltonian Echo Backpropagation[]()\n* [L\u00f3pez-Pastor, V\u00edctor](https://ui.adsabs.harvard.edu/search/?q=author:%22L%C3%B3pez-Pastor,+V%C3%ADctor%22);\n* [Marquardt, Florian](https://ui.adsabs.harvard.edu/search/?q=author:%22Marquardt,+Florian%22)\n#### Abstract\nA physical self-learning machine can be defined as a nonlinear dynamical system that can be trained on data (similar to artificial neural networks) but where the update of the internal degrees of freedom that serve as learnable parameters happens autonomously. In this way, neither external processing and feedback nor knowledge of (and control of) these internal degrees of freedom is required. We introduce a general scheme for self-learning in any time-reversible Hamiltonian system. It relies on implementing a time-reversal operation and injecting a small error signal on top of the echo dynamics. We show how the physical dynamics itself will then lead to the required gradient update of learnable parameters, independent of the details of the Hamiltonian. We illustrate the training of such a self-learning machine numerically for the case of coupled nonlinear wave fields and other examples.\nPublication:\nPhysical Review X\nPub Date:July 2023DOI:\n[10.1103/PhysRevX.13.031020](https://ui.adsabs.harvard.edu/link_gateway/2023PhRvX..13c1020L/doi:10.1103/PhysRevX.13.031020)**\n[10.48550/arXiv.2103.04992](https://ui.adsabs.harvard.edu/link_gateway/2023PhRvX..13c1020L/doi:10.48550/arXiv.2103.04992)**\narXiv:[arXiv:2103.04992](https://ui.adsabs.harvard.edu/link_gateway/2023PhRvX..13c1020L/arXiv:2103.04992)**Bibcode:[2023PhRvX..13c1020L](https://ui.adsabs.harvard.edu/abs/2023PhRvX..13c1020L/abstract)**Keywords:\n* Computer Science - Machine Learning;\n* Nonlinear Sciences - Adaptation and Self-Organizing Systems;\n* Physics - Data Analysis;\n* Statistics and Probability;\n* Physics - OpticsE-Print:Physical Review X 13, 031020 (2023)\n**\nfull text sources\nAPS\n|\n[**](https://ui.adsabs.harvard.edu/link_gateway/2023PhRvX..13c1020L/PUB_HTML)\nPreprint\n[**](https://ui.adsabs.harvard.edu/link_gateway/2023PhRvX..13c1020L/EPRINT_PDF)\n|\n[**](https://ui.adsabs.harvard.edu/link_gateway/2023PhRvX..13c1020L/EPRINT_HTML)\n",
          "original_query": "Self-learning machines based on hamiltonian echo backpropagation",
          "cleaned_query": "Self-learning machines based on hamiltonian echo backpropagation",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Neural Ordinary Differential Equations",
          "url": "http://papers.neurips.cc/paper/7892-neural-ordinary-differential-equations.pdf",
          "content": "Neural Ordinary Differential Equations\nRicky T. Q. Chen*, Yulia Rubanova*, Jesse Bettencourt*, David Duvenaud\nUniversity of Toronto, Vector Institute\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\nAbstract\nWe introduce a new family of deep neural network models. Instead of specifying a\ndiscrete sequence of hidden layers, we parameterize the derivative of the hidden\nstate using a neural network. The output of the network is computed using a black\u0002box differential equation solver. These continuous-depth models have constant\nmemory cost, adapt their evaluation strategy to each input, and can explicitly trade\nnumerical precision for speed. We demonstrate these properties in continuous-depth\nresidual networks and continuous-time latent variable models. We also construct\ncontinuous normalizing flows, a generative model that can train by maximum\nlikelihood, without partitioning or ordering the data dimensions. For training, we\nshow how to scalably backpropagate through any ODE solver, without access to its\ninternal operations. This allows end-to-end training of ODEs within larger models.\n1 Introduction\nResidual Network ODE Network\n\ufffd \ufffd \ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd \ufffd \ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\nFigure 1: Left: A Residual network defines a\ndiscrete sequence of finite transformations.\nRight: A ODE network defines a vector\nfield, which continuously transforms the state.\nBoth: Circles represent evaluation locations.\nModels such as residual networks, recurrent neural\nnetwork decoders, and normalizing flows build com\u0002plicated transformations by composing a sequence of\ntransformations to a hidden state:\nht+1 = ht + f(ht, \u03b8t) (1)\nwhere t \u2208 {0 . . . T} and ht \u2208 RD. These iterative\nupdates can be seen as an Euler discretization of a\ncontinuous transformation (Lu et al., 2017; Haber\nand Ruthotto, 2017; Ruthotto and Haber, 2018).\nWhat happens as we add more layers and take smaller\nsteps? In the limit, we parameterize the continuous\ndynamics of hidden units using an ordinary differen\u0002tial equation (ODE) specified by a neural network:\ndh(t)\ndt = f(h(t), t, \u03b8) (2)\nStarting from the input layer h(0), we can define the output layer h(T) to be the solution to this\nODE initial value problem at some time T. This value can be computed by a black-box differential\nequation solver, which evaluates the hidden unit dynamics f wherever necessary to determine the\nsolution with the desired accuracy. Figure 1 contrasts these two approaches.\nDefining and evaluating models using ODE solvers has several benefits:\nMemory efficiency In Section 2, we show how to compute gradients of a scalar-valued loss with\nrespect to all inputs of any ODE solver, without backpropagating through the operations of the solver.\nNot storing any intermediate quantities of the forward pass allows us to train our models with constant\nmemory cost as a function of depth, a major bottleneck of training deep models.\n32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr\u00e9al, Canada.\nAdaptive computation Euler\u2019s method is perhaps the simplest method for solving ODEs. There\nhave since been more than 120 years of development of efficient and accurate ODE solvers (Runge,\n1895; Kutta, 1901; Hairer et al., 1987). Modern ODE solvers provide guarantees about the growth\nof approximation error, monitor the level of error, and adapt their evaluation strategy on the fly to\nachieve the requested level of accuracy. This allows the cost of evaluating a model to scale with\nproblem complexity. After training, accuracy can be reduced for real-time or low-power applications.\nParameter efficiency When the hidden unit dynamics are parameterized as a continuous function\nof time, the parameters of nearby \u201clayers\u201d are automatically tied together. In Section 3, we show that\nthis reduces the number of parameters required on a supervised learning task.\nScalable and invertible normalizing flows An unexpected side-benefit of continuous transforma\u0002tions is that the change of variables formula becomes easier to compute. In Section 4, we derive\nthis result and use it to construct a new class of invertible density models that avoids the single-unit\nbottleneck of normalizing flows, and can be trained directly by maximum likelihood.\nContinuous time-series models Unlike recurrent neural networks, which require discretizing\nobservation and emission intervals, continuously-defined dynamics can naturally incorporate data\nwhich arrives at arbitrary times. In Section 5, we construct and demonstrate such a model.\n2 Reverse-mode automatic differentiation of ODE solutions\nThe main technical difficulty in training continuous-depth networks is performing reverse-mode\ndifferentiation (also known as backpropagation) through the ODE solver. Differentiating through\nthe operations of the forward pass is straightforward, but incurs a high memory cost and introduces\nadditional numerical error.\nWe treat the ODE solver as a black box, and compute gradients using the adjoint sensitivity\nmethod (Pontryagin et al., 1962). This approach computes gradients by solving a second, aug\u0002mented ODE backwards in time, and is applicable to all ODE solvers. This approach scales linearly\nwith problem size, has low memory cost, and explicitly controls numerical error.\nConsider optimizing a scalar-valued loss function L(), whose input is the result of an ODE solver:\nL(z(t1)) = L\n\ufffd\nz(t0) + \ufffd t1\nt0\nf(z(t), t, \u03b8)dt\ufffd= L(ODESolve(z(t0), f, t0, t1, \u03b8)) (3)\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\nFigure 2: Reverse-mode differentiation of an ODE\nsolution. The adjoint sensitivity method solves\nan augmented ODE backwards in time. The aug\u0002mented system contains both the original state and\nthe sensitivity of the loss with respect to the state.\nIf the loss depends directly on the state at multi\u0002ple observation times, the adjoint state must be\nupdated in the direction of the partial derivative of\nthe loss with respect to each observation.\nTo optimize L, we require gradients with respect\nto \u03b8. The first step is to determining how the\ngradient of the loss depends on the hidden state\nz(t) at each instant. This quantity is called the\nadjoint a(t) = \u2202L/\u2202z(t). Its dynamics are given\nby another ODE, which can be thought of as the\ninstantaneous analog of the chain rule:\nda(t)\ndt = \u2212a(t)\nT \u2202f(z(t), t, \u03b8)\n\u2202z (4)\nWe can compute \u2202L/\u2202z(t0) by another call to an\nODE solver. This solver must run backwards,\nstarting from the initial value of \u2202L/\u2202z(t1). One\ncomplication is that solving this ODE requires\nthe knowing value of z(t) along its entire tra\u0002jectory. However, we can simply recompute\nz(t) backwards in time together with the adjoint,\nstarting from its final value z(t1).\nComputing the gradients with respect to the pa\u0002rameters \u03b8 requires evaluating a third integral,\nwhich depends on both z(t) and a(t):\ndL\nd\u03b8 =\n\ufffd t0\nt1\na(t)\nT \u2202f(z(t), t, \u03b8)\n\u2202\u03b8 dt (5)\n2\nThe vector-Jacobian products a(t)T \u2202f\n\u2202z and a(t)T \u2202f\u2202\u03b8 in (4) and (5) can be efficiently evaluated by\nautomatic differentiation, at a time cost similar to that of evaluating f. All integrals for solving z, a\nand \u2202L\n\u2202\u03b8 can be computed in a single call to an ODE solver, which concatenates the original state, the\nadjoint, and the other partial derivatives into a single vector. Algorithm 1 shows how to construct the\nnecessary dynamics, and call an ODE solver to compute all gradients at once.\nAlgorithm 1 Reverse-mode derivative of an ODE initial value problem\nInput: dynamics parameters \u03b8, start time t0, stop time t1, final state z(t1), loss gradient \u2202L/\u2202z(t1)\ns0 = [z(t1), \u2202L\n\u2202z(t1) , 0|\u03b8|] \ufffd Define initial augmented state\ndef aug_dynamics([z(t), a(t), \u00b7], t, \u03b8): \ufffd Define dynamics on augmented state\nreturn [f(z(t), t, \u03b8), \u2212a(t)\nT \u2202f\n\u2202z , \u2212a(t)\nT \u2202f\n\u2202\u03b8 ] \ufffd Compute vector-Jacobian products\n[z(t0), \u2202L\n\u2202z(t0) , \u2202L\u2202\u03b8 ] = ODESolve(s0, aug_dynamics, t1, t0, \u03b8) \ufffd Solve reverse-time ODE\nreturn \u2202L\n\u2202z(t0) , \u2202L\u2202\u03b8 \ufffd Return gradients\nMost ODE solvers have the option to output the state z(t) at multiple times. When the loss depends\non these intermediate states, the reverse-mode derivative must be broken into a sequence of separate\nsolves, one between each consecutive pair of output times (Figure 2). At each observation, the adjoint\nmust be adjusted in the direction of the corresponding partial derivative \u2202L/\u2202z(ti).\nThe results above extend those of Stapor et al. (2018, section 2.4.2). An extended version of\nAlgorithm 1 including derivatives w.r.t. t0 and t1 can be found in Appendix C. Detailed derivations\nare provided in Appendix B. Appendix D provides Python code which computes all derivatives for\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd by extending the \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd automatic differentiation package. This\ncode also supports all higher-order derivatives. We have since released a PyTorch (Paszke et al.,\n2017) implementation, including GPU-based implementations of several standard ODE solvers at\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd.\n3 Replacing residual networks with ODEs for supervised learning\nIn this section, we experimentally investigate the training of neural ODEs for supervised learning.\nSoftware To solve ODE initial value problems numerically, we use the implicit Adams method\nimplemented in LSODE and VODE and interfaced through the \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd package. Being\nan implicit method, it has better guarantees than explicit methods such as Runge-Kutta but requires\nsolving a nonlinear optimization problem at every step. This setup makes direct backpropagation\nthrough the integrator difficult. We implement the adjoint sensitivity method in Python\u2019s \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\nframework (Maclaurin et al., 2015). For the experiments in this section, we evaluated the hidden\nstate dynamics and their derivatives on the GPU using Tensorflow, which were then called from the\nFortran ODE solvers, which were called from Python \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd code.\nTable 1: Performance on MNIST. \u2020From LeCun\net al. (1998).\nTest Error # Params Memory Time\n1-Layer MLP\u2020 1.60% 0.24 M - -\nResNet 0.41% 0.60 M O(L) O(L)\nRK-Net 0.47% 0.22 M O(L\u02dc) O(L\u02dc)\nODE-Net 0.42% 0.22 M O(1) O(L\u02dc)\nModel Architectu",
          "original_query": "Neural ordinary differential equations",
          "cleaned_query": "Neural ordinary differential equations",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "(PDF) Automatic differentiation in machine learning: A survey",
          "url": "https://www.researchgate.net/publication/272752440_Automatic_differentiation_in_machine_learning_A_survey",
          "content": "(PDF) Automatic differentiation in machine learning: A survey\nArticlePDF Available\n# Automatic differentiation in machine learning: A survey\n* April 2018\n* [Journal of Machine Learning Research](journal/Journal-of-Machine-Learning-Research-1532-4435)18(153):1-43\n* Source\n* [arXiv](https://www.researchgate.net/deref/http://arxiv.org/abs/1502.05767)\nAuthors:\n[](scientific-contributions/Atilim-Gunes-Baydin-59327645)\n[Atilim Gunes Baydin](scientific-contributions/Atilim-Gunes-Baydin-59327645)\n[Atilim Gunes Baydin](scientific-contributions/Atilim-Gunes-Baydin-59327645)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\n[](profile/Barak-Pearlmutter)\n[Barak A. Pearlmutter](profile/Barak-Pearlmutter)\n* [National University of Ireland, Maynooth](https://www.researchgate.net/institution/National-University-of-Ireland-Maynooth)\n[](scientific-contributions/Alexey-Radul-20039207)\n[Alexey Andreyevich Radul](scientific-contributions/Alexey-Radul-20039207)\n[Alexey Andreyevich Radul](scientific-contributions/Alexey-Radul-20039207)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\n[](profile/Jeffrey-Siskind)\n[Jeffrey Siskind](profile/Jeffrey-Siskind)\n* [Purdue University West Lafayette](https://www.researchgate.net/institution/Purdue-University-West-Lafayette)\n[Download full-text PDF](profile/Barak-Pearlmutter/publication/272752440_Automatic_differentiation_in_machine_learning_A_survey/links/555b4c1208ae91e75e7642fa/Automatic-differentiation-in-machine-learning-A-survey.pdf)[Read full-text](publication/272752440_Automatic_differentiation_in_machine_learning_A_survey#read)\n[Download full-text PDF](https://www.researchgate.net/profile/Barak-Pearlmutter/publication/272752440_Automatic_differentiation_in_machine_learning_A_survey/links/555b4c1208ae91e75e7642fa/Automatic-differentiation-in-machine-learning-A-survey.pdf)\n[Read full-text](publication/272752440_Automatic_differentiation_in_machine_learning_A_survey#read)\n[Download citation](https://www.researchgate.net/publication/272752440_Automatic_differentiation_in_machine_learning_A_survey/citation/download)\nCopy linkLink copied\n[\nRead full-text\n](publication/272752440_Automatic_differentiation_in_machine_learning_A_survey#read)[\nDownload citation\n](https://www.researchgate.net/publication/272752440_Automatic_differentiation_in_machine_learning_A_survey/citation/download)\nCopy linkLink copied\n## Abstract and Figures\nDerivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply \u201cautodiff\u201d, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other\u2019s results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names \u201cdynamic computational graphs\u201d and \u201cdifferentiable programming\u201d. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms \u201cautodiff\u201d, \u201cautomatic differentiation\u201d, and \u201csymbolic differentiation\u201d as these are encountered more and more in machine learning settings.\n[\n](https://www.researchgate.net/figure/Computational-graph-of-the-example-f-x-1-x-2-lnx-1-x-1-x-2-sinx-2-See_fig2_272752440)\n[\nComputational graph of the example f (x 1 , x 2 ) = ln(x 1 ) + x 1 x 2 \u2212sin(x 2 ). See Tables 2 and 3 for the definitions of the intermediate variables v \u22121 . . . v 5 .\n\u2026](https://www.researchgate.net/figure/Computational-graph-of-the-example-f-x-1-x-2-lnx-1-x-1-x-2-sinx-2-See_fig2_272752440)\n[\n](https://www.researchgate.net/figure/Survey-of-major-AD-implementations_tbl1_272752440)\n[\nSurvey of major AD implementations.\n\u2026](https://www.researchgate.net/figure/Survey-of-major-AD-implementations_tbl1_272752440)\n[\n](https://www.researchgate.net/figure/Overview-of-backpropagation-a-Training-pattern-is-fed-forward-generating_fig1_272752440)\n[\nOverview of backpropagation. (a) Training pattern is fed forward, generating corresponding output. (b) Error between actual and desired output is computed. (c) The error propagates back, through updates where a ratio of the gradient ( \u2202E \u2202w i\n\u2026](https://www.researchgate.net/figure/Overview-of-backpropagation-a-Training-pattern-is-fed-forward-generating_fig1_272752440)\nFigures - uploaded by[Barak A. Pearlmutter](profile/Barak-Pearlmutter)\nAuthor content\nAll figure content in this area was uploaded by Barak A. Pearlmutter\nContent may be subject to copyright.\n**Discover the world's research**\n* 25+ million members\n* 160+ million publication pages\n* 2.3+ billion citations[Join for free](signup.SignUp.html)\n[](publication/272752440_Automatic_differentiation_in_machine_learning_A_survey#read-preview)\nContent uploaded by[Barak A. Pearlmutter](profile/Barak-Pearlmutter)\nAuthor content\nAll content in this area was uploaded by Barak A. Pearlmutter on May 19, 2015\nContent may be subject to copyright.\n![](https://c5.rgstatic.net//images/research/empty-state/empty-full-text-request.svg)\nA preview of the PDF is not available\n... Today, optimization can also make use of automaticdifferentiation libraries that provide an efficient calculation of gradients for a wide range of functions. While this development has been primarily driven by the machinelearning community and the demand for efficient backpropagation algorithms, the applications of automatic differentiation are not limited to the field [63][64][65]. Several machine-learning frameworks exist that support automatic differentiation and highly parallelized execution on graphics-processing units (GPUs) such as Tensorflow [66], Pytorch [67], and JAX [68]. ...\n... Borrowing concepts from graph theory, the application of constraints can be represented using a tree-like data structure, similar to computational graphs in automatic differentiation and machine learning[64]. Value propagation in this \"parameter tree\" helps to reduce the number of function evaluations needed to calculate the R factor and its gradient. ...\n[Structural Optimization in Tensor LEED Using a Parameter Tree andR-Factor Gradients](publication/398560307_Structural_Optimization_in_Tensor_LEED_Using_a_Parameter_Tree_and_R-Factor_Gradients)\nPreprint\nFull-text available\n* Dec 2025\nQuantitative low-energy electron diffraction [LEEDI(V)] is a powerful method for surface-structure determination, based on a direct comparison of experimentally observedI(V)data with computations for a structure model. As the diffraction intensitiesIare highly sensitive to subtle structural changes, local structure optimization is essential for assessing the validity of a structure model and finding the best-fit structure. The calculation of diffraction intensities is well established, but the large number of evaluations required for reliable structural optimization renders it computationally demanding. The computational effort is mitigated by the tensor-LEED approximation, which accelerates optimization by applying a perturbative treatment of small deviations from a reference structure. Nevertheless, optimization of complex structures is a tedious process. Here, the problem of surface-structure optimization is reformulated using a tree-based data structure, which helps to avoid redundant function evaluations. In the new tensor-LEED implementation presented in this work, intensities are computed on the fly, eliminating limitations of previous algorithms that are limited to precomputed values at a grid of search parameters. It also enables the use of state-of-the-art optimization algorithms. Implemented in \\\\textsc{Python} with the JAX library, the method provides access to gradients of theRfactor and supports execution on graphics processing units (GPUs). Based on these developments, the computing time can be reduced by more than an order of magnitude.\n[View](publication/398560307_Structural_Optimization_in_Tensor_LEED_Using_a_Parameter_Tree_and_R-Factor_Gradients)\nShow abstract\n... Deep equilibrium (DEQ) networks Starting from (30) and consider again a learned reconstruction operator in the limit R \u03b8(g) = f \u221e, and assuming that the iterates converge to a fixed point (31). The difference for DEQ networks is that in fact an unrolled algorithm is trained, which means here that the forward pass of the reconstruction operator(29)is evaluated in the loss function. Clearly, while we cannot evaluate the limit we can make use of the fixed point formulation to compute the gradients for training as we will outline next shortly. ...\n... One therefore uses automatic differentiation (by reverse mode) for this purpose, see Appendix B and [32] for more details. Here we simply note that usage of automatic differentiation has been greatly simplified[29]thanks to well-maintained and highly optimised libraries that form the backbone of contemporary deep learning, like TensorFlow [3] or PyTorch [119]. These allow for computing gradients of standard neural network components, but they very rarely implement a simulator, e.g., the implementation of a forward operator in inverse problems. ...\n[Learned iterative networks: An operator learning perspective](publication/398512807_Learned_iterative_networks_An_operator_learning_perspective)\nPreprint\n* Dec 2025\n* [Andreas Hauptmann](https://www.researchgate.net/profile/Andreas-Hauptmann)\n* [\u00d6ktem Ozan](https://www.researchgate",
          "original_query": "Automatic differentiation in machine learning: a survey",
          "cleaned_query": "Automatic differentiation in machine learning: a survey",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Finite\u2011Nudge Equilibrium Propagation for Continually Running Recurrent Networks\nExtend Williams & Zipser\u2019s \u201ccontinually running\u201d RNN training to an energy-based formulation trained with Litman\u2019s finite\u2011nudge EP, so the network can be supervised at arbitrary times without defining training windows. Implement a two-phase (free/nudged) online rule for recurrent dynamics and benchmark against truncated BPTT on long-horizon tasks (copy, adding, event-driven prediction).",
        "Hamiltonian Echo Backpropagation for Neural ODE Vector Fields\nRecast Neural ODE training as a physical/time-reversible learning problem by mapping the ODE solver\u2019s forward/backward integration to Hamiltonian echo dynamics. Build a prototype where the vector field parameters are updated by injecting terminal loss as a small error signal at time-reversal, and compare gradient fidelity and stability versus adjoint sensitivity on stiff and chaotic dynamics.",
        "Path-Integral EP for Strong Error Signals in Energy-Based Continuous-Time Models\nUse the \u201cgeneralized EP via path integrals of loss\u2013energy covariances\u201d to train continuous-time energy-based models (ODE/SDE dynamics) without relying on infinitesimal nudges. Implement a discretized covariance-integral estimator that remains accurate under large target clamps (e.g., hard constraints), and evaluate on inverse problems and constrained control where standard EP breaks down.",
        "Hardware-Friendly Implicit Differentiation for Online Dynamical Systems Identification\nCombine the \u201cimplicit learning cookbook\u201d mechanisms (bilevel/implicit differentiation, zeroth-order options) with continually running recurrent nets for system identification where data arrive as streams. Develop an algorithm that updates parameters using locally measurable steady-state/limit-cycle statistics plus implicit gradients, and test on real-time identification of nonlinear oscillators and sensory-motor loops.",
        "Local Credit Assignment via Finite-Nudge CHL in Asymmetric (Non-Transposed) Recurrent Nets\nBuilding on finite\u2011nudge EP\u2019s exact gradient foundation, design a contrastive two-phase rule that tolerates mild weight asymmetry (no strict weight transport) by introducing auxiliary \u201csymmetry-tracking\u201d dynamics or penalties. Quantify how much asymmetry can be tolerated before learning diverges, and validate on recurrent benchmarks where feedback alignment is typically used.",
        "Adjoint-Free Training of Neural ODEs Using Contrastive Free-Energy Differences\nReplace Neural ODE adjoint sensitivity with a contrastive estimator: run two trajectories (free and nudged terminal condition or loss-augmented energy) and compute parameter updates from differences in local energy derivatives, akin to CHL/EP. Provide an end-to-end recipe that avoids storing solver internals and reduces sensitivity to solver discontinuities, then compare compute/memory/accuracy trade-offs.",
        "Event-Driven Supervision in Continuous-Time RNNs with Sparse Target Times\nExtend Williams & Zipser\u2019s temporal supervised setting (targets at selected times) to continuous-time Neural ODE-RNN hybrids where supervision arrives as irregular events. Derive an online update rule that injects finite nudges only at event times and propagates credit through ongoing dynamics locally (EP-style), evaluating on irregular medical time-series and asynchronous sensor fusion.",
        "Differentiable Programming Framework for Hybrid Physical Learners (Echo + EP + Implicit)\nCreate a unified AD/implicit-diff interface (per the AD survey) that treats a physical dynamical system as a differentiable program but allows swapping gradient mechanisms: adjoint ODE, finite\u2011nudge EP, Hamiltonian echo, or implicit differentiation. Deliver a reference implementation and ablation suite showing when each mechanism is most stable/efficient under hardware constraints (noise, quantization, limited readout).",
        "Noise-Robust Learning Rules for Physical and Neuromorphic Implementations of EP\nUse the finite\u2011nudge EP theory (Gibbs-Boltzmann state distributions) to design learning rules explicitly calibrated for measurement noise and thermal fluctuations in hardware. Implement estimators that leverage repeated short nudged/free samplings and variance reduction, and validate on analog/neuromorphic prototypes or high-fidelity simulators with realistic device noise.",
        "Energy-Based Regularization for Long-Horizon Memory in Continual Recurrent Learning\nAugment the continually running recurrent learning algorithm with an energy/free-energy regularizer that shapes attractor structure (e.g., encourages metastable states) to improve indefinite-length retention. Train on tasks requiring persistent memory and controlled forgetting, and analyze how free-energy landscape metrics (barrier heights, basin volumes) correlate with long-horizon generalization and stability."
      ],
      "generated_ideas_raw": "1. **Finite\u2011Nudge Equilibrium Propagation for Continually Running Recurrent Networks**\n   Extend Williams & Zipser\u2019s \u201ccontinually running\u201d RNN training to an energy-based formulation trained with Litman\u2019s finite\u2011nudge EP, so the network can be supervised at arbitrary times without defining training windows. Implement a two-phase (free/nudged) online rule for recurrent dynamics and benchmark against truncated BPTT on long-horizon tasks (copy, adding, event-driven prediction).\n\n2. **Hamiltonian Echo Backpropagation for Neural ODE Vector Fields**\n   Recast Neural ODE training as a physical/time-reversible learning problem by mapping the ODE solver\u2019s forward/backward integration to Hamiltonian echo dynamics. Build a prototype where the vector field parameters are updated by injecting terminal loss as a small error signal at time-reversal, and compare gradient fidelity and stability versus adjoint sensitivity on stiff and chaotic dynamics.\n\n3. **Path-Integral EP for Strong Error Signals in Energy-Based Continuous-Time Models**\n   Use the \u201cgeneralized EP via path integrals of loss\u2013energy covariances\u201d to train continuous-time energy-based models (ODE/SDE dynamics) without relying on infinitesimal nudges. Implement a discretized covariance-integral estimator that remains accurate under large target clamps (e.g., hard constraints), and evaluate on inverse problems and constrained control where standard EP breaks down.\n\n4. **Hardware-Friendly Implicit Differentiation for Online Dynamical Systems Identification**\n   Combine the \u201cimplicit learning cookbook\u201d mechanisms (bilevel/implicit differentiation, zeroth-order options) with continually running recurrent nets for system identification where data arrive as streams. Develop an algorithm that updates parameters using locally measurable steady-state/limit-cycle statistics plus implicit gradients, and test on real-time identification of nonlinear oscillators and sensory-motor loops.\n\n5. **Local Credit Assignment via Finite-Nudge CHL in Asymmetric (Non-Transposed) Recurrent Nets**\n   Building on finite\u2011nudge EP\u2019s exact gradient foundation, design a contrastive two-phase rule that tolerates mild weight asymmetry (no strict weight transport) by introducing auxiliary \u201csymmetry-tracking\u201d dynamics or penalties. Quantify how much asymmetry can be tolerated before learning diverges, and validate on recurrent benchmarks where feedback alignment is typically used.\n\n6. **Adjoint-Free Training of Neural ODEs Using Contrastive Free-Energy Differences**\n   Replace Neural ODE adjoint sensitivity with a contrastive estimator: run two trajectories (free and nudged terminal condition or loss-augmented energy) and compute parameter updates from differences in local energy derivatives, akin to CHL/EP. Provide an end-to-end recipe that avoids storing solver internals and reduces sensitivity to solver discontinuities, then compare compute/memory/accuracy trade-offs.\n\n7. **Event-Driven Supervision in Continuous-Time RNNs with Sparse Target Times**\n   Extend Williams & Zipser\u2019s temporal supervised setting (targets at selected times) to continuous-time Neural ODE-RNN hybrids where supervision arrives as irregular events. Derive an online update rule that injects finite nudges only at event times and propagates credit through ongoing dynamics locally (EP-style), evaluating on irregular medical time-series and asynchronous sensor fusion.\n\n8. **Differentiable Programming Framework for Hybrid Physical Learners (Echo + EP + Implicit)**\n   Create a unified AD/implicit-diff interface (per the AD survey) that treats a physical dynamical system as a differentiable program but allows swapping gradient mechanisms: adjoint ODE, finite\u2011nudge EP, Hamiltonian echo, or implicit differentiation. Deliver a reference implementation and ablation suite showing when each mechanism is most stable/efficient under hardware constraints (noise, quantization, limited readout).\n\n9. **Noise-Robust Learning Rules for Physical and Neuromorphic Implementations of EP**\n   Use the finite\u2011nudge EP theory (Gibbs-Boltzmann state distributions) to design learning rules explicitly calibrated for measurement noise and thermal fluctuations in hardware. Implement estimators that leverage repeated short nudged/free samplings and variance reduction, and validate on analog/neuromorphic prototypes or high-fidelity simulators with realistic device noise.\n\n10. **Energy-Based Regularization for Long-Horizon Memory in Continual Recurrent Learning**\n   Augment the continually running recurrent learning algorithm with an energy/free-energy regularizer that shapes attractor structure (e.g., encourages metastable states) to improve indefinite-length retention. Train on tasks requiring persistent memory and controlled forgetting, and analyze how free-energy landscape metrics (barrier heights, basin volumes) correlate with long-horizon generalization and stability.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Finite\u2011Nudge Equilibrium Propagation for Continually Running Recurrent Networks\nExtend Williams & Zipser\u2019s \u201ccontinually running\u201d RNN training to an energy-based formulation trained with Litman\u2019s finit",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hamiltonian Echo Backpropagation for Neural ODE Vector Fields\nRecast Neural ODE training as a physical/time-reversible learning problem by mapping the ODE solver\u2019s forward/backward integration to Hami",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Path-Integral EP for Strong Error Signals in Energy-Based Continuous-Time Models\nUse the \u201cgeneralized EP via path integrals of loss\u2013energy covariances\u201d to train continuous-time energy-based models (OD",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Hardware-Friendly Implicit Differentiation for Online Dynamical Systems Identification\nCombine the \u201cimplicit learning cookbook\u201d mechanisms (bilevel/implicit differentiation, zeroth-order options) with",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Local Credit Assignment via Finite-Nudge CHL in Asymmetric (Non-Transposed) Recurrent Nets\nBuilding on finite\u2011nudge EP\u2019s exact gradient foundation, design a contrastive two-phase rule that tolerates m",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Adjoint-Free Training of Neural ODEs Using Contrastive Free-Energy Differences\nReplace Neural ODE adjoint sensitivity with a contrastive estimator: run two trajectories (free and nudged terminal condi",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Event-Driven Supervision in Continuous-Time RNNs with Sparse Target Times\nExtend Williams & Zipser\u2019s temporal supervised setting (targets at selected times) to continuous-time Neural ODE-RNN hybrids w",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Differentiable Programming Framework for Hybrid Physical Learners (Echo + EP + Implicit)\nCreate a unified AD/implicit-diff interface (per the AD survey) that treats a physical dynamical system as a di",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Noise-Robust Learning Rules for Physical and Neuromorphic Implementations of EP\nUse the finite\u2011nudge EP theory (Gibbs-Boltzmann state distributions) to design learning rules explicitly calibrated for ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Energy-Based Regularization for Long-Horizon Memory in Continual Recurrent Learning\nAugment the continually running recurrent learning algorithm with an energy/free-energy regularizer that shapes attr",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 52,
      "paper_title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution",
      "contribution": "FuXi-Ocean is the first data-driven global ocean forecasting model achieving six-hourly predictions at eddy-resolving 1/12\u00b0 spatial resolution.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11354,
      "output_tokens": 1023,
      "predecessor_details": [
        {
          "success": true,
          "title": "Data Assimilation",
          "url": "https://www.hycom.org/data-assimilation",
          "content": "| |\n| --- |\n| - [Home](https://www.hycom.org/) - [Support Us](https://give.fsu.edu/coaps) - [Need Help?](javascript:void(0);) - [Ask a Question on the Forum](http://groups.google.com/a/hycom.org/group/forum) - [Trouble using THREDDS?](https://www.unidata.ucar.edu/software/tds/) - [Trouble using OPeNDAP?](http://docs.opendap.org/index.php/QuickStart) - [Trouble using NCSS?](https://docs.unidata.ucar.edu/tds/current/userguide/netcdf_subset_service_ref.html) - [Media](https://www.hycom.org/youtube) - [Data Server](https://www.hycom.org/dataserver) - [THREDDS](http://tds.hycom.org/thredds) - [HTTP / HTTPS](https://data.hycom.org/datasets) - [OPeNDAP](http://tds.hycom.org/thredds/catalog/datasets/catalog.html) - [FTP](ftp://ftp.hycom.org/datasets) - [Tools](https://www.hycom.org/tools) - [Calendar](https://www.hycom.org/tools/calendar) - [Status](https://www.hycom.org/tools/status) - [Panoply](http://www.giss.nasa.gov/tools/panoply/) - [Uptime](https://stats.uptimerobot.com/JQn3qt8AXL) - [Login / Logout](https://www.hycom.org/login) |\n\n- [About](https://www.hycom.org/about)\n\n- [HYCOM](https://www.hycom.org/hycom)\n - [Overview](https://www.hycom.org/hycom/overview)\n - [Documentation](https://www.hycom.org/hycom/documentation)\n - [Source Code](https://www.hycom.org/hycom/source-code)\n - [Contact Info](https://www.hycom.org/contact-info)\n- [Forum](http://groups.google.com/a/hycom.org/group/forum/)\n - [README](https://www.hycom.org/forum/readme)\n\n- [YouTube Videos](https://www.hycom.org/youtube)\n\n- [Data Server](https://www.hycom.org/dataserver)\n - [ESPC-D-V02](https://www.hycom.org/dataserver/espc-d-v02)\n - [Global Analysis](https://www.hycom.org/dataserver/espc-d-v02/global-analysis)\n - [GOFS 3.1](https://www.hycom.org/dataserver/gofs-3pt1/analysis)\n - [Global Analysis](https://www.hycom.org/dataserver/gofs-3pt1/analysis)\n - [Global Reanalysis](https://www.hycom.org/dataserver/gofs-3pt1/reanalysis)\n - [Ice Fields](https://www.hycom.org/dataserver/gofs-3pt1/reanalysis/ice)\n - [GOFS 3.0](https://www.hycom.org/dataserver/gofs-3pt0)\n - [Global Analysis](https://www.hycom.org/dataserver/gofs-3pt0/analysis)\n - [Global Reanalysis](https://www.hycom.org/dataserver/gofs-3pt0/reanalysis)\n - Gulf of Mexico\n - [GoM Analysis](https://www.hycom.org/data/gomu0pt04/expt-90pt1m000)\n - [GoM Reanalysis](https://www.hycom.org/dataserver/gom/gom-reanalysis)\n - [NAVGEM Forcing](https://www.hycom.org/dataserver/navgem)\n - [Legacy 1.3/1.4](https://www.hycom.org/dataserver/navgem/legacy-1pt3-1pt4)\n - [Legacy 1.1/1.2](https://www.hycom.org/dataserver/navgem/legacy-1pt1-1pt2)\n - [NOGAPS Forcing](https://www.hycom.org/dataserver/nogaps)\n - [NCEP CFSR](https://www.hycom.org/dataserver/ncep-cfsr)\n - [NCEP CFSv2](https://www.hycom.org/dataserver/ncep-cfsv2)\n - Access Methods\n - [http / https](https://www.hycom.org/dataserver/access-methods/http-https)\n - [ftp / ftps](https://www.hycom.org/dataserver/access-methods/ftp-ftps)\n - [rsync](https://www.hycom.org/dataserver/access-methods/rsync)\n - [THREDDS](https://www.hycom.org/dataserver/access-methods/thredds)\n - [NCSS](https://www.hycom.org/dataserver/access-methods/ncss)\n - [WCS](https://www.hycom.org/dataserver/access-methods/wcs)\n - [WMS](https://www.hycom.org/dataserver/access-methods/wms)\n\n- [Global](https://www.hycom.org/global)\n- [Basin](https://www.hycom.org/basin)\n- [Regional](https://www.hycom.org/regional)\n- [Coupled Simulations](https://www.hycom.org/coupled-simulations)\n- [Process Studies](https://www.hycom.org/process-studies)\n- [Data Assimilation](https://www.hycom.org/data-assimilation)\n- [Reanalysis](https://www.hycom.org/reanalysis)\n- [Miscellanea](https://www.hycom.org/miscellanea)\n\n- [Ocean Prediction](https://www.hycom.org/ocean-prediction)\n\n- [Outreach](https://www.hycom.org/outreach)\n- [Publications](https://www.hycom.org/publications)\n- [Meetings](https://www.hycom.org/meetings)\n- [Workshops](https://www.hycom.org/workshops)\n- [Web Links](https://www.hycom.org/weblinks)\n\n- [Request An Account](https://www.hycom.org/account-request)\n- [Contact Info](https://www.hycom.org/contact-info)\n- [Mailing Lists](https://www.hycom.org/mailing-lists)\n- [FAQs](https://www.hycom.org/faqs)\n\n[**Support HYCOM.org**\\\n**Donate Now!**](https://give.fsu.edu/coaps)\n\n| [Login](https://www.hycom.org/login) |\n| Data Assimilation\n\n| A hierarchy of data assimilation techniques are evaluated as a function of computational resources and prediction accuracy: 1. the Optimal Interpolation (OI) 2. the Parameter Matrix Objective Analysis algorithm (PMOA) 3. the Reduced Order Adaptive Filter (ROAF) 4. the Reduced Order Information Filter (ROIF) |\n| | |\n| --- |\n| Contributions |\n\n| | File | Year | Authors | Source | File size |\n| --- | --- | --- | --- | --- |\n| [An update on the comparison of sequential assimilation schemes](https://www.hycom.org/attachments/084_ashwanth_LOM_2009.pdf) | 2009 | Srinivasan, Chassignet, Thacker, Smedstad, Counillion, Bertino, Brankart, Brasseur, Chin, Cummings | RSMAS-CCS / University of Miami (LOM\u00a02009) | 2024 Kb |\n| [Improving the Parameterization of Errors Statistics for Data Assimilation](https://www.hycom.org/attachments/084_BrasseurOSorlandoSEEKHYCOM2.pdf) | 2008 | Brasseur, Broquet, Brankart, Castruccio, Lauvernet, Verron | LEGI, (Ocean\u00a0Sciences\u00a02008) | 3444 Kb |\n| [Demo and Comparison of Sequential Approaches for Altimeter Data Assimilation](https://www.hycom.org/attachments/084_Oceaan_Sciences_2008_Srinivasan.pdf) | 2008 | Srinivasan, Chassignet, Smedstad, Thacker, Bertino, Brasseur, Chin, Counillon, Cummings | FSU/COAPS, (Ocean\u00a0Sciences\u00a02008) | 3259 Kb |\n| [T/S relationships by local regression](https://www.hycom.org/attachments/084_6_Thacker.pdf) | 2007 | Thacker | NOAA/AOML, (10th\u00a0HYCOM) | 2213 Kb |\n| [Status and progress of NCODA assimilation in HYCOM](https://www.hycom.org/attachments/084_3_Cummings.pdf) | 2007 | Cummings | NRL, (11th\u00a0HYCOM) | 1192 Kb |\n| [HYCOM data assimilation at NCEP](https://www.hycom.org/attachments/084_5_Lozano.pdf) | 2007 | Lozano | MMAB/NCEP, (11th\u00a0HYCOM) | 2194 Kb |\n| [Gulf of Mexico data assimilation inter-comparison project](https://www.hycom.org/attachments/084_4_Ashwanth.pdf) | 2007 | Srinivasan | FSU, (11th\u00a0HYCOM) | 3608 Kb |\n| [Ensemble-based assimilation of HF-Radar surface currents in a West Florida Shelf](https://www.hycom.org/attachments/084_4_Barth.pdf) | 2007 | Barth | University of South Florida, (11th\u00a0HYCOM) | 2066 Kb |\n| [The Navy Coupled Ocean Data Assimilation (NCODA) in HYCOM](https://www.hycom.org/attachments/084_Smedstad.pdf) | 2006 | Smedstad | Planning Systems, (10th\u00a0HYCOM) | 3866 Kb |\n| [Multivariate properties of the Ensemble Optimal Interpolation in the GoM](https://www.hycom.org/attachments/084_Counillon.pdf) | 2006 | Counillon | NERSC, (10th\u00a0HYCOM) | 2210 Kb |\n| [On a 3D variational assimilation scheme in hybrid coordinates](https://www.hycom.org/attachments/084_Lozano-2.pdf) | 2006 | Lozano | NCEP/NOAA, (10th\u00a0HYCOM) | 979 Kb |\n| [Gulf of Mexico data assimilation comparison exercise](https://www.hycom.org/attachments/084_Lozano-1.pdf) | 2006 | Lozano | NCEP/NOAA, (10th\u00a0HYCOM) | 437 Kb |\n| [Gulf of Mexico data assimilation comparison exercise](https://www.hycom.org/attachments/084_Chassignet-2.pdf) | 2006 | Chassignet | FSU/COAPS, (10th\u00a0HYCOM) | 4756 Kb |\n| [Constrained Data Assimilation](https://www.hycom.org/attachments/084_Thacker.pdf) | 2006 | Thacker | NOAA/AOML, (10th\u00a0HYCOM) | 91 Kb |\n| [The NCODA implementation with re-layerization](https://www.hycom.org/attachments/084_H.Kang.pdf) | 2005 | Kang | NOAA/AOML, (9th\u00a0HYCOM) | 1795 Kb |\n| [Status of NRL coupled ocean data assimilation (NCODA) system](https://www.hycom.org/attachments/084_J.Cummings(2).pdf) | 2005 | Cummings | NRL, (9th HYCOM) | 470 Kb |\n| [Multi-processor implementation of ROIF assimilation scheme for HYCOM](https://www.hycom.org/attachments/084_A.Srinivasan_ROIF.pdf) | 2005 | Srinivasan | UM/RSMAS, (9th HYCOM) | 2200 Kb |\n| [Implementation of the NRL coupled ocean data assimiltion (NCODA) system in HYCOM](https://www.hycom.org/attachments/084_O.M.Smedsatd.pdf) | 2005 | Smedstad | Planning Systems, (9th\u00a0HYCOM) | 1807 Kb |\n| [Diagnostics for variability in HYCOM runs with data assimilation](https://www.hycom.org/attachments/084_Z.Garraffo(2).pdf) | 2005 | Garraffo | UM/RSMAS, (9th\u00a0HYCOM) | 1574 Kb |\n| [A hindcast experiment with the 1/12 North Atlantic HYCOM model](https://www.hycom.org/attachments/084_L.Parent(2).pdf) | 2005 | Parent | NRL, (9th HYCOM) | 2454 Kb |\n| [Implementation of the MVOI assimilation scheme in HYCOM](https://www.hycom.org/attachments/084_O.M.Smedstad.pdf) | 2004 | Smedstad | Planning Systems, (8th\u00a0HYCOM) | 1560 Kb |\n| [Implementation of the EnKF at NRL-Stennis](https://www.hycom.org/attachments/084_H.Ngodock.pdf) | 2004 | Ngodock, Smedstad | DMS/USM, PSI, (8th\u00a0HYCOM) | 68 Kb |\n| [Handling salinity when assimilating XBT data](https://www.hycom.org/attachments/084_C.Thacker.pdf) | 2004 | Thacker | NOAA/AOML, (8th\u00a0HYCOM) | 1567 Kb |\n| [Description of recent updates to the NRL Coupled Ocean Data Assimilation System](https://www.hycom.org/attachments/084_J.Cummings.pdf) | 2004 | Cummings | NRL, Monterey, (8th\u00a0HYCOM) | 356 Kb |\n| [The 1/12\u00b0 N. Atlantic Ocean Prediction System](https://www.hycom.org/attachments/084_HYCOM-Smedstad.pdf) | 2003 | Smedstad, Lunde | NRL, (7th HYCOM) | 2440 Kb |\n| [Review of the NRL Multivariate Ocean Data Assimilation System](https://www.hycom.org/attachments/084_HYCOM-Cummings.pdf) | 2003 | Cummings, Hodur | NRL, (7th HYCOM) | 881 Kb |\n| [Reduced Order Information Filter update](https://www.hycom.org/attachments/084_Ashwanth-ROIF.pdf) | 2003 | Chin, Srinivasan | UM/RSMAS, (7th\u00a0HYCOM) | 97 Kb |\n| [Implementation of the SEEK filter in HYCOM](https://www.hycom.org/attachments/084_HYCOM-Brasseur.pdf) | 2003 | Brasseur | CNRS, Grenoble, (7th\u00a0HYCOM) | 6395 Kb |\n| [HYCOM Ocean Prediction in France](https://www.hycom.org/attachments/084_HYCOM-Remy.pdf) | 2003 | Baraille | SHOM, (7th\u00a0HYCOM) | 1282 Kb |\n\nAttach",
          "original_query": "The HYCOM (Hybrid Coordinate Ocean Model) data assimilative system",
          "cleaned_query": "The HYCOM (Hybrid Coordinate Ocean Model) data assimilative system",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Forecasting the eddying ocean with a deep neural network - Nature",
          "url": "https://www.nature.com/articles/s41467-025-57389-2",
          "content": "\n \n Introduction The ocean is turbulent, containing eddies with a wide range of sizes. Among them, the mesoscale eddies form the major fraction of the ocean kinetic energy reservoir 1, 2. These mesoscale eddies redistribute heat and materials in the ocean, contributing dominantly to the short-term (from daily to weekly time scales) variations of ocean thermohaline structures 3, 4, regulating the primary productivity and further marine ecosystem 5, 6, and acting as a major driver of extreme events like marine heatwaves 7. They also interact strongly with the overlaying atmosphere, influencing air temperature, humidity, winds, cloud fraction, and rainfall within local marine atmospheric boundary layer 8, 9 as well as atmospheric synoptic variability 10, 11 and large-scale circulations 12. Therefore, accurate eddy-resolving ocean forecasts are not only essential for supporting marine activities and managements but also necessary to improve weather forecast accuracy. So far, ocean forecasts rely primarily on ocean general circulation models (OGCMs) that make forecasts by numerically discretizing and integrating the governing partial differential equations of the ocean. However, constructing an accurate eddy-resolving global ocean forecast system (GOFS) based on the OGCMs (i.e., the numerical GOFS) remains a challenging issue both computationally and scientifically. On the one hand, resolving mesoscale eddies requires a grid size of O(10\u2009km) or even finer 13, 14, imposing a massive computational burden for operating global-scale OGCMs especially for implementing advanced data assimilation 15 and making ensemble forecast 16. In fact, the eddy-resolving numerical GOFSs did not emerge until the recent decade partially due to the enlarged computational resources. On the other hand, the chaotic nature of mesoscale eddies makes their forecast very sensitive to errors in initial conditions, boundary forcings, and OGCMs. In particular, despite development over about a half-century, there are still many uncertainties in OGCMs, including numerical errors caused by discretization, uncertainties in parameterizations of unresolved processes, and insufficient representation of interactions of the ocean with other components of the Earth system 17. Artificial intelligence (AI)-based methods provide a data-driven approach for making forecasts and have been successfully applied to the global medium-range weather forecasts 18, 19, 20, 21, 22, 23, 24, 25. Their success is achieved primarily from high-quality atmospheric reanalysis training datasets and customized deep neural network (DNN) designed to capture the hidden representations of atmospheric dynamics and alleviate accumulative error when rolling out multistep autoregressive forecasts 18, 19, 20, 21, 22, 23, 24, 25. These AI-based forecast systems show competitive forecast performance yet substantially reduce the computational burden compared to their numerical counterparts. The successful application of AI-based methods in medium-range weather forecasts has been inspiring their ocean-related usage, including the OGCM emulators, the short-term ocean forecast as well as the interannual-to-decadal ocean prediction 26, 27, 28, 29, although it has been well recognized that ocean reanalysis datasets are less robust compared to their atmospheric counterparts primarily due to the sparsity of ocean observations 30. It should be noted that the existing DNN architectures developed for the medium-range weather forecast may not be suitable for the eddy-resolving ocean forecast. Air-sea interactions play an important role in the short-term ocean variability, especially in the surface mixed layer 31. However, these interactions have not been explicitly incorporated into the AI-based medium-range weather forecast systems except for providing the SST as an input. Furthermore, the existing AI-based methods tend to smooth out mesoscale weather phenomena 32, 33 and are expected to dampen ocean mesoscale eddies in a similar way. Such a blurring effect is tolerable for the medium-range weather forecast as its variability is generally dominated by synoptic processes 34, but not so for short-term ocean forecast, at which time scale mesoscale eddies make a major contribution to the variability 35 (Supplementary Fig.\u00a0 S1). In this study, we present WenHai, an AI-based GOFS for short-term eddy-resolving forecast across the global upper ocean (0\u2013643\u2009m). WenHai explicitly incorporates atmospheric forcings into the DNN by exploiting the bulk formulae 36 on air-sea fluxes. Furthermore, the design of WenHai\u2019s architecture is guided by the characteristics of mesoscale eddies to better preserve their variabilities. As demonstrated below, these features make WenHai outperform state-of-the-art numerical and AI-based GOFSs in forecasting the eddying ocean. Results An AI-based eddy-resolving GOFS Trained on a state-of-the-art eddy-resolving (1/12\u00b0) global ocean reanalysis dataset 37 (See \u2018GLORYS reanalysis\u2019 in Methods), WenHai forecasts 1/12\u00b0 daily averaged sea surface height (SSH) and three-dimensional temperature, salinity, and horizontal current in the upper 643\u2009m across the global ocean in an autoregressive way. WenHai utilizes the Swin-Transformer 38 as its backbone. The training process is decomposed into two stages. WenHai is first pre-trained to minimize the loss for the one-day forecast. Then we adopt a finetune technique 20 to minimize the accumulative loss for a sequence of autoregressive forecasts over 5 days, which improves WenHai\u2019s performance at long forecast lead times. The architecture of WenHai\u00a0(Fig. 1) and its training details are elaborated in Methods (See \u2018WenHai model\u2019 in Methods). Fig. 1: A schematic of WenHai\u2019s architecture. The surface atmosphere and ocean variables on the day \\(t+l\\) are first combined to compute the air-sea momentum, heat, and freshwater fluxes based on the bulk formulae (green block), where t is an arbitrary date index and l is the forecast lead time (indexing in 1-day intervals). Then, the ocean variables and air-sea fluxes are sent to a deep neural network (red block) that forecasts temporal tendency between ocean variables on the days \\(t+l\\) and \\(t+l+1\\). The tendency field is added to the ocean variables on the day \\(t+l\\) to yield the ocean variable forecast on the day \\(t+l+1\\) \u00a0(blue block). Finally, the above processes are iterated to generate a sequence of forecasts. Maps created with Cartopy 65. Background land image provided by NASA Earth Observatory. Full size image We exploit domain knowledge in the air-sea interactions and ocean dynamics to guide WenHai\u2019s architecture design, which enhances its capacity to forecast the eddying ocean. First, to explicitly represent the atmospheric forcings, we implement a specialized block for computing air-sea momentum, heat and freshwater fluxes from the surface atmosphere variables (e.g., air temperature, winds, etc.) based on the bulk formulae (Supplementary Note\u00a0 1 and Supplementary Fig.\u00a0 S2). Second, the forecast output is chosen as the temporal tendency of an ocean variable between the two consecutive days rather than the ocean variable on the following day. This is essential to preserve the mesoscale eddy variabilities, as mesoscale eddies dominate the day-to-day variations not the absolute values of ocean variables (Supplementary Fig.\u00a0 S3). Similarly, we put more weight on the loss function in the upper part of the ocean, as mesoscale eddies are generally near-surface intensified 39. Finally, the land region is masked to make WenHai focus on forecasting the ocean variability. Supplementary Fig.\u00a0 S4 provides a visualization of sea surface kinetic energy (KE), sea surface height (SSH), and sea surface temperature (SST) forecast in the Kuroshio Extension region by WenHai. Here, the GLORYS reanalysis is approximated as the ground truth to test the capacity of WenHai to forecast the eddying ocean, with caution about the fidelity of the GLORYS reanalysis in representing reality. WenHai does capture the temporal evolution of KE, SSH, and SST reasonably well, outperforming the persistent forecast that assumes the future ocean state would be the same as the initial condition. In the following sections, we will provide more systematic and quantified assessments of the forecast performance of WenHai and compare it with a state-of-the-art numerical GOFS and AI-based GOFS. Outperformance of WenHai against a state-of-the-art numerical GOFS To evaluate the forecast performance of WenHai, we compare it with a state-of-the-art eddy-resolving numerical GOFS, i.e., the 1/12\u00b0 operational ocean analysis and forecasting systems (GLO12v4 40, 41) from the France Mercator Oc\u00e9an (See \u2018GLO12v4\u2019 in Methods). WenHai is initialized from the same initial condition and forced by the same atmospheric forecast product as GLO12v4 during April-November 2024. We remark that using the same initial condition is essential to make a fair comparison between different forecast systems, as errors in the initial condition could have substantial influences on the forecast performance (Supplementary Note\u00a0 2 and Supplementary Fig.\u00a0 S5). The ocean variables involved in the comparison are selected following the GODAE Ocean-View Inter-comparison and Validation Task Team (IV-TT) Class 4 framework 42, including the SST and 15-m current measured by drifting buoys, temperature and salinity profiles measured by Argo profiling floats and level-3 along-track sea level anomaly (SLA) measured by satellite altimeters (See \u2018Observational datasets\u2019 in Methods). Two metrics are used to quantify the forecast performance of WenHai and GLO12v4 (See \u2018Verification metrics\u2019 in Methods). The first is the root mean square error (RMSE), a conventional point-to-point verification metric adopted by the GODAE Ocean-View IV-TT Class 4 framework. However, it has been recognized that verification on a point-to-point basis is less appropriate for assessing the performance of high-resolutio",
          "original_query": "Forecasting the eddying ocean with a deep neural network",
          "cleaned_query": "Forecasting the eddying ocean with a deep neural network",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Mercator Ocean International Makes AI Ocean Forecasting ...",
          "url": "http://www.afp.com/en/infos/mercator-ocean-international-makes-ai-ocean-forecasting-operational-glonet-validated",
          "content": "Mercator Ocean International Makes AI Ocean Forecasting Operational with GLONET Validated by OceanBench at NeurIPS 2025 | AFP.com\n[Skip to main content](#main-content)\n[![](http://www.afp.com/themes/custom/afp9/assets/svg/logo-afp.svg)](http://www.afp.com/en)\n[Connexion](https://news.afp.com/#/u/login)\n[Contact us](http://www.afp.com/en/contact-us)\nen\n* [Fran\u00e7ais](http://www.afp.com/fr/node/3807251)\n* [English](http://www.afp.com/en/infos/mercator-ocean-international-makes-ai-ocean-forecasting-operational-glonet-validated)\n* [Espa\u00f1ol](http://www.afp.com/es/node/3807251)\n* [Portugu\u00eas](http://www.afp.com/pt/node/3807251)\n* [Deutsch](http://www.afp.com/de/node/3807251)\n* [\u0627\u0644\u0639\u0631\u0628\u064a\u0629](http://www.afp.com/ar/node/3807251)\n[Login](https://news.afp.com/#/u/login)\nPr\u00e9c\u00e9dent\n* Montreal (AFP)| 10/12/2025 - 16:06:30| Bank of Canada maintains key interest rate of 2.25 percent\n* Vienna (AFP)| 10/12/2025 - 15:24:17| Austrian court rejects Ukraine tycoon&#039;s US extradition: statement\n* Oslo (AFP)| 10/12/2025 - 14:22:29| Nobel peace laureate Machado says &#039;we must be willing to fight for freedom&#039;: speech delivered by daughter\n* Oslo (AFP)| 10/12/2025 - 14:16:24| Nobel peace laureate Machado blasts &#039;state terrorism&#039; in Venezuela: speech delivered by daughter\n* Oslo (AFP)| 10/12/2025 - 14:14:56| Nobel Peace Prize winner Machado will be back in Venezuela &#039;very soon&#039;: daughter\n* Oslo (AFP)| 10/12/2025 - 13:53:38| Daughter of absent Peace Prize laureate Machado accepts Nobel on her behalf\n* Oslo (AFP)| 10/12/2025 - 13:46:46| Nobel Peace Prize committee chair urges Venezuela&#039;s Maduro to &#039;step down&#039;\n* Kinshasa (AFP)| 10/12/2025 - 13:17:24| Burundi closes border with DR Congo after new M23 advance: security officials\n* Kinshasa (AFP)| 10/12/2025 - 13:11:53| Burundi closes border with DR Congo after new M23 advance: security officials\n* Paris (AFP)| 10/12/2025 - 12:26:31| Ukraine allies to hold Thursday video call: France\nSuivant\n[Previous](http://www.afp.com/en/infos/csg-xponent-sweeps-top-tier-analyst-industry-recognition-customer-journey-management)[Back to summary](http://www.afp.com/en/access-our-content/partners/partners/business-wire)[Next](http://www.afp.com/en/infos/netjets-bring-starlink-high-speed-connectivity-fleet)\n## Business Wire\n[![](https://www.afp.com/sites/default/files/afppartenaire/202003/logobusinesswire.png)](http://www.afp.com/en/access-our-content/partners/partners/business-wire)\n# Mercator Ocean International Makes AI Ocean Forecasting Operational with GLONET Validated by OceanBench at NeurIPS 2025\n10Dec 2025\n**SAN DIEGO &amp; TOULOUSE, France**\n**At NeurIPS 2025, one of the world\u2019s leading AI conferences, Mercator Ocean unveiled OceanBench, the first open benchmark designed to assess AI Ocean forecasting models. This milestone follows the publication of Mercator Ocean\u2019s AI model GLONET in September. Together, OceanBench and GLONET represent a decisive step toward integrating artificial intelligence into operational ocean forecasting, strengthening Europe\u2019s leadership in trustworthy, AI-driven predictions at global scale.**\nPresented by Anass El Aouni, Oceanographer and machine learning (ML) researcher, and Quentin Gaudel, ML Systems Architect, OceanBench establishes a unified standard to evaluate AI models against fundamental ocean dynamics. GLONET is the first forecasting system to be validated through OceanBench for operational use.\n**Anass El Aouni :**\u201c*OceanBench provides a transparent standard to evaluate AI-driven ocean forecasts*.*By comparing artificial intelligence with physics-based models, we can highlight strengths, identify areas for improvement, and guide the development of more reliable forecasting tools. There is no single \u2018best\u2019 model, but the benchmark ensures fair, science-based comparisons and helps users select the most suitable solution for each application.*\u201d\n**Quentin Gaudel:***\u201cArtificial intelligence is transforming the speed and scale of ocean prediction, but trust requires careful validation. OceanBench allows the community to rigorously measure performance while keeping AI models grounded in ocean physics. Combined with physics-based systems, AI can accelerate forecasts and broaden access to ocean information. This is critical for advancing operational forecasting and supporting Europe\u2019s Digital Twin Ocean vision.*\u201d\n**A new standard for AI Ocean prediction**\nWhile AI has revolutionized weather forecasts, its operational use in ocean forecasting has been limited by the lack of shared standards for quality, validation, and physical realism. OceanBench fills this gap by providing:\n* Open data and open-source tools for reproducible evaluation,\n* Three evaluation tracks: Model-to-Reanalysis, Model-to-Analysis, Model-to-Observations,\n* Physical diagnostics that verify forecasts against known ocean dynamics.\nThe first benchmark round includes four global forecasting systems: the high-resolution physics-based GLO12, and three ML ocean emulators \u2014GLONET, XiHe, and Wenhai. Early results indicate AI models deliver rapid, accurate forecasts in many areas, but high-resolution physics-based systems remain essential for capturing fine-scale and high-frequency ocean processes. This combined approach is key to building reliable, operational solutions.\n**GLONET, Mercator Ocean\u2019s new AI global forecasting system**\nPublished in September 2025, and following full validation through OceanBench, GLONET is ready for operational deployment**.**Its key features include:\n-**Daily 10-day global forecasts generated within seconds,\n**- Forecast variables including sea surface height, temperature, salinity, and currents across 21 depth levels,\n- Trained on Mercator Ocean\u2019s GLORYS12 reanalysis, ensuring physical consistency.\n**Marie Drevillon, Head of Operations at Mercator Ocean and co-author of OceanBench:**\u201c*With GLONET validated through OceanBench, AI becomes part of our operational forecasting toolkit. These capabilities enable faster simulations, broader access to ocean information, and ultimately more robust, science-based decision support.*\u201d\n**Supporting Europe\u2019s Digital Ocean Vision**\nThe launch of OceanBench and deployment of GLONET mark significant steps toward Europe\u2019s vision for a Digital Twin Ocean capable of providing fast, reliable, and physically robust predictions. By establishing a community-driven standard for AI evaluation, OceanBench strengthens Europe\u2019s leadership in trustworthy, operational AI ocean science.\nBy transitioning into an intergovernmental organization (IGO) representing its Member States, Mercator Ocean fosters long-term European collective action in support of this shared digital ocean vision.\n**Resources**\n* **OceanBench Paper**(NeurIPS 2025):[NeurIPS Poster OceanBench: A Benchmark for Data-Driven Global Ocean Forecasting systems](https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https://neurips.cc/virtual/2025/loc/san-diego/poster/121394&amp;esheet=54372911&amp;newsitemid=20251210914358&amp;lan=en-US&amp;anchor=NeurIPS+Poster+OceanBench:+A+Benchmark+for+Data-Driven+Global+Ocean+Forecasting+systems&amp;index=1&amp;md5=a43adb211992ed0525f1efc4706f1de4)\n* **OceanBench Benchmark**:[https://oceanbench.lab.dive.edito.eu/](https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https://oceanbench.lab.dive.edito.eu/&amp;esheet=54372911&amp;newsitemid=20251210914358&amp;lan=en-US&amp;anchor=https://oceanbench.lab.dive.edito.eu/&amp;index=2&amp;md5=b41baf90b63861eb500af5f26289df58)\n* **GLONET**:[GLONET: Mercator's End\u2010to\u2010End Neural Global Ocean Forecasting System - El Aouni - 2025 - Journal of Geophysical Research: Machine Learning and Computation - Wiley Online Library](https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2025JH000686&amp;esheet=54372911&amp;newsitemid=20251210914358&amp;lan=en-US&amp;anchor=GLONET:+Mercator's+End&#8208;to&#8208;End+Neural+Global+Ocean+Forecasting+System+-+El+Aouni+-+2025+-+Journal+of+Geophysical+Research:+Machine+Learning+and+Computation+-+Wiley+Online+Library&amp;index=3&amp;md5=807b540078031e137a0566575de0fabc)\n**About OceanBench V1.0\n***\u201cOceanBench: A Benchmark for Data-Driven Global Ocean Forecasting Systems\u201d: published*and presented at NeurIPS 2025. Authors include Anass El Aouni, Quentin Gaudel, Juan Emmanuel Johnson, Charly Regnier, Julien Le Sommer, Simon van Gennip, Ronan Fablet, Marie Drevillon, Yann Drillet, and Pierre-Yves Le Traon. OceanBench is implemented by[Mercator Ocean International](https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https://mercator-ocean.eu/&amp;esheet=54372911&amp;newsitemid=20251210914358&amp;lan=en-US&amp;anchor=Mercator+Ocean+International&amp;index=4&amp;md5=089210aa72ab690bcb5122841088eae7)in collaboration with:*Programme prioritaire de recherche Oc\u00e9an &amp; Climat, IMT Atlantique, Universit\u00e9 de Grenoble Alpes, IGEO Institute de Geociencias*.\nAbout Mercator Ocean\nMercator Ocean International is a global leader in digital oceanography and ocean forecasting. Since 2014, it has operated the Copernicus Marine Service on behalf of the European Commission, providing free, reliable ocean data and forecasts worldwide. The organization develops advanced digital ocean systems and leads the European Digital Twin Ocean initiative, supporting ocean decision-making with predictive, science-based insights. Currently evolving into a new intergovernmental organization (IGO), Mercator Ocean is strengthening collaboration across Europe and globally to advance digital ocean systems for a sustainable ocean.[www.mercator-ocean.eu](https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https://www.mercator-ocean.eu&amp;esheet=54372911&amp;newsitemid=20251210914358&amp;lan=en-US&amp;anchor=www.mercator-ocean.eu&amp;index=5&amp;md5=43a233a4102451b652cfe3a991288e8d)\n![](https://cts.businesswire.com/ct/CT?id=bwnews&amp;sty=20251210914358r1&amp;sid=fraf7&amp;distro=nx&amp;lang=en)\nView source version on businesswire.com:[h",
          "original_query": "Glonet: Mercator\u2019s end-to-end neural forecasting system",
          "cleaned_query": "Glonet: Mercator\u2019s end-to-end neural forecasting system",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Accurate medium-range global weather forecasting with 3D ... - Nature",
          "url": "https://www.nature.com/articles/s41586-023-06185-3",
          "content": "Accurate medium-range global weather forecasting with 3D neural networks | Nature\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature](https://media.springernature.com/full/nature-cms/uploads/product/nature/header-86f1267ea01eccd46b530284be10585e.svg)](https://www.nature.com/)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41586-023-06185-3?error=cookies_not_supported&code=c5cddab8-987b-4b7c-b53e-10f9daffb475)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41586)\n* [RSS feed](https://www.nature.com/nature.rss)\nAccurate medium-range global weather forecasting with 3D neural networks\n[Download PDF](https://www.nature.com/articles/s41586-023-06185-3.pdf)\n[Download PDF](https://www.nature.com/articles/s41586-023-06185-3.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:05 July 2023# Accurate medium-range global weather forecasting with 3D neural networks\n* [Kaifeng Bi](#auth-Kaifeng-Bi-Aff1)[ORCID:orcid.org/0009-0008-7872-1197](https://orcid.org/0009-0008-7872-1197)[1](#Aff1),\n* [Lingxi Xie](#auth-Lingxi-Xie-Aff1)[ORCID:orcid.org/0000-0003-4831-9451](https://orcid.org/0000-0003-4831-9451)[1](#Aff1),\n* [Hengheng Zhang](#auth-Hengheng-Zhang-Aff1)[ORCID:orcid.org/0009-0006-3241-6862](https://orcid.org/0009-0006-3241-6862)[1](#Aff1),\n* [Xin Chen](#auth-Xin-Chen-Aff1)[ORCID:orcid.org/0000-0003-3686-6212](https://orcid.org/0000-0003-3686-6212)[1](#Aff1),\n* [Xiaotao Gu](#auth-Xiaotao-Gu-Aff1)[ORCID:orcid.org/0009-0002-7896-4750](https://orcid.org/0009-0002-7896-4750)[1](#Aff1)&amp;\n* \u2026* [Qi Tian](#auth-Qi-Tian-Aff1)[ORCID:orcid.org/0000-0002-7252-5047](https://orcid.org/0000-0002-7252-5047)[1](#Aff1)Show authors\n[*Nature*](https://www.nature.com/)**volume619**,pages533\u2013538 (2023)[Cite this article](#citeas)\n* 313kAccesses\n* 1300Citations\n* 1905Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41586-023-06185-3/metrics)\n### Subjects\n* [Atmospheric dynamics](https://www.nature.com/subjects/atmospheric-dynamics)\n* [Computer science](https://www.nature.com/subjects/computer-science)\nAn[Author Correction](https://doi.org/10.1038/s41586-023-06545-z)to this article was published on 14 September 2023\nThis article has been[updated](#change-history)\n## Abstract\nWeather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial differential equations that describe the transition between those states[1](https://www.nature.com/articles/s41586-023-06185-3#ref-CR1). However, this procedure is computationally expensive. Recently, artificial-intelligence-based methods[2](https://www.nature.com/articles/s41586-023-06185-3#ref-CR2)have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still significantly lower than that of NWP methods. Here we introduce an artificial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39\u2009years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world\u2019s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)[3](https://www.nature.com/articles/s41586-023-06185-3#ref-CR3). Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking tropical cyclones is also higher than that of ECMWF-HRES.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-022-11936-9/MediaObjects/41598_2022_11936_Fig1_HTML.png)\n### [ECMWF short-term prediction accuracy improvement by deep learning](https://www.nature.com/articles/s41598-022-11936-9?fromPaywallRec=false)\nArticleOpen access12 May 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs43247-025-02502-y/MediaObjects/43247_2025_2502_Fig1_HTML.png)\n### [The operational medium-range deterministic weather forecasting can be extended beyond a 10-day lead time](https://www.nature.com/articles/s43247-025-02502-y?fromPaywallRec=false)\nArticleOpen access03 July 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-62024-1/MediaObjects/41467_2025_62024_Fig1_HTML.png)\n### [A data-to-forecast machine learning system for global weather](https://www.nature.com/articles/s41467-025-62024-1?fromPaywallRec=false)\nArticleOpen access19 July 2025\n## Main\nWeather forecasting is an important application of scientific computing that aims to predict future weather changes, especially in regards to extreme weather events. In the past decade, high-performance computing systems have greatly accelerated research in the field of numerical weather prediction (NWP) methods[1](https://www.nature.com/articles/s41586-023-06185-3#ref-CR1). Conventional NWP methods are primarily concerned with describing the transitions between discretized grids of atmospheric states using partial differential equations (PDEs) and then solving them with numerical simulations[4](#ref-CR4),[5](#ref-CR5),[6](https://www.nature.com/articles/s41586-023-06185-3#ref-CR6). These methods are often slow; a single simulation for a ten-day forecast can take hours of computation in a supercomputer that has hundreds of nodes[7](https://www.nature.com/articles/s41586-023-06185-3#ref-CR7). In addition, conventional NWP algorithms largely rely on parameterization, which uses approximate functions to capture unresolved processes, where errors can be introduced by approximation[8](https://www.nature.com/articles/s41586-023-06185-3#ref-CR8),[9](https://www.nature.com/articles/s41586-023-06185-3#ref-CR9).\nThe rapid development of deep learning[10](https://www.nature.com/articles/s41586-023-06185-3#ref-CR10)has introduced a promising direction, which the scientific community refers to as artificial intelligence (AI)-based methods[2](https://www.nature.com/articles/s41586-023-06185-3#ref-CR2),[11](#ref-CR11),[12](#ref-CR12),[13](#ref-CR13),[14](#ref-CR14),[15](#ref-CR15),[16](https://www.nature.com/articles/s41586-023-06185-3#ref-CR16). Here, the methodology is to train a deep neural network to capture the relationship between the input (reanalysis weather data at a given point in time) and the output (reanalysis weather data at the target point in time). On specialized computational devices such as graphics processing units (GPUs), AI-based methods are extremely fast. To give a recent example, FourCastNet[2](https://www.nature.com/articles/s41586-023-06185-3#ref-CR2)takes only 7\u2009s to compute a 100-member, 24-hour forecast, which is orders of magnitudes faster than conventional NWP methods. However, the accuracy of FourCastNet is still below satisfactory; its root mean square error (RMSE) of a 5-day Z500\u00a0(500\u00a0hPa geopotential) forecast is 484.5, which is much worse than the 333.7 reported by the operational integrated forecasting system (IFS) of the European Centre for Medium-Range Weather Forecasts (ECMWF)[3](https://www.nature.com/articles/s41586-023-06185-3#ref-CR3). In a recent survey[17](https://www.nature.com/articles/s41586-023-06185-3#ref-CR17), researchers agreed that AI holds great potential, but admitted that \u201ca number of fundamental breakthroughs are needed\u201d before AI-based methods can beat NWP.\nThese breakthroughs seem to be happening earlier than expected. Here we present Pangu-Weather (see[Methods](https://www.nature.com/articles/s41586-023-06185-3#Sec8)for an explanation of the name \u2018Pangu\u2019), a powerful AI-based weather forecasting system that produces stronger deterministic forecast results than the operational IFS on all tested weather variables against reanalysis data. Our technical contributions are two-fold. First, we integrated height information into a new dimension so that the input and output of our deep neural networks can be conceptualized in three dimensions. We further designed a three-dimensional (3D) Earth-specific transformer (3DEST) architecture to inject Earth-specific priors into the deep networks. Our experiments show that 3D models, by formulating height into an individual dimension, have the ability to capture the relationship between atmospheric states in different pressure levels and thus yield significant accuracy gains, compared with two-dimensional models such as FourCastNet[2](https://www.nature.com/articles/s41586-023-06185-3#ref-CR2). Second, we applied a hierarchical temporal aggregation algorithm that involves training a series of models with increasing forecast lead times. Hence, in the testing stage, the number of iterations needed for medium-range weather forecasting was largely reduced, and the cumulative forecast errors were alleviated. Experiments on the fifth generation of ECMWF reanalysis (ERA5) data[18](https://www.nature.com/",
          "original_query": "Accurate medium-range global weather forecasting with 3D neural networks",
          "cleaned_query": "Accurate medium-range global weather forecasting with 3D neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Better informed marine operations and management ...",
          "url": "https://figshare.utas.edu.au/articles/journal_contribution/Better_informed_marine_operations_and_management_multidisciplinary_efforts_in_ocean_forecasting_research_for_socioeconomic_benefit/22954568",
          "content": "Item - Better informed marine operations and management: multidisciplinary efforts in ocean forecasting research for socioeconomic benefit - University of Tasmania - Figshare\nSkip to main content\n[![University of Tasmania](https://s3-eu-west-1.amazonaws.com/876az-branding-figshare/utas/logo_header.png)](https://figshare.utas.edu.au/)\n[Browse](https://figshare.utas.edu.au/browse)\nBrowse and SearchSearch\n# Better&#32;informed&#32;marine&#32;operations&#32;and&#32;management&#58;&#32;multidisciplinary&#32;efforts&#32;in&#32;ocean&#32;forecasting&#32;research&#32;for&#32;socioeconomic&#32;benefit\n* [https://hdl.handle.net/102.100.100/565820](https://hdl.handle.net/102.100.100/565820)\nCopy identifier URL to clipboardIdentifier Info\nCite[Download(858.15 kB)](https://figshare.utas.edu.au/ndownloader/files/40691618/1)ShareEmbed\njournal&#32;contribution\nposted on2023-05-19, 07:12authored by[Andreas&#32;SchillerAndreas Schiller](https://figshare.utas.edu.au/authors/Andreas_Schiller/14748826),Davidson,&#32;F,DiGiacomo,&#32;PM,Wilmer-Becker,&#32;K\nIn&#32;Numerical&#32;Weather&#32;Prediction&#32;&#40;NWP&#41;,&#32;forecast&#32;systems&#32;are&#32;used&#32;to&#32;transform&#32;meteorological&#32;observations&#32;into&#32;forecasts&#32;that&#32;provide&#32;the&#32;basis&#32;for&#32;information&#32;services&#32;to&#32;the&#32;general&#32;public&#32;and&#32;many&#32;other&#32;users.&#32;In&#32;oceanography,&#32;in&#32;order&#32;to&#32;best&#32;exploit&#32;the&#32;observations,&#32;it&#32;is&#32;similarly&#32;necessary&#32;to&#32;transform&#32;them&#32;into&#32;coherent&#32;analyses&#32;and&#32;predictions&#32;that&#32;can&#32;be&#32;the&#32;basis&#32;for&#32;information&#32;services&#32;about&#32;the&#32;marine&#32;environment,&#32;its&#32;ecosystem,&#32;and&#32;the&#32;cryosphere&#32;that&#32;can&#32;be&#32;used&#32;in&#32;many&#32;applications&#32;and&#32;that&#32;can&#32;provide&#32;boundary&#32;data&#32;for&#32;weather&#32;predictions.&#32;Marine&#32;industries&#32;&#40;e.g.,&#32;commercial&#32;fishing,&#32;aquaculture,&#32;shipping,&#32;oil&#32;and&#32;gas,&#32;renewable&#32;energy,&#32;tourism&#41;,&#32;government&#32;agencies&#32;&#40;e.g.,&#32;those&#32;responsible&#32;for&#32;search&#32;and&#32;rescue,&#32;defense,&#32;coastal&#32;management,&#32;environmental&#32;protection&#41;,&#32;and&#32;other&#32;stakeholders&#32;&#40;e.g.,&#32;recreation,&#32;water&#32;sports,&#32;artisanal&#32;and&#32;sport&#32;fishing&#41;&#32;depend&#32;on&#32;timely&#32;and&#32;accurate&#32;information&#32;about&#32;the&#32;marine&#32;environment.&#32;This&#32;includes&#32;ocean&#32;physical&#32;and&#32;biological&#32;states,&#32;the&#32;weather,&#32;and&#32;for&#32;some&#32;areas,&#32;ice&#32;cover&#32;from&#32;hours&#32;to&#32;weeks&#32;in&#32;advance&#32;as&#32;well&#32;as&#32;in&#32;the&#32;past.&#32;Supported&#32;by&#32;progress&#32;in&#32;numerical&#32;ocean&#32;modeling&#32;and&#32;data&#32;assimilation&#32;methods&#59;&#32;increased&#32;supercomputing&#32;capacity&#59;&#32;and&#32;most&#32;importantly,&#32;enhanced,&#32;routine,&#32;and&#32;sustained&#32;in&#32;situ&#32;and&#32;remotely&#32;sensed&#32;ocean&#32;observations,&#32;the&#32;last&#32;decade&#32;saw&#32;the&#32;development&#32;and&#32;operational&#32;implementation&#32;of&#32;mesoscale&#32;&#40;eddy-resolving&#41;&#32;short-&#32;and&#32;medium-range&#32;&#40;days&#32;to&#32;weeks&#41;&#32;ocean&#32;forecasting&#32;and&#32;reanalysis&#32;capabilities&#32;at&#32;many&#32;operational&#32;weather&#32;and&#32;ocean&#32;forecasting&#32;centers.&#32;The&#32;building&#32;and&#32;maintaining&#32;of&#32;operational&#32;ocean&#32;forecasting&#32;systems&#32;require&#32;a&#32;wide&#32;range&#32;of&#32;expertise.&#32;Most&#32;global&#32;ocean&#32;forecasting&#32;systems&#32;transition&#32;from&#32;research&#32;and&#32;demonstration&#32;modes&#32;to&#32;sustained,&#32;permanent&#32;operational&#32;capabilities&#32;with&#32;attendant&#32;infrastructure&#32;&#40;European&#32;Commission&#32;2015&#41;.&#32;Beyond&#32;the&#32;traditional&#32;short-term&#32;forecasting&#32;of&#32;physical&#32;ocean&#32;properties&#32;&#40;temperature,&#32;salinity,&#32;surface&#32;height,&#32;currents,&#32;waves&#41;,&#32;marine&#32;activities&#32;such&#32;as&#32;water&#32;quality&#32;and&#32;habitat&#32;management&#32;as&#32;well&#32;as&#32;climate&#32;research&#32;increasingly&#32;rely&#32;on&#32;operational&#32;oceanographic&#32;data&#32;and&#32;products.&#32;To&#32;satisfy&#32;existing&#32;and&#32;new&#32;requirements&#32;for&#32;end-use&#32;applications,&#32;such&#32;as&#32;coastal&#32;protection,&#32;ecosystem&#32;monitoring&#32;and&#32;forecasting,&#32;and&#32;climate&#32;monitoring,&#32;these&#32;operational&#32;ocean&#32;forecasting&#32;systems&#32;must&#32;be&#32;sustained,&#32;as&#32;well&#32;as&#32;evolve&#32;and&#32;improve,&#32;to&#32;remain&#32;relevant&#32;with&#32;broad&#32;utility.\n## History\n## Related Materials\n1. 1.\nDOI -Is supplement to[Better&#32;informed&#32;marine&#32;operations&#32;and&#32;management&#58;&#32;multidisciplinary&#32;efforts&#32;in&#32;ocean&#32;forecasting&#32;research&#32;for&#32;socioeconomic&#32;benefit](https://doi.org/10.1175/BAMS-D-15-00102.1)\n## Publication title\nAmerican&#32;Meteorological&#32;Society.&#32;Bulletin\n## Volume\n97\n## Issue\n9\n## Pagination\n1553-1558\n## ISSN\n0003-0007\n## Department/School\nInstitute&#32;for&#32;Marine&#32;and&#32;Antarctic&#32;Studies\n## Publisher\nAmer&#32;Meteorological&#32;Soc\n## Place of publication\n45&#32;Beacon&#32;St,&#32;Boston,&#32;USA,&#32;Ma,&#32;02108-3693\n## Rights statement\nCopyright&#32;2016&#32;American&#32;Meteorological&#32;Society&#32;&#40;AMS&#41;.&#32;Permission&#32;to&#32;use&#32;figures,&#32;tables,&#32;and&#32;brief&#32;excerpts&#32;from&#32;this&#32;work&#32;in&#32;scientific&#32;and&#32;educational&#32;works&#32;is&#32;hereby&#32;granted&#32;provided&#32;that&#32;the&#32;source&#32;is&#32;acknowledged.&#32;Any&#32;use&#32;of&#32;material&#32;in&#32;this&#32;work&#32;that&#32;is&#32;determined&#32;to&#32;be&#32;&#8220;fair&#32;use&#8221;&#32;under&#32;Section&#32;107&#32;of&#32;the&#32;U.S.&#32;Copyright&#32;Act&#32;or&#32;that&#32;satisfies&#32;the&#32;conditions&#32;specified&#32;in&#32;Section&#32;108&#32;of&#32;the&#32;U.S.&#32;Copyright&#32;Act&#32;&#40;17&#32;USC&#32;&#167;108&#41;&#32;does&#32;not&#32;require&#32;the&#32;AMS&#8217;s&#32;permission.&#32;Republication,&#32;systematic&#32;reproduction,&#32;posting&#32;in&#32;electronic&#32;form,&#32;such&#32;as&#32;on&#32;a&#32;website&#32;or&#32;in&#32;a&#32;searchable&#32;database,&#32;or&#32;other&#32;uses&#32;of&#32;this&#32;material,&#32;except&#32;as&#32;exempted&#32;by&#32;the&#32;above&#32;statement,&#32;requires&#32;written&#32;permission&#32;or&#32;a&#32;license&#32;from&#32;the&#32;AMS.&#32;All&#32;AMS&#32;journals&#32;and&#32;monograph&#32;publications&#32;are&#32;registered&#32;with&#32;the&#32;Copyright&#32;Clearance&#32;Center&#32;&#40;http&#58;&#47;&#47;www.copyright.com&#41;.&#32;Questions&#32;about&#32;permission&#32;to&#32;use&#32;materials&#32;for&#32;which&#32;AMS&#32;holds&#32;the&#32;copyright&#32;can&#32;also&#32;be&#32;directed&#32;to&#32;the&#32;AMS&#32;Permissions&#32;Officer&#32;at&#32;permissions&#64;ametsoc.org.&#32;Additional&#32;details&#32;are&#32;provided&#32;in&#32;the&#32;AMS&#32;Copyright&#32;Policy&#32;statement,&#32;available&#32;on&#32;the&#32;AMS&#32;website&#32;&#40;http&#58;&#47;&#47;www.ametsoc.org&#47;CopyrightInformation&#41;.\n## Socio-economic Objectives\nExpanding&#32;knowledge&#32;in&#32;the&#32;earth&#32;sciences\n## Repository Status\n* Open\n## [Usage metrics](https://info.figshare.com/user-guide/usage-metrics-and-statistics/)\n**0**\n**0**\n**0**\n[![University Of Tasmania](https://876az-branding-figshare.s3.eu-west-1.amazonaws.com/utas/logo_new.png)](https://figshare.utas.edu.au/)\n## Categories\n* [Meteorology](https://figshare.utas.edu.au/search?q=:category:%20&quot;Meteorology&quot;)\n## Keywords\n[forecasting](https://figshare.utas.edu.au/search?q=:keyword:%20%22forecasting%22)[oceanography](https://figshare.utas.edu.au/search?q=:keyword:%20%22oceanography%22)[weather](https://figshare.utas.edu.au/search?q=:keyword:%20%22weather%22)[climate](https://figshare.utas.edu.au/search?q=:keyword:%20%22climate%22)\n## Licence\n[In&#32;Copyright](http://rightsstatements.org/vocab/InC/1.0/)\n## Exports\nSelect an option\nRefWorksRefWorks\nBibTeXBibTeX\nRef. managerRef. manager\nEndnoteEndnote\nDataCiteDataCite\nNLMNLM\nDCDC",
          "original_query": "Better informed marine operations and management: Multidisciplinary efforts in ocean forecasting research for socio-economic benefit",
          "cleaned_query": "Better informed marine operations and management: Multidisciplinary efforts in ocean forecasting research for socio-economic benefit",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "A Data-Driven Model for Global Ocean Eddy-Resolving Forecasting",
          "url": "https://arxiv.org/abs/2402.02995",
          "content": "Authors: [Xiang Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+X), [Renzhi Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+R), [Ningzi Hu](https://arxiv.org/search/physics?searchtype=author&query=Hu,+N), [Pinqiang Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+P), [Peng Huo](https://arxiv.org/search/physics?searchtype=author&query=Huo,+P), [Guihua Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+G), [Huizan Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+H), [Senzhang Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+S), [Junxing Zhu](https://arxiv.org/search/physics?searchtype=author&query=Zhu,+J), [Jianbo Xu](https://arxiv.org/search/physics?searchtype=author&query=Xu,+J), [Jun Yin](https://arxiv.org/search/physics?searchtype=author&query=Yin,+J), [Senliang Bao](https://arxiv.org/search/physics?searchtype=author&query=Bao,+S), [Ciqiang Luo](https://arxiv.org/search/physics?searchtype=author&query=Luo,+C), [Ziqing Zu](https://arxiv.org/search/physics?searchtype=author&query=Zu,+Z), [Yi Han](https://arxiv.org/search/physics?searchtype=author&query=Han,+Y), [Weimin Zhang](https://arxiv.org/search/physics?searchtype=author&query=Zhang,+W), [Kaijun Ren](https://arxiv.org/search/physics?searchtype=author&query=Ren,+K), [Kefeng Deng](https://arxiv.org/search/physics?searchtype=author&query=Deng,+K), [Junqiang Song](https://arxiv.org/search/physics?searchtype=author&query=Song,+J)\n\n[View PDF](https://arxiv.org/pdf/2402.02995) [HTML (experimental)](https://arxiv.org/html/2402.02995v4)\n\n> Abstract:The leading operational Global Ocean Forecasting Systems (GOFSs) use physics-driven numerical forecasting models that solve the partial differential equations with expensive computation. Recently, specifically in atmosphere weather forecasting, data-driven models have demonstrated significant potential for speeding up environmental forecasting by orders of magnitude, but there is still no data-driven GOFS that matches the forecasting accuracy of the numerical GOFSs. In this paper, we propose the first data-driven 1/12\u00b0 resolution global ocean eddy-resolving forecasting model named XiHe, which is established from the 25-year France Mercator Ocean International's daily GLORYS12 reanalysis data. XiHe is a hierarchical transformer-based framework coupled with two special designs. One is the land-ocean mask mechanism for focusing exclusively on the global ocean circulation. The other is the ocean-specific block for effectively capturing both local ocean information and global teleconnection. Extensive experiments are conducted under satellite observations, in situ observations, and the IV-TT Class 4 evaluation framework of the world's leading operational GOFSs from January 2019 to December 2020. The results demonstrate that XiHe achieves stronger forecast performance in all testing variables than existing leading operational numerical GOFSs including Mercator Ocean Physical SYstem (PSY4), Global Ice Ocean Prediction System (GIOPS), BLUElinK OceanMAPS (BLK), and Forecast Ocean Assimilation Model (FOAM). Particularly, the accuracy of ocean current forecasting of XiHe out to 60 days is even better than that of PSY4 in just 10 days. Additionally, XiHe is able to forecast the large-scale circulation and the mesoscale eddies. Furthermore, it can make a 10-day forecast in only 0.35 seconds, which accelerates the forecast speed by thousands of times compared to the traditional numerical GOFSs.\n\n## Submission history\n\nFrom: Xiang Wang \\[ [view email](https://arxiv.org/show-email/dc761f38/2402.02995)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2402.02995v1)**\nMon, 5 Feb 2024 13:34:19 UTC (11,304 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2402.02995v2)**\nThu, 8 Feb 2024 14:13:56 UTC (11,300 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2402.02995v3)**\nTue, 20 Aug 2024 12:27:13 UTC (18,708 KB)\n\n**\\[v4\\]**\nTue, 22 Oct 2024 09:29:56 UTC (18,762 KB)",
          "original_query": "Xihe: A data-driven model for global ocean eddy-resolving forecasting",
          "cleaned_query": "Xihe: A data-driven model for global ocean eddy-resolving forecasting",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Neural Reduced-Order Information Filter (N-ROIF) for Online AI Ocean Forecast Updates\nDevelop a hybrid assimilation scheme that treats a transformer forecast model (e.g., WenHai/XiHe/GLONET) as the dynamical core and performs updates in a learned latent space using a ROIF-style information update. Implement observation operators for altimetry, SST, Argo T/S, and surface currents, and compare against HYCOM/NCODA OI/PMOA baselines under OceanBench \u201cModel-to-Observations\u201d scoring.",
        "Physics-Diagnostic Losses from OceanBench to Prevent Eddy Smoothing\nCreate differentiable training losses aligned with OceanBench physical diagnostics (e.g., geostrophic consistency between SSH gradients and surface currents, mixed-layer heat budget residuals, mesoscale spectral slope constraints). Retrain/finetune existing transformer backbones with these losses and quantify improvements in eddy amplitude, propagation speed, and fine-scale variance relative to standard MSE/MAE objectives.",
        "Uncertainty-Aware AI GOFS via Learned Covariances for Ensemble Generation\nTrain an AI model to output both state and flow-dependent uncertainty (diagonal + low-rank covariance) using ideas from ROAF/ROIF error-statistics parameterization. Use these covariances to generate calibrated ensembles (either latent perturbations or diffusion-style sampling) and validate reliability (CRPS, spread\u2013skill) under OceanBench across regions with sparse observations.",
        "Air\u2013Sea Flux Closure Learning: Replacing Bulk Formulae with a Neural Flux Operator\nBuilding on WenHai\u2019s explicit forcing pathway, learn a neural air\u2013sea flux operator that ingests atmospheric fields and ocean surface state to predict momentum/heat/freshwater flux corrections relative to bulk formulae. Test whether learned closure reduces systematic biases in mixed-layer temperature/salinity and improves short-term marine heatwave onset forecasts.",
        "Coupled AI Weather\u2013Ocean Forecasting: Forcing Consistency from Pangu-Weather to WenHai/XiHe\nCreate an end-to-end pipeline where Pangu-Weather provides meteorological forcing ensembles that drive an AI ocean forecaster, with consistency losses enforcing physically plausible air\u2013sea exchange and coherent synoptic\u2013mesoscale interactions. Quantify whether coupled AI improves forecast skill in regions where atmospheric variability strongly controls ocean mixed-layer evolution (storms, monsoon regions).",
        "Multi-Objective Operational Tuning for Socioeconomic Use-Cases (Shipping, Fisheries, Offshore)\nUsing the \u201cmarine operations and management\u201d requirements, define task-weighted objectives (e.g., current shear for routing, SST fronts for fisheries, extreme SSH/currents for offshore safety) and fine-tune AI GOFS models with these metrics while maintaining OceanBench physics diagnostics. Produce a reproducible \u201capplication track\u201d extension to OceanBench that links forecast improvements to operational decision KPIs."
      ],
      "generated_ideas_raw": "1. **Neural Reduced-Order Information Filter (N-ROIF) for Online AI Ocean Forecast Updates**  \nDevelop a hybrid assimilation scheme that treats a transformer forecast model (e.g., WenHai/XiHe/GLONET) as the dynamical core and performs updates in a learned latent space using a ROIF-style information update. Implement observation operators for altimetry, SST, Argo T/S, and surface currents, and compare against HYCOM/NCODA OI/PMOA baselines under OceanBench \u201cModel-to-Observations\u201d scoring.\n\n2. **Ocean-Specific Hierarchical Temporal Aggregation for 10\u201360 Day Eddy Forecast Rollouts**  \nAdapt Pangu-Weather\u2019s hierarchical temporal aggregation to ocean forecasting by training multi-step \u201cstride\u201d predictors (1-day, 5-day, 10-day) that are consistency-regularized to reduce error accumulation and eddy damping. Evaluate whether this improves XiHe-style long-lead current skill (10\u201360 days) while preserving mesoscale SSH variance and eddy kinetic energy spectra.\n\n3. **Physics-Diagnostic Losses from OceanBench to Prevent Eddy Smoothing**  \nCreate differentiable training losses aligned with OceanBench physical diagnostics (e.g., geostrophic consistency between SSH gradients and surface currents, mixed-layer heat budget residuals, mesoscale spectral slope constraints). Retrain/finetune existing transformer backbones with these losses and quantify improvements in eddy amplitude, propagation speed, and fine-scale variance relative to standard MSE/MAE objectives.\n\n4. **Uncertainty-Aware AI GOFS via Learned Covariances for Ensemble Generation**  \nTrain an AI model to output both state and flow-dependent uncertainty (diagonal + low-rank covariance) using ideas from ROAF/ROIF error-statistics parameterization. Use these covariances to generate calibrated ensembles (either latent perturbations or diffusion-style sampling) and validate reliability (CRPS, spread\u2013skill) under OceanBench across regions with sparse observations.\n\n5. **Air\u2013Sea Flux Closure Learning: Replacing Bulk Formulae with a Neural Flux Operator**  \nBuilding on WenHai\u2019s explicit forcing pathway, learn a neural air\u2013sea flux operator that ingests atmospheric fields and ocean surface state to predict momentum/heat/freshwater flux corrections relative to bulk formulae. Test whether learned closure reduces systematic biases in mixed-layer temperature/salinity and improves short-term marine heatwave onset forecasts.\n\n6. **Cross-System Transfer Learning Between GLORYS12 and HYCOM Reanalyses for Robust Generalization**  \nPretrain on GLORYS12 (as in WenHai/XiHe/GLONET) and then domain-adapt to HYCOM reanalysis/analysis products using mask-aware feature alignment and bias-correction heads. Measure out-of-domain robustness by holding out entire basins and assessing whether the model maintains eddy statistics and current skill when initialized from different operational centers.\n\n7. **Coastal Downscaling with Land\u2013Ocean Mask Transformers and Observation-Constrained Super-Resolution**  \nExtend XiHe\u2019s land\u2013ocean mask mechanism to a two-stage global-to-coastal model: a global 1/12\u00b0 forecaster feeding a coastal super-resolution module (1/60\u00b0\u20131/120\u00b0) constrained by tide gauges, HF radar currents, and coastal altimetry. Target actionable coastal products (surface currents, SSH extremes) and evaluate benefits for search-and-rescue-style drift metrics.\n\n8. **Observation Impact and Adaptive Sampling Driven by AI Forecast Sensitivity**  \nCompute forecast sensitivity maps (via gradients or adjoints in latent space) to identify where new observations (e.g., targeted Argo deployments, gliders, additional altimeter tracks) would most reduce 5\u201310 day SSH/current errors. Couple this with a PMOA-like adaptive error-statistics update and validate in OSSEs using GLORYS12 as truth.\n\n9. **Coupled AI Weather\u2013Ocean Forecasting: Forcing Consistency from Pangu-Weather to WenHai/XiHe**  \nCreate an end-to-end pipeline where Pangu-Weather provides meteorological forcing ensembles that drive an AI ocean forecaster, with consistency losses enforcing physically plausible air\u2013sea exchange and coherent synoptic\u2013mesoscale interactions. Quantify whether coupled AI improves forecast skill in regions where atmospheric variability strongly controls ocean mixed-layer evolution (storms, monsoon regions).\n\n10. **Multi-Objective Operational Tuning for Socioeconomic Use-Cases (Shipping, Fisheries, Offshore)**  \nUsing the \u201cmarine operations and management\u201d requirements, define task-weighted objectives (e.g., current shear for routing, SST fronts for fisheries, extreme SSH/currents for offshore safety) and fine-tune AI GOFS models with these metrics while maintaining OceanBench physics diagnostics. Produce a reproducible \u201capplication track\u201d extension to OceanBench that links forecast improvements to operational decision KPIs.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Neural Reduced-Order Information Filter (N-ROIF) for Online AI Ocean Forecast Updates\nDevelop a hybrid assimilation scheme that treats a transformer forecast model (e.g., WenHai/XiHe/GLONET) as the dy",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Physics-Diagnostic Losses from OceanBench to Prevent Eddy Smoothing\nCreate differentiable training losses aligned with OceanBench physical diagnostics (e.g., geostrophic consistency between SSH gradie",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Uncertainty-Aware AI GOFS via Learned Covariances for Ensemble Generation\nTrain an AI model to output both state and flow-dependent uncertainty (diagonal + low-rank covariance) using ideas from ROAF/R",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Air\u2013Sea Flux Closure Learning: Replacing Bulk Formulae with a Neural Flux Operator\nBuilding on WenHai\u2019s explicit forcing pathway, learn a neural air\u2013sea flux operator that ingests atmospheric fields a",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Coupled AI Weather\u2013Ocean Forecasting: Forcing Consistency from Pangu-Weather to WenHai/XiHe\nCreate an end-to-end pipeline where Pangu-Weather provides meteorological forcing ensembles that drive an AI",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Multi-Objective Operational Tuning for Socioeconomic Use-Cases (Shipping, Fisheries, Offshore)\nUsing the \u201cmarine operations and management\u201d requirements, define task-weighted objectives (e.g., current",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 53,
      "paper_title": "Superposition Yields Robust Neural Scaling",
      "contribution": "The paper identifies representation superposition as a central driver of neural scaling laws, providing insights into the conditions under which these scaling laws can be enhanced or may break down.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "quality_content": 3,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 6781,
      "output_tokens": 1034,
      "predecessor_details": [
        {
          "success": true,
          "title": "Relative Scaling Laws for LLMs",
          "url": "https://arxiv.org/html/2510.24626",
          "content": "Relative Scaling Laws for LLMs\n# Relative Scaling Laws for LLMs\nWilliam Held\u03c3,\u03b3,David Hall\u03bcPercy Liang\u03c3Diyi Yang\u03c3\u03c3Stanford University\u03bcOpenAthena\u03b3Georgia Institute of Technologyheld@stanford.eduContact: held@stanford.edu\n###### Abstract\nScaling laws describe how language models improve with additional data, parameters, and compute. While widely used, they are typically measured on aggregate test sets. Aggregate evaluations yield clean trends but average over heterogeneous subpopulations, obscuring performance disparities. We introducerelative scaling laws, which track how performance gaps between test distributions evolve with scale rather than focusing solely on absolute error. Using 255 decoder-only Transformers trained under matched-compute (*IsoFLOP*) budgets from101810^{18}\u2013102010^{20}FLOPs on standard pretraining datasets, we find diverse trajectories: academic domains on MMLU converge toward parity; regional English dialects shift depending on population size; and clusters of AI risk behaviours split, with capability- and influence-related risks increasing during pretraining while adversarial risks do not. These results show that although scaling improves overall performance, it is not a universal equalizer. To support further study, we release all model checkpoints from this work to enable practitioners to measure*relative*alongside traditional scaling laws, in order to better prioritize robustness challenges in light of the bitter lesson111All models trained and used in this work are available on[HuggingFace](https://huggingface.co/collections/marin-community/all-marin-isoflops). The experimental code to train and evaluate these models is available in the[Marin Github](https://github.com/marin-community/marin/blob/main/experiments/isoflop_sweep.py), while analysis and plotting code is available in a separate[project repository](https://github.com/Helw150/relative-scaling-laws). Experimental logs for all experiments are viewable on the[Marin data browser](https://marin.community/data-browser/experiment/?path=gs://marin-us-central1/experiments/isoflop_sweep-fd786f.json).\n![Refer to caption](x1.png)Figure 1:Relative scaling law case studies.Scaling compute has uneven effects (illustrated here with models trained on DCLM> (Li et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib27)> )\nfrom101810^{18}\u2013102010^{20}FLOPs): (left) knowledge domains, (center) English variation, and (right) AI risk behaviours. We propose relative scaling laws as a method to measure which gaps close with scale and which persist or widen.\n## 1Introduction\nNeural scaling laws show that language model error typically decreases as a power law with increases in model size, data, and compute> (Hestness et\u00a0al., [> 2017\n](https://arxiv.org/html/2510.24626v1#bib.bib18)> ; Kaplan et\u00a0al., [> 2020\n](https://arxiv.org/html/2510.24626v1#bib.bib24)> ; Hoffmann et\u00a0al., [> 2022\n](https://arxiv.org/html/2510.24626v1#bib.bib19)> )\n. These trends suggest that \u201cbigger is better\u201d, with only rare cases of inverse scaling> (McKenzie et\u00a0al., [> 2023\n](https://arxiv.org/html/2510.24626v1#bib.bib37)> ; Sharma et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib53)> )\n. However, because these laws average over heterogeneous test distributions, the*rate*of improvement may not be uniform across subdomains> (Magnusson et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib33)> )\n.\nIn practice, gains from scale may favor some areas more than others, much as economic growth can deliver uneven returns across groups and increase inequality> (Piketty, [> 2015\n](https://arxiv.org/html/2510.24626v1#bib.bib44)> )\n.\nWe introduce*relative scaling laws*to study this dimension of scaling effects. Whereas traditional scaling laws describe absolute improvements, relative scaling laws quantify how*performance gaps*between settings evolve with scale. This separates disparities at small scales\n\u2013often shaped by confounding factors such as inherent data entropy \u2014from differences in improvement rate, which more directly capture the response to scale. The relative law is fit directly as a power law by regressing the ratio of treatment to baseline error on compute. This procedure is no harder than fitting absolute laws, but indicates whether gaps persist, narrow, or widen as compute increases. This provides a concrete lens on distributional consequences of scaling model compute\n, with implications for robustness, fairness, and risk.\nTo support such analyses, we train 255 decoder-only Transformers under matched-compute (*IsoFLOP*) budgets from101810^{18}to102010^{20}FLOPs, consisting of 85 models on each of three pretraining datasets. Training under fixed compute ensures that comparisons reflect the tradeoff between model size and data size, avoiding confounds that otherwise complicate scaling-law studies> (Hoffmann et\u00a0al., [> 2022\n](https://arxiv.org/html/2510.24626v1#bib.bib19)> ; Besiroglu et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib2)> )\n. The datasets span three distinct design philosophies\u2014permissively licensed corpora, filtered web data, and hybrid web+synthetic mixtures\u2014so that we can test whether scaling trends generalize across training data sources. We release the full model suite, providing a resource analogous to> Biderman et\u00a0al. (\n[> 2023\n](https://arxiv.org/html/2510.24626v1#bib.bib4)> )\nfor downstream scaling-law evaluation> (Roberts et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.24626v1#bib.bib48)> ; Hu et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib21)> )\n.\nFinally, we demonstrate the scope of relative scaling laws in three case studies. First, we analyze MMLU> (Hendrycks et\u00a0al., [> 2021\n](https://arxiv.org/html/2510.24626v1#bib.bib17)> )\nsub-domains to measure how knowledge scales across academic disciplines. Second, we evaluate robustness to English variation, testing generalization across regional English using the International Corpus of English (ICE)> (Greenbaum, [> 1996\n](https://arxiv.org/html/2510.24626v1#bib.bib12)> )\n. Third, we assess how relative risks emerge during pretraining using Anthropic\u2019s AI risk evaluations from> Perez et\u00a0al. (\n[> 2023\n](https://arxiv.org/html/2510.24626v1#bib.bib43)> )\n. Across all these settings, we fit both traditional and relative scaling laws.\nContributions.Our contributions combine conceptual, resource, and empirical components:\n1. 1.\nRelative scaling framework.We formalize*relative scaling laws*, which separate initial disparities from differences in improvement rate. Formulated as a power law, relative scaling provides a clear diagnostic of which distributions benefit the most from scaling.\n2. 2.\nOpen-source scaling suite.We train and release 255 decoder-only Transformers under IsoFLOP budgets from101810^{18}\u2013102010^{20}FLOPs across three corpora\u2014CommonPile> (Kandpal et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.24626v1#bib.bib23)> )\n,DCLM Baseline> (Li et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib27)> )\n, andNemotron-CC> (Su et\u00a0al., [> 2025\n](https://arxiv.org/html/2510.24626v1#bib.bib55)> )\n. The suite enables reproducible study of both traditional and relative scaling laws.\n3. 3.\nEmpirical case studies.We apply relative scaling laws to three domains: academic knowledge (Massively Multitask Language Understanding benchmark; MMLU), linguistic variation (International Corpus of English; ICE), and AI risk (Anthropic Advanced AI Risk). Together, these studies show a range of relative scaling effects highlighting the non-uniformity of scale\u2019s impacts on distributional robustness.\n## 2Relative Scaling Laws\nRelative scaling laws follow directly from the assumptions of classical scaling laws. Absolute errorEEis assumed to decrease as a power law in scaleFF(e.g., FLOPs, tokens, or parameters),\n|E\u200b(F)=\u03b1\u200bF\u2212\u03b2,E(F)=\\\\alpha F^{-\\\\beta},||\nwith\u03b1&gt;0\\\\alpha&gt;&gt;0as the initial error level and\u03b2\u22650\\\\beta\\\\geq 0as the rate of improvement with scale> (Kaplan et\u00a0al., [> 2020\n](https://arxiv.org/html/2510.24626v1#bib.bib24)> )\n. These constants are empirically fit based on sample populations of training runs.\nIn order to relativize performance gains, we compare two conditions: a*baseline*(the reference, here the most favored under current practice) and a*treatment*of interest. Their relative errorGGis\n|G\u200b(F)=Etreatment\u200b(F)Ebaseline\u200b(F)=\u03b3\u200bF\u0394\u200b\u03b2G(F)=\\\\frac{E\\_{\\\\text{treatment}}(F)}{E\\_{\\\\text{baseline}}(F)}=\\\\gamma F^{\\\\Delta\\\\beta}||\nwhere\u03b3=\u03b1treatment/\u03b1baseline\\\\gamma=\\\\alpha\\_{\\\\text{treatment}}/\\\\alpha\\_{\\\\text{baseline}}captures the initial disparity and\u0394\u200b\u03b2=\u03b2baseline\u2212\u03b2treatment\\\\Delta\\\\beta=\\\\beta\\_{\\\\text{baseline}}-\\\\beta\\_{\\\\text{treatment}}the difference in improvement rates. If\u0394\u200b\u03b2&lt;0\\\\Delta\\\\beta&lt;&lt;0, the treatment improves faster and the gap narrows; if\u0394\u200b\u03b2&gt;0\\\\Delta\\\\beta&gt;&gt;0, it improves more slowly and the gap widens; if\u0394\u200b\u03b2=0\\\\Delta\\\\beta=0, the gap remains constant222In this work, we only interpret the slope if the sign is significant atP&lt;0.05P&lt;0.05by a bootstrap significance test. We recommend this as a best practice for interpreting\u0394\u200b\u03b2\\\\Delta\\\\beta..\nThis form parallels the subgroup laws of> Rolf et\u00a0al. (\n[> 2021\n](https://arxiv.org/html/2510.24626v1#bib.bib49)> )\n, who model subgroup loss as a mixture of power-law terms for in-group and total data. Our formulation is looser \u2014we do not require subgroup allocations \u2014but the sign of\u0394\u200b\u03b2\\\\Delta\\\\betastill forecasts whether gaps shrink or persist. While relative loss can correspond to small absolute differences at low loss, small absolute loss gaps can lead to large differences in downstream utility for large scale models> (Wei et\u00a0al., [> 2022\n](https://arxiv.org/html/2510.24626v1#bib.bib59)> ; Du et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib8)> )\nwhich motivates this scale-invariant metric rather than absolute disparity> (Yeh et\u00a0al., [> 2024\n](https://arxiv.org/html/2510.24626v1#bib.bib65)> )\n333Beyond test-distribution disparities, relative scaling can be used to compare mod",
          "original_query": "Scaling laws for neural language models",
          "cleaned_query": "Scaling laws for neural language models",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "[PDF] Scaling Laws from the Data Manifold Dimension",
          "url": "https://jmlr.csail.mit.edu/papers/volume23/20-1111/20-1111.pdf",
          "content": "Journal of Machine Learning Research 23 (2022) 1-34 Submitted 10/20; Revised 4/21; Published 2/22\nScaling Laws from the Data Manifold Dimension\nUtkarsh Sharma usharma7@jhu.edu\nDepartment of Physics and Astronomy\nJohns Hopkins University\nBaltimore, MD 21218, USA\nJared Kaplan jaredk@jhu.edu\nDepartment of Physics and Astronomy\nJohns Hopkins University\nBaltimore, MD 21218, USA\nEditor: Daniel Lee\nAbstract\nWhen data is plentiful, the test loss achieved by well-trained neural networks scales as\na power-law L \u221d N \u2212\u03b1 in the number of network parameters N. This empirical scaling\nlaw holds for a wide variety of data modalities, and may persist over many orders of\nmagnitude. The scaling law can be explained if neural models are effectively just performing\nregression on a data manifold of intrinsic dimension d. This simple theory predicts that the\nscaling exponents \u03b1 \u2248 4/d for cross-entropy and mean-squared error losses. We confirm\nthe theory by independently measuring the intrinsic dimension and the scaling exponents\nin a teacher/student framework, where we can study a variety of d and \u03b1 by dialing the\nproperties of random teacher networks. We also test the theory with CNN image classifiers\non several datasets and with GPT-type language models.\nKeywords: scaling laws, data manifold, model capacity, under-parameterized, intrinsic\ndimension\n1. Introduction\nNeural Network based Machine Learning has made enormous progress in a wide variety of\ndomains. Scale has been a key ingredient in this success: large amounts of computation,\nlarge datasets, and large models with millions or billions of parameters.\nNot only is scale beneficial to performance, but the benefits from scale can be predicted\nprecisely. Recent works Hestness et al. (2017, 2019); Rosenfeld et al. (2019); Kaplan et al.\n(2020) studying a variety of data modalities and model architectures all find the same\nscaling relation in the underfitting regime. In particular, the dependence of the test loss\non the number of model parameters N has the following properties, and each suggests a\ncorresponding question:\n\u2022 As the number of model parameters N is increased, the cross-entropy test loss of\nwell-trained and well-tuned models scales with N as a power-law\nL(N) \u221d\n1\nN\u03b1\n(1.1)\n\u00a92022 Utkarsh Sharma and Jared Kaplan.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided\nat http://jmlr.org/papers/v23/20-1111.html.\nSharma and Kaplan\n10\n1 10\n2\n4/\n10\n1\n10\n2\nIntrinsic Dimension\nIntrinsic Dimension vs 4/\nID = 4/ for reference\nTeacher Student\nCIFAR10\nMNIST\nFashion MNIST\nSVHN\nGPT\nFigure 1: This figure shows the relationship between the measured intrinsic dimension (ID)\nof the data manifold and 4\n\u03b1\n, where \u03b1 is the model size scaling exponent. We\ninclude data from fully-connected teacher/student experiments, simple CNNs,\nand GPT-type Radford et al. (2018, 2019) language models (represented as a\nlower-bound due to large uncertainties with large IDs).\nwith observed values such as \u03b1 \u2248 0.076 for language modeling Kaplan et al. (2020),\nand much larger \u03b1 \u2248 0.5 observed for image classification Rosenfeld et al. (2019).\nWhy do we encounter this simple functional form, and what determines the value of\nthe exponent \u03b1?\n\u2022 Scaling holds very accurately across a wide range of N, sometimes spanning many\norders of magnitude Hestness et al. (2017, 2019); Kaplan et al. (2020). Why does\nscaling persist over a large range of model sizes, and what determines the Nmax where\nit eventually breaks down?\n\u2022 Empirically, the scaling exponent \u03b1 may not depend greatly on model architecture.\nFor example, LSTMs and Transformers scale similarly over a large range of N Kaplan\net al. (2020), with losses differing only by an overall, N-independent factor. Why\nwould scaling exponents be roughly independent of model architecture?\nWe will argue that a simple conjectural theory can address these questions while making a\nnumber of testable predictions.\n1.1 Main Ideas and Organization of the Paper\nThe key idea is that neural models map the data to a manifold with intrinsic dimension\nd, and then use added capacity to carve up this manifold into ever smaller sub-regions. If\n2\nScaling Laws from the Data Manifold Dimension\nthe underlying data varies continuously on the manifold, then the size of these sub-regions\n(rather than their number) determines the model\u2019s test loss. To shrink the size of the sub\u0002regions by a factor of 2 requires increasing the parameter count by a factor of 2d\n, and so\nthe inverse of the scaling exponent 1/\u03b1 will be proportional to the intrinsic dimension d of\nthe data manifold. We develop these ideas in detail in section 2.\nThe scaling exponent \u03b1 can be measured by training a succession of models of varying\nsize. To verify the theory on real-world datasets, we need an independent measurement of\nthe intrinsic dimension. In subsection 2.3 of the same section, we measure the intrinsic di\u0002mension d within the final hidden layer1 activations of trained networks, using the distances\namong nearest neighbor activation vectors Levina and Bickel (2005); Facco et al. (2017).\nWe also explain why simpler methods like principal component analysis (PCA) don\u2019t suffice.\nIn section 3, we test the theory in a student/teacher framework, which makes it possible\nto scan over a large range of \u03b1 and d and test more idiosyncratic features of the theory (see\nfigure 2). We also perform tests using CNNs for image classification, and by measuring the\nintrinsic dimension of GPT-type models Radford et al. (2018, 2019), where scaling exponent\nhave already been documented Kaplan et al. (2020).\nWe follow up with section 4 on related work and a discussion in section 5.\n2. A Simple Theory for Scaling in the Underfitting Regime\nIn this section we explain our theory, beginning with a toy model to discuss properties of\nregression in section 2.1. Then in section 2.2 we argue2that the toy model can be applied to\nrealistic neural networks with only a few small modifications. In section 2.3 we explain how\nwe measure the dimension of the data manifold, a necessary step in validating the theory.\n2.1 A Toy Model\nConsider one of the simplest scenarios for multidimensional regression. We are given a\nLipschitz function f : [0, 1]d \u2192 R, and we would like to approximate it as a piecewise\nconstant function c(x), by cutting [0, 1]dinto smaller hypercubes. If these hypercubes have\na side length s, then we will have\nN = s\n\u2212d\n(2.1)\ncubes, and so our approximation will depend on the N constant values c(x) takes within\neach hypercube. If the loss is mean-squared error (MSE), then it will be bounded by\nL =\nZ 1\n0\nd\ndx|f(x) \u2212 c(x)|2 . \u03bb2\n\ns\n2\nd\n\u0001\n(2.2)\nwhere \u03bb is the Lipschitz bound |f(x + y) \u2212 f(x)| < \u03bb|y|, and we have ignored overall\nnumerical factors. Translating the s-dependence into N, this means that L(N) .\n1\nN2/d up\nto a constant factor.\n1. It was shown in Ansuini et al. (2019) that the final hidden layer activations have the smallest intrinsic\ndimension in image classifiers. Our findings are largely consistent with this.\n2. one might say conjecture; for a more sophisticated perspective in a simpler context see Bickel et al.\n(2007)\n3\nSharma and Kaplan\nx1\nxk \u00b7\u00b7\u00b7\nTeacher with d \u21e1 k\nX1\nX2\nM = X1 \u21e5 X2\nFigure 2: Left: This shows the setup of a teacher network, emphasizing how we can control\nthe data manifold dimension via the number of input features k. Right: When\nthe data manifold is a product and the teacher T(X) = T1(X1) + T2(X2), then\nstudent networks can learn T by combining sub-networks and behaving, in effect,\nlike an ensemble. Then we predict 4/\u03b1 \u2248 dmax, the maximum d among the\ncomponents.\nIf the model is piecewise linear instead of piecewise constant and f(x) is smooth with\nbounded derivatives, then the deviation |f(x) \u2212 c(x)| \u221d s\n2\n, and so the L\n2\nloss will scale3 as\ns\n4\n. We would predict\nL(N) \u221d\n1\nN4/d (2.4)\nThis will be important later, since networks with ReLU activations produce piecewise linear\nfunctions.\nFinally, consider the case where fi(x) encode a smooth probability distribution over\ni = 1, \u00b7 \u00b7 \u00b7 , k possibilities, and we replace the MSE loss with the KL divergence. If the ci(x)\nare a piecewise linear model for the logits, then we also find that L \u221d s\n4\n. So the KL and\nMSE losses will scale with the same exponent in N at a given value of d. We demonstrate\nthis in appendix A.5; it is a simple consequence of the fact that the expansion of DKL(p||q)\nin (q \u2212 p) begins at second order. Note that if we use a cross-entropy instead of the KL\ndivergence, the loss will scale in the same way towards a fixed constant value, the entropy\nof the true distribution.\n3. A straightforward generalization suggests that if c(x) is composed of piece-wise k-degree polynomials,\nand we use a loss |f \u2212 c|\np\n, then\nL(s) \u221d s\n(k+1)p\n(2.3)\nin the infinite data limit. But if p is large then c(x) within each hypercube will utilize many parameters.\nWe test the p-dependence of this prediction in figure 5.\n4\nScaling Laws from the Data Manifold Dimension\n2.2 A Conjectural Theory for Neural Networks\nNeural Networks perform well on data with thousands or even millions of dimensions. It\nis widely believed that this is possible because neural networks map the data into a much\nlower-dimensional \u2018data manifold\u2019, preserving and focusing on the features that are relevant\nfor the task.\nWe emphasize that the data manifold is a feature of both the dataset and the task or\nloss function that has been optimized. Classifiers need only attend to features relevant\nfor classification. Similarly, in the case of autoregressive models the data manifold would\nconsist only of the features necessary to predict the next token in a sequence. So the data\nmanifold for such a model (as we are defining it) may have many fewer dimensions than\nthe space of full sequences, such as complete images or text samples. Properties of the data\nmanifold may also depend on the model that is learning it, such as its architecture and\nactivation functions.\nWe can explain ",
          "original_query": "Scaling laws from the data manifold dimension",
          "cleaned_query": "Scaling laws from the data manifold dimension",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Toy Models of Superposition - Transformer Circuits Thread",
          "url": "https://transformer-circuits.pub/2022/toy_model/index.html",
          "content": "Toy Models of Superposition\n[Transformer Circuits Thread](https://transformer-circuits.pub/)\n# Toy Models of Superposition\n### Authors\n[Nelson Elhage\u2217](https://nelhage.com/),Tristan Hume\u2217,Catherine Olsson\u2217,Nicholas Schiefer\u2217,[Tom Henighan](https://tomhenighan.com),Shauna Kravec,[Zac Hatfield-Dodds](https://zhd.dev),Robert Lasenby,Dawn Drain,Carol Chen,Roger Grosse,Sam McCandlish,Jared Kaplan,Dario Amodei,[Martin Wattenberg](https://www.bewitched.com/)\u2217,[Christopher Olah\u2021](https://colah.github.io/)\n### Affiliations\n[Anthropic,](https://www.anthropic.com/)[Harvard](https://www.harvard.edu/)\n### Published\nSept 14, 2022\n\\* Core Research Contributor;\u2021 Correspondence to[colah@anthropic.com](colah@anthropic.com);[Author contributions statement below](#).\nIt would be very convenient if the individual neurons of artificial neural networks corresponded to cleanly interpretable features of the input. For example, in an \u201cideal\u201d ImageNet classifier, each neuron would fire only in the presence of a specific visual feature, such as the color red, a left-facing curve, or a dog snout. Empirically, in models we have studied, some of the neurons do cleanly map to features. But it isn't always the case that features correspond so cleanly to neurons, especially in large language models where it actually seems rare for neurons to correspond to clean features. This brings up many questions. Why is it that neurons sometimes align with features and sometimes don't? Why do some models and tasks have many of these clean neurons, while they're vanishingly rare in others?\nIn this paper, we use toy models \u2014small ReLU networks trained on synthetic data with sparse input features \u2014to investigate how and when models represent more features than they have dimensions. We call this phenomenonsuperposition. When features are sparse, superposition allows compression beyond what a linear model would do, at the cost of \"interference\" that requires nonlinear filtering.\nConsider a toy model where we train an embedding of five features of varying importanceWhere \u201cimportance\u201d is a scalar multiplier on mean squared error loss.in two dimensions, add a ReLU afterwards for filtering, and vary the sparsity of the features. With dense features, the model learns to represent an orthogonal basis of the most important two features (similar to what Principal Component Analysis might give us), and the other three features are not represented. But if we make the features sparse, this changes:\nThis figure and a few others can be reproduced using the[toy model framework Colab notebook](https://colab.research.google.com/github/anthropics/toy-models-of-superposition/blob/main/toy_models.ipynb)in our[Github repo](https://github.com/anthropics/toy-models-of-superposition)\nNot only can models store additional features in superposition by tolerating some interference, but we'll show that, at least in certain limited cases,models can perform computation while in superposition. (In particular, we'll show that models can put simple circuits computing the absolute value function in superposition.) This leads us to hypothesize thattheneural networks we observe in practice are in some sense noisily simulating larger, highly sparse networks.\u00a0In other words, it's possible that models we train can be thought of as doing \u201cthe same thing as\u201d an imagined much-larger model, representing the exact same features but with no interference.\nFeature superposition isn't a novel idea. A number of previous interpretability papers have considered it, and it's very closely related to the long-studied topic of compressed sensing in mathematics, as well as the ideas of distributed, dense, and population codes in neuroscienceand deep learning. What, then, is the contribution of this paper?\nFor interpretability researchers, our main contribution is providing a direct demonstration that superposition occurs in artificial neural networks given a relatively natural setup, suggesting this may also occur in practice. That is, we show a case where interpreting neural networks as having sparse structure in superposition isn't just a useful post-hoc interpretation, but actually the \"ground truth\" of a model. We offer a theory of when and why this occurs, revealing a[phase diagram](#phase-change)for superposition. This[explains](#privileged-basis)why neurons are sometimes \"monosemantic\" responding to a single feature, and sometimes \"polysemantic\"responding to many unrelated features. We also discover that, at least in our toy model, superposition exhibits[complex geometric structure](#geometry).\nBut our results may also be of broader interest. We find preliminary evidence that superposition may be linked to adversarial examples and grokking, and might also suggest a theory for the performance of mixture\u00a0of experts\u00a0models. More broadly, the toy model we investigate has unexpectedly rich structure, exhibiting[phase changes](#phase-change), a[geometric structure](#geometry)based on uniform polytopes,[\"energy level\"-like jumps](#learning-jumps)during training, and a[phenomenon](#geometry-uniform)which is qualitatively similar to the fractional quantum Hall effect\u00a0in physics, among other striking phenomena. We originally investigated the subject to gain understanding of cleanly-interpretable neurons in larger models, but we've found these toy models to be surprisingly interesting in their own right.\n#### [Key Results\u00a0From Our Toy Models](#key-results)\nIn our toy models, we are able to demonstrate that:\n* Superposition is a real, observed phenomenon.\n* Both monosemantic and polysemantic neurons can form.\n* At least some kinds of computation can be performed in superposition.\n* Whether features are stored in superposition isgoverned by a phase change.\n* Superposition organizes features into geometric structuressuch as digons, triangles, pentagons, and tetrahedrons.\nOur toy models are simple ReLU networks, so it seems fair to say that neural networks exhibit these properties in at least some regimes, but it's very unclear what to generalize to real networks.\n## [Definitions and Motivation: Features, Directions, and Superposition](#motivation)\nIn our work, we\u00a0often think of neural networks\u00a0as havingfeatures of the inputrepresented asdirectionsin activation space. This isn't a trivial claim. It isn't obvious what kind of structure we should expect neural network representations to have. When we say something like \"word embeddings have a gender direction\" or \"vision models have curve detector neurons\", one is implicitly making strong claims about the structure of network representations.\nDespite this, we believe this kind of \"linear representation hypothesis\" is supported both by significant empirical findings and theoretical arguments. One might think of this as two separate properties, which we'll explore in more detail shortly:\n* Decomposability:Network representations can be\u00a0described in terms of\u00a0independently understandable features.\n* Linearity:Features are represented by direction.\nIf we hope to reverse engineer neural networks, weneeda property like decomposability. Decomposability is what[allows us to reason about the model](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)without fitting the whole thing in our heads! But it's not enough for things to be decomposable: we need to be able to access the decomposition somehow. In order to do this, we need toidentifythe individual features within a representation. In a linear representation, this corresponds to determining which directions in activation space correspond to which independent features of the input.\nSometimes,\u00a0identifying feature directions\u00a0is very easy because features seem to correspond to neurons.\u00a0For example, many neurons in the early layers of InceptionV1 clearly correspond to features (e.g. curve detector neurons). Why is it that we sometimes get this extremely helpful property, but in other cases don't? We hypothesize that there are really two countervailing forces driving this:\n* Privileged Basis:Only some representations have aprivileged basiswhich encourages features to align with basis directions (i.e. to correspond to neurons).\n* Superposition:Linear representations can represent more features than dimensions, using a strategy we callsuperposition. This can be seen as neural networkssimulating larger networks. This pushes featuresawayfrom corresponding to neurons.\nSuperposition has been hypothesized in previous work, and in some cases, assuming something like superposition has been shown to help find interpretable structure. However, we're not aware of feature superposition having been unambiguously demonstrated to occur in neural networks before (demonstrates a closely related phenomenon of model superposition).\u00a0The goal of this paper is to change that, demonstrating superposition and exploring how it interacts with privileged bases. If superposition occurs in networks, it deeply influences what approaches to interpretability research make sense, so unambiguous demonstration seems important.\nThe goal of this section will be to motivate these ideas and unpack them in detail.\nIt's worth noting that many of the ideas in this section have close connections to ideas in other lines of interpretability research (especially disentanglement), neuroscience (distributed representations, population codes, etc), compressed sensing, and many other lines of work. This section will focus on articulating our perspective on the problem. We'll discuss these other lines of work in detail in[Related Work](#related-work).\n### [Empirical Phenomena](#motivation-empirical)\nWhen we talk about \"features\" and how they're represented, this is ultimately theory building around several observed empirical phenomena. Before describing how we conceptualize those results, we'll simply describe some of the major results motivating our thinking:\n* Word Embeddings- A famous result byMikolov et al.found that word embeddings appear\u00a0to have directions which cor",
          "original_query": "Toy models of superposition",
          "cleaned_query": "Toy models of superposition",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Intrinsic-Dimension-Aware Relative Scaling Laws\n- Measure intrinsic manifold dimension (e.g., Levina\u2013Bickel estimator on hidden activations) separately for each evaluation subpopulation (MMLU domains, English dialect regions, risk-behavior clusters) across the IsoFLOP checkpoint suite. Fit a predictive model linking each group\u2019s relative scaling exponent to its estimated intrinsic dimension, testing whether widening/narrowing gaps can be forecast from geometry rather than only empirical trend lines.",
        "Superposition as a Mechanism for Persistent Performance Gaps\n- Quantify superposition in specific layers (e.g., via feature sparsity/overlap metrics or dictionary learning on activations) for groups that show diverging relative scaling. Test the hypothesis that groups whose representations remain highly polysemantic under scale exhibit slower relative improvement, and validate by targeted interventions (e.g., sparsity regularization or architectural bottlenecks) to reduce superposition and observe whether the relative gap shrinks.",
        "Phase-Transition Mapping: When Do Subpopulations Become \u201cMonosemantic\u201d?\n- Using the released checkpoints, track whether subpopulation-specific features transition from polysemantic to more monosemantic organization at particular compute scales (a \u201cphase change\u201d analogous to toy superposition results). Identify compute thresholds (or model/data tradeoffs at fixed FLOPs) where such transitions occur, and connect these thresholds to changes in relative scaling trajectories (gap closing vs widening).",
        "Compute-Optimal Data Reweighting to Flatten Relative Scaling Disparities\n- Design an IsoFLOP-constrained pretraining schedule that dynamically reweights data from underperforming subpopulations (dialects or domains) based on their current relative error ratio. Evaluate whether such adaptive curricula can change the *relative* scaling exponent (not just absolute loss) while keeping total compute fixed, yielding an actionable recipe for \u201cgap-aware\u201d scaling.",
        "Manifold-Dimension Shaping via Synthetic Data to Alter Relative Exponents\n- Create controlled synthetic augmentations (e.g., paraphrases, dialect transformations, domain-style rewrites) intended to reduce intrinsic dimension for specific subpopulations or align manifolds across groups. Test whether these augmentations measurably change the estimated intrinsic dimension in hidden activations and correspondingly improve that group\u2019s relative scaling law (steeper improvement rate under scale).",
        "Risk-Behavior Geometry: Linking Capability/Influence vs Adversarial Risks to Representation Structure\n- For the AI risk behavior clusters that split in relative scaling trajectories, estimate (i) intrinsic dimension and (ii) superposition degree of the internal representations associated with each behavior class. Determine whether rising capability/influence risks correlate with increased representational capacity allocated in superposition, while adversarial risks remain \u201coff-manifold\u201d and thus do not scale similarly\u2014offering a mechanistic account of the observed divergence.",
        "Relative Scaling Under Architectural Sparsity: MoE vs Dense as Superposition Control\n- Train matched-compute (IsoFLOP) mixtures-of-experts and dense baselines, then re-run relative scaling analyses across the same subpopulations. Test whether conditional computation reduces harmful interference (superposition) for minority subpopulations, improving their relative scaling exponents, and identify which routing/regularization choices best equalize gap trajectories.",
        "Predicting Which Gaps Will Widen Using Early-Training Proxies\n- Build a forecasting model that uses small-compute checkpoints to predict long-run relative scaling behavior for each subpopulation, using features like early intrinsic-dimension estimates, early superposition metrics, and early relative error ratios. This would enable practical \u201cearly warning\u201d signals for which fairness/robustness gaps are likely to persist or worsen as organizations scale training runs.",
        "Intervention Study: Targeted De-superposition in Specific Layers to Improve Dialect Robustness\n- Identify layers where dialect-specific performance gaps correlate most strongly with polysemantic features (e.g., via probing or attribution to internal directions). Apply targeted regularizers or orthogonality constraints in those layers during continued pretraining (or lightweight finetuning) and measure whether dialect relative scaling curves shift toward parity without degrading aggregate scaling trends.",
        "Unified Theory: Relative Scaling Exponents from (Dimension \u00d7 Sparsity)\n- Propose and empirically test a combined model where subpopulation scaling behavior depends on both intrinsic manifold dimension (from the manifold-dimension theory) and effective feature sparsity/interference (from superposition toy models). Fit a simple functional form (e.g., \u03b1_group \u2248 f(d_group, sparsity_group)) across many tasks/subpopulations and validate out-of-sample on new distributions, yielding a concrete, testable account of why \u201cscale is not a universal equalizer.\u201d"
      ],
      "generated_ideas_raw": "1. **Intrinsic-Dimension-Aware Relative Scaling Laws**\n   - Measure intrinsic manifold dimension (e.g., Levina\u2013Bickel estimator on hidden activations) separately for each evaluation subpopulation (MMLU domains, English dialect regions, risk-behavior clusters) across the IsoFLOP checkpoint suite. Fit a predictive model linking each group\u2019s relative scaling exponent to its estimated intrinsic dimension, testing whether widening/narrowing gaps can be forecast from geometry rather than only empirical trend lines.\n\n2. **Superposition as a Mechanism for Persistent Performance Gaps**\n   - Quantify superposition in specific layers (e.g., via feature sparsity/overlap metrics or dictionary learning on activations) for groups that show diverging relative scaling. Test the hypothesis that groups whose representations remain highly polysemantic under scale exhibit slower relative improvement, and validate by targeted interventions (e.g., sparsity regularization or architectural bottlenecks) to reduce superposition and observe whether the relative gap shrinks.\n\n3. **Phase-Transition Mapping: When Do Subpopulations Become \u201cMonosemantic\u201d?**\n   - Using the released checkpoints, track whether subpopulation-specific features transition from polysemantic to more monosemantic organization at particular compute scales (a \u201cphase change\u201d analogous to toy superposition results). Identify compute thresholds (or model/data tradeoffs at fixed FLOPs) where such transitions occur, and connect these thresholds to changes in relative scaling trajectories (gap closing vs widening).\n\n4. **Compute-Optimal Data Reweighting to Flatten Relative Scaling Disparities**\n   - Design an IsoFLOP-constrained pretraining schedule that dynamically reweights data from underperforming subpopulations (dialects or domains) based on their current relative error ratio. Evaluate whether such adaptive curricula can change the *relative* scaling exponent (not just absolute loss) while keeping total compute fixed, yielding an actionable recipe for \u201cgap-aware\u201d scaling.\n\n5. **Manifold-Dimension Shaping via Synthetic Data to Alter Relative Exponents**\n   - Create controlled synthetic augmentations (e.g., paraphrases, dialect transformations, domain-style rewrites) intended to reduce intrinsic dimension for specific subpopulations or align manifolds across groups. Test whether these augmentations measurably change the estimated intrinsic dimension in hidden activations and correspondingly improve that group\u2019s relative scaling law (steeper improvement rate under scale).\n\n6. **Risk-Behavior Geometry: Linking Capability/Influence vs Adversarial Risks to Representation Structure**\n   - For the AI risk behavior clusters that split in relative scaling trajectories, estimate (i) intrinsic dimension and (ii) superposition degree of the internal representations associated with each behavior class. Determine whether rising capability/influence risks correlate with increased representational capacity allocated in superposition, while adversarial risks remain \u201coff-manifold\u201d and thus do not scale similarly\u2014offering a mechanistic account of the observed divergence.\n\n7. **Relative Scaling Under Architectural Sparsity: MoE vs Dense as Superposition Control**\n   - Train matched-compute (IsoFLOP) mixtures-of-experts and dense baselines, then re-run relative scaling analyses across the same subpopulations. Test whether conditional computation reduces harmful interference (superposition) for minority subpopulations, improving their relative scaling exponents, and identify which routing/regularization choices best equalize gap trajectories.\n\n8. **Predicting Which Gaps Will Widen Using Early-Training Proxies**\n   - Build a forecasting model that uses small-compute checkpoints to predict long-run relative scaling behavior for each subpopulation, using features like early intrinsic-dimension estimates, early superposition metrics, and early relative error ratios. This would enable practical \u201cearly warning\u201d signals for which fairness/robustness gaps are likely to persist or worsen as organizations scale training runs.\n\n9. **Intervention Study: Targeted De-superposition in Specific Layers to Improve Dialect Robustness**\n   - Identify layers where dialect-specific performance gaps correlate most strongly with polysemantic features (e.g., via probing or attribution to internal directions). Apply targeted regularizers or orthogonality constraints in those layers during continued pretraining (or lightweight finetuning) and measure whether dialect relative scaling curves shift toward parity without degrading aggregate scaling trends.\n\n10. **Unified Theory: Relative Scaling Exponents from (Dimension \u00d7 Sparsity)**\n   - Propose and empirically test a combined model where subpopulation scaling behavior depends on both intrinsic manifold dimension (from the manifold-dimension theory) and effective feature sparsity/interference (from superposition toy models). Fit a simple functional form (e.g., \u03b1_group \u2248 f(d_group, sparsity_group)) across many tasks/subpopulations and validate out-of-sample on new distributions, yielding a concrete, testable account of why \u201cscale is not a universal equalizer.\u201d",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Intrinsic-Dimension-Aware Relative Scaling Laws\n- Measure intrinsic manifold dimension (e.g., Levina\u2013Bickel estimator on hidden activations) separately for each evaluation subpopulation (MMLU domains,",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Superposition as a Mechanism for Persistent Performance Gaps\n- Quantify superposition in specific layers (e.g., via feature sparsity/overlap metrics or dictionary learning on activations) for groups t",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Phase-Transition Mapping: When Do Subpopulations Become \u201cMonosemantic\u201d?\n- Using the released checkpoints, track whether subpopulation-specific features transition from polysemantic to more monosemanti",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Compute-Optimal Data Reweighting to Flatten Relative Scaling Disparities\n- Design an IsoFLOP-constrained pretraining schedule that dynamically reweights data from underperforming subpopulations (diale",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Manifold-Dimension Shaping via Synthetic Data to Alter Relative Exponents\n- Create controlled synthetic augmentations (e.g., paraphrases, dialect transformations, domain-style rewrites) intended to re",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Risk-Behavior Geometry: Linking Capability/Influence vs Adversarial Risks to Representation Structure\n- For the AI risk behavior clusters that split in relative scaling trajectories, estimate (i) intr",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Relative Scaling Under Architectural Sparsity: MoE vs Dense as Superposition Control\n- Train matched-compute (IsoFLOP) mixtures-of-experts and dense baselines, then re-run relative scaling analyses ac",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Predicting Which Gaps Will Widen Using Early-Training Proxies\n- Build a forecasting model that uses small-compute checkpoints to predict long-run relative scaling behavior for each subpopulation, usin",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Intervention Study: Targeted De-superposition in Specific Layers to Improve Dialect Robustness\n- Identify layers where dialect-specific performance gaps correlate most strongly with polysemantic featu",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Unified Theory: Relative Scaling Exponents from (Dimension \u00d7 Sparsity)\n- Propose and empirically test a combined model where subpopulation scaling behavior depends on both intrinsic manifold dimension",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 54,
      "paper_title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
      "contribution": "The paper establishes that CNNs predominantly rely on local shape features rather than being inherently biased towards texture, offering a new evaluation of feature reliance through controlled suppression.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "quality_content": 3,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 6720,
      "output_tokens": 1001,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE",
          "url": "https://openreview.net/pdf?id=Bygh9j09KX",
          "content": "Published as a conference paper at ICLR 2019\nIMAGENET-TRAINED CNNS ARE BIASED TOWARDS\nTEXTURE; INCREASING SHAPE BIAS IMPROVES\nACCURACY AND ROBUSTNESS\nRobert Geirhos\nUniversity of Tubingen & IMPRS-IS \u00a8\nrobert.geirhos@bethgelab.org\nPatricia Rubisch\nUniversity of Tubingen & U. of Edinburgh \u00a8\np.rubisch@sms.ed.ac.uk\nClaudio Michaelis\nUniversity of Tubingen & IMPRS-IS \u00a8\nclaudio.michaelis@bethgelab.org\nMatthias Bethge\u2217\nUniversity of Tubingen \u00a8\nmatthias.bethge@bethgelab.org\nFelix A. Wichmann\u2217\nUniversity of Tubingen \u00a8\nfelix.wichmann@uni-tuebingen.de\nWieland Brendel\u2217\nUniversity of Tubingen \u00a8\nwieland.brendel@bethgelab.org\nABSTRACT\nConvolutional Neural Networks (CNNs) are commonly thought to recognise ob\u0002jects by learning increasingly complex representations of object shapes. Some\nrecent studies suggest a more important role of image textures. We here put these\nconflicting hypotheses to a quantitative test by evaluating CNNs and human ob\u0002servers on images with a texture-shape cue conflict. We show that ImageNet\u0002trained CNNs are strongly biased towards recognising textures rather than shapes,\nwhich is in stark contrast to human behavioural evidence and reveals fundamen\u0002tally different classification strategies. We then demonstrate that the same standard\narchitecture (ResNet-50) that learns a texture-based representation on ImageNet\nis able to learn a shape-based representation instead when trained on \u2018Stylized\u0002ImageNet\u2019, a stylized version of ImageNet. This provides a much better fit for\nhuman behavioural performance in our well-controlled psychophysical lab setting\n(nine experiments totalling 48,560 psychophysical trials across 97 observers) and\ncomes with a number of unexpected emergent benefits such as improved object\ndetection performance and previously unseen robustness towards a wide range of\nimage distortions, highlighting advantages of a shape-based representation.\n(a) Texture image\n81.4% Indian elephant\n10.3% indri\n8.2% black swan\n(b) Content image\n71.1% tabby cat\n17.3% grey fox\n3.3% Siamese cat\n(c) Texture-shape cue conflict\n63.9% Indian elephant\n26.4% indri\n9.6% black swan\nFigure 1: Classification of a standard ResNet-50 of (a) a texture image (elephant skin: only texture\ncues); (b) a normal image of a cat (with both shape and texture cues), and (c) an image with a\ntexture-shape cue conflict, generated by style transfer between the first two images.\n\u2217\nJoint senior authors\n1\nPublished as a conference paper at ICLR 2019\n1 INTRODUCTION\nHow are Convolutional Neural Networks (CNNs) able to reach impressive performance on complex\nperceptual tasks such as object recognition (Krizhevsky et al., 2012) and semantic segmentation\n(Long et al., 2015)? One widely accepted intuition is that CNNs combine low-level features (e.g.\nedges) to increasingly complex shapes (such as wheels, car windows) until the object (e.g. car) can\nbe readily classified. As Kriegeskorte (2015) puts it, \u201cthe network acquires complex knowledge\nabout the kinds of shapes associated with each category. [...] High-level units appear to learn\nrepresentations of shapes occurring in natural images\u201d (p. 429). This notion also appears in other\nexplanations, such as in LeCun et al. (2015): Intermediate CNN layers recognise \u201cparts of familiar\nobjects, and subsequent layers [...] detect objects as combinations of these parts\u201d (p. 436). We term\nthis explanation the shape hypothesis.\nThis hypothesis is supported by a number of empirical findings. Visualisation techniques like De\u0002convolutional Networks (Zeiler & Fergus, 2014) often highlight object parts in high-level CNN fea\u0002tures.1 Moreover, CNNs have been proposed as computational models of human shape perception\nby Kubilius et al. (2016), who conducted an impressive number of experiments comparing human\nand CNN shape representations and concluded that CNNs \u201cimplicitly learn representations of shape\nthat reflect human shape perception\u201d (p. 15). Ritter et al. (2017) discovered that CNNs develop a\nso-called \u201cshape bias\u201d just like children, i.e. that object shape is more important than colour for\nobject classification (although see Hosseini et al. (2018) for contrary evidence). Furthermore, CNNs\nare currently the most predictive models for human ventral stream object recognition (e.g. Cadieu\net al., 2014; Yamins et al., 2014); and it is well-known that object shape is the single most impor\u0002tant cue for human object recognition (Landau et al., 1988), much more than other cues like size or\ntexture (which may explain the ease at which humans recognise line drawings or millennia-old cave\npaintings).\nOn the other hand, some rather disconnected findings point to an important role of object textures\nfor CNN object recognition. CNNs can still classify texturised images perfectly well, even if the\nglobal shape structure is completely destroyed (Gatys et al., 2017; Brendel & Bethge, 2019). Con\u0002versely, standard CNNs are bad at recognising object sketches where object shapes are preserved\nyet all texture cues are missing (Ballester & de Araujo, 2016). Additionally, two studies suggest that \u00b4\nlocal information such as textures may actually be sufficient to \u201csolve\u201d ImageNet object recogni\u0002tion: Gatys et al. (2015) discovered that a linear classifier on top of a CNN\u2019s texture representation\n(Gram matrix) achieves hardly any classification performance loss compared to original network\nperformance. More recently, Brendel & Bethge (2019) demonstrated that CNNs with explicitly con\u0002strained receptive field sizes throughout all layers are able to reach surprisingly high accuracies on\nImageNet, even though this effectively limits a model to recognising small local patches rather than\nintegrating object parts for shape recognition. Taken together, it seems that local textures indeed\nprovide sufficient information about object classes\u2014ImageNet object recognition could, in princi\u0002ple, be achieved through texture recognition alone. In the light of these findings, we believe that it\nis time to consider a second explanation, which we term the texture hypothesis: in contrast to the\ncommon assumption, object textures are more important than global object shapes for CNN object\nrecognition.\nResolving these two contradictory hypotheses is important both for the deep learning community\n(to increase our understanding of neural network decisions) as well as for the human vision and\nneuroscience communities (where CNNs are being used as computational models of human object\nrecognition and shape perception). In this work we aim to shed light on this debate with a num\u0002ber of carefully designed yet relatively straightforward experiments. Utilising style transfer (Gatys\net al., 2016), we created images with a texture-shape cue conflict such as the cat shape with elephant\ntexture depicted in Figure 1c. This enables us to quantify texture and shape biases in both humans\nand CNNs. To this end, we perform nine comprehensive and careful psychophysical experiments\ncomparing humans against CNNs on exactly the same images, totalling 48,560 psychophysical tri\u0002als across 97 observers. These experiments provide behavioural evidence in favour of the texture\nhypothesis: A cat with an elephant texture is an elephant to CNNs, and still a cat to humans. Beyond\nquantifying existing biases, we subsequently present results for our two other main contributions:\n1To avoid any confusion caused by different meanings of the term \u2018feature\u2019, we consistently use it to refer\nto properties of CNNs (learned features) rather than to object properties (such as colour). When referring to\nphysical objects, we use the term \u2018cue\u2019 instead.\n2\nPublished as a conference paper at ICLR 2019\nAlexNet AlexNet AlexNet AlexNet AlexNet AlexNet\n100\nGoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet\n100\nVGG\u221216 VGG\u221216 VGG\u221216\n100\nResNet\u221250 ResNet\u221250 ResNet\u221250 ResNet\u221250 ResNet\u221250 ResNet\u221250\n100\nHumans Humans Humans\n99 97 99 100100 98\n44 49 48 54\n75\n40\n28 24 18\n87\n100100100100\n90\noriginal greyscale silhouette edges texture\nFigure 2: Accuracies and example stimuli for five different experiments without cue conflict.\nchanging biases, and discovering emergent benefits of changed biases. We show that the texture bias\nin standard CNNs can be overcome and changed towards a shape bias if trained on a suitable data\nset. Remarkably, networks with a higher shape bias are inherently more robust to many different\nimage distortions (for some even reaching or surpassing human performance, despite never being\ntrained on any of them) and reach higher performance on classification and object recognition tasks.\n2 METHODS\nIn this section we outline the core elements of paradigm and procedure. Extensive details to facilitate\nreplication are provided in the Appendix. Data, code and materials are available from this repository:\nhttps://github.com/rgeirhos/texture-vs-shape\n2.1 PSYCHOPHYSICAL EXPERIMENTS\nAll psychophysical experiments were conducted in a well-controlled psychophysical lab setting and\nfollow the paradigm of Geirhos et al. (2018), which allows for direct comparisons between human\nand CNN classification performance on exactly the same images. Briefly, in each trial participants\nwere presented a fixation square for 300 ms, followed by a 300 ms presentation of the stimulus\nimage. After the stimulus image we presented a full-contrast pink noise mask (1/f spectral shape)\nfor 200 ms to minimise feedback processing in the human visual system and to thereby make the\ncomparison to feedforward CNNs as fair as possible. Subsequently, participants had to choose one\nof 16 entry-level categories by clicking on a response screen shown for 1500 ms. On this screen,\nicons of all 16 categories were arranged in a 4 \u00d7 4 grid. Those categories were airplane, bear,\nbicycle, bird, boat, bottle, car, cat, chair, clock, dog, elephant, keyboard,\nknife, oven and truck. Those are the so-called \u201c16-class-ImageNet\u201d categories introduced in\nGeirhos et al. (2018).\nThe same images were fed to four CNNs pre-trained on standard ImageNet, namel",
          "original_query": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
          "cleaned_query": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Image Style Transfer Using Convolutional Neural Networks",
          "url": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf",
          "content": "Image Style Transfer Using Convolutional Neural Networks\nLeon A. Gatys\nCentre for Integrative Neuroscience, University of Tubingen, Germany \u00a8\nBernstein Center for Computational Neuroscience, Tubingen, Germany \u00a8\nGraduate School of Neural Information Processing, University of Tubingen, Germany \u00a8\nleon.gatys@bethgelab.org\nAlexander S. Ecker\nCentre for Integrative Neuroscience, University of Tubingen, Germany \u00a8\nBernstein Center for Computational Neuroscience, Tubingen, Germany \u00a8\nMax Planck Institute for Biological Cybernetics, Tubingen, Germany \u00a8\nBaylor College of Medicine, Houston, TX, USA\nMatthias Bethge\nCentre for Integrative Neuroscience, University of Tubingen, Germany \u00a8\nBernstein Center for Computational Neuroscience, Tubingen, Germany \u00a8\nMax Planck Institute for Biological Cybernetics, Tubingen, Germany \u00a8\nAbstract\nRendering the semantic content of an image in different\nstyles is a difficult image processing task. Arguably, a major\nlimiting factor for previous approaches has been the lack of\nimage representations that explicitly represent semantic in\u0002formation and, thus, allow to separate image content from\nstyle. Here we use image representations derived from Con\u0002volutional Neural Networks optimised for object recogni\u0002tion, which make high level image information explicit. We\nintroduce A Neural Algorithm of Artistic Style that can sep\u0002arate and recombine the image content and style of natural\nimages. The algorithm allows us to produce new images of\nhigh perceptual quality that combine the content of an ar\u0002bitrary photograph with the appearance of numerous well\u0002known artworks. Our results provide new insights into the\ndeep image representations learned by Convolutional Neu\u0002ral Networks and demonstrate their potential for high level\nimage synthesis and manipulation.\n1. Introduction\nTransferring the style from one image onto another can\nbe considered a problem of texture transfer. In texture trans\u0002fer the goal is to synthesise a texture from a source image\nwhile constraining the texture synthesis in order to preserve\nthe semantic content of a target image. For texture synthesis\nthere exist a large range of powerful non-parametric algo\u0002rithms that can synthesise photorealistic natural textures by\nresampling the pixels of a given source texture [7, 30, 8, 20].\nMost previous texture transfer algorithms rely on these non\u0002parametric methods for texture synthesis while using differ\u0002ent ways to preserve the structure of the target image. For\ninstance, Efros and Freeman introduce a correspondence\nmap that includes features of the target image such as im\u0002age intensity to constrain the texture synthesis procedure\n[8]. Hertzman et al. use image analogies to transfer the tex\u0002ture from an already stylised image onto a target image[13].\nAshikhmin focuses on transferring the high-frequency tex\u0002ture information while preserving the coarse scale of the\ntarget image [1]. Lee et al. improve this algorithm by addi\u0002tionally informing the texture transfer with edge orientation\ninformation [22].\nAlthough these algorithms achieve remarkable results,\nthey all suffer from the same fundamental limitation: they\nuse only low-level image features of the target image to in\u0002form the texture transfer. Ideally, however, a style transfer\nalgorithm should be able to extract the semantic image con\u0002tent from the target image (e.g. the objects and the general\nscenery) and then inform a texture transfer procedure to ren\u0002der the semantic content of the target image in the style of\nthe source image. Therefore, a fundamental prerequisite is\nto find image representations that independently model vari\u0002ations in the semantic image content and the style in which\n12414\nInput image\nContent\nRepresentations\nStyle\nRepresentations\nConvolutional Neural Network\nStyle Reconstructions\nContent Reconstructions\na b c d e\na b c d e\nFigure 1. Image representations in a Convolutional Neural Network (CNN). A given input image is represented as a set of filtered images\nat each processing stage in the CNN. While the number of different filters increases along the processing hierarchy, the size of the filtered\nimages is reduced by some downsampling mechanism (e.g. max-pooling) leading to a decrease in the total number of units per layer of the\nnetwork. Content Reconstructions. We can visualise the information at different processing stages in the CNN by reconstructing the input\nimage from only knowing the network\u2019s responses in a particular layer. We reconstruct the input image from from layers \u2018conv1 2\u2019 (a),\n\u2018conv2 2\u2019 (b), \u2018conv3 2\u2019 (c), \u2018conv4 2\u2019 (d) and \u2018conv5 2\u2019 (e) of the original VGG-Network. We find that reconstruction from lower layers is\nalmost perfect (a\u2013c). In higher layers of the network, detailed pixel information is lost while the high-level content of the image is preserved\n(d,e). Style Reconstructions. On top of the original CNN activations we use a feature space that captures the texture information of an\ninput image. The style representation computes correlations between the different features in different layers of the CNN. We reconstruct\nthe style of the input image from a style representation built on different subsets of CNN layers ( \u2018conv1 1\u2019 (a), \u2018conv1 1\u2019 and \u2018conv2 1\u2019\n(b), \u2018conv1 1\u2019, \u2018conv2 1\u2019 and \u2018conv3 1\u2019 (c), \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019 and \u2018conv4 1\u2019 (d), \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019, \u2018conv4 1\u2019\nand \u2018conv5 1\u2019 (e). This creates images that match the style of a given image on an increasing scale while discarding information of the\nglobal arrangement of the scene.\nit is presented. Such factorised representations were pre\u0002viously achieved only for controlled subsets of natural im\u0002ages such as faces under different illumination conditions\nand characters in different font styles [29] or handwritten\ndigits and house numbers [17].\nTo generally separate content from style in natural im\u0002ages is still an extremely difficult problem. However, the re\u0002cent advance of Deep Convolutional Neural Networks [18]\nhas produced powerful computer vision systems that learn\nto extract high-level semantic information from natural im\u0002ages. It was shown that Convolutional Neural Networks\ntrained with sufficient labeled data on specific tasks such\nas object recognition learn to extract high-level image con\u0002tent in generic feature representations that generalise across\ndatasets [6] and even to other visual information processing\ntasks [19, 4, 2, 9, 23], including texture recognition [5] and\nartistic style classification [15].\nIn this work we show how the generic feature represen\u0002tations learned by high-performing Convolutional Neural\nNetworks can be used to independently process and ma\u0002nipulate the content and the style of natural images. We\nintroduce A Neural Algorithm of Artistic Style, a new algo\u00022415\nrithm to perform image style transfer. Conceptually, it is a\ntexture transfer algorithm that constrains a texture synthe\u0002sis method by feature representations from state-of-the-art\nConvolutional Neural Networks. Since the texture model is\nalso based on deep image representations, the style transfer\nmethod elegantly reduces to an optimisation problem within\na single neural network. New images are generated by per\u0002forming a pre-image search to match feature representations\nof example images. This general approach has been used\nbefore in the context of texture synthesis [12, 25, 10] and to\nimprove the understanding of deep image representations\n[27, 24]. In fact, our style transfer algorithm combines a\nparametric texture model based on Convolutional Neural\nNetworks [10] with a method to invert their image repre\u0002sentations [24].\n2. Deep image representations\nThe results presented below were generated on the ba\u0002sis of the VGG network [28], which was trained to perform\nobject recognition and localisation [26] and is described ex\u0002tensively in the original work [28]. We used the feature\nspace provided by a normalised version of the 16 convo\u0002lutional and 5 pooling layers of the 19-layer VGG network.\nWe normalized the network by scaling the weights such that\nthe mean activation of each convolutional filter over images\nand positions is equal to one. Such re-scaling can be done\nfor the VGG network without changing its output, because\nit contains only rectifying linear activation functions and no\nnormalization or pooling over feature maps. We do not use\nany of the fully connected layers. The model is publicly\navailable and can be explored in the caffe-framework [14].\nFor image synthesis we found that replacing the maximum\npooling operation by average pooling yields slightly more\nappealing results, which is why the images shown were gen\u0002erated with average pooling.\n2.1. Content representation\nGenerally each layer in the network defines a non-linear\nfilter bank whose complexity increases with the position of\nthe layer in the network. Hence a given input image ~x is\nencoded in each layer of the Convolutional Neural Network\nby the filter responses to that image. A layer with Nl dis\u0002tinct filters has Nl feature maps each of size Ml\n, where Ml\nis the height times the width of the feature map. So the re\u0002sponses in a layer l can be stored in a matrix F\nl \u2208 RNl\u00d7Ml\nwhere F\nl\nij is the activation of the i\nth filter at position j in\nlayer l.\nTo visualise the image information that is encoded at\ndifferent layers of the hierarchy one can perform gradient\ndescent on a white noise image to find another image that\nmatches the feature responses of the original image (Fig 1,\ncontent reconstructions) [24]. Let ~p and ~x be the original\nimage and the image that is generated, and P\nl\nand F\nl\ntheir\nrespective feature representation in layer l. We then define\nthe squared-error loss between the two feature representa\u0002tions\nLcontent(~p, ~x, l) = 1\n2\nX\ni,j\n\nF\nl\nij \u2212 P\nl\nij \u00012\n. (1)\nThe derivative of this loss with respect to the activations in\nlayer l equals\n\u2202Lcontent\n\u2202Fl\nij\n=\n(\nF\nl \u2212 Pl\n\u0001\nij if F\nl\nij > 0\n0 if F\nl\nij < 0 ,\n(2)\nfrom which the gradient with respect to the image ~x can\nbe computed using standard e",
          "original_query": "Image style transfer using convolutional neural networks",
          "cleaned_query": "Image style transfer using convolutional neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Deep convolutional networks do not classify based on global object ...",
          "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613",
          "content": "- [Article](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613)\n- [Authors](https://journals.plos.org/ploscompbiol/article/authors?id=10.1371/journal.pcbi.1006613)\n- [Metrics](https://journals.plos.org/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1006613)\n- [Comments](https://journals.plos.org/ploscompbiol/article/comments?id=10.1371/journal.pcbi.1006613)\n- Media Coverage\n\n- [Reader Comments](https://journals.plos.org/ploscompbiol/article/comments?id=10.1371/journal.pcbi.1006613)\n- [Figures](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613)\n\n## Figures\n\n![Fig 1](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g001)\n\n![Fig 2](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g002)\n\n![Fig 3](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g003)\n\n![Fig 4](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g004)\n\n![Fig 5](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g005)\n\n![Fig 6](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g006)\n\n![Fig 7](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g007)\n\n![Fig 8](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g008)\n\n![Fig 9](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g009)\n\n![Fig 10](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g010)\n\n![Fig 11](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g011)\n\n![Fig 12](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g012)\n\n![Fig 13](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g013)\n\n![Fig 14](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g014)\n\n![Fig 15](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g015)\n\n![Fig 16](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g016)\n\n![Fig 17](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g017)\n\n![Fig 18](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g018)\n\n![Fig 19](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g019)\n\n![Fig 20](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g020)\n\n![Fig 21](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g021)\n\n![Fig 22](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g022)\n\n![Fig 23](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g023)\n\n![Fig 24](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g024)\n\n![Fig 25](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g025)\n\n![Table 1](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.t001)\n\n![Fig 26](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g026)\n\n![Fig 27](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g027)\n\n![Fig 28](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g028)\n\n![Table 2](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.t002)\n\n## Abstract\n\nDeep convolutional networks (DCNNs) are achieving previously unseen performance in object classification, raising questions about whether DCNNs operate similarly to human vision. In biological vision, shape is arguably the most important cue for recognition. We tested the role of shape information in DCNNs trained to recognize objects. In Experiment 1, we presented a trained DCNN with object silhouettes that preserved overall shape but were filled with surface texture taken from other objects. Shape cues appeared to play some role in the classification of artifacts, but little or none for animals. In Experiments 2\u20134, DCNNs showed no ability to classify glass figurines or outlines but correctly classified some silhouettes. Aspects of these results led us to hypothesize that DCNNs do not distinguish object\u2019s bounding contours from other edges, and that DCNNs access some local shape features, but not global shape. In Experiment 5, we tested this hypothesis with displays that preserved local features but disrupted global shape, and vice versa. With disrupted global shape, which reduced human accuracy to 28%, DCNNs gave the same classification labels as with ordinary shapes. Conversely, local contour changes eliminated accurate DCNN classification but caused no difficulty for human observers. These results provide evidence that DCNNs have access to some local shape information in the form of local edge relations, but they have no access to global object shapes.\n\n## Author summary\n\n\u201cDeep learning\u201d systems\u2013specifically, deep convolutional neural networks (DCNNs)\u2013have recently achieved near human levels of performance in object recognition tasks. It has been suggested that the processing in these systems may model or explain object perception abilities in biological vision. For humans, shape is the most important cue for recognizing objects. We tested whether deep convolutional neural networks trained to recognize objects make use of object shape. Our findings indicate that other cues, such as surface texture, play a larger role in deep network classification than in human recognition. Most crucially, we show that deep learning systems have no sensitivity to the overall shape of an object. Whereas deep learning systems can access some local shape features, such as local orientation relations, they are not sensitive to the arrangement of these edge features or global shape in general, and they do not appear to distinguish bounding contours of objects from other edge information. These findings show a crucial divergence between artificial visual systems and biological visual processes.\n\n**Citation:** Baker N, Lu H, Erlikhman G, Kellman PJ (2018) Deep convolutional networks do not classify based on global object shape. PLoS Comput Biol 14(12):\ne1006613.\nhttps://doi.org/10.1371/journal.pcbi.1006613\n\n**Editor:** Wolfgang Einh\u00e4user, Technische Universitat Chemnitz, GERMANY\n\n**Received:** November 3, 2017; **Accepted:** October 31, 2018; **Published:** December 7, 2018\n\n**Copyright:** \u00a9 2018 Baker et al. This is an open access article distributed under the terms of the [Creative Commons Attribution License](http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\n\n**Data Availability:** All relevant data are within the manuscript and its Supporting Information files.\n\n**Funding:** This research was funded by the National Science Foundation Research Traineeship for ModEling and uNdersTanding human behaviOR (MENTOR) DGE-1829071 ( [http://www.math.ucla.edu/~bertozzi/NRT/index.html](http://www.math.ucla.edu/~bertozzi/NRT/index.html)) to NB and the Advancing Theory and Application in Perceptual and Adaptive Learning to Improve Community College Mathematics NSF Grant ECR-1644916 ( [https://www.nsf.gov/awardsearch/showAward?AWD\\_ID=1644916&HistoricalAwards=false](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1644916&HistoricalAwards=false)) to PJK. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\n\n**Competing interests:** The authors have declared that no competing interests exist.\n\n## Introduction\n\nMachine vision is one of the most challenging problems in artificial intelligence. Task-general image understanding is so difficult that it constitutes an \u201cAI complete\u201d problem \\[ [1](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613#pcbi.1006613.ref001)\\], that is, a problem of sufficient difficulty and generality that it requires intelligence on a par with humans. If solved, it would be considered equivalent to the first successful completion of a Turing test \\[ [2](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613#pcbi.1006613.ref002), [3](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613#pcbi.1006613.ref003)\\]. While the general problem of image understanding is still far outside the capabilities of modern artificial systems, algorithms are beginning to reach near human capabilities on certain specialized tasks. In particular, deep convolutional neural network (DCNN) algorithms are achieving previously unseen performance on object recognition tasks.\n\nSince their first entrance \\[ [4](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613#pcbi.1006613.ref004)\\] in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), deep convolutional networks have substantially outperformed other state of the art recognition algorithms (e.g., \\[ [5](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613#pcbi.1006613.ref005)\\]), t",
          "original_query": "Deep convolutional networks do not classify based on global object shape",
          "cleaned_query": "Deep convolutional networks do not classify based on global object shape",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Adaptive Stylization Curriculum for Controllable Shape Bias\nTrain ImageNet-scale CNNs with a curriculum that gradually increases style-transfer strength (via Gram-matrix style matching) while monitoring a held-out texture\u2013shape cue conflict set to adaptively tune stylization per class. The contribution is a practical recipe to *target* a desired shape-bias level (rather than \u201cmore stylization is better\u201d) and to quantify how much stylization is needed for robustness without sacrificing clean accuracy.",
        "Contour-First Pretraining with Silhouette\u2013Texture Decoupling\nPretrain a backbone on paired data: (a) object silhouettes/outlines and (b) the same shapes filled with mismatched textures, enforcing invariance to fill texture while preserving class identity. This directly addresses the \u201cno global shape\u201d finding by forcing networks to use bounding contours and global geometry, then evaluates transfer to natural images and robustness benchmarks.",
        "Edge Hierarchy Regularization: Penalize Reliance on Non-Boundary Edges\nUse automatic boundary proposals (e.g., from weakly-supervised segmentation or classical edge detectors) to label likely object boundaries vs interior texture edges, then add a loss that encourages class evidence to concentrate on boundary edges (via attribution/Grad-CAM alignment). The contribution is a mechanistic test of the hypothesis that CNNs treat all edges equally, and a concrete training intervention to privilege global outline information.",
        "Multi-Objective Training: Joint Style-Texture Prediction + Shape Classification\nAugment classifiers with an auxiliary head that predicts style statistics (e.g., Gram-matrix descriptors) of the input while the main head must be invariant to them (via adversarial or gradient reversal). This turns Gatys-style representations into an explicit nuisance variable, producing a model that can *represent* texture without *depending* on it for label decisions.",
        "Class-Conditional Shape Bias: Why Animals Stay Texture-Biased\nSystematically analyze (and then correct) class-dependent shape reliance by training with stylization intensity and silhouette augmentation scheduled per superclass (animals vs artifacts), then measuring cue-conflict behavior per class. The contribution is a targeted explanation-and-fix pipeline: identify which classes remain texture-driven and apply minimal interventions to equalize global shape use.",
        "Architectural Add-on for Global Shape Integration: Contour Graph Module\nAdd a lightweight module that converts predicted edge maps into a contour graph (nodes = junctions, edges = contour segments) and performs message passing to produce a global-shape embedding fused with CNN features. This is actionable (plug-in to ResNet-like models) and directly tests whether explicit global contour aggregation closes the gap highlighted in Paper",
        "Shape-Biased Models for Detection: Stylized Pretraining + Fine-Tune on COCO\nExtend the \u201cemergent benefits\u201d claim by pretraining on Stylized-ImageNet (or adaptive stylization) and then fine-tuning detectors/segmenters, measuring improvements specifically under texture shifts (e.g., synthetic domain randomization, corruption, or cross-domain datasets). The contribution is a rigorous link between shape bias and downstream localization tasks, with ablations that isolate which robustness gains come from shape vs other factors.",
        "Instance-Level Cue Conflict Training: On-the-Fly Style Transfer as Data Augmentation\nDuring supervised training, generate cue-conflict images by style-transferring randomly selected textures onto the current image while keeping the label fixed, but also include \u201ctexture decoy\u201d negatives to prevent shortcut learning (e.g., same texture with different shape). The contribution is an efficient, scalable training protocol that operationalizes the cue-conflict paradigm as a regularizer, not just an evaluation tool."
      ],
      "generated_ideas_raw": "1. **Adaptive Stylization Curriculum for Controllable Shape Bias**\n   Train ImageNet-scale CNNs with a curriculum that gradually increases style-transfer strength (via Gram-matrix style matching) while monitoring a held-out texture\u2013shape cue conflict set to adaptively tune stylization per class. The contribution is a practical recipe to *target* a desired shape-bias level (rather than \u201cmore stylization is better\u201d) and to quantify how much stylization is needed for robustness without sacrificing clean accuracy.\n\n2. **Contour-First Pretraining with Silhouette\u2013Texture Decoupling**\n   Pretrain a backbone on paired data: (a) object silhouettes/outlines and (b) the same shapes filled with mismatched textures, enforcing invariance to fill texture while preserving class identity. This directly addresses the \u201cno global shape\u201d finding by forcing networks to use bounding contours and global geometry, then evaluates transfer to natural images and robustness benchmarks.\n\n3. **Edge Hierarchy Regularization: Penalize Reliance on Non-Boundary Edges**\n   Use automatic boundary proposals (e.g., from weakly-supervised segmentation or classical edge detectors) to label likely object boundaries vs interior texture edges, then add a loss that encourages class evidence to concentrate on boundary edges (via attribution/Grad-CAM alignment). The contribution is a mechanistic test of the hypothesis that CNNs treat all edges equally, and a concrete training intervention to privilege global outline information.\n\n4. **Patch-Shuffle vs Outline-Shuffle Benchmark Suite (Global vs Local Shape Audit)**\n   Create a standardized dataset where local contour fragments are preserved but globally permuted (patch-shuffle), and conversely where global silhouette is preserved but local edge relations are perturbed (outline-shuffle), extending the manipulations from Paper 3 at scale. The contribution is an actionable diagnostic that separates \u201clocal edge relations\u201d from \u201cglobal shape integration,\u201d enabling architecture and training comparisons beyond single-lab stimuli.\n\n5. **Multi-Objective Training: Joint Style-Texture Prediction + Shape Classification**\n   Augment classifiers with an auxiliary head that predicts style statistics (e.g., Gram-matrix descriptors) of the input while the main head must be invariant to them (via adversarial or gradient reversal). This turns Gatys-style representations into an explicit nuisance variable, producing a model that can *represent* texture without *depending* on it for label decisions.\n\n6. **Class-Conditional Shape Bias: Why Animals Stay Texture-Biased**\n   Systematically analyze (and then correct) class-dependent shape reliance by training with stylization intensity and silhouette augmentation scheduled per superclass (animals vs artifacts), then measuring cue-conflict behavior per class. The contribution is a targeted explanation-and-fix pipeline: identify which classes remain texture-driven and apply minimal interventions to equalize global shape use.\n\n7. **Architectural Add-on for Global Shape Integration: Contour Graph Module**\n   Add a lightweight module that converts predicted edge maps into a contour graph (nodes = junctions, edges = contour segments) and performs message passing to produce a global-shape embedding fused with CNN features. This is actionable (plug-in to ResNet-like models) and directly tests whether explicit global contour aggregation closes the gap highlighted in Paper 3.\n\n8. **Shape-Biased Models for Detection: Stylized Pretraining + Fine-Tune on COCO**\n   Extend the \u201cemergent benefits\u201d claim by pretraining on Stylized-ImageNet (or adaptive stylization) and then fine-tuning detectors/segmenters, measuring improvements specifically under texture shifts (e.g., synthetic domain randomization, corruption, or cross-domain datasets). The contribution is a rigorous link between shape bias and downstream localization tasks, with ablations that isolate which robustness gains come from shape vs other factors.\n\n9. **Instance-Level Cue Conflict Training: On-the-Fly Style Transfer as Data Augmentation**\n   During supervised training, generate cue-conflict images by style-transferring randomly selected textures onto the current image while keeping the label fixed, but also include \u201ctexture decoy\u201d negatives to prevent shortcut learning (e.g., same texture with different shape). The contribution is an efficient, scalable training protocol that operationalizes the cue-conflict paradigm as a regularizer, not just an evaluation tool.\n\n10. **Human\u2013Model Alignment on Shape Judgments via Psychophysics-Inspired Losses**\n   Collect a small but targeted set of human judgments on cue-conflict and outline variants (similar to Paper 1\u2019s psychophysics), then fine-tune models with a secondary loss to match human choice distributions (not just labels). The contribution is a concrete alignment method that uses minimal human data to steer representations toward human-like shape processing and tests whether this improves robustness to distortions and distribution shift.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Stylization Curriculum for Controllable Shape Bias\nTrain ImageNet-scale CNNs with a curriculum that gradually increases style-transfer strength (via Gram-matrix style matching) while monitori",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Contour-First Pretraining with Silhouette\u2013Texture Decoupling\nPretrain a backbone on paired data: (a) object silhouettes/outlines and (b) the same shapes filled with mismatched textures, enforcing inva",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Edge Hierarchy Regularization: Penalize Reliance on Non-Boundary Edges\nUse automatic boundary proposals (e.g., from weakly-supervised segmentation or classical edge detectors) to label likely object b",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Multi-Objective Training: Joint Style-Texture Prediction + Shape Classification\nAugment classifiers with an auxiliary head that predicts style statistics (e.g., Gram-matrix descriptors) of the input w",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Class-Conditional Shape Bias: Why Animals Stay Texture-Biased\nSystematically analyze (and then correct) class-dependent shape reliance by training with stylization intensity and silhouette augmentatio",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Architectural Add-on for Global Shape Integration: Contour Graph Module\nAdd a lightweight module that converts predicted edge maps into a contour graph (nodes = junctions, edges = contour segments) an",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Shape-Biased Models for Detection: Stylized Pretraining + Fine-Tune on COCO\nExtend the \u201cemergent benefits\u201d claim by pretraining on Stylized-ImageNet (or adaptive stylization) and then fine-tuning dete",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Instance-Level Cue Conflict Training: On-the-Fly Style Transfer as Data Augmentation\nDuring supervised training, generate cue-conflict images by style-transferring randomly selected textures onto the ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 55,
      "paper_title": "On Linear Mode Connectivity of Mixture-of-Experts Architectures",
      "contribution": "This paper investigates Linear Mode Connectivity (LMC) within Mixture-of-Experts (MoE) architectures, proposing a matching algorithm for aligning independently trained MoEs to discover low-loss paths in parameter space.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9542,
      "output_tokens": 1051,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Linear Mode Connectivity and the Lottery Ticket Hypothesis",
          "url": "http://proceedings.mlr.press/v119/frankle20a/frankle20a.pdf",
          "content": "Linear Mode Connectivity and the Lottery Ticket Hypothesis\nJonathan Frankle 1 Gintare Karolina Dziugaite 2 Daniel M. Roy 3 4 Michael Carbin 1\nAbstract\nWe study whether a neural network optimizes to\nthe same, linearly connected minimum under dif\u0002ferent samples of SGD noise (e.g., random data\norder and augmentation). We find that standard\nvision models become stable to SGD noise in this\nway early in training. From then on, the outcome\nof optimization is determined to a linearly con\u0002nected region. We use this technique to study\niterative magnitude pruning (IMP), the procedure\nused by work on the lottery ticket hypothesis to\nidentify subnetworks that could have trained in\nisolation to full accuracy. We find that these sub\u0002networks only reach full accuracy when they are\nstable to SGD noise, which either occurs at initial\u0002ization for small-scale settings (MNIST) or early\nin training for large-scale settings (ResNet-50 and\nInception-v3 on ImageNet).\n1. Introduction\nWhen training a neural network with mini-batch stochastic\ngradient descent (SGD), training examples are presented to\nthe network in a random order within each epoch. In many\ncases, each example also undergoes random data augmen\u0002tation. This randomness can be seen as noise that varies\nfrom training run to training run and alters the network\u2019s tra\u0002jectory through the optimization landscape, even when the\ninitialization and hyperparameters are fixed. In this paper,\nwe investigate how this SGD noise affects the outcome of\noptimizing neural networks and the role this effect plays in\nsparse, lottery ticket networks (Frankle & Carbin, 2019).\nInstability analysis. To study these questions, we propose\ninstability analysis. The goal of instability analysis is to\ndetermine whether the outcome of optimizing a particular\nneural network is stable to SGD noise. Figure 1 (left) visual\u0002izes instability analysis. First, we create a network N with\nrandom initialization W0. We then train two copies of N\n1MIT CSAIL 2Element AI 3University of Toronto 4Vector Insti\u0002tute. Correspondence to: Jonathan Frankle.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\nW0\nW1\nT W2T\nInstability\nW0\nWk\nW1\nT W2T\nInstability\nFigure 1. A diagram of instability analysis from step 0 (left) and\nstep k (right) when comparing networks using linear interpolation.\nwith different samples of SGD noise (i.e., different random\ndata orders and augmentations). Finally, we compare the\nresulting networks to measure the effect of these different\nsamples of SGD noise on the outcome of optimization. If\nthe networks are sufficiently similar according to a criterion,\nwe determine N to be stable to SGD noise. We also study\nthis behavior starting from the state of N at step k of train\u0002ing (Figure 1 right). Doing so allows us to determine when\nthe outcome of optimization becomes stable to SGD noise.\nThere are many possible ways in which to compare the\nnetworks that result from instability analysis (Appendix G).\nWe use the behavior of the optimization landscape along\nthe line between these networks (blue in Figure 1). Does\nerror remain flat or even decrease (meaning the networks\nare in the same, linearly connected minimum), or is there a\nbarrier of increased error? We define the linear interpolation\ninstability of N to SGD noise as the maximum increase\nin error along this path (red). We consider N stable to\nSGD noise if error does not increase along this path, i.e.,\ninstability \u2248 0. This means N will find the same, linearly\nconnected minimum regardless of the sample of SGD noise.\nBy linearly interpolating at the end of training in this fashion,\nwe assess a linear form of mode connectivity, a phenomenon\nwhere the minima found by two networks are connected\nby a path of nonincreasing error. Freeman & Bruna (2017),\nDraxler et al. (2018), and Garipov et al. (2018) show that the\nmodes of standard vision networks trained from different\ninitializations are connected by nonlinear paths of constant\nerror or loss. Based on this work, we expect that all networks\nwe examine are connected by such paths. However, the\nmodes found by Draxler et al. and Garipov et al. are not\nconnected by linear paths. The only extant example of\nlinear mode connectivity is by Nagarajan & Kolter (2019),\nwho train MLPs from the same initialization on disjoint\nLinear Mode Connectivity and the Lottery Ticket Hypothesis\nsubsets of MNIST and find that the resulting networks are\nconnected by linear paths of constant test error. In contrast,\nwe explore linear mode connectivity from points throughout\ntraining, we do so at larger scales, and we focus on different\nsamples of SGD noise rather than disjoint samples of data.\nWe perform instability analysis on standard networks for\nMNIST, CIFAR-10, and ImageNet. All but the smallest\nMNIST network are unstable to SGD noise at initialization\naccording to linear interpolation. However, by a point early\nin training (3% for ResNet-20 on CIFAR-10 and 20% for\nResNet-50 on ImageNet), all networks become stable to\nSGD noise. From this point on, the outcome of optimization\nis determined to a linearly connected minimum.\nThe lottery ticket hypothesis. Finally, we show that insta\u0002bility analysis and linear interpolation are valuable scientific\ntools for understanding other phenomena in deep learning.\nSpecifically, we study the sparse networks discussed by the\nrecent lottery ticket hypothesis (LTH; Frankle & Carbin,\n2019). The LTH conjectures that, at initialization, neural\nnetworks contain sparse subnetworks that can train in isola\u0002tion to full accuracy.\nEmpirical evidence for the LTH consists of experiments\nusing a procedure called iterative magnitude pruning (IMP).\nOn small networks for MNIST and CIFAR-10, IMP retroac\u0002tively finds subnetworks at initialization that can train to the\nsame accuracy as the full network (we call such subnetworks\nmatching). Importantly, IMP finds matching subnetworks\nat nontrivial sparsity levels, i.e., those beyond which sub\u0002networks found by trivial random pruning are matching.\nIn more challenging settings, however, there is no empiri\u0002cal evidence for the LTH: IMP subnetworks of VGGs and\nResNets on CIFAR-10 and ImageNet are not matching at\nnontrivial sparsities (Liu et al., 2019; Gale et al., 2019).\nWe show that instability analysis distinguishes known cases\nwhere IMP succeeds and fails to find matching subnetworks\nat nontrivial sparsities, providing the first basis for under\u0002standing the mixed results in the literature. Namely, IMP\nsubnetworks are only matching when they are stable to\nSGD noise according to linear interpolation. Using this\ninsight, we identify new scenarios where we can find sparse,\nmatching subnetworks at nontrivial sparsities, including in\nmore challenging settings (e.g., ResNet-50 on ImageNet).\nIn these settings, sparse IMP subnetworks become stable to\nSGD noise early in training rather than at initialization, just\nas we find with the unpruned networks. Moreover, these\nstable IMP subnetworks are also matching. In other words,\nearly in training (if not at initialization), sparse subnetworks\nemerge that can complete training in isolation and reach full\naccuracy. These findings shed new light on neural network\ntraining dynamics, hint at possible mechanisms underly\u0002ing lottery ticket phenomena, and extend the lottery ticket\nobservations to larger scales.\nContributions. We make the following contributions:\n\u2022 We introduce instability analysis to determine whether\nthe outcome of optimizing a neural network is stable to\nSGD noise, and we suggest linear mode connectivity for\nmaking this determination.\n\u2022 On a range of image classification benchmarks including\nstandard networks on ImageNet, we observe that net\u0002works become stable to SGD noise early in training.\n\u2022 We use instability analysis to distinguish successes and\nfailures of IMP (the method behind extant lottery ticket\nresults). Namely, sparse IMP subnetworks are matching\nonly when they are stable to SGD noise.\n\u2022 We generalize IMP to find subnetworks early in train\u0002ing rather than at initialization. We show that IMP sub\u0002networks become stable and matching when set to their\nweights from early in training, making it possible to ex\u0002tend the lottery ticket observations to larger scales.\n2. Preliminaries and Methodology\nInstability analysis. To perform instability analysis on a\nnetwork N with weights W, we make two copies of N and\ntrain them with different random samples of SGD noise (i.e.,\ndifferent data orders and augmentations), producing trained\nweights W1\nT\nand W2\nT\n. We compare these weights with a\nfunction, producing a value we call the instability of N to\nSGD noise. We then determine whether this value satisfies\na criterion indicating that N is stable to SGD noise. The\nweights of N could be randomly initialized (W = W0 in\nFigure 1) or the result of k training steps (W = Wk).\nFormally, we model SGD by function As\u2192t: R\nD \u00d7 U \u2192\nR\nD that maps weights Ws \u2208 R\nD at step s and SGD ran\u0002domness u \u223c U to weights Wt \u2208 R\nD at step t by training\nfor t \u2212 s steps (s, t \u2208 {0, .., T}). Algorithm 1 outlines\ninstability analysis with a function f : R\nD \u00d7 RD \u2192 R.\nAlgorithm 1 Compute instability of Wk with function f.\n1: Train Wk to W1\nT with noise u1 \u223c U: W1T = A\nk\u2192T\n(Wk, u1)\n2: Train Wk to W2\nT with noise u2 \u223c U: W2T = A\nk\u2192T\n(Wk, u2)\n3: Return f(W1\nT , W2T ), i.e., the instability of Wk to SGD noise.\nLinear interpolation. Consider a path p on the optimiza\u0002tion landscape between networks W1 and W2. We define\nthe error barrier height of p as the maximum increase in\nerror from that of W1 and W2 along path p. For instability\nanalysis, we use as our function f the error barrier height\nalong the linear path between two networks W1 and W2.\n1\nFormally, let E(W) be the (train or test) error of a network\nwith weights W. Let E\u03b1(W1, W2) = E(\u03b1W1 + (1\u2212\u03b1)W2)\nfor \u03b1 \u2208 [0, 1] be the error of the network created by linearly\ninterpolating between W1 and W2. Let Esup(W1, W2) =\nsup\u03b1 E\u03b1(W1",
          "original_query": "Linear mode connectivity and the lottery ticket hypothesis",
          "cleaned_query": "Linear mode connectivity and the lottery ticket hypothesis",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "The role of permutation invariance in linear mode connectivity of ...",
          "url": "https://research.google/pubs/the-role-of-permutation-invariance-in-linear-mode-connectivity-of-neural-networks/",
          "content": "# The role of permutation invariance in linear mode connectivity of neural networks\n\nRahim Entezari\n\n[Hanie Sedghi](https://research.google/people/haniesedghi/)\n\nOlga Saukh\n\nBehnam Neyshabur\n\nICLR (2022)\n\n[Download](https://arxiv.org/abs/2110.06296) [Google Scholar](https://scholar.google.com/scholar?lr&ie=UTF-8&oe=UTF-8&q=The role of permutation invariance in linear mode connectivity of neural networks Rahim Entezari Olga Saukh Hanie Sedghi Behnam Neyshabur)\n\nCopy Bibtex\n\n### Abstract\n\nUnderstanding the loss landscape of deep neural networks has been the subject of many studies due to its close connections to optimization and generalization. Prior work has shown that there is often a performance barrier along the linear interpolation of the weights of two models trained with different initial seeds. In this work, we first empirically investigate how different model parameters and data distributions impact such performance barriers. Next, we consider the invariances in the function space of neural networks that arise from permutation of hidden units. We investigate this through extensive experiments and provide several pieces of evidence that if these invariances are taken into account, many of the barriers vanish.\n\n### Research Areas\n\n- [Machine Intelligence](https://research.google/research-areas/machine-intelligence/)\n\n## Meet the teams driving innovation\n\nOur teams advance the state of the art through research, systems engineering, and collaboration across Google.\n\n[See our teams](https://research.google/teams/)",
          "original_query": "The role of permutation invariance in linear mode connectivity of neural networks",
          "cleaned_query": "The role of permutation invariance in linear mode connectivity of neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[1910.05653] Model Fusion via Optimal Transport - arXiv",
          "url": "https://arxiv.org/abs/1910.05653",
          "content": "[1910.05653] Model Fusion via Optimal Transport[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1910.05653\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1910.05653**(cs)\n[Submitted on 12 Oct 2019 ([v1](https://arxiv.org/abs/1910.05653v1)), last revised 16 May 2023 (this version, v6)]\n# Title:Model Fusion via Optimal Transport\nAuthors:[Sidak Pal Singh](https://arxiv.org/search/cs?searchtype=author&amp;query=Singh,+S+P),[Martin Jaggi](https://arxiv.org/search/cs?searchtype=author&amp;query=Jaggi,+M)\nView a PDF of the paper titled Model Fusion via Optimal Transport, by Sidak Pal Singh and Martin Jaggi\n[View PDF](https://arxiv.org/pdf/1910.05653)> > Abstract:\n> Combining different models is a widely used paradigm in machine learning applications. While the most common approach is to form an ensemble of models and average their individual predictions, this approach is often rendered infeasible by given resource constraints in terms of memory and computation, which grow linearly with the number of models. We present a layer-wise model fusion algorithm for neural networks that utilizes optimal transport to (soft-) align neurons across the models before averaging their associated parameters.\n> We show that this can successfully yield &#34;one-shot&#34; knowledge transfer (i.e, without requiring any retraining) between neural networks trained on heterogeneous non-i.i.d. data. In both i.i.d. and non-i.i.d. settings , we illustrate that our approach significantly outperforms vanilla averaging, as well as how it can serve as an efficient replacement for the ensemble with moderate fine-tuning, for standard convolutional networks (like VGG11), residual networks (like ResNet18), and multi-layer perceptrons on CIFAR10, CIFAR100, and MNIST. Finally, our approach also provides a principled way to combine the parameters of neural networks with different widths, and we explore its application for model compression. The code is available at the following link, [> this https URL\n](https://github.com/sidak/otfusion)> . Comments:|NeurIPS 2020 conference proceedings (early version featured in the Optimal Transport &amp; Machine Learning workshop, NeurIPS 2019)|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:1910.05653](https://arxiv.org/abs/1910.05653)[cs.LG]|\n|(or[arXiv:1910.05653v6](https://arxiv.org/abs/1910.05653v6)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1910.05653](https://doi.org/10.48550/arXiv.1910.05653)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Sidak Pal Singh [[view email](https://arxiv.org/show-email/f928ad4f/1910.05653)]\n**[[v1]](https://arxiv.org/abs/1910.05653v1)**Sat, 12 Oct 2019 22:07:15 UTC (778 KB)\n**[[v2]](https://arxiv.org/abs/1910.05653v2)**Thu, 19 Dec 2019 13:05:55 UTC (1,049 KB)\n**[[v3]](https://arxiv.org/abs/1910.05653v3)**Fri, 21 Feb 2020 15:16:06 UTC (1,928 KB)\n**[[v4]](https://arxiv.org/abs/1910.05653v4)**Fri, 19 Jun 2020 16:42:43 UTC (3,623 KB)\n**[[v5]](https://arxiv.org/abs/1910.05653v5)**Sun, 14 Feb 2021 21:50:57 UTC (2,124 KB)\n**[v6]**Tue, 16 May 2023 17:57:37 UTC (2,125 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Model Fusion via Optimal Transport, by Sidak Pal Singh and Martin Jaggi\n* [View PDF](https://arxiv.org/pdf/1910.05653)\n* [TeX Source](https://arxiv.org/src/1910.05653)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1910.05653&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1910.05653&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2019-10](https://arxiv.org/list/cs.LG/2019-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/1910.05653?context=cs)\n[stat](https://arxiv.org/abs/1910.05653?context=stat)\n[stat.ML](https://arxiv.org/abs/1910.05653?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1910.05653)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1910.05653)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1910.05653)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1910.html#abs-1910-05653)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1910-05653)\n[Sidak Pal Singh]()\n[Martin Jaggi]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1910.05653)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Model fusion via optimal transport",
          "cleaned_query": "Model fusion via optimal transport",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Empirical Analysis of the Hessian of Over-Parametrized Neural ...",
          "url": "https://arxiv.org/abs/1706.04454",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1706.04454** (cs)\n\n\\[Submitted on 14 Jun 2017 ( [v1](https://arxiv.org/abs/1706.04454v1)), last revised 7 May 2018 (this version, v3)\\]\n\n# Title:Empirical Analysis of the Hessian of Over-Parametrized Neural Networks\n\nAuthors: [Levent Sagun](https://arxiv.org/search/cs?searchtype=author&query=Sagun,+L), [Utku Evci](https://arxiv.org/search/cs?searchtype=author&query=Evci,+U), [V. Ugur Guney](https://arxiv.org/search/cs?searchtype=author&query=Guney,+V+U), [Yann Dauphin](https://arxiv.org/search/cs?searchtype=author&query=Dauphin,+Y), [Leon Bottou](https://arxiv.org/search/cs?searchtype=author&query=Bottou,+L)\n\nView a PDF of the paper titled Empirical Analysis of the Hessian of Over-Parametrized Neural Networks, by Levent Sagun and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/1706.04454)\n\n> Abstract:We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the flatness of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create large connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.\n\n| | |\n| --- | --- |\n| Comments: | Minor update for ICLR 2018 Workshop Track presentation |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1706.04454](https://arxiv.org/abs/1706.04454) \\[cs.LG\\] |\n| (or [arXiv:1706.04454v3](https://arxiv.org/abs/1706.04454v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1706.04454](https://doi.org/10.48550/arXiv.1706.04454) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Levent Sagun \\[ [view email](https://arxiv.org/show-email/87d4f5bb/1706.04454)\\] **[\\[v1\\]](https://arxiv.org/abs/1706.04454v1)**\nWed, 14 Jun 2017 12:50:00 UTC (391 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1706.04454v2)**\nWed, 8 Nov 2017 11:36:52 UTC (427 KB)\n**\\[v3\\]**\nMon, 7 May 2018 15:43:39 UTC (952 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Empirical Analysis of the Hessian of Over-Parametrized Neural Networks, by Levent Sagun and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/1706.04454)\n- [TeX Source](https://arxiv.org/src/1706.04454)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1706.04454&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1706.04454&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2017-06](https://arxiv.org/list/cs.LG/2017-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1706.04454?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.04454)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.04454)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.04454)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#SagunEGDB17) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/SagunEGDB17)\n\n[Levent Sagun](https://dblp.uni-trier.de/search/author?author=Levent%20Sagun) [Utku Evci](https://dblp.uni-trier.de/search/author?author=Utku%20Evci) [V. Ugur G\u00fcney](https://dblp.uni-trier.de/search/author?author=V.%20Ugur%20G%C3%BCney) [Yann Dauphin](https://dblp.uni-trier.de/search/author?author=Yann%20Dauphin) [Yann N. Dauphin](https://dblp.uni-trier.de/search/author?author=Yann%20N.%20Dauphin)\n\n\u2026\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1706.04454) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Empirical analysis of the hessian of over-parametrized neural networks",
          "cleaned_query": "Empirical analysis of the hessian of over-parametrized neural networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Re-Basin via Implicit Sinkhorn Differentiation - CVF Open Access",
          "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Pena_Re-Basin_via_Implicit_Sinkhorn_Differentiation_CVPR_2023_paper.pdf",
          "content": "Re-basin via implicit Sinkhorn differentiation\nFidel A. Guerrero Pena\u02dc\nfidel-alejandro.guerrero-pena@etsmtl.ca\nHeitor Rapela Medeiros\nheitor.rapela-medeiros.1@ens.etsmtl.ca\nThomas Dubail\nthomas.dubail.1@ens.etsmtl.ca\nMasih Aminbeidokhti\nmasih.aminbeidokhti.1@ens.etsmtl.ca\nEric Granger\neric.granger@etsmtl.ca\nMarco Pedersoli\nmarco.pedersoli@etsmtl.ca\nLIVIA, Dept. of Systems Engineering\nETS Montreal, Canada\nAbstract\nThe recent emergence of new algorithms for permuting\nmodels into functionally equivalent regions of the solution\nspace has shed some light on the complexity of error sur\u0002faces and some promising properties like mode connectivity.\nHowever, finding the permutation that minimizes some ob\u0002jectives is challenging, and current optimization techniques\nare not differentiable, which makes it difficult to integrate\ninto a gradient-based optimization, and often leads to sub\u0002optimal solutions. In this paper, we propose a Sinkhorn re\u0002basin network with the ability to obtain the transportation\nplan that better suits a given objective. Unlike the cur\u0002rent state-of-art, our method is differentiable and, there\u0002fore, easy to adapt to any task within the deep learning do\u0002main. Furthermore, we show the advantage of our re-basin\nmethod by proposing a new cost function that allows per\u0002forming incremental learning by exploiting the linear mode\nconnectivity property. The benefit of our method is com\u0002pared against similar approaches from the literature un\u0002der several conditions for both optimal transport and linear\nmode connectivity. The effectiveness of our continual learn\u0002ing method based on re-basin is also shown for several com\u0002mon benchmark datasets, providing experimental results\nthat are competitive with the state-of-art. The source code\nis provided at https://github.com/fagp/sinkhorn-rebasin.\n1. Introduction\nDespite the success of deep learning (DL) across many\napplication domains, the loss surfaces of neural networks\n(NNs) are not well understood. Even for shallow NNs, the\nnumber of saddle points and local optima can increase expo\u0002nentially with the number of parameters [4,13]. The permu\u0002\u03b8A\n\u03b8B\n\u03c0P (\u03b8B)\n(a)\nNaive\nWM [2]\nSinkhorn\n\u03bb\nC(\u03b8)\n(b)\nFigure 1. (a) Loss landscape for the polynomial approximation\ntask [27]. \u03b8A and \u03b8B are models found by SGD. LMC suggests\nthat re-basin the model \u03b8B would result in a functionally equiv\u0002alent model \u03c0P (\u03b8B), with no barrier on its linear interpolation\n(1 \u2212 \u03bb)\u03b8A + \u03bb\u03c0P (\u03b8B). (b) Comparison of the cost in the lin\u0002ear path along \u03bb before and after re-basin using weight matching\n(WM) [2] and our Sinkhorn. The dashed line in the figures corre\u0002sponds with the naive path, and the solid line is the path after the\nproposed Sinkhorn re-basin. The blue line represents WM path.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore. 20237\ntation symmetry of neurons in each layer allows the same\nfunction to be represented with many different parameter\nvalues of the same network. Symmetries imposed by these\ninvariances help us to better understand the structure of the\nloss landscape [6, 11, 13].\nPrevious studies establish that minima found by Stochas\u0002tic Gradient Descent (SGD) are not only connected in the\nnetwork parameter\u2019s space by a path of non-increasing loss,\nbut also permutation symmetries may allow us to con\u0002nect those points linearly with no detriment to the loss\n[9, 11\u201313, 15, 24]. This phenomenon is often referred to as\nlinear mode connectivity (LMC) [24]. For instance, Fig. 1a\nshows a portion of the loss landscape for the polynomial\napproximation task [27] using the method proposed by Li\net al. [16]. \u03b8A and \u03b8B are two minima found by SGD in\ndifferent basins with an energy barrier between the pair.\nLMC suggests that if one considers permutation invariance,\nwe can teleport solutions into a single basin where there is\nalmost no loss barrier between different solutions [2, 11].\nIn literature, this mechanism is called re-basin [2]. How\u0002ever, efficiently searching for permutation symmetries that\nbring all solutions to one basin is a challenging problem\n[11]. Three main approaches for matching units between\ntwo NNs have been explored in the literature [2]. Some\nstudies propose a data-dependent algorithm that associates\nunits across two NNs by matching their activations [2, 26].\nSince activation-based matching is data dependent, it helps\nto adjust permutations to certain desired kinds of classes or\ndomains [26]. Instead of associating units by their activa\u0002tions, one could align the weights of the model itself [2,26],\nwhich is independent of the dataset, and therefore the com\u0002putational cost is much lower. Finally, the third approach\nis to iteratively adjust the permutation of weights. In par\u0002ticular, Ainsworth et al. [2] have proposed alternating be\u0002tween models alignment and barrier minimization using a\nStraight-Through Estimator. Unfortunately, the proposed\napproaches so far are either non-differentiable [2, 11, 26] or\ncomputationally expensive [2], making the solution difficult\nto be extended to other applications, with a different objec\u0002tive. For instance, adapting those methods for incremental\nlearning by including the algorithm for weight matching be\u0002tween two models trained on different domains is not trivial\nbecause of the difficulties in optimizing new objectives.\nIn this work, inspired by [21], we relax the permutation\nmatrix with the Sinkhorn operator [1], and use it to solve\nthe re-basin problem in a differentiable fashion. To avoid\nthe high cost for computing gradients in the proposal of\nMena et al. [21], we use the implicit differentiation algo\u0002rithm proposed in [10], which has been shown to be more\ncost-effective. Our re-basin formulation allows defining any\ndifferentiable objective as a loss function.\nA direct application of re-basin is the merger of diverse\nmodels without significantly degrading their performance\n[2, 5, 12, 13, 28]. Applications like federate learning [2],\nensembling [12], or model initialization [5] exploit such a\nmerger by selecting a model in the line connecting the mod\u0002els to be combined. To show the effectiveness of our ap\u0002proach, we propose a new continual learning algorithm that\ncombines models trained on different domains. Our con\u0002tinual learning algorithm differs from previous state-of-art\napproaches [22] because it directly estimates a model at the\nintersection of previous and new knowledge, by exploiting\nthe LMC property observed in SGD-based solutions.\nOur main contribution can be summarized as follows:\n(1) Solving the re-basin for optimal transportation using\nimplicit Sinkhorn differentiation, enabling better differen\u0002tiable solutions that can be integrated on any loss.\n(2) A powerful way to use our re-basin method based on the\nSinkhorn operator for incremental learning by considering\nit as a model merging problem and leveraging LMC.\n(3) An extensive set of experiments that validate our method\nfor: (i) finding the optimal permutation to transform a\nmodel to another one equivalent; (ii) linear mode connec\u0002tivity, to linearly connect two models such that their loss\nis almost identical along the entire connecting line in the\nweights space; and (iii) learning new domains and tasks in\u0002crementally while not forgetting the previous ones.\n2. Related work\nRe-basin. Recently, in the NN community, re-basin has\nbeen demonstrating useful properties. The main goal of\nsuch re-basin approaches is to obtain functionally equiva\u0002lent models in a different region of the weight space fol\u0002lowing some pre-defined objective. Permutation symme\u0002tries are a well-known example of transformations that al\u0002lows performing re-basin. In particular, Entezari et al. [11]\nshow that the invariances of NNs using random permuta\u0002tions on SGD solutions are likely to have almost zero barri\u0002ers, and therefore the randomness in terms of permutations\ndoes not impact the quality of the final training result of the\nmodel. A simulated annealing-based algorithm was pro\u0002posed for doing a re-basin with an elevated computational\ncost, making it impractical to use, especially for bigger\nmodels. Ainsworth et al. [2] proposed three new re-basin\nalgorithms that rely on solving linear assignment problems\nto find permutation matrices that satisfy their encoded ob\u0002jective. The methods showed to perform well, especially\nin achieving linear mode connectivity. On the downside,\nnew objectives are difficult to plug into their framework.\nTheir solution uses greedy algorithms that do not guaran\u0002tee finding the optimal solution, as shown in our experi\u0002ments. Using a similar approach, Benzing et al. [5] found\nstrong evidence that two random initialization of a NN after\npermutation can lead to a good performance, showing that\nthe random initialization is already in the same loss valley\nduring the initialization. Finally, [3] uses the concepts of\n20238\nWasserstein barycenter and Gromov-Wasserstein barycen\u0002ter, offering a NN model fusion framework with insights\nabout linear mode connectivity of SGD solutions. Even\nthough the previous works presented solutions to perform\nre-basin by solving linear assignment problems, their ap\u0002proach fails to generalize well for other objectives. Using\ngradient descent-based algorithms seems to be a more suit\u0002able approach.\nMode connectivity. Mode connectivity is the task of find\u0002ing low-barrier paths connecting models within the weight\nspace. In their work, Garipov et al. [13] found that the lo\u0002cal optima of deep learning models are connected by sim\u0002ple curves. The fast geometric ensembling method was\nproposed as an application for their proposal. Almost at\nthe same time, [9] proposed a nudged elastic band-based\nmethod to construct continuous paths between minima of\nNN architectures. Finally, Frankle et al. [12] studied the\nsensitivity of different levels of SGD noise on NNs. These\npioneering works are the basis",
          "original_query": "Re-basin via implicit Sinkhorn differentiation",
          "cleaned_query": "Re-basin via implicit Sinkhorn differentiation",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Stability-to-SGD Noise as an Early-Stopping Criterion for Lottery Tickets\nUse instability analysis (linear interpolation barrier across SGD noise seeds) as a measurable trigger to stop dense pretraining and begin IMP-style pruning exactly when training enters the \u201cstable\u201d region. Evaluate whether pruning at this stability onset yields higher-sparsity matching subnetworks and reduces total compute versus fixed-percentage warmup heuristics on CIFAR/ImageNet-scale models.",
        "Hessian-Outlier Tracking to Predict Linear Mode Connectivity Onset\nHypothesize that the emergence/stabilization of LMC corresponds to a characteristic evolution of the Hessian spectrum (e.g., outlier magnitudes plateau while the bulk remains near zero). Build an online estimator (Lanczos/trace methods) that monitors top Hessian eigenvalues during training and predicts the step when models become linearly connected under SGD noise, then validate against direct interpolation-barrier measurements.",
        "Permutation-Aware Instability Analysis (Rebasin-then-Interpolate)\nExtend Frankle et al.\u2019s instability analysis by first applying permutation alignment (weight matching / Sinkhorn re-basin) between two runs before measuring linear interpolation barriers. Quantify how much of \u201cinstability to SGD noise\u201d is explained by permutation symmetries vs. genuinely different basins, across architectures (CNNs, ResNets, ViTs) and training regimes (augmentations, label noise).",
        "Joint Optimal-Transport Fusion + Connectivity-Constrained Fine-Tuning\nCombine OT-based model fusion (layer-wise neuron alignment) with an explicit penalty that enforces low loss along the interpolation path between source models and the fused model (a \u201cconnectivity regularizer\u201d). Test whether this yields a one-shot merged model that not only matches ensemble accuracy but is also provably in the same linearly connected region as each participant, improving robustness of federated averaging under non-i.i.d. data.",
        "Connectivity-Preserving Pruning via Sinkhorn Alignment Between Dense and Sparse Models\nDuring IMP, re-basin the pruned subnetwork to the dense network (or vice versa) using differentiable Sinkhorn permutations at each pruning round, and select masks that minimize the post-alignment interpolation barrier. This turns \u201cmatching subnetworks\u201d into \u201clinearly connected subnetworks,\u201d aiming for higher reproducibility across seeds and better transfer when training sparse models in isolation.",
        "Data-Distribution Controls on LMC: Manipulating Hessian Outliers via Synthetic Cluster Structure\nUsing Sagun et al.\u2019s claim that data complexity affects Hessian outliers, systematically vary dataset cluster structure/separability (synthetic mixtures, controlled label noise) and measure resulting changes in (a) Hessian outliers and (b) linear interpolation barriers before/after permutation alignment. The contribution is an empirical map linking data geometry \u2192 curvature outliers \u2192 (permutation-aware) mode connectivity, clarifying when barriers are fundamentally data-driven.",
        "Barrier-Minimizing Training: Add a \u201cCross-Seed Linear Path\u201d Regularizer\nTrain two replicas from the same initialization under different SGD noise (as in instability analysis) and add a differentiable penalty for maximal loss along their linear interpolation (approximated by a few sampled \u03bb points). Study whether this accelerates entry into the stable-to-noise, linearly connected region and improves generalization or calibration, compared to standard sharpness/flatness regularizers.",
        "Permutation-Equivariant Federated Aggregation with Differentiable Sinkhorn\nReplace FedAvg\u2019s raw weight averaging with a pipeline that (i) aligns client models to a server reference via Sinkhorn re-basin (differentiable, objective-driven) and (ii) performs OT-informed fusion. Benchmark under extreme non-i.i.d. partitions and heterogeneous widths, measuring not only accuracy but also post-aggregation connectivity (barrier) to each client solution as a diagnostic of \u201cmerge quality.\u201d",
        "Connectivity-Based Model Compression: Fuse Wider-to-Narrower Networks While Preserving Linear Paths\nExtend OT fusion\u2019s \u201cdifferent widths\u201d capability by selecting a narrow target model and optimizing the transport plan to minimize (a) output discrepancy and (b) the interpolation barrier between the fused narrow model and each source wide model after alignment. This yields a practical compression method where the compressed model remains in a linearly connected region to the original(s), enabling safer fine-tuning and better retention.",
        "Curvature-Aware Re-Basin Costs: Using Hessian Information to Align Neurons\nDesign new Sinkhorn/OT cost functions that incorporate curvature (e.g., per-neuron Fisher/Hessian-diagonal or sensitivity measures) rather than only weight/activation similarity. Test whether curvature-aware alignment more reliably eliminates interpolation barriers (especially in deeper layers) and improves zero-shot fusion/continual learning performance by matching neurons that play similar roles in the loss landscape."
      ],
      "generated_ideas_raw": "1. **Stability-to-SGD Noise as an Early-Stopping Criterion for Lottery Tickets**\n   Use instability analysis (linear interpolation barrier across SGD noise seeds) as a measurable trigger to stop dense pretraining and begin IMP-style pruning exactly when training enters the \u201cstable\u201d region. Evaluate whether pruning at this stability onset yields higher-sparsity matching subnetworks and reduces total compute versus fixed-percentage warmup heuristics on CIFAR/ImageNet-scale models.\n\n2. **Hessian-Outlier Tracking to Predict Linear Mode Connectivity Onset**\n   Hypothesize that the emergence/stabilization of LMC corresponds to a characteristic evolution of the Hessian spectrum (e.g., outlier magnitudes plateau while the bulk remains near zero). Build an online estimator (Lanczos/trace methods) that monitors top Hessian eigenvalues during training and predicts the step when models become linearly connected under SGD noise, then validate against direct interpolation-barrier measurements.\n\n3. **Permutation-Aware Instability Analysis (Rebasin-then-Interpolate)**\n   Extend Frankle et al.\u2019s instability analysis by first applying permutation alignment (weight matching / Sinkhorn re-basin) between two runs before measuring linear interpolation barriers. Quantify how much of \u201cinstability to SGD noise\u201d is explained by permutation symmetries vs. genuinely different basins, across architectures (CNNs, ResNets, ViTs) and training regimes (augmentations, label noise).\n\n4. **Joint Optimal-Transport Fusion + Connectivity-Constrained Fine-Tuning**\n   Combine OT-based model fusion (layer-wise neuron alignment) with an explicit penalty that enforces low loss along the interpolation path between source models and the fused model (a \u201cconnectivity regularizer\u201d). Test whether this yields a one-shot merged model that not only matches ensemble accuracy but is also provably in the same linearly connected region as each participant, improving robustness of federated averaging under non-i.i.d. data.\n\n5. **Connectivity-Preserving Pruning via Sinkhorn Alignment Between Dense and Sparse Models**\n   During IMP, re-basin the pruned subnetwork to the dense network (or vice versa) using differentiable Sinkhorn permutations at each pruning round, and select masks that minimize the post-alignment interpolation barrier. This turns \u201cmatching subnetworks\u201d into \u201clinearly connected subnetworks,\u201d aiming for higher reproducibility across seeds and better transfer when training sparse models in isolation.\n\n6. **Data-Distribution Controls on LMC: Manipulating Hessian Outliers via Synthetic Cluster Structure**\n   Using Sagun et al.\u2019s claim that data complexity affects Hessian outliers, systematically vary dataset cluster structure/separability (synthetic mixtures, controlled label noise) and measure resulting changes in (a) Hessian outliers and (b) linear interpolation barriers before/after permutation alignment. The contribution is an empirical map linking data geometry \u2192 curvature outliers \u2192 (permutation-aware) mode connectivity, clarifying when barriers are fundamentally data-driven.\n\n7. **Barrier-Minimizing Training: Add a \u201cCross-Seed Linear Path\u201d Regularizer**\n   Train two replicas from the same initialization under different SGD noise (as in instability analysis) and add a differentiable penalty for maximal loss along their linear interpolation (approximated by a few sampled \u03bb points). Study whether this accelerates entry into the stable-to-noise, linearly connected region and improves generalization or calibration, compared to standard sharpness/flatness regularizers.\n\n8. **Permutation-Equivariant Federated Aggregation with Differentiable Sinkhorn**\n   Replace FedAvg\u2019s raw weight averaging with a pipeline that (i) aligns client models to a server reference via Sinkhorn re-basin (differentiable, objective-driven) and (ii) performs OT-informed fusion. Benchmark under extreme non-i.i.d. partitions and heterogeneous widths, measuring not only accuracy but also post-aggregation connectivity (barrier) to each client solution as a diagnostic of \u201cmerge quality.\u201d\n\n9. **Connectivity-Based Model Compression: Fuse Wider-to-Narrower Networks While Preserving Linear Paths**\n   Extend OT fusion\u2019s \u201cdifferent widths\u201d capability by selecting a narrow target model and optimizing the transport plan to minimize (a) output discrepancy and (b) the interpolation barrier between the fused narrow model and each source wide model after alignment. This yields a practical compression method where the compressed model remains in a linearly connected region to the original(s), enabling safer fine-tuning and better retention.\n\n10. **Curvature-Aware Re-Basin Costs: Using Hessian Information to Align Neurons**\n   Design new Sinkhorn/OT cost functions that incorporate curvature (e.g., per-neuron Fisher/Hessian-diagonal or sensitivity measures) rather than only weight/activation similarity. Test whether curvature-aware alignment more reliably eliminates interpolation barriers (especially in deeper layers) and improves zero-shot fusion/continual learning performance by matching neurons that play similar roles in the loss landscape.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Stability-to-SGD Noise as an Early-Stopping Criterion for Lottery Tickets\nUse instability analysis (linear interpolation barrier across SGD noise seeds) as a measurable trigger to stop dense pretraini",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hessian-Outlier Tracking to Predict Linear Mode Connectivity Onset\nHypothesize that the emergence/stabilization of LMC corresponds to a characteristic evolution of the Hessian spectrum (e.g., outlier ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Permutation-Aware Instability Analysis (Rebasin-then-Interpolate)\nExtend Frankle et al.\u2019s instability analysis by first applying permutation alignment (weight matching / Sinkhorn re-basin) between two",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Joint Optimal-Transport Fusion + Connectivity-Constrained Fine-Tuning\nCombine OT-based model fusion (layer-wise neuron alignment) with an explicit penalty that enforces low loss along the interpolatio",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Connectivity-Preserving Pruning via Sinkhorn Alignment Between Dense and Sparse Models\nDuring IMP, re-basin the pruned subnetwork to the dense network (or vice versa) using differentiable Sinkhorn per",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Data-Distribution Controls on LMC: Manipulating Hessian Outliers via Synthetic Cluster Structure\nUsing Sagun et al.\u2019s claim that data complexity affects Hessian outliers, systematically vary dataset c",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Barrier-Minimizing Training: Add a \u201cCross-Seed Linear Path\u201d Regularizer\nTrain two replicas from the same initialization under different SGD noise (as in instability analysis) and add a differentiable ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Permutation-Equivariant Federated Aggregation with Differentiable Sinkhorn\nReplace FedAvg\u2019s raw weight averaging with a pipeline that (i) aligns client models to a server reference via Sinkhorn re-bas",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Connectivity-Based Model Compression: Fuse Wider-to-Narrower Networks While Preserving Linear Paths\nExtend OT fusion\u2019s \u201cdifferent widths\u201d capability by selecting a narrow target model and optimizing t",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Curvature-Aware Re-Basin Costs: Using Hessian Information to Align Neurons\nDesign new Sinkhorn/OT cost functions that incorporate curvature (e.g., per-neuron Fisher/Hessian-diagonal or sensitivity mea",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 56,
      "paper_title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model",
      "contribution": "OpenHOI introduces the first framework for synthesizing open-world hand-object interactions using multimodal large language models.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 2,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11163,
      "output_tokens": 948,
      "predecessor_details": [
        {
          "success": true,
          "title": "Text-guided 3D Motion Generation for Hand-Object Interaction - arXiv",
          "url": "https://arxiv.org/abs/2404.00562",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2404.00562** (cs)\n\n\\[Submitted on 31 Mar 2024 ( [v1](https://arxiv.org/abs/2404.00562v1)), last revised 2 Apr 2024 (this version, v2)\\]\n\n# Title:Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction\n\nAuthors: [Junuk Cha](https://arxiv.org/search/cs?searchtype=author&query=Cha,+J), [Jihyeon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim,+J), [Jae Shin Yoon](https://arxiv.org/search/cs?searchtype=author&query=Yoon,+J+S), [Seungryul Baek](https://arxiv.org/search/cs?searchtype=author&query=Baek,+S)\n\nView a PDF of the paper titled Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction, by Junuk Cha and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2404.00562) [HTML (experimental)](https://arxiv.org/html/2404.00562v2)\n\n> Abstract:This paper introduces the first text-guided work for generating the sequence of hand-object interaction in 3D. The main challenge arises from the lack of labeled data where existing ground-truth datasets are nowhere near generalizable in interaction type and object category, which inhibits the modeling of diverse 3D hand-object interaction with the correct physical implication (e.g., contacts and semantics) from text prompts. To address this challenge, we propose to decompose the interaction generation task into two subtasks: hand-object contact generation; and hand-object motion generation. For contact generation, a VAE-based network takes as input a text and an object mesh, and generates the probability of contacts between the surfaces of hands and the object during the interaction. The network learns a variety of local geometry structure of diverse objects that is independent of the objects' category, and thus, it is applicable to general objects. For motion generation, a Transformer-based diffusion model utilizes this 3D contact map as a strong prior for generating physically plausible hand-object motion as a function of text prompts by learning from the augmented labeled dataset; where we annotate text labels from many existing 3D hand and object motion data. Finally, we further introduce a hand refiner module that minimizes the distance between the object surface and hand joints to improve the temporal stability of the object-hand contacts and to suppress the penetration artifacts. In the experiments, we demonstrate that our method can generate more realistic and diverse interactions compared to other baseline methods. We also show that our method is applicable to unseen objects. We will release our model and newly labeled data as a strong foundation for future research. Codes and data are available in: [this https URL](https://github.com/JunukCha/Text2HOI).\n\n| | |\n| --- | --- |\n| Comments: | Accepted to CVPR 2024 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2404.00562](https://arxiv.org/abs/2404.00562) \\[cs.CV\\] |\n| (or [arXiv:2404.00562v2](https://arxiv.org/abs/2404.00562v2) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2404.00562](https://doi.org/10.48550/arXiv.2404.00562) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Junuk Cha \\[ [view email](https://arxiv.org/show-email/14b1730a/2404.00562)\\] **[\\[v1\\]](https://arxiv.org/abs/2404.00562v1)**\nSun, 31 Mar 2024 04:56:30 UTC (40,787 KB)\n**\\[v2\\]**\nTue, 2 Apr 2024 02:08:55 UTC (40,787 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction, by Junuk Cha and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2404.00562)\n- [HTML (experimental)](https://arxiv.org/html/2404.00562v2)\n- [TeX Source](https://arxiv.org/src/2404.00562)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2404.00562&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2404.00562&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-04](https://arxiv.org/list/cs.CV/2024-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2404.00562?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2404.00562)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00562)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2404.00562)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2404.00562) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction",
          "cleaned_query": "Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "DiffH2O: Diffusion-Based Synthesis of Hand-Object ...",
          "url": "https://arxiv.org/abs/2403.17827",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2403.17827** (cs)\n\n\\[Submitted on 26 Mar 2024 ( [v1](https://arxiv.org/abs/2403.17827v1)), last revised 23 Dec 2024 (this version, v2)\\]\n\n# Title:DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions\n\nAuthors: [Sammy Christen](https://arxiv.org/search/cs?searchtype=author&query=Christen,+S), [Shreyas Hampali](https://arxiv.org/search/cs?searchtype=author&query=Hampali,+S), [Fadime Sener](https://arxiv.org/search/cs?searchtype=author&query=Sener,+F), [Edoardo Remelli](https://arxiv.org/search/cs?searchtype=author&query=Remelli,+E), [Tomas Hodan](https://arxiv.org/search/cs?searchtype=author&query=Hodan,+T), [Eric Sauser](https://arxiv.org/search/cs?searchtype=author&query=Sauser,+E), [Shugao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+S), [Bugra Tekin](https://arxiv.org/search/cs?searchtype=author&query=Tekin,+B)\n\nView a PDF of the paper titled DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions, by Sammy Christen and Shreyas Hampali and Fadime Sener and Edoardo Remelli and Tomas Hodan and Eric Sauser and Shugao Ma and Bugra Tekin\n\n[View PDF](https://arxiv.org/pdf/2403.17827) [HTML (experimental)](https://arxiv.org/html/2403.17827v2)\n\n> Abstract:Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. In this paper, we propose a novel method, dubbed DiffH2O, which can synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and an text-based manipulation stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the manipulation phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses and helps in generating realistic hand-object interactions. Third, we propose two different guidance schemes to allow more control of the generated motions: grasp guidance and detailed textual guidance. Grasp guidance takes a single target grasping pose and guides the diffusion model to reach this grasp at the end of the grasping stage, which provides control over the grasping pose. Given a grasping motion from this stage, multiple different actions can be prompted in the manipulation phase. For the textual guidance, we contribute comprehensive text descriptions to the GRAB dataset and show that they enable our method to have more fine-grained control over hand-object interactions. Our quantitative and qualitative evaluation demonstrates that the proposed method outperforms baseline methods and leads to natural hand-object motions.\n\n| | |\n| --- | --- |\n| Comments: | Project Page: [this https URL](https://diffh2o.github.io/) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2403.17827](https://arxiv.org/abs/2403.17827) \\[cs.CV\\] |\n| | (or [arXiv:2403.17827v2](https://arxiv.org/abs/2403.17827v2) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2403.17827](https://doi.org/10.48550/arXiv.2403.17827) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | SIGGRAPH Asia Conference Papers, Article 145, 2024 |\n| Related DOI: | [https://doi.org/10.1145/3680528.3687563](https://doi.org/10.1145/3680528.3687563) Focus to learn more DOI(s) linking to related resources |\n\n## Submission history\n\nFrom: Sammy Christen \\[ [view email](https://arxiv.org/show-email/3fd16b00/2403.17827)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2403.17827v1)**\nTue, 26 Mar 2024 16:06:42 UTC (24,893 KB)\n\n**\\[v2\\]**\nMon, 23 Dec 2024 17:36:22 UTC (28,773 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions, by Sammy Christen and Shreyas Hampali and Fadime Sener and Edoardo Remelli and Tomas Hodan and Eric Sauser and Shugao Ma and Bugra Tekin\n\n- [View PDF](https://arxiv.org/pdf/2403.17827)\n- [HTML (experimental)](https://arxiv.org/html/2403.17827v2)\n- [TeX Source](https://arxiv.org/src/2403.17827)\n- [Other Formats](https://arxiv.org/format/2403.17827)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2403.17827&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2403.17827&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-03](https://arxiv.org/list/cs.CV/2024-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2403.17827?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2403.17827?context=cs.AI)\n\n[cs.GR](https://arxiv.org/abs/2403.17827?context=cs.GR)\n\n[cs.LG](https://arxiv.org/abs/2403.17827?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2403.17827)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17827)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2403.17827)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2403.17827&description=DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2403.17827&title=DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2403.17827) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "DiffH2O: Diffusion-based synthesis of hand-object interactions from textual descriptions",
          "cleaned_query": "DiffH2O: Diffusion-based synthesis of hand-object interactions from textual descriptions",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "HOIGPT: Learning Long Sequence Hand-Object Interaction ... - arXiv",
          "url": "https://arxiv.org/abs/2503.19157",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2503.19157** (cs)\n\n\\[Submitted on 24 Mar 2025\\]\n\n# Title:HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models\n\nAuthors: [Mingzhen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+M), [Fu-Jen Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu,+F), [Bugra Tekin](https://arxiv.org/search/cs?searchtype=author&query=Tekin,+B), [Kevin J Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang,+K+J), [Haoyu Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+H), [Weiyao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+W), [Xingyu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+X), [Pierre Gleize](https://arxiv.org/search/cs?searchtype=author&query=Gleize,+P), [Hongfei Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue,+H), [Siwei Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu,+S), [Kris Kitani](https://arxiv.org/search/cs?searchtype=author&query=Kitani,+K), [Matt Feiszli](https://arxiv.org/search/cs?searchtype=author&query=Feiszli,+M), [Hao Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+H)\n\nView a PDF of the paper titled HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models, by Mingzhen Huang and 12 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.19157) [HTML (experimental)](https://arxiv.org/html/2503.19157v1)\n\n> Abstract:We introduce HOIGPT, a token-based generative method that unifies 3D hand-object interactions (HOI) perception and generation, offering the first comprehensive solution for captioning and generating high-quality 3D HOI sequences from a diverse range of conditional signals (\\\\eg text, objects, partial sequences). At its core, HOIGPT utilizes a large language model to predict the bidrectional transformation between HOI sequences and natural language descriptions. Given text inputs, HOIGPT generates a sequence of hand and object meshes; given (partial) HOI sequences, HOIGPT generates text descriptions and completes the sequences. To facilitate HOI understanding with a large language model, this paper introduces two key innovations: (1) a novel physically grounded HOI tokenizer, the hand-object decomposed VQ-VAE, for discretizing HOI sequences, and (2) a motion-aware language model trained to process and generate both text and HOI tokens. Extensive experiments demonstrate that HOIGPT sets new state-of-the-art performance on both text generation (+2.01% R Precision) and HOI generation (-2.56 FID) across multiple tasks and benchmarks.\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2503.19157](https://arxiv.org/abs/2503.19157) \\[cs.CV\\] |\n| | (or [arXiv:2503.19157v1](https://arxiv.org/abs/2503.19157v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2503.19157](https://doi.org/10.48550/arXiv.2503.19157) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Mingzhen Huang \\[ [view email](https://arxiv.org/show-email/fa7f401d/2503.19157)\\]\n\n**\\[v1\\]**\nMon, 24 Mar 2025 21:25:29 UTC (2,367 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models, by Mingzhen Huang and 12 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.19157)\n- [HTML (experimental)](https://arxiv.org/html/2503.19157v1)\n- [TeX Source](https://arxiv.org/src/2503.19157)\n- [Other Formats](https://arxiv.org/format/2503.19157)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2503.19157&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2503.19157&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2025-03](https://arxiv.org/list/cs.CV/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.19157?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.19157)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.19157)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.19157)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2503.19157&description=HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2503.19157&title=HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.19157) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models",
          "cleaned_query": "HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Universal 3D Object Understanding for Embodied Interaction - arXiv",
          "url": "https://arxiv.org/abs/2402.17766",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2402.17766** (cs)\n\n\\[Submitted on 27 Feb 2024 ( [v1](https://arxiv.org/abs/2402.17766v1)), last revised 12 Jul 2024 (this version, v3)\\]\n\n# Title:ShapeLLM: Universal 3D Object Understanding for Embodied Interaction\n\nAuthors: [Zekun Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi,+Z), [Runpei Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong,+R), [Shaochen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Haoran Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng,+H), [Chunrui Han](https://arxiv.org/search/cs?searchtype=author&query=Han,+C), [Zheng Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge,+Z), [Li Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi,+L), [Kaisheng Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+K)\n\nView a PDF of the paper titled ShapeLLM: Universal 3D Object Understanding for Embodied Interaction, by Zekun Qi and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2402.17766) [HTML (experimental)](https://arxiv.org/html/2402.17766v3)\n\n> Abstract:This paper presents ShapeLLM, the first 3D Multimodal Large Language Model (LLM) designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages. ShapeLLM is built upon an improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view image distillation for enhanced geometry understanding. By utilizing ReCon++ as the 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed instruction-following data and tested on our newly human-curated benchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3D geometry understanding and language-unified 3D interaction tasks, such as embodied visual grounding. Project page: [this https URL](https://qizekun.github.io/shapellm/)\n\n| | |\n| --- | --- |\n| Comments: | Accepted at ECCV 2024 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2402.17766](https://arxiv.org/abs/2402.17766) \\[cs.CV\\] |\n| (or [arXiv:2402.17766v3](https://arxiv.org/abs/2402.17766v3) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2402.17766](https://doi.org/10.48550/arXiv.2402.17766) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Runpei Dong \\[ [view email](https://arxiv.org/show-email/3f14c175/2402.17766)\\] **[\\[v1\\]](https://arxiv.org/abs/2402.17766v1)**\nTue, 27 Feb 2024 18:57:12 UTC (7,432 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2402.17766v2)**\nWed, 6 Mar 2024 15:11:37 UTC (7,420 KB)\n**\\[v3\\]**\nFri, 12 Jul 2024 15:36:15 UTC (9,266 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled ShapeLLM: Universal 3D Object Understanding for Embodied Interaction, by Zekun Qi and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2402.17766)\n- [HTML (experimental)](https://arxiv.org/html/2402.17766v3)\n- [TeX Source](https://arxiv.org/src/2402.17766)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2402.17766&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2402.17766&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-02](https://arxiv.org/list/cs.CV/2024-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2402.17766?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.17766)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17766)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.17766)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2402.17766) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Shapellm: Universal 3d object understanding for embodied interaction",
          "cleaned_query": "Shapellm: Universal 3d object understanding for embodied interaction",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "3D-AffordanceLLM: Harnessing Large Language Models for ... - arXiv",
          "url": "https://arxiv.org/html/2502.20041",
          "content": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds\n# 3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds\nHengshuo Chu1, Xiang Deng\u2020\u2020{\\\\dagger}\u20201, Qi Lv1, Xiaoyang Chen1,Yinchuan Li2, Jianye Hao2, Liqiang Nie\u2020\u2020{\\\\dagger}\u20201\n1Harbin Institute of Technology (Shenzhen),2Huawei Noah\u2019s Ark Lab\n###### Abstract\n3D Affordance detection is a challenging problem with broad applications on various robotic tasks.\nExisting methods typically formulate the detection paradigm as a label-based semantic segmentation task.\nThis paradigm relies on predefined labels and lacks the ability to comprehend complex natural language, resulting in limited generalization in open-world scene.\nTo address these limitations, we reformulate the traditional affordance detection paradigm intoInstruction Reasoning Affordance Segmentation(IRAS) task.\nThis task is designed to output a affordance mask region given a query reasoning text, which avoids fixed categories of input labels.\nWe accordingly propose the3D-AffordanceLLM(3D-ADLLM), a framework designed for reasoning affordance detection in 3D open-scene.\nSpecifically, 3D-ADLLM introduces large language models (LLMs) to 3D affordance perception with a custom-designed decoder for generating affordance masks, thus achieving open-world reasoning affordance detection.\nIn addition, given the scarcity of 3D affordance datasets for training large models, we seek to extract knowledge from general segmentation data and transfer it to affordance detection.\nThus, we propose a multi-stage training strategy that begins with a novel pre-training task, i.e.,Referring Object Part Segmentation(ROPS).\nThis stage is designed to equip the model with general recognition and segmentation capabilities at the object-part level.\nThen followed by fine-tuning with the IRAS task, 3D-ADLLM obtains the reasoning ability for affordance detection.\nIn summary, 3D-ADLLM leverages the rich world knowledge and human-object interaction reasoning ability of LLMs, achieving approximately an 8% improvement in mIoU on open-vocabulary affordance detection tasks.\n11footnotetext:Corresponding authors\n## 1Introduction\nRobots are increasingly integrating into various aspects of our daily life> (Matheson et\u00a0al., [> 2019\n](https://arxiv.org/html/2502.20041v3#bib.bib23)> )\n.\nAs we progress toward developing the next generation of more advanced robotic agents, it is essential to enable robots to comprehend natural language instructions within context and to perceive task-relevant information in their surroundings.\nThis skill is particularly vital for seamless interactions in unstructured environments, such as homes, where adaptability to diverse situations is crucial.\nSpecifically, the robots need to not only identify the objects in the environments but also locate the specific regions of each object that are suitable for interaction:affordance.\nThe concept of affordance was introduced by ecological psychologist James Gibson> (Gibson, [> 1966\n](https://arxiv.org/html/2502.20041v3#bib.bib10)> )\nand has since played a significant role in various robotic applications, including object recognition> (Hong et\u00a0al., [> 2023a\n](https://arxiv.org/html/2502.20041v3#bib.bib11)> ; Hou et\u00a0al., [> 2021\n](https://arxiv.org/html/2502.20041v3#bib.bib13)> )\n, action anticipation> (Roy &amp; Fernando, [> 2021\n](https://arxiv.org/html/2502.20041v3#bib.bib37)> )\n, agent activity recognition> (Chen et\u00a0al., [> 2023\n](https://arxiv.org/html/2502.20041v3#bib.bib2)> )\n, and object functionality understanding> (Li et\u00a0al., [> 2023\n](https://arxiv.org/html/2502.20041v3#bib.bib18)> )\n.\nAffordance describes potential interactions between robots and their environment, such as using a knife\u2019s blade for cutting tasks. Detecting affordances is challenging due to object diversity and complexity> (Min et\u00a0al., [> 2016\n](https://arxiv.org/html/2502.20041v3#bib.bib26)> )\n.\nTraditionally, 2D images and CNNs are\nused> (Nguyen et\u00a0al., [> 2016\n](https://arxiv.org/html/2502.20041v3#bib.bib31)> ; Do et\u00a0al., [> 2018\n](https://arxiv.org/html/2502.20041v3#bib.bib8)> ; Pacheco-Ortega &amp; Mayol-Cuervas, [> 2022\n](https://arxiv.org/html/2502.20041v3#bib.bib33)> )\n> (Krizhevsky et\u00a0al., [> 2012\n](https://arxiv.org/html/2502.20041v3#bib.bib16)> )\n, but 2D information lacks the depth necessary for precise manipulation, necessitating 3D transformations> (Deng et\u00a0al., [> 2021\n](https://arxiv.org/html/2502.20041v3#bib.bib7)> )\n.\nWith advanced depth cameras, 3D point clouds have become a widely used modality in robotic applications> (Liu et\u00a0al., [> 2019\n](https://arxiv.org/html/2502.20041v3#bib.bib20)> )\n. Unlike conventional images, 3D point clouds offer robots direct and detailed 3D information about surrounding objects and environments.\nHence, the 3D affordance detection has been deemed as a critical step in bridging perception and manipulation in the physical world for an embodied agent, thus has\nshown substantial impact on practical applications such as robotic manipulation> (Geng et\u00a0al., [> 2023\n](https://arxiv.org/html/2502.20041v3#bib.bib9)> ; Moldovan et\u00a0al., [> 2012\n](https://arxiv.org/html/2502.20041v3#bib.bib29)> )\n.\nWhile current approaches are limited by fixed label sets> (Deng et\u00a0al., [> 2021\n](https://arxiv.org/html/2502.20041v3#bib.bib7)> ; Mo et\u00a0al., [> 2022\n](https://arxiv.org/html/2502.20041v3#bib.bib28)> )\n, reducing flexibility and generalization in dynamic settings.\nTo overcome the fixed label set problem in affordance detection, Nguyen et al.> (Nguyen et\u00a0al., [> 2023\n](https://arxiv.org/html/2502.20041v3#bib.bib32)> )\nhave incorporated a text encoder to enable models to handle certain levels of open-vocabulary detection, but these algorithms still rely on a classification based training paradigm.\nAs a result, they lack the ability for rapid and continuous learning when presented with new affordance label data. Furthermore, current affordance detection methods also heavily rely on the predefined labels and lack the ability to understand and reason over long contextual text.\nAdditionally, the scarcity of 3D affordance datasets> (Deng et\u00a0al., [> 2021\n](https://arxiv.org/html/2502.20041v3#bib.bib7)> ; Nguyen et\u00a0al., [> 2023\n](https://arxiv.org/html/2502.20041v3#bib.bib32)> )\nconstrains the effective training of large-scale models.\nTowards these issues, we redefine the 3D affordance detection as anInstruction Reasoning Affordance Segmentation(IRAS) task and accordingly propose3D-AffordanceLLM(3D-ADLLM).\nThe IRAS task is designed to output an affordance mask region in response to complex, reasoning-based query text, overcoming the limitations of fixed affordance labels and the difficulty of understanding complex instructions.\nOur 3D-ADLLM framework introduces large language models (LLMs) to 3D affordance perception with a specifically designed decoder for generating affordance masks, thus achieving open-world reasoning affordance detection.\nSpecifically, we introduce an additional token,&lt;AFF&gt;, into the original LLM vocabulary. When the&lt;AFF&gt;token is generated, its hidden embedding is further decoded into the corresponding segmentation mask.\nBy representing the segmentation mask as an embedding, 3D-ADLLM not only gains segmentation capability but also benefits from end-to-end training.\nHowever, due to the scarcity of 3D affordance datasets for training large models, we propose a multi-stage training strategy to extract knowledge from general segmentation data and transfer it to affordance detection.\nThis process involves pre-training on PartNet> (Mo et\u00a0al., [> 2019\n](https://arxiv.org/html/2502.20041v3#bib.bib27)> )\nwithReferring Object Part Segmentation(ROPS) tasks to acquire the object-part level general recognition and segmentation knowledge.\nSubsequently, we fine-tune the model with the IRAS task to achieve context-aware reasoning ability and robust performance in open-set zero-shot affordance detection.\nOur main contributions are summarized as follows:\n* \u2022Different from the existing affordance detection methods that rely on fixed sets of labels, we address this limitation by introducing a new detection paradigm based on theInstruction Reasoning Affordance Segmentation(IRAS) task.\nBy reforming the label-based semantic segmentation task in the traditional affordance detection paradigm into a natural language-driven reasoning affordance segmentation task, our model enables more flexible and context-aware reasoning, facilitating effective zero-shot learning capabilities.\n* \u2022To address the IRAS tasks driven by\nsemantic complex natural language, we consequently propose the3D AffordanceLLM(3D-ADLLM) model, combining a large language model (LLM) with a carefully designed Affordance Decoder.\nOur 3D-ADLLM framework can understand semantically-rich, long-context instructions and leverages the LLM\u2019s world knowledge for superior open-vocabulary affordance detection.\n* \u2022Due to the scarcity of 3D affordance datasets for training large models, we propose a multi-stage training strategy to transfer general segmentation knowledge into affordance detection.\nFirst, the model is equipped with general recognition and segmentation knowledge through a novel pretraining task, i.e., the Referring Object Part Segmentation (ROPS).\nSubsequently, the model is fine-tuned with the IRAS task to handle context-aware reasoning and\naffordance region prediction.\n## 2Related Work\nAffordance Detection.Originating from the 2D domain, initial work in affordance detection primarily focused on identifying objects with affordances> (Do et\u00a0al., [> 2018\n](https://arxiv.org/html/2502.20041v3#bib.bib8)> )\n.\nBuilding on this foundation, later studies> (Lu et\u00a0al., [> 2022\n](https://arxiv.org/html/2502.20041v3#bib.bib21)> )\nintroduced linguistic descriptions to improve detection, but they continued to emphasize object-level affordances, lacking fine-grained analysis.\nAddressing this problem, subsequent research> (Che",
          "original_query": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds",
          "cleaned_query": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "Gaze-guided Hand-Object Interaction Synthesis: Dataset ...",
          "url": "https://arxiv.org/abs/2403.16169",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2403.16169** (cs)\n\n\\[Submitted on 24 Mar 2024 ( [v1](https://arxiv.org/abs/2403.16169v1)), last revised 7 Jan 2025 (this version, v5)\\]\n\n# Title:Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method\n\nAuthors: [Jie Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian,+J), [Ran Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji,+R), [Lingxiao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+L), [Suting Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni,+S), [Yuexin Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+Y), [Lan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+L), [Jingyi Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+J), [Ye Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi,+Y), [Jingya Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J)\n\nView a PDF of the paper titled Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method, by Jie Tian and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/2403.16169) [HTML (experimental)](https://arxiv.org/html/2403.16169v5)\n\n> Abstract:Gaze plays a crucial role in revealing human attention and intention, particularly in hand-object interaction scenarios, where it guides and synchronizes complex tasks that require precise coordination between the brain, hand, and object. Motivated by this, we introduce a novel task: Gaze-Guided Hand-Object Interaction Synthesis, with potential applications in augmented reality, virtual reality, and assistive technologies. To support this task, we present GazeHOI, the first dataset to capture simultaneous 3D modeling of gaze, hand, and object interactions. This task poses significant challenges due to the inherent sparsity and noise in gaze data, as well as the need for high consistency and physical plausibility in generating hand and object motions. To tackle these issues, we propose a stacked gaze-guided hand-object interaction diffusion model, named GHO-Diffusion. The stacked design effectively reduces the complexity of motion generation. We also introduce HOI-Manifold Guidance during the sampling stage of GHO-Diffusion, enabling fine-grained control over generated motions while maintaining the data manifold. Additionally, we propose a spatial-temporal gaze feature encoding for the diffusion condition and select diffusion results based on consistency scores between gaze-contact maps and gaze-interaction trajectories. Extensive experiments highlight the effectiveness of our method and the unique contributions of our dataset. More details in [this https URL](https://takiee.github.io/gaze-hoi/).\n\n| | |\n| --- | --- |\n| Comments: | Project Page: [this https URL](https://takiee.github.io/gaze-hoi/) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2403.16169](https://arxiv.org/abs/2403.16169) \\[cs.CV\\] |\n| (or [arXiv:2403.16169v5](https://arxiv.org/abs/2403.16169v5) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2403.16169](https://doi.org/10.48550/arXiv.2403.16169) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jie Tian \\[ [view email](https://arxiv.org/show-email/173c4b3e/2403.16169)\\] **[\\[v1\\]](https://arxiv.org/abs/2403.16169v1)**\nSun, 24 Mar 2024 14:24:13 UTC (2,356 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2403.16169v2)**\nTue, 26 Mar 2024 06:39:30 UTC (2,807 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2403.16169v3)**\nThu, 28 Mar 2024 06:56:45 UTC (2,803 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2403.16169v4)**\nThu, 22 Aug 2024 13:51:34 UTC (3,677 KB)\n**\\[v5\\]**\nTue, 7 Jan 2025 14:39:31 UTC (3,753 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method, by Jie Tian and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/2403.16169)\n- [HTML (experimental)](https://arxiv.org/html/2403.16169v5)\n- [TeX Source](https://arxiv.org/src/2403.16169)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2403.16169&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2403.16169&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-03](https://arxiv.org/list/cs.CV/2024-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2403.16169?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2403.16169)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2403.16169)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2403.16169)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2403.16169) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method",
          "cleaned_query": "Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Physically Grounded HOI Tokenizer with Explicit Contact/Force Channels\n- Improve HOIGPT\u2019s hand-object decomposed VQ-VAE tokenizer by adding discrete channels for contact state transitions (attach/detach), frictional slip, and approximate force direction inferred from motion/contact. Train the tokenizer to preserve these invariants so the language model can generate sequences with more stable contacts and fewer physically impossible manipulations.",
        "Closed-Loop HOI Control: Sequence Completion from Streaming Observations\n- Build on HOIGPT\u2019s partial-sequence completion to create a closed-loop predictor that continuously re-plans the next seconds of HOI given streaming hand/object state (and optional gaze) plus text goal. Train with trajectory perturbations and recovery objectives so the model learns to correct mistakes (missed grasp, object slip) rather than only generating open-loop motions."
      ],
      "generated_ideas_raw": "1. **Affordance-to-Contact Priors for Open-Vocabulary Text2HOI**\n   - Combine 3D-AffordanceLLM\u2019s instruction-conditioned affordance masks with Text2HOI\u2019s contact-map VAE by converting predicted affordance regions into probabilistic hand\u2013object contact priors. Train a joint model that aligns language \u2192 affordance regions \u2192 contact distributions, improving generalization to novel objects and uncommon actions with minimal HOI labels.\n\n2. **Language-Editable HOI via Contact-Map \u201cDiff-Edits\u201d**\n   - Extend Text2HOI/DiffH2O with an editing interface where users modify interactions by editing contact maps (e.g., \u201cuse fingertip instead of palm,\u201d \u201cavoid touching the blade\u201d) and re-sampling only the motion segment affected by those contacts. Implement localized diffusion in time and space (contact-conditioned inpainting) to preserve the rest of the trajectory while changing grasp/manipulation semantics.\n\n3. **Hierarchical Long-Horizon HOI Planning: LLM Tokens \u2192 Diffusion Motion**\n   - Use HOIGPT to generate a high-level token plan for long tasks (e.g., \u201cgrasp \u2192 lift \u2192 rotate \u2192 place\u201d), then instantiate each sub-skill with DiffH2O\u2019s staged diffusion (grasp stage + manipulation stage). Add a consistency loss ensuring adjacent skills\u2019 terminal contacts/poses match, yielding coherent multi-minute interaction sequences rather than single-action clips.\n\n4. **Gaze-and-Text Joint Guidance for Intention-Consistent HOI Synthesis**\n   - Fuse GazeHOI\u2019s spatiotemporal gaze encoding with text prompts to create a multi-condition guidance scheme: gaze guides \u201cwhat/where\u201d and text guides \u201cwhy/how.\u201d Add a cross-modal consistency objective between predicted gaze-contact maps (from gaze) and generated physical contacts (from Text2HOI/DiffH2O) to reduce semantically implausible hand placements.\n\n5. **Uncertainty-Aware Contact Prediction for Unseen Objects**\n   - Augment Text2HOI\u2019s contact VAE to output calibrated uncertainty (e.g., via ensembles or diffusion over contact maps) and propagate that uncertainty into motion generation as stochastic constraints. Evaluate whether uncertainty-guided sampling reduces penetration/contact jitter on out-of-distribution meshes and supports safer downstream robotics use.\n\n6. **Physically Grounded HOI Tokenizer with Explicit Contact/Force Channels**\n   - Improve HOIGPT\u2019s hand-object decomposed VQ-VAE tokenizer by adding discrete channels for contact state transitions (attach/detach), frictional slip, and approximate force direction inferred from motion/contact. Train the tokenizer to preserve these invariants so the language model can generate sequences with more stable contacts and fewer physically impossible manipulations.\n\n7. **ShapeLLM-Driven HOI Generation from Partial Geometry (Single-View/Partial Point Cloud)**\n   - Replace the \u201cfull object mesh\u201d assumption in Text2HOI/DiffH2O by conditioning on ShapeLLM\u2019s ReCon++ features from partial point clouds (as in real robot perception). Train with synthetic occlusions and partial scans so the model can still generate plausible contacts and motions when object geometry is incomplete or noisy.\n\n8. **Counterfactual HOI Dataset Synthesis via Affordance Swaps**\n   - Create a new benchmark by generating counterfactual interactions: keep the same text action but swap object affordance structure (e.g., \u201cpour\u201d with containers of varying spouts/handles) using 3D-AffordanceLLM masks as controllable variables. Use this dataset to measure causal reliance on geometry vs. language and to train models that adapt contacts when affordances change.\n\n9. **Two-Hand Coordination Constraints for Bimanual Manipulation with Role Assignment**\n   - Extend DiffH2O\u2019s one/two-handed synthesis by explicitly modeling bimanual roles (stabilizing hand vs. manipulating hand) predicted from text and object affordances. Implement role-conditioned coupling terms (relative pose, shared contact stability) and evaluate on tasks like \u201copen a jar,\u201d \u201cfold a cloth-like object proxy,\u201d or \u201chold-and-screw\u201d with improved coordination realism.\n\n10. **Closed-Loop HOI Control: Sequence Completion from Streaming Observations**\n   - Build on HOIGPT\u2019s partial-sequence completion to create a closed-loop predictor that continuously re-plans the next seconds of HOI given streaming hand/object state (and optional gaze) plus text goal. Train with trajectory perturbations and recovery objectives so the model learns to correct mistakes (missed grasp, object slip) rather than only generating open-loop motions.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Physically Grounded HOI Tokenizer with Explicit Contact/Force Channels\n- Improve HOIGPT\u2019s hand-object decomposed VQ-VAE tokenizer by adding discrete channels for contact state transitions (attach/deta",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Closed-Loop HOI Control: Sequence Completion from Streaming Observations\n- Build on HOIGPT\u2019s partial-sequence completion to create a closed-loop predictor that continuously re-plans the next seconds o",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 57,
      "paper_title": "Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think",
      "contribution": "Representation Entanglement for Generation (REG) enhances the training efficiency and quality of image generation in diffusion models by entangling class tokens from pretrained models with low-level image latents.",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "quality_content": 4,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8375,
      "output_tokens": 911,
      "predecessor_details": [
        {
          "success": true,
          "title": "sihyun-yu/REPA: [ICLR'25 Oral] Representation Alignment ...",
          "url": "https://github.com/sihyun-yu/REPA",
          "content": "GitHub - sihyun-yu/REPA: [ICLR&#39;25 Oral] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/sihyun-yu/REPA)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n \nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n \nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/sihyun-yu/REPA)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=sihyun-yu/REPA)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[sihyun-yu](https://github.com/sihyun-yu)/**[REPA](https://github.com/sihyun-yu/REPA)**Public\n* [Notifications](https://github.com/login?return_to=/sihyun-yu/REPA)You must be signed in to change notification settings\n* [Fork70](https://github.com/login?return_to=/sihyun-yu/REPA)\n* [Star1.5k](https://github.com/login?return_to=/sihyun-yu/REPA)\n[ICLR'25 Oral] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think\n[sihyun.me/REPA/](http://sihyun.me/REPA/)\n### License\n[MIT license](https://github.com/sihyun-yu/REPA/blob/main/LICENSE)\n[1.5kstars](https://github.com/sihyun-yu/REPA/stargazers)[70forks](https://github.com/sihyun-yu/REPA/forks)[Branches](https://github.com/sihyun-yu/REPA/branches)[Tags](https://github.com/sihyun-yu/REPA/tags)[Activity](https://github.com/sihyun-yu/REPA/activity)\n[Star](https://github.com/login?return_to=/sihyun-yu/REPA)\n[Notifications](https://github.com/login?return_to=/sihyun-yu/REPA)You must be signed in to change notification settings\n# sihyun-yu/REPA\nmain\n[Branches](https://github.com/sihyun-yu/REPA/branches)[Tags](https://github.com/sihyun-yu/REPA/tags)\n[](https://github.com/sihyun-yu/REPA/branches)[](https://github.com/sihyun-yu/REPA/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[20 Commits](https://github.com/sihyun-yu/REPA/commits/main/)\n[](https://github.com/sihyun-yu/REPA/commits/main/)\n|\n[models](https://github.com/sihyun-yu/REPA/tree/main/models)\n|\n[models](https://github.com/sihyun-yu/REPA/tree/main/models)\n|\n|\n|\n[preprocessing](https://github.com/sihyun-yu/REPA/tree/main/preprocessing)\n|\n[preprocessing](https://github.com/sihyun-yu/REPA/tree/main/preprocessing)\n|\n|\n|\n[.gitignore](https://github.com/sihyun-yu/REPA/blob/main/.gitignore)\n|\n[.gitignore](https://github.com/sihyun-yu/REPA/blob/main/.gitignore)\n|\n|\n|\n[LICENSE](https://github.com/sihyun-yu/REPA/blob/main/LICENSE)\n|\n[LICENSE](https://github.com/sihyun-yu/REPA/blob/main/LICENSE)\n|\n|\n|\n[README.md](https://github.com/sihyun-yu/REPA/blob/main/README.md)\n|\n[README.md](https://github.com/sihyun-yu/REPA/blob/main/README.md)\n|\n|\n|\n[dataset.py](https://github.com/sihyun-yu/REPA/blob/main/dataset.py)\n|\n[dataset.py](https://github.com/sihyun-yu/REPA/blob/main/dataset.py)\n|\n|\n|\n[generate.py](https://github.com/sihyun-yu/REPA/blob/main/generate.py)\n|\n[generate.py](https://github.com/sihyun-yu/REPA/blob/main/generate.py)\n|\n|\n|\n[generate\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/generate_t2i.py)\n|\n[generate\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/generate_t2i.py)\n|\n|\n|\n[loss.py](https://github.com/sihyun-yu/REPA/blob/main/loss.py)\n|\n[loss.py](https://github.com/sihyun-yu/REPA/blob/main/loss.py)\n|\n|\n|\n[requirements.txt](https://github.com/sihyun-yu/REPA/blob/main/requirements.txt)\n|\n[requirements.txt](https://github.com/sihyun-yu/REPA/blob/main/requirements.txt)\n|\n|\n|\n[samplers.py](https://github.com/sihyun-yu/REPA/blob/main/samplers.py)\n|\n[samplers.py](https://github.com/sihyun-yu/REPA/blob/main/samplers.py)\n|\n|\n|\n[samplers\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/samplers_t2i.py)\n|\n[samplers\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/samplers_t2i.py)\n|\n|\n|\n[train.py](https://github.com/sihyun-yu/REPA/blob/main/train.py)\n|\n[train.py](https://github.com/sihyun-yu/REPA/blob/main/train.py)\n|\n|\n|\n[train\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/train_t2i.py)\n|\n[train\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/train_t2i.py)\n|\n|\n|\n[utils.py](https://github.com/sihyun-yu/REPA/blob/main/utils.py)\n|\n[utils.py](https://github.com/sihyun-yu/REPA/blob/main/utils.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Representation Alignment for Generation:\nTraining Diffusion Transformers Is Easier Than You Think\n[](#-representation-alignment-for-generation-training-diffusion-transformers-is-easier-than-you-think)\n[![arXiv](https://camo.githubusercontent.com/6c07d72b64b0db5486ebae513c9a43700473c6c2fda6b3657c1cfc0cf6c32e42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f617258697625323070617065722d323431302e30363934302d6233316231622e737667)](https://arxiv.org/abs/2410.06940)[![PWC](https://camo.githubusercontent.com/17e2ef4e8930d284567c9c7c6beee28524ad3aa766c13db1413ed08bc807b3af/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f70617065727377697468636f64652e636f6d2f62616467652f726570726573656e746174696f6e2d616c69676e6d656e742d666f722d67656e65726174696f6e2f696d6167652d67656e65726174696f6e2d6f6e2d696d6167656e65742d32353678323536)](https://paperswithcode.com/sota/image-generation-on-imagenet-256x256?p=representation-alignment-for-generation)\n[Sihyun\u00a0Yu](https://sihyun.me/)1**\u00b7**[Sangkyung\u00a0Kwak](https://www.linkedin.com/in/SangkyungKwak/)1**\u00b7**[Huiwon\u00a0Jang](https://huiwon-jang.github.io/)1**\u00b7**[Jongheon\u00a0Jeong](https://jh-jeong.github.io/)2\n[Jonathan\u00a0Huang](http://jonathan-huang.org/)3**\u00b7**[Jinwoo\u00a0Shin](https://alinlab.kaist.ac.kr/shin.html)1\\***\u00b7**[Saining\u00a0Xie](https://www.sainingxie.com/)4\\*\n1KAIST2Korea University3Scaled Foundations4New York University\n\\*Equal Advising\n### [[project page](https://sihyun.me/REPA)]\u2003[[arXiv](http://arxiv.org/abs/2410.06940)]\n[](#project-pagearxiv)\n**Summary**: We propose REPresentation Alignment (REPA), a method that aligns noisy input states in diffusion models with representations from pretrained visual encoders. This significantly improves training efficiency and generation quality. REPA speeds up SiT training by 17.5x and achieves state-of-the-art FID=1.42.\n### 1. Environment setup\n[](#1-environment-setup)\n```\nconda create -n repa python=3.9 -y\nconda activate repa\npip install -r requirements.txt\n```\n### 2. Dataset\n[](#2-dataset)\n#### Dataset download\n[](#dataset-download)\nCurrently, we provide experiments for[ImageNet](https://www.kaggle.com/competitions/imagenet-object-localization-challenge/data). You can place the data that you want and can specifiy it via`--data-dir`arguments in training scripts. Please refer to our[preprocessing guide](https://github.com/sihyun-yu/REPA/tree/master/preprocessing).\n### 3. Training\n[](#3-training)\n```\naccelerate launch train.py \\\\\n--report-to=\"wandb\"\\\\\n--allow-tf32 \\\\\n--mixed-precision=\"fp16\"\\\\\n--seed=0 \\\\\n--path-type=\"linear\"\\\\\n--prediction=\"v\"\\\\\n--weighting=\"uniform\"\\\\\n--model=\"SiT-XL/2\"\\\\\n--enc-type=\"dinov2-vit-b\"\\\\\n--proj-coeff=0.5 \\\\\n--encoder-depth=8 \\\\\n--output-dir=\"exps\"\\\\\n--exp-name=\"linear-dinov2-b-enc8\"\\\\\n--data-dir=[YOUR\\_DATA\\_PATH]\n```\nThen this script will automatically create the folder in`exps`to save logs and checkpoints. You can adjust the following options:\n* `--models`:`[SiT-B/2, SiT-L/2, SiT-XL/2]`\n* `--enc-type`:`[dinov2-vit-b, dinov2-vit-l, dinov2-vit-g, dinov1-vit-b, mocov3-vit-b, , mocov3-vit-l, clip-vit-L, jepa-vit-h, mae-vit-l]`\n* `--proj-coeff`: Any values larger than 0\n* `--encoder-depth`: Any values between 1 to the depth of the model\n* `--output-dir`: Any directory that you want to save checkpoints and logs\n* `--exp-name`: Any string name (the folder will be created under`output-dir`)\nFor DINOv2 models, it will be automatically downloaded from`torch.hub`. For CLIP models, it will be also automatically downloaded from the CLIP repository. For other pretrained visual encoders, please download the model weights from the below links and place into the following directories with these names:\n* `dinov1`: Download the ViT-B/16 model from the[`DINO`](https://github.com/facebookresearch/dino)repository and place it as`./ckpts/dinov1\\_vitb.pth`\n* `mocov3`: Download the ViT-B/16 or ViT-L/16 model from the[`RCG`](https://github.com/LTH14/rcg)repository and place them as`./ckpts/mocov3\\_vitb.pth`or`./ckpts/mocov3\\_vitl.pth`\n* `jepa`: Download the ViT-H/14 model (ImageNet-1K) from the[`I-JEPA`](https://github.com/facebookresearch/ijepa)repository and place it as`./ckpts/ijepa\\_vith.pth`\n* `mae`: Download the ViT-L model from[`MAE`](https://github.com/facebookresearch/mae)repository and place it as`./ckpts/mae\\_vitl.pth`\n**[12/17/2024]**: We also support training on 512x512 resolution (ImageNet) and a text-to-image generation on MS-COCO.\nFor ImageNet 512x512, please use the following script:\n```\naccelerate launch train.py \\\\\n--report-to=\"wandb\"\\\\\n--allow-tf32 \\\\\n--mixed-precision=\"fp16\"\\\\\n--seed=0 \\\\\n--path-type=\"linear\"\\\\\n--prediction=\"v\"\\\\\n--weighting=\"uniform\"\\\\\n--model=\"SiT",
          "original_query": "REPA: Robust Efficient Pretraining Alignment for Generative Models",
          "cleaned_query": "REPA: Robust Efficient Pretraining Alignment for Generative Models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "SiT: Exploring Flow and Diffusion-based Generative Models ... - arXiv",
          "url": "https://arxiv.org/html/2401.08740",
          "content": "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n11institutetext:New York University# SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\nNanye MaMark GoldsteinMichael S. AlbergoNicholas M. BoffiEric Vanden-EijndenEqual advising.Saining Xie\u2020\n###### Abstract\nWe present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: learning in discrete or continuous time, the objective function, the interpolant that connects the distributions, and deterministic or stochastic sampling.\nBy carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet256\u00d7256256256256\\\\times 256256 \u00d7256and512\u00d7512512512512\\\\times 512512 \u00d7512benchmark using the exact same model structure, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06 and 2.62, respectively.\nCode is available here:[https://github.com/willisma/SiT](https://github.com/willisma/SiT)\n## 1Introduction\nContemporary success in image generation has come from a combination of algorithmic advances, improvements in model architecture, and progress in scaling neural network models and data.\nState-of-the-art diffusion models> [\n[> 25\n](https://arxiv.org/html/2401.08740v2#bib.bib25)> , [> 53\n](https://arxiv.org/html/2401.08740v2#bib.bib53)> ]\nproceed by incrementally transforming data into Gaussian noise as prescribed by an iterative stochastic process, which can be specified either in discrete or continuous time.\nAt an abstract level, this corruption process can be viewed as defining a time-dependent distribution that is iteratively smoothed from the original data distribution into a standard normal distribution.\nDiffusion models learn to reverse this corruption process and push Gaussian noise backwards along this connection to obtain data samples.\nThe objects learned to perform this transformation conventionally predict either the noise in the corruption process> [\n[> 25\n](https://arxiv.org/html/2401.08740v2#bib.bib25)> ]\nor the score of the distribution that connects the data and the Gaussian> [\n[> 64\n](https://arxiv.org/html/2401.08740v2#bib.bib64)> ]\n, though alternatives of these choices exist> [\n[> 56\n](https://arxiv.org/html/2401.08740v2#bib.bib56)> , [> 28\n](https://arxiv.org/html/2401.08740v2#bib.bib28)> ]\n.\nTable 1:Scalable Interpolant Transformers.We systematically vary the following aspects of a generative model:time discretization,model prediction,interpolant, andsampler. The resulting Scalable Interpolant Transformer (SiT) model, under identical training compute, consistently outperforms the Diffusion Transformer (DiT) in generating 256\u00d7256 ImageNet images. All models employ a patch size of 2. In this work, we ask the question:What is the source of the performance gain?\n|Model|Params(M)|Training Steps|FID\u2193\u2193\\\\downarrow\u2193|\nDiT-S|33|400K|68.4|\nSiT-S|33|400K|57.6|\nDiT-B|130|400K|43.5|\nSiT-B|130|400K|33.0|\nDiT-L|458|400K|23.3|\nSiT-L|458|400K|18.8|\nDiT-XL|675|400K|19.5|\nSiT-XL|675|400K|17.2|\nDiT-XL|675|7M|9.6|\nSiT-XL|675|7M|8.3|\nDiT-XL(cfg=1.5)(cfg=1.5){}\\_{\\\\text{(cfg=1.5)}}start\\_FLOATSUBSCRIPT (cfg=1.5) end\\_FLOATSUBSCRIPT|675|7M|2.27|\nSiT-XL(cfg=1.5)(cfg=1.5){}\\_{\\\\text{(cfg=1.5)}}start\\_FLOATSUBSCRIPT (cfg=1.5) end\\_FLOATSUBSCRIPT|675|7M|2.06|\n![Refer to caption](extracted/5873883/Images/sota_512.png)Figure 1:Selected samples from SiT-XL models trained on ImageNet> [\n[> 55\n](https://arxiv.org/html/2401.08740v2#bib.bib55)> ]\nat512\u00d7512512512512\\\\times 512512 \u00d7512and256\u00d7256256256256\\\\times 256256 \u00d7256resolution with cfg = 4.0, respectively.\n![Refer to caption](x1.png)\n![Refer to caption](x2.png)\n![Refer to caption](x3.png)\n![Refer to caption](x4.png)\nFigure 2:SiT improves FID across all model sizes.FID-50K over training iterations for both DiT and SiT. All results are produced by a Euler-Maruyama sampler using 250 integration steps. Across all model sizes, SiT converges much faster.\nWhile diffusion models originally represented these objects with a U-Net architecture> [\n[> 25\n](https://arxiv.org/html/2401.08740v2#bib.bib25)> , [> 54\n](https://arxiv.org/html/2401.08740v2#bib.bib54)> ]\n, recent work has highlighted that architectural advances in vision such as the Vision Transformer (ViT)> [\n[> 21\n](https://arxiv.org/html/2401.08740v2#bib.bib21)> ]\ncan be incorporated into the standard diffusion model pipeline to improve performance> [\n[> 50\n](https://arxiv.org/html/2401.08740v2#bib.bib50)> ]\n.\nOrthogonally, significant research effort has gone into exploring the structure of the noising process, which has been shown to lead to performance benefits> [\n[> 37\n](https://arxiv.org/html/2401.08740v2#bib.bib37)> , [> 33\n](https://arxiv.org/html/2401.08740v2#bib.bib33)> , [> 36\n](https://arxiv.org/html/2401.08740v2#bib.bib36)> , [> 60\n](https://arxiv.org/html/2401.08740v2#bib.bib60)> ]\n.\nYet, many of these efforts do not move past the notion of passing data through a diffusion process with an equilibrium distribution, which is a restricted type of connection between the data and the Gaussian.\nRecently-introducedstochastic interpolants> [\n[> 2\n](https://arxiv.org/html/2401.08740v2#bib.bib2)> ]\nlift this constraint and introduce more flexibility in the noise-data connection.\nIn this paper, we systematically explore the effect of this flexibility on performance in large scale image generation.\nIntuitively, we expect that the difficulty of thelearning problemcan be related to both the specific connection chosen and the object that is learned.\nOur aim is to clarify these design choices, so as to simplify the learning problem and thereby improve performance.\nTo understand where potential benefits arise in the learning problem, we start with Denoising Diffusion Probabilistic Models (DDPMs) and sweep through adaptations of: (i) which object to learn, and (ii) which interpolant to choose to reveal best practices.\nIn addition to the learning problem, there is asampling problemthat must be solved at inference time.\nIt has been acknowledged for diffusion models that sampling can be either deterministic or stochastic> [\n[> 63\n](https://arxiv.org/html/2401.08740v2#bib.bib63)> ]\n, and the choice of sampling method can be made after the learning process.\nYet, the diffusion coefficients used for stochastic sampling are typically presented as intrinsically tied to the forward noising process, which need not be the case in general.\nThroughout this paper, we explore how the design of the interpolant and the use of the resulting model as either a deterministic or a stochastic sampler impact performance.\nWe gradually transition from a typical denoising diffusion model to an interpolant model by taking a series of orthogonal steps in the design space.\nAs we progress, we carefully evaluate how each move away from the diffusion model impacts the performance.\nIn summary, ourmain contributionsare:\n* \u2022We systematically study the SiT design space through the combinations of the four key components:time discretization,model prediction,interpolant, andsampler.\n* \u2022We provide theoretical motivation for the choice of each component and study how they lead to improved practical performance.\n* \u2022We exploit the tunability of the diffusion coefficient of the stochastic sampler, and show that its adaptation can tighten control of the KL-divergence between the model and the target. We show how this leads to empirical benefits without any additional re-training.\n* \u2022Combining the best design choices identified in each component, our SiT model surpasses Diffusion Transformer(DiT) on both256\u00d7256256256256\\\\times 256256 \u00d7256and512\u00d7512512512512\\\\times 512512 \u00d7512image resolution, achieving FID-50K scores of 2.06 and 2.62, respectively, without modifying any structure or hyperparameter of the model.\n## 2SiT: Scalable Interpolant Transformers\nWe begin by recalling the main ingredients for building flow-based and diffusion-based generative models.\n### 2.1Flows and diffusions\nFlow and diffusion models both utilize stochastic processes to gradually turn noise\ud835\udf3a\u223c\ud835\uddad\u2062(0,\ud835\udc08)similar-to\ud835\udf3a\ud835\uddad0\ud835\udc08{\\\\boldsymbol{\\\\varepsilon}}\\\\sim\\\\mathsf{N}(0,\\\\mathbf{I})bold\\_italic\\_\u03b5 \u223csansserif\\_N ( 0 , bold\\_I )into data\ud835\udc31\u2217\u223cp\u2062(\ud835\udc31)similar-tosubscript\ud835\udc31\ud835\udc5d\ud835\udc31\\\\mathbf{x}\\_{\\*}\\\\sim p(\\\\mathbf{x})bold\\_x start\\_POSTSUBSCRIPT \u2217end\\_POSTSUBSCRIPT \u223citalic\\_p ( bold\\_x )for the generating task.\nSuch time-dependent processes can be summarized as follow\n|\ud835\udc31t=\u03b1t\u2062\ud835\udc31\u2217+\u03c3t\u2062\ud835\udf3a,subscript\ud835\udc31\ud835\udc61subscript\ud835\udefc\ud835\udc61subscript\ud835\udc31subscript\ud835\udf0e\ud835\udc61\ud835\udf3a\\\\displaystyle\\\\mathbf{x}\\_{t}=\\\\alpha\\_{t}{\\\\mathbf{x}}\\_{\\*}+\\\\sigma\\_{t}{\\\\boldsymbol{%\n\\\\varepsilon}},bold\\_x start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_\u03b1 start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT bold\\_x start\\_POSTSUBSCRIPT \u2217end\\_POSTSUBSCRIPT + italic\\_\u03c3 start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT bold\\_italic\\_\u03b5 ,||(1)|\nwhere\u03b1tsubscript\ud835\udefc\ud835\udc61\\\\alpha\\_{t}italic\\_\u03b1 start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPTis a decreasing function oft\ud835\udc61titalic\\_tand\u03c3tsubscript\ud835\udf0e\ud835\udc61\\\\sigma\\_{t}italic\\_\u03c3 start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPTis an increasing function oft\ud835\udc61titalic\\_t.\nStochastic interpolants and other flow matching methods> [\n[> 4\n](https://arxiv.org/html/2401.08740v2#bib.bib4)> , [> 2\n](https://arxiv.org/html/2401.08740v2#bib.bib2)> , [> 41\n](https://arxiv.org/html/2401.08740v2#bib.bib41)> , [> 43\n](https://arxiv.org/html/2401.08740v2#bib.bib43)> ]\nrestrict the process\u00a0([1](https://arxiv.org/html/2401.08740v2#S2.E1)) ont\u2208[0,1]\ud835\udc6101t\\\\in[0,1]italic\\_t \u2208[ 0 , 1 ], and set\u03b10=\u03c31=1subscript\ud835\udefc0subscript\ud835\udf0e11\\\\alpha\\_{0}=\\\\sigma\\_{1}=1italic\\_\u03b1 st",
          "original_query": "Scalable Interpolant Transformers (SiT)",
          "cleaned_query": "Scalable Interpolant Transformers (SiT)",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "Latent Denoising Diffusion Models - Emergent Mind",
          "url": "https://www.emergentmind.com/topics/latent-denoising-diffusion-models",
          "content": "Latent Denoising Diffusion Models\nLatent Denoising Diffusion Models\nPapers\nTopics\nLightbulb On Streamline Icon: https://streamlinehq.com\nAuthors\nRecent\n[View all](https://www.emergentmind.com/history)\nMagnifying Glass Streamline Icon: https://streamlinehq.com\n \n \n2000 character limit reached\n[SponsorInformation Square Streamline Icon: https://streamlinehq.com](https://www.emergentmind.com/sponsorship)\n[![](https://d2zk8qdx2y1ber.cloudfront.net/assets/sponsors/paperpile-logo-w343-d7fd7c8c33d166ed3ac6f5bb45f26f8b89eca7f00c1e1433295e91ce6a7d2fae.png)](https://www.paperpile.com?utm_source=emergentmind&utm_medium=sidebar-image)\nOrganize your preprints, BibTeX, and PDFs with Paperpile.\n[Get 30 days free](https://www.paperpile.com?utm_source=emergentmind&utm_medium=sidebar-text)\nChrome Extension\nEnhance arXiv with our new Chrome Extension.\n[Chrome Extension](https://chromewebstore.google.com/detail/emergent-mind-\u2014-arxiv-int/hgmnadjffdiipehljmhagdgpaoiiklml)\n# Latent Denoising Diffusion Models\nUpdated 17 November 2025\n* Latent denoising diffusion models are generative frameworks that perform denoising in a lower-dimensional latent space using encoder\u2013decoder architectures.\n* They reduce computational overhead and accelerate sampling by applying the diffusion process to latent codes instead of raw observed data.\n* Extensions with bridge models, GAN variants, and variational inference enable advanced applications in image synthesis, restoration, and semantic manipulation.\nLatent[Denoising Diffusion Models](https://www.emergentmind.com/topics/denoising-diffusion-models-dms)comprise a class of generative, inference, and regularization frameworks in which a denoising diffusion process\u2014usually realized by a neural scoring or denoiser network\u2014is performed within the latent space of a powerful encoder\u2013decoder or autoencoder. By operating in latent space, rather than at the level of observed data (e.g., pixel arrays), these models drastically reduce computational overhead, accelerate sampling, and often enable enhanced expressivity and semantic manipulation of internal representations, including for image synthesis, image restoration, generative bridging, and posterior inference. The following sections detail foundational mathematical formulations and architectures, advanced bridge and[GAN](https://www.emergentmind.com/topics/conditional-generative-adversarial-network-gan)variants, regularization and inverse-problem applications, theoretical insights, and representative empirical studies.\n## 1. Mathematical Foundations and Model Classes\nLatent denoising diffusion typically begins with a pretrained encoderE\\\\mathcal{E}Emapping datax\u2208Rnx \\\\in \\\\mathbb{R}^nx\u2208Rninto a lower-dimensional latentz0z\\_0z0\u200b(e.g.,z0=E(x)z\\_0 = \\\\mathcal{E}(x)z0\u200b=E(x)). A forward noising process produces a sequence{zt}t=1T\\\\{z\\_t\\\\}\\_{t=1}^{T}{zt\u200b}t=1T\u200bvia conditional Gaussian kernels:q(zt\u2223zt\u22121)=N(zt;1\u2212\u03b2tzt\u22121,\u03b2tI)q(z\\_t \\\\mid z\\_{t-1}) = \\\\mathcal{N}(z\\_t; \\\\sqrt{1-\\\\beta\\_t}\\\\,z\\_{t-1},\\\\,\\\\beta\\_t I)q(zt\u200b\u2223zt\u22121\u200b)=N(zt\u200b;1\u2212\u03b2t\u200b\u200bzt\u22121\u200b,\u03b2t\u200bI)with linearly or cosine-scheduled\u03b2t\\\\beta\\_t\u03b2t\u200b. The cumulative form gives closed-form marginals:zt=\u03b1\u02c9tz0+1\u2212\u03b1\u02c9t\u03f5,\u03f5\u223cN(0,I)z\\_t = \\\\sqrt{\\\\bar\\\\alpha\\_t}\\\\,z\\_0 + \\\\sqrt{1 - \\\\bar\\\\alpha\\_t}\\\\,\\\\epsilon, \\~\\~\\~ \\\\epsilon \\\\sim \\\\mathcal{N}(0, I)zt\u200b=\u03b1\u02c9t\u200b\u200bz0\u200b+1\u2212\u03b1\u02c9t\u200b\u200b\u03f5,\u03f5\u223cN(0,I)([Zhang, 11 Feb 2024](https://www.emergentmind.com/papers/2402.07129),[Rhee et al., 30 Jul 2025](https://www.emergentmind.com/papers/2508.03727),[Traub, 2022](https://www.emergentmind.com/papers/2210.11058),[Vlassis et al., 2023](https://www.emergentmind.com/papers/2306.04411)). The reverse denoising process is either stochastic (as in[DDPM](https://www.emergentmind.com/topics/three-channel-denoising-diffusion-probabilistic-model-ddpm)variants) or deterministic ([DDIM](https://www.emergentmind.com/topics/denoising-diffusion-implicit-model-ddim)), parameterized by a score network or direct mean/variance predictions:p\u03b8(zt\u22121\u2223zt)=N(zt\u22121;\u03bc\u03b8(zt,t),\u03a3\u03b8(zt,t))p\\_{\\\\theta}(z\\_{t-1} \\\\mid z\\_t) = \\\\mathcal{N}(z\\_{t-1}; \\\\mu\\_\\\\theta(z\\_t, t),\\\\,\\\\Sigma\\_\\\\theta(z\\_t, t))p\u03b8\u200b(zt\u22121\u200b\u2223zt\u200b)=N(zt\u22121\u200b;\u03bc\u03b8\u200b(zt\u200b,t),\u03a3\u03b8\u200b(zt\u200b,t))with mean given in\u03f5\\\\epsilon\u03f5-prediction parameterization as\n\u03bc\u03b8(zt,t)=1\u03b1t(zt\u2212\u03b2t1\u2212\u03b1\u02c9t\u03f5\u03b8(zt,t))\\\\mu\\_\\\\theta(z\\_t, t) = \\\\frac{1}{\\\\sqrt{\\\\alpha\\_t}}\\\\left(z\\_t - \\\\frac{\\\\beta\\_t}{\\\\sqrt{1-\\\\bar\\\\alpha\\_t}\\\\, \\\\epsilon\\_\\\\theta(z\\_t, t)}\\\\right)\u03bc\u03b8\u200b(zt\u200b,t)=\u03b1t\u200b\u200b1\u200b(zt\u200b\u22121\u2212\u03b1\u02c9t\u200b\u200b\u03f5\u03b8\u200b(zt\u200b,t)\u03b2t\u200b\u200b)\n([Vlassis et al., 2023](https://www.emergentmind.com/papers/2306.04411),[Zhang, 11 Feb 2024](https://www.emergentmind.com/papers/2402.07129),[Rhee et al., 30 Jul 2025](https://www.emergentmind.com/papers/2508.03727)). Deterministic DDIM variants set the variance\u03c3t\u21920\\\\sigma\\_t \\\\to 0\u03c3t\u200b\u21920for fast, high-fidelity sampling in latent space.\n## 2. Architectural Variants and Conditioning\nAutoencoders (VQ-GAN, convolutional VAEs, point-cloud autoencoders) define the latent manifold ([Trinh et al., 17 Jun 2024](https://www.emergentmind.com/papers/2406.11713),[Vlassis et al., 2023](https://www.emergentmind.com/papers/2306.04411),[Traub, 2022](https://www.emergentmind.com/papers/2210.11058)):\n* **EncoderE\\\\mathcal{E}E:**Maps images to latent codes (oftenR4\u00d7H/8\u00d7W/8\\\\mathbb{R}^{4\\\\times H/8 \\\\times W/8}R4\u00d7H/8\u00d7W/8or $784$-dim for grain point clouds).\n* **DecoderD\\\\mathcal{D}D:**Maps latent codes back to images or 3D structures. Training favors perceptual or patch-based GAN losses and omits strict priors for maximal expressivity ([Trinh et al., 17 Jun 2024](https://www.emergentmind.com/papers/2406.11713)).\nDenoiser networks are typically U-Net backbones, featuring:\n* Sinusoidal time embeddings and cross-attention for conditioning (e.g., text, semantic, or wavelet embeddings) ([Rhee et al., 30 Jul 2025](https://www.emergentmind.com/papers/2508.03727)).\n* Residual blocks with skip connections and bottlenecks for hierarchical feature propagation ([Zhang, 11 Feb 2024](https://www.emergentmind.com/papers/2402.07129)).\n* Adaptive group normalization and joint representation conditioning for semantic controllability ([Traub, 2022](https://www.emergentmind.com/papers/2210.11058)).\nTable: Latent[Denoising Diffusion Model](https://www.emergentmind.com/topics/denoising-diffusion-model)Building Blocks\n|Component|Implementation Example|Reference|\nEncoderE\\\\mathcal{E}E|VQ-GAN, Point Cloud AE, VAE, ConvNet|([Trinh et al., 17 Jun 2024](https://www.emergentmind.com/papers/2406.11713),[Vlassis et al., 2023](https://www.emergentmind.com/papers/2306.04411))|\nDenoiser\u03f5\u03b8\\\\epsilon\\_\\\\theta\u03f5\u03b8\u200b|U-Net w/ time embedding, cross-attn|([Rhee et al., 30 Jul 2025](https://www.emergentmind.com/papers/2508.03727),[Zhang, 11 Feb 2024](https://www.emergentmind.com/papers/2402.07129))|\nDecoderD\\\\mathcal{D}D|Symmetric to encoder; reconstructs image/structure|([Trinh et al., 17 Jun 2024](https://www.emergentmind.com/papers/2406.11713),[Traub, 2022](https://www.emergentmind.com/papers/2210.11058))|\nClassical latent models operate unconditionally onzT\u223cN(0,I)z\\_T \\\\sim \\\\mathcal{N}(0, I)zT\u200b\u223cN(0,I), optionally supervised via conditioning representationsrrrlearned jointly or injected through cross-attention ([Traub, 2022](https://www.emergentmind.com/papers/2210.11058)).\n## 3. Bridge, GAN, and Inference Extensions\n**Denoising Diffusion Bridge Models (DDBMs):**Generalize standard latent diffusion by training the score on arbitrary source-target latent pairs, enabling image-to-image translation, semantic editing, and optimal transport in latent space ([Zhou et al., 2023](https://www.emergentmind.com/papers/2309.16948)). The formalism incorporates Doob\u2019shhh-transform and explicit scoring of intermediate bridge distributions,\ndzt=f(zt,t)dt+g(t)2\u2207ztlog\u2061p(T,zT\u2223t,zt)dt+g(t)dWtdz\\_t = f(z\\_t,t) dt + g(t)^2 \\\\nabla\\_{z\\_t} \\\\log p(T,z\\_T | t, z\\_t) dt + g(t) dW\\_tdzt\u200b=f(zt\u200b,t)dt+g(t)2\u2207zt\u200b\u200blogp(T,zT\u200b\u2223t,zt\u200b)dt+g(t)dWt\u200b\nBridges allow efficient translation, editing, and compositionality without restriction to pure noise priors.\n**Latent Denoising Diffusion[GANs](https://www.emergentmind.com/topics/attention-mechanism-and-generative-adversarial-networks-gans)(LDDGAN):**Couple latent diffusion to conditional GAN training for reverse transitions, drastically reducing the number of denoising steps required (T\u22648T \\\\leq 8T\u22648), and achieving sampling speeds competitive with state-of-the-art GANs ([Trinh et al., 17 Jun 2024](https://www.emergentmind.com/papers/2406.11713)). Weighted learning dynamically shifts loss focus between reconstruction and adversarial terms during training.\n**Diffusion-based[Variational Inference](https://www.emergentmind.com/topics/variational-inference-vi)(DDVI):**Embeds denoising diffusion processes as black-box posteriors within variational inference, exceeding the flexibility/expressiveness of[normalizing flows](https://www.emergentmind.com/topics/normalizing-flows)or adversarial posteriors ([Piriyakulkij et al., 5 Jan 2024](https://www.emergentmind.com/papers/2401.02739)). The[ELBO](https://www.emergentmind.com/topics/evidence-lower-bound-elbo)is augmented by a wake-sleep regularizer, maximizing marginal likelihood bounds and directly fitting structured posteriors.\n## 4. Regularization, Restoration, and Inverse Problems\n[Latent diffusion models](https://www.emergentmind.com/topics/latent-diffusion-models-ldms)serve as learned priors for variational formulation of restoration and inverse problems, particularly via half-quadratic splitting (HQS) ([Cascarano et al., 28 Mar 2025](https://www.emergentmind.com/papers/2503.22563)):J(x)=D(x;y)+\u03bbR(x)J(x) = D(x; y) + \\\\lambda R(x)J(x)=D(x;y)+\u03bbR(x)whereD(x;y)=12\u2225Ax\u2212y\u222522D(x; y) = \\\\frac{1}{2} \\\\|Ax - y\\\\|\\_2^2D(x;y)=21\u200b\u2225Ax\u2212y\u222522\u200b, andR(x)R(x)R(x)is the implicit regularizer derived from the generative decoder of a pretrained latent denoising diffusion model. Alternating minimization between data fidelity and latent prior\u2014using a fast latent DDIM denoiser and a quadratic penalty update\u2014permits competitive restoration performance across denoising, deblurring, and super-resolution, with",
          "original_query": "Latent Denoising Diffusion Models",
          "cleaned_query": "Latent Denoising Diffusion Models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Denoising Diffusion Autoencoders are Unified Self-supervised ...",
          "url": "https://www.semanticscholar.org/paper/df4b6713abfe226d06099d7749f8b47903ac087b",
          "content": "@article{Xiang2023DenoisingDA,\ntitle={Denoising Diffusion Autoencoders are Unified Self-supervised Learners},\nauthor={Weilai Xiang and Hongyu Yang and Di Huang and Yunhong Wang},\njournal={2023 IEEE/CVF International Conference on Computer Vision (ICCV)},\nyear={2023},\npages={15756-15766},\nurl={https://api.semanticscholar.org/CorpusID:257623083}\n} This paper shows that the networks in diffusion models, namely denoising diffusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image generation, DDAE has already learned strongly linear-separable representations within its intermediate layers without auxiliary encoders, thus making diffusion pre-training emerge as a general approach for generative-and-discriminative dual learning. Figures and Tables from this paper 92 Citations 74 References",
          "original_query": "Denoising Diffusion Autoencoders are Unified Self-Supervised Learners",
          "cleaned_query": "Denoising Diffusion Autoencoders are Unified Self-Supervised Learners",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "REPA-Interpolant Alignment for SiT (RA-SiT)\nExtend REPA-style representation alignment to Scalable Interpolant Transformers by aligning intermediate SiT features to a frozen discriminative teacher (e.g., DINO/CLIP) across multiple interpolant times. Evaluate whether alignment benefits differ by interpolant choice (stochastic vs deterministic) and whether it reduces the number of training steps needed to match SiT baseline FID.",
        "Time-Localized Representation Alignment Schedules\nDevelop an alignment curriculum that activates REPA losses only at specific time regions (early/mid/late) of diffusion/interpolant trajectories, based on where representations become linearly separable (per DDAE findings). Run controlled ablations to identify time windows that maximize downstream linear probe accuracy while preserving (or improving) generation FID.",
        "Latent-Space REPA for LDMs with Encoder-Aware Alignment\nApply representation alignment inside latent denoising diffusion models by aligning denoiser features to both (a) an image teacher on decoded reconstructions and (b) a latent teacher derived from the encoder\u2019s bottleneck. Test whether \u201cdual-space\u201d alignment improves semantic controllability and sample efficiency without increasing pixel-space compute.",
        "Unified Generative\u2013Discriminative Training via Multi-Head DDAE Probes\nAdd lightweight linear/MLP probe heads to multiple DDAE/DiT/SiT blocks during generative training and jointly optimize a self-supervised discriminative objective (e.g., contrastive or masked-patch prediction) using the same features that REPA aligns. Quantify whether joint training outperforms post-hoc linear probing (DDAE) and pure alignment (REPA) on transfer tasks at fixed generative quality.",
        "Interpolant Selection by Representation Separability Metrics\nCreate an automatic interpolant/timestep policy for SiT that chooses between candidate interpolants (and diffusion coefficients) by monitoring online representation quality signals (e.g., linear separability or CKA similarity to a teacher) rather than only likelihood/FID proxies. Validate whether the chosen policies generalize across datasets (ImageNet \u2192 COCO) and model scales.",
        "Cross-Modal REPA for Text-to-Image DiT/SiT\nExtend REPA to text-conditioned generation by aligning image-path features with text-aligned embedding spaces (e.g., CLIP image-text joint space) at selected timesteps while keeping the text encoder frozen. Measure gains in prompt faithfulness, compositionality, and reduced reliance on classifier-free guidance scales (maintaining FID).",
        "Representation-Aligned Distillation for Fast Deterministic Sampling\nUse REPA as a distillation signal to train a low-step deterministic sampler (DDIM/ODE for diffusion or deterministic interpolants for SiT) where student trajectories match teacher intermediate representations rather than only final samples. Benchmark how many steps can be removed before perceptual quality drops, and whether representation matching stabilizes aggressive step reduction.",
        "Blockwise Alignment to Improve DiT/SiT Scaling Laws\nInvestigate which transformer blocks benefit most from alignment by applying REPA losses to different depth ranges (early vs late vs all) and analyzing compute/quality tradeoffs. Produce scaling curves (params, tokens, steps) showing whether selective alignment yields better FID-per-GFLOP and better transfer accuracy than uniform alignment.",
        "Latent Bridge Models with Representation Constraints for Inverse Problems\nCombine latent diffusion \u201cbridge\u201d formulations with representation alignment so that intermediate states remain consistent with a task-specific representation (e.g., a restoration/perception network) while transporting between degraded and clean latent distributions. Test on deblurring/super-resolution with a fixed compute budget, comparing against standard latent diffusion restoration pipelines.",
        "Diagnosing and Correcting Feature Collapse in Diffusion Transformers\nBuild diagnostics that track representation rank, mutual information proxies, and teacher-student similarity across timesteps to detect feature collapse modes during DiT/SiT training. Use the diagnostics to trigger adaptive alignment strength or interpolant coefficient changes, and report improved training stability (fewer divergences) at large scale and high resolution."
      ],
      "generated_ideas_raw": "1. **REPA-Interpolant Alignment for SiT (RA-SiT)**\n   Extend REPA-style representation alignment to Scalable Interpolant Transformers by aligning intermediate SiT features to a frozen discriminative teacher (e.g., DINO/CLIP) across multiple interpolant times. Evaluate whether alignment benefits differ by interpolant choice (stochastic vs deterministic) and whether it reduces the number of training steps needed to match SiT baseline FID.\n\n2. **Time-Localized Representation Alignment Schedules**\n   Develop an alignment curriculum that activates REPA losses only at specific time regions (early/mid/late) of diffusion/interpolant trajectories, based on where representations become linearly separable (per DDAE findings). Run controlled ablations to identify time windows that maximize downstream linear probe accuracy while preserving (or improving) generation FID.\n\n3. **Latent-Space REPA for LDMs with Encoder-Aware Alignment**\n   Apply representation alignment inside latent denoising diffusion models by aligning denoiser features to both (a) an image teacher on decoded reconstructions and (b) a latent teacher derived from the encoder\u2019s bottleneck. Test whether \u201cdual-space\u201d alignment improves semantic controllability and sample efficiency without increasing pixel-space compute.\n\n4. **Unified Generative\u2013Discriminative Training via Multi-Head DDAE Probes**\n   Add lightweight linear/MLP probe heads to multiple DDAE/DiT/SiT blocks during generative training and jointly optimize a self-supervised discriminative objective (e.g., contrastive or masked-patch prediction) using the same features that REPA aligns. Quantify whether joint training outperforms post-hoc linear probing (DDAE) and pure alignment (REPA) on transfer tasks at fixed generative quality.\n\n5. **Interpolant Selection by Representation Separability Metrics**\n   Create an automatic interpolant/timestep policy for SiT that chooses between candidate interpolants (and diffusion coefficients) by monitoring online representation quality signals (e.g., linear separability or CKA similarity to a teacher) rather than only likelihood/FID proxies. Validate whether the chosen policies generalize across datasets (ImageNet \u2192 COCO) and model scales.\n\n6. **Cross-Modal REPA for Text-to-Image DiT/SiT**\n   Extend REPA to text-conditioned generation by aligning image-path features with text-aligned embedding spaces (e.g., CLIP image-text joint space) at selected timesteps while keeping the text encoder frozen. Measure gains in prompt faithfulness, compositionality, and reduced reliance on classifier-free guidance scales (maintaining FID).\n\n7. **Representation-Aligned Distillation for Fast Deterministic Sampling**\n   Use REPA as a distillation signal to train a low-step deterministic sampler (DDIM/ODE for diffusion or deterministic interpolants for SiT) where student trajectories match teacher intermediate representations rather than only final samples. Benchmark how many steps can be removed before perceptual quality drops, and whether representation matching stabilizes aggressive step reduction.\n\n8. **Blockwise Alignment to Improve DiT/SiT Scaling Laws**\n   Investigate which transformer blocks benefit most from alignment by applying REPA losses to different depth ranges (early vs late vs all) and analyzing compute/quality tradeoffs. Produce scaling curves (params, tokens, steps) showing whether selective alignment yields better FID-per-GFLOP and better transfer accuracy than uniform alignment.\n\n9. **Latent Bridge Models with Representation Constraints for Inverse Problems**\n   Combine latent diffusion \u201cbridge\u201d formulations with representation alignment so that intermediate states remain consistent with a task-specific representation (e.g., a restoration/perception network) while transporting between degraded and clean latent distributions. Test on deblurring/super-resolution with a fixed compute budget, comparing against standard latent diffusion restoration pipelines.\n\n10. **Diagnosing and Correcting Feature Collapse in Diffusion Transformers**\n   Build diagnostics that track representation rank, mutual information proxies, and teacher-student similarity across timesteps to detect feature collapse modes during DiT/SiT training. Use the diagnostics to trigger adaptive alignment strength or interpolant coefficient changes, and report improved training stability (fewer divergences) at large scale and high resolution.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "REPA-Interpolant Alignment for SiT (RA-SiT)\nExtend REPA-style representation alignment to Scalable Interpolant Transformers by aligning intermediate SiT features to a frozen discriminative teacher (e.",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Time-Localized Representation Alignment Schedules\nDevelop an alignment curriculum that activates REPA losses only at specific time regions (early/mid/late) of diffusion/interpolant trajectories, based",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Latent-Space REPA for LDMs with Encoder-Aware Alignment\nApply representation alignment inside latent denoising diffusion models by aligning denoiser features to both (a) an image teacher on decoded re",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Unified Generative\u2013Discriminative Training via Multi-Head DDAE Probes\nAdd lightweight linear/MLP probe heads to multiple DDAE/DiT/SiT blocks during generative training and jointly optimize a self-supe",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Interpolant Selection by Representation Separability Metrics\nCreate an automatic interpolant/timestep policy for SiT that chooses between candidate interpolants (and diffusion coefficients) by monitor",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Cross-Modal REPA for Text-to-Image DiT/SiT\nExtend REPA to text-conditioned generation by aligning image-path features with text-aligned embedding spaces (e.g., CLIP image-text joint space) at selected",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Representation-Aligned Distillation for Fast Deterministic Sampling\nUse REPA as a distillation signal to train a low-step deterministic sampler (DDIM/ODE for diffusion or deterministic interpolants fo",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Blockwise Alignment to Improve DiT/SiT Scaling Laws\nInvestigate which transformer blocks benefit most from alignment by applying REPA losses to different depth ranges (early vs late vs all) and analyz",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Latent Bridge Models with Representation Constraints for Inverse Problems\nCombine latent diffusion \u201cbridge\u201d formulations with representation alignment so that intermediate states remain consistent wit",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Diagnosing and Correcting Feature Collapse in Diffusion Transformers\nBuild diagnostics that track representation rank, mutual information proxies, and teacher-student similarity across timesteps to de",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 58,
      "paper_title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation",
      "contribution": "Dynam3D presents a dynamic layered 3D representation model that enhances vision-and-language navigation by improving spatial understanding and flexibility in changing environments.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 1,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9719,
      "output_tokens": 1133,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Learning Transferable Visual Models From Natural Language ...",
          "url": "https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf",
          "content": "Learning Transferable Visual Models From Natural Language Supervision\nAlec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1 Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1\nGirish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1Ilya Sutskever 1\nAbstract\nState-of-the-art computer vision systems are\ntrained to predict a fixed set of predetermined\nobject categories. This restricted form of super\u0002vision limits their generality and usability since\nadditional labeled data is needed to specify any\nother visual concept. Learning directly from raw\ntext about images is a promising alternative which\nleverages a much broader source of supervision.\nWe demonstrate that the simple pre-training task\nof predicting which caption goes with which im\u0002age is an efficient and scalable way to learn SOTA\nimage representations from scratch on a dataset\nof 400 million (image, text) pairs collected from\nthe internet. After pre-training, natural language\nis used to reference learned visual concepts (or\ndescribe new ones) enabling zero-shot transfer\nof the model to downstream tasks. We study\nthe performance of this approach by benchmark\u0002ing on over 30 different existing computer vi\u0002sion datasets, spanning tasks such as OCR, action\nrecognition in videos, geo-localization, and many\ntypes of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is\noften competitive with a fully supervised baseline\nwithout the need for any dataset specific training.\nFor instance, we match the accuracy of the orig\u0002inal ResNet-50 on ImageNet zero-shot without\nneeding to use any of the 1.28 million training\nexamples it was trained on.\n1. Introduction and Motivating Work\nPre-training methods which learn directly from raw text\nhave revolutionized NLP over the last few years (Dai &\nLe, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad\u0002ford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).\nTask-agnostic objectives such as autoregressive and masked\nlanguage modeling have scaled across many orders of mag-\n*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\nCorrespondence to: <{alec, jongwook}@openai.com>.\nnitude in compute, model capacity, and data, steadily im\u0002proving capabilities. The development of \u201ctext-to-text\u201d as\na standardized input-output interface (McCann et al., 2018;\nRadford et al., 2019; Raffel et al., 2019) has enabled task\u0002agnostic architectures to zero-shot transfer to downstream\ndatasets removing the need for specialized output heads or\ndataset specific customization. Flagship systems like GPT-3\n(Brown et al., 2020) are now competitive across many tasks\nwith bespoke models while requiring little to no dataset\nspecific training data.\nThese results suggest that the aggregate supervision acces\u0002sible to modern pre-training methods within web-scale col\u0002lections of text surpasses that of high-quality crowd-labeled\nNLP datasets. However, in other fields such as computer\nvision it is still standard practice to pre-train models on\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\nCould scalable pre-training methods which learn directly\nfrom web text result in a similar breakthrough in computer\nvision? Prior work is encouraging.\nOver 20 years ago Mori et al. (1999) explored improving\ncontent based image retrieval by training a model to pre\u0002dict the nouns and adjectives in text documents paired with\nimages. Quattoni et al. (2007) demonstrated it was possi\u0002ble to learn more data efficient image representations via\nmanifold learning in the weight space of classifiers trained\nto predict words in captions associated with images. Sri\u0002vastava & Salakhutdinov (2012) explored deep represen\u0002tation learning by training multimodal Deep Boltzmann\nMachines on top of low-level image and text tag feature\nfeatures. Joulin et al. (2016) modernized this line of work\nand demonstrated that CNNs trained to predict words in\nimage captions learn useful image representations. They\nconverted the title, description, and hashtag metadata of im\u0002ages in the YFCC100M dataset (Thomee et al., 2016) into\na bag-of-words multi-label classification task and showed\nthat pre-training AlexNet (Krizhevsky et al., 2012) to pre\u0002dict these labels learned representations which preformed\nsimilarly to ImageNet-based pre-training on transfer tasks.\nLi et al. (2017) then extended this approach to predicting\nphrase n-grams in addition to individual words and demon\u0002strated the ability of their system to zero-shot transfer to\nother image classification datasets by scoring target classes\nbased on their dictionary of learned visual n-grams and\nLearning Transferable Visual Models From Natural Language Supervision 2\nI1\u00b7T2I1\u00b7T3 \u2026\nI2\u00b7T1I2\u00b7T3 \u2026\nI3\u00b7T1I3\u00b7T2 \u2026\n\u22ee \u22ee \u22ee\nI1\u00b7T1\nI2\u00b7T2\nI3\u00b7T3\n(1) Contrastive pre-training\nImage\nEncoder\nText\nEncoder Pepper the\naussie pup\nT1 T2 T3 \u2026\nI1\nI2\nI3\n\u22ee\n(2) Create dataset classifier from label text\nplane\ncar\ndog\n\u22ee\nbird\nA photo of\na {object}.\n\u22ee\nText\nEncoder\nT1 T2 T3 TN\n\u2026\n(3) Use for zero-shot prediction\nImage\nEncoder\nI1I1\u00b7T2I1I1\u00b7T1\u00b7TN\n\u2026\n\u2026\nA photo of\n a dog.\nTN\nIN\u00b7T1IN\u00b7T2IN\u00b7T3\nI1\u00b7TN\nI2\u00b7TN\nI3\u00b7TN\n\u22ee\nIN \u2026\n\u2026\n\u22ee \u22f1\nIN\u00b7TN\nI1\u00b7T3\nFigure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\nexamples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the\ntarget dataset\u2019s classes.\npredicting the one with the highest score. Adopting more\nrecent architectures and pre-training approaches, VirTex\n(Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al.,\n2020), and ConVIRT (Zhang et al., 2020) have recently\ndemonstrated the potential of transformer-based language\nmodeling, masked language modeling, and contrastive ob\u0002jectives to learn image representations from text.\nWhile exciting as proofs of concept, using natural language\nsupervision for image representation learning is still rare.\nThis is likely because demonstrated performance on com\u0002mon benchmarks is much lower than alternative approaches.\nFor example, Li et al. (2017) reach only 11.5% accuracy\non ImageNet in a zero-shot setting. This is well below the\n88.4% accuracy of the current state of the art (Xie et al.,\n2020). It is even below the 50% accuracy of classic com\u0002puter vision approaches (Deng et al., 2012). Instead, more\nnarrowly scoped but well-targeted uses of weak supervision\nhave improved performance. Mahajan et al. (2018) showed\nthat predicting ImageNet related hashtags on Instagram im\u0002ages is an effective pre-training task. When fine-tuned to\nImageNet these pre-trained models increased accuracy by\nover 5% and improved the overall state of the art at the time.\nKolesnikov et al. (2019) and Dosovitskiy et al. (2020) have\nalso demonstrated large gains on a broader set of transfer\nbenchmarks by pre-training models to predict the classes of\nthe noisily labeled JFT-300M dataset.\nThis line of work represents the current pragmatic middle\nground between learning from a limited amount of super\u0002vised \u201cgold-labels\u201d and learning from practically unlimited\namounts of raw text. However, it is not without compro\u0002mises. Both works carefully design, and in the process limit,\ntheir supervision to 1000 and 18291 classes respectively.\nNatural language is able to express, and therefore supervise,\na much wider set of visual concepts through its general\u0002ity. Both approaches also use static softmax classifiers to\nperform prediction and lack a mechanism for dynamic out\u0002puts. This severely curtails their flexibility and limits their\n\u201czero-shot\u201d capabilities.\nA crucial difference between these weakly supervised mod\u0002els and recent explorations of learning image representations\ndirectly from natural language is scale. While Mahajan et al.\n(2018) and Kolesnikov et al. (2019) trained their models for\naccelerator years on millions to billions of images, VirTex,\nICMLM, and ConVIRT trained for accelerator days on one\nto two hundred thousand images. In this work, we close\nthis gap and study the behaviors of image classifiers trained\nwith natural language supervision at large scale. Enabled\nby the large amounts of publicly available data of this form\non the internet, we create a new dataset of 400 million (im\u0002age, text) pairs and demonstrate that a simplified version of\nConVIRT trained from scratch, which we call CLIP, for Con\u0002trastive Language-Image Pre-training, is an efficient method\nof learning from natural language supervision. We study\nthe scalability of CLIP by training a series of eight models\nspanning almost 2 orders of magnitude of compute and ob\u0002serve that transfer performance is a smoothly predictable\nfunction of compute (Hestness et al., 2017; Kaplan et al.,\n2020). We find that CLIP, similar to the GPT family, learns\nto perform a wide set of tasks during pre-training including\nOCR, geo-localization, action recognition, and many others.\nWe measure this by benchmarking the zero-shot transfer\nperformance of CLIP on over 30 existing datasets and find\nit can be competitive with prior task-specific supervised\nmodels. We also confirm these findings with linear-probe\nLearning Transferable Visual Models From Natural Language Supervision 3\nrepresentation learning analysis and show that CLIP out\u0002performs the best publicly available ImageNet model while\nalso being more computationally efficient. We additionally\nfind that zero-shot CLIP models are much more robust than\nequivalent accuracy supervised ImageNet models which\nsuggests that zero-shot evaluation of task-agnostic models is\nmuch more representative of a model\u2019s capability. These re\u0002sults have significant policy and ethical implications, which\nwe consider in Section 7.\n2M 33M 67M 134M 268M 400M\n# of images processed\n0\n5\n10\n15\n20\n25\n30\n35\n40\nZero-Shot ImageNet Accuracy\n4X efficiency 3X efficiency\nBag of Words Contrastive (CLIP)\nBag of Words Prediction\nTransformer Lan",
          "original_query": "Learning transferable visual models from natural language supervision",
          "cleaned_query": "Learning transferable visual models from natural language supervision",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments",
          "url": "https://ieeexplore.ieee.org/document/8578485",
          "content": "Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments \\| IEEE Conference Publication \\| IEEE Xplore\n\n### IEEE Account\n\n- [Change Username/Password](https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Update Address](https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Purchase Details\n\n- [Payment Options](https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Order History](https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n\n### Profile Information\n\n- [Communications Preferences](https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Profession and Education](https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Technical Interests](https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Need Help?\n\n- **US & Canada:** +1 800 678 4333\n- **Worldwide:** +1 732 981 0060\n\n- [Contact & Support](https://ieeexplore.ieee.org/xpl/contact)\n\n- [About IEEE _Xplore_](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n- [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n- [Help](https://ieeexplore.ieee.org/Xplorehelp)\n- [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n- [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n- [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n- [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n- [Privacy & Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\n\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n\n\u00a9 Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.",
          "original_query": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments",
          "cleaned_query": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2306.12156] Fast Segment Anything - arXiv",
          "url": "https://arxiv.org/abs/2306.12156",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2306.12156** (cs)\n\n\\[Submitted on 21 Jun 2023\\]\n\n# Title:Fast Segment Anything\n\nAuthors: [Xu Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao,+X), [Wenchao Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding,+W), [Yongqi An](https://arxiv.org/search/cs?searchtype=author&query=An,+Y), [Yinglong Du](https://arxiv.org/search/cs?searchtype=author&query=Du,+Y), [Tao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+T), [Min Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+M), [Ming Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+M), [Jinqiao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J)\n\nView a PDF of the paper titled Fast Segment Anything, by Xu Zhao and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2306.12156)\n\n> Abstract:The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at [this https URL](https://github.com/CASIA-IVA-Lab/FastSAM).\n\n| | |\n| --- | --- |\n| Comments: | Technical Report. The code is released at [this https URL](https://github.com/CASIA-IVA-Lab/FastSAM) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2306.12156](https://arxiv.org/abs/2306.12156) \\[cs.CV\\] |\n| | (or [arXiv:2306.12156v1](https://arxiv.org/abs/2306.12156v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2306.12156](https://doi.org/10.48550/arXiv.2306.12156) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Yongqi An \\[ [view email](https://arxiv.org/show-email/6d2d92ac/2306.12156)\\]\n\n**\\[v1\\]**\nWed, 21 Jun 2023 10:08:29 UTC (6,125 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Fast Segment Anything, by Xu Zhao and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2306.12156)\n- [TeX Source](https://arxiv.org/src/2306.12156)\n- [Other Formats](https://arxiv.org/format/2306.12156)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2306.12156&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2306.12156&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2023-06](https://arxiv.org/list/cs.CV/2023-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2306.12156?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2306.12156?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2306.12156)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2306.12156)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2306.12156)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2306.12156&description=Fast Segment Anything) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2306.12156&title=Fast Segment Anything)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2306.12156) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Fast segment anything",
          "cleaned_query": "Fast segment anything",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "g3D-LF: Generalizable 3D-Language Feature Fields for Embodied ...",
          "url": "https://arxiv.org/html/2411.17030",
          "content": "g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks\n# g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks\nZihan Wang \u2003\u2003Gim Hee Lee\nSchool of Computing, National University of Singapore\nzihan.wang@u.nus.edu\n###### Abstract\nWe introduceGeneralizable 3D-Language Feature Fields (g3D-LF), a 3D representation model pre-trained on large-scale 3D-language dataset for embodied tasks.Our g3D-LFprocesses posed RGB-D images from agents to encode feature fieldsfor: 1) Novel view representation predictions from any position in the 3D scene; 2) Generations of BEV maps centered on the agent; 3) Querying targets using multi-granularity language within the above-mentioned representations.Our representationcan be generalized to unseen environments, enabling real-time construction and dynamic updates. By volume rendering latent features along sampled rays and integrating semantic and spatial relationships through multiscale encoders,our g3D-LFproduces representations at different scales and perspectives, aligned with multi-granularity language, via multi-level contrastive learning.Furthermore, we prepare a large-scale 3D-language dataset to align the representations of the feature fields with language.Extensive experiments on Vision-and-Language Navigationunderboth Panorama and Monocular settings, Zero-shot Object Navigation, and Situated Question Answering tasks highlight the significant advantages and effectiveness of ourg3D-LFfor embodied tasks. The code is available at[https://github.com/MrZihan/g3D-LF](https://github.com/MrZihan/g3D-LF).\n## 1Introduction\nEmbodied agents seek to understand 3D environments, enabling interaction with environments and humanby performingtasks such as Question Answering> [\n[> 37\n](https://arxiv.org/html/2411.17030v1#bib.bib37)> , [> 4\n](https://arxiv.org/html/2411.17030v1#bib.bib4)> , [> 40\n](https://arxiv.org/html/2411.17030v1#bib.bib40)> ]\n, Navigation> [\n[> 6\n](https://arxiv.org/html/2411.17030v1#bib.bib6)> , [> 39\n](https://arxiv.org/html/2411.17030v1#bib.bib39)> , [> 62\n](https://arxiv.org/html/2411.17030v1#bib.bib62)> , [> 3\n](https://arxiv.org/html/2411.17030v1#bib.bib3)> , [> 27\n](https://arxiv.org/html/2411.17030v1#bib.bib27)> , [> 28\n](https://arxiv.org/html/2411.17030v1#bib.bib28)> ]\n,*etc*.\nTo this end, various 3D scene representation models tailored for embodied tasks have been proposed, including point cloud-based models> [\n[> 73\n](https://arxiv.org/html/2411.17030v1#bib.bib73)> , [> 22\n](https://arxiv.org/html/2411.17030v1#bib.bib22)> , [> 11\n](https://arxiv.org/html/2411.17030v1#bib.bib11)> ]\n, 3D occupancy> [\n[> 34\n](https://arxiv.org/html/2411.17030v1#bib.bib34)> ]\n, hybrid voxel> [\n[> 14\n](https://arxiv.org/html/2411.17030v1#bib.bib14)> ]\n, and feature fields> [\n[> 49\n](https://arxiv.org/html/2411.17030v1#bib.bib49)> , [> 64\n](https://arxiv.org/html/2411.17030v1#bib.bib64)> , [> 57\n](https://arxiv.org/html/2411.17030v1#bib.bib57)> , [> 44\n](https://arxiv.org/html/2411.17030v1#bib.bib44)> ]\n.\nFor multimodal embodied tasks in large-scale scenes, 3D representation models typically need: 1) generalization to unseen scenes, 2) construct and update representations in real time, and 3) open-vocabulary semantic space. The generalizable 3D feature fields provides the above advantages and has been widely explored across various embodied tasks. Unlike point cloud-based models that depend on complete and low-noise point clouds which are less robust, the implicit representations of the feature fields are derived from the 2D foundation model, preserving semantic expressiveness even with few-shot observations from 3D scenes. As shown in Figure[1](https://arxiv.org/html/2411.17030v1#S1.F1), the feature fields model uses RGB-D images as input to encode and update implicit scene representations, which are then used to predict novel view, panorama and BEV map representations associated with language through volume rendering.These predicted representations can assist embodied tasks such as navigation planning> [\n[> 57\n](https://arxiv.org/html/2411.17030v1#bib.bib57)> , [> 58\n](https://arxiv.org/html/2411.17030v1#bib.bib58)> , [> 44\n](https://arxiv.org/html/2411.17030v1#bib.bib44)> ]\n,*etc*.\n![Refer to caption](x1.png)\nFigure 1:Our g3D-LF uses posed RGB-D images from the agent to predict novel view and BEV map representations at various scales within the 3D scene, aligned with multi-granularity language through 3D-language pre-training. The representation isapplicable to embodied tasks like visual navigation and embodied question answering, facilitating scene representation, language-guided querying, and navigation planning.\nHowever, several significant drawbacks remain in these feature fields models:1) The supervision for the predicted representations comes from 2D foundation models,e.g., CLIP> [\n[> 45\n](https://arxiv.org/html/2411.17030v1#bib.bib45)> ]\nand DINOv2> [\n[> 42\n](https://arxiv.org/html/2411.17030v1#bib.bib42)> ]\ngreatly limits the understanding for 3D spatial relationships; 2) These models are trained without language supervision, resulting in a substantial gap with language semantics; 3) The large-scale representations,e.g., panorama and BEV map from feature fields is particularly challenging for long text understanding. These issues severely limit the potential of the feature fields model on language-guided embodied tasks.\nTo circumvent the above-mentioned issues, we introduce Generalizable 3D-Language Feature Fields (g3D-LF), a 3D representation model pre-trained on large-scale 3D-language dataset for embodied tasks.Wefirst curate and consolidatea large amount of 3D-language data from previous works> [\n[> 23\n](https://arxiv.org/html/2411.17030v1#bib.bib23)> , [> 7\n](https://arxiv.org/html/2411.17030v1#bib.bib7)> , [> 66\n](https://arxiv.org/html/2411.17030v1#bib.bib66)> ]\nto train ourg3D-LFmodel.These data include5K indoor scenes and almost 1M language descriptions of multiple granularities. The text annotations include object categories, object characteristics, object relationships, and the spatial layout of the entire scene, which are employed to supervise multiscale encoders of theg3D-LFmodel.We then design our g3D-LF model to learn generalizable 3D-language feature fields.To this end, we employ multi-level contrastive learning for multi-scale encoders to align predicted representations and language across different scales.For the regional representation within the novel view,acontrastive loss is calculated across 1,883 indoor object categories. For the predicted novel view representation, both the CLIP visual representations and language are employed for contrastive training to balance generalization ability and language alignment. For large-scale panorama and BEV representations, we propose the fine-grained contrastive learning based on the affinity matrix to achieve long text understanding.\nThe pre-trainedg3D-LFmodel issubsequentlyevaluated on various embodied tasks, including vision-and-language navigation (monocular setting> [\n[> 58\n](https://arxiv.org/html/2411.17030v1#bib.bib58)> ]\nand panorama setting> [\n[> 57\n](https://arxiv.org/html/2411.17030v1#bib.bib57)> ]\n), zero-shot object navigation> [\n[> 62\n](https://arxiv.org/html/2411.17030v1#bib.bib62)> ]\n, and situated question answering> [\n[> 37\n](https://arxiv.org/html/2411.17030v1#bib.bib37)> ]\n, gains significant performance improvements.\nIn this work, ourmain contributionsinclude:\n* \u2022We organize a large-scale 3D-language dataset to train the feature fields model.\n* \u2022This work proposes the Generalizable 3D-Language Feature Fields (g3D-LF)witha multi-level contrastive learning framework to align the multi-scale representations of feature fields with multi-granularity language.\n* \u2022Our proposed g3D-LFmodel improves multiple baseline methods to state-of-the-art performance across various embodied tasks,thus validatingthe potential ofourgeneralizable feature fields for Embodied AI.\n## 2Related Work\nGeneralizable 3D Feature Fields.The neural radiance field (NeRF)> [\n[> 41\n](https://arxiv.org/html/2411.17030v1#bib.bib41)> ]\nhas gained significant popularity in various AI tasks, which predicts the RGB image from an arbitrary viewpoint in a 3D scene. Furthermore, some works leverage NeRF-based methods to predict novel view representations instead of RGB values, enabling 3D semantic segmentation> [\n[> 51\n](https://arxiv.org/html/2411.17030v1#bib.bib51)> ]\nand 3D language grounding> [\n[> 24\n](https://arxiv.org/html/2411.17030v1#bib.bib24)> ]\n. However, these methods with implicit MLP networks can only synthesize novel view representations in seen scenes, which makes it difficult to generalize to unseen large-scale scenes and adapt to many embodied AI tasks (e.g., navigation). To this end, some works> [\n[> 57\n](https://arxiv.org/html/2411.17030v1#bib.bib57)> , [> 44\n](https://arxiv.org/html/2411.17030v1#bib.bib44)> , [> 50\n](https://arxiv.org/html/2411.17030v1#bib.bib50)> ]\nattempt to encode 2D visual observations into 3D representations (called Generalizable 3D Feature Fields) via the depth map. Through volume rendering> [\n[> 41\n](https://arxiv.org/html/2411.17030v1#bib.bib41)> ]\n, these models decode novel view representations from the feature fields and align them with open-world features (e.g., CLIP embeddings> [\n[> 45\n](https://arxiv.org/html/2411.17030v1#bib.bib45)> ]\n). The 3D feature fields can generalize to unseen scenes, enabling real-time construction and dynamic updates. However, the drawback of these models lies in the fact that the supervision of their predicted representations comes from 2D visual models, which limits their performance in language-guided embodied tasks. Our work offers a feasible approach to training the 3D feature fields model with large-scale 3D-language data.\nVision-and-Language Navigation.Vision-and-Language Navigation (VLN)> [\n[> 3\n](https://arxiv.org/html/2411.17030v1#bib.bib3)> , [> 27\n](https://arxiv.org/html/2411.17030v1#bib.bib27)> , [> 69\n](https://arxiv.org/h",
          "original_query": "Generalizable 3D-language feature fields for embodied tasks",
          "cleaned_query": "Generalizable 3D-language feature fields for embodied tasks",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "Matterport3D: Learning from RGB-D Data in Indoor Environments",
          "url": "https://arxiv.org/abs/1709.06158",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:1709.06158** (cs)\n\n\\[Submitted on 18 Sep 2017\\]\n\n# Title:Matterport3D: Learning from RGB-D Data in Indoor Environments\n\nAuthors: [Angel Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang,+A), [Angela Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai,+A), [Thomas Funkhouser](https://arxiv.org/search/cs?searchtype=author&query=Funkhouser,+T), [Maciej Halber](https://arxiv.org/search/cs?searchtype=author&query=Halber,+M), [Matthias Nie\u00dfner](https://arxiv.org/search/cs?searchtype=author&query=Nie%C3%9Fner,+M), [Manolis Savva](https://arxiv.org/search/cs?searchtype=author&query=Savva,+M), [Shuran Song](https://arxiv.org/search/cs?searchtype=author&query=Song,+S), [Andy Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng,+A), [Yinda Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+Y)\n\nView a PDF of the paper titled Matterport3D: Learning from RGB-D Data in Indoor Environments, by Angel Chang and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/1709.06158)\n\n> Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:1709.06158](https://arxiv.org/abs/1709.06158) \\[cs.CV\\] |\n| (or [arXiv:1709.06158v1](https://arxiv.org/abs/1709.06158v1) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1709.06158](https://doi.org/10.48550/arXiv.1709.06158) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Matthias Nie\u00dfner \\[ [view email](https://arxiv.org/show-email/77fff39b/1709.06158)\\] **\\[v1\\]**\nMon, 18 Sep 2017 20:34:48 UTC (18,615 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Matterport3D: Learning from RGB-D Data in Indoor Environments, by Angel Chang and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/1709.06158)\n- [TeX Source](https://arxiv.org/src/1709.06158)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1709.06158&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1709.06158&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2017-09](https://arxiv.org/list/cs.CV/2017-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1709.06158?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1709.06158)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1709.06158)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1709.06158)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1709.html#abs-1709-06158) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1709-06158)\n\n[Angel X. Chang](https://dblp.uni-trier.de/search/author?author=Angel%20X.%20Chang) [Angela Dai](https://dblp.uni-trier.de/search/author?author=Angela%20Dai) [Thomas A. Funkhouser](https://dblp.uni-trier.de/search/author?author=Thomas%20A.%20Funkhouser) [Maciej Halber](https://dblp.uni-trier.de/search/author?author=Maciej%20Halber) [Matthias Nie\u00dfner](https://dblp.uni-trier.de/search/author?author=Matthias%20Nie%C3%9Fner)\n\n\u2026\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1709.06158) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Matterport3D: Learning from rgb-d data in indoor environments",
          "cleaned_query": "Matterport3D: Learning from rgb-d data in indoor environments",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2312.08168] Chat-Scene: Bridging 3D Scene and Large ...",
          "url": "https://arxiv.org/abs/2312.08168",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2312.08168** (cs)\n\n\\[Submitted on 13 Dec 2023 ( [v1](https://arxiv.org/abs/2312.08168v1)), last revised 28 Sep 2024 (this version, v4)\\]\n\n# Title:Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers\n\nAuthors: [Haifeng Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+H), [Yilun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+Y), [Zehan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Z), [Rongjie Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+R), [Runsen Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+R), [Tai Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+T), [Luping Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+L), [Xize Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng,+X), [Yang Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao,+Y), [Jiangmiao Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang,+J), [Zhou Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao,+Z)\n\nView a PDF of the paper titled Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers, by Haifeng Huang and 10 other authors\n\n[View PDF](https://arxiv.org/pdf/2312.08168) [HTML (experimental)](https://arxiv.org/html/2312.08168v4)\n\n> Abstract:Recent advancements in 3D Large Language Models (LLMs) have demonstrated promising capabilities for 3D scene understanding. However, previous methods exhibit deficiencies in general referencing and grounding capabilities for intricate scene comprehension. In this paper, we introduce the use of object identifiers and object-centric representations to interact with scenes at the object level. Specifically, we decompose the input 3D scene into a set of object proposals, each assigned a unique identifier token, which enables efficient object referencing and grounding during user-assistant interactions. Given the scarcity of scene-language data, we model the scene embeddings as a sequence of explicit object-level embeddings, derived from semantic-rich 2D or 3D representations. By employing object identifiers, we transform diverse 3D scene-language tasks into a unified question-answering format, facilitating joint training without the need for additional task-specific heads. With minimal fine-tuning on all downstream tasks, our model significantly outperforms existing methods on benchmarks including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2312.08168](https://arxiv.org/abs/2312.08168) \\[cs.CV\\] |\n| | (or [arXiv:2312.08168v4](https://arxiv.org/abs/2312.08168v4) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2312.08168](https://doi.org/10.48550/arXiv.2312.08168) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Haifeng Huang \\[ [view email](https://arxiv.org/show-email/35c3ed63/2312.08168)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2312.08168v1)**\nWed, 13 Dec 2023 14:27:45 UTC (4,084 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2312.08168v2)**\nFri, 15 Dec 2023 06:15:33 UTC (4,089 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2312.08168v3)**\nThu, 26 Sep 2024 16:51:37 UTC (5,081 KB)\n\n**\\[v4\\]**\nSat, 28 Sep 2024 03:56:28 UTC (5,577 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers, by Haifeng Huang and 10 other authors\n\n- [View PDF](https://arxiv.org/pdf/2312.08168)\n- [HTML (experimental)](https://arxiv.org/html/2312.08168v4)\n- [TeX Source](https://arxiv.org/src/2312.08168)\n- [Other Formats](https://arxiv.org/format/2312.08168)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2312.08168&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2312.08168&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2023-12](https://arxiv.org/list/cs.CV/2023-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2312.08168?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2312.08168)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2312.08168)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2312.08168)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2312.08168&description=Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2312.08168&title=Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2312.08168) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Chat-scene: Bridging 3D scene and large language models with object identifiers",
          "cleaned_query": "Chat-scene: Bridging 3D scene and large language models with object identifiers",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "CLIP-Guided FastSAM for Open-Vocabulary Instance Segmentation\nTrain FastSAM-style CNN instance segmentation to predict masks plus CLIP-aligned region embeddings, so each segment can be classified/retrieved via text prompts at inference. Use pseudo-labeling on web images: generate masks with FastSAM, then align each mask\u2019s pooled visual feature to its paired caption tokens with a contrastive loss. Evaluate on open-vocabulary segmentation benchmarks and downstream robotics perception where category lists are unknown."
      ],
      "generated_ideas_raw": "1. **CLIP-Guided FastSAM for Open-Vocabulary Instance Segmentation**\n   Train FastSAM-style CNN instance segmentation to predict masks plus CLIP-aligned region embeddings, so each segment can be classified/retrieved via text prompts at inference. Use pseudo-labeling on web images: generate masks with FastSAM, then align each mask\u2019s pooled visual feature to its paired caption tokens with a contrastive loss. Evaluate on open-vocabulary segmentation benchmarks and downstream robotics perception where category lists are unknown.\n\n2. **Language-Queryable 3D Feature Fields with Segment-Level Supervision**\n   Extend g3D-LF by injecting 2D segment proposals (FastSAM) from each RGB view into the 3D feature field update, associating each proposal with a language embedding (CLIP) before volumetric fusion. This creates a 3D \u201csegment graph\u201d embedded in the field, enabling queries like \u201cthe chair with a red cushion\u201d to return 3D-consistent object extents. Benchmark on Matterport3D-based object localization and situated QA with improved grounding consistency across viewpoints.\n\n3. **Object-Identifier Tokens for Navigation Memory in VLN**\n   Combine Chat-Scene\u2019s object identifier mechanism with VLN by maintaining a persistent set of discovered object IDs during an episode (from RGB-D projections or g3D-LF BEV outputs). Modify the instruction follower to resolve references (\u201cturn left at the sofa\u201d) by attending to identifier tokens tied to spatial memory, rather than transient visual features. Evaluate on VLN in Matterport3D with metrics for reference resolution accuracy and long-horizon instruction completion.\n\n4. **Multi-Granularity Language Contrastive Pretraining for BEV Maps**\n   Pretrain an agent-centric BEV encoder using g3D-LF-style multiscale representations, but align BEV patches to multi-granularity text (object phrases, room descriptions, route snippets) via CLIP-like contrastive objectives. Create positives by projecting Matterport3D 3D semantics into BEV and pairing with templated and mined natural language; create hard negatives via spatially adjacent but semantically different regions. Test zero-shot transfer to navigation planning and open-vocab place recognition.\n\n5. **Real-Time Open-Vocabulary Scene Graph Construction from RGB-D**\n   Build an online pipeline that uses FastSAM to segment each frame, lifts masks into 3D with poses (Matterport3D-style), and assigns each object a Chat-Scene identifier plus a CLIP-derived name distribution. Incrementally update relations (on-top-of, next-to, inside) using geometric constraints from the reconstructed mesh/point cloud, yielding a queryable 3D scene graph. Evaluate on ScanRefer/ScanQA-style grounding plus embodied tasks requiring relation reasoning.\n\n6. **Instruction-Following with Uncertainty-Aware Language Grounding**\n   Introduce calibrated uncertainty for text-to-visual grounding by estimating confidence over candidate objects/regions (from CLIP similarity + segmentation quality + view coverage from g3D-LF). Use this uncertainty to drive active perception: when ambiguity is high, the agent selects viewpoints that maximally reduce entropy over referenced entities. Demonstrate improved success rate and fewer catastrophic errors on ambiguous VLN instructions in Matterport3D.\n\n7. **Long-Instruction Summarization into Spatial Subgoals via 3D Memory**\n   Use an LLM to rewrite long VLN instructions into an ordered list of spatial subgoals expressed in terms of object identifiers and BEV waypoints (e.g., \u201cgo to #obj12 (couch), then to doorway near #obj7\u201d). Ground subgoals using g3D-LF BEV predictions and Chat-Scene-style object tokens, enabling stepwise verification and replanning. Evaluate on long-horizon VLN settings with improved interpretability and recovery from deviations.\n\n8. **Cross-View Consistency Losses for Segment-Then-Fuse 3D Reconstruction**\n   When integrating 2D segments into a 3D feature field, add a cross-view consistency objective: the same lifted 3D object should re-render to consistent masks across nearby viewpoints. Use Matterport3D poses to enforce cycle consistency between (segment \u2192 lift \u2192 render \u2192 segment) pipelines, improving object permanence and reducing ID switches. Measure gains in 3D grounding stability, object navigation, and multi-turn 3D dialog referencing.\n\n9. **Low-Compute Contrastive Pretraining on RGB-D Panoramas for Embodied CLIP**\n   Train a CLIP-like model specialized for indoor embodied settings by leveraging Matterport3D panoramas paired with automatically generated text (room type, object lists, spatial relations from 3D annotations). Incorporate depth-aware augmentations and a lightweight vision backbone (inspired by FastSAM\u2019s efficiency) to make the representation deployable on robots. Evaluate zero-shot transfer to VLN, open-vocab object nav, and indoor retrieval compared to generic CLIP.\n\n10. **Interactive 3D Referring Expression Resolution with Identifier-Aware Clarification**\n   Extend Chat-Scene to support clarification questions when a user\u2019s referring expression maps to multiple candidate objects in 3D (e.g., \u201cthe chair\u201d \u2192 {#3, #9}). The model proposes discriminative follow-ups based on available attributes/relations derived from g3D-LF features and segment-derived colors/material cues (\u201cDo you mean #3 near the window or #9 by the table?\u201d). Benchmark on multi-turn 3D grounding tasks with reduced failure under ambiguous language and improved user-aligned interaction.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "CLIP-Guided FastSAM for Open-Vocabulary Instance Segmentation\nTrain FastSAM-style CNN instance segmentation to predict masks plus CLIP-aligned region embeddings, so each segment can be classified/retr",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 59,
      "paper_title": "Learning (Approximately) Equivariant Networks via Constrained Optimization",
      "contribution": "The paper introduces Adaptive Constrained Equivariance (ACE), a framework that systematically relaxes equivariance constraints during training to improve performance in equivariant neural networks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 11586,
      "output_tokens": 1078,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1602.07576] Group Equivariant Convolutional Networks - arXiv",
          "url": "https://arxiv.org/abs/1602.07576",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1602.07576** (cs)\n\n\\[Submitted on 24 Feb 2016 ( [v1](https://arxiv.org/abs/1602.07576v1)), last revised 3 Jun 2016 (this version, v3)\\]\n\n# Title:Group Equivariant Convolutional Networks\n\nAuthors: [Taco S. Cohen](https://arxiv.org/search/cs?searchtype=author&query=Cohen,+T+S), [Max Welling](https://arxiv.org/search/cs?searchtype=author&query=Welling,+M)\n\nView a PDF of the paper titled Group Equivariant Convolutional Networks, by Taco S. Cohen and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/1602.07576)\n\n> Abstract:We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1602.07576](https://arxiv.org/abs/1602.07576) \\[cs.LG\\] |\n| (or [arXiv:1602.07576v3](https://arxiv.org/abs/1602.07576v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1602.07576](https://doi.org/10.48550/arXiv.1602.07576) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Proceedings of the International Conference on Machine Learning (ICML), 2016 |\n\n## Submission history\n\nFrom: Taco Cohen \\[ [view email](https://arxiv.org/show-email/38ca562a/1602.07576)\\] **[\\[v1\\]](https://arxiv.org/abs/1602.07576v1)**\nWed, 24 Feb 2016 16:17:15 UTC (105 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1602.07576v2)**\nFri, 11 Mar 2016 18:26:26 UTC (106 KB)\n**\\[v3\\]**\nFri, 3 Jun 2016 10:54:16 UTC (454 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Group Equivariant Convolutional Networks, by Taco S. Cohen and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/1602.07576)\n- [TeX Source](https://arxiv.org/src/1602.07576)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1602.07576&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1602.07576&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2016-02](https://arxiv.org/list/cs.LG/2016-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1602.07576?context=cs) [stat](https://arxiv.org/abs/1602.07576?context=stat) [stat.ML](https://arxiv.org/abs/1602.07576?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1602.07576)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1602.07576)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1602.07576)\n\n### [2 blog links](https://arxiv.org/tb/1602.07576)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1602.html#CohenW16) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/CohenW16)\n\n[Taco S. Cohen](https://dblp.uni-trier.de/search/author?author=Taco%20S.%20Cohen) [Max Welling](https://dblp.uni-trier.de/search/author?author=Max%20Welling)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1602.07576) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Group equivariant convolutional networks",
          "cleaned_query": "Group equivariant convolutional networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Energy functions for early vision and analog networks - PubMed",
          "url": "https://pubmed.ncbi.nlm.nih.gov/2742915/",
          "content": "Energy functions for early vision and analog networks - PubMed\nClipboard, Search History, and several other advanced features are temporarily unavailable.\n[Skip to main page content](#article-details)\n![U.S. flag](https://cdn.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png)\nAn official website of the United States government\nHere's how you know\n![Dot gov](https://cdn.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg)\n**The .gov means it\u2019s official.**\nFederal government websites often end in .gov or .mil. Before\nsharing sensitive information, make sure you\u2019re on a federal\ngovernment site.\n![Https](https://cdn.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg)\n**The site is secure.**\nThe**https://**ensures that you are connecting to the\nofficial website and that any information you provide is encrypted\nand transmitted securely.\n[![NIH NLM Logo](https://cdn.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg)](https://www.ncbi.nlm.nih.gov/)\n[Log in](https://account.ncbi.nlm.nih.gov)Show account info\nClose#### Account\nLogged in as:\n**username**\n* [Dashboard](https://pubmed.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://pubmed.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://pubmed.ncbi.nlm.nih.gov/account/settings/)\n* [Log out](https://pubmed.ncbi.nlm.nih.gov/account/signout/)\n[Access keys](https://www.ncbi.nlm.nih.gov/guide/browsers/#ncbi_accesskeys)[NCBI Homepage](https://www.ncbi.nlm.nih.gov)[MyNCBI Homepage](https://pubmed.ncbi.nlm.nih.gov/myncbi/)[Main Content](#maincontent)[Main Navigation](#)\n[![pubmed logo](https://cdn.ncbi.nlm.nih.gov/pubmed/6aa07a65-6caa-4bbc-acaa-48623b669c29/core/images/pubmed-logo-blue.svg)](https://pubmed.ncbi.nlm.nih.gov/)[](#)\nSearch:[](#)Search\n[Advanced](https://pubmed.ncbi.nlm.nih.gov/advanced/)[Clipboard](https://pubmed.ncbi.nlm.nih.gov/clipboard/)\n[User Guide](https://pubmed.ncbi.nlm.nih.gov/help/)\nSaveEmail\nSend to\n* [Clipboard](#)\n* [My Bibliography](https://account.ncbi.nlm.nih.gov/?back_url=https://pubmed.ncbi.nlm.nih.gov/2742915/#open-bibliography-panel)\n* [Collections](https://account.ncbi.nlm.nih.gov/?back_url=https://pubmed.ncbi.nlm.nih.gov/2742915/#open-collections-panel)\n* [Citation manager](#)\nDisplay options\nDisplay options\nFormatAbstractPubMedPMID\n## Save citation to file\nFormat:Summary (text)PubMedPMIDAbstract (text)CSV\nCreate fileCancel\n## Email citation\nEmail address has not been verified. Go to[My NCBI account settings](https://account.ncbi.nlm.nih.gov/settings/)to confirm your email and then refresh this page.\nTo:\nSubject:\nBody:\nFormat:SummarySummary (text)AbstractAbstract (text)\nMeSH and other data\nSend emailCancel\n### Add to Collections\n* Create a new collection\n* Add to an existing collection\nName your collection:\nName must be less than 100 characters\nChoose a collection:\nUnable to load your collection due to an error\n[Please try again](#)\nAddCancel\n### Add to My Bibliography\n* My Bibliography\nUnable to load your delegates due to an error\n[Please try again](#)\nAddCancel\n## Your saved search\nName of saved search:\nSearch terms:\n[Test search terms](#)\nWould you like email updates of new search results?Saved Search Alert Radio Buttons\n* Yes\n* No\nEmail:([change](https://www.ncbi.nlm.nih.gov/account/settings/))\nFrequency:MonthlyWeeklyDaily\nWhich day?The first SundayThe first MondayThe first TuesdayThe first WednesdayThe first ThursdayThe first FridayThe first SaturdayThe first dayThe first weekday\nWhich day?SundayMondayTuesdayWednesdayThursdayFridaySaturday\nReport format:SummarySummary (text)AbstractAbstract (text)PubMed\nSend at most:1 item5 items10 items20 items50 items100 items200 items\nSend even when there aren't any new results\nOptional text in email:\nSaveCancel\n## Create a file for external citation management software\nCreate fileCancel\n## Your RSS Feed\nName of RSS Feed:\nNumber of items displayed:510152050100\nCreate RSSCancel\nRSS LinkCopy\n### Actions\nCite\nCollections\nAdd to Collections\n* Create a new collection\n* Add to an existing collection\nName your collection:\nName must be less than 100 characters\nChoose a collection:\nUnable to load your collection due to an error\n[Please try again](#)\nAddCancel\nPermalink\nPermalink\nCopy\nDisplay options\nDisplay options\nFormatAbstractPubMedPMID\nBiol Cybern\nActions\n* [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Biol+Cybern%22[jour]&amp;sort=date&amp;sort_order=desc)\n* [Search in NLM Catalog](https://www.ncbi.nlm.nih.gov/nlmcatalog?term=\"Biol+Cybern\"[Title+Abbreviation])\n* [Add to Search](#)\n.1989;61(2):115-23.\ndoi: 10.1007/BF00204595.\n# Energy functions for early vision and analog networks\n[A L Yuille](https://pubmed.ncbi.nlm.nih.gov/?term=Yuille+AL&amp;cauthor_id=2742915)[1](#full-view-affiliation-1)\nAffiliationsExpand\n### Affiliation\n* 1Harvard University, Cambridge, MA 02138.\n* PMID:**2742915**\n* DOI:[10.1007/BF00204595](https://doi.org/10.1007/bf00204595)\nItem in Clipboard\n# Energy functions for early vision and analog networks\nA L Yuille.Biol Cybern.1989.\nShow details\nDisplay options\nDisplay options\nFormatAbstractPubMedPMID\nBiol Cybern\nActions\n* [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Biol+Cybern%22[jour]&amp;sort=date&amp;sort_order=desc)\n* [Search in NLM Catalog](https://www.ncbi.nlm.nih.gov/nlmcatalog?term=\"Biol+Cybern\"[Title+Abbreviation])\n* [Add to Search](#)\n.1989;61(2):115-23.\ndoi: 10.1007/BF00204595.\n### Author\n[A L Yuille](https://pubmed.ncbi.nlm.nih.gov/?term=Yuille+AL&amp;cauthor_id=2742915)[1](#short-view-affiliation-1)\n### Affiliation\n* 1Harvard University, Cambridge, MA 02138.\n* PMID:**2742915**\n* DOI:[10.1007/BF00204595](https://doi.org/10.1007/bf00204595)\nItem in Clipboard\nCite\nDisplay options\nDisplay options\nFormatAbstractPubMedPMID\n## Abstract\nThis paper describes attempts to model the modules of early vision in terms of minimizing energy functions, in particular energy functions allowing discontinuities in the solution. It examines the success of using Hopfield-style analog networks for solving such problems. Finally it discusses the limitations of the energy function approach.\n[PubMed Disclaimer](https://pubmed.ncbi.nlm.nih.gov/disclaimer/)\n## References\n1. 1. Spat Vis. 1988;3(1):15-44\n-[PubMed](https://pubmed.ncbi.nlm.nih.gov/3153661/)\n2. 1. Proc R Soc Lond B Biol Sci. 1979 May 23;204(1156):301-28\n-[PubMed](https://pubmed.ncbi.nlm.nih.gov/37518/)\n2. 1. Nature. 1985 Sep 26-Oct 2;317(6035):314-9\n-[PubMed](https://pubmed.ncbi.nlm.nih.gov/2413361/)\n2. 1. Proc Natl Acad Sci U S A. 1982 Apr;79(8):2554-8\n-[PubMed](https://pubmed.ncbi.nlm.nih.gov/6953413/)\n2. 1. Vision Res. 1983;23(12):1719-24\n-[PubMed](https://pubmed.ncbi.nlm.nih.gov/6666076/)\nShow all 13 references\n## Publication types\n* Research Support, Non-U.S. Gov&#x27;t\nActions\n* [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Research+Support,+Non-U.S.+Gov%27t%22[pt]&amp;sort=date&amp;sort_order=desc)\n* [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Research+Support,+Non-U.S.+Gov't)\n* [Add to Search](#)\n* Research Support, U.S. Gov&#x27;t, Non-P.H.S.\nActions\n* [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Research+Support,+U.S.+Gov%27t,+Non-P.H.S.%22[pt]&amp;sort=date&amp;sort_order=desc)\n* [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Research+Support,+U.S.+Gov't,+Non-P.H.S.)\n* [Add to Search](#)\n## MeSH terms\n* Models, Neurological\\*\nActions\n* [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Models,+Neurological%22[MAJR]&amp;sort=date&amp;sort_order=desc)\n* [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Models,+Neurological)\n* [Add to Search](#)\n* Motion Perception / physiology\\*\nActions\n* [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Motion+Perception/physiology%22[MAJR]&amp;sort=date&amp;sort_order=desc)\n* [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Motion+Perception)\n* [Add to Search](#)\n* Visual Perception / physiology\\*\nActions\n* [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Visual+Perception/physiology%22[MAJR]&amp;sort=date&amp;sort_order=desc)\n* [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Visual+Perception)\n* [Add to Search](#)\n[x]\nCite\nCopyDownload .nbib.nbib\nFormat:AMAAPAMLANLM\n**Send To**\n* [Clipboard](#)\n* [Email](https://account.ncbi.nlm.nih.gov/?back_url=https://pubmed.ncbi.nlm.nih.gov/2742915/#open-email-panel)\n* [Save](#)\n* [My Bibliography](https://account.ncbi.nlm.nih.gov/?back_url=https://pubmed.ncbi.nlm.nih.gov/2742915/#open-bibliography-panel)\n* [Collections](https://account.ncbi.nlm.nih.gov/?back_url=https://pubmed.ncbi.nlm.nih.gov/2742915/#open-collections-panel)\n* [Citation Manager](#)[x]\nNCBI Literature Resources\n[MeSH](https://www.ncbi.nlm.nih.gov/mesh/)[PMC](https://www.ncbi.nlm.nih.gov/pmc/)[Bookshelf](https://www.ncbi.nlm.nih.gov/books)[Disclaimer](https://pubmed.ncbi.nlm.nih.gov/disclaimer/)\nThe PubMed wordmark and PubMed logo are registered trademarks of the U.S. Department of Health and Human Services (HHS). Unauthorized use of these marks is strictly prohibited.",
          "original_query": "Energy functions for early vision and analog networks",
          "cleaned_query": "Energy functions for early vision and analog networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2305.17592] Approximation-Generalization Trade-offs ...",
          "url": "https://arxiv.org/abs/2305.17592",
          "content": "\n Download PDF \nThe explicit incorporation of task-specific inductive biases through symmetry has emerged as a general design precept in the development of high-performance machine learning models. For example, group equivariant neural networks have demonstrated impressive performance across various domains and applications such as protein and drug design. A prevalent intuition about such models is that the integration of relevant symmetry results in enhanced generalization. Moreover, it is posited that when the data and/or the model may only exhibit $\\textit{approximate}$ or $\\textit{partial}$ symmetry, the optimal or best-performing model is one where the model symmetry aligns with the data symmetry. In this paper, we conduct a formal unified investigation of these intuitions. To begin, we present general quantitative bounds that demonstrate how models capturing task-specific symmetries lead to improved generalization. In fact, our results do not require the transformations to be finite or even form a group and can work with partial or approximate equivariance. Utilizing this quantification, we examine the more general question of model mis-specification i.e. when the model symmetries don't align with the data symmetries. We establish, for a given symmetry group, a quantitative comparison between the approximate/partial equivariance of the model and that of the data distribution, precisely connecting model equivariance error and data equivariance error. Our result delineates conditions under which the model equivariance error is optimal, thereby yielding the best-performing model for the given task and data.\n \n \n Submission history From: Shubhendu Trivedi [ view email] [v1] \n Sat, 27 May 2023 22:53:37 UTC (145 KB) \n ||||I|||| Skip to main content\n We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\n > cs > arXiv:2305.17592\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:2305.17592 (cs)\n [Submitted on 27 May 2023]\n\n Title: Approximation-Generalization Trade-offs under (Approximate) Group Equivariance\n\n Authors: Mircea Petrache, Shubhendu Trivedi\n Download a PDF of the paper titled Approximation-Generalization Trade-offs under (Approximate) Group Equivariance, by Mircea Petrache and 1 other authors\n Download PDF\n Abstract: The explicit incorporation of task-specific inductive biases through symmetry has emerged as a general design precept in the development of high-performance machine learning models. For example, group equivariant neural networks have demonstrated impressive performance across various domains and applications such as protein and drug design. A prevalent intuition about such models is that the integration of relevant symmetry results in enhanced generalization. Moreover, it is posited that when the data and/or the model may only exhibit $\\textit{approximate}$ or $\\textit{partial}$ symmetry, the optimal or best-performing model is one where the model symmetry aligns with the data symmetry. In this paper, we conduct a formal unified investigation of these intuitions. To begin, we present general quantitative bounds that demonstrate how models capturing task-specific symmetries lead to improved generalization. In fact, our results do not require the transformations to be finite or even form a group and can work with partial or approximate equivariance. Utilizing this quantification, we examine the more general question of model mis-specification i.e. when the model symmetries don't align with the data symmetries. We establish, for a given symmetry group, a quantitative comparison between the approximate/partial equivariance of the model and that of the data distribution, precisely connecting model equivariance error and data equivariance error. Our result delineates conditions under which the model equivariance error is optimal, thereby yielding the best-performing model for the given task and data.\n Subjects: Machine Learning (cs.LG) ; Machine Learning (stat.ML)\n Cite as: arXiv:2305.17592 [cs.LG] \n (or arXiv:2305.17592v1 [cs.LG] for this version) \n https://doi.org/10.48550/arXiv.2305.17592 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Shubhendu Trivedi [view email]\n [v1] Sat, 27 May 2023 22:53:37 UTC (145 KB)\n Full-text links:\n\n Access Paper:\n\n Download a PDF of the paper titled Approximation-Generalization Trade-offs under (Approximate) Group Equivariance, by Mircea Petrache and 1 other authors\n * Download PDF\n * PostScript\n * Other Formats\n (view license)\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 2305\n Change to browse by:\n cs\n stat\n stat.ML\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export BibTeX citation Loading...\n\n BibTeX formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n DagsHub Toggle\n DagsHub (What is DagsHub?)\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Link to Influence Flower\n Influence Flower (What are Influence Flowers?)\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Approximation-generalization trade-offs under (approximate) group equivariance",
          "cleaned_query": "Approximation-generalization trade-offs under (approximate) group equivariance",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2306.02426] Resilient Constrained Learning - arXiv",
          "url": "https://arxiv.org/abs/2306.02426",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2306.02426** (cs)\n\n\\[Submitted on 4 Jun 2023 ( [v1](https://arxiv.org/abs/2306.02426v1)), last revised 11 Jan 2024 (this version, v4)\\]\n\n# Title:Resilient Constrained Learning\n\nAuthors: [Ignacio Hounie](https://arxiv.org/search/cs?searchtype=author&query=Hounie,+I), [Alejandro Ribeiro](https://arxiv.org/search/cs?searchtype=author&query=Ribeiro,+A), [Luiz F. O. Chamon](https://arxiv.org/search/cs?searchtype=author&query=Chamon,+L+F+O)\n\nView a PDF of the paper titled Resilient Constrained Learning, by Ignacio Hounie and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2306.02426)\n\n> Abstract:When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems that adapt to disruptions by modifying their operation. We show conditions under which this balance can be achieved and introduce a practical algorithm to compute it, for which we derive approximation and generalization guarantees. We showcase the advantages of this resilient learning method in image classification tasks involving multiple potential invariances and in heterogeneous federated learning.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2306.02426](https://arxiv.org/abs/2306.02426) \\[cs.LG\\] |\n| | (or [arXiv:2306.02426v4](https://arxiv.org/abs/2306.02426v4) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2306.02426](https://doi.org/10.48550/arXiv.2306.02426) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ignacio Hounie \\[ [view email](https://arxiv.org/show-email/76b6a382/2306.02426)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2306.02426v1)**\nSun, 4 Jun 2023 18:14:18 UTC (371 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2306.02426v2)**\nMon, 25 Sep 2023 19:26:06 UTC (378 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2306.02426v3)**\nTue, 31 Oct 2023 18:22:09 UTC (397 KB)\n\n**\\[v4\\]**\nThu, 11 Jan 2024 15:30:24 UTC (397 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Resilient Constrained Learning, by Ignacio Hounie and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2306.02426)\n- [TeX Source](https://arxiv.org/src/2306.02426)\n- [Other Formats](https://arxiv.org/format/2306.02426)\n\n[![license icon](https://arxiv.org/icons/licenses/by-nc-sa-4.0.png)view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2306.02426&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2306.02426&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-06](https://arxiv.org/list/cs.LG/2023-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2306.02426?context=cs)\n\n[stat](https://arxiv.org/abs/2306.02426?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2306.02426?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2306.02426)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2306.02426)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2306.02426)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2306.02426&description=Resilient Constrained Learning) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2306.02426&title=Resilient Constrained Learning)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2306.02426) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Resilient constrained learning",
          "cleaned_query": "Resilient constrained learning",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] SchNet: A continuous-filter convolutional neural network for ...",
          "url": "http://papers.neurips.cc/paper/6700-schnet-a-continuous-filter-convolutional-neural-network-for-modeling-quantum-interactions.pdf",
          "content": "SchNet: A continuous-filter convolutional neural\nnetwork for modeling quantum interactions\nK. T. Sch\u00fctt1\u2217, P.-J. Kindermans1, H. E. Sauceda2, S. Chmiela1\nA. Tkatchenko3, K.-R. M\u00fcller1,4,5\u2020\n1 Machine Learning Group, Technische Universit\u00e4t Berlin, Germany\n2 Theory Department, Fritz-Haber-Institut der Max-Planck-Gesellschaft, Berlin, Germany\n3 Physics and Materials Science Research Unit, University of Luxembourg, Luxembourg\n4 Max-Planck-Institut f\u00fcr Informatik, Saarbr\u00fccken, Germany\n5 Dept. of Brain and Cognitive Engineering, Korea University, Seoul, South Korea\n\u2217 kristof.schuett@tu-berlin.de \u2020 klaus-robert.mueller@tu-berlin.de\nAbstract\nDeep learning has the potential to revolutionize quantum chemistry as it is ideally\nsuited to learn representations for structured data and speed up the exploration\nof chemical space. While convolutional neural networks have proven to be the\nfirst choice for images, audio and video data, the atoms in molecules are not\nrestricted to a grid. Instead, their precise locations contain essential physical\ninformation, that would get lost if discretized. Thus, we propose to use continuous\u0002filter convolutional layers to be able to model local correlations without requiring\nthe data to lie on a grid. We apply those layers in SchNet: a novel deep learning\narchitecture modeling quantum interactions in molecules. We obtain a joint model\nfor the total energy and interatomic forces that follows fundamental quantum\u0002chemical principles. Our architecture achieves state-of-the-art performance for\nbenchmarks of equilibrium molecules and molecular dynamics trajectories. Finally,\nwe introduce a more challenging benchmark with chemical and structural variations\nthat suggests the path for further work.\n1 Introduction\nThe discovery of novel molecules and materials with desired properties is crucial for applications\nsuch as batteries, catalysis and drug design. However, the vastness of chemical compound space\nand the computational cost of accurate quantum-chemical calculations prevent an exhaustive explo\u0002ration. In recent years, there have been increased efforts to use machine learning for the accelerated\ndiscovery of molecules and materials with desired properties [1\u20139]. However, these methods are\nonly applied to stable systems in so-called equilibrium, i.e., local minima of the potential energy\nsurface E(r1, . . . , rn) where riis the position of atom i. Data sets such as the established QM9\nbenchmark [10] contain only equilibrium molecules. Predicting stable atom arrangements is in itself\nan important challenge in quantum chemistry and material science.\nIn general, it is not clear how to obtain equilibrium conformations without optimizing the atom\npositions. Therefore, we need to compute both the total energy E(r1, . . . , rn) and the forces acting\non the atoms\nFi(r1, . . . , rn) = \u2212\n\u2202E\n\u2202ri\n(r1, . . . , rn). (1)\nOne possibility is to use a less computationally costly, however, also less accurate quantum-chemical\napproximation. Instead, we choose to extend the domain of our machine learning model to both\ncompositional (chemical) and configurational (structural) degrees of freedom.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nIn this work, we aim to learn a representation for molecules using equilibrium and non-equilibrium\nconformations. Such a general representation for atomistic systems should follow fundamental\nquantum-mechanical principles. Most importantly, the predicted force field has to be curl-free.\nOtherwise, it would be possible to follow a circular trajectory of atom positions such that the energy\nkeeps increasing, i.e., breaking the law of energy conservation. Furthermore, the potential energy\nsurface as well as its partial derivatives have to be smooth, e.g., in order to be able to perform geometry\noptimization. Beyond that, it is beneficial that the model incorporates the invariance of the molecular\nenergy with respect to rotation, translation and atom indexing. Being able to model both chemical\nand conformational variations constitutes an important step towards ML-driven quantum-chemical\nexploration.\nThis work provides the following key contributions:\n\u2022 We propose continuous-filter convolutional (cfconv) layers as a means to move beyond\ngrid-bound data such as images or audio towards modeling objects with arbitrary positions\nsuch as astronomical observations or atoms in molecules and materials.\n\u2022 We propose SchNet: a neural network specifically designed to respect essential quantum\u0002chemical constraints. In particular, we use the proposed cfconv layers in R\n3\nto model\ninteractions of atoms at arbitrary positions in the molecule. SchNet delivers both rotationally\ninvariant energy prediction and rotationally equivariant force predictions. We obtain a\nsmooth potential energy surface and the resulting force-field is guaranteed to be energy\u0002conserving.\n\u2022 We present a new, challenging benchmark \u2013 ISO17 \u2013 including both chemical and confor\u0002mational changes3\n. We show that training with forces improves generalization in this setting\nas well.\n2 Related work\nPrevious work has used neural networks and Gaussian processes applied to hand-crafted features to\nfit potential energy surfaces [11\u201316]. Graph convolutional networks for circular fingerprint [17] and\nmolecular graph convolutions [18] learn representations for molecules of arbitrary size. They encode\nthe molecular structure using neighborhood relationships as well as bond features, e.g., one-hot\nencodings of single, double and triple bonds. In the following, we briefly review the related work that\nwill be used in our empirical evaluation: gradient domain machine learning (GDML), deep tensor\nneural networks (DTNN) and enn-s2s.\nGradient-domain machine learning (GDML) Chmiela et al. [19] proposed GDML as a method\nto construct force fields that explicitly obey the law of energy conservation. GDML captures the\nrelationship between energy and interatomic forces (see Eq. 1) by training the gradient of the energy\nestimator. The functional relationship between atomic coordinates and interatomic forces is thus\nlearned directly and energy predictions are obtained by re-integration. However, GDML does not\nscale well due to its kernel matrix growing quadratically with the number of atoms as well as the\nnumber of examples. Beyond that, it is not designed to represent different compositions of atom\ntypes unlike SchNet, DTNN and enn-s2s.\nDeep tensor neural networks (DTNN) Sch\u00fctt et al. [20] proposed the DTNN for molecules that\nare inspired by the many-body Hamiltonian applied to the interactions of atoms. They have been\nshown to reach chemical accuracy on a small set of molecular dynamics trajectories as well as QM9.\nEven though the DTNN shares the invariances with our proposed architecture, its interaction layers\nlack the continuous-filter convolution interpretation. It falls behind in accuracy compared to SchNet\nand enn-s2s.\nenn-s2s Gilmer et al. [21] proposed the enn-s2s as a variant of message-passing neural networks that\nuses bond type features in addition to interatomic distances. It achieves state-of-the-art performance\non all properties of the QM9 benchmark [21]. Unfortunately, it cannot be used for molecular dynamics\npredictions (MD-17). This is caused by discontinuities in their potential energy surface due to the\n3\nISO17 is publicly available at www.quantum-machine.org.\n2\nFigure 1: The discrete filter (left) is not able to capture the subtle positional changes of the atoms\nresulting in discontinuous energy predictions E\u02c6 (bottom left). The continuous filter captures these\nchanges and yields smooth energy predictions (bottom right).\ndiscreteness of the one-hot encodings in their input. In contrast, SchNet does not use such features\nand yields a continuous potential energy surface by using continuous-filter convolutional layers.\n3 Continuous-filter convolutions\nIn deep learning, convolutional layers operate on discretized signals such as image pixels [22, 23],\nvideo frames [24] or digital audio data [25]. While it is sufficient to define the filter on the same\ngrid in these cases, this is not possible for unevenly spaced inputs such as the atom positions of a\nmolecule (see Fig. 1). Other examples include astronomical observations [26], climate data [27]\nand the financial market [28]. Commonly, this can be solved by a re-sampling approach defining\na representation on a grid [7, 29, 30]. However, choosing an appropriate interpolation scheme is\na challenge on its own and, possibly, requires a large number of grid points. Therefore, various\nextensions of convolutional layers even beyond the Euclidean space exist, e.g., for graphs [31, 32]\nand 3d shapes[33]. Analogously, we propose to use continuous filters that are able to handle unevenly\nspaced data, in particular, atoms at arbitrary positions.\nGiven the feature representations of n objects Xl = (x\nl\n1\n, . . . , x\nl\nn\n) with x\nl\ni \u2208 R\nF at locations\nR = (r1, . . . , rn) with ri \u2208 R\nD, the continuous-filter convolutional layer l requires a filter-generating\nfunction\nWl: R\nD \u2192 RF\n,\nthat maps from a position to the corresponding filter values. This constitutes a generalization of a\nfilter tensor in discrete convolutional layers. As in dynamic filter networks [34], this filter-generating\nfunction is modeled with a neural network. While dynamic filter networks generate weights restricted\nto a grid structure, our approach generalizes this to arbitrary position and number of objects. The\noutput x\nl+1\ni\nfor the convolutional layer at position riis then given by\nx\nl+1\ni = (Xl\n\u2217 Wl)i =\nX\nj\nx\nl\nj \u25e6 Wl\n(ri \u2212 rj ), (2)\nwhere \"\u25e6\" represents the element-wise multiplication. We apply these convolutions feature-wise\nfor computational efficiency [35]. The interactions between feature maps are handled by separate\nobject-wise or, specifically, atom-wise layers in SchNet.\n4 SchNet\nSchNet is designed to learn a representation for the prediction of molecular energies and atomic\nforces",
          "original_query": "Schnet: A continuous-filter convolutional neural network for modeling quantum interactions",
          "cleaned_query": "Schnet: A continuous-filter convolutional neural network for modeling quantum interactions",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Residual Pathway Priors for Soft Equivariance Constraints - arXiv",
          "url": "https://arxiv.org/abs/2112.01388",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2112.01388** (cs)\n\n\\[Submitted on 2 Dec 2021\\]\n\n# Title:Residual Pathway Priors for Soft Equivariance Constraints\n\nAuthors: [Marc Finzi](https://arxiv.org/search/cs?searchtype=author&query=Finzi,+M), [Gregory Benton](https://arxiv.org/search/cs?searchtype=author&query=Benton,+G), [Andrew Gordon Wilson](https://arxiv.org/search/cs?searchtype=author&query=Wilson,+A+G)\n\nView a PDF of the paper titled Residual Pathway Priors for Soft Equivariance Constraints, by Marc Finzi and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2112.01388)\n\n> Abstract:There is often a trade-off between building deep learning systems that are expressive enough to capture the nuances of the reality, and having the right inductive biases for efficient learning. We introduce Residual Pathway Priors (RPPs) as a method for converting hard architectural constraints into soft priors, guiding models towards structured solutions, while retaining the ability to capture additional complexity. Using RPPs, we construct neural network priors with inductive biases for equivariances, but without limiting flexibility. We show that RPPs are resilient to approximate or misspecified symmetries, and are as effective as fully constrained models even when symmetries are exact. We showcase the broad applicability of RPPs with dynamical systems, tabular data, and reinforcement learning. In Mujoco locomotion tasks, where contact forces and directional rewards violate strict equivariance assumptions, the RPP outperforms baseline model-free RL agents, and also improves the learned transition models for model-based RL.\n\n| | |\n| --- | --- |\n| Comments: | NeurIPS 2021. Code available at [this https URL](https://github.com/mfinzi/residual-pathway-priors) |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2112.01388](https://arxiv.org/abs/2112.01388) \\[cs.LG\\] |\n| | (or [arXiv:2112.01388v1](https://arxiv.org/abs/2112.01388v1) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2112.01388](https://doi.org/10.48550/arXiv.2112.01388) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Andrew Wilson \\[ [view email](https://arxiv.org/show-email/03e2bf8a/2112.01388)\\]\n\n**\\[v1\\]**\nThu, 2 Dec 2021 16:18:17 UTC (1,160 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Residual Pathway Priors for Soft Equivariance Constraints, by Marc Finzi and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2112.01388)\n- [TeX Source](https://arxiv.org/src/2112.01388)\n- [Other Formats](https://arxiv.org/format/2112.01388)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2112.01388&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2112.01388&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-12](https://arxiv.org/list/cs.LG/2021-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2112.01388?context=cs)\n\n[stat](https://arxiv.org/abs/2112.01388?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2112.01388?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2112.01388)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2112.01388)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2112.01388)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2112.html#abs-2112-01388) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2112-01388)\n\n[Marc Finzi](https://dblp.uni-trier.de/search/author?author=Marc%20Finzi)\n\n[Andrew Gordon Wilson](https://dblp.uni-trier.de/search/author?author=Andrew%20Gordon%20Wilson)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2112.01388&description=Residual Pathway Priors for Soft Equivariance Constraints) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2112.01388&title=Residual Pathway Priors for Soft Equivariance Constraints)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2112.01388) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Residual pathway priors for soft equivariance constraints",
          "cleaned_query": "Residual pathway priors for soft equivariance constraints",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Equivariance-Aware Resilient Constraint Tuning for Vision Models\nExtend Resilient Constrained Learning to treat *equivariance error* (as in approximate/partial equivariance bounds) as an explicit, learnable constraint alongside accuracy. Implement an algorithm that adaptively relaxes/strengthens rotation/reflection equivariance during training of G-CNNs based on validation-estimated data symmetry, and quantify when this improves generalization under distribution shift (e.g., rotated vs. non-rotated test sets).",
        "Energy-Minimization Interpretable G-CNNs for Discontinuity-Preserving Segmentation\nCombine Yuille-style discontinuity-allowing energy functionals with group-equivariant feature maps by designing a G-CNN that outputs parameters of an energy (data term + equivariant regularizer + edge/discontinuity term). Train end-to-end by unrolling a small number of Hopfield/gradient-descent inference steps, yielding an interpretable segmentation/denoising system that preserves sharp boundaries while retaining rotation/reflection equivariance.",
        "Equivariance Error as a Regularizer: Calibrated Soft Constraints Across Layers\nIntroduce a layerwise \u201cequivariance budget\u201d where each layer in a G-CNN (or equivariant MLP/RNN) is penalized by a measurable equivariance deviation term, with weights learned via resilient constrained learning. This produces architectures that concentrate strict equivariance where it helps (early layers) while allowing later layers to deviate for task-specific asymmetries, and provides diagnostics linking where symmetry is broken to performance.",
        "Federated Learning of Heterogeneous Symmetries with Group-Conditioned G-Convolutions\nExtend the heterogeneous federated setting in Resilient Constrained Learning by allowing each client to have a different approximate symmetry group (e.g., camera rotations, sensor orientations). Implement a shared backbone with group-conditioned filters (choose group elements or mixture weights per client) and a global objective that trades off accuracy with per-client equivariance error, improving personalization without overfitting.",
        "Symmetry-Misalignment Benchmarks with Controlled Approximate Equivariance\nCreate a suite of synthetic-to-real benchmarks where the true data-generating process has a tunable degree of symmetry breaking (e.g., rotated MNIST with systematic stroke-thickness bias by angle, CIFAR with orientation-dependent lighting). Use these benchmarks to experimentally validate the approximation\u2013generalization trade-offs and identify regimes where soft priors (RPP) outperform hard G-CNN constraints.",
        "Equivariant Energy-Based Models with Learnable Discontinuity Priors\nDevelop an energy-based model (EBM) whose energy is parameterized by group-equivariant convolutions, but includes learnable discontinuity terms inspired by line-process/robust penalties from early vision. Train with score matching or contrastive methods, and compare against feedforward G-CNNs on tasks where multi-modal or ambiguous structure benefits from explicit energy landscapes (denoising, inpainting, contour completion).",
        "Curl-Free and Approximately Equivariant Dynamics Models for Model-Based RL\nCombine SchNet\u2019s energy/force consistency (curl-free forces from a learned potential) with Residual Pathway Priors to learn dynamics that are *nearly* equivariant while respecting conservation laws. Implement in Mujoco-like environments by learning a potential-based component plus a residual non-conservative term, then study how controlling equivariance and non-conservativeness affects long-horizon planning stability and generalization to perturbed terrains."
      ],
      "generated_ideas_raw": "1. **Equivariance-Aware Resilient Constraint Tuning for Vision Models**  \n   Extend Resilient Constrained Learning to treat *equivariance error* (as in approximate/partial equivariance bounds) as an explicit, learnable constraint alongside accuracy. Implement an algorithm that adaptively relaxes/strengthens rotation/reflection equivariance during training of G-CNNs based on validation-estimated data symmetry, and quantify when this improves generalization under distribution shift (e.g., rotated vs. non-rotated test sets).\n\n2. **Energy-Minimization Interpretable G-CNNs for Discontinuity-Preserving Segmentation**  \n   Combine Yuille-style discontinuity-allowing energy functionals with group-equivariant feature maps by designing a G-CNN that outputs parameters of an energy (data term + equivariant regularizer + edge/discontinuity term). Train end-to-end by unrolling a small number of Hopfield/gradient-descent inference steps, yielding an interpretable segmentation/denoising system that preserves sharp boundaries while retaining rotation/reflection equivariance.\n\n3. **Soft-Equivariant SchNet via Residual Pathway Priors for Symmetry-Breaking Chemistry**  \n   Add Residual Pathway Priors to SchNet so that E(3)-invariance/equivariance is encouraged but not enforced, allowing controlled symmetry breaking (e.g., external fields, surfaces, or anisotropic environments). Evaluate on datasets with known symmetry violations (adsorption on slabs, solvent boxes, electric fields) and measure trade-offs between force accuracy and equivariance error.\n\n4. **Data-Driven Symmetry Discovery with Quantified Approximate Equivariance**  \n   Build a pipeline that estimates which transformations (not necessarily a group) the dataset approximately respects, using the paper-3 equivariance error notions as estimators. Use this to automatically select or compose candidate G-convolution groups (translations/rotations/reflections) or continuous symmetries for SchNet-like models, then validate that the selected symmetry set yields predictable gains per the theoretical bounds.\n\n5. **Equivariance Error as a Regularizer: Calibrated Soft Constraints Across Layers**  \n   Introduce a layerwise \u201cequivariance budget\u201d where each layer in a G-CNN (or equivariant MLP/RNN) is penalized by a measurable equivariance deviation term, with weights learned via resilient constrained learning. This produces architectures that concentrate strict equivariance where it helps (early layers) while allowing later layers to deviate for task-specific asymmetries, and provides diagnostics linking where symmetry is broken to performance.\n\n6. **Federated Learning of Heterogeneous Symmetries with Group-Conditioned G-Convolutions**  \n   Extend the heterogeneous federated setting in Resilient Constrained Learning by allowing each client to have a different approximate symmetry group (e.g., camera rotations, sensor orientations). Implement a shared backbone with group-conditioned filters (choose group elements or mixture weights per client) and a global objective that trades off accuracy with per-client equivariance error, improving personalization without overfitting.\n\n7. **Equivariant Continuous-Filter Convolutions for Non-Grid Early Vision Energies**  \n   Generalize SchNet\u2019s continuous-filter convolutions to 2D image coordinates (continuous pixel locations or superpixels) while enforcing/encouraging planar group equivariances (SE(2), reflections) in the filter-generating networks. Use this to model early-vision energies (smoothness + discontinuities) on irregular sampling (event cameras, foveated sensors), testing robustness to rotations and varying sampling density.\n\n8. **Symmetry-Misalignment Benchmarks with Controlled Approximate Equivariance**  \n   Create a suite of synthetic-to-real benchmarks where the true data-generating process has a tunable degree of symmetry breaking (e.g., rotated MNIST with systematic stroke-thickness bias by angle, CIFAR with orientation-dependent lighting). Use these benchmarks to experimentally validate the approximation\u2013generalization trade-offs and identify regimes where soft priors (RPP) outperform hard G-CNN constraints.\n\n9. **Equivariant Energy-Based Models with Learnable Discontinuity Priors**  \n   Develop an energy-based model (EBM) whose energy is parameterized by group-equivariant convolutions, but includes learnable discontinuity terms inspired by line-process/robust penalties from early vision. Train with score matching or contrastive methods, and compare against feedforward G-CNNs on tasks where multi-modal or ambiguous structure benefits from explicit energy landscapes (denoising, inpainting, contour completion).\n\n10. **Curl-Free and Approximately Equivariant Dynamics Models for Model-Based RL**  \n   Combine SchNet\u2019s energy/force consistency (curl-free forces from a learned potential) with Residual Pathway Priors to learn dynamics that are *nearly* equivariant while respecting conservation laws. Implement in Mujoco-like environments by learning a potential-based component plus a residual non-conservative term, then study how controlling equivariance and non-conservativeness affects long-horizon planning stability and generalization to perturbed terrains.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Equivariance-Aware Resilient Constraint Tuning for Vision Models\nExtend Resilient Constrained Learning to treat *equivariance error* (as in approximate/partial equivariance bounds) as an explicit, lea",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Energy-Minimization Interpretable G-CNNs for Discontinuity-Preserving Segmentation\nCombine Yuille-style discontinuity-allowing energy functionals with group-equivariant feature maps by designing a G-C",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Equivariance Error as a Regularizer: Calibrated Soft Constraints Across Layers\nIntroduce a layerwise \u201cequivariance budget\u201d where each layer in a G-CNN (or equivariant MLP/RNN) is penalized by a measur",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Federated Learning of Heterogeneous Symmetries with Group-Conditioned G-Convolutions\nExtend the heterogeneous federated setting in Resilient Constrained Learning by allowing each client to have a diff",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Symmetry-Misalignment Benchmarks with Controlled Approximate Equivariance\nCreate a suite of synthetic-to-real benchmarks where the true data-generating process has a tunable degree of symmetry breakin",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Equivariant Energy-Based Models with Learnable Discontinuity Priors\nDevelop an energy-based model (EBM) whose energy is parameterized by group-equivariant convolutions, but includes learnable disconti",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Curl-Free and Approximately Equivariant Dynamics Models for Model-Based RL\nCombine SchNet\u2019s energy/force consistency (curl-free forces from a learned potential) with Residual Pathway Priors to learn d",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 60,
      "paper_title": "SAGE: A Unified Framework for Generalizable Object State Recognition with State-Action Graph Embedding",
      "contribution": "SAGE introduces a unified framework for recognizing object physical states and their temporal evolutions using State-Action Graph Embedding, enhancing generalization to unseen objects and actions.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 3,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 8550,
      "output_tokens": 1156,
      "predecessor_details": [
        {
          "success": true,
          "title": "Learning Object States and State-Modifying Actions From ...",
          "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Soucek_Look_for_the_Change_Learning_Object_States_and_State-Modifying_Actions_CVPR_2022_paper.pdf",
          "content": "Look for the Change: Learning Object States and\nState-Modifying Actions from Untrimmed Web Videos\nToma\u00b4s Sou \u02c7 cek \u02c7\n1\nJean-Baptiste Alayrac2 Antoine Miech2\nIvan Laptev3Josef Sivic1\n1CIIRC CTU 2DeepMind 3ENS/Inria\ntomas.soucek@cvut.cz\nhttps://data.ciirc.cvut.cz/public/projects/2022LookForTheChange/\nAbstract\nHuman actions often induce changes of object states\nsuch as \u201ccutting an apple\u201d, \u201ccleaning shoes\u201d or \u201cpouring\ncoffee\u201d. In this paper, we seek to temporally localize ob\u0002ject states (e.g. \u201cempty\u201d and \u201cfull\u201d cup) together with the\ncorresponding state-modifying actions (\u201cpouring coffee\u201d)\nin long uncurated videos with minimal supervision. The\ncontributions of this work are threefold. First, we develop\na self-supervised model for jointly learning state-modifying\nactions together with the corresponding object states from\nan uncurated set of videos from the Internet. The model is\nself-supervised by the causal ordering signal, i.e. initial ob\u0002ject state \u2192 manipulating action \u2192 end state. Second, to\ncope with noisy uncurated training data, our model incor\u0002porates a noise adaptive weighting module supervised by a\nsmall number of annotated still images, that allows to ef\u0002ficiently filter out irrelevant videos during training. Third,\nwe collect a new dataset with more than 2600 hours of video\nand 34 thousand changes of object states, and manually an\u0002notate a part of this data to validate our approach. Our re\u0002sults demonstrate substantial improvements over prior work\nin both action and object state-recognition in video.\n1. Introduction\nHuman actions often induce changes of the state of an\nobject, as illustrated in Figure 1. Examples include \u201ccut\u0002ting an apple\u201d, \u201ccleaning shoes\u201d, \u201ctying a tie\u201d or \u201cfilling-up\na cup with coffee\u201d. People can easily recognize such ac\u0002tions and the resulting changes of object states [12], for ex\u0002ample, when watching instructional videos. Furthermore,\npeople can reproduce the actions in their environment, e.g.\n1Czech Institute of Informatics, Robotics and Cybernetics at the Czech\nTechnical University in Prague.\n3Departement d\u2019informatique de l\u2019ENS, \u00b4 Ecole normale sup \u00b4 erieure, \u00b4\nCNRS, PSL Research University, 75005 Paris, France.\n0:001:272:232:332:493:275:006:228:378:53\ninitial state action end state\nwood drilling\n0:000:090:140:170:220:260:370:400:420:53\ninitial state action end state\neggs peeling\nFigure 1. Examples of object states and state-modifying ac\u0002tions learnt by our model from a dataset of long uncurated web\nvideos. In each example the top row shows: the initial state in the\nvideo (left), the state-modifying action (middle), and the end-state\n(right). The bottom row shows video frames sampled from the en\u0002tire video with their corresponding timestamps. It illustrates the\ndifficulty of finding the correct temporal localization of the object\nstates and the actions in the entire video.\nwhen following a recipe from a cooking video. However,\nartificial system with similar cognitive abilities is yet to be\ndeveloped. Existing methods for recognizing object states\nand state-modifying actions address small-scale setups (5\nobjects and short manually curated videos) [3] or controlled\nenvironments [18]. At the same time, progress on auto\u0002matic understanding of causal relations between actions and\nobject states in the wild would be a major step in embod\u0002ied video understanding and robotics. However, the task is\nchallenging given the large amount and variability of exist\u0002ing object-action pairs as well as the difficulty of manually\ncollecting and annotating video data for it.\nIn this paper, we investigate whether the learning of ob\u0002ject states and corresponding state-modifying actions can\nbe scaled-up to noisy uncurated videos from the web while\n13956\nusing only minimal supervision. The contribution of this\nwork is threefold as we outline below.\nFirst, we develop a self-supervised model for jointly\nlearning state-modifying actions and object states from an\nuncurated set of videos obtained from a video search en\u0002gine. We explore the causal ordering in the video as a free\nsupervisory signal and use it to discover the changing states\nof objects and state-modifying actions. We define it by the\nsequence of initial object state \u2192 manipulating action \u2192\nend state, as illustrated in Figure 1. While the prior work\non this problem [3] was limited to closed-form linear clas\u0002sifiers, our model is amenable to large-scale learning using\nstochastic gradient descent and supports non-linear multi\u0002layer models.\nSecond, to cope with noisy uncurated data that may in\u0002clude a large proportion of irrelevant videos (e.g. videos of\nApple laptops when learning \u201ccutting an apple\u201d), our model\nincorporates a noise adaptive weighting module that allows\nto filter out irrelevant videos. This noise adaptive weight\u0002ing module is supervised by a small number of still images\ndepicting the two states of the object, which are easy to col\u0002lect using currently available image search engines. This\nattention mechanism allows us to scale our method to noisy\nuncurated data, as we show by experimental results.\nThird, we collect a new \u201cChangeIt\u201d dataset with more\nthan 2600 hours of video and 34 thousand changes of object\nstates. We manually annotate a portion of this data for eval\u0002uation. To validate our approach, we show results on this\nnew uncurated dataset as well as on the existing smaller cu\u0002rated video dataset from [3]. We ablate key components of\nour method and demonstrate substantial improvements over\nprior work both in action and object state localization. The\ndataset, the code, and a trained model are publicly available.\n2. Related work\nVideo and Language. A large body of work in automatic\nvideo understanding studies the use of natural language or\nspeech data to train models for action and object state recog\u0002nition. Prior work [4,16,19, 28,29,43,45, 50,53,60,64, 67]\nleveraged image and video description datasets [37, 45, 51,\n55,65,71] to learn a joint vision-language embedding space,\nwhere visual and textual data are semantically aligned. In\nparticular, [43, 53, 64] observed that object state and action\nrecognition implicitly emerges, to some extent, from these\nvision and language models. In fact, the aligned vision and\ntext training data often provides detailed descriptions of ac\u0002tions, objects and their different states. In contrast to these\nworks, we explicitly model the causal nature of actions and\ntheir impact on object states in order to leverage this strong\ninductive bias in our model.\nObject attributes and action modifiers. Learning object\nattributes (e.g. sliced, diced) has been approached in a su\u0002pervised manner in still images [46\u201348, 52, 69] with the\nfocus on the compositional nature of the attributes. Simi\u0002larly, others have studied learning modifiers of actions (e.g.\nquickly) [21] from short clips (20 seconds) mined from web\u0002instructional videos. Related to this, Doughty et al. [20]\nanalyzed how the visual changes of object states can be\nused for skill determination in videos. The composition\u0002ality of natural language has also been explored for learn\u0002ing factored video-language embeddings for actions, ob\u0002jects and their attributes for retrieval applications [64] or to\nlearn a contextualized language-object embedding [8]. Ex\u0002plicit models of changes in object states and the associated\nstate-modifying actions have been explored in egocentric\nvideos [24,38]. Others have considered significantly reduc\u0002ing the amount of supervision by learning object states from\nweb images gathered by querying a web-image search en\u0002gine [32]. Closely related to us, there is the work of [22]\nthat used a temporal cycle consistency loss between text and\nvision in instructional videos to find better targets for next\nframe prediction. By doing so, they implicitly discover po\u0002tential object state changes but do not quantitatively evalu\u0002ate the correctness and quality of those. Others directly fo\u0002cus on unsupervised learning of object states and the state\u0002modifying actions from video [3, 18]. However, their work\ncovers only a small-scale learning from a set of trimmed\nand curated videos [3] or a constrained scenario of videos\nobserving a single specific scene [18]. In contrast, we\nconsider large-scale learning from noisy untrimmed videos\nfrom the web.\nOrdering as a form of supervision. The arrow of time is\na strong signal [63] to learn about actions. Indeed, many\nactions happen in a certain order [7]. For example, you\nneed toopen a bottle before being able to pour something\nfrom it. This can be used as a source of supervision. Past\nwork [2,10,11,35,56,62,70] has leveraged such supervision\nto discover and temporally localize actions in untrimmed\nvideos. Similarly, the natural ordering of recurring events\nhas been used to distinguish key events from the back\u0002ground [74]. [72] trained a generative model from time\u0002lapse videos to generate the future state of an object al\u0002tered by time. Others have looked at the related task of\nnext frame or action prediction as another form of super\u0002vision [15, 22, 27, 39, 40, 44, 54, 58]. In contrast, we use as\nthe supervisory signal the strong causal ordering constraints\nthat relate states of objects and the state-modifying actions.\nAction recognition and localization. The problem of de\u0002tecting, classifying and localizing human actions has been\nextensively addressed by methods exploring motion and\ntemporal evolution of appearance in a video. Models for\naction recognition typically operate on short video clips\ntrimmed to encompass a single action. Such models em\u0002ploy a mix of 2D and 3D convolutions [14, 25, 26, 61] or\ntransformers and temporal attention [9, 23, 68]. Action lo\u0002calization methods often generate action proposals in the\n13957\nNoise adaptive learning Labelling with causal ordering constraints\ninitial st. action end st. l(v)\nTime\nLoss = \u03c9(v)\n(\ufe01\nLh(v, l(v)) + \u03bbLg(v, l(v)))\ufe01\nAggreg.\nAdaptive weight \u03c9(v) Temporal labels l(v)\nwhole apple cut apple Input video v\nx1 h1(x1) g(x1) h2(x1) ls1\nh1(x2) g(x2) h2(x2) n/a\n",
          "original_query": "Learning Object States and Actions from Instructional Videos",
          "cleaned_query": "Learning Object States and Actions from Instructional Videos",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Vidi2: Large Multimodal Models for Video Understanding and Creation",
          "url": "https://arxiv.org/html/2511.19529",
          "content": "Vidi2: Large Multimodal Models for Video Understanding and Creation\n# ![[Uncaptioned image]](x1.png)Vidi2: Large Multimodal Models for Video Understanding and Creation\nIntelligent Editing Team111A detailed contributor list can be found in Section[7](https://arxiv.org/html/2511.19529v1#S7)., Intelligent Creation, ByteDance Inc.\nSan Jose/Seattle, US\n[https://bytedance.github.io/vidi-website/](https://bytedance.github.io/vidi-website/)\n## Abstract\nVideo has emerged as the primary medium for communication and creativity on the Internet, driving strong demand for scalable, high-quality video production.\nVidi models continue to evolve toward next-generation video creation and have achieved state-of-the-art performance in multimodal temporal retrieval (TR).\nIn its second release, Vidi2 advances video understanding with fine-grained spatio-temporal grounding (STG) and extends its capability to video question answering (Video QA), enabling comprehensive multimodal reasoning.\nGiven a text query, Vidi2 can identify not only the corresponding timestamps but also the bounding boxes of target objects within the output time ranges.\nThis end-to-end spatio-temporal grounding capability enables potential applications in complex editing scenarios, such as plot or character understanding, automatic multi-view switching, and intelligent, composition-aware reframing and cropping.\nTo enable comprehensive evaluation of STG in practical settings, we introduce a new benchmark,VUE-STG, which offers four key improvements over existing STG datasets:\n1)Video duration: spans from roughly1010s to3030mins, enabling long-context reasoning;\n2)Query format: queries are mostly converted into noun phrases while preserving sentence-level expressiveness;\n3)Annotation quality: all ground-truth time ranges and bounding boxes are manually annotated with high accuracy;\n4)Evaluation metric: a refined vIoU/tIoU/vIoU-Intersection scheme for multi-segment spatio-temporal evaluation.\nIn addition, we upgrade the previous VUE-TR benchmark toVUE-TR-V2, achieving a more balanced video-length distribution and more user-style queries.\nRemarkably, the Vidi2 model substantially outperforms leading proprietary systems, such as Gemini 3 Pro (Preview) and GPT-5, on both VUE-TR-V2 and VUE-STG, while achieving competitive results with popular open-source models with similar scale on video QA benchmarks.\n![Refer to caption](x2.png)Figure 1:Spatio-temporal grounding and temporal retrieval results on the proposed benchmarks.\n## 1Introduction\nAs an inherently multimodal medium, video has become a dominant channel for online communication and information sharing.\nHowever, producing high-quality videos remains challenging for most users, particularly when performing trimming or editing operations on mobile devices.\nThe first release ofVidi> [\n> team2025vidi\n> ]\ndemonstrated strong temporal understanding across text, visual, and audio modalities, revealing its potential for automatic video trimming and understanding.\nTo advance toward next-generation video creation systems capable of handling complex editing scenarios, Vidi models have been continually evolving to support more comprehensive multimodal perception and reasoning capabilities.\nThe second release, Vidi2, introduces for the first time an end-to-end spatio-temporal grounding (STG) capability, identifying not only the temporal segments corresponding to a text query but also the spatial locations of relevant objects within those frames.\nNotably, existing academic models> [\n> gu2024context\n> , > gu2025knowing\n> , > jin2022embracing\n> , > lin2023collaborative\n> , > su2021stvgbert\n> , > wasim2024videogrounding\n> ]\nand state-of-the-art industrial systems such as Gemini 3 Pro (Preview)> [\n> Gemini\n> , > comanici2025gemini\n> ]\nand GPT-5> [\n> OpenAI2025GPT5SystemCard\n> ]\nare not yet capable of producing such fine-grained grounding results in a unified text output format (see Fig.[1](https://arxiv.org/html/2511.19529v1#Sx1.F1)).\nAs illustrated in Fig.[2](https://arxiv.org/html/2511.19529v1#S1.F2), the query \u201ca man standing up from a kneeling position\u201d represents a particularly difficult case involving multiple people in a dark scene.\nVidi2 accurately localizes the corresponding time ranges and distinguishes the target person from others through precise bounding boxes in an end-to-end manner. It requires a comprehensive understanding of the visual and temporal dynamics of the described action.\nThis fine-grained spatio-temporal perception highlights Vidi2\u2019s potential to enable advanced editing workflows, such as plot-level understanding, automatic multi-view switching, and composition-aware reframing for professional video creation.\n![Refer to caption](x3.png)Figure 2:Examples of spatio-temporal grounding queries and their corresponding time ranges and object tubes (timestamps with bounding boxes, shown in yellow). Bounding boxes are expressed in percentage coordinates. The example video has a total duration of387387seconds (i.e.,06:27), and the query has been converted into a noun-style format. Facial regions are blurred to protect privacy.\nTo enable comprehensive evaluation of spatio-temporal grounding (STG) in realistic scenarios, we introduce a new benchmark, VUE-STG, featuring videos that range from1010seconds to3030minutes, substantially longer than existing STG datasets> [\n> chen2019weakly\n> , > tang2021human\n> , > yamaguchi2017spatio\n> , > zhang2020does\n> ]\n.\nCompared to prior work, VUE-STG includes videos and annotated tubes of much greater temporal extent, along with more complex, multimodal queries that often incorporate audio cues to assist temporal localization.\nAll timestamps and bounding boxes are manually annotated with high precision, ensuring reliable spatio-temporal alignment.\nIn addition, we upgrade the previous VUE-TR benchmark to VUE-TR-V2, featuring more human-authored, free-form queries and a more balanced video-length distribution with an increased proportion of long and ultra-long videos, making the task more realistic and challenging.\nOn the VUE-STG benchmark, Vidi2 surpasses leading proprietary models (Gemini 3 Pro (Preview) and GPT-5) by a substantial margin, demonstrating state-of-the-art spatio-temporal grounding capability among large multimodal models.\nCompared with the previous version, Vidi2 also shows significant improvements in temporal retrieval, particularly for long videos.\nOn the updated VUE-TR-V2 benchmark, it outperforms Gemini 3 Pro (Preview)222Note that Gemini 3 Pro shows a notable improvement over Gemini-2.5-Pro-0325 on temporal retrieval.and GPT-5 by a wide margin across \u201cMedium\u201d to \u201cUltra-Long\u201d video categories, confirming its leading performance in temporal reasoning.\nFurthermore, Vidi2 extends its capability to generic video question answering, marking a step toward a foundation model for comprehensive video understanding with strong spatio-temporal perception.\n## 2Model Overview\nVidi2 builds upon the foundation of Vidi> [\n> team2025vidi\n> ]\nwith substantial upgrades in architecture, training data, and task capabilities, most notably the support for spatio-temporal grounding (STG) and video question answering (Video QA). These improvements collectively enable fine-grained multimodal understanding across text, visual, and audio modalities.\n##### Architecture:\nVidi2 retains the multimodal architecture of Vidi> [\n> team2025vidi\n> ]\n, designed to jointly process text, visual, and audio inputs, while introducing key enhancements in both the encoder and LLM backbone (e.g., Gemma-3> [\n> team2025gemma\n> ]\n).\nAll results presented in this report are based on the 12B-parameter configuration.\nTo achieve robust performance across videos of different lengths, we re-design the adaptive token compression strategy, improving the balance between short and long video representation efficiency.\nAdditionally, a single image is treated as a11-second silent video, ensuring a unified encoding interface for both image and video inputs.\n##### Training Data:\nVidi2 follows a training pipeline similar to the previous Vidi version but scales up in both data volume and data diversity.\nWhile synthetic data remains essential for coverage and stability, increasing the proportion of real video data significantly enhances performance across all video-related tasks.\nThe supervised fine-tuning (SFT) dataset for temporal retrieval has been expanded and refined in both quality and quantity.\nFurthermore, we incorporate additional generic QA data during SFT to strengthen open-ended reasoning on images and videos, improving the model\u2019s versatility on downstream applications such as highlight detection, dense captioning, and video summarization.\n##### Spatio-Temporal Grounding:\nTo support the newly introduced spatio-temporal grounding (STG) capability, we introduce task-specific data across all training stages.\nSpecifically, we leverage existing image-level spatial grounding datasets to synthesize large-scale spatio-temporal video grounding pairs, effectively bridging spatial and temporal alignment.\nIn addition, we curate a substantial collection of real-world video STG annotations, which contribute significantly to the model\u2019s ability to localize both time ranges and bounding boxes with high precision.\n##### Video Question Answering:\nThe previous Vidi> [\n> team2025vidi\n> ]\nmodel demonstrated the effectiveness of the temporal-aware multimodal alignment training stage (see Sec.\u00a04 in> [\n> team2025vidi\n> ]\n) for temporal retrieval.\nVidi2 further validates the generalization capability of this multimodal alignment strategy to generic video QA.\nBy incorporating video QA data during the post-training phase, Vidi2 is able to answer both visual and auditory questions with high accuracy.\nWe also observe that scaling up multimodal alignment data consistently improves overall video QA performance.\n## 3Evaluation Benchmark\nTo comprehensively evaluate the performance and generalization of Vidi2, we consider three representative catego",
          "original_query": "Large Language Models for Video Understanding",
          "cleaned_query": "Large Language Models for Video Understanding",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "Do Pre-trained Vision-Language Models Encode Object States?",
          "url": "https://arxiv.org/abs/2409.10488",
          "content": "\n View PDF \n HTML (experimental) \nFor a vision-language model (VLM) to understand the physical world, such as cause and effect, a first step is to capture the temporal dynamics of the visual world, for example how the physical states of objects evolve over time (e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs pre-trained on web-scale data learn to encode object states, which can be extracted with zero-shot text prompts. We curate an object state recognition dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models trained with contrastive and generative objectives. We observe that while these state-of-the-art vision-language models can reliably perform object recognition, they consistently fail to accurately distinguish the objects' physical states. Through extensive experiments, we identify three areas for improvements for VLMs to better encode object states, namely the quality of object localization, the architecture to bind concepts to objects, and the objective to learn discriminative visual and language encoders on object states. Data and code are released.\n \n \n Submission history From: Kaleb Newman [ view email] [v1] \nMon, 16 Sep 2024 17:22:18 UTC (1,847 KB) \n",
          "original_query": "Object State Recognition using Vision-Language Models",
          "cleaned_query": "Object State Recognition using Vision-Language Models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] RoboEXP: Action-Conditioned Scene Graph via Interactive ...",
          "url": "https://jianghanxiao.github.io/roboexp-web/roboexp.pdf",
          "content": "RoboEXP: Action-Conditioned Scene Graph via\nInteractive Exploration for Robotic Manipulation\nHanxiao Jiang1,2 Binghao Huang1 Ruihai Wu4 Zhuoran Li5\nShubham Garg3 Hooshang Nayyeri3 Shenlong Wang2 Yunzhu Li1\n1Columbia University 2University of Illinois Urbana-Champaign 3Amazon\n4\nPeking University 5National University of Singapore\nAbstract: We introduce the novel task of interactive scene exploration, wherein\nrobots autonomously explore environments and produce an action-conditioned\nscene graph (ACSG) that captures the structure of the underlying environment.\nThe ACSG accounts for both low-level information (geometry and semantics)\nand high-level information (action-conditioned relationships between different\nentities) in the scene. To this end, we present the Robotic Exploration (RoboEXP)\nsystem, which incorporates the Large Multimodal Model (LMM) and an explicit\nmemory design to enhance our system\u2019s capabilities. The robot reasons about\nwhat and how to explore an object, accumulating new information through the\ninteraction process and incrementally constructing the ACSG. Leveraging the\nconstructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP\nsystem in facilitating a wide range of real-world manipulation tasks involving\nrigid, articulated objects, nested objects, and deformable objects. Project Page:\nhttps://jianghanxiao.github.io/roboexp-web/\nKeywords: Action-Conditioned Scene Graph, Foundation Models for Robotics,\nScene Exploration, Robotic Manipulation\n1 Introduction\nImagine a household robot designed to prepare breakfast. This robot must perform various tasks such\nas conducting inventory checks in cabinets, fetching food from the fridge, gathering utensils from\ndrawers, and spotting leftovers under food covers. Key to its success is the ability to interact with and\nexplore the environment, especially to find items that aren\u2019t immediately visible. Equipping it with\nsuch capabilities is crucial for the robot to effectively complete its everyday tasks. Robot exploration\nand active perception have long been challenging areas in robotics. Various techniques have been\nproposed, including information-theoretic approaches [1\u20137], frontier-based methods [8\u201310], imitation\nlearning [11, 12] and reinforcement learning [13\u201317]. Nevertheless, previous research has primarily\nfocused on exploring static environments by merely changing viewpoints in a navigation setting.\nIn this work, we investigate the interactive scene exploration task, where the goal is to efficiently\nidentify all objects, including those that are directly observable and those that can only be discovered\nthrough interaction between the robot and the environment (Fig. 1). Towards this goal, we present a\nnovel scene representation called action-conditioned 3D scene graph (ACSG). Unlike conventional\n3D scene graphs that focus on encoding static relations, ACSG encodes both spatial relationships and\nlogical associations indicative of action effects (e.g., opening a fridge will reveal an apple inside). We\nthen show that our task can be formulated as a problem of ACSG construction and traversal.\nTo tackle the interactive scene exploration task, we propose a novel, real-world robotic exploration\nframework, the RoboEXP system. At the core of our system is a large foundational model-powered\ninstantiation of action-conditioned 3D scene graph. Specifically, our framework consists of four\nmodules: perception, memory, decision-making, and action (Fig. 3).\nRoboEXP can handle diverse exploration tasks in a zero-shot manner, constructing complex action\u0002conditioned 3D scene graph in various scenarios, including those involving obstructing objects and\n8th Conference on Robot Learning (CoRL 2024), Munich, Germany.\n(b) Exploitation (Leverage Scene Graph) (a) Exploration (Build Scene Graph) Figure 1: Interactive Exploration to Construct an Action-Conditioned Scene Graph (ACSG) for Robotic\nManipulation. (a) Exploration: The robot autonomously explores by interacting with the environment to\ngenerate a comprehensive ACSG. This graph is used to catalog the locations and relationships of items. (b)\nExploitation: Utilizing the constructed scene graph, the robot completes downstream tasks by efficiently\norganizing the necessary items according to the desired spatial and relational constraints.\nrequiring multi-step reasoning (Fig. 2). We evaluate our system across various settings , demonstrating\nits adaptability and robustness. The system also effectively manages different human interventions.\nMoreover, we show that our reconstructed action-conditioned 3D scene graph demonstrates strong\ncapacity in performing multiple complex downstream tasks. Action-conditioned 3D scene graph\nadvances LLM/LMM-guided robotic manipulation and decision-making research [18, 19], extend\u0002ing their operation domain from environments with known or observable objects to complicated\nenvironments with unknown or unobserved ones. To our knowledge, this is the first of its kind.\n2 Related Works\nScene Graphs [20, 21] represent objects and their relations [22\u201324] in a scene via a graph structure.\nPrevious studies generate scene graphs from images [21, 25, 26] or 3D scenes [27], and further with\nthe assistance of large language models (LLMs) [28, 29]. While previous works model scene graphs\nin static 2D or 3D scenes, we generate action-conditioned scene graphs that integrate actions as core\nelements, depicting interactive relationships between objects and actions. Our work is also related to\nNeuro-Symbolic Representations that integrate neural networks with symbolic reasoning. Prior\nworks explored understanding scenes and describing robotic skills in symbolic texts to interpret\ndemonstrations [30, 31], ground abstract actions for robotic primitives [32] and generate action plans\n[33\u201338]. Our proposed framework also constructs symbolic representations of the environment, but\nin the form of action-conditioned scene graphs for robotic manipulation.\nRobotic Exploration aims to autonomously navigate [9, 11, 39\u201345], interact with [12, 46, 46\u201350],\nand gather information [51\u201353] from environments it has never encountered before. The primary\nguiding principle behind robotic exploration is to reduce the uncertainty of the environment [5\u2013\n7, 39, 54, 55], making uncertainty quantification key for robotic exploration tasks. Curiosity-driven\nexploration has recently emerged as a promising approach, showing effective results in various\ncontexts [16, 56\u201358]. Recently, exploration has also been studied in the context of manipulation\n[59\u201366], aiming to better understand the scene by changing the state of the environment. Our work\nintroduces a new active exploration strategy for manipulation, uniquely defining a novel scene graph\u0002guided objective to guide the exploration process. Our work is also related to Active Perception,\n2\n(a) Real Robot (b) Low-Level Memory\nTable\nCabinet\nCabinet\nCondiment\nDrawer Handle\n(c) \nTape\nHigh-Level Memory\nPick\nDrawer Handle\nDoor Handle\nDoor Handle\nOpen\nOpen\nOpen\nOpen\nBanana\nInside\nInside\nBelong\nBelong\nObject Node\nAction Node\nt\nOn\nBelong\nBelong\nFigure 2: Action-Conditioned 3D Scene Graph from Interactive Scene Exploration. We depict a scenario\nwherein a robot arm explores a tabletop scene containing two cabinets and a condiment obstructing the left door.\n(a) The robot arm actively interacts with the scene, completing the interactive scene exploration process. (b)\nWe showcase the corresponding low-level memory in our ACSG. The small graph on the bottom-left of each\nvisualization represents a segment of the final scene graph. (c) We present the high-level memory of our ACSG.\nThe graph reveals that picking up the condiment serves as a precondition for opening the door, and opening the\nbottom drawer allows the observation of the concealed banana.\nwhich actively selects actions for an agent to help it perceive and understand the environment [1, 67].\nUnlike passive perception, actions offer more flexibility, such as control over viewpoints [2\u20134, 68],\nsensor configurations [15, 69], or adjustments to environmental configurations [70]. It can also reveal\ncertain scene properties that cannot be perceived in a passive manner, such as dynamic parameters [16,\n71] or articulation [62, 72, 73]. Our work falls into the category of actively exploring the environment\nto reveal what\u2019s inside or underneath objects. Differing from most previous active perception\nefforts, which are driven by handcrafted rules [74], information gain [75, 76], or reinforcement\nlearning [16, 77], our approach is guided by grounding the commonsense knowledge encoded in a\nLLM/LMM into an explicit scene graph representation.\nLanguage Models for Robotics. Large language models (LLMs) [78\u201380] and large multimodality\nmodels (LMMs) [81, 82] are bringing overwhelming influence into the robotics field, for their strong\ncapacity in common-sense knowledge and task-level planning. Previous studies have harnessed the\ncommon-sense knowledge of such large models to generate action candidates [83\u201385] and action\nsequences for task planning [80, 86\u201388], and generate code for robotic control and manipulation [18,\n89, 90]. More recently, VILA [19] utilized GPT-4V [81, 82] for vision-language planning. In our\nRoboEXP system, we leverage GPT-4V for decision-making in two crucial roles to select and verify\nactions. Moreover, instead of memorizing everything using large models in a brute force way, our\nsystem employs explicit memory to enhance the decision-making process.\n3 Action-Conditioned 3D Scene Graph\nAn action-conditioned 3D scene graph (ACSG) is an actionable, spatial-topological representation\nthat models objects and their interactive and spatial relations in a scene. Formally, ACSG is a directed\nacyclic graph G = (V, E) where each node represents either an object or an action, and edges E\nrepresent their interaction relations. The object node oi = (si, pi) \u2208 V encodes the semantics si\nand geometry pi of each object, whereas the action",
          "original_query": "Action Conditioned Models for Robotic Manipulation",
          "cleaned_query": "Action Conditioned Models for Robotic Manipulation",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2507.15569] DynImg: Key Frames with Visual Prompts are Good ...",
          "url": "https://arxiv.org/abs/2507.15569",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2507.15569** (cs)\n\n\\[Submitted on 21 Jul 2025\\]\n\n# Title:DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding\n\nAuthors: [Xiaoyi Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao,+X), [Chenwei Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+C), [Hao Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+H), [Tingyu Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng,+T), [Xiaofeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+X), [Yun Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng,+Y), [Xingang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+X)\n\nView a PDF of the paper titled DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding, by Xiaoyi Bao and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2507.15569) [HTML (experimental)](https://arxiv.org/html/2507.15569v1)\n\n> Abstract:In recent years, the introduction of Multi-modal Large Language Models (MLLMs) into video understanding tasks has become increasingly prevalent. However, how to effectively integrate temporal information remains a critical research focus. Traditional approaches treat spatial and temporal information separately. Due to issues like motion blur, it is challenging to accurately represent the spatial information of rapidly moving objects. This can lead to temporally important regions being underemphasized during spatial feature extraction, which in turn hinders accurate spatio-temporal interaction and video understanding. To address this limitation, we propose an innovative video representation method called Dynamic-Image (DynImg). Specifically, we introduce a set of non-key frames as temporal prompts to highlight the spatial areas containing fast-moving objects. During the process of visual feature extraction, these prompts guide the model to pay additional attention to the fine-grained spatial features corresponding to these regions. Moreover, to maintain the correct sequence for DynImg, we employ a corresponding 4D video Rotary Position Embedding. This retains both the temporal and spatial adjacency of DynImg, helping MLLM understand the spatio-temporal order within this combined format. Experimental evaluations reveal that DynImg surpasses the state-of-the-art methods by approximately 2% across multiple video understanding benchmarks, proving the effectiveness of our temporal prompts in enhancing video comprehension.\n\n| | |\n| --- | --- |\n| Comments: | Accepted by ICCV 2025 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2507.15569](https://arxiv.org/abs/2507.15569) \\[cs.CV\\] |\n| (or [arXiv:2507.15569v1](https://arxiv.org/abs/2507.15569v1) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.15569](https://doi.org/10.48550/arXiv.2507.15569) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Xiaoyi Bao \\[ [view email](https://arxiv.org/show-email/68a71786/2507.15569)\\] **\\[v1\\]**\nMon, 21 Jul 2025 12:50:49 UTC (9,948 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding, by Xiaoyi Bao and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2507.15569)\n- [HTML (experimental)](https://arxiv.org/html/2507.15569v1)\n- [TeX Source](https://arxiv.org/src/2507.15569)\n- [Other Formats](https://arxiv.org/format/2507.15569)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2507.15569&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2507.15569&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2025-07](https://arxiv.org/list/cs.CV/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.15569?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.15569)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.15569)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.15569)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.15569) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Dynamic Representations in Video Understanding",
          "cleaned_query": "Dynamic Representations in Video Understanding",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Few-Shot Object Detection: Research Advances and ...",
          "url": "https://arxiv.org/abs/2404.04799",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2404.04799** (cs)\n\n\\[Submitted on 7 Apr 2024\\]\n\n# Title:Few-Shot Object Detection: Research Advances and Challenges\n\nAuthors: [Zhimeng Xin](https://arxiv.org/search/cs?searchtype=author&query=Xin,+Z), [Shiming Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+S), [Tianxu Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+T), [Yuanjie Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao,+Y), [Weiping Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding,+W), [Xinge You](https://arxiv.org/search/cs?searchtype=author&query=You,+X)\n\nView a PDF of the paper titled Few-Shot Object Detection: Research Advances and Challenges, by Zhimeng Xin and Shiming Chen and Tianxu Wu and Yuanjie Shao and Weiping Ding and Xinge You\n\n[View PDF](https://arxiv.org/pdf/2404.04799) [HTML (experimental)](https://arxiv.org/html/2404.04799v1)\n\n> Abstract:Object detection as a subfield within computer vision has achieved remarkable progress, which aims to accurately identify and locate a specific object from images or videos. Such methods rely on large-scale labeled training samples for each object category to ensure accurate detection, but obtaining extensive annotated data is a labor-intensive and expensive process in many real-world scenarios. To tackle this challenge, researchers have explored few-shot object detection (FSOD) that combines few-shot learning and object detection techniques to rapidly adapt to novel objects with limited annotated samples. This paper presents a comprehensive survey to review the significant advancements in the field of FSOD in recent years and summarize the existing challenges and solutions. Specifically, we first introduce the background and definition of FSOD to emphasize potential value in advancing the field of computer vision. We then propose a novel FSOD taxonomy method and survey the plentifully remarkable FSOD algorithms based on this fact to report a comprehensive overview that facilitates a deeper understanding of the FSOD problem and the development of innovative solutions. Finally, we discuss the advantages and limitations of these algorithms to summarize the challenges, potential research direction, and development trend of object detection in the data scarcity scenario.\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2404.04799](https://arxiv.org/abs/2404.04799) \\[cs.CV\\] |\n| | (or [arXiv:2404.04799v1](https://arxiv.org/abs/2404.04799v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2404.04799](https://doi.org/10.48550/arXiv.2404.04799) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zhimeng Xin \\[ [view email](https://arxiv.org/show-email/aad0f65f/2404.04799)\\]\n\n**\\[v1\\]**\nSun, 7 Apr 2024 03:37:29 UTC (1,575 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Few-Shot Object Detection: Research Advances and Challenges, by Zhimeng Xin and Shiming Chen and Tianxu Wu and Yuanjie Shao and Weiping Ding and Xinge You\n\n- [View PDF](https://arxiv.org/pdf/2404.04799)\n- [HTML (experimental)](https://arxiv.org/html/2404.04799v1)\n- [TeX Source](https://arxiv.org/src/2404.04799)\n- [Other Formats](https://arxiv.org/format/2404.04799)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2404.04799&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2404.04799&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-04](https://arxiv.org/list/cs.CV/2024-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2404.04799?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2404.04799)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2404.04799)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2404.04799)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2404.04799&description=Few-Shot Object Detection: Research Advances and Challenges) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2404.04799&title=Few-Shot Object Detection: Research Advances and Challenges)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2404.04799) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Few-Shot Learning in Object Recognition",
          "cleaned_query": "Few-Shot Learning in Object Recognition",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Action-Conditioned State Graphs from Video: Importing ACSG Concepts into Web Video Understanding\nTranslate RoboEXP\u2019s Action-Conditioned Scene Graph (ACSG) to passive video: build graphs where nodes are objects with state attributes and edges are state-modifying actions that causally link state\u2080\u2192state\u2081. Learn graph edges from ChangeIt causal ordering and ground nodes/edges spatially using STG tubes. Show the graph supports compositional queries (\u201cfind when the cup becomes full after being placed under the machine\u201d) and improves long-context retrieval.",
        "Counterfactual State Learning: \u201cNo-Change\u201d Negatives and Causal Consistency Losses\nBuild a training set of hard negatives where the action occurs but the expected state does *not* change (e.g., stirring an already-mixed liquid; pretending to cut). Train with a causal consistency objective that penalizes predicting a change without visual evidence, improving robustness to narration-only or mimed actions common in web videos. Validate by constructing counterfactual clips from ChangeIt and measuring false-positive change detections.",
        "Long-Video Change Retrieval with Joint TR+STG: From \u201cFind the Scene\u201d to \u201cFind the Change\u201d\nExtend VUE-TR/VUE-STG tasks to \u201cchange retrieval\u201d queries (\u201cmoment when the door goes from closed to open\u201d) requiring both temporal retrieval and tube grounding of the affected object. Train a unified model that first retrieves candidate segments (TR), then refines with tube-level change scoring (STG), using ChangeIt-style causal supervision. Release an evaluation protocol combining tIoU for the change moment and vIoU for the object tube."
      ],
      "generated_ideas_raw": "1. **Tube-of-Change: Spatio-Temporal Grounding for Object State Transitions**\n   Build a model that extends Vidi2-style STG to output *two* object tubes (pre-state and post-state) plus the transition interval, trained using ChangeIt\u2019s causal ordering (state\u2080 \u2192 action \u2192 state\u2081). Create pseudo-labels by aligning high-confidence state classifiers at distant timestamps and forcing a localized \u201ctransition\u201d segment in between. Evaluate on ChangeIt annotations with a new metric: tube vIoU for state endpoints + tIoU for the transition window.\n\n2. **State-Aware DynImg: Transition-Prompted Keyframe Bundles for State Recognition**\n   Adapt DynImg by selecting non-key \u201ctemporal prompts\u201d specifically around suspected change points (high temporal gradient in state logits from Paper 1 models). Replace generic prompts with *transition-centric* prompts (frames just before/after change), and train an MLLM to answer state questions and localize change intervals. Compare against vanilla DynImg on ChangeIt-Frames and long-video subsets.\n\n3. **Object-Centric State Binding for VLMs via Tube-Masked Contrastive Pretraining**\n   Address Paper 3\u2019s failure modes by introducing an object-binding pretraining stage: use STG tubes (from Vidi2 or an STG teacher) to apply tube-masked pooling and contrastive losses between state-specific prompts (\u201ca sliced apple\u201d) and *tube crops* rather than whole frames. Add hard negatives with same object/different state within the same video to force discriminative state encoding. Measure gains in zero-shot state classification and robustness to distractor objects.\n\n4. **Few-Shot State-Change Detection with \u201cState Prototypes\u201d from Web Images**\n   Combine Paper 1\u2019s still-image supervision idea with FSOD: learn class-agnostic detectors that adapt to novel *state categories* (e.g., \u201cclosed\u2192open\u201d, \u201cempty\u2192full\u201d) from 1\u20135 state exemplar images per endpoint. Use prototype-based adaptation to detect and localize state-specific regions in video frames, then infer change segments via temporal consistency constraints. Benchmark on held-out object categories in ChangeIt and report few-shot transfer curves.\n\n5. **Noise-Adaptive Video Filtering 2.0: LMM-Assisted Relevance Scoring with Rationales**\n   Upgrade Paper 1\u2019s noise adaptive weighting by using an LMM (Vidi2-style) to produce a relevance score *and* a short textual rationale tied to visual evidence (e.g., \u201cshows apple being sliced; clear before/after\u201d). Train a lightweight student reweighter on these rationales + visual features, enabling scalable filtering without running the LMM at full cost. Quantify improvements in downstream state/action localization when training on noisier web crawls.\n\n6. **Action-Conditioned State Graphs from Video: Importing ACSG Concepts into Web Video Understanding**\n   Translate RoboEXP\u2019s Action-Conditioned Scene Graph (ACSG) to passive video: build graphs where nodes are objects with state attributes and edges are state-modifying actions that causally link state\u2080\u2192state\u2081. Learn graph edges from ChangeIt causal ordering and ground nodes/edges spatially using STG tubes. Show the graph supports compositional queries (\u201cfind when the cup becomes full after being placed under the machine\u201d) and improves long-context retrieval.\n\n7. **Active Perception for State Verification: Robot Executes Minimal Actions to Disambiguate States**\n   Use RoboEXP\u2019s interactive exploration to resolve ambiguous visual states identified by Paper 3 (e.g., \u201cclean vs dirty,\u201d \u201con vs off\u201d). When the VLM is uncertain, the robot chooses an information-gain action (open/tilt/turn on) predicted by an ACSG to reveal the state. Evaluate in real scenes by measuring state-classification accuracy vs number of interactions and task success in downstream manipulation.\n\n8. **Counterfactual State Learning: \u201cNo-Change\u201d Negatives and Causal Consistency Losses**\n   Build a training set of hard negatives where the action occurs but the expected state does *not* change (e.g., stirring an already-mixed liquid; pretending to cut). Train with a causal consistency objective that penalizes predicting a change without visual evidence, improving robustness to narration-only or mimed actions common in web videos. Validate by constructing counterfactual clips from ChangeIt and measuring false-positive change detections.\n\n9. **Long-Video Change Retrieval with Joint TR+STG: From \u201cFind the Scene\u201d to \u201cFind the Change\u201d**\n   Extend VUE-TR/VUE-STG tasks to \u201cchange retrieval\u201d queries (\u201cmoment when the door goes from closed to open\u201d) requiring both temporal retrieval and tube grounding of the affected object. Train a unified model that first retrieves candidate segments (TR), then refines with tube-level change scoring (STG), using ChangeIt-style causal supervision. Release an evaluation protocol combining tIoU for the change moment and vIoU for the object tube.\n\n10. **State-Augmented Video Editing: Automatic Cut Points and Reframing Driven by State Transitions**\n   Turn the understanding advances into a creation/editing pipeline (aligned with Vidi2\u2019s editing motivation): detect state transitions (Paper 1) and generate edit suggestions\u2014trim to \u201cbefore\u2192action\u2192after\u201d structure and reframe around the object tube (Vidi2 STG). Add DynImg-style prompts to stabilize fast motion during the action segment for better reframing. Evaluate with user studies and objective metrics (transition coverage, object-in-frame rate, and edit compactness).",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Action-Conditioned State Graphs from Video: Importing ACSG Concepts into Web Video Understanding\nTranslate RoboEXP\u2019s Action-Conditioned Scene Graph (ACSG) to passive video: build graphs where nodes ar",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Counterfactual State Learning: \u201cNo-Change\u201d Negatives and Causal Consistency Losses\nBuild a training set of hard negatives where the action occurs but the expected state does *not* change (e.g., stirri",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Long-Video Change Retrieval with Joint TR+STG: From \u201cFind the Scene\u201d to \u201cFind the Change\u201d\nExtend VUE-TR/VUE-STG tasks to \u201cchange retrieval\u201d queries (\u201cmoment when the door goes from closed to open\u201d) re",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 61,
      "paper_title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
      "contribution": "The paper critically evaluates the effectiveness of Reinforcement Learning with Verifiable Rewards (RLVR) in enhancing the reasoning capabilities of large language models (LLMs) and reveals that the improvements are primarily superficial, as they do not generate fundamentally new reasoning patterns beyond those already established by base models.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "quality_content": 3,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 5,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 6693,
      "output_tokens": 884,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1707.06347] Proximal Policy Optimization Algorithms - arXiv",
          "url": "https://arxiv.org/abs/1707.06347",
          "content": "[1707.06347] Proximal Policy Optimization Algorithms[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1707.06347\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1707.06347**(cs)\n[Submitted on 20 Jul 2017 ([v1](https://arxiv.org/abs/1707.06347v1)), last revised 28 Aug 2017 (this version, v2)]\n# Title:Proximal Policy Optimization Algorithms\nAuthors:[John Schulman](https://arxiv.org/search/cs?searchtype=author&amp;query=Schulman,+J),[Filip Wolski](https://arxiv.org/search/cs?searchtype=author&amp;query=Wolski,+F),[Prafulla Dhariwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Dhariwal,+P),[Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford,+A),[Oleg Klimov](https://arxiv.org/search/cs?searchtype=author&amp;query=Klimov,+O)\nView a PDF of the paper titled Proximal Policy Optimization Algorithms, by John Schulman and 4 other authors\n[View PDF](https://arxiv.org/pdf/1707.06347)> > Abstract:\n> We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a &#34;surrogate&#34; objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. Subjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:1707.06347](https://arxiv.org/abs/1707.06347)[cs.LG]|\n|(or[arXiv:1707.06347v2](https://arxiv.org/abs/1707.06347v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1707.06347](https://doi.org/10.48550/arXiv.1707.06347)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: John Schulman [[view email](https://arxiv.org/show-email/b7d77275/1707.06347)]\n**[[v1]](https://arxiv.org/abs/1707.06347v1)**Thu, 20 Jul 2017 02:32:33 UTC (2,178 KB)\n**[v2]**Mon, 28 Aug 2017 09:20:06 UTC (2,537 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Proximal Policy Optimization Algorithms, by John Schulman and 4 other authors\n* [View PDF](https://arxiv.org/pdf/1707.06347)\n* [TeX Source](https://arxiv.org/src/1707.06347)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1707.06347&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1707.06347&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2017-07](https://arxiv.org/list/cs.LG/2017-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/1707.06347?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1707.06347)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1707.06347)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1707.06347)\n### [18 blog links](https://arxiv.org/tb/1707.06347)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/SchulmanWDRK17)\n[John Schulman]()\n[Filip Wolski]()\n[Prafulla Dhariwal]()\n[Alec Radford]()\n[Oleg Klimov]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1707.06347)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Proximal policy optimization algorithms",
          "cleaned_query": "Proximal policy optimization algorithms",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Tulu 3: Pushing Frontiers in Open Language Model Post-Training",
          "url": "https://arxiv.org/html/2411.15124",
          "content": "T\u00fclu 3: Pushing Frontiers in Open Language Model Post-Training\n\\\\authorOne\n[1,\\*]Nathan Lambert\\\\varheartsuit\\\\varheartsuit{}^{{\\\\color[rgb]{0.9296875,0.00390625,0.48828125}\\\\varheartsuit}}start\\_FLOATSUPERSCRIPT end\\_FLOATSUPERSCRIPT\\\\authorOne[1]Jacob Morrison\\\\varheartsuit\\\\varheartsuit{}^{{\\\\color[rgb]{0.9296875,0.00390625,0.48828125}\\\\varheartsuit}}start\\_FLOATSUPERSCRIPT end\\_FLOATSUPERSCRIPT\\\\authorOne[1,2]Valentina Pyatkin\\\\varheartsuit\\\\varheartsuit{}^{{\\\\color[rgb]{0.9296875,0.00390625,0.48828125}\\\\varheartsuit}}start\\_FLOATSUPERSCRIPT end\\_FLOATSUPERSCRIPT\\\\authorOne[1]Shengyi Huang\\\\varheartsuit\\\\varheartsuit{}^{{\\\\color[rgb]{0.9296875,0.00390625,0.48828125}\\\\varheartsuit}}start\\_FLOATSUPERSCRIPT end\\_FLOATSUPERSCRIPT\\\\authorOne[1,2]Hamish Ivison\\\\varheartsuit\\\\varheartsuit{}^{{\\\\color[rgb]{0.9296875,0.00390625,0.48828125}\\\\varheartsuit}}start\\_FLOATSUPERSCRIPT end\\_FLOATSUPERSCRIPT\\\\authorOne[1]Faeze Brahman\\\\varheartsuit\\\\varheartsuit{}^{{\\\\color[rgb]{0.9296875,0.00390625,0.48828125}\\\\varheartsuit}}start\\_FLOATSUPERSCRIPT end\\_FLOATSUPERSCRIPT\\\\authorOne[1]Lester James V. Miranda\\\\varheartsuit\\\\varheartsuit{}^{{\\\\color[rgb]{0.9296875,0.00390625,0.48828125}\\\\varheartsuit}}start\\_FLOATSUPERSCRIPT end\\_FLOATSUPERSCRIPT\\\\authorTwo[2]Alisa Liu\\\\authorTwo[1]Nouha Dziri\\\\authorTwo[1]Xinxi Lyu\\\\authorTwo[1]Yuling Gu\\\\authorTwo[1]Saumya Malik\\\\authorTwo[2]Victoria Graf\\\\authorTwo[1]Jena D. Hwang\\\\authorTwo[1]Jiangjiang Yang\\\\authorTwo[1]Ronan Le Bras\\\\authorTwo[1]Oyvind Tafjord\\\\authorTwo[1]Chris Wilhelm\\\\authorThree[1]Luca Soldaini\\\\authorThree[1,2]Noah A. Smith\\\\authorThree[1,2]Yizhong Wang\\\\authorThree[1]Pradeep Dasigi\\\\authorThree[1,2]Hannaneh Hajishirzi\n1]Allen Institute for AI\n2]University of Washington\\\\contribution[\\*]T\u00fclu3 was a team effort.\\\\varheartsuit\\\\varheartsuit{\\\\color[rgb]{0.9296875,0.00390625,0.48828125}\\\\varheartsuit}marks core contributors. See full author contributionshere.\nContacttulu@allenai.org.\n# T\u00fclu3: Pushing Frontiers in\nOpen Language Model Post-Training\n###### Abstract\nLanguage model post-training is applied to refine behaviors and unlock new skills across a wide range of language models, but open recipes for applying these techniques lag behind proprietary ones.\nThe underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency.\nTo bridge this gap, we introduceT\u00fclu3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques.T\u00fclu3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku.\nThe training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR).\nWithT\u00fclu3, we build a multi-task evaluation scheme for post-training with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks.\nWe conclude with analysis and discussion of training methods that did not reliably improve performance.\nTheT\u00fclu3 release includes model weights, a demo, and the complete recipe \u2014datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting theT\u00fclu3 approach to more domains.\n\\\\metadata\n[![[Uncaptioned image]](x1.png)Tulu 3 8B:][Llama-3.1-Tulu-3-8B](https://hf.co/allenai/Llama-3.1-Tulu-3-8B)\\\\metadata[![[Uncaptioned image]](x1.png)Tulu 3 70B:][Llama-3.1-Tulu-3-70B](https://hf.co/allenai/Llama-3.1-Tulu-3-70B)\\\\metadata[![[Uncaptioned image]](x1.png)Tulu 3 405B:][Llama-3.1-Tulu-3-405B](https://hf.co/allenai/Llama-3.1-Tulu-3-405B)\\\\metadata[![[Uncaptioned image]](x1.png)Tulu 3Data:][tulu-3-datasets-673b8df14442393f7213f372](https://hf.co/collections/allenai/tulu-3-datasets-673b8df14442393f7213f372)\\\\metadata[![[Uncaptioned image]](x2.png)Tulu 3 Code:][open-instruct](https://github.com/allenai/open-instruct)\\\\metadata[![[Uncaptioned image]](x3.png)T\u00fclu\u00a03 Eval:][olmes](https://github.com/allenai/olmes)\\\\metadata[![[Uncaptioned image]](extracted/6352378/figures_images/ai2_logo.png)Demo:][playground.allenai.org](https://playground.allenai.org/)\nTable 1:Models, datasets, and code released withT\u00fclu3.Demo:[https://playground.allenai.org/](https://playground.allenai.org/)\nModel Checkpoints|\nStage|Llama 3.1 8B|Llama 3.1 70B|\nBase Model|[meta-llama/Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B)|[meta-llama/Llama-3.1-70B](https://huggingface.co/meta-llama/Llama-3.1-70B)|\nSFT|[allenai/Llama-3.1-Tulu-3-8B-SFT](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT)|[allenai/Llama-3.1-Tulu-3-70B-SFT](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B-SFT)|\nDPO|[allenai/Llama-3.1-Tulu-3-8B-DPO](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-DPO)|[allenai/Llama-3.1-Tulu-3-70B-DPO](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B-DPO)|\nFinal Model(RLVR)|[allenai/Llama-3.1-Tulu-3-8B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B)||\nRM:[allenai/Llama-3.1-Tulu-3-8B-RM](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-RM)|[allenai/Llama-3.1-Tulu-3-70B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B)||\n|||\nStage|Llama 3.1 405B||\nBase Model|[meta-llama/Llama-3.1-405B](https://huggingface.co/meta-llama/Llama-3.1-405B)||\nSFT|[allenai/Llama-3.1-Tulu-3-405B-SFT](https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B-SFT)||\nDPO|[allenai/Llama-3.1-Tulu-3-405B-DPO](https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B-DPO)||\nFinal Model(RLVR)|[allenai/Llama-3.1-Tulu-3-405B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B)|RM:Same as 8B/70B|\nCodebases / Tools|\nType|![[Uncaptioned image]](x4.png)Link|\nTraining|[allenai/open-instruct](https://www.github.com/allenai/open-instruct)|\nT\u00fclu\u00a03 Eval|[allenai/olmes](https://www.github.com/allenai/olmes)|\nDecontamination|[allenai/open-instruct/tree/main/decontamination](https://www.github.com/allenai/open-instruct/tree/main/decontamination)|\nPreference Data Inference|[allenai/birr](https://www.github.com/allenai/birr)|\nInstruction Datasets|\nType|Domain|![[Uncaptioned image]](x5.png)Link|\nFull mix|General|[allenai/tulu-3-sft-mixture](https://huggingface.co/datasets/allenai/tulu-3-sft-mixture)|\nTask Specific|Precise Instruction Following|[allenai/tulu-3-sft-personas-instruction-following](https://huggingface.co/datasets/allenai/tulu-3-sft-personas-instruction-following)|\nSubsets|MATH|[allenai/tulu-3-sft-personas-math](https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math)|\n|Grade School Math|[allenai/tulu-3-sft-personas-math-grade](https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade)|\n|Python Code|[allenai/tulu-3-sft-personas-code](https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code)|\nPreference Mixes|\nModel|![[Uncaptioned image]](x6.png)Link|\nLlama 3.1 405B|[allenai/llama-3.1-tulu-3-405b-preference-mixture](https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-405b-preference-mixture)|\nLlama 3.1 70B|[allenai/llama-3.1-tulu-3-70b-preference-mixture](https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-70b-preference-mixture)|\nLlama 3.1 8B|[allenai/llama-3.1-tulu-3-8b-preference-mixture](https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-8b-preference-mixture)|\nSpecific Preference Datasets|\nDomain|![[Uncaptioned image]](x7.png)Link|\nPrecise Instruction Following|[allenai/tulu-3-pref-personas-instruction-following](https://huggingface.co/datasets/allenai/tulu-3-pref-personas-instruction-following)|\nGeneral|[allenai/tulu-3-sft-prompts-ultrafeedback](https://huggingface.co/datasets/allenai/tulu-3-sft-prompts-ultrafeedback)|\nGeneral|[allenai/tulu-3-wildchat-ultrafeedback](https://huggingface.co/datasets/allenai/tulu-3-wildchat-ultrafeedback)|\nRL with Verifiable Rewards Training Datasets|\nDomain|![[Uncaptioned image]](x8.png)Link|\nFull Mix|[allenai/RLVR-GSM-MATH-IF-Mixed-Constraints](https://huggingface.co/datasets/allenai/RLVR-GSM-MATH-IF-Mixed-Constraints)|\nGSM Only|[allenai/RLVR-GSM](https://huggingface.co/datasets/allenai/RLVR-GSM)|\nMATH Only|[allenai/RLVR-MATH](https://huggingface.co/datasets/allenai/RLVR-MATH)|\nIFeval Only|[allenai/RLVR-IFeval](https://huggingface.co/datasets/allenai/RLVR-IFeval)|\n![Refer to caption](x9.png)Figure 1:An overview of theT\u00fclu3 recipe. This includes: data curation targeting general and target capabilities, training strategies and a standardized evaluation suite for development and final evaluation stage.\n## 1Introduction\n*\u201cJust as the camel shares its burdens with others in the caravan, the wise share their insights to lighten the load of ignorance.\u201d \u2013Proverb generated by*T\u00fclu3.\nPost-training \u2014the collection of techniques including instruction tuning, reinforcement learning from human feedback, and other types of finetuning \u2014has become a crucial step in building frontier language models> (\n> OpenAI2024\n> ; > Anthropic2024\n> )\n, yet developments to these techniques are frequently not accompanied by open resources and recipes.\nFully open source counterparts (e.g.,T\u00fclu2> (\n> ivison2023camels\n> )\nand Zephyr-\u03b2\ud835\udefd\\\\betaitalic\\_\u03b2> (\n> tunstall2023zephyr\n> )\n) often rely on simpler-to-implement and cheaper pipelines and have become outdated on many metrics.\nTo close the gap between open and closed post training, we introduceT\u00fclu111A t\u00fclu is a hybrid camel bred between Bactrian camel and dromedary:[https://en.wikipedia.org/wiki/Hybrid\\_camel](https://en.wikipedia.org/wiki/Hybrid_camel).3, a family of open state-of-the-art post-trained models, alongside all of the data, training recipes, code, infrastructure, and evaluation framework. Integrating partial details from proprietary methods with no",
          "original_query": "Gpts: Pushing frontiers in open language model post-training",
          "cleaned_query": "Gpts: Pushing frontiers in open language model post-training",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "[PDF] GPT-4 Technical Report - OpenAI",
          "url": "https://cdn.openai.com/papers/gpt-4.pdf",
          "content": "GPT-4 Technical Report\nOpenAI\u2217\nAbstract\nWe report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\non various professional and academic benchmarks, including passing a simulated\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer\u0002based model pre-trained to predict the next token in a document. The post-training\nalignment process results in improved performance on measures of factuality and\nadherence to desired behavior. A core component of this project was developing\ninfrastructure and optimization methods that behave predictably across a wide\nrange of scales. This allowed us to accurately predict some aspects of GPT-4\u2019s\nperformance based on models trained with no more than 1/1,000th the compute of\nGPT-4.\n1 Introduction\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\ntext inputs and producing text outputs. Such models are an important area of study as they have the\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\nand machine translation. As such, they have been the subject of substantial interest and progress in\nrecent years [1\u201334].\nOne of the main goals of developing such models is to improve their ability to understand and generate\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.\nThis contrasts with GPT-3.5, which scores in the bottom 10%.\nOn a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models\nand most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).\nOn the MMLU benchmark [35, 36], an English-language suite of multiple-choice questions covering\n57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but\nalso demonstrates strong performance in other languages. On translated variants of MMLU, GPT-4\nsurpasses the English-language state-of-the-art in 24 of 26 languages considered. We discuss these\nmodel capability results, as well as model safety improvements and results, in more detail in later\nsections.\nThis report also discusses a key challenge of the project, developing deep learning infrastructure and\noptimization methods that behave predictably across a wide range of scales. This allowed us to make\npredictions about the expected performance of GPT-4 (based on small runs trained in similar ways)\nthat were tested against the final run to increase confidence in our training.\nDespite its capabilities, GPT-4 has similar limitations to earlier GPT models [1, 37, 38]: it is not fully\nreliable (e.g. can suffer from \u201challucinations\u201d), has a limited context window, and does not learn\n\u2217\nPlease cite this work as \u201cOpenAI (2023)\". Full authorship contribution statements appear at the end of the\ndocument. Correspondence regarding this technical report can be sent to gpt4-report@openai.com\narXiv:submit/4812508 [cs.CL] 27 Mar 2023\nfrom experience. Care should be taken when using the outputs of GPT-4, particularly in contexts\nwhere reliability is important.\nGPT-4\u2019s capabilities and limitations create significant and novel safety challenges, and we believe\ncareful study of these challenges is an important area of research given the potential societal impact.\nThis report includes an extensive system card (after the Appendix) describing some of the risks we\nforesee around bias, disinformation, over-reliance, privacy, cybersecurity, proliferation, and more.\nIt also describes interventions we made to mitigate potential harms from the deployment of GPT-4,\nincluding adversarial testing with domain experts, and a model-assisted safety pipeline.\n2 Scope and Limitations of this Technical Report\nThis report focuses on the capabilities, limitations, and safety properties of GPT-4. GPT-4 is a\nTransformer-style model [39] pre-trained to predict the next token in a document, using both publicly\navailable data (such as internet data) and data licensed from third-party providers. The model was\nthen fine-tuned using Reinforcement Learning from Human Feedback (RLHF) [40]. Given both\nthe competitive landscape and the safety implications of large-scale models like GPT-4, this report\ncontains no further details about the architecture (including model size), hardware, training compute,\ndataset construction, training method, or similar.\nWe are committed to independent auditing of our technologies, and shared some initial steps and\nideas in this area in the system card accompanying this release.2 We plan to make further technical\ndetails available to additional third parties who can advise us on how to weigh the competitive and\nsafety considerations above against the scientific value of further transparency.\n3 Predictable Scaling\nA large focus of the GPT-4 project was building a deep learning stack that scales predictably. The\nprimary reason is that for very large training runs like GPT-4, it is not feasible to do extensive\nmodel-specific tuning. To address this, we developed infrastructure and optimization methods that\nhave very predictable behavior across multiple scales. These improvements allowed us to reliably\npredict some aspects of the performance of GPT-4 from smaller models trained using 1, 000\u00d7 \u2013\n10, 000\u00d7 less compute.\n3.1 Loss Prediction\nThe final loss of properly-trained large language models is thought to be well approximated by power\nlaws in the amount of compute used to train the model [41, 42, 2, 14, 15].\nTo verify the scalability of our optimization infrastructure, we predicted GPT-4\u2019s final loss on our\ninternal codebase (not part of the training set) by fitting a scaling law with an irreducible loss term\n(as in Henighan et al. [15]): L(C) = aCb + c, from models trained using the same methodology\nbut using at most 10,000x less compute than GPT-4. This prediction was made shortly after the run\nstarted, without use of any partial results. The fitted scaling law predicted GPT-4\u2019s final loss with\nhigh accuracy (Figure 1).\n3.2 Scaling of Capabilities on HumanEval\nHaving a sense of the capabilities of a model before training can improve decisions around alignment,\nsafety, and deployment. In addition to predicting final loss, we developed methodology to predict\nmore interpretable metrics of capability. One such metric is pass rate on the HumanEval dataset [43],\nwhich measures the ability to synthesize Python functions of varying complexity. We successfully\npredicted the pass rate on a subset of the HumanEval dataset by extrapolating from models trained\nwith at most 1, 000\u00d7 less compute (Figure 2).\nFor an individual problem in HumanEval, performance may occasionally worsen with scale. Despite\nthese challenges, we find an approximate power law relationship \u2212EP [log(pass_rate(C))] = \u03b1\u2217C\n\u2212k\n2\nIn addition to the accompanying system card, OpenAI will soon publish additional thoughts on the social\nand economic implications of AI systems, including the need for effective regulation.\n2\nObserved\nPrediction\ngpt-4\n100p 10n 1\u00b5 100\u00b5 0.01 1\nCompute\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\nBits per word\nOpenAI codebase next word prediction\nFigure 1. Performance of GPT-4 and smaller models. The metric is final loss on a dataset derived\nfrom our internal codebase. This is a convenient, large dataset of code tokens which is not contained in\nthe training set. We chose to look at loss because it tends to be less noisy than other measures across\ndifferent amounts of training compute. A power law fit to the smaller models (excluding GPT-4) is\nshown as the dotted line; this fit accurately predicts GPT-4\u2019s final loss. The x-axis is training compute\nnormalized so that GPT-4 is 1.\nObserved\nPrediction\ngpt-4\n1\u00b5 10\u00b5 100\u00b5 0.001 0.01 0.1 1\nCompute\n0\n1\n2\n3\n4\n5\n\u2013 Mean Log Pass Rate\nCapability prediction on 23 coding problems\nFigure 2. Performance of GPT-4 and smaller models. The metric is mean log pass rate on a subset of\nthe HumanEval dataset. A power law fit to the smaller models (excluding GPT-4) is shown as the dotted\nline; this fit accurately predicts GPT-4\u2019s performance. The x-axis is training compute normalized so that\nGPT-4 is 1.\n3\nwhere k and \u03b1 are positive constants, and P is a subset of problems in the dataset. We hypothesize\nthat this relationship holds for all problems in this dataset. In practice, very low pass rates are difficult\nor impossible to estimate, so we restrict to problems P and models M such that given some large\nsample budget, every problem is solved at least once by every model.\nWe registered predictions for GPT-4\u2019s performance on HumanEval before training completed, using\nonly information available prior to training. All but the 15 hardest HumanEval problems were split\ninto 6 difficulty buckets based on the performance of smaller models. The results on the 3\nrd easiest\nbucket are shown in Figure 2, showing that the resulting predictions were very accurate for this\nsubset of HumanEval problems where we can accurately estimate log(pass_rate) for several smaller\nmodels. Predictions on the other five buckets performed almost as well, the main exception being\nGPT-4 underperforming our predictions on the easiest bucket.\nCertain capabilities remain hard to predict. For example, the Inverse Scaling Prize [44] proposed\nseveral tasks for which model performance decreases as a function of scale. Similarly to a recent\nresult by Wei et al. [45], we find that GPT-4 reverses this trend, as shown on one of the tasks called\nHindsight Neglect [46] in Figure 3.\nada babbage curie gpt-3.5 gpt-4\nModel\n0\n50\n100\nAccuracy\nInv",
          "original_query": "GPT-4 technical report",
          "cleaned_query": "GPT-4 technical report",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Verifier-Conditioned PPO (VC-PPO) for RLVR\nExtend RLVR by conditioning the policy update on a calibrated \u201cverifiability score\u201d (e.g., probability that a verifier can judge correctness for the task), and use that to weight the PPO surrogate objective per token or per segment. Test whether VC-PPO improves sample efficiency by focusing exploration on trajectories where verifiable rewards are informative.",
        "PPO with Token-Level Trust Regions for Long-Form Reasoning\nReplace a single trajectory-level ratio clip with token-level or span-level trust regions that constrain updates more strongly on high-uncertainty reasoning steps (e.g., where the policy entropy spikes or the verifier is brittle). Evaluate improvements in long-context factuality and reasoning consistency, especially on tasks where verifiers only partially observe correctness.",
        "Hybrid DPO\u2192PPO Curriculum Driven by Reward Predictability\nDesign an automatic curriculum that begins with DPO (stable, offline) and transitions to PPO/RLVR only when reward/verification signals become sufficiently predictable (e.g., low variance, high agreement among multiple verifiers). Quantify whether this reduces the common instability of RLHF-like training and improves reproducibility across random seeds and scales.",
        "Ensemble-Verifier RLVR to Reduce Reward Model Exploits\nInstead of a single verifier, train an ensemble of heterogeneous verifiers (symbolic checkers, retrieval-based graders, smaller LMs) and define the PPO reward as a robust aggregation (e.g., trimmed mean or disagreement-penalized reward). Test whether this reduces exploitability and improves out-of-distribution reliability relative to single-verifier RLVR.",
        "Failure-Mode Targeted Post-Training Using Adversarial Task Mining\nBuild an \u201cadversarial miner\u201d that continually proposes prompts where the current model fails verifiable checks (or violates policy constraints), then prioritize these samples in PPO/RLVR updates. This creates a closed-loop post-training system that directly attacks hallucinations, jailbreak susceptibility, or reasoning gaps, and can be evaluated with predictable scaling metrics to estimate diminishing returns."
      ],
      "generated_ideas_raw": "1. **Adaptive Clip Scheduling for PPO in LLM Post-Training**\n   Develop a PPO variant where the clipping range (\u03b5) is dynamically adjusted based on observed KL divergence and reward-model (or verifier) uncertainty during post-training. Evaluate whether this stabilizes RLHF/RLVR updates and reduces reward hacking compared to fixed-\u03b5 PPO baselines on T\u00fclu 3-style multi-task evals.\n\n2. **Verifier-Conditioned PPO (VC-PPO) for RLVR**\n   Extend RLVR by conditioning the policy update on a calibrated \u201cverifiability score\u201d (e.g., probability that a verifier can judge correctness for the task), and use that to weight the PPO surrogate objective per token or per segment. Test whether VC-PPO improves sample efficiency by focusing exploration on trajectories where verifiable rewards are informative.\n\n3. **Predictable Scaling Laws for Post-Training Gains**\n   Using GPT-4\u2019s emphasis on predictable scaling, fit scaling laws that predict *post-training uplift* (SFT\u2192DPO\u2192RLVR/PPO) as a function of model size, preference dataset size, and verifier coverage. Produce a methodology to forecast whether an alignment run is worth doing at larger scales before spending the compute, and validate it across T\u00fclu 3 checkpoints (8B/70B/405B).\n\n4. **Multimodal RLVR with Image-Text Verifiable Rewards**\n   Create a multimodal RLVR benchmark where rewards are derived from verifiable checks on image-grounded questions (e.g., counting, spatial relations, OCR consistency), aligning with GPT-4\u2019s multimodal setting. Train an image-text assistant using PPO with these verifiers and quantify how verifiable reward coverage affects hallucination rates in multimodal responses.\n\n5. **Decontamination-Aware Preference Optimization**\n   Build a pipeline that integrates T\u00fclu 3-style benchmark decontamination signals directly into DPO/PPO training by downweighting or filtering preference pairs likely contaminated with eval content. Measure whether this reduces \u201cbenchmark memorization\u201d while preserving real capability, using held-out unseen tasks and adversarially similar evaluation sets.\n\n6. **PPO with Token-Level Trust Regions for Long-Form Reasoning**\n   Replace a single trajectory-level ratio clip with token-level or span-level trust regions that constrain updates more strongly on high-uncertainty reasoning steps (e.g., where the policy entropy spikes or the verifier is brittle). Evaluate improvements in long-context factuality and reasoning consistency, especially on tasks where verifiers only partially observe correctness.\n\n7. **Hybrid DPO\u2192PPO Curriculum Driven by Reward Predictability**\n   Design an automatic curriculum that begins with DPO (stable, offline) and transitions to PPO/RLVR only when reward/verification signals become sufficiently predictable (e.g., low variance, high agreement among multiple verifiers). Quantify whether this reduces the common instability of RLHF-like training and improves reproducibility across random seeds and scales.\n\n8. **Ensemble-Verifier RLVR to Reduce Reward Model Exploits**\n   Instead of a single verifier, train an ensemble of heterogeneous verifiers (symbolic checkers, retrieval-based graders, smaller LMs) and define the PPO reward as a robust aggregation (e.g., trimmed mean or disagreement-penalized reward). Test whether this reduces exploitability and improves out-of-distribution reliability relative to single-verifier RLVR.\n\n9. **Compute-Efficient RLVR via Off-Policy Replay with PPO Constraints**\n   Introduce a replay buffer of past model rollouts and apply constrained PPO updates that correct for off-policy data via importance sampling while enforcing a KL budget (trust region). Benchmark wall-clock and GPU-hour reductions for T\u00fclu 3-like post-training while maintaining (or improving) alignment and helpfulness metrics.\n\n10. **Failure-Mode Targeted Post-Training Using Adversarial Task Mining**\n   Build an \u201cadversarial miner\u201d that continually proposes prompts where the current model fails verifiable checks (or violates policy constraints), then prioritize these samples in PPO/RLVR updates. This creates a closed-loop post-training system that directly attacks hallucinations, jailbreak susceptibility, or reasoning gaps, and can be evaluated with predictable scaling metrics to estimate diminishing returns.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Verifier-Conditioned PPO (VC-PPO) for RLVR\nExtend RLVR by conditioning the policy update on a calibrated \u201cverifiability score\u201d (e.g., probability that a verifier can judge correctness for the task), a",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "PPO with Token-Level Trust Regions for Long-Form Reasoning\nReplace a single trajectory-level ratio clip with token-level or span-level trust regions that constrain updates more strongly on high-uncert",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hybrid DPO\u2192PPO Curriculum Driven by Reward Predictability\nDesign an automatic curriculum that begins with DPO (stable, offline) and transitions to PPO/RLVR only when reward/verification signals become",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Ensemble-Verifier RLVR to Reduce Reward Model Exploits\nInstead of a single verifier, train an ensemble of heterogeneous verifiers (symbolic checkers, retrieval-based graders, smaller LMs) and define t",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Failure-Mode Targeted Post-Training Using Adversarial Task Mining\nBuild an \u201cadversarial miner\u201d that continually proposes prompts where the current model fails verifiable checks (or violates policy con",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 62,
      "paper_title": "Learning to Learn with Contrastive Meta-Objective",
      "contribution": "The paper introduces ConML, a meta-learning framework that utilizes task identity as additional supervision through contrastive learning to enhance generalizability.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8775,
      "output_tokens": 1008,
      "predecessor_details": [
        {
          "success": true,
          "title": "Learning to Learn: Introduction and Overview - Springer Link",
          "url": "https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1",
          "content": "Learning to Learn: Introduction and Overview | Springer Nature Link (formerly SpringerLink)\n[Skip to main content](#main-content)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1?)\n# Learning to Learn: Introduction and Overview\n* Chapter\n* pp 3\u201317\n* [Cite this chapter](#citeas)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/book/978-1-4615-5529-2?as=webp)Learning to Learn](https://link.springer.com/book/10.1007/978-1-4615-5529-2)\n* [Sebastian Thrun](#auth-Sebastian-Thrun)&amp;\n* [Lorien Pratt](#auth-Lorien-Pratt)\n* 3282Accesses\n* 444Citations\n* 7[Altmetric](https://link.altmetric.com/details/36897609)\n## Abstract\nOver the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications (see e.g., [Langley, 1992; Widrow et al., 1994]).\nThis is a preview of subscription content,[log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1?error=cookies_not_supported&code=988ab554-48a8-4f69-be38-722cea986301)to check access.\n## Access this chapter\n[Log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1?error=cookies_not_supported&code=988ab554-48a8-4f69-be38-722cea986301)\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n## Preview\nUnable to display preview.[Download preview\nPDF.](https://page-one.springer.com/pdf/preview/10.1007/978-1-4615-5529-2_1)\nUnable to display preview.[Download preview\nPDF.](https://page-one.springer.com/pdf/preview/10.1007/978-1-4615-5529-2_1)\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-19-2879-6?as&#x3D;webp)\n### [Machine Learning](https://link.springer.com/10.1007/978-981-19-2879-6_2?fromPaywallRec=true)\nChapter\u00a9 2023\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-97-7426-5?as&#x3D;webp)\n### [A Study of Machine Learning Dynamics: Algorithms, Applications, and Fundamental Frameworks](https://link.springer.com/10.1007/978-981-97-7426-5_4?fromPaywallRec=true)\nChapter\u00a9 2025\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-97-1260-1?as&#x3D;webp)\n### [Machine Learning: Future Prospectus and Research Direction](https://link.springer.com/10.1007/978-981-97-1260-1_14?fromPaywallRec=true)\nChapter\u00a9 2024\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Learning algorithms](https://link.springer.com/subjects/learning-algorithms)\n* [Learning Process](https://link.springer.com/subjects/learning-process)\n* [Learning Theory](https://link.springer.com/subjects/learning-theory)\n* [Machine Learning](https://link.springer.com/subjects/machine-learning)\n* [Organizational Learning](https://link.springer.com/subjects/organizational-learning)\n* [Statistical Learning](https://link.springer.com/subjects/statistical-learning)\n## References\n* Y. S. Abu-Mostafa. A method for learning from hints. In S. J. Hanson, J. Cowan, and C. L. Giles, editors,*Advances in Neural Information Processing Systems 5*, pages 73\u201380, San Mateo, CA, 1993. Morgan Kaufmann.\n[Google Scholar]()\n* W.-K. Ahn and W. F. Brewer. Psychological studies of explanation-based learning. In G. DeJong, editor,*Investigating Explanation-Based Learning*. Kluwer Academic Publishers, Boston/ Dordrecht/London, 1993.\n[Google Scholar]()\n* W.-K. Ahn, R. Mooney, W. F. Brewer, and G. F. DeJong. Schema acquisition from one example: Psychological evidence for explanation-based learning. In*Proceedings of the Ninth Annual Conference of the Cognitive Science Society*, Seattle, WA, July 1987.\n[Google Scholar]()\n* C. A. Atkeson. Using locally weighted regression for robot learning. In*Proceedings of the 1991 IEEE International Conference on Robotics and Automation*, pages 958\u2013962, Sacramento, CA, April 1991.\n[Google Scholar]()\n* A. G. Barto, S. J. Bradtke, and S. P. Singh. Learning to act using real-time dynamic programming.*Artificial Intelligence*, 72:81\u2013138, 1995.\n[Article](https://doi.org/10.1016/0004-3702(94)00011-O)[Google Scholar]()\n* J. Baxter. The Canonical Distortion Measure for Vector Quantization and Function Approximation. Chapter 7 in this book.\n[Google Scholar]()\n* J. Baxter.*Learning Internal Representations*. PhD thesis, Flinders University, Australia, 1995.\n[Google Scholar]()\n* D. Beymer and T. Poggio. Face recognition from one model view. In*Proceedings of the International Conference on Computer Vision*, 1995.\n[Google Scholar]()\n* A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Occams razor.*Information Processing Letters*, 24:377\u2013380, 1987.\n[Article](https://doi.org/10.1016/0020-0190(87)90114-1)[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=896392)[MATH](http://www.emis.de/MATH-item?0653.68084)[Google Scholar]()\n* C.E. Brodley.*Recursive Automatic Algorithm Selection for Inductive Learning*. PhD thesis, University of Massachusetts, Amherst, MA 01003, August 1994. also available as COINS Technical Report 94-61.\n[Google Scholar]()\n* R. Caruana. Multitask learning: A knowledge-based of source of inductive bias. In P. E. Utgoff, editor,*Proceedings of the Tenth International Conference on Machine Learning*, pages 41\u201348, San Mateo, CA, 1993. Morgan Kaufmann.\n[Google Scholar]()\n* R. Caruana. Algorithms and applications for multitask learning. In L. Saitta, editor,*Proceedings of the Thirteenth International Conference on Machine Learning*, San Mateo, CA, July 1996. Morgan Kaufmann.\n[Google Scholar]()\n* R. Caruana and S. Baluja. Using the future to\u2019 sort out\u2019 the present: Rankprop and multitask learning for medical risk evaluation. In D. Touretzky, M. Mozer, and M.E. Hasselmo, editors,*Advances in Neural Information Processing Systems 8*, Cambridge, MA, 1996. MIT Press. to appear.\n[Google Scholar]()\n* R. Caruana, D.L. Silver, J. Baxter, T.M. Mitchell, L.Y. Pratt, and Thrun. S. Workshop on \u201cLearning to learn: Knowledge consolidation and transfer in inductive systems\u201d. Workshop, held at NIPS-95, Vail, CO, see World Wide Web at[http://www.cs.cmu](http://www.cs.cmu.edu/afscs.cmu), December 1995.\n* N.L. Cramer. A representation for the adaptive generation of simple sequential programs. In J.J. Grefenstette, editor,*Proceedings of First International Conference on Genetic Algorithms and their Applications*, pages 183\u2013187, Pittsburgh, PA, 1985.\n[Google Scholar]()\n* P. Dayan and G. E. Hinton. Feudal reinforcement learning. In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors,*Advances in Neural Information Processing Systems 5*, San Mateo, CA, 1993. Morgan Kaufmann.\n[Google Scholar]()\n* L. DeRaedt, N. Lavra\u010d, and S. D\u017eeroski. Multiple predicate learning. In*Proceedings of IJCAI-93*, pages 1037\u20131042, Chamberry, France, July 1993. IJCAI, Inc.\n[Google Scholar]()\n* A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. A general lower bound on the number of examples needed for learning.*Information and Computation*, 82:247\u2013261, 1989.\n[Article](https://doi.org/10.1016/0890-5401(89)90002-3)[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=1016683)[MATH](http://www.emis.de/MATH-item?0679.68158)[Google Scholar]()\n* R. Franke. Scattered data interpolation: Tests of some methods.*Mathematics of Computation*, 38(157):181\u2013200, January 1982.\n[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=637296)[MATH](http://www.emis.de/MATH-item?0476.65005)[Google Scholar]()\n* J. H. Friedman. Flexible metric nearest neighbor classification. November 1994.\n[Google Scholar]()\n* S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the bias/variance dilemma.*Neural Computation*, 4:1\u201358, 1992.\n[Article](https://doi.org/10.1162/neco.1992.4.1.1)[Google Scholar]()\n* T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classification. Submitted for publication, December 1994.\n[Google Scholar]()\n* H. Hild and A. Waibel. Multi-speaker/speaker-independent architectures for the multi-state time delay neural network. In*Proceedings of the International Conference on Acoustics, Speech and Signal Processing*, pages II 255\u2013258. IEEE, April 1993.\n[Google Scholar]()\n* T. Hume and M.J. Pazzani. Learning sets of related concepts: A shared task model. In*Proceedings of the Eighteenth Annual Conference of the Cognitive Science Society*, 1996.\n[Google Scholar]()\n* L. P. Kaelbling. Hierarchical learning in stochastic domains: Preliminary results. In P. E. Utgoff, editor,*Proceedings of the Tenth International Conference on Machine Learning*, pages 167\u2013173, San Mateo, CA, 1993. Morgan Kaufmann.\n[Google Scholar]()\n* M. Kearns and U. Vazirani.*Introduction to Computational Learning Theory*. MIT Press, Cambridge, MA, 1994.\n[Google Scholar]()\n* J. Koza.*Genetic Programming: On the Programming of Computers by Means of Natural Selection*. MIT Press, Cambridge, MA, 1992.\n[MATH](http://www.emis.de/MATH-item?0850.68161)[Google Scholar]()\n* J. Koza.*Genetic Programming II: Automatic Discovery of Reusable Programs*. MIT Press, Cambridge, MA, 1994.\n[MATH](http://www.emis.de/MATH-item?0850.68160)[Google Scholar]()\n* J. Laird, P. Rosenbloom, and A. Newell. Chunking in SOAR: The anatomy of a general learning mechanism.*Machine Learning*, 1(1): 11\u201346, 1986.\n[Google Sch",
          "original_query": "Learning to learn: Introduction and overview",
          "cleaned_query": "Learning to learn: Introduction and overview",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
          "url": "https://proceedings.mlr.press/v70/finn17a/finn17a.pdf",
          "content": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nChelsea Finn 1 Pieter Abbeel 1 2 Sergey Levine 1\nAbstract\nWe propose an algorithm for meta-learning that\nis model-agnostic, in the sense that it is com\u0002patible with any model trained with gradient de\u0002scent and applicable to a variety of different\nlearning problems, including classification, re\u0002gression, and reinforcement learning. The goal\nof meta-learning is to train a model on a vari\u0002ety of learning tasks, such that it can solve new\nlearning tasks using only a small number of train\u0002ing samples. In our approach, the parameters of\nthe model are explicitly trained such that a small\nnumber of gradient steps with a small amount\nof training data from a new task will produce\ngood generalization performance on that task. In\neffect, our method trains the model to be easy\nto fine-tune. We demonstrate that this approach\nleads to state-of-the-art performance on two few\u0002shot image classification benchmarks, produces\ngood results on few-shot regression, and acceler\u0002ates fine-tuning for policy gradient reinforcement\nlearning with neural network policies.\n1. Introduction\nLearning quickly is a hallmark of human intelligence,\nwhether it involves recognizing objects from a few exam\u0002ples or quickly learning new skills after just minutes of\nexperience. Our artificial agents should be able to do the\nsame, learning and adapting quickly from only a few exam\u0002ples, and continuing to adapt as more data becomes avail\u0002able. This kind of fast and flexible learning is challenging,\nsince the agent must integrate its prior experience with a\nsmall amount of new information, while avoiding overfit\u0002ting to the new data. Furthermore, the form of prior ex\u0002perience and new data will depend on the task. As such,\nfor the greatest applicability, the mechanism for learning to\nlearn (or meta-learning) should be general to the task and\n1University of California, Berkeley 2OpenAI. Correspondence\nto: Chelsea Finn.\nProceedings of the 34 th International Conference on Machine\nLearning, Sydney, Australia, PMLR 70, 2017. Copyright 2017\nby the author(s).\nthe form of computation required to complete the task.\nIn this work, we propose a meta-learning algorithm that\nis general and model-agnostic, in the sense that it can be\ndirectly applied to any learning problem and model that\nis trained with a gradient descent procedure. Our focus\nis on deep neural network models, but we illustrate how\nour approach can easily handle different architectures and\ndifferent problem settings, including classification, regres\u0002sion, and policy gradient reinforcement learning, with min\u0002imal modification. In meta-learning, the goal of the trained\nmodel is to quickly learn a new task from a small amount\nof new data, and the model is trained by the meta-learner\nto be able to learn on a large number of different tasks.\nThe key idea underlying our method is to train the model\u2019s\ninitial parameters such that the model has maximal perfor\u0002mance on a new task after the parameters have been up\u0002dated through one or more gradient steps computed with\na small amount of data from that new task. Unlike prior\nmeta-learning methods that learn an update function or\nlearning rule (Schmidhuber, 1987; Bengio et al., 1992;\nAndrychowicz et al., 2016; Ravi & Larochelle, 2017), our\nalgorithm does not expand the number of learned param\u0002eters nor place constraints on the model architecture (e.g.\nby requiring a recurrent model (Santoro et al., 2016) or a\nSiamese network (Koch, 2015)), and it can be readily com\u0002bined with fully connected, convolutional, or recurrent neu\u0002ral networks. It can also be used with a variety of loss func\u0002tions, including differentiable supervised losses and non\u0002differentiable reinforcement learning objectives.\nThe process of training a model\u2019s parameters such that a\nfew gradient steps, or even a single gradient step, can pro\u0002duce good results on a new task can be viewed from a fea\u0002ture learning standpoint as building an internal representa\u0002tion that is broadly suitable for many tasks. If the internal\nrepresentation is suitable to many tasks, simply fine-tuning\nthe parameters slightly (e.g. by primarily modifying the top\nlayer weights in a feedforward model) can produce good\nresults. In effect, our procedure optimizes for models that\nare easy and fast to fine-tune, allowing the adaptation to\nhappen in the right space for fast learning. From a dynami\u0002cal systems standpoint, our learning process can be viewed\nas maximizing the sensitivity of the loss functions of new\ntasks with respect to the parameters: when the sensitivity\nis high, small local changes to the parameters can lead to\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nlarge improvements in the task loss.\nThe primary contribution of this work is a simple model\u0002and task-agnostic algorithm for meta-learning that trains\na model\u2019s parameters such that a small number of gradi\u0002ent updates will lead to fast learning on a new task. We\ndemonstrate the algorithm on different model types, includ\u0002ing fully connected and convolutional networks, and in sev\u0002eral distinct domains, including few-shot regression, image\nclassification, and reinforcement learning. Our evaluation\nshows that our meta-learning algorithm compares favor\u0002ably to state-of-the-art one-shot learning methods designed\nspecifically for supervised classification, while using fewer\nparameters, but that it can also be readily applied to regres\u0002sion and can accelerate reinforcement learning in the pres\u0002ence of task variability, substantially outperforming direct\npretraining as initialization.\n2. Model-Agnostic Meta-Learning\nWe aim to train models that can achieve rapid adaptation, a\nproblem setting that is often formalized as few-shot learn\u0002ing. In this section, we will define the problem setup and\npresent the general form of our algorithm.\n2.1. Meta-Learning Problem Set-Up\nThe goal of few-shot meta-learning is to train a model that\ncan quickly adapt to a new task using only a few datapoints\nand training iterations. To accomplish this, the model or\nlearner is trained during a meta-learning phase on a set\nof tasks, such that the trained model can quickly adapt to\nnew tasks using only a small number of examples or trials.\nIn effect, the meta-learning problem treats entire tasks as\ntraining examples. In this section, we formalize this meta\u0002learning problem setting in a general manner, including\nbrief examples of different learning domains. We will dis\u0002cuss two different learning domains in detail in Section 3.\nWe consider a model, denoted f, that maps observa\u0002tions x to outputs a. During meta-learning, the model\nis trained to be able to adapt to a large or infinite num\u0002ber of tasks. Since we would like to apply our frame\u0002work to a variety of learning problems, from classifica\u0002tion to reinforcement learning, we introduce a generic\nnotion of a learning task below. Formally, each task\nT = {L(x1, a1, . . . , xH, aH), q(x1), q(xt+1|xt, at), H}\nconsists of a loss function L, a distribution over initial ob\u0002servations q(x1), a transition distribution q(xt+1|xt, at),\nand an episode length H. In i.i.d. supervised learning prob\u0002lems, the length H = 1. The model may generate samples\nof length H by choosing an output at at each time t. The\nloss L(x1, a1, . . . , xH, aH) \u2192 R, provides task-specific\nfeedback, which might be in the form of a misclassification\nloss or a cost function in a Markov decision process.\nmeta-learning\nlearning/adaptation\n\u03b8\n\u2207L1\n\u2207L2\n\u2207L3\n\u03b8\n\u2217\n1 \u03b8\n\u2217\n2\n\u03b8\n\u2217\n3\nFigure 1. Diagram of our model-agnostic meta-learning algo\u0002rithm (MAML), which optimizes for a representation \u03b8 that can\nquickly adapt to new tasks.\nIn our meta-learning scenario, we consider a distribution\nover tasks p(T ) that we want our model to be able to adapt\nto. In the K-shot learning setting, the model is trained to\nlearn a new task Ti drawn from p(T ) from only K samples\ndrawn from qi and feedback LTi generated by Ti. During\nmeta-training, a task Tiis sampled from p(T ), the model\nis trained with K samples and feedback from the corre\u0002sponding loss LTi\nfrom Ti, and then tested on new samples\nfrom Ti. The model f is then improved by considering how\nthe test error on new data from qi changes with respect to\nthe parameters. In effect, the test error on sampled tasks Ti\nserves as the training error of the meta-learning process. At\nthe end of meta-training, new tasks are sampled from p(T ),\nand meta-performance is measured by the model\u2019s perfor\u0002mance after learning from K samples. Generally, tasks\nused for meta-testing are held out during meta-training.\n2.2. A Model-Agnostic Meta-Learning Algorithm\nIn contrast to prior work, which has sought to train re\u0002current neural networks that ingest entire datasets (San\u0002toro et al., 2016; Duan et al., 2016b) or feature embed\u0002dings that can be combined with nonparametric methods at\ntest time (Vinyals et al., 2016; Koch, 2015), we propose a\nmethod that can learn the parameters of any standard model\nvia meta-learning in such a way as to prepare that model\nfor fast adaptation. The intuition behind this approach is\nthat some internal representations are more transferrable\nthan others. For example, a neural network might learn\ninternal features that are broadly applicable to all tasks in\np(T ), rather than a single individual task. How can we en\u0002courage the emergence of such general-purpose representa\u0002tions? We take an explicit approach to this problem: since\nthe model will be fine-tuned using a gradient-based learn\u0002ing rule on a new task, we will aim to learn a model in such\na way that this gradient-based learning rule can make rapid\nprogress on new tasks drawn from p(T ), without overfit\u0002ting. In effect, we will aim to find model parameters that\nare sensitive to changes in the task, such that small changes\nin the parameters will produce large improvements on the\nloss function of any task drawn from p(T ), when altered in\nthe direction of the gradient of that loss (see Figure 1). We\nModel-Agnostic Meta-Lea",
          "original_query": "Model-agnostic meta-learning for fast adaptation of deep networks",
          "cleaned_query": "Model-agnostic meta-learning for fast adaptation of deep networks",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] A Simple Framework for Contrastive Learning of Visual ...",
          "url": "https://proceedings.mlr.press/v119/chen20j/chen20j.pdf",
          "content": "A Simple Framework for Contrastive Learning of Visual Representations\nTing Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton 1\nAbstract\nThis paper presents SimCLR: a simple framework\nfor contrastive learning of visual representations.\nWe simplify recently proposed contrastive self\u0002supervised learning algorithms without requiring\nspecialized architectures or a memory bank. In\norder to understand what enables the contrastive\nprediction tasks to learn useful representations,\nwe systematically study the major components of\nour framework. We show that (1) composition of\ndata augmentations plays a critical role in defining\neffective predictive tasks, (2) introducing a learn\u0002able nonlinear transformation between the repre\u0002sentation and the contrastive loss substantially im\u0002proves the quality of the learned representations,\nand (3) contrastive learning benefits from larger\nbatch sizes and more training steps compared to\nsupervised learning. By combining these findings,\nwe are able to considerably outperform previous\nmethods for self-supervised and semi-supervised\nlearning on ImageNet. A linear classifier trained\non self-supervised representations learned by Sim\u0002CLR achieves 76.5% top-1 accuracy, which is a\n7% relative improvement over previous state-of\u0002the-art, matching the performance of a supervised\nResNet-50. When fine-tuned on only 1% of the\nlabels, we achieve 85.8% top-5 accuracy, outper\u0002forming AlexNet with 100\u00d7 fewer labels. 1\n1. Introduction\nLearning effective visual representations without human\nsupervision is a long-standing problem. Most mainstream\napproaches fall into one of two classes: generative or dis\u0002criminative. Generative approaches learn to generate or\notherwise model pixels in the input space (Hinton et al.,\n2006; Kingma & Welling, 2013; Goodfellow et al., 2014).\n1Google Research, Brain Team. Correspondence to: Ting Chen\n.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\n1Code available at https://github.com/google-research/simclr.\nFigure 1. ImageNet Top-1 accuracy of linear classifiers trained\non representations learned with different self-supervised meth\u0002ods (pretrained on ImageNet). Gray cross indicates supervised\nResNet-50. Our method, SimCLR, is shown in bold.\nHowever, pixel-level generation is computationally expen\u0002sive and may not be necessary for representation learning.\nDiscriminative approaches learn representations using objec\u0002tive functions similar to those used for supervised learning,\nbut train networks to perform pretext tasks where both the in\u0002puts and labels are derived from an unlabeled dataset. Many\nsuch approaches have relied on heuristics to design pretext\ntasks (Doersch et al., 2015; Zhang et al., 2016; Noroozi &\nFavaro, 2016; Gidaris et al., 2018), which could limit the\ngenerality of the learned representations. Discriminative\napproaches based on contrastive learning in the latent space\nhave recently shown great promise, achieving state-of-the\u0002art results (Hadsell et al., 2006; Dosovitskiy et al., 2014;\nOord et al., 2018; Bachman et al., 2019).\nIn this work, we introduce a simple framework for con\u0002trastive learning of visual representations, which we call\nSimCLR. Not only does SimCLR outperform previous work\n(Figure 1), but it is also simpler, requiring neither special\u0002ized architectures (Bachman et al., 2019; H\u00e9naff et al., 2019)\nnor a memory bank (Wu et al., 2018; Tian et al., 2019; He\net al., 2019; Misra & van der Maaten, 2019).\nIn order to understand what enables good contrastive repre\u0002sentation learning, we systematically study the major com\u0002ponents of our framework and show that:\nA Simple Framework for Contrastive Learning of Visual Representations\n\u2022 Composition of multiple data augmentation operations\nis crucial in defining the contrastive prediction tasks that\nyield effective representations. In addition, unsupervised\ncontrastive learning benefits from stronger data augmen\u0002tation than supervised learning.\n\u2022 Introducing a learnable nonlinear transformation be\u0002tween the representation and the contrastive loss substan\u0002tially improves the quality of the learned representations.\n\u2022 Representation learning with contrastive cross entropy\nloss benefits from normalized embeddings and an appro\u0002priately adjusted temperature parameter.\n\u2022 Contrastive learning benefits from larger batch sizes and\nlonger training compared to its supervised counterpart.\nLike supervised learning, contrastive learning benefits\nfrom deeper and wider networks.\nWe combine these findings to achieve a new state-of-the-art\nin self-supervised and semi-supervised learning on Ima\u0002geNet ILSVRC-2012 (Russakovsky et al., 2015). Under the\nlinear evaluation protocol, SimCLR achieves 76.5% top-1\naccuracy, which is a 7% relative improvement over previous\nstate-of-the-art (H\u00e9naff et al., 2019). When fine-tuned with\nonly 1% of the ImageNet labels, SimCLR achieves 85.8%\ntop-5 accuracy, a relative improvement of 10% (H\u00e9naff et al.,\n2019). When fine-tuned on other natural image classifica\u0002tion datasets, SimCLR performs on par with or better than\na strong supervised baseline (Kornblith et al., 2019) on 10\nout of 12 datasets.\n2. Method\n2.1. The Contrastive Learning Framework\nInspired by recent contrastive learning algorithms (see Sec\u0002tion 7 for an overview), SimCLR learns representations\nby maximizing agreement between differently augmented\nviews of the same data example via a contrastive loss in\nthe latent space. As illustrated in Figure 2, this framework\ncomprises the following four major components.\n\u2022 A stochastic data augmentation module that transforms\nany given data example randomly resulting in two cor\u0002related views of the same example, denoted x\u02dci and x\u02dcj ,\nwhich we consider as a positive pair. In this work, we\nsequentially apply three simple augmentations: random\ncropping followed by resize back to the original size, ran\u0002dom color distortions, and random Gaussian blur. As\nshown in Section 3, the combination of random crop and\ncolor distortion is crucial to achieve a good performance.\n\u2022 A neural network base encoder f(\u00b7) that extracts repre\u0002sentation vectors from augmented data examples. Our\nframework allows various choices of the network archi\u0002tecture without any constraints. We opt for simplicity\nand adopt the commonly used ResNet (He et al., 2016)\n\u2190\u2212 Representation \u2212\u2192\nx\nx\u02dci x\u02dcj\nhi hj\nzi zj\nt \u223c T\nt\n0 \u223c T\nf(\u00b7) f(\u00b7)\ng(\u00b7) g(\u00b7)\nMaximize agreement\nFigure 2. A simple framework for contrastive learning of visual\nrepresentations. Two separate data augmentation operators are\nsampled from the same family of augmentations (t \u223c T and\nt\n0 \u223c T ) and applied to each data example to obtain two correlated\nviews. A base encoder network f(\u00b7) and a projection head g(\u00b7)\nare trained to maximize agreement using a contrastive loss. After\ntraining is completed, we throw away the projection head g(\u00b7) and\nuse encoder f(\u00b7) and representation h for downstream tasks.\nto obtain hi = f(x\u02dci) = ResNet(x\u02dci) where hi \u2208 R\nd\nis\nthe output after the average pooling layer.\n\u2022 A small neural network projection head g(\u00b7) that maps\nrepresentations to the space where contrastive loss is\napplied. We use a MLP with one hidden layer to obtain\nzi = g(hi) = W(2)\u03c3(W(1)hi) where \u03c3 is a ReLU non\u0002linearity. As shown in section 4, we find it beneficial to\ndefine the contrastive loss on zi\u2019s rather than hi\u2019s.\n\u2022 A contrastive loss function defined for a contrastive pre\u0002diction task. Given a set {x\u02dck} including a positive pair\nof examples x\u02dci and x\u02dcj , the contrastive prediction task\naims to identify x\u02dcj in {x\u02dck}k6=i for a given x\u02dci.\nWe randomly sample a minibatch of N examples and define\nthe contrastive prediction task on pairs of augmented exam\u0002ples derived from the minibatch, resulting in 2N data points.\nWe do not sample negative examples explicitly. Instead,\ngiven a positive pair, similar to (Chen et al., 2017), we treat\nthe other 2(N \u2212 1) augmented examples within a minibatch\nas negative examples. Let sim(u, v) = u\n>v/kukkvk de\u0002note the dot product between `2 normalized u and v (i.e.\ncosine similarity). Then the loss function for a positive pair\nof examples (i, j) is defined as\n`i,j = \u2212 log exp(sim(zi\n, zj )/\u03c4 )\nP2N\nk=1 1[k6=i] exp(sim(zi\n, zk)/\u03c4 )\n, (1)\nwhere 1[k6=i] \u2208 {0, 1} is an indicator function evaluating to\n1 iff k 6= i and \u03c4 denotes a temperature parameter. The fi\u0002nal loss is computed across all positive pairs, both (i, j)\nand (j, i), in a mini-batch. This loss has been used in\nprevious work (Sohn, 2016; Wu et al., 2018; Oord et al.,\n2018); for convenience, we term it NT-Xent (the normalized\ntemperature-scaled cross entropy loss).\nA Simple Framework for Contrastive Learning of Visual Representations\nAlgorithm 1 SimCLR\u2019s main learning algorithm.\ninput: batch size N, constant \u03c4 , structure of f, g, T .\nfor sampled minibatch {xk}\nN\nk=1 do\nfor all k \u2208 {1, . . . , N} do\ndraw two augmentation functions t\u223c T , t\n0 \u223c T\n# the first augmentation\nx\u02dc2k\u22121 = t(xk)\nh2k\u22121 = f(x\u02dc2k\u22121) # representation\nz2k\u22121 = g(h2k\u22121) # projection\n# the second augmentation\nx\u02dc2k = t\n0\n(xk)\nh2k = f(x\u02dc2k) # representation\nz2k = g(h2k) # projection\nend for\nfor all i \u2208 {1, . . . , 2N} and j \u2208 {1, . . . , 2N} do\nsi,j = z\n>\ni zj/(kzikkzjk) # pairwise similarity\nend for\ndefine `(i, j) as `(i, j)=\u2212 log P\nexp(si,j /\u03c4)\n2N\nk=1 1[k6=i] exp(si,k/\u03c4)\nL =\n1\n2N\nPN\nk=1 [`(2k\u22121, 2k) + `(2k, 2k\u22121)]\nupdate networks f and g to minimize L\nend for\nreturn encoder network f(\u00b7), and throw away g(\u00b7)\nAlgorithm 1 summarizes the proposed method.\n2.2. Training with Large Batch Size\nTo keep it simple, we do not train the model with a memory\nbank (Wu et al., 2018; He et al., 2019). Instead, we vary\nthe training batch size N from 256 to 8192. A batch size\nof 8192 gives us 16382 negative examples per positive pair\nfrom both augmentation views. Training with large batch\nsize may be unstable when using standard SGD/Momentum\nwith linear learning rate scaling (Goyal et al., 2017). To\nstabilize the training, we use the LARS optimizer (",
          "original_query": "A simple framework for contrastive learning of visual representations",
          "cleaned_query": "A simple framework for contrastive learning of visual representations",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Function Contrastive Learning of Transferable Meta-Representations",
          "url": "http://proceedings.mlr.press/v139/gondal21a/gondal21a.pdf",
          "content": "Function Contrastive Learning of Transferable Meta-Representations\nMuhammad Waleed Gondal 1 Shruti Joshi 1 Nasim Rahaman 1 2 Stefan Bauer 1 3 Manuel Wuthrich \u00a8\n1\nBernhard Scholkopf \u00a8\n1\nAbstract\nMeta-learning algorithms adapt quickly to new\ntasks that are drawn from the same task distribu\u0002tion as the training tasks. The mechanism leading\nto fast adaptation is the conditioning of a down\u0002stream predictive model on the inferred represen\u0002tation of the task\u2019s underlying data generative\nprocess, or function. This meta-representation,\nwhich is computed from a few observed exam\u0002ples of the underlying function, is learned jointly\nwith the predictive model. In this work, we study\nthe implications of this joint training on the trans\u0002ferability of the meta-representations. Our goal\nis to learn meta-representations that are robust\nto noise in the data and facilitate solving a wide\nrange of downstream tasks that share the same un\u0002derlying functions. To this end, we propose a de\u0002coupled encoder-decoder approach to supervised\nmeta-learning, where the encoder is trained with\na contrastive objective to find a good representa\u0002tion of the underlying function. In particular, our\ntraining scheme is driven by the self-supervision\nsignal indicating whether two sets of examples\nstem from the same function. Our experiments\non a number of synthetic and real-world datasets\nshow that the representations we obtain outper\u0002form strong baselines in terms of downstream per\u0002formance and noise robustness, even when these\nbaselines are trained in an end-to-end manner.\n1. Introduction\nMany supervised learning problems are concerned with ap\u0002proximating a data-generating function f : X \u2192 Y given\na finite set of N samples, {xi, yi = f(xi)}\nN\ni=1. Expressive\nmodels, such as deep neural networks, are known to excel\nat this function approximation task, but they often heavily\n1Max Planck Institute for Intelligent Systems, Tubingen, Ger- \u00a8\nmany 2Mila, University of Montreal, Montreal, Canada 3CIFAR\nAzrieli Global Scholar. Correspondence to: Muhammad Waleed\nGondal.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nrely on the number of samples N being large. This poses\nfurther challenges: in many domains of interest, sourcing\nenough data is a challenging endeavour; further, the process\nof training such models can be prohibitively slow for many\napplications. This is exacerbated by the fact that in the\ntypical setting, each new data-generating function encoun\u0002tered requires that the model be retrained. In other words,\nthe model is not shared between data-generating functions,\neven when training a model to approximate one function can\npotentially be beneficial for approximating another function.\nTo overcome these challenges, a variety of meta-learning\nmethods have been proposed (Vinyals et al., 2016; Snell\net al., 2017; Garnelo et al., 2018a; Ravi & Larochelle, 2016;\nFinn et al., 2017). In the present work, we are interested in a\nclass of models that use encoder-decoder architectures such\nas Conditional Neural Processes (CNPs) (Garnelo et al.,\n2018a) and Generative Query Networks (GQNs) (Eslami\net al., 2018). In the first stage, an encoder is used to infer\na fixed-dimensional representation of a given function f\nfrom just a few input-output examples Of = {(xi, yi)}i,\nthe context dataset. We call it the meta-representation of the\nfunction r = h\u03c6(Of), where h is an encoder parameterized\nby \u03c6. In the second stage, the meta-representation is then\nused to condition a predictive model in order to solve a\ndownstream prediction task related to that function. For\ninstance, the task may consist of predicting the function\nvalue y at unseen locations x or classifying images after\nobserving only a few pixels (in that case, x is the pixel\nlocation and y is the pixel value). This two-stage process has\nmultiple benefits. First, the extraction of prior knowledge\nabout f directly from the training data, in the form of meta\u0002representation, reduces the need for specifying inductive\nbiases (model architectures, training details, etc.) particular\nto f. Thus, it allows learning to be shared between functions\nsuch that a single model can be trained on a distribution over\nfunctions. Second, the computation of meta-representations\nis efficient and can be done online. Third, the computation\nof meta-representations provides flexibility to solve a variety\nof downstream tasks concerning a specific function.\nHowever, CNPs optimize encoder jointly with the decoder\non the downstream prediction task, i.e., prediction of func\u0002tion values y at unseen locations x, as illustrated in Fig\u0002ure 1(a). This ties the meta-representation\u2019s quality to the\nFunction Contrastive Learning of Transferable Meta-Representations\n(a) CNP (b) FCRL (encoder training) (c) FCRL (transfer)\nFigure 1. The difference in the training of CNP (Garnelo et al., 2018a) and FCRL for learning meta-representations r of functions. (shown\nleft) CNP learns the aggregated representation r of the context set by maximizing the conditional likelihood of the target data. (shown\ncenter) Training of FCRL encoder h\u03c6 by contrasting context sets of different functions. Note that the target inputs xN+1 etc., are not used\nat this stage. (shown right) Using the pretrained FCRL encoder h\u03c6, we train separate decoders p\n\u2217\n\u03c8 for each downstream task, shown in\ngrey boxes. The dotted arrows indicate the transfer of inferred meta-representations to the tasks.\ncombined encoder and decoder performances on this par\u0002ticular task and thereby makes it susceptible to supervision\ncollapse, i.e. the representations lose any information which\nis irrelevant for solving the training task, but may be nec\u0002essary for the transfer to new tasks (Doersch et al., 2020).\nMoreover, many real-world tasks are noisy, and the predic\u0002tion task might entail reconstructing high dimensional data,\nsuch as images in GQNs (Eslami et al., 2018). The corre\u0002sponding objective function can cause the model to waste\nits capacity on reconstructing unimportant features such as\nstatic backgrounds and noise, while ignoring visually small\nbut important details in its learned representation (Anand\net al., 2019; Kipf et al., 2019). This is crucial for many real\u0002world applications; for instance, in order to manipulate a\nsmall object in a complex scene, the model\u2019s ability to infer\nthe object\u2019s shape carries more importance than inferring its\ncolor or reconstructing the static background.\nIn this work, we study the generalization of a func\u0002tion\u2019s meta-representations in terms of their transferabil\u0002ity to downstream tasks and their robustness to noise.\nWe empirically show that the joint optimization of meta\u0002representations and a prediction task is detrimental to the\ntransferability of meta-representations and makes them vul\u0002nerable to noise. To address this issue, we propose a decou\u0002pled encoder-decoder training scheme, wherein the encoder\nis exclusively trained by a novel contrastive learning frame\u0002work which we call FCRL (Function Contrastive Represen\u0002tation Learning). Instead of relying on reconstructions, it\nlearns by contrasting sets of input-output pairs sampled from\ndifferent functions. The key idea is that two sets of samples\nfrom the same function should have similar latent repre\u0002sentations, while representations of sets of samples from\ndifferent functions should remain easily distinguishable.\nFCRL retains the useful properties of meta-representations\nsuch as shared learning and sample efficiency while improv\u0002ing its transferability to downstream tasks and robustness\nto noise. Unlike contemporary meta-learning algorithms,\nmeta-representations in FCRL are explicitly optimized over\na distribution of functions rather than tasks.\nTo evaluate the effectiveness of the proposed method, we\nconduct comprehensive experiments on diverse downstream\nproblems, including classification, regression, parameter\nidentification, scene understanding, scene reconstruction\nand reinforcement learning. We consider different datasets,\nranging from simple 1D and 2D regression to challenging\nsimulated and real-world scenes. In particular, we find\nthat a downstream predictor trained with our (pre-trained)\nencoder compares favorably to related methods on these\ntasks, including ones where the predictor is trained jointly\nwith the encoder.\n2. Preliminaries\n2.1. Problem Setting\nConsider a distribution over data-generating functions p(f).\nLet f be a sample from this distribution f \u223c p(f), where\nf : X \u2192 Y with X = R\nd\nand Y \u2286 R\nd\n0\n:\ny = f(x, \u03be); \u03be \u223c Z (1)\nwhere \u03be is sampled from some noise distribution Z. Let\nOf = {(xi, yi)}\nN\ni=1 be a set of few observed examples of\na function f, referred to as the context set, and consider a\nset of downstream tasks T . Here, each task T \u2208 T can be\ndefined as a mapping defined over f. In the case of few shot\nregression (see Section 4.1), T maps from f to a predictive\nmodel p\u03c8(y|x). In the case of parameter identification, T\nmaps from f to some scalar or vector valued parameter of\nf. Our goal is therefore to learn an encoder which maps a\ncontext set Ofto a representation of f that can interchange\u0002ably be used for multiple downstream tasks T defined on\nthe same function (without requiring retraining).\n2.2. Background\nIn this section, we briefly discuss a class of meta-learning\nmethods that are particularly relevant to our encoder\u0002decoder setting, namely conditional neural processes (CNPs)\nand generative query networks (GQNs) (Garnelo et al.,\n2018a;b; Eslami et al., 2018).\nFunction Contrastive Learning of Transferable Meta-Representations\nConditional Neural Processes (CNPs). The key proposal\nin CNPs (applied to few-shot learning) is to express a\ndistribution over predictor functions given a context set.\nThey learn the meta-representations r by jointly training\nthe encoder and decoder, as illustrated in Figure 1(a). To\nthis end, they first encode the context Ofinto individ\u0002ual representations ri = h\u03a6(xi\n, yi) \u2200i \u2208 [N], where ",
          "original_query": "Function contrastive learning of transferable meta-representations",
          "cleaned_query": "Function contrastive learning of transferable meta-representations",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Learning to Model the Tail - NIPS papers",
          "url": "https://papers.nips.cc/paper/7278-learning-to-model-the-tail",
          "content": "\n Learning to Model the Tail \n \nPart of\n Advances in Neural Information Processing Systems 30 (NIPS 2017)\n \n \n \n Authors \n Yu-Xiong Wang, Deva Ramanan, Martial Hebert \n Abstract \n We describe an approach to learning from long-tailed, imbalanced datasets that are prevalent in real-world settings. Here, the challenge is to learn accurate \"few-shot'' models for classes in the tail of the class distribution, for which little data is available. We cast this problem as transfer learning, where knowledge from the data-rich classes in the head of the distribution is transferred to the data-poor classes in the tail. Our key insights are as follows. First, we propose to transfer meta-knowledge about learning-to-learn from the head classes. This knowledge is encoded with a meta-network that operates on the space of model parameters, that is trained to predict many-shot model parameters from few-shot model parameters. Second, we transfer this meta-knowledge in a progressive manner, from classes in the head to the \"body'', and from the \"body'' to the tail. That is, we transfer knowledge in a gradual fashion, regularizing meta-networks for few-shot regression with those trained with more training data. This allows our final network to capture a notion of model dynamics, that predicts how model parameters are likely to change as more training data is gradually added. We demonstrate results on image classification datasets (SUN, Places, and ImageNet) tuned for the long-tailed setting, that significantly outperform common heuristics, such as data resampling or reweighting. \n \n Requests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings. \n Use the \"Report an Issue\" link to request a name change. \n Do not remove: This comment is monitored to verify that the site is working properly",
          "original_query": "Learning to model the tail",
          "cleaned_query": "Learning to model the tail",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Meta-Contrastive MAML: Self-Supervised Inner-Loop Adaptation\nTrain MAML-style initializations where the inner-loop objective is contrastive (SimCLR) on unlabeled support sets, and the outer-loop optimizes downstream few-shot performance. This makes adaptation possible without labels at test time and tests whether \u201ceasy-to-fine-tune\u201d priors can be learned from purely instance-discrimination signals.",
        "Function-Level SimCLR for Task Distributions (F-SimCLR)\nExtend SimCLR to operate on *sets of (x,y) samples* from a task (as in Function Contrastive Learning), using augmentations over context sets (subsampling, input warps, noise injection) to define positive pairs from the same underlying function. Evaluate transfer by freezing the function encoder and training multiple decoders for heterogeneous downstream tasks (regression, classification over function identity, calibration), directly probing representation transferability.",
        "Progressive Meta-Networks for MAML Updates in Long-Tailed Few-Shot\nCombine \u201cLearning to Model the Tail\u201d progressive head\u2192body\u2192tail transfer with MAML by learning a meta-network that predicts *per-layer inner-loop learning rates and update directions* as a function of class/sample count. Concretely, train on head classes to predict effective gradient steps, then progressively regularize these predictors as data availability shrinks, targeting stability and performance on extreme-tail classes.",
        "Noise-Robust Meta-Representations via Multi-View Context Contrast\nBuild on FCRL by generating multiple noisy/augmented \u201cviews\u201d of the same context set and enforcing invariance in the function embedding while remaining sensitive to informative variation (e.g., heteroscedastic noise). Benchmark on real long-tailed or weakly-labeled datasets with controlled corruption to quantify improvements in adaptation under label noise and covariate shift.",
        "Adaptive Augmentation Policies for Contrastive-to-Few-Shot Transfer\nLearn augmentation distributions (cropping/color/blur strength, plus task-level set augmentations) that *explicitly optimize few-shot adaptation speed* under a MAML outer loop. The key contribution is a differentiable augmentation policy trained end-to-end to maximize post-adaptation accuracy, testing the hypothesis from SimCLR that augmentation choice is the main lever for transferable representations.",
        "Parameter-Space Contrastive Learning for Few-Shot Classifiers\nInspired by \u201cLearning to Model the Tail\u201d (meta-networks operating on parameters), perform contrastive learning directly in *model-parameter space*: treat parameters learned from different few-shot subsets of the same class as positives and different classes as negatives. This yields a representation of \u201cclassifiers as points,\u201d enabling a meta-network to denoise or extrapolate few-shot classifiers toward many-shot solutions.",
        "Task Retrieval for Rapid Adaptation Using Function Embedding Indexes\nUse FCRL-style function embeddings to build an approximate nearest-neighbor index over previously seen tasks; at test time, retrieve similar tasks and initialize either (a) MAML parameters or (b) class-specific heads using a weighted combination of retrieved solutions. This is actionable as an algorithmic plug-in that reduces adaptation steps and can be evaluated by measuring accuracy vs. number of gradient updates.",
        "Meta-Calibrated Few-Shot Learning on the Tail\nDevelop a meta-learning objective that optimizes not only accuracy after few-shot adaptation (MAML) but also *probability calibration* specifically for tail classes (e.g., minimize expected calibration error post-update). Combine progressive transfer (head\u2192tail) with a calibration-aware outer loss to address overconfidence that commonly worsens under imbalance and low data.",
        "Unified Encoder for Instance + Function Contrastive Learning\nTrain a shared backbone with two simultaneous contrastive objectives: instance-level (SimCLR positives are two views of an image) and function-level (FCRL positives are two context sets from the same task/function). The contribution is a multi-granularity representation that can serve both standard recognition and meta-learning, evaluated by freezing the encoder and comparing transfer across (i) linear probing, (ii) few-shot, and (iii) function regression tasks.",
        "Gradient-Sensitivity Regularization to Improve \u201cEase of Fine-Tuning\u201d\nOperationalize MAML\u2019s dynamical-systems view by adding an explicit regularizer that shapes the Hessian/gradient sensitivity of new-task losses around the initialization (e.g., encourage well-conditioned curvature along task-relevant directions). Test whether such sensitivity shaping improves few-shot adaptation and tail-class transfer without increasing inner-loop steps, providing a principled mechanism for \u201clearning to learn\u201d beyond standard meta-loss optimization."
      ],
      "generated_ideas_raw": "1. **Meta-Contrastive MAML: Self-Supervised Inner-Loop Adaptation**\n   Train MAML-style initializations where the inner-loop objective is contrastive (SimCLR) on unlabeled support sets, and the outer-loop optimizes downstream few-shot performance. This makes adaptation possible without labels at test time and tests whether \u201ceasy-to-fine-tune\u201d priors can be learned from purely instance-discrimination signals.\n\n2. **Function-Level SimCLR for Task Distributions (F-SimCLR)**\n   Extend SimCLR to operate on *sets of (x,y) samples* from a task (as in Function Contrastive Learning), using augmentations over context sets (subsampling, input warps, noise injection) to define positive pairs from the same underlying function. Evaluate transfer by freezing the function encoder and training multiple decoders for heterogeneous downstream tasks (regression, classification over function identity, calibration), directly probing representation transferability.\n\n3. **Progressive Meta-Networks for MAML Updates in Long-Tailed Few-Shot**\n   Combine \u201cLearning to Model the Tail\u201d progressive head\u2192body\u2192tail transfer with MAML by learning a meta-network that predicts *per-layer inner-loop learning rates and update directions* as a function of class/sample count. Concretely, train on head classes to predict effective gradient steps, then progressively regularize these predictors as data availability shrinks, targeting stability and performance on extreme-tail classes.\n\n4. **Noise-Robust Meta-Representations via Multi-View Context Contrast**\n   Build on FCRL by generating multiple noisy/augmented \u201cviews\u201d of the same context set and enforcing invariance in the function embedding while remaining sensitive to informative variation (e.g., heteroscedastic noise). Benchmark on real long-tailed or weakly-labeled datasets with controlled corruption to quantify improvements in adaptation under label noise and covariate shift.\n\n5. **Adaptive Augmentation Policies for Contrastive-to-Few-Shot Transfer**\n   Learn augmentation distributions (cropping/color/blur strength, plus task-level set augmentations) that *explicitly optimize few-shot adaptation speed* under a MAML outer loop. The key contribution is a differentiable augmentation policy trained end-to-end to maximize post-adaptation accuracy, testing the hypothesis from SimCLR that augmentation choice is the main lever for transferable representations.\n\n6. **Parameter-Space Contrastive Learning for Few-Shot Classifiers**\n   Inspired by \u201cLearning to Model the Tail\u201d (meta-networks operating on parameters), perform contrastive learning directly in *model-parameter space*: treat parameters learned from different few-shot subsets of the same class as positives and different classes as negatives. This yields a representation of \u201cclassifiers as points,\u201d enabling a meta-network to denoise or extrapolate few-shot classifiers toward many-shot solutions.\n\n7. **Task Retrieval for Rapid Adaptation Using Function Embedding Indexes**\n   Use FCRL-style function embeddings to build an approximate nearest-neighbor index over previously seen tasks; at test time, retrieve similar tasks and initialize either (a) MAML parameters or (b) class-specific heads using a weighted combination of retrieved solutions. This is actionable as an algorithmic plug-in that reduces adaptation steps and can be evaluated by measuring accuracy vs. number of gradient updates.\n\n8. **Meta-Calibrated Few-Shot Learning on the Tail**\n   Develop a meta-learning objective that optimizes not only accuracy after few-shot adaptation (MAML) but also *probability calibration* specifically for tail classes (e.g., minimize expected calibration error post-update). Combine progressive transfer (head\u2192tail) with a calibration-aware outer loss to address overconfidence that commonly worsens under imbalance and low data.\n\n9. **Unified Encoder for Instance + Function Contrastive Learning**\n   Train a shared backbone with two simultaneous contrastive objectives: instance-level (SimCLR positives are two views of an image) and function-level (FCRL positives are two context sets from the same task/function). The contribution is a multi-granularity representation that can serve both standard recognition and meta-learning, evaluated by freezing the encoder and comparing transfer across (i) linear probing, (ii) few-shot, and (iii) function regression tasks.\n\n10. **Gradient-Sensitivity Regularization to Improve \u201cEase of Fine-Tuning\u201d**\n   Operationalize MAML\u2019s dynamical-systems view by adding an explicit regularizer that shapes the Hessian/gradient sensitivity of new-task losses around the initialization (e.g., encourage well-conditioned curvature along task-relevant directions). Test whether such sensitivity shaping improves few-shot adaptation and tail-class transfer without increasing inner-loop steps, providing a principled mechanism for \u201clearning to learn\u201d beyond standard meta-loss optimization.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Meta-Contrastive MAML: Self-Supervised Inner-Loop Adaptation\nTrain MAML-style initializations where the inner-loop objective is contrastive (SimCLR) on unlabeled support sets, and the outer-loop optim",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Function-Level SimCLR for Task Distributions (F-SimCLR)\nExtend SimCLR to operate on *sets of (x,y) samples* from a task (as in Function Contrastive Learning), using augmentations over context sets (su",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Progressive Meta-Networks for MAML Updates in Long-Tailed Few-Shot\nCombine \u201cLearning to Model the Tail\u201d progressive head\u2192body\u2192tail transfer with MAML by learning a meta-network that predicts *per-laye",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Noise-Robust Meta-Representations via Multi-View Context Contrast\nBuild on FCRL by generating multiple noisy/augmented \u201cviews\u201d of the same context set and enforcing invariance in the function embeddin",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Adaptive Augmentation Policies for Contrastive-to-Few-Shot Transfer\nLearn augmentation distributions (cropping/color/blur strength, plus task-level set augmentations) that *explicitly optimize few-sho",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Parameter-Space Contrastive Learning for Few-Shot Classifiers\nInspired by \u201cLearning to Model the Tail\u201d (meta-networks operating on parameters), perform contrastive learning directly in *model-paramete",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Task Retrieval for Rapid Adaptation Using Function Embedding Indexes\nUse FCRL-style function embeddings to build an approximate nearest-neighbor index over previously seen tasks; at test time, retriev",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Meta-Calibrated Few-Shot Learning on the Tail\nDevelop a meta-learning objective that optimizes not only accuracy after few-shot adaptation (MAML) but also *probability calibration* specifically for ta",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Unified Encoder for Instance + Function Contrastive Learning\nTrain a shared backbone with two simultaneous contrastive objectives: instance-level (SimCLR positives are two views of an image) and funct",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Gradient-Sensitivity Regularization to Improve \u201cEase of Fine-Tuning\u201d\nOperationalize MAML\u2019s dynamical-systems view by adding an explicit regularizer that shapes the Hessian/gradient sensitivity of new-",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 63,
      "paper_title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
      "contribution": "KVzip introduces a query-agnostic KV cache eviction method that enables the reuse of compressed KV caches across diverse queries, significantly reducing memory overhead and attention latency.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "quality_content": 3,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 6977,
      "output_tokens": 904,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1706.03762] Attention Is All You Need - arXiv",
          "url": "https://arxiv.org/abs/1706.03762",
          "content": "[1706.03762] Attention Is All You Need[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.03762\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1706.03762**(cs)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03762v1)), last revised 2 Aug 2023 (this version, v7)]\n# Title:Attention Is All You Need\nAuthors:[Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani,+A),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+N),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Llion Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+L),[Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gomez,+A+N),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[Illia Polosukhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Polosukhin,+I)\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n[View PDF](https://arxiv.org/pdf/1706.03762)[HTML (experimental)](https://arxiv.org/html/1706.03762v7)> > Abstract:\n> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments:|15 pages, 5 figures|\nSubjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)[cs.CL]|\n|(or[arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Llion Jones [[view email](https://arxiv.org/show-email/f53b7360/1706.03762)]\n**[[v1]](https://arxiv.org/abs/1706.03762v1)**Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\n**[[v2]](https://arxiv.org/abs/1706.03762v2)**Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.03762v3)**Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\n**[[v4]](https://arxiv.org/abs/1706.03762v4)**Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\n**[[v5]](https://arxiv.org/abs/1706.03762v5)**Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\n**[[v6]](https://arxiv.org/abs/1706.03762v6)**Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\n**[v7]**Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03762)\n* [HTML (experimental)](https://arxiv.org/html/1706.03762v7)\n* [TeX Source](https://arxiv.org/src/1706.03762)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03762&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03762&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2017-06](https://arxiv.org/list/cs.CL/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03762?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.03762?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03762)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03762)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03762)\n### [123 blog links](https://arxiv.org/tb/1706.03762)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17)\n[Ashish Vaswani]()\n[Noam Shazeer]()\n[Niki Parmar]()\n[Jakob Uszkoreit]()\n[Llion Jones]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1706.03762)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Attention is all you need",
          "cleaned_query": "Attention is all you need",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Dynamic Context Pruning for Efficient and Interpretable ... - arXiv",
          "url": "https://arxiv.org/html/2305.15805",
          "content": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers\n# Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers\nSotiris Anagnostidis\u03bcDario Pavllo\u03bcLuca Biggio\u03bc,\u03bdLorenzo Noci\u03bc&amp;Aurelien Lucchi\u03c4Thomas Hofmann\u03bc\n\u03bcETH Z\u00fcrich\n\u03bdML, CSEM SA\n\u03c4University of Basel\n###### Abstract\nAutoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model\u2019s expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model\u2019s decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to2\u00d72\\\\times2 \u00d7increase in inference throughput and even greater memory savings.\n\u2020\u2020Correspondence[sanagnos@inf.ethz.ch](mailto:sanagnos@inf.ethz.ch).\n## 1Introduction\nThe introduction of Transformers> (Vaswani et\u00a0al., [> 2017\n](https://arxiv.org/html/2305.15805v3#bib.bib51)> )\nin Large Language Models (LLMs) has profoundly influenced the landscape of Natural Language Processing (NLP), due to their appealing scaling properties> (Kaplan et\u00a0al., [> 2020\n](https://arxiv.org/html/2305.15805v3#bib.bib22)> )\nand their ability to train efficiently on modern hardware architectures designed for extensive parallel computing.\nAs LLMs grow larger and more complex, the challenges associated with training and deploying them become more prominent. Especially challenging is the quest for processing increasingly longer sequences, as pure self-attention layers scale quadratically in sequence length during train and inference.\nTo address this limitation, several efforts focus on efficient implementations of the attention mechanism on dedicated hardware> (Dao et\u00a0al., [> 2022\n](https://arxiv.org/html/2305.15805v3#bib.bib10)> ; Touvron et\u00a0al., [> 2023\n](https://arxiv.org/html/2305.15805v3#bib.bib49)> )\n, or on algorithmic procedures to directly tackle the quadratic complexity. The latter direction has led to numerous variants sacrificing the generality of the standard attention mechanism in favor of more efficient alternatives> (Tay et\u00a0al., [> 2020\n](https://arxiv.org/html/2305.15805v3#bib.bib48)> ; Kitaev et\u00a0al., [> 2020\n](https://arxiv.org/html/2305.15805v3#bib.bib25)> ; Choromanski et\u00a0al., [> 2020b\n](https://arxiv.org/html/2305.15805v3#bib.bib7)> ; Katharopoulos et\u00a0al., [> 2020\n](https://arxiv.org/html/2305.15805v3#bib.bib23)> ; Zaheer et\u00a0al., [> 2020\n](https://arxiv.org/html/2305.15805v3#bib.bib59)> ; Shi et\u00a0al., [> 2021\n](https://arxiv.org/html/2305.15805v3#bib.bib45)> ; Lin et\u00a0al., [> 2022\n](https://arxiv.org/html/2305.15805v3#bib.bib30)> ; Zhu and Soricut, [> 2021\n](https://arxiv.org/html/2305.15805v3#bib.bib61)> ; Dai et\u00a0al., [> 2020\n](https://arxiv.org/html/2305.15805v3#bib.bib9)> )\n, some of which are illustrated in Fig.[1](https://arxiv.org/html/2305.15805v3#S1.F1). Specifically, a large number of these methods focus either on sparsifying the attention weights, reducing the size of the available context to each token, or compressing the number of tokens to reduce the size of the attention matrix.\nThese methods, however, are inherently static, in the sense that each token is either forced to attend to a fixed pre-specified context window, or the input context is compressed to a fixed dimensionality, regardless of the information content of the input sequence. Furthermore, a performance gap still exists with respect to pure self-attention in many applications, thus implying the existence of a non-trivial trade-off between the span of the attention context and the model\u2019s capabilities> (Dao et\u00a0al., [> 2022\n](https://arxiv.org/html/2305.15805v3#bib.bib10)> ; Sun et\u00a0al., [> 2021\n](https://arxiv.org/html/2305.15805v3#bib.bib47)> ; Beltagy et\u00a0al., [> 2020\n](https://arxiv.org/html/2305.15805v3#bib.bib2)> )\n.\nTo address these challenges, and enhance inference efficiency, while staying faithful to pure self-attention, we pose the following question:\nCan we dynamically prune past content based on the available context,|\nwhile preserving as much as possible the expressivity of the model?|\nIn response to this question, we introduce a novel method for context pruning in Transformer-based decoder architectures. Our approach adds a minimal amount of additional training parameters that enable individual tokens to dynamically remove portions of the input sequence in a layer-wise fashion. Once part of the context is removed, it is disregarded for the remaining part of the autoregressive generation process, leading to reduced memory usage and computational requirements during inference. To this end, we also design a dynamic data structure that implements efficient insertion/removal of tokens from the context while supporting batched inference. In contrast to traditional methods relying on local or sparse attention, which may not capture the nuances and dynamic nature of the data over long contexts, ours leverages contextual cues to dynamically determine the relevance of the available information through a learned mechanism. This is achieved by making use of a sparse sigmoid function> (Peters et\u00a0al., [> 2019\n](https://arxiv.org/html/2305.15805v3#bib.bib38)> ; Martins et\u00a0al., [> 2020\n](https://arxiv.org/html/2305.15805v3#bib.bib31)> )\n. As demonstrated by our experimental evaluations, this allows us to extract and utilize essential details in a more adaptive and accurate manner. The degree of pruning can be effectively controlled through a hyperparameter that effectively accounts for the sparsity level.\nOur technique serves as a modular building block for existing pre-trained models and can be easily integrated through a minimal fine-tuning stage. For our study, we focus on GPT-2 models> (Radford et\u00a0al., [> 2019\n](https://arxiv.org/html/2305.15805v3#bib.bib40)> )\nas they are publicly available and widely benchmarked, but due to the uniformity of modern architectures, our approach can be straightforwardly extended to any autoregressive Transformer. Moreover, since our method is based on context pruning, it can be seamlessly combined with other approaches aimed at improving inference efficiency, such as quantization, weight pruning, approximate attention, or other hardware optimizations.\n![Refer to caption](x1.png)Figure 1:Visualization of the causal attention weights associated with standard, local, sparse causal attention, and our approach. Adaptively sparse attention (rightmost) prunes weights dynamically for each token, and it does not impose any restricting inductive biases on the final attention structure.\nWe find that up to80%percent8080\\\\%80 %of the context can be successfully pruned, with minimal deterioration in terms of perplexity and zero-shot performance, while requiring significantly fewer resources during inference. We showcase how these improvements can lead to measurable practical gains, by providing an efficient implementation that reduces memory usage for caching during token generation. More specifically, for larger context sizes we get up to50%percent5050\\\\%50 %wall-time latency reduction for each generation step, while still decoding with up to2\u00d72\\\\times2 \u00d7larger batch sizes, leading thus to significant performance benefits. These findings highlight the potential of context pruning as a powerful technique to enhance the efficiency and interpretability of Transformers in NLP.\n## 2Related Work\nDespite exhibiting human-level performance on a number of challenging tasks, LLMs are resource intensive and inefficient. While the human brain consumes roughly the amount of energy equivalent to a dim light bulb, top-performing GPT models require multiple GPUs with\u223csimilar-to\\\\sim\u223c80GB of memory each for inference> (Strubell et\u00a0al., [> 2019\n](https://arxiv.org/html/2305.15805v3#bib.bib46)> ; Frantar and Alistarh, [> 2023a\n](https://arxiv.org/html/2305.15805v3#bib.bib12)> )\n. Several research efforts have been focusing on improving their efficiency and memory requirements from several different angles.\n#### Weight Pruning and Quantization.\nModern LLMs have high memory and compute requirements for both training and testing. To address this limitation, a number of research efforts> (Kwon et\u00a0al., [> 2022\n](https://arxiv.org/html/2305.15805v3#bib.bib27)> ; Frantar et\u00a0al., [> 2023\n](https://arxiv.org/html/2305.15805v3#bib.bib15)> ; Frantar and Alistarh, [> 2023b\n](https://arxiv.org/html/2305.15805v3#bib.bib13)> )\nhave resorted to the established practice of weight pruning> (Hassibi et\u00a0al., [> 1993\n](https://arxiv.org/html/2305.15805v3#bib.bib17)> )\nto efficiently compress the original model to a more manageable size. Remarkably, a large percentage of the original weights can be safely removed, resulting in only marginal perplexity growth> (Bahl et\u00a0al., [> 1983\n](https://arxiv.org/html/2305.15805v3#bib.bib1)> )\n. An alternative approach to reduce the memory and compute, is quantization> (Dettmers et\u00a0al., [> 2022\n](https://arxiv.org/html/2305.15805v3#bib.bib11)> ; Yao et\u00a0al., [> 2022\n](https://arxiv.org/html/2305.15805v3#bib.bib57)> ; Xiao et\u00a0al., [> 2022\n](https://arxiv.org",
          "original_query": "Dynamic context pruning for efficient and interpretable autoregressive transformers",
          "cleaned_query": "Dynamic context pruning for efficient and interpretable autoregressive transformers",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "SnapKV: LLM Knows What You are Looking for Before Generation",
          "url": "https://arxiv.org/html/2404.14469",
          "content": "SnapKV: LLM Knows What You are Looking for Before Generation\n# SnapKV: LLM Knows What You are Looking for Before Generation\nYuhong Li1Yingbing Huang111footnotemark:1Bowen Yang2Bharat Venkitesh2Acyr Locatelli2\nHanchen Ye1Tianle Cai3Patrick Lewis2Deming Chen1\n1University of Illinois Urbana-Champaign2Cohere3Princeton University\n1{leeyh, yh21, hanchen8, dchen}@illinois.edu\n2{bowen, bharat, acyr, patrick}@cohere.com\n3tianle.cai@princeton.eduequal contribution\n###### Abstract\nLarge Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introducesSnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.\nWe discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an \u2018observation\u2019 window located at the end of the prompts. Drawing on this insight,SnapKVautomatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically,SnapKVachieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover,SnapKVcan process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggestSnapKV\u2019s potential for practical applications.\n## 1Introduction\nMany leading LLMs have started to handle longer contexts, overcoming the difficulties in context maintenance and attention mechanism scalability, such as GPT-4> [\n[> 1\n](https://arxiv.org/html/2404.14469v2#bib.bib1)> ]\nand Command-R> [\n[> 2\n](https://arxiv.org/html/2404.14469v2#bib.bib2)> ]\nwith context length 128K, Claude-3> [\n[> 3\n](https://arxiv.org/html/2404.14469v2#bib.bib3)> ]\nwith 200K, and Gemini-Pro-1.5 with 1M> [\n[> 4\n](https://arxiv.org/html/2404.14469v2#bib.bib4)> ]\n. Despite their impressive capabilities, LLMs still face significant challenges when dealing with long context prompts. Specifically, the KV cache in attention calculation becomes less efficient when processing long context. During inference time, as prompt length increases, the decoding latency per step grows linearly due to the attention calculation across past KVs. Moreover, the large KV cache requires significant memory capacity, increasing hardware demands and limiting model scalability.\nThere are many approaches to mitigate these problems, such as KV cache eviction during generation stage> [\n[> 5\n](https://arxiv.org/html/2404.14469v2#bib.bib5)> , [> 6\n](https://arxiv.org/html/2404.14469v2#bib.bib6)> , [> 7\n](https://arxiv.org/html/2404.14469v2#bib.bib7)> , [> 8\n](https://arxiv.org/html/2404.14469v2#bib.bib8)> ]\n. However, most of these methods lack a detailed evaluation in long-context settings. Moreover, they mainly focus on compressing the KV cache appended during decoding steps, while overlooking the realistic problem of compressing KV cache for prompts, which is typically the bottleneck in memory efficiency.\nIn practical applications like chatbots and agents, where prompts range from multi-turn conversations to extensive articles or codebases> [\n[> 1\n](https://arxiv.org/html/2404.14469v2#bib.bib1)> , [> 9\n](https://arxiv.org/html/2404.14469v2#bib.bib9)> , [> 10\n](https://arxiv.org/html/2404.14469v2#bib.bib10)> ]\n, prompts are often much larger than generated responses such as summaries and code pieces, thus creating significant inference latency and memory utilization overhead. Additional challenge lies in compressing KV cache for such vast prompts without losing crucial information for accurate generation, especially in scenarios with various noisy contexts.\n![Refer to caption](x1.png)Figure 1:The graph shows the simplified workflow ofSnapKV, where the orange area represents the cluster of features per head selected bySnapKV. These features are then used to form new Key-Value pairs concatenated with the features in the observation window. Together, the selected prefix and observation windows constitute the new KV cache utilized for the generation.\nIn our paper, we find an important attention allocation phenomenon: only a portion of prompt tokens convey essential information for response generation, and these tokens remain unchanged during generation. To validate the robustness of this finding, we design a thorough set of experiments across diverse prompts in terms of length, format, and content. From our observations, we derive an innovative and intuitive method,SnapKV, which can smartly identify the attention allocation pattern and compress the KV cache for long sequence prompts without compromising the model\u2019s accuracy. With its comprehensive design,SnapKVdemonstrates its effectiveness on various datasets and can be easily integrated into popular deep-learning frameworks with just a few code adjustments. Our contributions are as follows:\n* \u2022We design experiments to explore the attention allocation pattern during generation, focusing on two key questions:\n1. 1.\nIs there a consistent attention allocation pattern for input sequence tokens?\n2. 2.\nIs it feasible to identify this pattern prior to the generation stage?\nOur finding suggests that for LLMs, the attention allocation of most input sequence tokens stay consistent during generation. Thus,LLMs knows what you are looking for before generation.\n3. \u2022Inspired by our observations above, we develop an efficient and fine-tuning-free algorithm,SnapKV, which efficiently identifies critical attention features and compresses KV cache correspondingly with minimal model modification (See Fig.[1](https://arxiv.org/html/2404.14469v2#S1.F1)).\n4. \u2022We evaluateSnapKVacross diverse LLMs and long-sequence datasets.SnapKVshows comparable accuracy with full KV caching method while achieving improved decoding speed and memory efficiency. Meanwhile, we conduct the pressure test with Needle-in-a-Haystack to further demonstrate its memory efficiency and information retrieval ability.\n## 2Related Works\nMany previous works compress the KV cache by selectively dropping KVs using different algorithms. In StreamLLM> [\n[> 5\n](https://arxiv.org/html/2404.14469v2#bib.bib5)> ]\n, only the most recent tokens and attention sinks (first few tokens) are retained to reduce the KV cache size, making it lose the important information carried by the discarded middle tokens111[https://github.com/mit-han-lab/streaming-llm?tab=readme-ov-file#faq](https://github.com/mit-han-lab/streaming-llm?tab=readme-ov-file#faq). Heavy-Hitter Oracle (H2O)> [\n[> 6\n](https://arxiv.org/html/2404.14469v2#bib.bib6)> ]\nintroduces a policy that greedily drops KVs during generation based on a scoring function derived from cumulative attention. While this approach effectively compresses the KVs appended to the cache during generation, it overlooks compression of prompt KVs, which is crucial for reducing memory and computational overhead. Building on a similar concept, Adaptive KV Compression (FastGen)> [\n[> 8\n](https://arxiv.org/html/2404.14469v2#bib.bib8)> ]\nimplements a dual-phase algorithm that encompasses four KV cache compression policies. Initially, it identifies optimal policies through profiling results obtained from prompt encoding. Subsequently, it dynamically evicts caches during the generation phase based on these policies. Nonetheless, it faces the similar problem with H2O. ScissorHands> [\n[> 7\n](https://arxiv.org/html/2404.14469v2#bib.bib7)> ]\nfocuses on identifying and retaining pivotal tokens that exhibit a consistent attention weight pattern with previous token windows during generation steps. However, this method concentrates solely on the window of previous pivotal tokens in generation and neglects the extensive prompt that contains essential information for generating accurate responses. This oversight could lead to an inability to extract detailed information from prompts.\nIn summary, existing methods have not effectively addressed the challenges encountered in real-world applications, where prompts are exceptionally long yet require accurate information retrieval. Although these techniques may reduce the KV cache size during generation, they do not address the primary challenges of understanding complex prompt contexts, leaving critical issues unresolved.\n![Refer to caption](x2.png)Figure 2:The overlap rates between attention features of the input sequence, selected by various windows along the input and during generation, with each line representing a model layer.\n![Refer to caption](x3.png)Figure 3:The layer-wise overlap rates between input sequence attention features selected by the last window of input sequence and those selected by 4 windows along generation.\n## 3Observations\nIn this section, we present our observations regarding the attention allocation patterns in the Query-Key matrix during token generation. Our analysis utilizes samples from Ultrachat> [\n[> 11\n](https://arxiv.org/html/2404.14469v2#bib.bib11)> ]\n, a multi-turns, high-quality instruction dataset consisting of 1.4 million dialogues. We further filter the sequences with response length greater than 512 and prompt length greater than 3k. Our findings are concluded into two key observations as follows:\n* \u2022Pattern can be identified before generation.In this experiment, we spl",
          "original_query": "SnapKV: LLM knows what you are looking for before generation",
          "cleaned_query": "SnapKV: LLM knows what you are looking for before generation",
          "content_quality": "good_from_html"
        }
      ],
      "generated_ideas": [
        "Hybrid Prune-and-Cluster KV Compression\nCombine Dynamic Context Pruning\u2019s learnable token dropping with SnapKV\u2019s per-head clustering to first remove globally uninformative tokens, then cluster remaining KV positions per head. Evaluate whether the two-stage pipeline improves memory/latency at fixed accuracy versus either method alone, and identify when pruning harms head-specific feature coverage.",
        "Layer-Adaptive Context Pruning Schedules\nExtend Dynamic Context Pruning with a policy that assigns different sparsity targets per layer (e.g., aggressive early-layer pruning, conservative late-layer retention) based on measured attention entropy or gradient-based saliency. Implement as a lightweight controller trained during fine-tuning and benchmark on long-context QA and summarization to map \u201cwhere\u201d context is most disposable.",
        "Pre-Generation \u201cObservation Window\u201d Predictors for Pruning Decisions\nGeneralize SnapKV\u2019s finding (attention patterns predictable from an end-of-prompt observation window) to predict which tokens Dynamic Context Pruning will drop before decoding starts. Train a small predictor that outputs token keep/drop scores from the observation window features, enabling one-shot prompt compression and faster prompt prefill without autoregressive rollouts.",
        "Uncertainty-Calibrated Token Retention for Long-Context Reliability\nAdd an uncertainty estimator (e.g., variance across heads/layers or Monte-Carlo dropout) to gate pruning/compression: keep more context when the model is uncertain and prune harder when confident. This yields an actionable \u201cadaptive compute\u201d mechanism and can be evaluated on Needle-in-a-Haystack and adversarial distraction benchmarks to quantify robustness gains.",
        "Interpretability-Grounded Pruning Audits (\u201cWhy was this token kept?\u201d)\nLeverage Dynamic Context Pruning\u2019s interpretability angle to produce human-readable rationales and audits: for each retained token, attribute which future generations depended on it (e.g., via attention flow or counterfactual deletion tests). Build an evaluation suite that scores pruning explanations against synthetic ground-truth dependencies and real tasks requiring citation/attribution.",
        "Task-Conditional KV Compression via Prompt-Type Routing\nTrain a router that classifies prompt segments (dialogue history, instructions, retrieved docs, code, tables) and applies specialized pruning/compression rules per segment (e.g., preserve system/instruction tokens, cluster retrieved evidence). Implement segment-aware constraints within pruning (never-drop sets, quota per segment) and measure improvements in instruction-following and retrieval-augmented generation.",
        "Continual KV Cache Maintenance Across Multi-Turn Conversations\nDevelop a persistent KV cache manager for chat that periodically prunes/merges historical turns using Dynamic Context Pruning, while using SnapKV-style per-head important positions to preserve long-term user preferences and commitments. Evaluate over long multi-session dialogues with memory tests, comparing against sliding windows and naive summarization.",
        "Pruning-Friendly Positional Encoding and Re-Indexing\nInvestigate positional encoding schemes that remain stable under token deletion/reordering in the cache (a mismatch risk when context is dynamically removed). Propose a \u201cpruning-invariant\u201d position representation or re-indexing method that updates positions after deletions with minimal computation, and test whether it reduces accuracy regressions at high sparsity.",
        "Head Specialization Diagnostics for Compression Policy Learning\nBuild a diagnostic toolkit that quantifies each head\u2019s \u201cfeature role\u201d (e.g., syntax, coreference, retrieval matching) and then learns head-specific compression budgets (some heads get more KV slots than others). Use SnapKV\u2019s observation that heads consistently focus on specific prompt features, and validate whether head-budgeting improves efficiency without hurting specific capabilities (e.g., coreference)."
      ],
      "generated_ideas_raw": "1. **Hybrid Prune-and-Cluster KV Compression**\n   Combine Dynamic Context Pruning\u2019s learnable token dropping with SnapKV\u2019s per-head clustering to first remove globally uninformative tokens, then cluster remaining KV positions per head. Evaluate whether the two-stage pipeline improves memory/latency at fixed accuracy versus either method alone, and identify when pruning harms head-specific feature coverage.\n\n2. **Layer-Adaptive Context Pruning Schedules**\n   Extend Dynamic Context Pruning with a policy that assigns different sparsity targets per layer (e.g., aggressive early-layer pruning, conservative late-layer retention) based on measured attention entropy or gradient-based saliency. Implement as a lightweight controller trained during fine-tuning and benchmark on long-context QA and summarization to map \u201cwhere\u201d context is most disposable.\n\n3. **Pre-Generation \u201cObservation Window\u201d Predictors for Pruning Decisions**\n   Generalize SnapKV\u2019s finding (attention patterns predictable from an end-of-prompt observation window) to predict which tokens Dynamic Context Pruning will drop before decoding starts. Train a small predictor that outputs token keep/drop scores from the observation window features, enabling one-shot prompt compression and faster prompt prefill without autoregressive rollouts.\n\n4. **Uncertainty-Calibrated Token Retention for Long-Context Reliability**\n   Add an uncertainty estimator (e.g., variance across heads/layers or Monte-Carlo dropout) to gate pruning/compression: keep more context when the model is uncertain and prune harder when confident. This yields an actionable \u201cadaptive compute\u201d mechanism and can be evaluated on Needle-in-a-Haystack and adversarial distraction benchmarks to quantify robustness gains.\n\n5. **Interpretability-Grounded Pruning Audits (\u201cWhy was this token kept?\u201d)**\n   Leverage Dynamic Context Pruning\u2019s interpretability angle to produce human-readable rationales and audits: for each retained token, attribute which future generations depended on it (e.g., via attention flow or counterfactual deletion tests). Build an evaluation suite that scores pruning explanations against synthetic ground-truth dependencies and real tasks requiring citation/attribution.\n\n6. **Task-Conditional KV Compression via Prompt-Type Routing**\n   Train a router that classifies prompt segments (dialogue history, instructions, retrieved docs, code, tables) and applies specialized pruning/compression rules per segment (e.g., preserve system/instruction tokens, cluster retrieved evidence). Implement segment-aware constraints within pruning (never-drop sets, quota per segment) and measure improvements in instruction-following and retrieval-augmented generation.\n\n7. **Continual KV Cache Maintenance Across Multi-Turn Conversations**\n   Develop a persistent KV cache manager for chat that periodically prunes/merges historical turns using Dynamic Context Pruning, while using SnapKV-style per-head important positions to preserve long-term user preferences and commitments. Evaluate over long multi-session dialogues with memory tests, comparing against sliding windows and naive summarization.\n\n8. **Pruning-Friendly Positional Encoding and Re-Indexing**\n   Investigate positional encoding schemes that remain stable under token deletion/reordering in the cache (a mismatch risk when context is dynamically removed). Propose a \u201cpruning-invariant\u201d position representation or re-indexing method that updates positions after deletions with minimal computation, and test whether it reduces accuracy regressions at high sparsity.\n\n9. **Head Specialization Diagnostics for Compression Policy Learning**\n   Build a diagnostic toolkit that quantifies each head\u2019s \u201cfeature role\u201d (e.g., syntax, coreference, retrieval matching) and then learns head-specific compression budgets (some heads get more KV slots than others). Use SnapKV\u2019s observation that heads consistently focus on specific prompt features, and validate whether head-budgeting improves efficiency without hurting specific capabilities (e.g., coreference).\n\n10. **End-to-End Training of KV Cache Compression as a Differentiable Module**\n   Create a differentiable approximation to SnapKV\u2019s clustering/selection (e.g., soft k-medoids or Gumbel-top-k) and integrate it with Dynamic Context Pruning in training so the model co-adapts to compressed caches. Compare fine-tuning-free, fine-tuning-only, and co-trained variants on 16K\u2013128K contexts, reporting compute/quality trade-offs and generalization to unseen lengths.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hybrid Prune-and-Cluster KV Compression\nCombine Dynamic Context Pruning\u2019s learnable token dropping with SnapKV\u2019s per-head clustering to first remove globally uninformative tokens, then cluster remaini",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Layer-Adaptive Context Pruning Schedules\nExtend Dynamic Context Pruning with a policy that assigns different sparsity targets per layer (e.g., aggressive early-layer pruning, conservative late-layer r",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Pre-Generation \u201cObservation Window\u201d Predictors for Pruning Decisions\nGeneralize SnapKV\u2019s finding (attention patterns predictable from an end-of-prompt observation window) to predict which tokens Dynam",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Uncertainty-Calibrated Token Retention for Long-Context Reliability\nAdd an uncertainty estimator (e.g., variance across heads/layers or Monte-Carlo dropout) to gate pruning/compression: keep more cont",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Interpretability-Grounded Pruning Audits (\u201cWhy was this token kept?\u201d)\nLeverage Dynamic Context Pruning\u2019s interpretability angle to produce human-readable rationales and audits: for each retained token",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Task-Conditional KV Compression via Prompt-Type Routing\nTrain a router that classifies prompt segments (dialogue history, instructions, retrieved docs, code, tables) and applies specialized pruning/co",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Continual KV Cache Maintenance Across Multi-Turn Conversations\nDevelop a persistent KV cache manager for chat that periodically prunes/merges historical turns using Dynamic Context Pruning, while usin",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Pruning-Friendly Positional Encoding and Re-Indexing\nInvestigate positional encoding schemes that remain stable under token deletion/reordering in the cache (a mismatch risk when context is dynamicall",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Head Specialization Diagnostics for Compression Policy Learning\nBuild a diagnostic toolkit that quantifies each head\u2019s \u201cfeature role\u201d (e.g., syntax, coreference, retrieval matching) and then learns he",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 64,
      "paper_title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
      "contribution": "HyperET introduces a paradigm for effectively training multi-modal large language models in hyperbolic space to align visual and textual representations across varying levels of granularity with improved efficiency.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 1,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9084,
      "output_tokens": 1208,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Learning Transferable Visual Models From Natural Language ...",
          "url": "https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf",
          "content": "Learning Transferable Visual Models From Natural Language Supervision\nAlec Radford * 1 Jong Wook Kim * 1 Chris Hallacy 1 Aditya Ramesh 1 Gabriel Goh 1 Sandhini Agarwal 1\nGirish Sastry 1 Amanda Askell 1 Pamela Mishkin 1 Jack Clark 1 Gretchen Krueger 1Ilya Sutskever 1\nAbstract\nState-of-the-art computer vision systems are\ntrained to predict a fixed set of predetermined\nobject categories. This restricted form of super\u0002vision limits their generality and usability since\nadditional labeled data is needed to specify any\nother visual concept. Learning directly from raw\ntext about images is a promising alternative which\nleverages a much broader source of supervision.\nWe demonstrate that the simple pre-training task\nof predicting which caption goes with which im\u0002age is an efficient and scalable way to learn SOTA\nimage representations from scratch on a dataset\nof 400 million (image, text) pairs collected from\nthe internet. After pre-training, natural language\nis used to reference learned visual concepts (or\ndescribe new ones) enabling zero-shot transfer\nof the model to downstream tasks. We study\nthe performance of this approach by benchmark\u0002ing on over 30 different existing computer vi\u0002sion datasets, spanning tasks such as OCR, action\nrecognition in videos, geo-localization, and many\ntypes of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is\noften competitive with a fully supervised baseline\nwithout the need for any dataset specific training.\nFor instance, we match the accuracy of the orig\u0002inal ResNet-50 on ImageNet zero-shot without\nneeding to use any of the 1.28 million training\nexamples it was trained on.\n1. Introduction and Motivating Work\nPre-training methods which learn directly from raw text\nhave revolutionized NLP over the last few years (Dai &\nLe, 2015; Peters et al., 2018; Howard & Ruder, 2018; Rad\u0002ford et al., 2018; Devlin et al., 2018; Raffel et al., 2019).\nTask-agnostic objectives such as autoregressive and masked\nlanguage modeling have scaled across many orders of mag-\n*Equal contribution 1OpenAI, San Francisco, CA 94110, USA.\nCorrespondence to: <{alec, jongwook}@openai.com>.\nnitude in compute, model capacity, and data, steadily im\u0002proving capabilities. The development of \u201ctext-to-text\u201d as\na standardized input-output interface (McCann et al., 2018;\nRadford et al., 2019; Raffel et al., 2019) has enabled task\u0002agnostic architectures to zero-shot transfer to downstream\ndatasets removing the need for specialized output heads or\ndataset specific customization. Flagship systems like GPT-3\n(Brown et al., 2020) are now competitive across many tasks\nwith bespoke models while requiring little to no dataset\nspecific training data.\nThese results suggest that the aggregate supervision acces\u0002sible to modern pre-training methods within web-scale col\u0002lections of text surpasses that of high-quality crowd-labeled\nNLP datasets. However, in other fields such as computer\nvision it is still standard practice to pre-train models on\ncrowd-labeled datasets such as ImageNet (Deng et al., 2009).\nCould scalable pre-training methods which learn directly\nfrom web text result in a similar breakthrough in computer\nvision? Prior work is encouraging.\nOver 20 years ago Mori et al. (1999) explored improving\ncontent based image retrieval by training a model to pre\u0002dict the nouns and adjectives in text documents paired with\nimages. Quattoni et al. (2007) demonstrated it was possi\u0002ble to learn more data efficient image representations via\nmanifold learning in the weight space of classifiers trained\nto predict words in captions associated with images. Sri\u0002vastava & Salakhutdinov (2012) explored deep represen\u0002tation learning by training multimodal Deep Boltzmann\nMachines on top of low-level image and text tag feature\nfeatures. Joulin et al. (2016) modernized this line of work\nand demonstrated that CNNs trained to predict words in\nimage captions learn useful image representations. They\nconverted the title, description, and hashtag metadata of im\u0002ages in the YFCC100M dataset (Thomee et al., 2016) into\na bag-of-words multi-label classification task and showed\nthat pre-training AlexNet (Krizhevsky et al., 2012) to pre\u0002dict these labels learned representations which preformed\nsimilarly to ImageNet-based pre-training on transfer tasks.\nLi et al. (2017) then extended this approach to predicting\nphrase n-grams in addition to individual words and demon\u0002strated the ability of their system to zero-shot transfer to\nother image classification datasets by scoring target classes\nbased on their dictionary of learned visual n-grams and\nLearning Transferable Visual Models From Natural Language Supervision 2\nI1\u00b7T2I1\u00b7T3 \u2026\nI2\u00b7T1I2\u00b7T3 \u2026\nI3\u00b7T1I3\u00b7T2 \u2026\n\u22ee \u22ee \u22ee\nI1\u00b7T1\nI2\u00b7T2\nI3\u00b7T3\n(1) Contrastive pre-training\nImage\nEncoder\nText\nEncoder Pepper the\naussie pup\nT1 T2 T3 \u2026\nI1\nI2\nI3\n\u22ee\n(2) Create dataset classifier from label text\nplane\ncar\ndog\n\u22ee\nbird\nA photo of\na {object}.\n\u22ee\nText\nEncoder\nT1 T2 T3 TN\n\u2026\n(3) Use for zero-shot prediction\nImage\nEncoder\nI1I1\u00b7T2I1I1\u00b7T1\u00b7TN\n\u2026\n\u2026\nA photo of\n a dog.\nTN\nIN\u00b7T1IN\u00b7T2IN\u00b7T3\nI1\u00b7TN\nI2\u00b7TN\nI3\u00b7TN\n\u22ee\nIN \u2026\n\u2026\n\u22ee \u22f1\nIN\u00b7TN\nI1\u00b7T3\nFigure 1. Summary of our approach. While standard image models jointly train an image feature extractor and a linear classifier to predict\nsome label, CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training\nexamples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the\ntarget dataset\u2019s classes.\npredicting the one with the highest score. Adopting more\nrecent architectures and pre-training approaches, VirTex\n(Desai & Johnson, 2020), ICMLM (Bulent Sariyildiz et al.,\n2020), and ConVIRT (Zhang et al., 2020) have recently\ndemonstrated the potential of transformer-based language\nmodeling, masked language modeling, and contrastive ob\u0002jectives to learn image representations from text.\nWhile exciting as proofs of concept, using natural language\nsupervision for image representation learning is still rare.\nThis is likely because demonstrated performance on com\u0002mon benchmarks is much lower than alternative approaches.\nFor example, Li et al. (2017) reach only 11.5% accuracy\non ImageNet in a zero-shot setting. This is well below the\n88.4% accuracy of the current state of the art (Xie et al.,\n2020). It is even below the 50% accuracy of classic com\u0002puter vision approaches (Deng et al., 2012). Instead, more\nnarrowly scoped but well-targeted uses of weak supervision\nhave improved performance. Mahajan et al. (2018) showed\nthat predicting ImageNet related hashtags on Instagram im\u0002ages is an effective pre-training task. When fine-tuned to\nImageNet these pre-trained models increased accuracy by\nover 5% and improved the overall state of the art at the time.\nKolesnikov et al. (2019) and Dosovitskiy et al. (2020) have\nalso demonstrated large gains on a broader set of transfer\nbenchmarks by pre-training models to predict the classes of\nthe noisily labeled JFT-300M dataset.\nThis line of work represents the current pragmatic middle\nground between learning from a limited amount of super\u0002vised \u201cgold-labels\u201d and learning from practically unlimited\namounts of raw text. However, it is not without compro\u0002mises. Both works carefully design, and in the process limit,\ntheir supervision to 1000 and 18291 classes respectively.\nNatural language is able to express, and therefore supervise,\na much wider set of visual concepts through its general\u0002ity. Both approaches also use static softmax classifiers to\nperform prediction and lack a mechanism for dynamic out\u0002puts. This severely curtails their flexibility and limits their\n\u201czero-shot\u201d capabilities.\nA crucial difference between these weakly supervised mod\u0002els and recent explorations of learning image representations\ndirectly from natural language is scale. While Mahajan et al.\n(2018) and Kolesnikov et al. (2019) trained their models for\naccelerator years on millions to billions of images, VirTex,\nICMLM, and ConVIRT trained for accelerator days on one\nto two hundred thousand images. In this work, we close\nthis gap and study the behaviors of image classifiers trained\nwith natural language supervision at large scale. Enabled\nby the large amounts of publicly available data of this form\non the internet, we create a new dataset of 400 million (im\u0002age, text) pairs and demonstrate that a simplified version of\nConVIRT trained from scratch, which we call CLIP, for Con\u0002trastive Language-Image Pre-training, is an efficient method\nof learning from natural language supervision. We study\nthe scalability of CLIP by training a series of eight models\nspanning almost 2 orders of magnitude of compute and ob\u0002serve that transfer performance is a smoothly predictable\nfunction of compute (Hestness et al., 2017; Kaplan et al.,\n2020). We find that CLIP, similar to the GPT family, learns\nto perform a wide set of tasks during pre-training including\nOCR, geo-localization, action recognition, and many others.\nWe measure this by benchmarking the zero-shot transfer\nperformance of CLIP on over 30 existing datasets and find\nit can be competitive with prior task-specific supervised\nmodels. We also confirm these findings with linear-probe\nLearning Transferable Visual Models From Natural Language Supervision 3\nrepresentation learning analysis and show that CLIP out\u0002performs the best publicly available ImageNet model while\nalso being more computationally efficient. We additionally\nfind that zero-shot CLIP models are much more robust than\nequivalent accuracy supervised ImageNet models which\nsuggests that zero-shot evaluation of task-agnostic models is\nmuch more representative of a model\u2019s capability. These re\u0002sults have significant policy and ethical implications, which\nwe consider in Section 7.\n2M 33M 67M 134M 268M 400M\n# of images processed\n0\n5\n10\n15\n20\n25\n30\n35\n40\nZero-Shot ImageNet Accuracy\n4X efficiency 3X efficiency\nBag of Words Contrastive (CLIP)\nBag of Words Prediction\nTransformer Lan",
          "original_query": "Learning transferable visual models from natural language supervision",
          "cleaned_query": "Learning transferable visual models from natural language supervision",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "SAM 3: Segment Anything with Concepts | Research",
          "url": "https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/",
          "content": "SAM 3: Segment Anything with Concepts | Research - AI at Meta\n[![Meta](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/252294889_575082167077436_6034106545912333281_n.svg/meta-logo-primary_standardsize.svg?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=Ir6GHtYUwz8Q7kNvwGhtYTi&amp;_nc_oc=AdnY7qEwhcR8LFKkhojkDa_tJbHcl-O_55ylVFj2uSkFe_5Zidge6NxWjalEPA0ljUDjWtCCTCunLDN9_JgFALVq&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=cEB1EhnYUB9lQdB7CRn0-A&amp;oh=00_AfnHaG_HDshTqdsIUcXxjLAMtsJT-uzJgqlzT_l5Rkp3GQ&amp;oe=695AD239)](#)\n* [Meta AI](#)\n* [AI Research](#)\n* [The Latest](https://ai.meta.com/blog/)\n* [About](#)\n* [Get Llama](https://www.llama.com/?utm_source=ai_meta_site&amp;utm_medium=web&amp;utm_content=AI_nav&amp;utm_campaign=09252025_moment)\n* [Try Meta AI](https://www.meta.ai/?utm_source=ai_meta_site&amp;utm_medium=web&amp;utm_content=AI_nav&amp;utm_campaign=09252025_moment)\n* [](https://ai.meta.com/)\n#### RESEARCH\n#### COMPUTER VISION\n# SAM 3: Segment Anything with Concepts\nNovember 19, 2025\n## Abstract\nWe present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks\nobjects in images and videos based on concept prompts, which we define as either short noun\nphrases (e.g., \u201cyellow school bus\u201d), image exemplars, or a combination of both. Promptable Concept\nSegmentation (PCS) takes such prompts and returns segmentation masks and unique identities for\nall matching object instances. To advance PCS, we build a scalable data engine that produces a\nhigh-quality dataset with 4M unique concept labels, including hard negatives, across images and\nvideos. Our model consists of an image-level detector and a memory-based video tracker that share\na single backbone. Recognition and localization are decoupled with a presence head, which boosts\ndetection accuracy. SAM 3 delivers a 2\u00d7 gain over existing systems in both image and video PCS, and\nimproves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with\nour new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.\n[\nDownload the Paper\n](https://scontent-sea1-1.xx.fbcdn.net/v/t39.2365-6/585895112_1502482260871702_2839727966936571770_n.pdf?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=mSNrP7iYMZgQ7kNvwG1-HYt&amp;_nc_oc=AdmG-e9AMH7UwgUe85n-OiT7yVHxFaavgYCAuvZBu_cYivBwT1B0AyjSDjyMetmd22VuaD9Xi4t1B0Wgrh8vY5ER&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=cEB1EhnYUB9lQdB7CRn0-A&amp;oh=00_AfkvGdYgaObDCVUDyDl54L69mESDSlzNHbJqDsdeZ3EcKQ&amp;oe=695B01ED)\n#### AUTHORS\nWritten by\nNicolas Carion\nLaura Gustafson\nYuan-Ting Hu\nShoubhik Debnath\nRonghang Hu\nDidac Suris Coll-Vinent\nChaitanya Ryali\nKalyan Vasudev Alwala\nHaitham Khedr\nAndrew Huang\nJie Lei\nTengyu Ma\nBaishan Guo\nArpit Kalla\nMarkus Marks\nJoseph Greer\nMeng Wang\nPeize Sun\nRoman R\u00e4dle\nTriantafyllos Afouras\nEffrosyni Mavroudi\nKatherine Xu\nTsung-Han Wu\nYu Zhou\nLiliane Momeni\nRishi Hazra\nShuangrui Ding\nSagar Vaze\nFrancois Porcher\nFeng Li\nSiyuan Li\nAishwarya Kamath\nHo Kei Cheng\n[Piotr Dollar](https://ai.meta.com/people/1101307851116193/piotr-dollar/)\n[Nikhila Ravi](https://ai.meta.com/people/376342865239921/nikhila-ravi/)\nKate Saenko\nPengchuan Zhang\n[Christoph Feichtenhofer](https://ai.meta.com/people/417502060773583/christoph-feichtenhofer/)\nPublisher\narxiv\nResearch Topics\n[Computer Vision](https://ai.meta.com/research/computer-vision/)\n### Related Publications\nDecember 18, 2025\n#### COMPUTER VISION\n#### We Can Hide More Bits: The Unused Watermarking Capacity in Theory and Practice\nAleksandar Petrov,Pierre Fernandez,Tom\u00e1\u0161 Sou\u010dek,Hady Elsahar\nDecember 18, 2025\n[Read the Paper**](https://ai.meta.com/research/publications/we-can-hide-more-bits-the-unused-watermarking-capacity-in-theory-and-practice/)\nDecember 18, 2025\n#### COMPUTER VISION\n#### Learning to Watermark in the Latent Space of Generative Models\nSylvestre Rebuffi,Tuan Tran,Valeriu Lacatusu,Pierre Fernandez,Tom\u00e1\u0161 Sou\u010dek,Tom Sander,Hady Elsahar,Alexandre Mourachko\nDecember 18, 2025\n[Read the Paper**](https://ai.meta.com/research/publications/learning-to-watermark-in-the-latent-space-of-generative-models/)\nDecember 18, 2025\n#### RESEARCH\n#### COMPUTER VISION\n#### Pixel Seal: Adversarial-only training for invisible image and video watermarking\nTom\u00e1\u0161 Sou\u010dek,Pierre Fernandez,Hady Elsahar,Sylvestre Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko\nDecember 18, 2025\n[Read the Paper**](https://ai.meta.com/research/publications/pixel-seal-adversarial-only-training-for-invisible-image-and-video-watermarking/)\nDecember 16, 2025\n#### SPEECH &amp; AUDIO\n#### COMPUTER VISION\n#### SAM Audio: Segment Anything in Audio\nBowen Shi,Andros Tjandra,John Hoffman,Helin Wang,Yi-Chiao Wu,Luya Gao,Julius Richter,[Matt Le](https://ai.meta.com/people/1852792045150192/matt-le/),Apoorv Vyas,Sanyuan Chen,[Christoph Feichtenhofer](https://ai.meta.com/people/417502060773583/christoph-feichtenhofer/),[Piotr Dollar](https://ai.meta.com/people/1101307851116193/piotr-dollar/),[Wei-Ning Hsu](https://ai.meta.com/people/1121169632408298/wei-ning-hsu/),[Ann Lee](https://ai.meta.com/people/436943822139907/ann-lee/)\nDecember 16, 2025\n[Read the Paper**](https://ai.meta.com/research/publications/sam-audio-segment-anything-in-audio/)\n[\nSee All Papers\n](https://ai.meta.com/global_search/?content_types[0]=publication&amp;page=1)\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.2365-6/91627620_211684986754811_4465357771142856704_n.jpg?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=1ADF0f3mTnMQ7kNvwF2nI2q&amp;_nc_oc=AdnpenpA-6PuLCi5QPM_Cp0GLm9lsRgktxKqCGDKHX8IHsMUg4WP8gzk3lw5nln8wIZkr1BiQay71ynA8lFIyLXm&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=cEB1EhnYUB9lQdB7CRn0-A&amp;oh=00_AfkCgeB0EiBnqbjgAD45XZY2GdrQmbyRAE6ScAbFUyrIqw&amp;oe=696F5E1A)\n## Help Us Pioneer The Future of AI\n##### We share our open source frameworks, tools, libraries, and models for everything from research exploration to large-scale production deployment.\n[\nJoin our Team\n](https://ai.meta.com/join-us/)\n[Our approach](https://ai.meta.com/about)****\n[About AI at Meta](https://ai.meta.com/about)\n[People](https://ai.meta.com/results/?content_types[0]=person&amp;sort_by=random)\n[Careers]()\n[Research](https://ai.meta.com/research)****\n[Infrastructure](https://ai.meta.com/infrastructure)\n[Resources](https://ai.meta.com/resources)\n[Demos](https://aidemos.meta.com/)\n[Meta AI](https://ai.meta.com/meta-ai/)****\n[Explore Meta AI](https://ai.meta.com/meta-ai/)\n[Get Meta AI](https://ai.meta.com/get-meta-ai/)\n[AI Studio](https://ai.meta.com/ai-studio/)\n[Latest news](https://ai.meta.com/blog)****\n[Blog](https://ai.meta.com/blog)\n[Newsletter](https://ai.meta.com/subscribe)\nFoundational models\n****\n[Llama](https://www.llama.com/)\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.2365-6/87524316_2677189655726266_6338721200264445952_n.svg?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=JnL1VKbqlpAQ7kNvwGDPbxW&amp;_nc_oc=Adl9iMjsQPBiziCy3e6NarpcFRbM15ccsKUHUml6mOf2FCodJ2HEGl0r_SRe7mQkobw2dhnRHbEBXNpjxUXfZvIk&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=cEB1EhnYUB9lQdB7CRn0-A&amp;oh=00_Aflz3h8Qe8dYRKus0Zg8yRyIoeYxZ6igjiqe5ZN0BM_Q7w&amp;oe=696F7438)\n[\n![](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=FLms6UMPrsUQ7kNvwFczscV&amp;_nc_oc=AdnAs6EsKD9Vbpda3ylhRdtbK_tVkRMxfdyCsOZDH4x3KBtlJFJx45thla9442VuJhz_yhjWtUJlyEgGLFtPImr9&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=cEB1EhnYUB9lQdB7CRn0-A&amp;oh=00_AfkXv60xgb46-hD6ZkgHbIoldZ6t8dHzqomBBCXOdcblOw&amp;oe=695AFAA7)\n![](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=FLms6UMPrsUQ7kNvwFczscV&amp;_nc_oc=AdnAs6EsKD9Vbpda3ylhRdtbK_tVkRMxfdyCsOZDH4x3KBtlJFJx45thla9442VuJhz_yhjWtUJlyEgGLFtPImr9&amp;_nc_zt=14&amp;_nc_ht=scontent-sea1-1.xx&amp;_nc_gid=cEB1EhnYUB9lQdB7CRn0-A&amp;oh=00_AfkXv60xgb46-hD6ZkgHbIoldZ6t8dHzqomBBCXOdcblOw&amp;oe=695AFAA7)\n](https://www.facebook.com/aiatmeta/)\n[\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=ExpFeovaFvAQ7kNvwGrZj3a&amp;_nc_oc=AdnhWM-D8gIHsjGW6dez9puD5AwSlgHsbHH5CibvokCfqtYQq86TvjLEsbMr7mVQyeHkdEIS4gUeHEdJqyQ3ZCOf&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=cEB1EhnYUB9lQdB7CRn0-A&amp;oh=00_AflcQuPmsrF8wd4OKJj4muSfcrND-Y0oH_UM7rm8oim00w&amp;oe=695AF2E2)\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=ExpFeovaFvAQ7kNvwGrZj3a&amp;_nc_oc=AdnhWM-D8gIHsjGW6dez9puD5AwSlgHsbHH5CibvokCfqtYQq86TvjLEsbMr7mVQyeHkdEIS4gUeHEdJqyQ3ZCOf&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=cEB1EhnYUB9lQdB7CRn0-A&amp;oh=00_AflcQuPmsrF8wd4OKJj4muSfcrND-Y0oH_UM7rm8oim00w&amp;oe=695AF2E2)\n](https://twitter.com/aiatmeta/)\n[\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=ctoWCpmjocwQ7kNvwEefab8&amp;_nc_oc=Adk8rc6d96O1QRoc7xy4J7PA3hv5_xLzZ4RFKH4Vv_d5-0yWf-Ub3qtWsT-Hg6brwnWYovMBSKaykY0LNS4wXfSn&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=cEB1EhnYUB9lQdB7CRn0-A&amp;oh=00_AfnD1plf8afiWN2RGU34ZsLwiuM9yOrqH1282xzhlYuebg&amp;oe=695AE67B)\n![](https://scontent-sea5-1.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=ctoWCpmjocwQ7kNvwEefab8&amp;_nc_oc=Adk8rc6d96O1QRoc7xy4J7PA3hv5_xLzZ4RFKH4Vv_d5-0yWf-Ub3qtWsT-Hg6brwnWYovMBSKaykY0LNS4wXfSn&amp;_nc_zt=14&amp;_nc_ht=scontent-sea5-1.xx&amp;_nc_gid=cEB1EhnYUB9lQdB7CRn0-A&amp;oh=00_AfnD1plf8afiWN2RGU34ZsLwiuM9yOrqH1282xzhlYuebg&amp;oe=695AE67B)\n](https://www.linkedin.com/showcase/aiatmeta)\n[\n![](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/3",
          "original_query": "Segment anything",
          "cleaned_query": "Segment anything",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Hyperbolic Contrastive Learning for Visual Representations ...",
          "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Hyperbolic_Contrastive_Learning_for_Visual_Representations_Beyond_Objects_CVPR_2023_paper.pdf",
          "content": "Hyperbolic Contrastive Learning for Visual Representations beyond Objects\nSongwei Ge\u21e41, Shlok Mishra\u21e41, Simon Kornblith2,\nChun-Liang Li2, David Jacobs1,3\n1University of Maryland, College Park, 2Google Research, 3Meta\n{songweig,shlokm,dwj}@umd.edu, {chunliang,skornblith}@google.com\nAbstract\nAlthough self-/un-supervised methods have led to rapid\nprogress in visual representation learning, these methods\ngenerally treat objects and scenes using the same lens. In this\npaper, we focus on learning representations for objects and\nscenes that preserve the structure among them. Motivated by\nthe observation that visually similar objects are close in the\nrepresentation space, we argue that the scenes and objects\nshould instead follow a hierarchical structure based on their\ncompositionality. To exploit such a structure, we propose a\ncontrastive learning framework where a Euclidean loss is\nused to learn object representations and a hyperbolic loss is\nused to encourage representations of scenes to lie close to\nrepresentations of their constituent objects in a hyperbolic\nspace. This novel hyperbolic objective encourages the scene\u0002object hypernymy among the representations by optimizing\nthe magnitude of their norms. We show that when pretrain\u0002ing on the COCO and OpenImages datasets, the hyperbolic\nloss improves downstream performance of several baselines\nacross multiple datasets and tasks, including image classifi\u0002cation, object detection, and semantic segmentation. We also\nshow that the properties of the learned representations allow\nus to solve various vision tasks that involve the interaction\nbetween scenes and objects in a zero-shot fashion.\n1. Introduction\nOur visual world is diverse and structured. Imagine taking\na close-up of a box of cereal in the morning. If we zoom out\nslightly, we may see different nearby objects such as a pitcher\nof milk, a cup of hot coffee, today\u2019s newspaper, or reading\nglasses. Zooming out further, we will probably recognize\nthat these items are placed on a dining table with the kitchen\nas background rather than inside a bathroom. Such scene\u0002object structure is diverse, yet not completely random. In\nthis paper, we aim at learning visual representations of both\nthe cereal box (objects) and the entire dining table (scenes) in\n\u21e4Equal Contribution. The order is decided randomly.\n\nFigure 1. Illustration of the representation space learned by our\nmodels. Object images of the same class tend to gather near the\ncenter around similar directions, while the scene images are far\naway in these directions with larger norms.\nthe same space while preserving such hierarchical structures.\nUn-/self-supervised learning has become a standard\nmethod to learn visual representations [7, 12, 24, 26, 27, 51].\nAlthough these methods attain superior performance over\nsupervised pretraining on object-centric datasets such as Im\u0002ageNet [6], inferior results are observed on images depicting\nmultiple objects such as OpenImages or COCO [68]. Several\nmethods have been proposed to mitigate this issue, but all fo\u0002cus either on learning improved object representations [1,68]\nor dense pixel representations [39, 64, 69], instead of explic\u0002itly modeling representations for scene images. The object\nrepresentations learned by these methods present a natural\ntopology [67]. That is, the objects from visually similar\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore. 6840\nclasses lie close to each other in the representation space.\nHowever, it is not clear how the representations of scene\nimages should fit into that topology. Directly applying exist\u0002ing contrastive learning results in a sub-optimal topology of\nscenes and objects as well as unsatisfactory performance, as\nwe will show in the experiments. To this end, we argue that\na hierarchical structure can be naturally adopted. Consider\u0002ing that the same class of objects can be placed in different\nscenes, we construct a hierarchical structure to describe such\nrelationships, where the root nodes are the visually similar\nobjects, and the scene images consisting of them are placed\nas the descendants. We call this structure the object-centric\nscene hierarchy.\nThe intermediate modeling difficulty induced by this\nstructure is the combinatorial explosion. A finite number of\nobjects leads to exponentially many different possible scenes.\nConsequently, Euclidean space may require an arbitrarily\nlarge number of dimensions to faithfully embed these scenes,\nwhereas it is known that any infinite trees can be embedded\nwithout distortion in a 2D hyperbolic space [25]. Therefore,\nwe propose to employ a hyperbolic objective to regularize\nthe scene representations. To learn representations of scenes,\nin the general setting of contrastive learning, we sample co\u0002occurring scene-object pairs as positive pairs, and objects\nthat are not part of that scene as negative samples, and use\nthese pairs to compute an auxiliary hyperbolic contrastive\nobjective. Our model is trained to reduce the distance be\u0002tween positive pairs and push away the negative pairs in a\nhyperbolic space.\nContrastive learning usually has objectives defined on a\nhypersphere [12, 27]. By discarding the norm information,\nthese models circumvent the shortcut of minimizing losses\nthrough tuning the norms and obtain better downstream per\u0002formance. However, the norm of the representation can also\nbe used to encode useful representational structure. In hy\u0002perbolic space, the magnitude of a vector often plays the\nrole of modeling the hypernymy of the hierarchical struc\u0002ture [45, 53, 59]. When projecting the representations to\nthe hyperbolic space, the norm information is preserved and\nused to determine the Riemannian distance, which eventually\naffects the loss. Since hyperbolic space is diffeomorphic and\nconformal to Euclidean space, our hyperbolic contrastive\nloss is differentiable and complementary to the original con\u0002trastive objective.\nWhen training simultaneously with the original con\u0002trastive objective for objects and our proposed hyperbolic\ncontrastive objective for scenes, the resulting representation\nspace exhibits a desired hierarchical structure while leaving\nthe object clustering topology intact as shown in Figure 1.\nWe demonstrate the effectiveness of the hyperbolic objective\nunder several frameworks on multiple downstream tasks. We\nalso show that the properties of the representations allow us\nto perform various vision tasks in a zero-shot way, from label\nuncertainty quantification to out-of-context object detection.\nOur contributions are summarized below:\n1. We propose a hyperbolic contrastive loss that regular\u0002izes scene representations so that they follow an object\u0002centric hierarchy, with positive and negative pairs sam\u0002pled from the hierarchy.\n2. We demonstrate that our learned representations trans\u0002fer better than representations learned using vanilla\ncontrastive loss on a variety of downstream tasks, in\u0002cluding object detection, semantic segmentation, and\nlinear classification.\n3. We show that the magnitude of representation norms\neffectively reflect the scene-objective hypernymy.\n2. Method\nIn this section, we elaborate upon our approach to learn\u0002ing visual representations of object and scene images. We\nstart by describing the hierarchical structure between objects\nand scenes that we wish to enforce in the learned representa\u0002tion space.\n2.1. Object-Centric Scene Hierarchy\nFrom simple object co-occurrence statistics [19, 41] to\nfiner object relationships [30,32], using hierarchical relation\u0002ships between objects and scenes to understand images is\nnot new. Previous studies primarily work on an image-level\nhierarchy by dividing an image into its lower-level elements\nrecursively: a scene contains multiple objects, an object has\ndifferent parts, and each part may consist of even lower-level\nfeatures [14, 29, 48]. While this is intuitive, it describes\na hierarchical structure contained in the individual images.\nInstead, we study the structure presented among different\nimages. Our goal is to learn a representation space for im\u0002ages of both objects and scenes across the entire dataset.\nTo this end, we argue that it is more natural to consider an\nobject-centric hierarchy.\nIt is known that when training an image classifier, the\nobjects from visually similar classes often lie close to each\nother in the representation space [67], which has become\nthe cornerstone of contrastive learning. Motivated by this\nobservation, we believe that the representation of each scene\nimage should also be close to the object clusters it consists\nof. However, modeling scenes requires a much larger vol\u0002ume due to the exponential number of possible compositions\nof objects. Another way to think about the object-centric\nhierarchy is through the generality and specificity as often\ndiscussed in the language literature [42, 45]. An object con\u0002cept is general when standing alone in the visual world, and\nit will become specific when a certain context is given. For\nexample, \u201ca desk\u201d is thought to be a more general concept\nthan \u201ca desk in a classroom with a boy sitting on it\u201d.\n6841\nTherefore, we propose to study an object-centric hierar\u0002chy across the entire dataset. Formally, given a set of images\nS = {s1, s2, \u00b7\u00b7\u00b7 , sn}, Oi = {o1\ni , o2i , \u00b7\u00b7\u00b7 , oni\ni } are the ob\u0002ject bounding boxes contained in the image si. We define\nthe regions of scene Ri = {r1\ni , r2i , \u00b7\u00b7\u00b7 , rmi\ni } to be partial\nareas of the image si that contain multiple objects such that\nr\nj\ni = [kok\ni , where ok\ni 2 Oi and object k is in the region j.\nWe define the object-centric hierarchy T = (V,E) to be\nthat V = S [ O [ R, where R = R1 [ \u00b7\u00b7\u00b7 [ Rn and\nO = O1 [ \u00b7\u00b7\u00b7 [ On. For u, v 2 V , e = (u, v) is an edge of\nT if u \u2713 v or v \u2713 u. Note that the natural scene images S\nare always put as the leaf nodes.\n2.2. Representation Learning beyond O",
          "original_query": "Hyperbolic contrastive learning for visual representations beyond objects",
          "cleaned_query": "Hyperbolic contrastive learning for visual representations beyond objects",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Hyperbolic trigonometry and its application in the Poincar\u00e9 ball ...",
          "url": "https://www.academia.edu/53029883/Hyperbolic_trigonometry_and_its_application_in_the_Poincar%C3%A9_ball_model_of_hyperbolic_geometry",
          "content": "(PDF) Hyperbolic trigonometry and its application in the Poincar\u00e9 ball model of hyperbolic geometry\nAcademia.edu no longer supports Internet Explorer.\nTo browse Academia.edu and the wider internet faster and more securely, please take a few seconds to[upgrade your browser](https://www.academia.edu/upgrade-browser).\n[![Academia.edu](https://a.academia-assets.com/images/academia-logo-redesign-2015-A.svg)![Academia.edu](https://a.academia-assets.com/images/academia-logo-redesign-2015.svg)](https://www.academia.edu/)\n* [Log In](https://www.academia.edu/login)\n* [Sign Up](https://www.academia.edu/signup)\n* more\n* * [About](https://www.academia.edu/about)\n* [Press](https://www.academia.edu/press)\n* [Papers](https://www.academia.edu/documents)\n* [Terms](https://www.academia.edu/terms)\n* [Privacy](https://www.academia.edu/privacy)\n* [Copyright](https://www.academia.edu/copyright)\n* [We&#39;re Hiring!](https://www.academia.edu/hiring)\n* [Help Center](https://support.academia.edu/hc/en-us)\n* less\nOutline\nkeyboard\\_arrow\\_down\n[Title](#title)\n[Abstract](#abstract)\n[Key Takeaways](#key-takeaways)\n[Introduction](#outer_page_1)\n[References](#references)\n[FAQs](#faq)\n![First page of \u201cHyperbolic trigonometry and its application in the Poincar\u00e9 ball model of hyperbolic geometry\u201d](https://0.academia-photos.com/attachment_thumbnails/70005914/mini_magick20210920-19594-16q6pvs.png?1632164953)![PDF Icon](https://a.academia-assets.com/images/single_work_splash/adobe_icon.svg)\ndownload\nDownload Free PDF\nDownload Free PDF\n# Hyperbolic trigonometry and its application in the Poincar\u00e9 ball model of hyperbolic geometry\n[![Profile image of Abraham Ungar](https://0.academia-photos.com/104358935/41535423/33631079/s65_abraham.ungar.jpg)Abraham Ungar](https://independent.academia.edu/UngarAbraham)\n2001, Computers &amp; Mathematics with Applications\n[https://doi.org/10.1016/S0898-1221(01)85012-4](https://doi.org/10.1016/S0898-1221(01)85012-4)\nvisibility\n\u2026description\n13 pages\ndescriptionSee full PDFdownloadDownload PDF\nbookmarkSave to LibraryshareShare\nclose\n![](https://a.academia-assets.com/images/academia-logo-capital-white.svg)#### Sign up for access to the world's latest research\nSign up for freearrow\\_forward\ncheckGet notified about relevant papers\ncheckSave papers to use in your research\ncheckJoin the discussion with peers\ncheckTrack your impact\n## Abstract\nHyperbolic trigonometry is developed and illustrated in this article along lines pa+lel to Euclidean trigonometry by exposing the hyperbolic trigonometric law of cosines and of sines in the Poincarb ball model of n-dimensional hyperbolic geometry, as well as their application. The Poincarb ball model of three-dimensional hyperbolic geometry is becoming increasingly important in the construction of hyperbolic browsers in computer graphics. These allow in computer graphics the exploitation of hyperbolic geometry in the development of visualization techniques. It is, therefore, clear that hyperbolic trigonometry in the Poincare ball model of hyperbolic geometry, as presented here, will prove useful in the development of efficient hyperbolic browsers in computer graphics. Hyperbolic trigonometry is governed by gyrovector spaces in the same way that Euclidean trigonometry is governed by vector spaces. The capability of gyrovector space theory to capture analogies and its powerful elegance is thus demonstrated once more.\n...Read more\n## Key takeaways\n![sparkles](https://a.academia-assets.com/images/icons/sparkle.svg)\nAI\n1. Hyperbolic trigonometry parallels Euclidean trigonometry through laws of cosines and sines in the Poincar\u00e9 model.\n2. The article illustrates hyperbolic trigonometric laws using a numerical example in the Poincar\u00e9 ball model.\n3. Mobius gyrovector spaces provide a foundation for hyperbolic geometry analogous to vector spaces for Euclidean geometry.\n4. Three-dimensional hyperbolic geometry enhances computer graphics via hyperbolic browsers, optimizing information visualization.\n5. Hyperbolic trigonometry can be applied to solve hyperbolic triangle problems, similar to Euclidean methods.\n## Related papers\n[Hyperbolic trigonometry in the Einstein relativistic velocity model of hyperbolic geometry](https://www.academia.edu/53029889/Hyperbolic_trigonometry_in_the_Einstein_relativistic_velocity_model_of_hyperbolic_geometry)\n[Abraham Ungar](https://independent.academia.edu/UngarAbraham)\nComputers &amp; Mathematics with Applications, 2000\nHyperbolic geometry is a fundamental aspect of modern physics. We explore in this paper the use of Einstein&#39;&#39;s velocity addition as a model of vector addition in hyperbolic geometry. Guided by analogies with ordinary vector addition, we develop hyperbolic vector spaces, called gyrovector spaces, which provide the setting for hyperbolic geometry in the same way that vector spaces provide the setting for Euclidean geometry. The resulting gyrovector spaces enable Euclidean trigonometry to be extended to hyperbolic trigonometry. In particular, we present the hyperbolic law of cosines and sines and the Hyperbolic Pythagorean Theorem emerges when the common vector addition is replaced by the Einstein velocity addition. (\\~\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/53029889/Hyperbolic_trigonometry_in_the_Einstein_relativistic_velocity_model_of_hyperbolic_geometry)\n[Introduction to Hyperbolic Geometry Al &#39; a](https://www.academia.edu/76075639/Introduction_to_Hyperbolic_Geometry_Al_a)\n[Khawla Muhtaseb](https://independent.academia.edu/KhawlaMuhtaseb)\n2012\nHyperbolic Geometry is a particular type of Non-Euclidean Geometry. We can see Hyperbolic Geometry and its application in our life . In this seminar we studied a brief history of Euclidean and Hyperbolic geometries, and discussed postulates of Euclidean Geometry . After that we introduced some definitions, axioms of Hyperbolic Geometry and main theorems in Neutral Geometry . Finally we talked about two models of Hyperbolic Geometry; Klein Disk Model and Poincar\u00e9 Disk Model.\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/76075639/Introduction_to_Hyperbolic_Geometry_Al_a)\n[Hyperbolic Geometry](https://www.academia.edu/111914929/Hyperbolic_Geometry)\n[Abraham A Ungar](https://independent.academia.edu/AbrahamUngar)\n2013\nRelativistic hyperbolic geometry is a model of the hyperbolic geometry of Lobachevsky and Bolyai in which Einstein addition of relativistically admissible velocities plays the role of vector addition. The adaptation of barycentric coordinates for use in relativistic hyperbolic geometry results in the relativistic barycentric coordinates. The latter are covariant with respect to the Lorentz transformation group just as the former are covariant with respect to the Galilei transformation group. Furthermore, the latter give rise to hyperbolically convex sets just as the former give rise to convex sets in Euclidean geometry. Convexity considerations are important in non-relativistic quantum mechanics where mixed states are positive barycentric combinations of pure states and where barycentric coordinates are interpreted as probabilities. In order to set the stage for its application in the geometry of relativistic quantum states, the notion of the relativistic barycentric coordinates that relativistic hyperbolic geometry admits is studied.\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/111914929/Hyperbolic_Geometry)\n[Introduction to Hyperbolic Geometry](https://www.academia.edu/127566590/Introduction_to_Hyperbolic_Geometry)\n[John Ratcliffe](https://independent.academia.edu/JohnRatcliffe1)\nThe American Mathematical Monthly, 1996\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/127566590/Introduction_to_Hyperbolic_Geometry)\n[Analytic Hyperbolic Geometry\u2014Mathematical Foundations and Applications by Abraham A. Ungar](https://www.academia.edu/114804404/Analytic_Hyperbolic_Geometry_Mathematical_Foundations_and_Applications_by_Abraham_A_Ungar)\n[Azniv Kasparian](https://uni-sofia.academia.edu/AznivKasparian)\n2006\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/114804404/Analytic_Hyperbolic_Geometry_Mathematical_Foundations_and_Applications_by_Abraham_A_Ungar)\n[Interfacing Visual Basic And Mathematica To Create An Application For Hyperbolic Geometry](https://www.academia.edu/75763051/Interfacing_Visual_Basic_And_Mathematica_To_Create_An_Application_For_Hyperbolic_Geometry)\n[Edgar Reyes](https://independent.academia.edu/EdgarReyes37)\n2003 Annual Conference Proceedings\nIn this paper, we will describe how one can use technology to provide students with graphical representations and animations as they study certain ideas and well-studied objects of hyperbolic geometry, which have application to web-based data structures. These objects and ideas include the Poincare disk model, distances, midpoints, angles, hyperbolic triangles, reflections about hyperbolic lines, and distancepreserving transformations. With regards to the technology, we will discuss an interactive applet that interfaces Visual Basic and Mathematica and shows graphic representations of the ideas and objects just mentioned.\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/75763051/Interfacing_Visual_Basic_And_Mathematica_To_Create_An_Application_For_Hyperbolic_Geometry)\n[What is a vector in hyperbolic geometry? And, what is a hyperbolic linear transformation?](https://www.academia.edu/81391222/What_is_a_vector_in_hyperbolic_geometry_And_what_is_a_hyperbolic_linear_transformation)\n[Edgar Reyes](https://independent.academia.edu/EdgarReyes245),[S. Li](https://independent.academia.edu/SLi10)\nsections.maa.org\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/81391222/What_is_a_vector_in_hyperbolic_geometry_And_what_is_a_hyperbolic_linear_transformation)\n[Answering typical student questions in hyperbolic geometry: a transformation group approach](https",
          "original_query": "Hyperbolic trigonometry and its application in the Poincar\u00e9 ball model of hyperbolic geometry",
          "cleaned_query": "Hyperbolic trigonometry and its application in the Poincar\u00e9 ball model of hyperbolic geometry",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Compositional Entailment Learning for Hyperbolic Vision-Language...",
          "url": "https://openreview.net/forum?id=3i13Gev2hV",
          "content": "**Keywords:** Vision-Language Models, Hyperbolic Geometry, Representation Learning, CLIP\n\n**TL;DR:** We explore the benefits brought in when using visual-semantic compositional hierarchies for learning hyperbolic representations through unsupervised contrastive training.\n\n**Abstract:**\n\nImage-text representation learning forms a cornerstone in vision-language models, where pairs of images and textual descriptions are contrastively aligned in a shared embedding space. Since visual and textual concepts are naturally hierarchical, recent work has shown that hyperbolic space can serve as a high-potential manifold to learn vision-language representation with strong downstream performance. In this work, for the first time we show how to fully leverage the innate hierarchical nature of hyperbolic embeddings by looking beyond individual image-text pairs. We propose Compositional Entailment Learning for hyperbolic vision-language models. The idea is that an image is not only described by a sentence but is itself a composition of multiple object boxes, each with their own textual description. Such information can be obtained freely by extracting nouns from sentences and using openly available localized grounding models. We show how to hierarchically organize images, image boxes, and their textual descriptions through contrastive and entailment-based objectives. Empirical evaluation on a hyperbolic vision-language model trained with millions of image-text pairs shows that the proposed compositional learning approach outperforms conventional Euclidean CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot and retrieval generalization and clearly stronger hierarchical performance.\n\n**Primary Area:** unsupervised, self-supervised, semi-supervised, and supervised representation learning\n\n**Code Of Ethics:** I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.\n\n**Anonymous Url:** I certify that there is no URL (e.g., github page) that could be used to find authors\u2019 identity.\n\n**No Acknowledgement Section:** I certify that there is no acknowledgement section in this submission for double blind review.\n\n**Submission Number:** 4111",
          "original_query": "Compositional entailment learning for hyperbolic vision-language models",
          "cleaned_query": "Compositional entailment learning for hyperbolic vision-language models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2101.01600] Learning the Predictability of the Future - arXiv",
          "url": "https://arxiv.org/abs/2101.01600",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2101.01600** (cs)\n\n\\[Submitted on 1 Jan 2021\\]\n\n# Title:Learning the Predictability of the Future\n\nAuthors: [D\u00eddac Sur\u00eds](https://arxiv.org/search/cs?searchtype=author&query=Sur%C3%ADs,+D), [Ruoshi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+R), [Carl Vondrick](https://arxiv.org/search/cs?searchtype=author&query=Vondrick,+C)\n\nView a PDF of the paper titled Learning the Predictability of the Future, by D\\\\'idac Sur\\\\'is and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2101.01600)\n\n> Abstract:We introduce a framework for learning from unlabeled video what is predictable in the future. Instead of committing up front to features to predict, our approach learns from data which features are predictable. Based on the observation that hyperbolic geometry naturally and compactly encodes hierarchical structure, we propose a predictive model in hyperbolic space. When the model is most confident, it will predict at a concrete level of the hierarchy, but when the model is not confident, it learns to automatically select a higher level of abstraction. Experiments on two established datasets show the key role of hierarchical representations for action prediction. Although our representation is trained with unlabeled video, visualizations show that action hierarchies emerge in the representation.\n\n| | |\n| --- | --- |\n| Comments: | Website: [this https URL](https://hyperfuture.cs.columbia.edu) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV) |\n| Cite as: | [arXiv:2101.01600](https://arxiv.org/abs/2101.01600) \\[cs.CV\\] |\n| | (or [arXiv:2101.01600v1](https://arxiv.org/abs/2101.01600v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2101.01600](https://doi.org/10.48550/arXiv.2101.01600) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Didac Sur\u00eds Coll-Vinent \\[ [view email](https://arxiv.org/show-email/b20cd588/2101.01600)\\]\n\n**\\[v1\\]**\nFri, 1 Jan 2021 18:58:36 UTC (12,893 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Learning the Predictability of the Future, by D\\\\'idac Sur\\\\'is and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2101.01600)\n- [TeX Source](https://arxiv.org/src/2101.01600)\n- [Other Formats](https://arxiv.org/format/2101.01600)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2101.01600&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2101.01600&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2021-01](https://arxiv.org/list/cs.CV/2021-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2101.01600?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2101.01600?context=cs.LG)\n\n[eess](https://arxiv.org/abs/2101.01600?context=eess)\n\n[eess.IV](https://arxiv.org/abs/2101.01600?context=eess.IV)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2101.01600)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2101.01600)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2101.01600)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2101.html#abs-2101-01600) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2101-01600)\n\n[D\u00eddac Sur\u00eds](https://dblp.uni-trier.de/search/author?author=D%C3%ADdac%20Sur%C3%ADs)\n\n[Carl Vondrick](https://dblp.uni-trier.de/search/author?author=Carl%20Vondrick)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2101.01600&description=Learning the Predictability of the Future) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2101.01600&title=Learning the Predictability of the Future)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2101.01600) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Learning the predictability of the future",
          "cleaned_query": "Learning the predictability of the future",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "SA-Co++: A Benchmark for Compositional and Hierarchical Prompt Stress-Tests\nCreate a benchmark extension to SA-Co that systematically varies (a) compositional complexity (\u201cstriped ceramic coffee mug with handle\u201d), (b) hierarchical specificity (vehicle\u2192car\u2192sedan), and (c) distractor structure using curated hard negatives. Provide standardized prompt sets with controlled synonymy/polysemy and evaluate models on both mask quality and concept-level correctness (including \u201cacceptable if predicted at a higher level\u201d). This would expose whether gains from hyperbolic objectives translate into measurable improvements in hierarchical generalization for promptable segmentation/tracking."
      ],
      "generated_ideas_raw": "1. **Hyperbolic Promptable Concept Segmentation (HyPCS)**\n   Build a SAM 3\u2013style PCS system where concept prompts, instance identities, and scene context are embedded in the Poincar\u00e9 ball, using norm to encode specificity (hypernym\u2192hyponym). Train with a joint loss: SAM 3 mask/track objectives + hyperbolic entailment constraints linking \u201cyellow school bus\u201d \u2192 \u201cbus\u201d \u2192 \u201cvehicle\u201d across images/videos. Evaluate whether hyperbolic geometry improves hard-negative separation and open-vocabulary instance disambiguation in SA-Co.\n\n2. **CLIP-to-SAM Concept Calibration via Presence-Head Distillation**\n   Use CLIP (or hyperbolic CLIP from compositional entailment learning) to generate concept-confidence targets for SAM 3\u2019s presence head, explicitly calibrating \u201cis the concept present?\u201d separately from \u201cwhere is it?\u201d. Create a distillation pipeline where CLIP similarity distributions over prompt variants supervise presence logits, while SAM 3 continues to learn localization from masks. Measure improvements in false-positive control for ambiguous prompts (\u201cbat\u201d, \u201cbank\u201d, \u201cseal\u201d) and long-tail concepts.\n\n3. **Predictability-Aware Tracking: Hierarchical Future Forecasting for PCS**\n   Extend \u201cLearning the Predictability of the Future\u201d to SAM 3\u2019s video tracker by predicting not only next-frame masks but also the *level of concept abstraction* that is predictable (instance \u2192 subtype \u2192 supertype). Implement a hyperbolic predictive head whose norm increases when the tracker can commit to fine-grained identity, and collapses toward higher-level concepts when uncertainty rises. Benchmark on occlusions, camera cuts, and small-object regimes where fine identity is often unpredictable.\n\n4. **Compositional Box-to-Mask Entailment Learning for Open-Vocab Segmentation**\n   Combine compositional entailment learning (image \u2194 boxes \u2194 text) with SAM 3 by treating SAM masks as the \u201cboxes\u201d in the hierarchy and enforcing entailment: mask-caption \u2192 image-caption and part \u2192 whole (e.g., \u201cwheel\u201d entails \u201ccar\u201d). Automatically mine nouns/phrases from captions and ground them with SAM 3 masks, then train a hyperbolic entailment loss over (mask, phrase) pairs with hard negatives from SA-Co. Target outcome: stronger part-whole consistency and improved zero-shot segmentation for compositional phrases.\n\n5. **Hyperbolic Scene\u2013Object Co-Occurrence Priors for Zero-Shot Prompt Expansion**\n   Build a hyperbolic scene-object graph from web-scale image-text pairs (CLIP-style) plus detected objects (OpenImages/COCO) to encode hierarchical co-occurrence (kitchen\u2192(mug, kettle, stove)). At inference, expand a user prompt into a ranked set of related prompts using geodesic neighborhoods (via hyperbolic distance) and use them as auxiliary prompts to SAM 3 for recall-boosting. Evaluate gains on rare concepts and under-specified prompts (\u201ctools on a table\u201d) without adding labeled data.\n\n6. **Geodesic Consistency Loss for Video Masks in the Poincar\u00e9 Ball**\n   Introduce a temporal regularizer for SAM 3 tracking that penalizes *geodesic acceleration* of an instance\u2019s concept embedding across frames: embeddings should move smoothly along hyperbolic geodesics unless appearance truly changes. Use hyperbolic trigonometry identities (law of cosines in the Poincar\u00e9 ball) to compute stable geodesic angles/lengths for the regularizer. Hypothesis: fewer identity switches and better robustness to viewpoint changes, especially for fine-grained concepts.\n\n7. **Hard-Negative Mining Using Hyperbolic Angular Margins**\n   Extend hyperbolic contrastive learning beyond objects by designing a negative-mining strategy where \u201chardness\u201d depends on both direction (semantic similarity) and norm (hierarchical proximity). Concretely, mine negatives that are close in angle but differ in norm (e.g., \u201cdog\u201d vs \u201cgolden retriever\u201d) and negatives that match norm but differ in direction (peer categories). Apply to CLIP-like pretraining and to SAM 3\u2019s concept recognition, aiming to improve fine-grained discrimination without sacrificing coarse-level generalization.\n\n8. **Concept-Conditioned Memory Compression for Long-Horizon Video Segmentation**\n   SAM 3 uses memory-based tracking; augment it with a concept-conditioned hyperbolic memory where prototypes are stored at multiple abstraction levels (instance/subtype/type). Use hyperbolic clustering to merge near-duplicate memories while preserving hierarchical coverage, reducing memory growth on long videos. Evaluate on long-form videos for stable tracking under repeating instances and re-entries, with metrics on compute/memory vs accuracy.\n\n9. **Cross-Modal Hierarchical Uncertainty: Norm as \u201cSpecificity Temperature\u201d**\n   Develop a unified uncertainty model for vision-language embeddings where prediction specificity is controlled by embedding norm (hyperbolic) and prompt variation (textual). Train a model to output calibrated distributions over hierarchy levels: when evidence is weak, it should prefer higher-level labels but still produce usable masks/tracks (presence head + segmentation). This directly links CLIP-style zero-shot classification, compositional entailment, and SAM 3 prompting into one controllable interface.\n\n10. **SA-Co++: A Benchmark for Compositional and Hierarchical Prompt Stress-Tests**\n   Create a benchmark extension to SA-Co that systematically varies (a) compositional complexity (\u201cstriped ceramic coffee mug with handle\u201d), (b) hierarchical specificity (vehicle\u2192car\u2192sedan), and (c) distractor structure using curated hard negatives. Provide standardized prompt sets with controlled synonymy/polysemy and evaluate models on both mask quality and concept-level correctness (including \u201cacceptable if predicted at a higher level\u201d). This would expose whether gains from hyperbolic objectives translate into measurable improvements in hierarchical generalization for promptable segmentation/tracking.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "SA-Co++: A Benchmark for Compositional and Hierarchical Prompt Stress-Tests\nCreate a benchmark extension to SA-Co that systematically varies (a) compositional complexity (\u201cstriped ceramic coffee mug w",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 65,
      "paper_title": "SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing",
      "contribution": "Introduction of SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic audio-visual scenes, and a novel training-free reasoning pipeline that enhances AV-LLMs' performance in understanding such environments.",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "quality_content": 4,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8275,
      "output_tokens": 974,
      "predecessor_details": [
        {
          "success": true,
          "title": "Systems of spatial reference in human memory - PubMed - NIH",
          "url": "https://pubmed.ncbi.nlm.nih.gov/11741344/",
          "content": "Clipboard, Search History, and several other advanced features are temporarily unavailable.\n\n [Skip to main page content](https://pubmed.ncbi.nlm.nih.gov/11741344/#article-details)\n\n![Dot gov](https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg)\n\n**The .gov means it\u2019s official.**\n\nFederal government websites often end in .gov or .mil. Before\nsharing sensitive information, make sure you\u2019re on a federal\ngovernment site.\n\n![Https](https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg)\n\n**The site is secure.**\n\nThe **https://** ensures that you are connecting to the\nofficial website and that any information you provide is encrypted\nand transmitted securely.\n\n[Access keys](https://www.ncbi.nlm.nih.gov/guide/browsers/#ncbi_accesskeys) [NCBI Homepage](https://www.ncbi.nlm.nih.gov) [MyNCBI Homepage](https://pubmed.ncbi.nlm.nih.gov/myncbi/) [Main Content](https://pubmed.ncbi.nlm.nih.gov/11741344/#maincontent) [Main Navigation](https://pubmed.ncbi.nlm.nih.gov/11741344/)\n\n[![pubmed logo](https://cdn.ncbi.nlm.nih.gov/pubmed/de3675f9-e3b3-4656-8215-6fc84d4c88ab/core/images/pubmed-logo-blue.svg)](https://pubmed.ncbi.nlm.nih.gov/)\n\nSearch:\nSearch\n\n[Advanced](https://pubmed.ncbi.nlm.nih.gov/advanced/) [Clipboard](https://pubmed.ncbi.nlm.nih.gov/clipboard/)\n\n[User Guide](https://pubmed.ncbi.nlm.nih.gov/help/)\n\nSave\n\nEmail\n\nSend to\n\n- [Clipboard](https://pubmed.ncbi.nlm.nih.gov/11741344/)\n- [My Bibliography](https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpubmed.ncbi.nlm.nih.gov%2F11741344%2F%23open-bibliography-panel)\n- [Collections](https://account.ncbi.nlm.nih.gov/?back_url=https%3A%2F%2Fpubmed.ncbi.nlm.nih.gov%2F11741344%2F%23open-collections-panel)\n- [Citation manager](https://pubmed.ncbi.nlm.nih.gov/11741344/)\n\nDisplay options\n\nDisplay options\n\nFormat\nAbstractPubMedPMID\n\n## Save citation to file\n\nFormat:\nSummary (text)PubMedPMIDAbstract (text)CSV\n\nCreate file\n\nCancel\n\n## Email citation\n\nSubject:\n1 selected item: 11741344 - PubMed\n\nTo:\n\nFrom:\n\nFormat:\nSummarySummary (text)AbstractAbstract (text)\n\nMeSH and other data\n\nSend email\n\nCancel\n\n### Add to Collections\n\n- Create a new collection\n- Add to an existing collection\n\nName your collection:\n\nName must be less than 100 characters\n\nChoose a collection:\n\nUnable to load your collection due to an error\n\n[Please try again](https://pubmed.ncbi.nlm.nih.gov/11741344/)\n\nAdd\n\nCancel\n\n### Add to My Bibliography\n\n- My Bibliography\n\nUnable to load your delegates due to an error\n\n[Please try again](https://pubmed.ncbi.nlm.nih.gov/11741344/)\n\nAdd\n\nCancel\n\n## Your saved search\n\nName of saved search:\n\nSearch terms:\n\n[Test search terms](https://pubmed.ncbi.nlm.nih.gov/11741344/)\n\nWould you like email updates of new search results?Saved Search Alert Radio Buttons\n\n- Yes\n- No\n\nEmail:\n( [change](https://www.ncbi.nlm.nih.gov/account/settings/))\n\nFrequency:\nMonthlyWeeklyDaily\n\nWhich day?\nThe first SundayThe first MondayThe first TuesdayThe first WednesdayThe first ThursdayThe first FridayThe first SaturdayThe first dayThe first weekday\n\nWhich day?\nSundayMondayTuesdayWednesdayThursdayFridaySaturday\n\nReport format:\nSummarySummary (text)AbstractAbstract (text)PubMed\n\nSend at most:\n1 item5 items10 items20 items50 items100 items200 items\n\nSend even when there aren't any new results\n\nOptional text in email:\n\nSave\n\nCancel\n\n## Create a file for external citation management software\n\nCreate file\n\nCancel\n\n## Your RSS Feed\n\nName of RSS Feed:\n\nNumber of items displayed:\n510152050100\n\nCreate RSS\n\nCancel\n\nRSS Link\nCopy\n\nFull text links\nCite\n\nDisplay options\n\nDisplay options\n\nFormat\nAbstractPubMedPMID\n\n## Abstract\n\nSeven experiments examined the spatial reference systems used in memory to represent the locations of objects in the environment. Participants learned the locations of common objects in a room and then made judgments of relative direction using their memories of the layout (e.g., \"Imagine you are standing at the shoe, facing the lamp; point to the clock\"). The experiments manipulated the number of views that observers were allowed to experience, the presence or absence of local and global reference systems (e.g., a rectangular mat on which objects were placed and the walls of the room, respectively), and the congruence of local and global reference systems. Judgments of relative direction were more accurate for imagined headings parallel to study views than for imagined headings parallel to novel views, even with up to three study views. However, study views misaligned with salient reference systems in the environment were not strongly represented if they were experienced in the context of aligned views. Novel views aligned with a local reference system were, under certain conditions, easier to imagine than were novel views misaligned with the local reference system. We propose that learning and remembering the spatial structure of the surrounding environment involves interpreting the layout in terms of a spatial reference system. This reference system is imposed on the environment but defined by egocentric experience.\n\nCopyright 2001 Academic Press.\n\n[PubMed Disclaimer](https://pubmed.ncbi.nlm.nih.gov/disclaimer/)\n\n## Similar articles\n\n- [Geometric cues, reference frames, and the equivalence of experienced-aligned and novel-aligned views in human spatial memory.](https://pubmed.ncbi.nlm.nih.gov/23305700/)\n\nKelly JW, Sjolund LA, Sturz BR.Kelly JW, et al.Cognition. 2013 Mar;126(3):459-74. doi: 10.1016/j.cognition.2012.11.007. Epub 2013 Jan 8.Cognition. 2013.PMID: 23305700Clinical Trial.\n\n- [Body- and environmental-stabilized processing of spatial knowledge.](https://pubmed.ncbi.nlm.nih.gov/18315416/)\n\nMou W, Li X, McNamara TP.Mou W, et al.J Exp Psychol Learn Mem Cogn. 2008 Mar;34(2):415-21. doi: 10.1037/0278-7393.34.2.415.J Exp Psychol Learn Mem Cogn. 2008.PMID: 18315416\n\n- [Sensorimotor alignment effects in the learning environment and in novel environments.](https://pubmed.ncbi.nlm.nih.gov/17983315/)\n\nKelly JW, Avraamides MN, Loomis JM.Kelly JW, et al.J Exp Psychol Learn Mem Cogn. 2007 Nov;33(6):1092-107. doi: 10.1037/0278-7393.33.6.1092.J Exp Psychol Learn Mem Cogn. 2007.PMID: 17983315\n\n- [Allocentric and egocentric updating of spatial memories.](https://pubmed.ncbi.nlm.nih.gov/14736303/)\n\nMou W, McNamara TP, Valiquette CM, Rump B.Mou W, et al.J Exp Psychol Learn Mem Cogn. 2004 Jan;30(1):142-57. doi: 10.1037/0278-7393.30.1.142.J Exp Psychol Learn Mem Cogn. 2004.PMID: 14736303Clinical Trial.\n\n- [The influence of spatial reference frames on imagined object- and viewer rotations.](https://pubmed.ncbi.nlm.nih.gov/10504883/)\n\nWraga M, Creem SH, Proffitt DR.Wraga M, et al.Acta Psychol (Amst). 1999 Sep;102(2-3):247-64. doi: 10.1016/s0001-6918(98)00057-2.Acta Psychol (Amst). 1999.PMID: 10504883Review.\n\n\nSee all similar articles\n\n## Cited by\n\n- [Frontal-midline theta and posterior alpha oscillations index early processing of spatial representations during active navigation.](https://pubmed.ncbi.nlm.nih.gov/37862831/)\n\nDu YK, Liang M, McAvan AS, Wilson RC, Ekstrom AD.Du YK, et al.Cortex. 2023 Dec;169:65-80. doi: 10.1016/j.cortex.2023.09.005. Epub 2023 Sep 30.Cortex. 2023.PMID: 37862831\n\n- [Perceived spatial presence and body orientation affect the recall of out-of-sight places in an immersive sketching experiment.](https://pubmed.ncbi.nlm.nih.gov/37819501/)\n\nGrochulla B, Mallot HA.Grochulla B, et al.Psychol Res. 2024 Mar;88(2):509-522. doi: 10.1007/s00426-023-01877-x. Epub 2023 Oct 11.Psychol Res. 2024.PMID: 37819501Free PMC article.\n\n- [Individual Differences in Spatial Orientation Modulate Perspective Taking in Listeners.](https://pubmed.ncbi.nlm.nih.gov/37663137/)\n\nLoy JE, Demberg V.Loy JE, et al.J Cogn. 2023 Sep 1;6(1):52. doi: 10.5334/joc.321. eCollection 2023.J Cogn. 2023.PMID: 37663137Free PMC article.\n\n- [Spatial updating in virtual reality for reproducing object locations in vista space-Boundaries, landmarks, and idiothetic cues.](https://pubmed.ncbi.nlm.nih.gov/37425154/)\n\nBorodaeva Z, Winkler S, Brade J, Klimant P, Jahn G.Borodaeva Z, et al.Front Psychol. 2023 Jun 22;14:1144861. doi: 10.3389/fpsyg.2023.1144861. eCollection 2023.Front Psychol. 2023.PMID: 37425154Free PMC article.\n\n- [Different Types of Survey-Based Environmental Representations: Egocentric vs. Allocentric Cognitive Maps.](https://pubmed.ncbi.nlm.nih.gov/37239306/)\n\nKozhevnikov M, Puri J.Kozhevnikov M, et al.Brain Sci. 2023 May 22;13(5):834. doi: 10.3390/brainsci13050834.Brain Sci. 2023.PMID: 37239306Free PMC article.\n\n\nSee all \"Cited by\" articles\n\n## Publication types\n\n- Research Support, U.S. Gov't, Non-P.H.S.\n\n\n\nActions\n\n\n\n\n\n- [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Research+Support%2C+U.S.+Gov%27t%2C+Non-P.H.S.%22%5Bpt%5D&sort=date&sort_order=desc)\n- [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Research+Support%2C+U.S.+Gov%27t%2C+Non-P.H.S.)\n- [Add to Search](https://pubmed.ncbi.nlm.nih.gov/11741344/)\n\n- Research Support, U.S. Gov't, P.H.S.\n\n\n\nActions\n\n\n\n\n\n- [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Research+Support%2C+U.S.+Gov%27t%2C+P.H.S.%22%5Bpt%5D&sort=date&sort_order=desc)\n- [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Research+Support%2C+U.S.+Gov%27t%2C+P.H.S.)\n- [Add to Search](https://pubmed.ncbi.nlm.nih.gov/11741344/)\n\n## MeSH terms\n\n- Adult\n\n\n\nActions\n\n\n\n\n\n- [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Adult%22%5BMeSH%5D&sort=date&sort_order=desc)\n- [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Adult)\n- [Add to Search](https://pubmed.ncbi.nlm.nih.gov/11741344/)\n\n- Female\n\n\n\nActions\n\n\n\n\n\n- [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Female%22%5BMeSH%5D&sort=date&sort_order=desc)\n- [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh?term=Female)\n- [Add to Search](https://pubmed.ncbi.nlm.nih.gov/11741344/)\n\n- Humans\n\n\n\nActions\n\n\n\n\n\n- [Search in PubMed](https://pubmed.ncbi.nlm.nih.gov/?term=%22Humans%22%5BMeSH%5D&sort=date&sort_order=desc)\n- [Search in MeSH](https://www.ncbi.nlm.nih.gov/mesh",
          "original_query": "Systems of spatial reference in human memory",
          "cleaned_query": "Systems of spatial reference in human memory",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] SpatialVLM: Endowing Vision-Language Models with Spatial ...",
          "url": "https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.pdf",
          "content": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nBoyuan Chen1,3,\u2217,\u2020, Zhuo Xu1,\u2217, Sean Kirmani1, Brian Ichter1,\nDorsa Sadigh1, Leonidas Guibas2, Fei Xia1\n1Google DeepMind, 2Google Research, 3MIT\nYou are a cleaning robot that is 1 \nmeter wide, now you are \nstanding in a living room and see \nthe image, you want to move \nfrom here to the door that leads \nto the backyard, do you think I \ncan go through the path between \nthe sofa and the table and \nchairs.\nUser As an AI, I'm unable to physically interact with environments, but I can provide some \ninsights based on the image provided. \u2026 From the image, we can make some visual \nestimations. Judging from the image, it appears that the sofa and the table are quite\nclose to each other, and there may not be enough space to accommodate a 1-meter\nwide robot\u2026.\n[VLM Reasoning] How wide is the path between the sofa and the table and chairs? \n[VLM Answer] 1.56m [Answer] Yes, the robot can go through the path between the\nsofa and the table and chairs since it is wider than the robot's width.\nGPT-4V*\nSpatial-VLM\nWhich can is the closest to the \nyellow finger that is on the right \nmost side of the picture, the \nblue, orange or the silver one?\nUser After estimating the distances by measuring the Euclidean distances between the \nyellow finger and each of the cans, it is clear that the silver can is the closest to the\nyellow finger with an approximate distance of 110 units. In comparison, the blue can \nis about 262 units away, and the orange can is approximately 233 units away from the \nyellow finger. \n[VLM Reasoning] How far is the yellow finger from the blue can? [VLM Answer] \n0.5m [VLM Reasoning] How far is the yellow finger from the orange can? [VLM \nAnser] 0.59m [VLM Reasoning] How far is the yellow finger from the silver can?\n[VLM Answer] 0.21m [Answer] The silver can is the closest to the yellow finger that\nis on the right most side of the picture.\nGPT-4V\nSpatial-VLM\nFigure 1. We present SpatialVLM, a data synthesis and pre-training mechanism to enhance VLMs\u2019 spatial reasoning capabilities. We demonstrate\nthat VLMs trained on our synthetic data exhibit strong spatial reasoning capabilities, and can generate metric distance estimation from 2D input\nimages, addressing blind spots of current state-of-the-art VLMs like GPT-4V. (\u2217 GPT-4V accessed Nov. 2023).\nAbstract\nUnderstanding and reasoning about spatial relationships\nis a fundamental capability for Visual Question Answering\n(VQA) and robotics. While Vision Language Models (VLM)\nhave demonstrated remarkable performance in certain VQA\nbenchmarks, they still lack capabilities in 3D spatial reason\u0002ing, such as recognizing quantitative relationships of physical\nobjects like distances or size difference. We hypothesize that\nVLMs\u2019 limited spatial reasoning capability is due to the lack\nof 3D spatial knowledge in training data and aim to solve this\nproblem by training VLMs with Internet-scale spatial reasoning\ndata. To this end, we present a system to facilitate this approach.\nWe first develop an automatic 3D spatial VQA data generation\nframework that scales up to 2 billion VQA examples on 10\nmillion real-world images. We then investigate various factors\nin training recipe including data quality, training pipeline and\n\u2217 Equal Contribution; \u2020 Work done during internship at Google DeepMind;\nCorrespond to boyuanc@mit.edu; zhuoxu,xiafei@google.com\nVLM architecture. Our work features the first Internet-scale 3D\nspatial reasoning dataset in metric space. By training a VLM\non such data, we significantly enhance its ability on both qual\u0002itative and quantitative spatial VQA. Finally, we demonstrate\nthat this VLM unlocks novel downstream applications in chain\u0002of-thought spatial reasoning and robotics due to its quantitative\nestimation capability. Website: https://spatial-vlm.github.io/\n1. Introduction\nVision language models (VLMs) have made significant progress\nin recent years across a variety of tasks including image cap\u0002tioning, visual question answering (VQA), embodied planning,\naction recognition, and more [2, 18, 24, 31]. While VLMs are\npowerful general-purpose models for a wide range of tasks,\nmost state-of-the-art VLMs still struggle with spatial reasoning,\ni.e. tasks that require understanding the position of objects in 3D\nspace, or spatial relationships between them. Spatial reasoning\ncapabilities are useful in their own right, but also for downstream\napplications such as in robotics or AR. For example, a spatial\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore. 14455\nreasoning-imbued VLM can be used as a better general-purpose\nreward annotator [51] and success detector [19].\nThe exploration of foundation models like VLMs is often\ninspired by human capabilities. Humans, through embodied ex\u0002periences and evolutionary development, possess innate spatial\nreasoning skills. We effortlessly determine spatial relationships,\nsuch as the positioning of objects relative to each other or esti\u0002mating distances and sizes, without complex chain-of-thoughts\nor mental computations. This natural proficiency in direct\nspatial reasoning tasks contrasts with the current limitations of\nVLMs and thus prevents them from accomplishing real-world\ntasks that requires multiple steps of spatial reasoning. This\ngap leads us to a compelling research question: can we imbue\nVLMs with spatial reasoning abilities akin to those of humans?\nTherefore, we hypothesize that the limited the spatial\nreasoning abilities of current VLMs is not due to a fundamental\nlimitation of their architecture, but rather is a limitation in\ncommon datasets available at scale on which such models are\ntrained. For example, many VLMs [13, 18, 42] are trained on\ninternet-scale datasets characterized by image-caption pairs [12],\nwhich contain limited spatial information. This is partially due\nto the difficulties of obtaining spatial-information-rich embodied\ndata or high-quality human annotations for 3D-aware queries.\nAutomatic data generation and augmentation techniques\nare one approach to deal with the data limitation problem\n[36, 50, 53, 63]. However, most previous data generation efforts\nfocus on rendering photorealistic images with ground truth\nsemantic annotation but overlook the richness of objects and\n3D relationships. In contrast, we focus on extracting spatial\ninformation directly from real world data in order to capture\nthe diversity and complexity of the true 3D world.\nOur key insight is that recent advancement in off-the-shelf\nvision models can automatically generate rich 3D spatial annota\u0002tions from 2D images. To this end, we propose a system called\nSpatialVLM that enables data generation and training of VLMs\nto enhance their spatial reasoning capabilities. Concretely,\nby combining 1) open-vocabulary detection, 2) metric depth\nestimation, 3) semantic segmentation and 4) object-centric\ncaptioning models, we can densely annotates real world data\nat scale. SpatialVLM converts the data generated by vision\nmodels into a format can be used to train VLMs on a mixture\nof captioning, VQA and spatial reasoning data. Training on\nsuch data gives SpatialVLM the perceptual foundations of the\n3D world which allows spatial reasoning through an LLM.\nThrough experiments, we find our trained VLM exhibit\nmany desirable capabilities. First, its ability to answer\nqualitative spatial questions is greatly enhanced. Secondly,\nit can perform quantitative estimation reliably despite noisy\ntraining data. Such capability not only gives it common\nsense knowledge about object sizes but also makes it useful\nas a open-vocabulary reward annotator for rearrangement\ntasks. Thirdly, we find this spatial Vision Language Model,\nbenefiting from its natural language interface, can perform\nspatial chain-of-thought to solve complex spatial reasoning\ntasks when combined with a powerful Large Language Model.\nOur main contributions are:\n\u2022 We endow VLMs quantitative spatial reasoning capability,\nwhich is a fundamental capability of humans.\n\u2022 We design a framework to automatically label 3D spatial\nreasoning VQA data based on real world images at the\nInternet scale.\n\u2022 We study various training recipes: data quality, training\npipeline, freeze/unfreeze visual encoder, etc, and investigate\nhow they affect the learning quality.\n\u2022 We show examples of new capabilities in complex reasoning\nand robotics unlocked by the introduced task and method.\n2. Related Work\nLearning Spatial Reasoning. Spatial distance estimation\nhas been traditionally addressed as a part of broader tasks, such\nas SLAM [8, 21] or depth estimation [23]. When applying\nthese spatial concepts to reasoning, prior works often focus on\nexplicit spatial scene memories [26, 27] or spatial scene graphs\n[29, 30, 59, 60]. Scene graphs allow interpretable, structured,\nstatistical relation learning based on the spatial structures they\nencode. To answer spatial problems in VQA formats, they\nmust handle it explicitly as a pathfinding problem on said\nscene graph. VLMs, on the other hand, are pretrained on large\namounts of loosely structured information from vision-language\ndatasets. Unlike scene graphs, the spatial understanding is\nencoded implicitly. We can infuse the depth and 3D structure\ninto the weights with an auxiliary task [34, 45], capturing the\nrelational information. In our work, we address the spatial\nrelationship problem directly in the VLM, without an explicit\nunderlying scene graph. In addition to understanding relative\nrelationships in qualitative terms, we also explore estimating\nexplicit metric distance relationships between objects in a scene.\nGrounding Vision-Language Models. Large language\nmodels (LLMs) are trained on internet-scale data, making them\neffective commonsense reasoners. However, LLMs (and by\nextension VLMs) may lack the necessary grounding to",
          "original_query": "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities",
          "cleaned_query": "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "AV-Master: Dual-Path Comprehensive Perception Makes Better Audio-Visual Question Answering",
          "url": "https://arxiv.org/abs/2510.18346",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2510.18346** (cs)\n\n\\[Submitted on 21 Oct 2025\\]\n\n# Title:AV-Master: Dual-Path Comprehensive Perception Makes Better Audio-Visual Question Answering\n\nAuthors: [Jiayu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J), [Qilang Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye,+Q), [Shuo Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye,+S), [Xun Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+X), [Zihan Song](https://arxiv.org/search/cs?searchtype=author&query=Song,+Z), [Zitong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+Z)\n\nView a PDF of the paper titled AV-Master: Dual-Path Comprehensive Perception Makes Better Audio-Visual Question Answering, by Jiayu Zhang and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2510.18346) [HTML (experimental)](https://arxiv.org/html/2510.18346v1)\n\n> Abstract:Audio-Visual Question Answering (AVQA) requires models to effectively utilize both visual and auditory modalities to answer complex and diverse questions about audio-visual scenes. However, existing methods lack sufficient flexibility and dynamic adaptability in temporal sampling and modality preference awareness, making it difficult to focus on key information based on the question. This limits their reasoning capability in complex scenarios. To address these challenges, we propose a novel framework named AV-Master. It enhances the model's ability to extract key information from complex audio-visual scenes with substantial redundant content by dynamically modeling both temporal and modality dimensions. In the temporal dimension, we introduce a dynamic adaptive focus sampling mechanism that progressively focuses on audio-visual segments most relevant to the question, effectively mitigating redundancy and segment fragmentation in traditional sampling methods. In the modality dimension, we propose a preference-aware strategy that models each modality's contribution independently, enabling selective activation of critical features. Furthermore, we introduce a dual-path contrastive loss to reinforce consistency and complementarity across temporal and modality dimensions, guiding the model to learn question-specific cross-modal collaborative representations. Experiments on four large-scale benchmarks show that AV-Master significantly outperforms existing methods, especially in complex reasoning tasks.\n\n| | |\n| --- | --- |\n| Comments: | 13 pages, 9 figures |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2510.18346](https://arxiv.org/abs/2510.18346) \\[cs.CV\\] |\n| (or [arXiv:2510.18346v1](https://arxiv.org/abs/2510.18346v1) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2510.18346](https://doi.org/10.48550/arXiv.2510.18346) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jiayu Zhang \\[ [view email](https://arxiv.org/show-email/817df841/2510.18346)\\] **\\[v1\\]**\nTue, 21 Oct 2025 06:58:34 UTC (1,389 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled AV-Master: Dual-Path Comprehensive Perception Makes Better Audio-Visual Question Answering, by Jiayu Zhang and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2510.18346)\n- [HTML (experimental)](https://arxiv.org/html/2510.18346v1)\n- [TeX Source](https://arxiv.org/src/2510.18346)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2510.18346&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2510.18346&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2025-10](https://arxiv.org/list/cs.CV/2025-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2510.18346?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2510.18346)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2510.18346)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2510.18346)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2510.18346) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Learning to answer questions in dynamic audio-visual scenarios",
          "cleaned_query": "Learning to answer questions in dynamic audio-visual scenarios",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Thinking in Space: How Multimodal Large Language Models ... - arXiv",
          "url": "https://arxiv.org/html/2412.14171",
          "content": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces\n# Thinking in Space: How Multimodal Large Language Models\nSee, Remember, and Recall Spaces\nJihan Yang1111Equal contribution.Shusheng Yang1\u2217Anjali W. Gupta1\u2217Rilyn Han2\u2217Li Fei-Fei3Saining Xie1\n1New York University2Yale University3Stanford University\n![[Uncaptioned image]](x1.png)[Project Page](https://vision-x-nyu.github.io/thinking-in-space.github.io/)![[Uncaptioned image]](x2.png)[Evaluation Code](https://github.com/vision-x-nyu/thinking-in-space)![[Uncaptioned image]](x3.png)[VSI-Bench](https://huggingface.co/datasets/nyu-visionx/VSI-Bench)\n###### Abstract\nHumans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also \u201cthink in space\u201d from videos?\nWe present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive\u2014though subhuman\u2014visual-spatial intelligence.\nWe probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models.\nNotably, prevailing linguistic reasoning techniques (*e.g*., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs\u2019 spatial distance ability.\n![[Uncaptioned image]](x4.png)\nFigure 1:Whether at home, in the workplace, or elsewhere, the ability to perceive a space, remember its layout, and retrieve this spatial information to answer questions on demand is a key aspect of visual-spatial intelligence. Recent Multimodal LLMs can understand general videos, but can they \u201cthink spatially\u201d when presented with a video recording of an environment? Can they build an accurate, implicit \u201c*cognitive map*\u201d that allows them to answer questions about a space? What are the strengths and limitations of using MLLMs to enhance spatial intelligence? We dig into these questions by setting up video data for MLLMs to watch, building a VQA benchmark to check their recall, and examining what the MLLMs actually remember and understand.\n## 1Introduction\nWhen shopping for furniture, we often try to recall our living room to imagine if a desired cabinet will fit. Estimating distances is difficult, yet after even a single viewing, humans can mentally reconstruct spaces, recalling objects in a room, their positions, and sizes. We live in a sensory-rich 3D world where visual signals surround and ground us, allowing us to perceive, understand, and interact with it.\nVisual-spatial intelligence entails perceiving and mentally manipulating spatial relationships> [\n[> 26\n](https://arxiv.org/html/2412.14171v2#bib.bib26)> ]\n; it requires myriad capabilities, including relational reasoning and the ability to transform between egocentric and allocentric perspectives ([Sec.2](https://arxiv.org/html/2412.14171v2#S2)). While Large Language Models (LLMs)> [\n[> 61\n](https://arxiv.org/html/2412.14171v2#bib.bib61)> , [> 88\n](https://arxiv.org/html/2412.14171v2#bib.bib88)> , [> 6\n](https://arxiv.org/html/2412.14171v2#bib.bib6)> , [> 105\n](https://arxiv.org/html/2412.14171v2#bib.bib105)> , [> 36\n](https://arxiv.org/html/2412.14171v2#bib.bib36)> , [> 67\n](https://arxiv.org/html/2412.14171v2#bib.bib67)> , [> 68\n](https://arxiv.org/html/2412.14171v2#bib.bib68)> , [> 9\n](https://arxiv.org/html/2412.14171v2#bib.bib9)> , [> 81\n](https://arxiv.org/html/2412.14171v2#bib.bib81)> , [> 82\n](https://arxiv.org/html/2412.14171v2#bib.bib82)> , [> 3\n](https://arxiv.org/html/2412.14171v2#bib.bib3)> , [> 77\n](https://arxiv.org/html/2412.14171v2#bib.bib77)> ]\nhave advanced linguistic intelligence, visual-spatial intelligence remains under-explored, despite its relevance to robotics> [\n[> 21\n](https://arxiv.org/html/2412.14171v2#bib.bib21)> , [> 8\n](https://arxiv.org/html/2412.14171v2#bib.bib8)> , [> 7\n](https://arxiv.org/html/2412.14171v2#bib.bib7)> , [> 64\n](https://arxiv.org/html/2412.14171v2#bib.bib64)> ]\n, autonomous driving> [\n[> 79\n](https://arxiv.org/html/2412.14171v2#bib.bib79)> ]\n, and AR/VR> [\n[> 12\n](https://arxiv.org/html/2412.14171v2#bib.bib12)> , [> 27\n](https://arxiv.org/html/2412.14171v2#bib.bib27)> , [> 55\n](https://arxiv.org/html/2412.14171v2#bib.bib55)> ]\n.\nMultimodal Large Language Models (MLLMs)> [\n[> 34\n](https://arxiv.org/html/2412.14171v2#bib.bib34)> , [> 78\n](https://arxiv.org/html/2412.14171v2#bib.bib78)> , [> 49\n](https://arxiv.org/html/2412.14171v2#bib.bib49)> , [> 1\n](https://arxiv.org/html/2412.14171v2#bib.bib1)> , [> 42\n](https://arxiv.org/html/2412.14171v2#bib.bib42)> , [> 49\n](https://arxiv.org/html/2412.14171v2#bib.bib49)> , [> 4\n](https://arxiv.org/html/2412.14171v2#bib.bib4)> , [> 15\n](https://arxiv.org/html/2412.14171v2#bib.bib15)> ]\n, which integrate language and vision, exhibit strong capacities to think and reason in open-ended dialog and practical tasks like web agents> [\n[> 35\n](https://arxiv.org/html/2412.14171v2#bib.bib35)> , [> 28\n](https://arxiv.org/html/2412.14171v2#bib.bib28)> , [> 33\n](https://arxiv.org/html/2412.14171v2#bib.bib33)> , [> 21\n](https://arxiv.org/html/2412.14171v2#bib.bib21)> ]\n.\nTo advance this intelligence in the visual-spatial realm, we introduceVSI-Bench, a video-based benchmark featuring over 5,000 question-answer pairs across nearly 290 real indoor-scene videos ([Sec.3](https://arxiv.org/html/2412.14171v2#S3)). Video data, by capturing continuous, temporal input, both parallels how we observe the world and enables richer spatial understanding and reasoning than static images. Evaluating open- and closed-source models onVSI-Benchreveals that even though a large performance gap exists between models and humans, MLLMs exhibit emerging visual-spatial intelligence despite the challenges of video understanding, textual understanding, and spatial reasoning ([Sec.4](https://arxiv.org/html/2412.14171v2#S4)).\nTo analyze model behavior and inspired by dual-coding theory> [\n[> 18\n](https://arxiv.org/html/2412.14171v2#bib.bib18)> ]\n, which posits that linguistic and visual processing are distinct yet complementary, we prompt selected models for self-explanations (linguistic) and cognitive maps (visual). Analyzing the self-explanations reveals that spatial reasoning, as compared to visual perception, linguistic intelligence, or temporal processing, is the main factor behind weak performance onVSI-Bench([Sec.5](https://arxiv.org/html/2412.14171v2#S5)). \u201c*Cognitive maps*\u201d, which represent internal layouts of environments> [\n[> 80\n](https://arxiv.org/html/2412.14171v2#bib.bib80)> , [> 62\n](https://arxiv.org/html/2412.14171v2#bib.bib62)> ]\n, allow us to evaluate MLLMs\u2019 implicit spatial world models and find that MLLMs build strong local models but weak global ones ([Sec.6](https://arxiv.org/html/2412.14171v2#S6)). Furthermore, standard linguistic reasoning techniques fail to enhance performance on our benchmark. However, explicitly generating and using cognitive maps improves spatial distance question-answering.\n![Refer to caption](x5.png)Figure 2:A taxonomy ofvisual-spatial intelligencecapabilities.\nExpressing visual-spatial intelligence is difficult (and often piecemeal), even for humans> [\n[> 26\n](https://arxiv.org/html/2412.14171v2#bib.bib26)> ]\n.\nWith this work, we aim to encourage the community to explore grounding frontier models with visual-spatial intelligence and to pave and illuminate this direction.\n## 2Visual-Spatial Intelligence\nWe discuss preliminaries and scope visual-spatial intelligence to provide context and a framework for later analysis.\nTerm Use.We use \u201cintelligence\u201d rather than \u201ccognition\u201d as it is broader, and \u201cspatial cognition\u201d is a branch of cognitive psychology> [\n[> 83\n](https://arxiv.org/html/2412.14171v2#bib.bib83)> ]\n. We prefix spatial intelligence in our work with \u201cvisual\u201d, as spatial intelligence exists irrespective of sensory modality (*e.g*., a blind person can perceive space through other senses)> [\n[> 26\n](https://arxiv.org/html/2412.14171v2#bib.bib26)> ]\n. Given our focus on video input, we discussvisual-spatial intelligence.\nInvestigation Scope.While classic spatial intelligence tests also include pen-paper tasks like the Mental Rotation Test> [\n[> 74\n](https://arxiv.org/html/2412.14171v2#bib.bib74)> ]\n, our focus is on visual-spatial intelligence as it applies to real-world environments, particularly in common spaces like homes, offices, and factories.\nTaxonomy.We provide a taxonomy of capabilities potentially required for visual-spatial intelligence ([Fig.2](https://arxiv.org/html/2412.14171v2#S1.F2)), based on cognitive psychology> [\n[> 26\n](https://arxiv.org/html/2412.14171v2#bib.bib26)> , [> 57\n](https://arxiv.org/html/2412.14171v2#bib.bib57)> , [> 62\n](https://arxiv.org/html/2412.14171v2#bib.bib62)> , [> 11\n](https://arxiv.org/html/2412.14171v2#bib.bib11)> ]\nand human experience with our benchmark tasks in[Sec.3](https://arxiv.org/html/2412.14171v2#S3). Visual perception, linguistic intelligence, temporal processing, and spatial reasoning are the four areas needed inVSI-Bench. For example,> [\n[> 11\n](https://arxiv.org/html/2412.14171v2#bib.bib11)> ]\nshows that visual object and spatial processing are neurally distinct, which motivates \u201cvisual perception\u201d and \u201cspatial reasoning\u201d as separate areas.\nWe break spatial reasoning into two broad capabilities: relational reasoning and egocentric-allocentric transformation.\n![Refer to caption](x6.png)Figure 3:Tasks demonstration ofVSI-Bench. Note: the questions above are simplified slightly for clarity and brevity.\nRelational reasoningis the ability to identify, via distance and direction, relationships between objects. It also encompasses reasoning about distance between objects by ",
          "original_query": "Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces",
          "cleaned_query": "Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces",
          "content_quality": "good_from_html"
        }
      ],
      "generated_ideas": [
        "Reference-Frame Aware SpatialVLM Pretraining\nTrain a SpatialVLM variant with synthetic VQA that explicitly labels/varies *egocentric vs allocentric* frames and *local vs global* environmental axes (e.g., mat-aligned vs room-wall-aligned), mirroring the manipulations in human memory experiments. Evaluate whether the model exhibits \u201calignment effects\u201d (better answers for headings parallel to experienced views) on VSI-Bench-style recall questions and controlled \u201cjudgment of relative direction\u201d prompts.",
        "Alignment-Conflict Benchmark for MLLM Spatial Memory\nBuild a new benchmark derived from VSI-Bench videos where local reference cues (e.g., rug/rectangular table) are systematically *misaligned* with global cues (room geometry), and camera exploration includes both aligned and misaligned study views. Measure whether models preferentially encode aligned views and under-represent misaligned ones, reproducing (or failing to reproduce) the human bias reported in Paper",
        "Audio-Anchored Spatial Reference Frames for AVQA\nAugment AV-Master with an \u201cacoustic landmark\u201d pathway that estimates coarse source direction/distance (e.g., TV, running water, speaker) and uses it as a stable reference axis when visual geometry is ambiguous. Create AVQA questions like \u201cIs the chair closer to the ringing phone than to the sink?\u201d and test whether audio-derived anchors reduce spatial errors under occlusion or motion blur.",
        "Uncertainty-Calibrated Distance and Clearance Estimation\nExtend SpatialVLM\u2019s metric outputs by training it to produce calibrated uncertainty intervals (e.g.,",
        "56m \u00b1 0.18m) using synthetic data with known noise models (camera intrinsics ambiguity, depth scale drift). Validate calibration on real VSI-Bench scenes and robotics-style passability questions, enabling safer downstream planning and more informative \u201cI\u2019m not sure\u201d behavior.\n",
        "Cross-View Consistency Loss for Spatial Memory Consolidation\nIntroduce a training objective for video MLLMs where predicted object maps from different subsequences (early vs late frames) must agree after estimated camera transformations, analogous to consolidating multiple study views into a stable memory. Test whether this reduces the \u201cfragmented map\u201d failure mode on VSI-Bench and improves relative-direction judgments for novel headings.",
        "Reference-System Probing via Counterfactual Scene Edits\nCreate counterfactual variants of the same scene/video by rotating only local cues (e.g., rug orientation) while keeping global room geometry fixed, and vice versa, then probe how model answers change. This yields a causal test of whether the model\u2019s spatial reasoning is grounded in local vs global reference systems, and whether it can resist spurious cue dominance."
      ],
      "generated_ideas_raw": "1. **Reference-Frame Aware SpatialVLM Pretraining**\n   Train a SpatialVLM variant with synthetic VQA that explicitly labels/varies *egocentric vs allocentric* frames and *local vs global* environmental axes (e.g., mat-aligned vs room-wall-aligned), mirroring the manipulations in human memory experiments. Evaluate whether the model exhibits \u201calignment effects\u201d (better answers for headings parallel to experienced views) on VSI-Bench-style recall questions and controlled \u201cjudgment of relative direction\u201d prompts.\n\n2. **Alignment-Conflict Benchmark for MLLM Spatial Memory**\n   Build a new benchmark derived from VSI-Bench videos where local reference cues (e.g., rug/rectangular table) are systematically *misaligned* with global cues (room geometry), and camera exploration includes both aligned and misaligned study views. Measure whether models preferentially encode aligned views and under-represent misaligned ones, reproducing (or failing to reproduce) the human bias reported in Paper 1.\n\n3. **Cognitive-Map Generation with Learned Axes Selection**\n   Extend \u201cexplicitly generating cognitive maps\u201d (Paper 4) by adding a differentiable module that predicts the most salient reference axes (local/global) from video and uses them to canonicalize the map. Train with auxiliary losses that reward consistent object coordinates under camera rotation, and test whether axis selection improves distance estimation and relative-direction queries beyond map generation alone.\n\n4. **Audio-Anchored Spatial Reference Frames for AVQA**\n   Augment AV-Master with an \u201cacoustic landmark\u201d pathway that estimates coarse source direction/distance (e.g., TV, running water, speaker) and uses it as a stable reference axis when visual geometry is ambiguous. Create AVQA questions like \u201cIs the chair closer to the ringing phone than to the sink?\u201d and test whether audio-derived anchors reduce spatial errors under occlusion or motion blur.\n\n5. **Dynamic Viewpoint Sampling for Metric 3D Accuracy**\n   Replace AV-Master\u2019s question-driven temporal focus sampling with a *geometry-driven* sampler that selects frames maximizing parallax and coverage of salient reference cues (walls, floor edges, rectangular rugs). Fine-tune on SpatialVLM synthetic metric QA and evaluate improvements on metric distance/clearance queries (e.g., \u201ccan a 1m robot pass here?\u201d) from sparse video.\n\n6. **Uncertainty-Calibrated Distance and Clearance Estimation**\n   Extend SpatialVLM\u2019s metric outputs by training it to produce calibrated uncertainty intervals (e.g., 1.56m \u00b1 0.18m) using synthetic data with known noise models (camera intrinsics ambiguity, depth scale drift). Validate calibration on real VSI-Bench scenes and robotics-style passability questions, enabling safer downstream planning and more informative \u201cI\u2019m not sure\u201d behavior.\n\n7. **Cross-View Consistency Loss for Spatial Memory Consolidation**\n   Introduce a training objective for video MLLMs where predicted object maps from different subsequences (early vs late frames) must agree after estimated camera transformations, analogous to consolidating multiple study views into a stable memory. Test whether this reduces the \u201cfragmented map\u201d failure mode on VSI-Bench and improves relative-direction judgments for novel headings.\n\n8. **Reference-System Probing via Counterfactual Scene Edits**\n   Create counterfactual variants of the same scene/video by rotating only local cues (e.g., rug orientation) while keeping global room geometry fixed, and vice versa, then probe how model answers change. This yields a causal test of whether the model\u2019s spatial reasoning is grounded in local vs global reference systems, and whether it can resist spurious cue dominance.\n\n9. **Embodied Navigation Reward Models from Spatial-VLM + Cognitive Maps**\n   Use SpatialVLM\u2019s metric reasoning and Paper 4\u2019s cognitive-map generation to build a reward/success detector for embodied tasks: given egocentric observations and a language goal (\u201cgo to the door behind the sofa\u201d), score candidate trajectories by predicted clearance, distance-to-goal, and map consistency. Evaluate in simulation on cluttered indoor navigation with narrow passages where metric errors are costly.\n\n10. **Human\u2013Model Comparative Study of View-Alignment Effects in Video**\n   Design a paired experiment where humans and MLLMs watch the same controlled exploration videos (1\u20133 study views) and answer identical \u201cimagine you are at X facing Y\u201d relative-direction questions plus metric distance estimates. Quantify where model behavior matches human alignment biases (Paper 1) versus where it diverges, producing targeted training curricula (e.g., more misaligned-view data) to close specific gaps.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Reference-Frame Aware SpatialVLM Pretraining\nTrain a SpatialVLM variant with synthetic VQA that explicitly labels/varies *egocentric vs allocentric* frames and *local vs global* environmental axes (e.",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Alignment-Conflict Benchmark for MLLM Spatial Memory\nBuild a new benchmark derived from VSI-Bench videos where local reference cues (e.g., rug/rectangular table) are systematically *misaligned* with g",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Audio-Anchored Spatial Reference Frames for AVQA\nAugment AV-Master with an \u201cacoustic landmark\u201d pathway that estimates coarse source direction/distance (e.g., TV, running water, speaker) and uses it as",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Uncertainty-Calibrated Distance and Clearance Estimation\nExtend SpatialVLM\u2019s metric outputs by training it to produce calibrated uncertainty intervals (e.g.,",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "56m \u00b1 0.18m) using synthetic data with known noise models (camera intrinsics ambiguity, depth scale drift). Validate calibration on real VSI-Bench scenes and robotics-style passability questions, enab",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Cross-View Consistency Loss for Spatial Memory Consolidation\nIntroduce a training objective for video MLLMs where predicted object maps from different subsequences (early vs late frames) must agree af",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Reference-System Probing via Counterfactual Scene Edits\nCreate counterfactual variants of the same scene/video by rotating only local cues (e.g., rug orientation) while keeping global room geometry fi",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 66,
      "paper_title": "A multiscale analysis of mean-field transformers in the moderate interaction regime",
      "contribution": "The paper provides a multiscale framework to analyze the dynamics of tokens in transformer models by treating them as mean-field interacting particles, especially in the moderate interaction regime.",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "quality_content": 4,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 8747,
      "output_tokens": 1033,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1706.03762] Attention Is All You Need - arXiv",
          "url": "https://arxiv.org/abs/1706.03762",
          "content": "[1706.03762] Attention Is All You Need[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.03762\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1706.03762**(cs)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03762v1)), last revised 2 Aug 2023 (this version, v7)]\n# Title:Attention Is All You Need\nAuthors:[Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani,+A),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+N),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Llion Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+L),[Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gomez,+A+N),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[Illia Polosukhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Polosukhin,+I)\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n[View PDF](https://arxiv.org/pdf/1706.03762)[HTML (experimental)](https://arxiv.org/html/1706.03762v7)> > Abstract:\n> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments:|15 pages, 5 figures|\nSubjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)[cs.CL]|\n|(or[arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Llion Jones [[view email](https://arxiv.org/show-email/f53b7360/1706.03762)]\n**[[v1]](https://arxiv.org/abs/1706.03762v1)**Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\n**[[v2]](https://arxiv.org/abs/1706.03762v2)**Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.03762v3)**Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\n**[[v4]](https://arxiv.org/abs/1706.03762v4)**Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\n**[[v5]](https://arxiv.org/abs/1706.03762v5)**Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\n**[[v6]](https://arxiv.org/abs/1706.03762v6)**Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\n**[v7]**Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03762)\n* [HTML (experimental)](https://arxiv.org/html/1706.03762v7)\n* [TeX Source](https://arxiv.org/src/1706.03762)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03762&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03762&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2017-06](https://arxiv.org/list/cs.CL/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03762?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.03762?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03762)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03762)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03762)\n### [123 blog links](https://arxiv.org/tb/1706.03762)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17)\n[Ashish Vaswani]()\n[Noam Shazeer]()\n[Niki Parmar]()\n[Jakob Uszkoreit]()\n[Llion Jones]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1706.03762)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Attention is all you need",
          "cleaned_query": "Attention is all you need",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Sinkformers: Transformers with Doubly Stochastic Attention - arXiv",
          "url": "https://arxiv.org/abs/2110.11773",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: Attention based models such as Transformers involve pairwise interactions\nbetween data points, modeled with a learnable attention matrix. Importantly,\nthis attention matrix is normalized with the SoftMax operator, which makes it\nrow-wise stochastic. In this paper, we propose instead to use Sinkhorn's\nalgorithm to make attention matrices doubly stochastic. We call the resulting\nmodel a Sinkformer. We show that the row-wise stochastic attention matrices in\nclassical Transformers get close to doubly stochastic matrices as the number of\nepochs increases, justifying the use of Sinkhorn normalization as an\ninformative prior. On the theoretical side, we show that, unlike the SoftMax\noperation, this normalization makes it possible to understand the iterations of\nself-attention modules as a discretized gradient-flow for the Wasserstein\nmetric. We also show in the infinite number of samples limit that, when\nrescaling both attention matrices and depth, Sinkformers operate a heat\ndiffusion. On the experimental side, we show that Sinkformers enhance model\naccuracy in vision and natural language processing tasks. In particular, on 3D\nshapes classification, Sinkformers lead to a significant improvement.\n \n \n \n \n Submission history From: Michael E. Sander [ view email]\n \n [v1] \n Fri, 22 Oct 2021 13:25:01 UTC (1,110 KB) [v2] \nMon, 24 Jan 2022 15:09:34 UTC (1,115 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > cs > arXiv:2110.11773\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:2110.11773 (cs)\n [Submitted on 22 Oct 2021 (v1), last revised 24 Jan 2022 (this version, v2)]\n\n Title: Sinkformers: Transformers with Doubly Stochastic Attention\n\n Authors: Michael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyr\u00e9\n Download PDF\n Abstract: Attention based models such as Transformers involve pairwise interactions between data points, modeled with a learnable attention matrix. Importantly, this attention matrix is normalized with the SoftMax operator, which makes it row-wise stochastic. In this paper, we propose instead to use Sinkhorn's algorithm to make attention matrices doubly stochastic. We call the resulting model a Sinkformer. We show that the row-wise stochastic attention matrices in classical Transformers get close to doubly stochastic matrices as the number of epochs increases, justifying the use of Sinkhorn normalization as an informative prior. On the theoretical side, we show that, unlike the SoftMax operation, this normalization makes it possible to understand the iterations of self-attention modules as a discretized gradient-flow for the Wasserstein metric. We also show in the infinite number of samples limit that, when rescaling both attention matrices and depth, Sinkformers operate a heat diffusion. On the experimental side, we show that Sinkformers enhance model accuracy in vision and natural language processing tasks. In particular, on 3D shapes classification, Sinkformers lead to a significant improvement.\n Comments: Accepted at AISTATS \n Subjects: Machine Learning (cs.LG) ; Machine Learning (stat.ML)\n Cite as: arXiv:2110.11773 [cs.LG] \n (or arXiv:2110.11773v2 [cs.LG] for this version) \n https://doi.org/10.48550/arXiv.2110.11773 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Michael E. Sander [view email]\n [v1] Fri, 22 Oct 2021 13:25:01 UTC (1,110 KB)\n [v2] Mon, 24 Jan 2022 15:09:34 UTC (1,115 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 2110\n Change to browse by:\n cs\n stat\n stat.ML\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n\n DBLP - CS Bibliography\n\n listing | bibtex\n Pierre Ablin\n Mathieu Blondel\n Gabriel Peyr\u00e9\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Sinkformers: Transformers with doubly stochastic attention",
          "cleaned_query": "Sinkformers: Transformers with doubly stochastic attention",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2305.05465] The emergence of clusters in self-attention dynamics",
          "url": "https://arxiv.org/abs/2305.05465",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2305.05465** (cs)\n\n\\[Submitted on 9 May 2023 ( [v1](https://arxiv.org/abs/2305.05465v1)), last revised 12 Feb 2024 (this version, v6)\\]\n\n# Title:The emergence of clusters in self-attention dynamics\n\nAuthors: [Borjan Geshkovski](https://arxiv.org/search/cs?searchtype=author&query=Geshkovski,+B), [Cyril Letrouit](https://arxiv.org/search/cs?searchtype=author&query=Letrouit,+C), [Yury Polyanskiy](https://arxiv.org/search/cs?searchtype=author&query=Polyanskiy,+Y), [Philippe Rigollet](https://arxiv.org/search/cs?searchtype=author&query=Rigollet,+P)\n\nView a PDF of the paper titled The emergence of clusters in self-attention dynamics, by Borjan Geshkovski and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2305.05465) [HTML (experimental)](https://arxiv.org/html/2305.05465v6)\n\n> Abstract:Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. Cluster locations are determined by the initial tokens, confirming context-awareness of representations learned by Transformers. Using techniques from dynamical systems and partial differential equations, we show that the type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \\[VSP'17\\] that leaders appear in a sequence of tokens when processed by Transformers.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Analysis of PDEs (math.AP); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2305.05465](https://arxiv.org/abs/2305.05465) \\[cs.LG\\] |\n| (or [arXiv:2305.05465v6](https://arxiv.org/abs/2305.05465v6) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2305.05465](https://doi.org/10.48550/arXiv.2305.05465) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Borjan Geshkovski \\[ [view email](https://arxiv.org/show-email/080722b4/2305.05465)\\] **[\\[v1\\]](https://arxiv.org/abs/2305.05465v1)**\nTue, 9 May 2023 14:04:42 UTC (70,588 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2305.05465v2)**\nWed, 17 May 2023 15:16:55 UTC (50,995 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2305.05465v3)**\nTue, 17 Oct 2023 11:20:44 UTC (28,354 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2305.05465v4)**\nMon, 8 Jan 2024 07:48:21 UTC (28,355 KB)\n**[\\[v5\\]](https://arxiv.org/abs/2305.05465v5)**\nTue, 6 Feb 2024 07:30:11 UTC (28,355 KB)\n**\\[v6\\]**\nMon, 12 Feb 2024 10:21:08 UTC (28,355 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The emergence of clusters in self-attention dynamics, by Borjan Geshkovski and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2305.05465)\n- [HTML (experimental)](https://arxiv.org/html/2305.05465v6)\n- [TeX Source](https://arxiv.org/src/2305.05465)\n- [Other Formats](https://arxiv.org/format/2305.05465)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2305.05465&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2305.05465&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-05](https://arxiv.org/list/cs.LG/2023-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2305.05465?context=cs) [math](https://arxiv.org/abs/2305.05465?context=math) [math.AP](https://arxiv.org/abs/2305.05465?context=math.AP) [stat](https://arxiv.org/abs/2305.05465?context=stat) [stat.ML](https://arxiv.org/abs/2305.05465?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2305.05465)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2305.05465)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2305.05465)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2305.05465) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "The emergence of clusters in self-attention dynamics",
          "cleaned_query": "The emergence of clusters in self-attention dynamics",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] arXiv:2504.14697v2 [cs.LG] 30 Apr 2025",
          "url": "https://arxiv.org/pdf/2504.14697?",
          "content": "QUANTITATIVE CLUSTERING IN MEAN-FIELD\nTRANSFORMER MODELS\nSHI CHEN, ZHENGJIANG LIN, YURY POLYANSKIY, AND PHILIPPE RIGOLLET\nAbstract. The evolution of tokens through a deep transformer models can\nbe modeled as an interacting particle system that has been shown to exhibit\nan asymptotic clustering behavior akin to the synchronization phenomenon\nin Kuramoto models. In this work, we investigate the long-time clustering\nof mean-field transformer models. More precisely, we establish exponential\nrates of contraction to a Dirac point mass for any suitably regular initializa\u0002tion under some assumptions on the parameters of transformer models, any\nsuitably regular mean-field initialization synchronizes exponentially fast with\nsome quantitative rates.\nContents\n1. Introduction 1\n2. Clustering in mean-field attention dynamics 3\n3. Clustering in general transformer models 8\n3.1. Critical Points 9\n3.2. Long Time Behavior 13\n4. \u0141ojasiewicz type inequality: Proof of Theorem 3.5 and Theorem 2.3 17\n5. Some Basic Derivatives and Estimates for the Proof of Theorem 3.6 23\n6. Almost Kuramoto Model: Proof of Theorem 3.6 25\n6.1. Rt is almost increasing 26\n6.2. Measures on Negative Caps Decrease Exponentially Fast 30\n7. More accurate C0, T0 in Theorem 3.4: Proof of Theorem 3.8 35\nReferences 46\n1. Introduction\nThe (self-)attention mechanism, initially introduced by [BCB15], forms the foun\u0002dation of the transformer architecture developed in [Vas17]. This revolutionary ar\u0002chitecture has become fundamental for large language models (LLMs), catalyzing\nremarkable advances in artificial intelligence.\nRecently, [GLPR25] proposed to study how a deep stack of attention layers\nprocesses information as a mean-field interacting particle system on the sphere S\nd\u00b41\nthat exhibits long-time clustering properties; see also [SABP22, GLPR24, KPR24,\nKBH24, CRMB24, GKPR24, BPA25, AST24, BKK`25, CACP25].\nThis model\u2014called attention dynamics\u2014captures the representation of tokens\nas they evolve through the successive layers of a transformer. In particular, the\n1\narXiv:2504.14697v2 [cs.LG] 30 Apr 2025\n2 S. CHEN, Z. LIN, Y. POLYANSKIY, AND P. RIGOLLET\nclustering phenomenon put forward in [GLPR24, GLPR25] is critical to under\u0002standing the structure of internal representations for these pervasive models. More\nspecifically, in attention dynamics, n tokens x1, . . . , xn P S\nd\u00b41\nevolve as\n(1.1) x9 iptq \u201c Pxiptq\n\u201c 1\nn\n\u00ffn\nj\u201c1\nxj ptqe\n\u03b2xxiptq,xj ptqy\u2030\nt \u011b 0, i \u201c 1, . . . , n ,\nwhere \u03b2 \u011b 0, Pxrys :\u201c y \u00b4 xx, yy x denotes the projection of y P IRd onto the\nhyperplane TxS\nd\u00b41\ntangent to the sphere at x P S\nd\u00b41\n. We refer to [GLPR25] for\na derivation of this model and its relationship to the attention mechanism and\nlayer normalization. When d \u201c 2 and \u03b2 \u201c 0 attention dynamics coincide with\nthe well-known Kuramoto model [Kur75, KK84, ABPV`05]. It was observed and\ndemonstrated in various situations that these tokens converge to a single token:\nxiptq \u00d1 x8 as t \u00d1 8. This phenomenon is called synchronization or simply\nclustering and we use these terms interchangeably.\nNote that the system of ODEs (1.1) is of the mean-field type. Indeed token i\ninteract with all tokens only through their empirical distribution at time t. We\ndenote this distribution by \u00b5t and recall that\n\u00b5t \u2013\n1\nn\n\u00ffn\ni\u201c1\n\u03b4xiptq.\nIn turn, the evolution of \u00b5t is governed by the continuity equation\nBt\u00b5t ` div\u02da p\u00b5tX\u00b5t,\u03b2q \u201c 0 , X\u00b5t,\u03b2pxq \u2013\n\u017c\nS\nd\u00b41\nPxryse\n\u03b2xx,yy\n(1.2) d\u00b5tpyq,\nwhere here and throughout the paper div\u02da \u201c div\u02da S\nd\u00b41 denotes the divergence operator\non the sphere.\nAs pointed out in [GLPR25], equation (1.2) describes a Wasserstein gradient\nflow that aims to maximize the functional\n(1.3) \u00b5 \u00de\u00d1 E\u03b2r\u00b5s \u2013\n1\n2\u03b2\n\u0133\ne\n\u03b2xx,yy d\u00b5pxq d\u00b5pyq,\nwhere both integrals are over S\nd\u00b41\n. It is easy to see that E is maximized at Dirac\npoint masses \u03b4x0for some x0 P S\nd\u00b41\n. This maximum energy state corresponds to a\nclustering of the tokens into a single point. Thanks to these observations, clustering\nof n tokens hinges on three classical tools from finite dimensional dynamical systems\ntheory: the dynamics for the the n-tuple px1ptq, . . . , xnptqq P pS\nd\u00b41\nq\nn can be shown\nto (i) converge by the \u0141ojasiewicz inequality, and (ii) avoid saddle points from\nalmost every initialization by the center-stable manifold theorem. Moreover, all\nstationary points are saddle points except for the global maximizers where x1 \u201c\n\u00a8 \u00a8 \u00a8 \u201c xn; [GLPR25, KPR24, CRMB24, MTG17].\nThe number n of tokens that can be processed simultaneously by a transformer\nmodel is called context length and scaling up its value is a major engineering en\u0002deavor because of its direct impact on performance\u2014current frontier models handle\ncontexts with millions of tokens [Goo24]. However, past work on attention dynam\u0002ics has largely focused on studying asymptotics where t \u00d1 8 and n remains finite\nimplicitly assuming that n ! t.\nIn this work, we investigate clustering properties for a continuum of tokens cor\u0002responding to n \u201c 8. The mean-field dynamics of the measure \u00b5t of tokens is\nQUANTITATIVE CLUSTERING IN MEAN-FIELD TRANSFORMER MODELS 3\ngoverned by the continuity equation (1.2) but we focus on the case where it is ini\u0002tialized at a measure \u00b50 that admits a density with respect to the uniform measure\non the sphere S\nd\u00b41\n. We call1this setup mean-field attention dynamics. Despite re\u0002cent efforts [CACP25] to study convergence of the finite-particle system as n \u00d1 8,\nexisting results do not imply asymptotic clustering for the mean-field attention dy\u0002namics for lack of a convergence that is uniform in time. Our results overcome\nthis limitation by developing the infinite-dimensional tools necessary to studying\ndirectly the mean-field dynamics.\nMore precisely, our contributions are as follows. First, we show that, echoing\nthe finite-dimensional case, stationary points for (1.2) are all saddle points for the\ninteraction energy E except for global maxima given by point masses. In particular,\nour proof extends the approach of [CRMB24] by exhibiting escape directions for\ncontinuous measures. However, in the absence of a counterpart to the center-stable\nmanifold theorem in infinite dimensions, this result is not sufficient to conclude to\nclustering. In fact, while infinite-dimensional versions of the \u0141ojasiewicz inequality\nhave been developed [Sim83, CM14], we show in Remark 2.2 that such inequalities\ncannot hold in general at critical points of the interaction energy E.\nNevertheless, we demonstrate that a stronger version of the \u0141ojasiewicz inequal\u0002ity, known as the Polyak-\u0141ojasiewicz (PL) inequality, holds around point masses\nfor measures supported on a spherical cap. From such PL inequalities, it follows\nreadily that the Wasserstein gradient flow (1.2) converges exponentially fast to a\nglobal maximizer of E when initialized on these measures with constrained support.\nThis PL inequality is employed in our main contribution, Theorem 2.4, which\nestablishes exponential rates of convergence for the mean-field attention dynam\u0002ics (1.2) initialized at any density f0 P L\n2\npS\nd\u00b41\nq for sufficiently small temperature\nparameter \u03b2 \u0103 \u03b20, where \u03b20 \u0105 0 depends on f0. Note that global convergence\nto point masses cannot hold at arbitrary temperatures. Indeed, for \u03b2 \u201c 100, we\nexhibit an equilibrium for mean-field attention dynamics that does not correspond\nto a single cluster in Example 2.6. This qualitative behavior is in sharp contrast\nwith the Kuramoto model where \u03b2 \u201c 0 and for which it can be proved that any\nregular initialization converges to to a point mass exponentially fast; see [MP22].\nOur main results for mean-field attention dynamics are stated in the next section.\nIn fact, these results are corollaries for our general results stated in Section 3. These\nconvergence results cover more general dynamics that correspond to less simplified\nversions of transformer models; see [GLPR25].\n2. Clustering in mean-field attention dynamics\nIn this section, we present our main clustering results on mean-field attention\ndynamics (1.2).\nRecall from [GLPR25] that the mean-field attention dynamics form a reverse\nWasserstein gradient flow of the interaction energy E\u03b2 defined in (1.3): X\u00b5,\u03b2 \u201c\n\u2207\u2207E\u03b2r\u00b5s\u2014see [CNWR24, AGS05] for an introduction to Wasserstein gradient flows.\n1While the term \u201cmean-field\" technically applies to the Vlasov PDE (1.2) with any initial\u0002ization, including a discrete one, it is common in the literature to use this term to denote such\nan evolution initialized at the measure that is absolutely continuous with respect to the uniform\nmeasure. To facilitate reading, we adopt the same abuse of language and use \"mean-field\" to\nindicate such an initialization.\n4 S. CHEN, Z. LIN, Y. POLYANSKIY, AND P. RIGOLLET\nIndeed, along (1.2) we have\nd\ndt\nE\u03b2r\u00b5ts \u201c \u017c\nS\nd\u00b41\n}X\u00b5t,\u03b2pxq}2\n2\n(2.1) d\u00b5tpxq \u011b 0 ,\nwith equality if and only if X\u00b5t,\u03b2pxq \u201c 0 for \u00b5t almost every x P S\nd\u00b41\n. This equality\ncase characterizes critical points of the energy E\u03b2. The next result shows that the\nonly critical points that are local maxima for E\u03b2 are in fact single point masses.\nProposition 2.1. Let d \u011b 3. For any \u03b2 \u0105 0, any local maxima of the interaction\nenergy E\u03b2 is a global maxima of the form \u00b5 \u201c \u03b4x0for some x0 P S\nd\u00b41\n.\nProposition 2.1 is a direct consequence of Theorem 3.1 and holds for more general\ntransformer models, including ones with learned parameters; see Section 3.\nWhen \u00b5 P PpS\nd\u00b41\nq consists of only a finite number of tokens, similar results on\nthe absence of nontrivial local maxima were described in [GLPR25, CRMB24] when\nd \u011b 3 following [MTG17]. Our proof of Proposition 2.1 is adapted from [CRMB24].\nWe note that this technique only applies to d \u011b 3 and the clustering of n tokens\nfor attention dynamics has been recently extended to d \u201c 2 in [PRY25] by refining\nthe strategy initiated in [GLPR25].\nAs mentioned in the introduction, this result is not sufficient to conclude to a\nglobal convergence of the mean-field attention dynamics (1.2) to a point mass in\nabsence because of the infinite dimensional nature of the problem. Nevertheles",
          "original_query": "Quantitative clustering in mean-field transformer models",
          "cleaned_query": "Quantitative clustering in mean-field transformer models",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Sinkhorn-Normalized Attention Dynamics: Mean-Field PDE and Clustering Rates\nReplace SoftMax by Sinkhorn normalization inside the mean-field attention dynamics and derive the corresponding continuum-limit PDE (tracking how doubly stochastic constraints modify the vector field/energy functional). Prove (or refute) exponential clustering under analogous assumptions to quantitative mean-field clustering, and identify how the Sinkhorn regularization strength changes contraction constants and basin geometry.",
        "Heat\u2013Synchronization Phase Diagram via Joint Depth/Temperature Scaling\nBuild a unified scaling theory that interpolates between the \u201cheat diffusion\u201d limit of Sinkformers and the \u201csynchronization to a Dirac\u201d limit in mean-field attention dynamics by jointly scaling depth, \u03b2 (attention sharpness), and Sinkhorn entropic regularization. Provide a rigorous phase diagram (diffusive vs clustering vs metastable multi-cluster regimes) and validate it by controlled experiments on synthetic token clouds and real sequence tasks.",
        "Leader Tokens as Attractors: Designing \u201cAnchor\u201d Mechanisms with Provable Basins\nFormalize \u201cleader emergence\u201d as attractor selection in the interacting-particle viewpoint and design an anchor-token mechanism (learned or inserted) that steers clustering toward semantically meaningful representatives (e.g., entity anchors, segment anchors). Prove conditions under which anchors become global attractors (or induce controlled multi-attractor behavior), and test improvements in long-context summarization and retrieval-style tasks.",
        "Finite-n to Mean-Field Uniform-in-Time Guarantees for Sinkhorn Attention\nDevelop uniform-in-time propagation-of-chaos bounds for transformers whose attention uses Sinkhorn normalization, addressing the gap noted in mean-field clustering work (finite context length vs continuum tokens). The key contribution is a non-asymptotic theorem bounding the discrepancy between n-token dynamics and the mean-field PDE over long depths, plus empirical confirmation on increasing context lengths.",
        "Wasserstein Gradient-Flow Training Regularizer for Transformers\nExploit the interpretation of attention iterations as discretized Wasserstein gradient flow (Sinkformers) and the energy-maximization view in mean-field clustering to create a regularizer that penalizes deviation from a target gradient-flow structure. Concretely, add a term matching layerwise updates to a learned potential\u2019s Wasserstein gradient (estimated from tokens), and evaluate whether this stabilizes training, improves calibration, or reduces attention pathologies.",
        "Metastability and Escape from Saddle Stationary Points in High Dimensions\nExtend the saddle-point/\u0141ojasiewicz-based convergence analysis in mean-field clustering to quantify metastable plateaus: how long token measures linger near non-maximizing stationary sets before collapsing. Provide sharp estimates of escape times as functions of dimension, \u03b2, and initialization regularity, and connect them to practical phenomena like delayed specialization of attention heads during training.",
        "Head-Coupled Mean-Field Dynamics: Multi-Head Synchronization vs Diversification\nModel multi-head attention as coupled interacting particle systems (one per head) sharing token states but different kernels/value spectra, and analyze when heads synchronize to the same cluster versus diversify into complementary clusters. Propose a coupling-aware regularizer (e.g., encourage orthogonal attractor directions across heads) and test whether it improves compositionality and reduces redundancy in attention patterns."
      ],
      "generated_ideas_raw": "1. **Sinkhorn-Normalized Attention Dynamics: Mean-Field PDE and Clustering Rates**  \n   Replace SoftMax by Sinkhorn normalization inside the mean-field attention dynamics and derive the corresponding continuum-limit PDE (tracking how doubly stochastic constraints modify the vector field/energy functional). Prove (or refute) exponential clustering under analogous assumptions to quantitative mean-field clustering, and identify how the Sinkhorn regularization strength changes contraction constants and basin geometry.\n\n2. **Heat\u2013Synchronization Phase Diagram via Joint Depth/Temperature Scaling**  \n   Build a unified scaling theory that interpolates between the \u201cheat diffusion\u201d limit of Sinkformers and the \u201csynchronization to a Dirac\u201d limit in mean-field attention dynamics by jointly scaling depth, \u03b2 (attention sharpness), and Sinkhorn entropic regularization. Provide a rigorous phase diagram (diffusive vs clustering vs metastable multi-cluster regimes) and validate it by controlled experiments on synthetic token clouds and real sequence tasks.\n\n3. **Spectral Control of Value Matrices to Prevent Collapse and Preserve Multi-Cluster Structure**  \n   Use the paper-3 result that limiting object type depends on the spectrum of the value matrix to design explicit spectral regularizers (e.g., penalize dominant eigenvalue gaps or enforce banded spectra) that maintain multiple stable clusters instead of single-point collapse. Implement this as a training-time constraint on value projections (per head/layer) and evaluate on tasks where preserving multiple entities is crucial (coreference, multi-object vision).\n\n4. **Boolean Low-Rank Attention Induction with Doubly Stochastic Constraints**  \n   Extend the 1D convergence-to-Boolean-low-rank result by studying whether doubly stochastic attention accelerates or alters the emergence of near-permutation/Boolean structure. Develop a training objective that encourages attention matrices to be simultaneously (i) approximately doubly stochastic and (ii) low-rank/Boolean, then measure effects on interpretability (leader tokens, alignment) and efficiency (sparsity/permutation factorization).\n\n5. **Leader Tokens as Attractors: Designing \u201cAnchor\u201d Mechanisms with Provable Basins**  \n   Formalize \u201cleader emergence\u201d as attractor selection in the interacting-particle viewpoint and design an anchor-token mechanism (learned or inserted) that steers clustering toward semantically meaningful representatives (e.g., entity anchors, segment anchors). Prove conditions under which anchors become global attractors (or induce controlled multi-attractor behavior), and test improvements in long-context summarization and retrieval-style tasks.\n\n6. **Finite-n to Mean-Field Uniform-in-Time Guarantees for Sinkhorn Attention**  \n   Develop uniform-in-time propagation-of-chaos bounds for transformers whose attention uses Sinkhorn normalization, addressing the gap noted in mean-field clustering work (finite context length vs continuum tokens). The key contribution is a non-asymptotic theorem bounding the discrepancy between n-token dynamics and the mean-field PDE over long depths, plus empirical confirmation on increasing context lengths.\n\n7. **Wasserstein Gradient-Flow Training Regularizer for Transformers**  \n   Exploit the interpretation of attention iterations as discretized Wasserstein gradient flow (Sinkformers) and the energy-maximization view in mean-field clustering to create a regularizer that penalizes deviation from a target gradient-flow structure. Concretely, add a term matching layerwise updates to a learned potential\u2019s Wasserstein gradient (estimated from tokens), and evaluate whether this stabilizes training, improves calibration, or reduces attention pathologies.\n\n8. **Metastability and Escape from Saddle Stationary Points in High Dimensions**  \n   Extend the saddle-point/\u0141ojasiewicz-based convergence analysis in mean-field clustering to quantify metastable plateaus: how long token measures linger near non-maximizing stationary sets before collapsing. Provide sharp estimates of escape times as functions of dimension, \u03b2, and initialization regularity, and connect them to practical phenomena like delayed specialization of attention heads during training.\n\n9. **Head-Coupled Mean-Field Dynamics: Multi-Head Synchronization vs Diversification**  \n   Model multi-head attention as coupled interacting particle systems (one per head) sharing token states but different kernels/value spectra, and analyze when heads synchronize to the same cluster versus diversify into complementary clusters. Propose a coupling-aware regularizer (e.g., encourage orthogonal attractor directions across heads) and test whether it improves compositionality and reduces redundancy in attention patterns.\n\n10. **Adaptive Sinkhorn Iterations per Layer: Accuracy\u2013Dynamics Tradeoffs and Guarantees**  \n   Treat the number of Sinkhorn iterations as a controllable discretization parameter that changes the effective dynamics (closer to doubly stochastic vs closer to SoftMax). Design an adaptive scheme that increases iterations only when the token distribution approaches regimes prone to undesirable collapse (detected via spectral/cluster diagnostics), and provide theoretical/empirical evidence for better accuracy\u2013compute tradeoffs on long-context and 3D shape benchmarks.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Sinkhorn-Normalized Attention Dynamics: Mean-Field PDE and Clustering Rates\nReplace SoftMax by Sinkhorn normalization inside the mean-field attention dynamics and derive the corresponding continuum-li",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Heat\u2013Synchronization Phase Diagram via Joint Depth/Temperature Scaling\nBuild a unified scaling theory that interpolates between the \u201cheat diffusion\u201d limit of Sinkformers and the \u201csynchronization to a ",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Leader Tokens as Attractors: Designing \u201cAnchor\u201d Mechanisms with Provable Basins\nFormalize \u201cleader emergence\u201d as attractor selection in the interacting-particle viewpoint and design an anchor-token mec",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Finite-n to Mean-Field Uniform-in-Time Guarantees for Sinkhorn Attention\nDevelop uniform-in-time propagation-of-chaos bounds for transformers whose attention uses Sinkhorn normalization, addressing th",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Wasserstein Gradient-Flow Training Regularizer for Transformers\nExploit the interpretation of attention iterations as discretized Wasserstein gradient flow (Sinkformers) and the energy-maximization vi",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Metastability and Escape from Saddle Stationary Points in High Dimensions\nExtend the saddle-point/\u0141ojasiewicz-based convergence analysis in mean-field clustering to quantify metastable plateaus: how l",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Head-Coupled Mean-Field Dynamics: Multi-Head Synchronization vs Diversification\nModel multi-head attention as coupled interacting particle systems (one per head) sharing token states but different ker",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 67,
      "paper_title": "Exploring Diffusion Transformer Designs via Grafting",
      "contribution": "The paper introduces grafting as a method for efficiently modifying pretrained diffusion transformer architectures to explore new designs without extensive retraining.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 5,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 7181,
      "output_tokens": 1058,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Denoising Diffusion Probabilistic Models - NeurIPS",
          "url": "https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf",
          "content": "Denoising Diffusion Probabilistic Models\nJonathan Ho\nUC Berkeley\njonathanho@berkeley.edu\nAjay Jain\nUC Berkeley\najayj@berkeley.edu\nPieter Abbeel\nUC Berkeley\npabbeel@cs.berkeley.edu\nAbstract\nWe present high quality image synthesis results using diffusion probabilistic models,\na class of latent variable models inspired by considerations from nonequilibrium\nthermodynamics. Our best results are obtained by training on a weighted variational\nbound designed according to a novel connection between diffusion probabilistic\nmodels and denoising score matching with Langevin dynamics, and our models nat\u0002urally admit a progressive lossy decompression scheme that can be interpreted as a\ngeneralization of autoregressive decoding. On the unconditional CIFAR10 dataset,\nwe obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On\n256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple\u0002mentation is available at https://github.com/hojonathanho/diffusion.\n1 Introduction\nDeep generative models of all kinds have recently exhibited high quality samples in a wide variety\nof data modalities. Generative adversarial networks (GANs), autoregressive models, flows, and\nvariational autoencoders (VAEs) have synthesized striking image and audio samples [14, 27, 3,\n58, 38, 25, 10, 32, 44, 57, 26, 33, 45], and there have been remarkable advances in energy-based\nmodeling and score matching that have produced images comparable to those of GANs [11, 55].\nFigure 1: Generated samples on CelebA-HQ 256 \u00d7 256 (left) and unconditional CIFAR10 (right)\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n!\nxT ! \u00b7\u00b7\u00b7 ! xt ! xt1 ! \u00b7\u00b7\u00b7 ! x0\np\u2713(xt1|xt)\nq(xt|xt1)\nFigure 2: The directed graphical model considered in this work.\nThis paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model\n(which we will call a \u201cdiffusion model\u201d for brevity) is a parameterized Markov chain trained using\nvariational inference to produce samples matching the data after finite time. Transitions of this chain\nare learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the\ndata in the opposite direction of sampling until signal is destroyed. When the diffusion consists of\nsmall amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional\nGaussians too, allowing for a particularly simple neural network parameterization.\nDiffusion models are straightforward to define and efficient to train, but to the best of our knowledge,\nthere has been no demonstration that they are capable of generating high quality samples. We\nshow that diffusion models actually are capable of generating high quality samples, sometimes\nbetter than the published results on other types of generative models (Section 4). In addition, we\nshow that a certain parameterization of diffusion models reveals an equivalence with denoising\nscore matching over multiple noise levels during training and with annealed Langevin dynamics\nduring sampling (Section 3.2) [55, 61]. We obtained our best sample quality results using this\nparameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.\nDespite their sample quality, our models do not have competitive log likelihoods compared to other\nlikelihood-based models (our models do, however, have log likelihoods better than the large estimates\nannealed importance sampling has been reported to produce for energy based models and score\nmatching [11, 55]). We find that the majority of our models\u2019 lossless codelengths are consumed\nto describe imperceptible image details (Section 4.3). We present a more refined analysis of this\nphenomenon in the language of lossy compression, and we show that the sampling procedure of\ndiffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit\nordering that vastly generalizes what is normally possible with autoregressive models.\n2 Background\nDiffusion models [53] are latent variable models of the form p\u03b8(x0) :=\nR\np\u03b8(x0:T ) dx1:T , where\nx1, . . . , xT are latents of the same dimensionality as the data x0 \u223c q(x0). The joint distribution\np\u03b8(x0:T ) is called the reverse process, and it is defined as a Markov chain with learned Gaussian\ntransitions starting at p(xT ) = N (xT ; 0, I):\np\u03b8(x0:T ) := p(xT )\nY\nT\nt=1\np\u03b8(xt\u22121|xt), p\u03b8(xt\u22121|xt) := N (xt\u22121; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t)) (1)\nWhat distinguishes diffusion models from other types of latent variable models is that the approximate\nposterior q(x1:T |x0), called the forward process or diffusion process, is fixed to a Markov chain that\ngradually adds Gaussian noise to the data according to a variance schedule \u03b21, . . . , \u03b2T :\nq(x1:T |x0) :=\nY\nT\nt=1\nq(xt|xt\u22121), q(xt|xt\u22121) := N (xt;\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI) (2)\nTraining is performed by optimizing the usual variational bound on negative log likelihood:\nE [\u2212 log p\u03b8(x0)] \u2264 Eq\n\u0014\n\u2212 log p\u03b8(x0:T )\nq(x1:T |x0)\n\u0015\n= Eq\n\u0014\n\u2212 log p(xT ) \u2212\nX\nt\u22651\nlog p\u03b8(xt\u22121|xt)\nq(xt|xt\u22121)\n\u0015\n=: L (3)\nThe forward process variances \u03b2t can be learned by reparameterization [33] or held constant as\nhyperparameters, and expressiveness of the reverse process is ensured in part by the choice of\nGaussian conditionals in p\u03b8(xt\u22121|xt), because both processes have the same functional form when\n\u03b2t are small [53]. A notable property of the forward process is that it admits sampling xt at an\narbitrary timestep t in closed form: using the notation \u03b1t\n:= 1 \u2212 \u03b2t and \u03b1\u00aft:=\nQt\ns=1 \u03b1s, we have\nq(xt|x0) = N (xt;\n\u221a\n\u03b1\u00aftx0,(1 \u2212 \u03b1\u00aft)I) (4)\n2\nEfficient training is therefore possible by optimizing random terms of L with stochastic gradient\ndescent. Further improvements come from variance reduction by rewriting L (3) as:\nEq\n\u0014\nDKL(q(xT |x0) k p(xT ))\n| {z }\nLT\n+\nX\nt>1\nDKL(q(xt\u22121|xt, x0) k p\u03b8(xt\u22121|xt))\n| {z }\nLt\u22121\n\u2212 log p\u03b8(x0|x1)\n| {z }\nL0\n\u0015\n(5)\n(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL\ndivergence to directly compare p\u03b8(xt\u22121|xt) against forward process posteriors, which are tractable\nwhen conditioned on x0:\nq(xt\u22121|xt, x0) = N (xt\u22121; \u00b5\u02dct(xt, x0), \u03b2\u02dc\ntI), (6)\nwhere \u00b5\u02dct(xt, x0) :=\n\u221a\n\u03b1\u00aft\u22121\u03b2t\n1 \u2212 \u03b1\u00aft\nx0 +\n\u221a\n\u03b1t(1 \u2212 \u03b1\u00aft\u22121)\n1 \u2212 \u03b1\u00aft\nxt and \u03b2\u02dc\nt\n:=\n1 \u2212 \u03b1\u00aft\u22121\n1 \u2212 \u03b1\u00aft\n\u03b2t (7)\nConsequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be\ncalculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance\nMonte Carlo estimates.\n3 Diffusion models and denoising autoencoders\nDiffusion models might appear to be a restricted class of latent variable models, but they allow a\nlarge number of degrees of freedom in implementation. One must choose the variances \u03b2t of the\nforward process and the model architecture and Gaussian distribution parameterization of the reverse\nprocess. To guide our choices, we establish a new explicit connection between diffusion models\nand denoising score matching (Section 3.2) that leads to a simplified, weighted variational bound\nobjective for diffusion models (Section 3.4). Ultimately, our model design is justified by simplicity\nand empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).\n3.1 Forward process and LT\nWe ignore the fact that the forward process variances \u03b2t are learnable by reparameterization and\ninstead fix them to constants (see Section 4 for details). Thus, in our implementation, the approximate\nposterior q has no learnable parameters, so LT is a constant during training and can be ignored.\n3.2 Reverse process and L1:T \u22121\nNow we discuss our choices in p\u03b8(xt\u22121|xt) = N (xt\u22121; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t)) for 1 < t \u2264 T. First,\nwe set \u03a3\u03b8(xt, t) = \u03c3\n2\nt\nI to untrained time dependent constants. Experimentally, both \u03c3\n2\nt = \u03b2t and\n\u03c3\n2\nt = \u03b2\u02dc\nt =\n1\u2212\u03b1\u00aft\u22121\n1\u2212\u03b1\u00aft\n\u03b2t had similar results. The first choice is optimal for x0 \u223c N (0, I), and the\nsecond is optimal for x0 deterministically set to one point. These are the two extreme choices\ncorresponding to upper and lower bounds on reverse process entropy for data with coordinatewise\nunit variance [53].\nSecond, to represent the mean \u00b5\u03b8(xt, t), we propose a specific parameterization motivated by the\nfollowing analysis of Lt. With p\u03b8(xt\u22121|xt) = N (xt\u22121; \u00b5\u03b8(xt, t), \u03c32\nt\nI), we can write:\nLt\u22121 = Eq\n\u0014\n1\n2\u03c3\n2\nt\nk\u00b5\u02dct(xt, x0) \u2212 \u00b5\u03b8(xt, t)k\n2\n\u0015\n+ C (8)\nwhere C is a constant that does not depend on \u03b8. So, we see that the most straightforward parameteri\u0002zation of \u00b5\u03b8\nis a model that predicts \u00b5\u02dct, the forward process posterior mean. However, we can expand\nEq. (8) further by reparameterizing Eq. (4) as xt(x0, \u000f) = \u221a\u03b1\u00aftx0 +\n\u221a\n1 \u2212 \u03b1\u00aft\u000f for \u000f \u223c N (0, I) and\napplying the forward process posterior formula (7):\nLt\u22121 \u2212 C = Ex0,\u000f\n\"\n1\n2\u03c3\n2\nt\n \n \n \n \n\u00b5\u02dct\n\u0012\nxt(x0, \u000f),\n1\n\u221a\n\u03b1\u00aft\n(xt(x0, \u000f) \u2212\n\u221a\n1 \u2212 \u03b1\u00aft\u000f)\n\u0013\n\u2212 \u00b5\u03b8(xt(x0, \u000f), t)\n \n \n \n \n2\n#\n(9)\n= Ex0,\u000f\n\"\n1\n2\u03c3\n2\nt\n \n \n \n \n1\n\u221a\n\u03b1t\n\u0012\nxt(x0, \u000f) \u2212\n\u03b2t \u221a\n1 \u2212 \u03b1\u00aft\n\u000f\n\u0013\n\u2212 \u00b5\u03b8(xt(x0, \u000f), t)\n \n \n \n \n2\n#\n(10)\n3\nAlgorithm 1 Training\n1: repeat\n2: x0 \u223c q(x0)\n3: t \u223c Uniform({1, . . . , T})\n4: \u000f \u223c N (0, I)\n5: Take gradient descent step on\n\u2207\u03b8\n \n \u000f \u2212 \u000f\u03b8(\n\u221a\n\u03b1\u00aftx0 +\n\u221a\n1 \u2212 \u03b1\u00aft\u000f, t)\n \n \n2\n6: until converged\nAlgorithm 2 Sampling\n1: xT \u223c N (0, I)\n2: for t = T, . . . , 1 do\n3: z \u223c N (0, I) if t > 1, else z = 0\n4: xt\u22121 = \u221a1\n\u03b1t\n\u0010\nxt \u2212 \u221a1\u2212\u03b1t\n1\u2212\u03b1\u00aft\n\u000f\u03b8(xt, t)\n\u0011\n+ \u03c3tz\n5: end for\n6: return x0\nEquation (10) reveals that \u00b5\u03b8 must predict \u221a\n1\n\u03b1t\n\u0010\nxt \u2212 \u221a\n\u03b2t\n1\u2212\u03b1\u00aft\n\u000f\n\u0011\ngiven xt. Since xt is available as\ninput to the model, we may choose the parameterization\n\u00b5\u03b8(xt, t) = \u00b5\u02dct\n\u0012\nxt,\n1\n\u221a\n\u03b1\u00aft\n(xt \u2212\n\u221a\n1 \u2212 \u03b1\u00aft\u000f\u03b8(xt))\u0013=\n1\n\u221a\n\u03b1t\n\u0012\nxt \u2212\n\u03b2t \u221a\n1 \u2212 \u03b1\u00aft\n\u000f\u03b8(xt, t)\n\u0013\n(11)\nwhere \u000f\u03b8 is a function approximator intended to predict \u000f from xt. To sample xt\u22121 \u223c p\u03b8(xt\u22121|xt) is\nto compute xt\u22121 = \u221a\n1\n\u03b1t\n\u0010\nxt \u2212 \u221a\n\u03b2t\n1\u2212\u03b1\u00aft\n\u000f\u03b8(xt, t)\n\u0011\n+\u03c3tz, where z \u223c N (0, I). The complete sampling\nprocedure, Algorithm 2, resembles Langevin dynamics with \u000f\u03b8 as a learned gradient of the data\ndensity. Furthermore, with the parameterization (11), Eq. (10) simplifies to:\nEx0,\u000f\n\u0014\n\u03b2\n2\nt\n2\u03c3\n2\nt \u03b1t(1 \u2212 \u03b1\u00aft)\n \n \u000f \u2212 \u000f\u03b8(\n\u221a\n\u03b1\u00aftx0 +\n\u221a\n1 \u2212 \u03b1\u00aft\u000f, t)\n \n \n2\n\u0015\n(12)\nwhich re",
          "original_query": "Denoising diffusion probabilistic models",
          "cleaned_query": "Denoising diffusion probabilistic models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture",
          "url": "https://openreview.net/forum?id=cB0BImqSS9&noteId=98BZANkxc8",
          "content": "\u00d7\n\n[Go to **NeurIPS 2023 Conference** homepage](https://openreview.net/group?id=NeurIPS.cc/2023/Conference)\n\n## Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture\n\n### [Daniel Y Fu](https://openreview.net/profile?id=~Daniel_Y_Fu1), [Simran Arora](https://openreview.net/profile?id=~Simran_Arora1), [Jessica Grogan](https://openreview.net/profile?id=~Jessica_Grogan1), [Isys Johnson](https://openreview.net/profile?id=~Isys_Johnson1), [Sabri Eyuboglu](https://openreview.net/profile?id=~Sabri_Eyuboglu1), [Armin W Thomas](https://openreview.net/profile?id=~Armin_W_Thomas1), [Benjamin Frederick Spector](https://openreview.net/profile?id=~Benjamin_Frederick_Spector1), [Michael Poli](https://openreview.net/profile?id=~Michael_Poli1), [Atri Rudra](https://openreview.net/profile?id=~Atri_Rudra1), [Christopher Re](https://openreview.net/profile?id=~Christopher_Re1)\n\nPublished: 21 Sept 2023, Last Modified: 15 Jan 2024NeurIPS 2023 oralEveryone[Revisions](https://openreview.net/revisions?id=cB0BImqSS9)[BibTeX](https://openreview.net/openreview.net)\n\n**Keywords:** structured matrices, transformers, efficiency\n\n**TL;DR:** We present Monarch Mixer, a new simple architecture that scales sub-quadratically in sequence length and model dimension.\n\n**Abstract:** Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE\u2014showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n**Submission Number:** 10349\n\nLoading",
          "original_query": "Monarch mixer: A simple sub-quadratic gemm-based architecture",
          "cleaned_query": "Monarch mixer: A simple sub-quadratic gemm-based architecture",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Distilling the Knowledge in a Neural Network - Semantic Scholar",
          "url": "https://www.semanticscholar.org/paper/Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals/0c908739fbff75f03469d13d4a1a07de3414ee19",
          "content": "Distilling Model Knowledge Computer Science 2015 This thesis presents a general framework for knowledge distillation, whereby a convenient model of the authors' choosing learns how to mimic a complex model, by observing the latter's behaviour and being penalized whenever it fails to reproduce it. Highly Influenced Model Fusion via Optimal Transport Sidak Pal Singh Martin Jaggi Computer Science 2020 This work presents a layer-wise model fusion algorithm for neural networks that utilizes optimal transport to (soft-) align neurons across the models before averaging their associated parameters, and shows that this can successfully yield \"one-shot\" knowledge transfer between neural networks trained on heterogeneous non-i.i.d. data. [PDF]",
          "original_query": "Distilling the knowledge in a neural network",
          "cleaned_query": "Distilling the knowledge in a neural network",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Revisiting Scaling Laws for Language Models: The Role of Data ...",
          "url": "https://aclanthology.org/2025.acl-long.1163.pdf",
          "content": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 23881\u201323899\nJuly 27 - August 1, 2025 \u00a92025 Association for Computational Linguistics\nRevisiting Scaling Laws for Language Models: The Role of Data Quality\nand Training Strategies\nZhengyu Chen1*, Siqi Wang 2*, Teng Xiao3, Yudong Wang 4,\nShiqi Chen 5, Xunliang Cai1, Junxian He 5, Jingang Wang1\n1 Meituan Inc. 2 The University of Hong Kong 3 Pennsylvania State University\n4 Peking University 5 The Hong Kong University of Science and Technology\n{chenzhengyu04,wangjingang02}@meituan.com\nAbstract\nTraditional scaling laws in natural language\nprocessing suggest that increasing model size\nand training data enhances performance. How\u0002ever, recent studies reveal deviations, partic\u0002ularly in large language models, where per\u0002formance improvements decelerate\u2014a phe\u0002nomenon known as sub-scaling. This paper\nrevisits these scaling laws by examining the\nimpact of data quality and training strategies\non model performance. Through extensive em\u0002pirical analysis of over 400 models, we iden\u0002tify high data density and non-optimal resource\nallocation as key factors contributing to sub\u0002scaling. High data density leads to diminishing\nreturns due to redundant information, while\noptimal resource allocation is crucial for sus\u0002tained performance improvements. We propose\na sub-optimal scaling law that better predicts\nperformance in sub-scaling regimes, highlight\u0002ing the importance of data quality and diversity.\n1 Introduction\nThe rapid advancement in natural language pro\u0002cessing (NLP) has been significantly driven by the\ndevelopment of increasingly large language mod\u0002els. These models, such as LLaMA (Touvron et al.,\n2023), Chinchilla (70B) (Hoffmann et al., 2022),\nGopher (280B) (Rae et al., 2021), and Megatron\u0002Turing NLG (530B) (Smith et al., 2022), have\nset new benchmarks across a variety of linguistic\ntasks. There is also a growing body of research on\nscaling strategies (McCandlish et al., 2018; Yang\net al., 2022, 2023; Wang et al., 2024), which could\nbe beneficial for large language models (LLMs).\nThe conventional wisdom suggests that augmenting\nmodel size and corresponding training data gener\u0002ally results in enhanced performance. This trend\nhas led to the popularization of a \u2019bigger is better\u2019\nparadigm within the field. This scaling up has been\n*Equal Contribution.\ndriven by the empirical observation that larger mod\u0002els trained on vast amounts of data tend to perform\nbetter on various natural language processing tasks\n(Komatsuzaki, 2019; Hernandez et al., 2022a).\nHowever, recent empirical studies (Hernandez\net al., 2022a; Hu et al., 2023; Porian et al., 2024;\nMuennighoff et al., 2024) have observed devia\u0002tions from this expected trend, particularly in the\ncontext of exceptionally large language models.\nThese deviations manifest as sub-scaling growth,\nwhere the rate of performance improvement de\u0002celerates as the model or dataset size continues to\nincrease. Specifically, (Hernandez et al., 2022a;\nMuennighoff et al., 2024) observe that sub-scaling\noccurs in scenarios involving repeated training data,\nleading to diminishing returns in performance. (Hu\net al., 2023) highlight that sub-scaling is particu\u0002larly pronounced in tasks requiring complex reason\u0002ing or multi-step processes. Furthermore, (Porian\net al., 2024) find that sub-scaling exists under non\u0002optimal training strategies with sub-optimal hyper\u0002parameters. Figure 1 provides a visualization of the\ndiminishing returns, clearly showing that as train\u0002ing progresses, the actual training loss values tend\nto be higher than those extrapolated from earlier\nstages, indicating how traditional scaling laws fall\nshort when dealing with extensive datasets and sug\u0002gests the need for a modified approach. Moreover,\n(Hernandez et al., 2022a; Muennighoff et al., 2024)\nhave similar sub-scaling observations in repeated\ndata and non-optimal training strategy. However,\nthere is a lack of systematic research on the sub\u0002scaling behavior of large language models (LLMs).\nFurther extending this observation to model per\u0002formance, Figure 2 displays the results of our tests\non the performance scaling law (Yang et al., 2024;\nIsik et al., 2024; Wu and Tang, 2024) with LLaMA\n2 and LLaMA 3 models. Despite LLaMA 3 incor\u0002porating advanced training strategies and improved\ndata quality, the performance improvements from\nLLaMA 2 to LLaMA 3 decelerate as the training\n23881\nFigure 1: Sub-scaling phenomenon in loss. Scaling law fits well with 5B training tokens, but as tokens increase,\nloss curve shows greater curvature, and fitting accuracy decreases, especially for larger models.\nFigure 2: (left) LLaMA 2\u2019s scaling curve outperforms LLaMA 3\u2019s, despite LLaMA 3\u2019s advanced strategies. (right)\nHigher-density datasets lead to sub-scaling. We propose metric density to measure redundancy and diversity: higher\ndensity indicates more redundancy and less diversity, leading to sub-scaling (see Section 2.1).\n10\n7 10\n8 109 1010\nModel Size\n2.5\n3.0\n3.5\n4.0\nLoss\n1e+18\n3e+18\n6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\nOptimal Model/Data Allocation\nScaling Law\n10\n6 10\n7 10\n8 109 1010 1011 1012\nModel Size\n2.0\n2.5\n3.0\n3.5\n4.0\nLoss\nLlama3 80B-1.5T\nLlama3 8B-15T\n1e+18\n3e+18\n6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\nScaling Law\nFigure 3: (left) With a fixed total compute budget, we adjust the model-to-data allocation ratio and plot the training\nloss against model size. A black curve connects the minimum points of each curve, illustrating the optimal\nChinchilla law. (right) However, current large language models, such as Llama3 8B, are trained on 15T tokens, with\na model-to-data allocation strategy that significantly deviates from the optimal Chinchilla law.\nflops increase, LLaMA 2 with 70B parameters out\u0002performs LLaMA 3 with 8B parameters.This dis\u0002crepancy, depicted in Figure 2, underscores the\ninadequacies of traditional scaling laws. Addition\u0002ally, when the scale of training data surpasses an\noptimal threshold relative to the available computa\u0002tional resources, sub-scaling law happens with such\nover-training (Gadre et al., 2024), potentially lead\u0002ing to diminishing returns in model performance.\nMoreover, there is a lack of understanding of the\ntraining dynamics of large language models and\nthe sub-scaling laws governing the training strate\u0002gies of language models. This motivates the ques\u0002tion: Under what conditions do sub-scaling laws\ninfluence the performance and efficiency of large\nlanguage models?\nThis study aims to systematically investigate the\nsub-scaling law phenomenon through an extensive\nempirical analysis involving over 400 models, rang\u0002ing from 20 million to 7 billion parameters, with\nvarying datasets and training strategies. Our find\u0002ings indicate that sub-scaling laws arise primarily\n23882\nfrom high data density and non-optimal training\nresource allocations. Specifically, we observed that\nboth factors contribute more significantly to per\u0002formance deceleration than previously anticipated.\nWe examine the sub-scaling phenomenon from two\nperspectives: data density and training strategy.\nHigh data density leads to diminishing marginal\ngains in performance as shown in Figure 2, while\noptimal resource allocation is crucial for sustain\u0002ing performance improvements as shown in Figure\n3. Further, we propose a sub-optimal scaling law\nthat generalizes the Chinchilla scaling law (Hoff\u0002mann et al., 2022) to better predict performance\nand loss in sub-scaling regimes. Our analysis re\u0002veals that the quality and diversity of training data\nare paramount, often outweighing the benefits of\nmere scale in model size. Key findings from our\nstudy include:\n1. Sub-Scaling Law Phenomenon: Traditional\nscaling laws fail to predict performance improve\u0002ments for large models and datasets. The per\u0002formance gains decelerate, leading to sub-scaling\ngrowth, especially in high data density scenarios\nand with non-optimal resource allocation.\n2. Training Strategies under Over-Training:\nCompared to Gadre et al. (2024), our study goes be\u0002yond identifying optimal hyper-parameters. Based\non the concept of Over-Training Ratio (OTR) to\nbalance model size and training data volume, we\nprovide a critical framework for optimal resource\nallocation, maximizing performance and efficiency.\n3. New Density Computation Method: We in\u0002troduce a new density computation method that\nconsiders both intra-cluster concentration and inter\u0002cluster separation. Unlike Muennighoff et al.\n(2024) that primarily focused on general data analy\u0002sis, this dual consideration offers a more insightful\nunderstanding of data quality and its influence.\n4. Sub-Optimal Scaling Law: Our introduction\nof a sub-optimal scaling law addresses the limi\u0002tations of traditional scaling laws, particularly in\nscenarios involving high-density datasets with re\u0002dundant information. We validate our proposed\nsub-optimal scaling law across various conditions.\n2 Analysis\nIn this section, we investigate the phenomenon\nof sub-scaling law from two perspectives: data\nand training strategy. Our analysis is based on ex\u0002tensive empirical evaluations involving over 400\nmodels, ranging from 20 million to 7 billion pa\u00025000 10000 15000 20000\nSteps\n3.2\n3.6\n4.0\nValidation Loss\n100M\nLow density\nHigh density\n2000 4000 6000 8000\nSteps\n3.0\n3.3\n3.6\nValidation Loss\n800M\nLow density\nHigh density\nFigure 4: We compare models with (left) 100M param\u0002eters and (right) 800M parameters trained on high and\nlow density dataset, which demonstrates that higher den\u0002sity results in a degressive performance increase.\nrameters, in different density datasets, and with\ndifferent model / data allocation and the selection\nof hyper-parameters, to investigate the sub-scaling\nlaw phenomenon. The architecture details of the\nused models are listed in Appendix Table 3.\n2.1 Analysis 1: The Perspective of Data\nPrevious works (Abbas et al., 2024; Sachdeva et al.,\n2024; Sorscher et al., 2022) have used data density\nto measure the quality and diversity of datasets. By\nfocusing on ",
          "original_query": "Scaling laws for neural language models",
          "cleaned_query": "Scaling laws for neural language models",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
          "url": "https://arxiv.org/abs/2408.15237",
          "content": "[2408.15237] The Mamba in the Llama: Distilling and Accelerating Hybrid Models[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2408.15237\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2408.15237**(cs)\n[Submitted on 27 Aug 2024 ([v1](https://arxiv.org/abs/2408.15237v1)), last revised 27 Jun 2025 (this version, v4)]\n# Title:The Mamba in the Llama: Distilling and Accelerating Hybrid Models\nAuthors:[Junxiong Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J),[Daniele Paliotta](https://arxiv.org/search/cs?searchtype=author&amp;query=Paliotta,+D),[Avner May](https://arxiv.org/search/cs?searchtype=author&amp;query=May,+A),[Alexander M. Rush](https://arxiv.org/search/cs?searchtype=author&amp;query=Rush,+A+M),[Tri Dao](https://arxiv.org/search/cs?searchtype=author&amp;query=Dao,+T)\nView a PDF of the paper titled The Mamba in the Llama: Distilling and Accelerating Hybrid Models, by Junxiong Wang and 4 other authors\n[View PDF](https://arxiv.org/pdf/2408.15237)[HTML (experimental)](https://arxiv.org/html/2408.15237v4)> > Abstract:\n> Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. Our top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN model. We also find that the distilled model has natural length extrapolation, showing almost perfect accuracy in the needle-in-a-haystack test at 20x the distillation length. Code and pre-trained checkpoints are open-sourced at [> this https URL\n](https://github.com/jxiw/MambaInLlama)> and [> this https URL\n](https://github.com/itsdaniele/speculative_mamba)> . Comments:|NeurIPS 2024. v4 updates: mention concurrent work of speculative decoding for SSM|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2408.15237](https://arxiv.org/abs/2408.15237)[cs.LG]|\n|(or[arXiv:2408.15237v4](https://arxiv.org/abs/2408.15237v4)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2408.15237](https://doi.org/10.48550/arXiv.2408.15237)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Junxiong Wang [[view email](https://arxiv.org/show-email/7b91cd33/2408.15237)]\n**[[v1]](https://arxiv.org/abs/2408.15237v1)**Tue, 27 Aug 2024 17:56:11 UTC (3,726 KB)\n**[[v2]](https://arxiv.org/abs/2408.15237v2)**Thu, 26 Dec 2024 05:27:51 UTC (4,508 KB)\n**[[v3]](https://arxiv.org/abs/2408.15237v3)**Wed, 8 Jan 2025 20:34:02 UTC (4,500 KB)\n**[v4]**Fri, 27 Jun 2025 07:54:57 UTC (935 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled The Mamba in the Llama: Distilling and Accelerating Hybrid Models, by Junxiong Wang and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2408.15237)\n* [HTML (experimental)](https://arxiv.org/html/2408.15237v4)\n* [TeX Source](https://arxiv.org/src/2408.15237)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2408.15237&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2408.15237&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-08](https://arxiv.org/list/cs.LG/2024-08)\nChange to browse by:\n[cs](https://arxiv.org/abs/2408.15237?context=cs)\n[cs.AI](https://arxiv.org/abs/2408.15237?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2408.15237)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2408.15237)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2408.15237)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2408.15237)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "The mamba in the llama: Distilling and accelerating hybrid models",
          "cleaned_query": "The mamba in the llama: Distilling and accelerating hybrid models",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Data Density Metrics for Generative Modeling: Predicting Sub-Scaling in Diffusion Training\nAdapt the \u201cdata density\u201d concept from LLM scaling to image datasets by defining redundancy/diversity metrics (e.g., embedding-space neighborhood concentration, near-duplicate rate, caption entropy for paired data). Run controlled diffusion trainings where dataset size increases but density is manipulated, and fit modified scaling curves to predict when adding more data yields diminishing returns in FID/likelihood.",
        "Compute-Optimal Allocation Laws for Diffusion Models (Steps \u00d7 Model Size \u00d7 Data Quality)\nExtend compute-optimal scaling (akin to Chinchilla-style allocation) to diffusion by explicitly modeling the tradeoff between denoiser size, number of diffusion steps used in training/sampling, and dataset quality/density. Produce an empirical \u201csub-optimal diffusion scaling law\u201d that predicts loss/FID under non-ideal schedules (e.g., too many redundant images or too many timesteps) and yields actionable guidance for resource allocation.",
        "Progressive Lossy Decompression with Learned Bit-Ordering Guided by Data Density\nBuild on DDPMs\u2019 interpretation as progressive decoding by learning an adaptive timestep/bit-allocation policy that prioritizes semantically meaningful structure before imperceptible details. Use a controller trained to minimize perceptual distortion at fixed bitrate, and test whether density-aware policies avoid \u201cwasting bits\u201d on redundant micro-textures while improving semantic fidelity at low bitrates.",
        "One-Shot Fusion of Multiple Diffusion Models via Optimal Transport Alignment\nApply neuron/feature alignment ideas (optimal transport layer-wise matching) to fuse independently trained diffusion denoisers (e.g., trained on different domains or data slices) into a single model without full retraining. Validate whether OT-aligned parameter averaging preserves sampling quality across domains better than na\u00efve averaging, and quantify when fusion works (architectural similarity, noise schedule compatibility).",
        "Teacher-Guided Speculative Decoding for Diffusion Using Hardware-Aware Proposal Models\nAdapt speculative decoding concepts (from accelerating Mamba/hybrids) to diffusion sampling: a fast proposal denoiser predicts multiple reverse steps, and a stronger teacher verifies/corrects them with an acceptance criterion based on predicted posterior consistency. Implement GPU-friendly batching of proposal steps and measure wall-clock speedups at fixed quality, including failure modes where speculation harms diversity."
      ],
      "generated_ideas_raw": "1. **Monarch-Structured U-Nets for Fast Diffusion Denoisers**  \nReplace the dense linear projections inside diffusion U-Nets (e.g., in attention/MLP blocks) with Monarch matrices to obtain sub-quadratic mixing over spatial tokens and channels. Benchmark training throughput and sampling speed vs. quality (FID, precision/recall) on CIFAR-10 and 256\u00d7256 LSUN, and ablate where Monarch helps most (early vs late blocks, channel-mixing vs token-mixing).\n\n2. **Distilling Diffusion Sampling Trajectories into a Lightweight Student Sampler**  \nUse knowledge distillation where a student network learns to reproduce a teacher diffusion model\u2019s reverse-step predictions (e.g., noise/score or x0 estimates) over multiple timesteps, with a curriculum that collapses many teacher steps into fewer student steps. Evaluate whether the student can achieve comparable FID at 5\u201320 sampling steps without changing the teacher, and compare distillation targets (noise, score, v-parameterization, or posterior mean).\n\n3. **Hybrid Diffusion\u2013SSM Generators: Mamba-Style Sequence Modules for Long-Range Image Structure**  \nDesign a diffusion denoiser that uses a hybrid backbone: retain a small fraction of attention (as in \u201cMamba in the Llama\u201d) but replace most global mixing with linear SSM/Mamba blocks to improve scalability on high-resolution images. Test whether the hybrid model preserves global coherence (e.g., faces/rooms) while reducing memory and latency, and measure how performance scales with resolution (256\u21921024).\n\n4. **Data Density Metrics for Generative Modeling: Predicting Sub-Scaling in Diffusion Training**  \nAdapt the \u201cdata density\u201d concept from LLM scaling to image datasets by defining redundancy/diversity metrics (e.g., embedding-space neighborhood concentration, near-duplicate rate, caption entropy for paired data). Run controlled diffusion trainings where dataset size increases but density is manipulated, and fit modified scaling curves to predict when adding more data yields diminishing returns in FID/likelihood.\n\n5. **Compute-Optimal Allocation Laws for Diffusion Models (Steps \u00d7 Model Size \u00d7 Data Quality)**  \nExtend compute-optimal scaling (akin to Chinchilla-style allocation) to diffusion by explicitly modeling the tradeoff between denoiser size, number of diffusion steps used in training/sampling, and dataset quality/density. Produce an empirical \u201csub-optimal diffusion scaling law\u201d that predicts loss/FID under non-ideal schedules (e.g., too many redundant images or too many timesteps) and yields actionable guidance for resource allocation.\n\n6. **Progressive Lossy Decompression with Learned Bit-Ordering Guided by Data Density**  \nBuild on DDPMs\u2019 interpretation as progressive decoding by learning an adaptive timestep/bit-allocation policy that prioritizes semantically meaningful structure before imperceptible details. Use a controller trained to minimize perceptual distortion at fixed bitrate, and test whether density-aware policies avoid \u201cwasting bits\u201d on redundant micro-textures while improving semantic fidelity at low bitrates.\n\n7. **Causal Monarch Diffusion for Autoregressive-like Generation in Time (Video/Audio)**  \nLeverage Monarch Mixer\u2019s causal parameterization to build a diffusion denoiser whose token mixing is causal in time, enabling efficient diffusion for long sequences (video frames or audio) without quadratic masking overhead. Evaluate on long-context generation (e.g., 10\u201330s audio, 64\u2013256 frame video) with metrics for temporal consistency and throughput at large sequence lengths.\n\n8. **One-Shot Fusion of Multiple Diffusion Models via Optimal Transport Alignment**  \nApply neuron/feature alignment ideas (optimal transport layer-wise matching) to fuse independently trained diffusion denoisers (e.g., trained on different domains or data slices) into a single model without full retraining. Validate whether OT-aligned parameter averaging preserves sampling quality across domains better than na\u00efve averaging, and quantify when fusion works (architectural similarity, noise schedule compatibility).\n\n9. **Teacher-Guided Speculative Decoding for Diffusion Using Hardware-Aware Proposal Models**  \nAdapt speculative decoding concepts (from accelerating Mamba/hybrids) to diffusion sampling: a fast proposal denoiser predicts multiple reverse steps, and a stronger teacher verifies/corrects them with an acceptance criterion based on predicted posterior consistency. Implement GPU-friendly batching of proposal steps and measure wall-clock speedups at fixed quality, including failure modes where speculation harms diversity.\n\n10. **Distilling Transformers into Sub-Quadratic Mixers for Long-Context LMs Under Sub-Scaling Regimes**  \nCombine the distillation framework with Monarch Mixer by distilling a pretrained Transformer LM into a Monarch-based student, then systematically vary data quality/density to study when distilled students inherit (or resist) sub-scaling. The key contribution is an actionable recipe linking (a) teacher choice, (b) student architecture (Monarch vs SSM hybrid), and (c) dataset density to predictable gains in perplexity-per-FLOP at 4K\u201332K context lengths.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Data Density Metrics for Generative Modeling: Predicting Sub-Scaling in Diffusion Training\nAdapt the \u201cdata density\u201d concept from LLM scaling to image datasets by defining redundancy/diversity metrics ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Compute-Optimal Allocation Laws for Diffusion Models (Steps \u00d7 Model Size \u00d7 Data Quality)\nExtend compute-optimal scaling (akin to Chinchilla-style allocation) to diffusion by explicitly modeling the tr",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Progressive Lossy Decompression with Learned Bit-Ordering Guided by Data Density\nBuild on DDPMs\u2019 interpretation as progressive decoding by learning an adaptive timestep/bit-allocation policy that prio",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "One-Shot Fusion of Multiple Diffusion Models via Optimal Transport Alignment\nApply neuron/feature alignment ideas (optimal transport layer-wise matching) to fuse independently trained diffusion denois",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Teacher-Guided Speculative Decoding for Diffusion Using Hardware-Aware Proposal Models\nAdapt speculative decoding concepts (from accelerating Mamba/hybrids) to diffusion sampling: a fast proposal deno",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 68,
      "paper_title": "Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness",
      "contribution": "The paper introduces a novel hybrid optimization method that combines steepest descent and conditional gradient approaches under a generalized notion of (L0,L1)-smoothness, facilitating efficient training in non-Euclidean spaces.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "quality_content": 6,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11300,
      "output_tokens": 1119,
      "predecessor_details": [
        {
          "success": true,
          "title": "The Frank-Wolfe algorithm: a short introduction",
          "url": "https://arxiv.org/abs/2311.05313",
          "content": "# Mathematics > Optimization and Control\n\n**arXiv:2311.05313** (math)\n\n\\[Submitted on 9 Nov 2023 ( [v1](https://arxiv.org/abs/2311.05313v1)), last revised 28 Nov 2023 (this version, v3)\\]\n\n# Title:The Frank-Wolfe algorithm: a short introduction\n\nAuthors: [Sebastian Pokutta](https://arxiv.org/search/math?searchtype=author&query=Pokutta,+S)\n\nView a PDF of the paper titled The Frank-Wolfe algorithm: a short introduction, by Sebastian Pokutta\n\n[View PDF](https://arxiv.org/pdf/2311.05313)\n\n> Abstract:In this paper we provide an introduction to the Frank-Wolfe algorithm, a method for smooth convex optimization in the presence of (relatively) complicated constraints. We will present the algorithm, introduce key concepts, and establish important baseline results, such as e.g., primal and dual convergence. We will also discuss some of its properties, present a new adaptive step-size strategy as well as applications.\n\n| | |\n| --- | --- |\n| Comments: | Introductory article for the Jahresbericht der Deutschen Mathematiker Vereinigung |\n| Subjects: | Optimization and Control (math.OC) |\n| Cite as: | [arXiv:2311.05313](https://arxiv.org/abs/2311.05313) \\[math.OC\\] |\n| | (or [arXiv:2311.05313v3](https://arxiv.org/abs/2311.05313v3) \\[math.OC\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2311.05313](https://doi.org/10.48550/arXiv.2311.05313) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Sebastian Pokutta \\[ [view email](https://arxiv.org/show-email/50a0520b/2311.05313)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2311.05313v1)**\nThu, 9 Nov 2023 12:18:50 UTC (1,702 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2311.05313v2)**\nSun, 19 Nov 2023 21:02:24 UTC (1,701 KB)\n\n**\\[v3\\]**\nTue, 28 Nov 2023 20:13:51 UTC (1,933 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The Frank-Wolfe algorithm: a short introduction, by Sebastian Pokutta\n\n- [View PDF](https://arxiv.org/pdf/2311.05313)\n- [TeX Source](https://arxiv.org/src/2311.05313)\n- [Other Formats](https://arxiv.org/format/2311.05313)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.OC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2311.05313&function=prev&context=math.OC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2311.05313&function=next&context=math.OC)\n\n[new](https://arxiv.org/list/math.OC/new) \\| [recent](https://arxiv.org/list/math.OC/recent) \\| [2023-11](https://arxiv.org/list/math.OC/2023-11)\n\nChange to browse by:\n\n[math](https://arxiv.org/abs/2311.05313?context=math)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2311.05313)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2311.05313)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2311.05313)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2311.05313&description=The Frank-Wolfe algorithm: a short introduction) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2311.05313&title=The Frank-Wolfe algorithm: a short introduction)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2311.05313) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Frank-Wolfe Algorithm: A Survey",
          "cleaned_query": "Frank-Wolfe Algorithm: A Survey",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] Trade-offs of Local SGD at Scale: An Empirical Study",
          "url": "https://josejg.com/papers/psgd-postlocal_neurips2020.pdf",
          "content": "Trade-offs of Local SGD at Scale: An Empirical Study\nTrade-offs of Local SGD at Scale: An Empirical Study\nJose Javier Gonzalez Ortiz josejg@mit.edu\nMIT CSAIL\nJonathan Frankle jfrankle@mit.edu\nMIT CSAIL\nMike Rabbat mikerabbat@fb.com\nFacebook AI Research\nAri Morcos arimorcos@fb.com\nFacebook AI Research\nNicolas Ballas ballasn@fb.com\nFacebook AI Research\nAbstract\nAs datasets and models become increasingly large, distributed training has become a\nnecessary component to allow deep neural networks to train in reasonable amounts of time.\nHowever, distributed training can have substantial communication overhead that hinders its\nscalability. One strategy for reducing this overhead is to perform multiple unsynchronized\nSGD steps independently on each worker between synchronization steps, a technique known\nas local SGD. We conduct a comprehensive empirical study of local SGD and related methods\non a large scale image classification task. We find that performing local SGD comes at a price:\nlower communication costs (and thereby faster training) are accompanied by lower accuracy.\nThis finding is in contrast from the smaller-scale experiments in prior work, suggesting that\nlocal SGD encounters challenges at scale. We further show that incorporating the slow\nmomentum framework of Wang et al. (2020) consistently improves accuracy without requiring\nadditional communication, hinting at future directions for potentially escaping this trade-off.\nKeywords: Deep Learning, Distributed Optimization, Local SGD, Convolutional Neural\nNetworks\n1. Introduction\nAs datasets and models continue to grow in size, it has become a common practice to train\ndeep neural networks in a distributed manner across multiple hardware workers Goyal et al.\n(2017); Shallue et al. (2018). Most deep learning models are currently optimized using some\nvariant of stochastic gradient descent Robbins and Monro (1951), often using a mini-batch\napproach Bottou (2010); Dekel et al. (2012). In distributed scenarios, the communication\noverhead necessary to synchronize gradients between workers can quickly dominate the time\nnecessary to compute the model updates, hindering the scalability of this approach. Moreover,\nbecause of the serial nature of neural network training, all the nodes must wait until the\nsynchronization completes, and performance is therefore dependent on the slowest node Dutta\net al. (2018); Ferdinand et al. (2020).\nThese issues have motivated the development of optimization algorithms that reduce the\namount of communication between workers. A simple yet practical example islocal SGD (closely\nrelated to federated averaging (McMahan et al., 2017)), where, instead of synchronizing the\n1\n0\n200\n400\n600\nTime [ms]\nK = 8 K = 16 K = 32 K = 64 K = 128\nResNet-50\nK = 256\n1 2 4 8\nH\n0\n200\n400\n600\nTime [ms]\n1 2 4 8\nH\n1 2 4 8\nH\n1 2 4 8\nH\n1 2 4 8\nH\n1 2 4 8\nH\nResNet-101\nCommunication\nOptimizer\nBackward Pass\nForward Pass\nData Loading\nFigure 1: Breakdown of the average wall-clock time per iteration during the training process.\nResults are reported for various numbers of workers (K) and numbers of local steps (H). Every\nnode has 8 workers. The main difference is the communication time, which decreases as we\nreduce the frequency of model averaging. For minibatch SGD (which we label as H = 1) with\nmultiple nodes (K >8), the communication time dominates other parts of training.\ngradients at every iteration, each worker performs multiple SGD steps locally and then averages\nthe model weights across all workers Zhang et al. (2016). Local SGD has been shown to have\ngood optimization properties from a theoretical standpoint Stich (2018); Zhang et al. (2016);\nWoodworth et al. (2020). However, while local SGD does speed up training, the resulting models\nare often less accurate compared to a synchronous minibatch SGD baseline (Lin et al., 2018).\nPost-local SGD is a variant of local SGD introduced by Lin et al. (2018) with the goal\nof remedying these problems. Post-local SGD divides training into two phases. In the first\nphase, workers perform synchronous minibatch SGD; in the second, they switch to local SGD.\nLin et al. claim that this approach generalizes better on large batch training than both local\nSGD and minibatch SGD while reducing communication for the second phase of training. The\nmajority of the analysis of local SGD and post-local SGD reported in Lin et al. (2018) is on\nCIFAR-10 Krizhevsky et al. (2009).\nMotivated by these results, we perform a thorough analysis of local SGD and post-local\nSGD on the ImageNet-1k Russakovsky et al. (2015) classification task, a de facto benchmark\nfor large-scale vision classification problems. As Figure 1 shows, inter-node communication\ncan dominate training time, becoming a bottleneck in the training process. We complement\nthe analysis of Lin et al. (2018), studying how the choice of learning rate schedule and the point\nat which to switch phases affect the generalization accuracy of models trained with post-local\nSGD. We find that post-local SGD at ImageNet-scale is a double-edged sword: decreases in\ncommunication costs (by increasing the number of local steps) are accompanied by decreases\nin accuracy. As a result, practitioners interested in post-local SGD must weigh the trade-offs\nbetween training speedup and reduction in the accuracy of the final model. Looking ahead,\nour analysis of the interaction between post-local SGD, learning rates, and momentum points\ntoward potential opportunities to escape these trade-offs.\nContributions. Our main contributions are as follows:\n1. We perform a comprehensive empirical study on ImageNet that identifies previously\nunreported scalability limitations of local and post-local SGD. Our analysis highlights\nhow, when compared to the fully synchronous baseline, local and post-local SGD suffer\nfrom non-trivial accuracy drops as workers or local steps increase.\n2\nTrade-offs of Local SGD at Scale: An Empirical Study\n2. Our analysis is the first to identify that post-local SGD performance heavily relies on\nthe choice of hyperparameters, including learning rate schedule and switching point.\n3. We show that using slow momentum Wang et al. (2020) together with post-local SGD\nachieves a better quality-performance trade-off.\n4. We show that switching to local SGD has a regularization effect on optimization that\nis only beneficial in the short term, suggesting it is always better to make the switch\nlater in training.\n2. Background and Related Work\nMinibatch SGD. Neural networks are typically trained with minibatch SGD. In minibatch\nSGD, the dataset D={(xi,yi)}i\u2208[N]is divided into non-overlapping subsets of size B known as\nminibatches. Gradient descent is performed sequentially on these minibatches, passing through\nthe entire dataset over the course of an epoch. The dataset is typically randomly shuffled\nbefore each epoch, meaning the minibatch composition and order are different on each pass\nthrough the dataset. See Algorithm 1 below for full details. In practice, networks are often\ntrained in a distributed data-parallel fashion across K workers Li et al. (2020). Each worker\nhas a separate copy of the weights and computes gradients using a disjoint subset of the data.\nThe entire dataset is reshuffled and split among workers at the beginning of every epoch. After\nthe backward pass, gradients are averaged across workers before updating the model weights.\nLocal SGD. In local SGD, described in Algorithm 2, the workers update the local copies\nof their weights, and every H >1 iterations they synchronize the weights across workers by\naveraging the weights stored on each worker. Papers on local SGD typically credit McDonald\net al. (2009), Zinkevich et al. (2010) and McDonald et al. (2010) with pioneering local SGD\nin pre-deep learning settings; these works train models to completion and average the final pa\u0002rameters. Zhang et al. (2016) explore local SGD with periodic averaging of models throughout\ntraining; they prove that it converges in convex settings (in fact, faster than minibatch SGD)\nin the face of gradient variance and show that it can optimize a LeNet-5 network on MNIST.\nPovey et al. (2014) and Su and Chen (2015) use local natural gradient descent with periodic\nsynchronization to optimize deep models. Kamp et al. (2018) synchronize updates among\nneighboring workers using a gossiping protocol. Zhou and Cong (2017), Stich (2018), Wang\nand Joshi (2018b), Yu et al. (2019), and Woodworth et al. (2020) prove convergence properties\nof local SGD under various technical assumptions. Zhou and Cong (2017) experiment on\nCIFAR-10 and show that accuracy drops off if synchronization occurs too infrequently. Wang\nand Joshi (2018a) start with infrequent averaging and then increase the communication over\nthe course of training. Periodic model averaging also takes place in federated learning, which\nfocuses on performing SGD in settings where data is distributed across many devices and a\nmodel can only be updated on the device where the data resides. McMahan et al. (2017), who\ninitiated this research literature, perform parallel SGD between devices with synchronization\non every step, although they explore synchronizing after many iterations. Local SGD is\ncommon in federated learning systems for performance reasons (Smith et al., 2017).\nPost-local SGD. Post-local SGD (Lin et al., 2018) is a hybrid in which training occurs\nsynchronously for the first part of training and switches to local SGD later in training. Post\u0002local SGD involves performing synchronous SGD for the first T steps of training and local\n3\nAlgorithm 1 Minibatch SGD with learning rate schedule \u03b3(t), batch size B, initial weights\nw0, and loss `.\n1: S \u2190 d N\nB\ne (iterations per epoch)\n2: for each epoch e do\n3: Shuffle dataset D={xi,yi}i\u2208N\n4: for each iteration m in 1 to S do\n5: t\u2190eS+m (the current step of training)\n6: o\u2190(m\u22121)B\n7: wt\u2190wt\u22121\u2212\u03b3(t)\n1\nB\nPB\ni=1\u2207`(f(xo+i\n;wt),yo+i)\nAlgorithm 2 Local SGD with learning rate schedule \u03b3(t), batch size B, initial weights w0,\nloss `, an",
          "original_query": "Stochastic Gradient Descent and the Trade-offs between Local and Global Performance",
          "cleaned_query": "Stochastic Gradient Descent and the Trade-offs between Local and Global Performance",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Gradient Clipping: Preventing Exploding Gradients in Deep Learning",
          "url": "https://mbrenndoerfer.com/writing/gradient-clipping-deep-learning",
          "content": "Gradient Clipping: Preventing Exploding Gradients in Deep Learning - Interactive | Michael Brenndoerfer | Michael Brenndoerfer\n[Writing](https://mbrenndoerfer.com/writing)[Books](https://mbrenndoerfer.com/books)[About](https://mbrenndoerfer.com/about)[Community](https://mbrenndoerfer.com/community)[Contact](https://mbrenndoerfer.com/contact)[Community](https://mbrenndoerfer.com/community)\nSign-in\n![](https://mbrenndoerfer.com/_next/image?url=https%3A%2F%2Fcnassets.uk%2Fheaders%2Fdefault_dark_fx.png&amp;w=1920&amp;q=75)![](https://mbrenndoerfer.com/_next/image?url=https%3A%2F%2Fcnassets.uk%2Fheaders%2Fdefault_light_fx.png&amp;w=1920&amp;q=75)\nBack# Gradient Clipping: Preventing Exploding Gradients in Deep Learning\nMichael BrenndoerferPublished:April 22, 2025Updated:April 24, 2025\u2022UpdatedApril 24, 2025\u202231min read\n[Data, Analytics &amp; AI](https://mbrenndoerfer.com/writing/categories/data-analytics-ai)[Machine Learning](https://mbrenndoerfer.com/writing/categories/machine-learning)[Language AI Handbook](https://mbrenndoerfer.com/writing/categories/language-ai-handbook)\nLearn how gradient clipping prevents training instability by capping gradient magnitudes. Master clip by value vs clip by norm strategies with PyTorch implementation.\n[\n![Language AI Handbook Cover](https://mbrenndoerfer.com/_next/image?url=%2Flanguage-ai-handbook%2Flanguage-ai-handbook-cover.jpg&amp;w=1920&amp;q=75)\nPart ofLanguage AI Handbook\nThis article is part of the free-to-readLanguage AI Handbook\n](https://mbrenndoerfer.com/books/language-ai-handbook)\nReading Level\nChoose your expertise level to adjust how many terms are explained. Beginners see more tooltips, experts see fewer to maintain reading flow. Hover overunderlined termsfor instant definitions.\nBeginnerMaximum helpIntermediateMedium helpExpertMinimal helpHide AllNo tooltips\n## [](#gradient-clipping)Gradient ClippingLink Copied\nDeep neural networks are powerful function approximators, but training them can be surprisingly fragile. One moment your loss is decreasing steadily, the next it explodes to infinity. The culprit?Exploding gradients. When gradients grow too large during[backpropagation](https://mbrenndoerfer.com/writing/backpropagation-algorithm-deep-learning-neural-networks),[weight updates](https://mbrenndoerfer.com/writing/mathematics-llm-fine-tuning-how-and-why-it-works-explained)become catastrophically destabilizing, and training collapses.\nGradient clippingprovides a simple but effective solution. By capping gradients at a maximumthresholdbefore applying updates, we prevent the runaway feedback loops that cause explosions. This technique is essential for training recurrent neural networks, transformers, and many other deep architectures. Without it, models with long computational paths would be nearly impossible to train.\nThis chapter covers how to detectexploding gradients, the two main clipping strategies (by value and by globalnorm), and when to apply each approach. We&#x27;ll implementgradient clippingfrom scratch and explore how to choose appropriate thresholds throughgradientnormmonitoring.\n## [](#the-exploding-gradient-problem)The Exploding Gradient ProblemLink Copied\nDuringbackpropagation, gradients flow backward through the network, accumulating contributions from each layer. In deep networks or recurrent architectures, this can create a[feedback loop](https://mbrenndoerfer.com/writing/continuous-feedback-and-improvement-ai-agents)where gradients compound multiplicatively.\nExploding Gradients\nExploding gradientsoccur whengradientmagnitudes grow exponentially duringbackpropagation. This happens when thechain ruleproduces products of values greater than 1 that compound across many layers, resulting in weight updates so large they destabilize training.\nConsider a simple[recurrent network](https://mbrenndoerfer.com/writing/history-rnn-recurrent-neural-networks)processing a sequence ofTTTtimesteps. At each step, thegradientis multiplied by the weight matrixW\\\\mathbf{W}W. If the largest[eigenvalue](https://mbrenndoerfer.com/writing/principal-component-analysis-complete-guide)of that matrix exceeds 1, gradients grow exponentially. AfterTTTmultiplications, thegradientmagnitude scales as:\n\u2225gT\u2225\u2248\u2225g0\u2225\u22c5\u03bbmax\u2061T\\\\|\\\\mathbf{g}\\_T\\\\| \\\\approx \\\\|\\\\mathbf{g}\\_0\\\\| \\\\cdot \\\\lambda\\_{\\\\max}^T\u2225gT\u200b\u2225\u2248\u2225g0\u200b\u2225\u22c5\u03bbmaxT\u200b\nwhere:\n* \u2225g0\u2225\\\\|\\\\mathbf{g}\\_0\\\\|\u2225g0\u200b\u2225: the initialgradientmagnitude at the output layer\n* \u2225gT\u2225\\\\|\\\\mathbf{g}\\_T\\\\|\u2225gT\u200b\u2225: thegradientmagnitude after backpropagating throughTTTtimesteps\n* \u03bbmax\u2061\\\\lambda\\_{\\\\max}\u03bbmax\u200b: the largesteigenvalueof the weight matrix (or itsspectral norm)\n* TTT: the number of timesteps (or layers) thegradientpasses through\nWithT=100T = 100T=100timesteps and\u03bbmax\u2061=1.1\\\\lambda\\_{\\\\max} = 1.1\u03bbmax\u200b=1.1, thegradientgrows by a factor of1.1100\u224813,7811.1^{100} \\\\approx 13,7811.1100\u224813,781. Even this modest 10% amplification per step compounds into a catastrophic explosion.\nIn[2]:\nCode\n```\n`# Simulating gradient growth in a recurrent networktimesteps=100growth\\_factor=1.1# Gradient multiplier per step# Track gradient magnitude over timegradient\\_magnitudes=[1.0]# Start with unit gradientfortinrange(timesteps):gradient\\_magnitudes.append(gradient\\_magnitudes[-1]\\*growth\\_factor)`\n```\n```\n`# Simulating gradient growth in a recurrent networktimesteps=100growth\\_factor=1.1# Gradient multiplier per step# Track gradient magnitude over timegradient\\_magnitudes=[1.0]# Start with unit gradientfortinrange(timesteps):gradient\\_magnitudes.append(gradient\\_magnitudes[-1]\\*growth\\_factor)`\n```\nOut[3]:\nConsole\n```\nGradient Growth Over Timesteps\n--------------------------------------------------\nInitial gradient magnitude: 1.00\nAfter 10 steps: 2.59\nAfter 50 steps: 117.39\nAfter 100 steps: 13780.61\nGrowth factor: 13781x\n```\nOut[4]:\nVisualization\n![Line plot showing exponential growth curve of gradient magnitude from 1 to nearly 14000 over 100 timesteps on linear scale.](https://mbrenndoerfer.com/_next/image?url=https%3A%2F%2Fcnassets.uk%2Fnotebooks%2F13_gradient_clipping_files%2Fexponential-gradient-growth-linear.png&amp;w=1920&amp;q=75)\nGradient magnitude on linear scale showing the characteristic hockey-stick curve of exponential growth.\n![Line plot showing exponential growth on log scale as a straight line with annotations at key points.](https://mbrenndoerfer.com/_next/image?url=https%3A%2F%2Fcnassets.uk%2Fnotebooks%2F13_gradient_clipping_files%2Fexponential-gradient-growth-log.png&amp;w=1920&amp;q=75)\nGradient magnitude on logarithmic scale confirms exponential growth with annotations showing 117\u00d7 amplification at step 50 and 13,781\u00d7 at step 100.\nThe exponential growth is striking. With just a 10% amplification per step, thegradientgrows nearly 14,000 times larger by the end of the sequence. This illustrates why even seemingly small spectral norms above 1.0 become catastrophic in deep or recurrent networks. In practice, these explosive gradients lead toNaNlosses, parameter values shooting to infinity, and complete training failure.\n### [](#detecting-gradient-explosions)Detecting Gradient ExplosionsLink Copied\nBefore clipping, you need to know when gradients are problematic. The clearest signal is thegradientnorm, which measures the overall magnitude of gradients across all parameters. For a model with parameters\u03b81,\u03b82,\u2026,\u03b8n\\\\theta\\_1, \\\\theta\\_2, \\\\ldots, \\\\theta\\_n\u03b81\u200b,\u03b82\u200b,\u2026,\u03b8n\u200b, each withgradient\u2207\u03b8iL\\\\nabla\\_{\\\\theta\\_i} L\u2207\u03b8i\u200b\u200bL, the globalL2 normis:\n\u2225\u2207L\u22252=\u2211i=1n\u2225\u2207\u03b8iL\u222522\\\\|\\\\nabla L\\\\|\\_2 = \\\\sqrt{\\\\sum\\_{i=1}^{n} \\\\|\\\\nabla\\_{\\\\theta\\_i} L\\\\|\\_2^2}\u2225\u2207L\u22252\u200b=i=1\u2211n\u200b\u2225\u2207\u03b8i\u200b\u200bL\u222522\u200b\u200b\nwhere:\n* \u2207\u03b8iL\\\\nabla\\_{\\\\theta\\_i} L\u2207\u03b8i\u200b\u200bL: thegradientof the lossLLLwith respect to parameter\u03b8i\\\\theta\\_i\u03b8i\u200b(a tensor)\n* \u2225\u2207\u03b8iL\u22252\\\\|\\\\nabla\\_{\\\\theta\\_i} L\\\\|\\_2\u2225\u2207\u03b8i\u200b\u200bL\u22252\u200b: theL2 normof that parameter&#x27;sgradient\n* \u2225\u2207L\u22252\\\\|\\\\nabla L\\\\|\\_2\u2225\u2207L\u22252\u200b: theglobal gradient normacross all parameters\nThis single scalar summarizes the entiregradient&#x27;s magnitude, making it easy to monitor and compare across training steps.\nIn[5]:\nCode\n```\n`importtorchimporttorch.nnasnn# Create a simple network to demonstrate gradient monitoringmodel=nn.Sequential(nn.Linear(10,64), nn.ReLU(), nn.Linear(64,64), nn.ReLU(), nn.Linear(64,1))# Compute gradients for a random batchx=torch.randn(32,10)y=torch.randn(32,1)loss=nn.MSELoss()(model(x), y)loss.backward()defcompute\\_gradient\\_norm(model):\"\"\"Compute the global L2 norm of all gradients.\"\"\"total\\_norm=0.0forparaminmodel.parameters():ifparam.gradisnotNone:total\\_norm+=param.grad.data.norm(2).item()\\*\\*2returntotal\\_norm\\*\\*0.5`\n```\n```\n`importtorchimporttorch.nnasnn# Create a simple network to demonstrate gradient monitoringmodel=nn.Sequential(nn.Linear(10,64), nn.ReLU(), nn.Linear(64,64), nn.ReLU(), nn.Linear(64,1))# Compute gradients for a random batchx=torch.randn(32,10)y=torch.randn(32,1)loss=nn.MSELoss()(model(x), y)loss.backward()defcompute\\_gradient\\_norm(model):\"\"\"Compute the global L2 norm of all gradients.\"\"\"total\\_norm=0.0forparaminmodel.parameters():ifparam.gradisnotNone:total\\_norm+=param.grad.data.norm(2).item()\\*\\*2returntotal\\_norm\\*\\*0.5`\n```\nOut[6]:\nConsole\n```\nTotal gradient norm: 0.6806\nPer-layer gradient norms:\n0.weight: 0.1575 (5.4% of total)\n0.bias: 0.0474 (0.5% of total)\n2.weight: 0.4153 (37.2% of total)\n2.bias: 0.1219 (3.2% of total)\n4.weight: 0.3790 (31.0% of total)\n4.bias: 0.3243 (22.7% of total)\n```\nTheglobal gradient normfor this network is relatively modest, typical for a well-initialized shallow network on random data. The per-layer breakdown shows how each parameter contributes to the total. In practice, you&#x27;d monitor thisnorm[over training](https://mbrenndoerfer.com/writing/inference-scaling-llm-deployment-optimization)batches to detect sudden spikes, which signal potentialgradientexplosions.\n### [](#what-causes-gradient-explosions)What Causes Gradient Explosions?Link Copied\nSeveral architectural and training choices makegradientexplosions more likely:\n* **Deep architectures**: More layers mean moregradientmultiplications. Each layer&#x27;sJa",
          "original_query": "Gradient Clipping: A Technique for Reducing Exploding Gradients",
          "cleaned_query": "Gradient Clipping: A Technique for Reducing Exploding Gradients",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[2511.11466] Non-Euclidean SGD for Structured Optimization - arXiv",
          "url": "https://arxiv.org/abs/2511.11466",
          "content": "# Mathematics > Optimization and Control\n\n**arXiv:2511.11466** (math)\n\n\\[Submitted on 14 Nov 2025\\]\n\n# Title:Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates\n\nAuthors: [Dmitry Kovalev](https://arxiv.org/search/math?searchtype=author&query=Kovalev,+D), [Ekaterina Borodich](https://arxiv.org/search/math?searchtype=author&query=Borodich,+E)\n\nView a PDF of the paper titled Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates, by Dmitry Kovalev and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2511.11466)\n\n> Abstract:Recently, several instances of non-Euclidean SGD, including SignSGD, Lion, and Muon, have attracted significant interest from the optimization community due to their practical success in training deep neural networks. Consequently, a number of works have attempted to explain this success by developing theoretical convergence analyses. Unfortunately, these results cannot properly justify the superior performance of these methods, as they could not beat the convergence rate of vanilla Euclidean SGD. We resolve this important open problem by developing a new unified convergence analysis under the structured smoothness and gradient noise assumption. In particular, our results indicate that non-Euclidean SGD (i) can exploit the sparsity or low-rank structure of the upper bounds on the Hessian and gradient noise, (ii) can provably benefit from popular algorithmic tools such as extrapolation or momentum variance reduction, and (iii) can match the state-of-the-art convergence rates of adaptive and more complex optimization algorithms such as AdaGrad and Shampoo.\n\n| | |\n| --- | --- |\n| Subjects: | Optimization and Control (math.OC); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2511.11466](https://arxiv.org/abs/2511.11466) \\[math.OC\\] |\n| (or [arXiv:2511.11466v1](https://arxiv.org/abs/2511.11466v1) \\[math.OC\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2511.11466](https://doi.org/10.48550/arXiv.2511.11466) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Dmitry Kovalev \\[ [view email](https://arxiv.org/show-email/ce16cf5b/2511.11466)\\] **\\[v1\\]**\nFri, 14 Nov 2025 16:38:15 UTC (26 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates, by Dmitry Kovalev and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2511.11466)\n- [TeX Source](https://arxiv.org/src/2511.11466)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.OC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2511.11466&function=prev&context=math.OC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2511.11466&function=next&context=math.OC)\n\n[new](https://arxiv.org/list/math.OC/new) \\| [recent](https://arxiv.org/list/math.OC/recent) \\| [2025-11](https://arxiv.org/list/math.OC/2025-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2511.11466?context=cs) [cs.LG](https://arxiv.org/abs/2511.11466?context=cs.LG) [math](https://arxiv.org/abs/2511.11466?context=math)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2511.11466)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2511.11466)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2511.11466)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2511.11466) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "On the Optimization of Non-Euclidean Structures",
          "cleaned_query": "On the Optimization of Non-Euclidean Structures",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "[PDF] ADAPTIVE GRADIENT METHODS WITH DYNAMIC BOUND OF ...",
          "url": "https://openreview.net/pdf?id=Bkg3g2R9FX",
          "content": "Published as a conference paper at ICLR 2019\nADAPTIVE GRADIENT METHODS WITH DYNAMIC\nBOUND OF LEARNING RATE\nLiangchen Luo\u2020\u2217, Yuanhao Xiong\u2021\u2217, Yan Liu\u00a7, Xu Sun\u2020\u00b6\n\u2020MOE Key Lab of Computational Linguistics, School of EECS, Peking University\n\u2021College of Information Science and Electronic Engineering, Zhejiang University\n\u00a7Department of Computer Science, University of Southern California\n\u00b6Center for Data Science, Beijing Institute of Big Data Research, Peking University\n\u2020{luolc,xusun}@pku.edu.cn \u2021xiongyh@zju.edu.cn \u00a7yanliu.cs@usc.edu\nABSTRACT\nAdaptive optimization methods such as ADAGRAD, RMSPROP and ADAM have\nbeen proposed to achieve a rapid training process with an element-wise scaling\nterm on learning rates. Though prevailing, they are observed to generalize poorly\ncompared with SGD or even fail to converge due to unstable and extreme learn\u0002ing rates. Recent work has put forward some algorithms such as AMSGRAD to\ntackle this issue but they failed to achieve considerable improvement over exist\u0002ing methods. In our paper, we demonstrate that extreme learning rates can lead\nto poor performance. We provide new variants of ADAM and AMSGRAD, called\nADABOUND and AMSBOUND respectively, which employ dynamic bounds on\nlearning rates to achieve a gradual and smooth transition from adaptive methods to\nSGD and give a theoretical proof of convergence. We further conduct experiments\non various popular tasks and models, which is often insufficient in previous work.\nExperimental results show that new variants can eliminate the generalization gap\nbetween adaptive methods and SGD and maintain higher learning speed early in\ntraining at the same time. Moreover, they can bring significant improvement over\ntheir prototypes, especially on complex deep networks. The implementation of\nthe algorithm can be found at https://github.com/Luolc/AdaBound.\n1 INTRODUCTION\nThere has been tremendous progress in first-order optimization algorithms for training deep neural\nnetworks. One of the most dominant algorithms is stochastic gradient descent (SGD) (Robbins &\nMonro, 1951), which performs well across many applications in spite of its simplicity. However,\nthere is a disadvantage of SGD that it scales the gradient uniformly in all directions. This may lead\nto poor performance as well as limited training speed when the training data are sparse. To address\nthis problem, recent work has proposed a variety of adaptive methods that scale the gradient by\nsquare roots of some form of the average of the squared values of past gradients. Examples of such\nmethods include ADAM (Kingma & Lei Ba, 2015), ADAGRAD (Duchi et al., 2011) and RMSPROP\n(Tieleman & Hinton, 2012). ADAM in particular has become the default algorithm leveraged across\nmany deep learning frameworks due to its rapid training speed (Wilson et al., 2017).\nDespite their popularity, the generalization ability and out-of-sample behavior of these adaptive\nmethods are likely worse than their non-adaptive counterparts. Adaptive methods often display faster\nprogress in the initial portion of the training, but their performance quickly plateaus on the unseen\ndata (development/test set) (Wilson et al., 2017). Indeed, the optimizer is chosen as SGD (or with\nmomentum) in several recent state-of-the-art works in natural language processing and computer\nvision (Luo et al., 2019; Wu & He, 2018), wherein these instances SGD does perform better than\nadaptive methods. Reddi et al. (2018) have recently proposed a variant of ADAM called AMSGRAD,\nhoping to solve this problem. The authors provide a theoretical guarantee of convergence but only\nillustrate its better performance on training data. However, the generalization ability of AMSGRAD\n\u2217Equal contribution. This work was done when the first and second authors were on an internship at DiDi\nAI Labs.\n1\nPublished as a conference paper at ICLR 2019\non unseen data is found to be similar to that of ADAM while a considerable performance gap still\nexists between AMSGRAD and SGD (Keskar & Socher, 2017; Chen et al., 2018).\nIn this paper, we first conduct an empirical study on ADAM and illustrate that both extremely large\nand small learning rates exist by the end of training. The results correspond with the perspective\npointed out by Wilson et al. (2017) that the lack of generalization performance of adaptive methods\nmay stem from unstable and extreme learning rates. In fact, introducing non-increasing learning\nrates, the key point in AMSGRAD, may help abate the impact of huge learning rates, while it\nneglects possible effects of small ones. We further provide an example of a simple convex opti\u0002mization problem to elucidate how tiny learning rates of adaptive methods can lead to undesirable\nnon-convergence. In such settings, RMSPROP and ADAM provably do not converge to an optimal\nsolution, and furthermore, however large the initial step size \u03b1 is, it is impossible for ADAM to fight\nagainst the scale-down term.\nBased on the above analysis, we propose new variants of ADAM and AMSGRAD, named AD\u0002ABOUND and AMSBOUND, which do not suffer from the negative impact of extreme learning\nrates. We employ dynamic bounds on learning rates in these adaptive methods, where the lower\nand upper bound are initialized as zero and infinity respectively, and they both smoothly converge\nto a constant final step size. The new variants can be regarded as adaptive methods at the beginning\nof training, and they gradually and smoothly transform to SGD (or with momentum) as time step\nincreases. In this framework, we can enjoy a rapid initial training process as well as good final\ngeneralization ability. We provide a convergence analysis for the new variants in the convex setting.\nWe finally turn to an empirical study of the proposed methods on various popular tasks and mod\u0002els in computer vision and natural language processing. Experimental results demonstrate that our\nmethods have higher learning speed early in training and in the meantime guarantee strong gener\u0002alization performance compared to several adaptive and non-adaptive methods. Moreover, they can\nbring considerable improvement over their prototypes especially on complex deep networks.\n2 NOTATIONS AND PRELIMINARIES\nNotations Given a vector \u03b8 \u2208 R\nd we denote its i-th coordinate by \u03b8i\n; we use \u03b8\nk\nto denote element\u0002wise power of k and k\u03b8k to denote its `2-norm; for a vector \u03b8t in the t-th iteration, the i-th coordinate\nof \u03b8t is denoted as \u03b8t,i by adding a subscript i. Given two vectors v, w \u2208 R\nd\n, we use hv, wi to denote\ntheir inner product, v \f w to denote element-wise product, v/w to denote element-wise division,\nmax(v, w) to denote element-wise maximum and min(v, w) to denote element-wise minimum. We\nuse S\nd\n+ to denote the set of all positive definite d \u00d7 d matrices. For a vector a \u2208 R\nd\nand a positive\ndefinite matrix M \u2208 R\nd\u00d7d\n, we use a/M to denote M\u22121a and \u221a\nM to denote M1/2. The projection\noperation \u03a0F,M(y) for M \u2208 Sd\n+ is defined as arg minx\u2208F kM1/2\n(x \u2212 y)k for y \u2208 R\nd\n. We say F\nhas bounded diameter D\u221e if kx \u2212 yk\u221e \u2264 D\u221e for all x, y \u2208 F.\nOnline convex programming A flexible framework to analyze iterative optimization methods is\nthe online optimization problem. It can be formulated as a repeated game between a player (the\nalgorithm) and an adversary. At step t, the algorithm chooses an decision xt \u2208 F, where F \u2282 R\nd\nis\na convex feasible set. Then the adversary chooses a convex loss function ft and the algorithm incurs\nloss ft(xt). The difference between the total loss PT\nt=1 ft(xt) and its minimum value for a fixed\ndecision is known as the regret, which is represented by RT =\nPT\nt=1 ft(xt)\u2212minx\u2208F PTt=1 ft(x).\nThroughout this paper, we assume that the feasible set F has bounded diameter and k\u2207ft(x)k\u221e is\nbounded for all t \u2208 [T] and x \u2208 F. We are interested in algorithms with little regret. Formally\nspeaking, our aim is to devise an algorithm that ensures RT = o(T), which implies that on average,\nthe model\u2019s performance converges to the optimal one. It has been pointed out that an online op\u0002timization algorithm with vanishing average regret yields a corresponding stochastic optimization\nalgorithm (Cesa-Bianchi et al., 2002). Thus, following Reddi et al. (2018), we use online gradient\ndescent and stochastic gradient descent synonymously.\nA generic overview of optimization methods We follow Reddi et al. (2018) to provide a generic\nframework of optimization methods in Algorithm 1 that encapsulates many popular adaptive and\nnon-adaptive methods. This is useful for understanding the properties of different optimization\nmethods. Note that the algorithm is still abstract since the functions \u03c6t : F\nt \u2192 Rd\nand \u03c8t :\nF\nd \u2192 Sd\n+ have not been specified. In this paper, we refer to \u03b1 as initial step size and \u03b1t/\n\u221a\nVt as\n2\nPublished as a conference paper at ICLR 2019\nAlgorithm 1 Generic framework of optimization methods\nInput: x1 \u2208 F, initial step size \u03b1, sequence of functions {\u03c6t, \u03c8t}\nT\nt=1\n1: for t = 1 to T do\n2: gt = \u2207ft(xt)\n3: mt = \u03c6t(g1, \u00b7 \u00b7 \u00b7 , gt) and Vt = \u03c8t(g1, \u00b7 \u00b7 \u00b7 , gt)\n4: \u03b1t = \u03b1/\u221at\n5: x\u02c6t+1 = xt \u2212 \u03b1tmt/\n\u221a\nVt\n6: xt+1 = \u03a0F,\n\u221a\nVt\n(\u02c6xt+1)\n7: end for\nlearning rate of the algorithm. Note that we employ a design of decreasing step size by \u03b1t = \u03b1/\u221at\nfor it is required for theoretical proof of convergence. However such an aggressive decay of step\nsize typically translates into poor empirical performance, while a simple constant step size \u03b1t =\n\u03b1 usually works well in practice. For the sake of clarity, we will use the decreasing scheme for\ntheoretical analysis and the constant schemem for empirical study in the rest of the paper.\nUnder such a framework, we can summarize the popular optimization methods in Table 1.1 A\nfew remarks are in order. We can see the scaling term \u03c8t is I in SGD(M), while adaptive methods\nintroduce different kinds of averaging of the squared values of past gradients. ADAM and RMSPROP\ncan be seen as variants of ADAGRAD, where the former ones use an exponential moving average as\nfunction \u03c8t instead of the simple average used in AD",
          "original_query": "Adaptive Gradient Methods with Dynamic Bound for Regularization",
          "cleaned_query": "Adaptive Gradient Methods with Dynamic Bound for Regularization",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance",
          "url": "https://arxiv.org/abs/2406.04142",
          "content": "# Mathematics > Optimization and Control\n\n**arXiv:2406.04142** (math)\n\n\\[Submitted on 6 Jun 2024 ( [v1](https://arxiv.org/abs/2406.04142v1)), last revised 4 Mar 2025 (this version, v2)\\]\n\n# Title:Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance\n\nAuthors: [Dimitris Oikonomou](https://arxiv.org/search/math?searchtype=author&query=Oikonomou,+D), [Nicolas Loizou](https://arxiv.org/search/math?searchtype=author&query=Loizou,+N)\n\nView a PDF of the paper titled Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance, by Dimitris Oikonomou and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2406.04142) [HTML (experimental)](https://arxiv.org/html/2406.04142v2)\n\n> Abstract:Stochastic gradient descent with momentum, also known as Stochastic Heavy Ball method (SHB), is one of the most popular algorithms for solving large-scale stochastic optimization problems in various machine learning tasks. In practical scenarios, tuning the step-size and momentum parameters of the method is a prohibitively expensive and time-consuming process. In this work, inspired by the recent advantages of stochastic Polyak step-size in the performance of stochastic gradient descent (SGD), we propose and explore new Polyak-type variants suitable for the update rule of the SHB method. In particular, using the Iterate Moving Average (IMA) viewpoint of SHB, we propose and analyze three novel step-size selections: MomSPS$\\_{\\\\max}$, MomDecSPS, and MomAdaSPS. For MomSPS$\\_{\\\\max}$, we provide convergence guarantees for SHB to a neighborhood of the solution for convex and smooth problems (without assuming interpolation). If interpolation is also satisfied, then using MomSPS$\\_{\\\\max}$, SHB converges to the true solution at a fast rate matching the deterministic HB. The other two variants, MomDecSPS and MomAdaSPS, are the first adaptive step-size for SHB that guarantee convergence to the exact minimizer - without a priori knowledge of the problem parameters and without assuming interpolation. Our convergence analysis of SHB is tight and obtains the convergence guarantees of stochastic Polyak step-size for SGD as a special case. We supplement our analysis with experiments validating our theory and demonstrating the effectiveness and robustness of our algorithms.\n\n| | |\n| --- | --- |\n| Comments: | 13th International Conference on Learning Representations (ICLR 2025) |\n| Subjects: | Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2406.04142](https://arxiv.org/abs/2406.04142) \\[math.OC\\] |\n| | (or [arXiv:2406.04142v2](https://arxiv.org/abs/2406.04142v2) \\[math.OC\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2406.04142](https://doi.org/10.48550/arXiv.2406.04142) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Dimitris Oikonomou \\[ [view email](https://arxiv.org/show-email/211cded8/2406.04142)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2406.04142v1)**\nThu, 6 Jun 2024 15:08:06 UTC (1,342 KB)\n\n**\\[v2\\]**\nTue, 4 Mar 2025 03:36:14 UTC (1,002 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance, by Dimitris Oikonomou and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2406.04142)\n- [HTML (experimental)](https://arxiv.org/html/2406.04142v2)\n- [TeX Source](https://arxiv.org/src/2406.04142)\n- [Other Formats](https://arxiv.org/format/2406.04142)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.OC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2406.04142&function=prev&context=math.OC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2406.04142&function=next&context=math.OC)\n\n[new](https://arxiv.org/list/math.OC/new) \\| [recent](https://arxiv.org/list/math.OC/recent) \\| [2024-06](https://arxiv.org/list/math.OC/2024-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2406.04142?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2406.04142?context=cs.LG)\n\n[math](https://arxiv.org/abs/2406.04142?context=math)\n\n[stat](https://arxiv.org/abs/2406.04142?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2406.04142?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2406.04142)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2406.04142)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2406.04142)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2406.04142&description=Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2406.04142&title=Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2406.04142) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Momentum Methods for Stochastic Optimization",
          "cleaned_query": "Momentum Methods for Stochastic Optimization",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Polyak-Style Adaptive Synchronization Frequency for Post-Local SGD\nUse stochastic Polyak step-size signals (loss decrease / gradient norm proxies) to adaptively choose the number of local steps \\(H\\) over training, rather than fixing a schedule or a single switch point as in post-local SGD. The key contribution is a practical controller that increases \\(H\\) when progress is stable and decreases \\(H\\) when drift harms generalization, with ablations isolating effects relative to slow momentum.",
        "Frank\u2013Wolfe Local Steps for Federated/Distributed Training with Complex Constraints\nDevelop a \u201cLocal Frank\u2013Wolfe Averaging\u201d method where each worker performs several Frank\u2013Wolfe steps on a constrained domain (e.g., simplex, nuclear-norm ball, group-sparsity polytope) before model averaging. Contribute convergence/duality-gap tracking under delayed synchronization plus experiments showing whether constraint-aware updates mitigate local-SGD degradation when constraints encode desired structure (sparsity/low-rank).",
        "Dual-Gap-Driven Step-Size and Clipping for Stable Large-Scale Training\nCombine Frank\u2013Wolfe\u2019s dual gap as an online stationarity/curvature certificate with gradient clipping to design an adaptive clip threshold and step-size that respond to \u201coptimization instability events.\u201d The contribution is a concrete algorithm that clips more aggressively when the dual gap suggests high curvature or noisy steps, and relaxes clipping when progress is smooth; test on Transformers/RNNs where exploding gradients occur.",
        "Dynamic Learning-Rate Bounding for Momentum + Polyak Step-Sizes\nExtend AdaBound/AMSBound to the momentum Polyak variants (MomSPS\\_\\{max\\}, MomDecSPS, MomAdaSPS) by adding time-varying lower/upper bounds on the effective step-size. The key contribution is a method that preserves fast early progress (adaptive/Polyak) while provably preventing extreme effective steps that can cause divergence or poor generalization, with theory in convex smooth settings and deep-net validation.",
        "Frank\u2013Wolfe with Stochastic Polyak Step-Sizes for Projection-Free Deep Learning\nCreate a projection-free variant where the Frank\u2013Wolfe step-size is chosen via a stochastic Polyak-like rule using mini-batch loss estimates and a running lower bound on the optimum (or surrogate). The key contribution is an actionable step-size recipe that reduces manual tuning in constrained deep training (e.g., norm-constrained fine-tuning, simplex attention constraints) and provides empirical comparisons to line-search and fixed schedules.",
        "A Benchmark Suite and Diagnostics for \u201cExtreme Effective Step-Size\u201d Failures Across Optimizers\nDevelop standardized diagnostics that quantify (a) effective per-parameter step sizes (as in AdaBound\u2019s motivation), (b) clipping activation statistics, and (c) local-SGD drift metrics, and correlate them with final generalization. The contribution is a reproducible benchmark (ImageNet + at least one Transformer task) plus a set of actionable indicators that predict when local SGD, adaptive methods, or non-Euclidean SGD will hit instability/accuracy trade-offs\u2014enabling fair, mechanistic comparisons."
      ],
      "generated_ideas_raw": "1. **Communication-Aware Local-SGD with Non-Euclidean Updates (Sign/Lion/Muon)**\n   Replace local SGD\u2019s Euclidean update with a non-Euclidean SGD step (e.g., SignSGD/Lion-style mirror map) on each worker, leveraging the structured smoothness/noise framework to predict when such updates reduce drift under infrequent averaging. Evaluate on ImageNet-scale settings where Paper 2 reports accuracy drops, measuring whether structured geometry narrows the speed\u2013accuracy trade-off without extra communication.\n\n2. **Polyak-Style Adaptive Synchronization Frequency for Post-Local SGD**\n   Use stochastic Polyak step-size signals (loss decrease / gradient norm proxies) to adaptively choose the number of local steps \\(H\\) over training, rather than fixing a schedule or a single switch point as in post-local SGD. The key contribution is a practical controller that increases \\(H\\) when progress is stable and decreases \\(H\\) when drift harms generalization, with ablations isolating effects relative to slow momentum.\n\n3. **Frank\u2013Wolfe Local Steps for Federated/Distributed Training with Complex Constraints**\n   Develop a \u201cLocal Frank\u2013Wolfe Averaging\u201d method where each worker performs several Frank\u2013Wolfe steps on a constrained domain (e.g., simplex, nuclear-norm ball, group-sparsity polytope) before model averaging. Contribute convergence/duality-gap tracking under delayed synchronization plus experiments showing whether constraint-aware updates mitigate local-SGD degradation when constraints encode desired structure (sparsity/low-rank).\n\n4. **Dual-Gap-Driven Step-Size and Clipping for Stable Large-Scale Training**\n   Combine Frank\u2013Wolfe\u2019s dual gap as an online stationarity/curvature certificate with gradient clipping to design an adaptive clip threshold and step-size that respond to \u201coptimization instability events.\u201d The contribution is a concrete algorithm that clips more aggressively when the dual gap suggests high curvature or noisy steps, and relaxes clipping when progress is smooth; test on Transformers/RNNs where exploding gradients occur.\n\n5. **Dynamic Learning-Rate Bounding for Momentum + Polyak Step-Sizes**\n   Extend AdaBound/AMSBound to the momentum Polyak variants (MomSPS\\_\\{max\\}, MomDecSPS, MomAdaSPS) by adding time-varying lower/upper bounds on the effective step-size. The key contribution is a method that preserves fast early progress (adaptive/Polyak) while provably preventing extreme effective steps that can cause divergence or poor generalization, with theory in convex smooth settings and deep-net validation.\n\n6. **Structured-Noise-Aware Gradient Clipping (Layer/Block Adaptive Thresholds)**\n   Use the structured gradient-noise/Hessian upper-bound viewpoint (Paper 4) to derive per-layer or per-block clipping thresholds that scale with estimated sparsity/low-rank structure instead of a single global norm threshold. The contribution is an implementable estimator (e.g., low-rank sketch of second-moment or blockwise noise proxies) and a clipping rule that targets layers most prone to explosion without over-damping the entire model.\n\n7. **Mirror-Descent Interpretation of AdaBound: A Unified Non-Euclidean Bounded-Adaptive Optimizer**\n   Recast AdaBound\u2019s bounded learning rates as a mirror-descent/non-Euclidean SGD method with a time-varying geometry that transitions toward Euclidean SGD. The contribution is (i) a principled choice of mirror map that yields AdaBound-like behavior as a special case and (ii) a new optimizer that inherits Paper 4\u2019s structured-rate benefits while maintaining bounded effective steps for generalization.\n\n8. **Frank\u2013Wolfe with Stochastic Polyak Step-Sizes for Projection-Free Deep Learning**\n   Create a projection-free variant where the Frank\u2013Wolfe step-size is chosen via a stochastic Polyak-like rule using mini-batch loss estimates and a running lower bound on the optimum (or surrogate). The key contribution is an actionable step-size recipe that reduces manual tuning in constrained deep training (e.g., norm-constrained fine-tuning, simplex attention constraints) and provides empirical comparisons to line-search and fixed schedules.\n\n9. **Drift-Compensated Slow Momentum for Local SGD via Non-Euclidean Correction**\n   Build on Paper 2\u2019s finding that slow momentum helps local SGD by adding a geometry-aware correction term: maintain a \u201cglobal\u201d slow-momentum buffer in a non-Euclidean space (sign/mirror coordinates) and map it back to parameter space at synchronization. The contribution is a specific algorithmic modification aimed at reducing inter-worker model drift during long local phases, with controlled experiments varying \\(K\\) and \\(H\\).\n\n10. **A Benchmark Suite and Diagnostics for \u201cExtreme Effective Step-Size\u201d Failures Across Optimizers**\n   Develop standardized diagnostics that quantify (a) effective per-parameter step sizes (as in AdaBound\u2019s motivation), (b) clipping activation statistics, and (c) local-SGD drift metrics, and correlate them with final generalization. The contribution is a reproducible benchmark (ImageNet + at least one Transformer task) plus a set of actionable indicators that predict when local SGD, adaptive methods, or non-Euclidean SGD will hit instability/accuracy trade-offs\u2014enabling fair, mechanistic comparisons.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Polyak-Style Adaptive Synchronization Frequency for Post-Local SGD\nUse stochastic Polyak step-size signals (loss decrease / gradient norm proxies) to adaptively choose the number of local steps \\(H\\) ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Frank\u2013Wolfe Local Steps for Federated/Distributed Training with Complex Constraints\nDevelop a \u201cLocal Frank\u2013Wolfe Averaging\u201d method where each worker performs several Frank\u2013Wolfe steps on a constrained",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Dual-Gap-Driven Step-Size and Clipping for Stable Large-Scale Training\nCombine Frank\u2013Wolfe\u2019s dual gap as an online stationarity/curvature certificate with gradient clipping to design an adaptive clip ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Dynamic Learning-Rate Bounding for Momentum + Polyak Step-Sizes\nExtend AdaBound/AMSBound to the momentum Polyak variants (MomSPS\\_\\{max\\}, MomDecSPS, MomAdaSPS) by adding time-varying lower/upper boun",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Frank\u2013Wolfe with Stochastic Polyak Step-Sizes for Projection-Free Deep Learning\nCreate a projection-free variant where the Frank\u2013Wolfe step-size is chosen via a stochastic Polyak-like rule using mini-",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "A Benchmark Suite and Diagnostics for \u201cExtreme Effective Step-Size\u201d Failures Across Optimizers\nDevelop standardized diagnostics that quantify (a) effective per-parameter step sizes (as in AdaBound\u2019s m",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 69,
      "paper_title": "Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion",
      "contribution": "The paper introduces a novel multimodal learning approach that dynamically balances the classification abilities of strong and weak modalities using a sustained boosting algorithm and adaptive classifier assignment.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "quality_content": 5,
      "crawl_rate": 1.0,
      "quality_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 7,
      "input_tokens": 7635,
      "output_tokens": 940,
      "predecessor_details": [
        {
          "success": true,
          "title": "A Decision-Theoretic Generalization of On-Line Learning and an ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S002200009791504X",
          "content": "References REFERENCES 1 What size net gives valid generalization? Adv. Neural Inform. Process. Systems I (1989), pp. 81-90 2 D. Blackwell An analog of the minimax theorem for vector payoffs Pacific J. Math., 6 (Spring 1956), pp. 1-8 3 L. Breiman, 1996, Bias, variance, and arcing classifiers, Statistics Dept. University of California 4 N. Cesa-Bianchi, Y. Freund, D. P. Helmhold, D. Haussler, R. E. Schapire, M. K. Warmuth, How to use expert advice, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, 1993, 382, 391 5 T. H. Chung, Approximate methods for sequential decision making using expert advice, Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, 1994, 183, 189 6 T.M. Cover Universal portfolios Math. Finance, 1 (Jan. 1991), pp. 1-29 7 T.G. Dietterich, G. Bakiri Solving multiclass learning problems via error-correcting output codes J. Artif. Intell. Res., 2 (January 1995), pp. 263-286 8 H. Drucker, C. Cortes Boosting decision trees Adv. Neural Inform. Process. Systems, 8 (1996) 9 H. Drucker, R. Schapire, P. Simard Boosting performance in neural networks Int. J. Pattern Recognition Artif. Intell., 7 (1993), pp. 705-719 10 Y. Freund, 1993, Data Filtering and Distribution Modeling Algorithms for Machine Learning, University of California at Santa Cruz 11 Y. Freund Boosting a weak learning algorithm by majority Inform. and Comput., 121 (September 1995), pp. 256-285 12 Y. Freund, R. E. Schapire, Experiments with a new boosting algorithm, Machine Learning: Proceedings of the Thirteenth International Conference, 1996, 148, 156 13 Y. Freund, R. E. Schapire, Game theory, on-line prediction and boosting, Proceedings of the Ninth Annual Conference on Computational Learning Theory, 1996, 325, 332 14 J. Hannan Approximation to Bayes risk in repeated play Contributions to the Theory of Games, Princeton Univ. Press, Princeton (1957) 15 D. Haussler, J. Kivinen, M.K. Warmuth Tight worst-case loss bounds for predicting with expert advice Computational Learning Theory: Second European Conference, EuroCOLT '95, Springer-Verlag, New York/Berlin (1995) 16 J.C. Jackson, M.W. Craven Learning sparse perceptrons Adv. Neural Inform. Process. Systems, 8 (1996) 17 M. Kearns, Y. Mansour, A. Y. Ng, D. Ron, An experimental and theoretical comparison of model selection methods, Proceedings of the Eighth Annual Conference on Computational Learning Theory, 1995 18 M.J. Kearns, U.V. Vazirani An Introduction to Computational Learning Theory, MIT Press, Cambridge (1994) 19 J. Kivinen, M.K. Warmuth Using experts for predicting continuous outcomes Computational Learning Theory: EuroCOLT '93, Springer-Verlag, New York/Berlin (1994) 20 N. Littlestone, M.K. Warmuth The weighted majority algorithm Inform. and Comput., 108 (1994), pp. 212-261 21 J. R. Quinlan, Bagging, boosting, and C4.5, Proceedings, Fourteenth National Conference on Artificial Intelligence, 1996 22 R.E. Schapire The strength of weak learnability Machine Learning, 5 (1990), pp. 197-227 23 Estimation of Dependences Based on Empirical Data, Springer-Verlag, New York/Berlin (1982) 24 V. G. Vovk, A game of prediction with expert advice, Proceedings of the Eighth Annual Conference on Computational Learning Theory, 1995 25 V. G. Vovk, Aggregating strategies, Proceedings of the Third Annual Workshop on Computational Learning Theory, 1990, 321, 383 26 R.S. Wenocur, R.M. Dudley Some special Vapnik\u2013Chervonenkis classes Discrete Mathematics, 33 (1981), pp. 313-318 Copyright \u00a9 1997 Academic Press. All rights reserved.",
          "original_query": "A decision-theoretic generalization of on-line learning and an application to boosting",
          "cleaned_query": "A decision-theoretic generalization of on-line learning and an application to boosting",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Greedy function approximation: A gradient boosting machine.",
          "url": "https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full",
          "content": "# Pardon Our Interruption\n\nAs you were browsing something about your browser made us think you were a bot. There are a few reasons this might happen:\n\n- You're a power user moving through this website with super-human speed.\n- You've disabled cookies in your web browser.\n- A third-party browser plugin, such as Ghostery or NoScript, is preventing JavaScript from running. Additional information is available in this [support article](http://ds.tl/help-third-party-plugins).\n\nTo regain access, please make sure that cookies and JavaScript are enabled before reloading the page.\n\n### Please stand by\n\nWe're getting everything ready for you. The page is loading, and you'll be on your way in just a few moments.\n\nThanks for your patience!",
          "original_query": "Greedy function approximation: a gradient boosting machine",
          "cleaned_query": "Greedy function approximation: a gradient boosting machine",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "ReconBoost: Boosting Can Achieve Modality Reconcilement - arXiv",
          "url": "https://arxiv.org/html/2405.09321",
          "content": "ReconBoost: Boosting Can Achieve Modality Reconcilement\n# ReconBoost: Boosting Can Achieve Modality Reconcilement\nCong HuaQianqian XuShilong BaoZhiyong YangQingming Huang\n###### Abstract\nThis paper explores a novel multi-modalalternatinglearning paradigm pursuing a reconciliation between the exploitation of uni-modal features and the exploration of cross-modal interactions. This is motivated by the fact that current paradigms of multi-modal learning tend to explore multi-modal features simultaneously. The resulting gradient prohibits further exploitation of the features in the weak modality, leading to modality competition, where the dominant modality overpowers the learning process. To address this issue, we study the modality-alternating learning paradigm to achieve reconcilement. Specifically, we propose a new method calledReconBoostto update a fixed modality each time. Herein, the learning objective is dynamically adjusted with a reconcilement regularization against competition with the historical models. By choosing a KL-based reconcilement, we show that the proposed method resembles Friedman\u2019s Gradient-Boosting (GB) algorithm, where the updated learner can correct errors made by others and help enhance the overall performance. The major difference with the classic GB is that we only preserve the newest model for each modality to avoid overfitting caused by ensembling strong learners. Furthermore, we propose a memory consolidation scheme and a global rectification scheme to make this strategy more effective. Experiments over six multi-modal benchmarks speak to the efficacy of the method. We release the code at[https://github.com/huacong/ReconBoost](https://github.com/huacong/ReconBoost).\nMulti-modal learning, modality, boosting,ICML\n## 1Introduction\nDeep learning has significantly advanced uni-modal tasks> (He et\u00a0al., [> 2016\n](https://arxiv.org/html/2405.09321v1#bib.bib23)> ; Tang et\u00a0al., [> 2017\n](https://arxiv.org/html/2405.09321v1#bib.bib52)> ; Li et\u00a0al., [> 2019\n](https://arxiv.org/html/2405.09321v1#bib.bib36)> ; Zhang et\u00a0al., [> 2020\n](https://arxiv.org/html/2405.09321v1#bib.bib67)> )\n. However, most real-world data usually follows a multi-modal nature (say text, video, and audio) in various fields such as data mining> (Cai et\u00a0al., [> 2011\n](https://arxiv.org/html/2405.09321v1#bib.bib4)> ; Jiang et\u00a0al., [> 2019\n](https://arxiv.org/html/2405.09321v1#bib.bib29)> )\n, computer vision> (Gallego et\u00a0al., [> 2022\n](https://arxiv.org/html/2405.09321v1#bib.bib19)> ; Wan et\u00a0al., [> 2023\n](https://arxiv.org/html/2405.09321v1#bib.bib55)> ; Shao et\u00a0al., [> 2024\n](https://arxiv.org/html/2405.09321v1#bib.bib50)> )\n, and medical diagnosis> (Chen et\u00a0al., [> 2022\n](https://arxiv.org/html/2405.09321v1#bib.bib6)> ; Ruan et\u00a0al., [> 2021\n](https://arxiv.org/html/2405.09321v1#bib.bib45)> )\n. Because of this, the deep learning community has recently focused more on multi-modal learning> (Wei et\u00a0al., [> 2022\n](https://arxiv.org/html/2405.09321v1#bib.bib59)> ; Jiang et\u00a0al., [> 2023a\n](https://arxiv.org/html/2405.09321v1#bib.bib30)> ; Feng et\u00a0al., [> 2022\n](https://arxiv.org/html/2405.09321v1#bib.bib14)> ; Zhang et\u00a0al., [> 2024\n](https://arxiv.org/html/2405.09321v1#bib.bib69)> )\n.\nThe prevailing paradigm in multi-modal learning typically employs ajoint learningstrategy, wherein a wealth of studies> (Shahroudy et\u00a0al., [> 2017\n](https://arxiv.org/html/2405.09321v1#bib.bib47)> ; Chen et\u00a0al., [> 2020\n](https://arxiv.org/html/2405.09321v1#bib.bib8)> ; Wang et\u00a0al., [> 2020b\n](https://arxiv.org/html/2405.09321v1#bib.bib57)> ; Deng &amp; Dragotti, [> 2021\n](https://arxiv.org/html/2405.09321v1#bib.bib10)> ; Zhang et\u00a0al., [> 2023\n](https://arxiv.org/html/2405.09321v1#bib.bib68)> ; Jiang et\u00a0al., [> 2023b\n](https://arxiv.org/html/2405.09321v1#bib.bib31)> ; Shao et\u00a0al., [> 2023\n](https://arxiv.org/html/2405.09321v1#bib.bib49)> )\nprimarily focus on integrating modality-specific features into ashared representationfor various downstream tasks.\n![Refer to caption](x1.png)(a)Audio Modality\n![Refer to caption](x2.png)(b)Visual Modality\n![Refer to caption](x3.png)(c)Multi-modal\nFigure 1:The performance among multi-modal learning competitors on the CREMA-D dataset. For audio modality and visual modality, we evaluate the encoders of different competitors by training linear classifiers on them. Uni represents the uni-modal training method.\nDespite great success, numerous experimental observations> (Du et\u00a0al., [> 2023\n](https://arxiv.org/html/2405.09321v1#bib.bib12)> ; Peng et\u00a0al., [> 2022\n](https://arxiv.org/html/2405.09321v1#bib.bib43)> )\nand recent theoretical evidence> (Huang et\u00a0al., [> 2022\n](https://arxiv.org/html/2405.09321v1#bib.bib27)> )\nhave pointed out that current paradigms of multi-modal learning encounterModality Competition,where the model is dominated by some of the modalities. Various studies> (Peng et\u00a0al., [> 2022\n](https://arxiv.org/html/2405.09321v1#bib.bib43)> ; Fan et\u00a0al., [> 2023\n](https://arxiv.org/html/2405.09321v1#bib.bib13)> ; Du et\u00a0al., [> 2023\n](https://arxiv.org/html/2405.09321v1#bib.bib12)> )\nhave been made to mitigate the modality competition issue. The primary concern is how to balance optimization progress across multi-modal learners and improve uni-modal feature exploitations. For instance, G-Blending> (Wang et\u00a0al., [> 2020a\n](https://arxiv.org/html/2405.09321v1#bib.bib56)> )\nadds a uni-modal classifier with additional supervised signals in multi-modal learning to blend modalities effectively. OGM> (Peng et\u00a0al., [> 2022\n](https://arxiv.org/html/2405.09321v1#bib.bib43)> )\nand PMR> (Fan et\u00a0al., [> 2023\n](https://arxiv.org/html/2405.09321v1#bib.bib13)> )\nwork on reducing the influence of the dominant modality and aiding the training of others through adaptive gradient modulation and dynamic loss functions, respectively. Besides, UMT> (Du et\u00a0al., [> 2023\n](https://arxiv.org/html/2405.09321v1#bib.bib12)> )\ndistills knowledge from well-trained uni-modal models in multi-modal learning which can effectively benefit from uni-modal learning.\nHowever, as depicted in Fig.[1](https://arxiv.org/html/2405.09321v1#S1.F1.fig1), existing algorithmsstill follow the joint learning strategy, suffering from limited performance trade-offs for modality competition. Expanding the gradient update rule, we find that joint learning tends to neglect the gradient from weak modality. The dominant modality that converges more quickly would eventually overpower the whole learning process.\nTherefore, in this paper, we turn to the following question:\nCan we achieve modality reconciliation via other learning paradigms?\nIn search of an answer, we propose an effective method namedReconBoost, where we alternate the learning for each modality. Intuitively, it naturally alleviates modality competition in the gradient space since the modality-specific gradients must be employed separately. To further enhance the effect of individual modalities, we propose a reconcilement regularization to maximize the diversity between the current update and historical models. Dynamically adjusting the learning objective via the regularization term further alleviates the modality competition issue induced by sticking to a particular modality. Theoretically, we show that by choosing a KL divergence> (Kullback &amp; Leibler, [> 1951\n](https://arxiv.org/html/2405.09321v1#bib.bib34)> )\nbased reconcilement term, our proposed method can realize an alternating version of the well-known gradient boosting method> (Friedman, [> 2001\n](https://arxiv.org/html/2405.09321v1#bib.bib18)> )\n. Specifically, the updated modality learner can focus on the errors made by others, thereby highlighting their complementarity. Unlike traditional boosting techniques> (Freund, [> 1995\n](https://arxiv.org/html/2405.09321v1#bib.bib15)> ; Freund &amp; Schapire, [> 1997\n](https://arxiv.org/html/2405.09321v1#bib.bib16)> )\n, which use weak learners like decision trees, our method employs DNN-based learners which are over-parameterized models. To avoid overfitting, we discard historical learners and only preserve the last learner for each modality, creating analternating-boosting strategy.Additionally, considering the differences between the traditional boosting techniques and our alternating-boosting strategy, we present a memory consolidation scheme and a global rectification scheme to reduce the risk of forgetting critical patterns in historical updates.\nFinally, we conduct empirical experiments on six multi-modal benchmarks and demonstrate that 1) ReconBoost can consistently outperform all the competitors significantly on all datasets. 2) ReconBoost can achieveModality Reconcilement.\n## 2Preliminary\nIn this section, we first review the task of multi-modal learning. Then, we further explain the current difficulties encountered in multi-modal learning.Due to space limitations, we present a brief overview of prior arts in App.[A](https://arxiv.org/html/2405.09321v1#A1).\n### 2.1The Task of Multi-modal Learning\nLet\ud835\udc9ft\u2062r\u2062a\u2062i\u2062n={(xi,yi)}i=1Nsubscript\ud835\udc9f\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bsuperscriptsubscriptsubscript\ud835\udc65\ud835\udc56subscript\ud835\udc66\ud835\udc56\ud835\udc561\ud835\udc41\\\\mathcal{D}\\_{train}=\\\\left\\\\{(x\\_{i},y\\_{i})\\\\right\\\\}\\_{i=1}^{N}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT = { ( italic\\_x start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , italic\\_y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ) } start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_N end\\_POSTSUPERSCRIPTbe a multi-modal dataset, whereN\ud835\udc41Nitalic\\_Nis the number of examples in the training set. Herein, each examplei\ud835\udc56iitalic\\_iconsists of a set of raw featuresxi={mik}k=1Msubscript\ud835\udc65\ud835\udc56superscriptsubscriptsuperscriptsubscript\ud835\udc5a\ud835\udc56\ud835\udc58\ud835\udc581\ud835\udc40x\\_{i}=\\\\{m\\_{i}^{k}\\\\}\\_{k=1}^{M}italic\\_x start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT = { italic\\_m start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_k end\\_POSTSUPERSCRIPT } start\\_POSTSUBSCRIPT italic\\_k = 1 end\\_POSTSUBSCRIPT star",
          "original_query": "Reconboost: Boosting can achieve modality reconcilement",
          "cleaned_query": "Reconboost: Boosting can achieve modality reconcilement",
          "content_quality": "good_from_html"
        },
        {
          "success": true,
          "title": "What Makes Training Multi-Modal Classification Networks Hard?",
          "url": "https://ieeexplore.ieee.org/document/9156420",
          "content": "What Makes Training Multi-Modal Classification Networks Hard? | IEEE Conference Publication | IEEE Xplore\n[**]()\n[](https://ieeexplore.ieee.org/rest/api/hpdata)\n### IEEE Account\n* [Change Username/Password]()\n* [Update Address]()\n### Purchase Details\n* [Payment Options]()\n* [Order History]()\n* [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n### Profile Information\n* [Communications Preferences]()\n* [Profession and Education]()\n* [Technical Interests]()\n### Need Help?\n* **US &amp; Canada:**+1 800 678 4333\n* **Worldwide:**+1 732 981 0060\n* [Contact &amp; Support](https://ieeexplore.ieee.org/xpl/contact)\n* [About IEEE*Xplore*](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n* [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n* [Help](https://ieeexplore.ieee.org/Xplorehelp)\n* [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n* [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n* [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n* [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n* [Privacy &amp; Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n&copy; Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.\n**",
          "original_query": "What makes training multi-modal classification networks hard?",
          "cleaned_query": "What makes training multi-modal classification networks hard?",
          "content_quality": "good"
        },
        {
          "success": true,
          "title": "Balanced Multimodal Learning via On-the-fly Gradient ...",
          "url": "https://arxiv.org/abs/2203.15332",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2203.15332** (cs)\n\n\\[Submitted on 29 Mar 2022\\]\n\n# Title:Balanced Multimodal Learning via On-the-fly Gradient Modulation\n\nAuthors: [Xiaokang Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng,+X), [Yake Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei,+Y), [Andong Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng,+A), [Dong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+D), [Di Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu,+D)\n\nView a PDF of the paper titled Balanced Multimodal Learning via On-the-fly Gradient Modulation, by Xiaokang Peng and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2203.15332)\n\n> Abstract:Multimodal learning helps to comprehensively understand the world, by integrating different senses. Accordingly, multiple input modalities are expected to boost model performance, but we actually find that they are not fully exploited even when the multimodal model outperforms its uni-modal counterpart. Specifically, in this paper we point out that existing multimodal discriminative models, in which uniform objective is designed for all modalities, could remain under-optimized uni-modal representations, caused by another dominated modality in some scenarios, e.g., sound in blowing wind event, vision in drawing picture event, etc. To alleviate this optimization imbalance, we propose on-the-fly gradient modulation to adaptively control the optimization of each modality, via monitoring the discrepancy of their contribution towards the learning objective. Further, an extra Gaussian noise that changes dynamically is introduced to avoid possible generalization drop caused by gradient modulation. As a result, we achieve considerable improvement over common fusion methods on different multimodal tasks, and this simple strategy can also boost existing multimodal methods, which illustrates its efficacy and versatility. The source code is available at \\\\url{ [this https URL](https://github.com/GeWu-Lab/OGM-GE_CVPR2022)}.\n\n| | |\n| --- | --- |\n| Comments: | Accepted by CVPR 2022 (ORAL) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2203.15332](https://arxiv.org/abs/2203.15332) \\[cs.CV\\] |\n| (or [arXiv:2203.15332v1](https://arxiv.org/abs/2203.15332v1) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2203.15332](https://doi.org/10.48550/arXiv.2203.15332) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Yake Wei \\[ [view email](https://arxiv.org/show-email/c269f548/2203.15332)\\] **\\[v1\\]**\nTue, 29 Mar 2022 08:26:38 UTC (505 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Balanced Multimodal Learning via On-the-fly Gradient Modulation, by Xiaokang Peng and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2203.15332)\n- [TeX Source](https://arxiv.org/src/2203.15332)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2203.15332&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2203.15332&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2022-03](https://arxiv.org/list/cs.CV/2022-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2203.15332?context=cs) [cs.AI](https://arxiv.org/abs/2203.15332?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2203.15332)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2203.15332)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2203.15332)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2203.15332) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Balanced multimodal learning via on-the-fly gradient modulation",
          "cleaned_query": "Balanced multimodal learning via on-the-fly gradient modulation",
          "content_quality": "good"
        }
      ],
      "generated_ideas": [
        "Decision-Theoretic ReconBoost with Regret Guarantees\nReformulate modality-alternating training (ReconBoost) as an online \u201cexperts\u201d problem where each modality-specific model is an expert, and the scheduler is a decision-theoretic controller. Derive explicit regret bounds (vs. the best fixed modality or best switching sequence) under KL-based reconcilement, then validate whether bounded-regret scheduling reduces modality competition and improves worst-case robustness across datasets.",
        "Adaptive Modality Scheduling via Gradient-Boosting Line Search\nExtend Friedman-style gradient boosting to multimodal alternating updates by adding an explicit per-step line search (step size) per modality based on loss decrease and reconcilement penalty. Implement a practical variant that estimates curvature using mini-batch Hessian traces, and test if adaptive step sizing prevents over-correction when a weak modality begins to improve.",
        "ReconBoost + OGM Hybrid: Alternating Updates with Intra-Step Gradient Modulation\nCombine ReconBoost\u2019s \u201cupdate one modality at a time\u201d with OGM\u2019s on-the-fly gradient modulation inside the selected modality update. Concretely, use OGM signals to (a) choose which modality to update next and (b) scale gradients within that modality to match cross-modal contribution targets, evaluating gains in both uni-modal encoder quality and fused performance.",
        "KL-Reconcilement Variants: f-Divergence and Bregman ReconBoost\nReplace KL reconcilement with alternative divergences (e.g., Jensen\u2013Shannon, \u03c7\u00b2, reverse-KL, or general Bregman divergences) to control whether the new modality update is conservative or exploratory. Provide an empirical map from divergence choice to behavior (e.g., stability vs. rapid correction) and a theoretical link to mirror descent / aggregating strategies from online learning.",
        "Memory-Consolidation as a Budgeted Expert Ensemble\nGeneralize ReconBoost\u2019s \u201ckeep only newest model per modality\u201d into a budgeted ensemble that retains a small set of historical snapshots chosen by an online pruning rule (e.g., remove the snapshot with least marginal loss reduction under a weighted-majority criterion). This makes consolidation principled and testable: quantify how many snapshots are needed to match full ensembling without overfitting.",
        "Modality Competition Diagnostics through Influence and Gradient Alignment\nBuild an actionable diagnostic toolkit that measures modality competition during training using (a) gradient cosine alignment across modalities, (b) influence-function estimates of each modality\u2019s features on the fused loss, and (c) per-modality \u201ccredit assignment\u201d curves. Use these diagnostics to predict failure modes early and to trigger switching/modulation in ReconBoost/OGM automatically.",
        "Missing/Corrupted Modality Robust ReconBoost with Adversarial Dropout Scheduling\nExtend modality-alternating boosting to handle partially missing modalities by training with adversarially chosen modality dropout patterns (worst-case removal schedule). Couple the dropout adversary with the modality-update scheduler (a minimax game akin to expert advice), and evaluate robustness to real sensor failures and distribution shifts.",
        "Weak-Modality Curriculum via Boosted Residual Targets\nIntroduce a curriculum where early training forces the weak modality to fit boosted residual targets derived from the strong modality\u2019s errors (explicit residual learning), then gradually relax to symmetric reconcilement. This operationalizes \u201cexploit uni-modal features first\u201d and can be tested by measuring whether weak-modality encoders gain linear-probe accuracy faster than with joint training.",
        "Global Rectification as Post-hoc Calibration Across Modalities\nTreat ReconBoost\u2019s global rectification as a calibration problem: learn a lightweight calibration head that enforces agreement constraints (e.g., temperature scaling + modality-dependent bias) while preserving each modality\u2019s decision boundary. Evaluate whether this improves reliability (ECE/Brier) and reduces dominance-induced overconfidence, especially when modalities disagree.",
        "Online Bandit Selection of Modality Updates under Compute Budgets\nModel \u201cwhich modality to update next\u201d as a contextual bandit where context includes gradient statistics, loss trends, and competition metrics; reward is validation loss improvement per FLOP. This yields a compute-aware multimodal trainer that can prioritize modalities dynamically under strict budgets, enabling deployment-oriented training where not all modalities can be updated equally often."
      ],
      "generated_ideas_raw": "1. **Decision-Theoretic ReconBoost with Regret Guarantees**  \n   Reformulate modality-alternating training (ReconBoost) as an online \u201cexperts\u201d problem where each modality-specific model is an expert, and the scheduler is a decision-theoretic controller. Derive explicit regret bounds (vs. the best fixed modality or best switching sequence) under KL-based reconcilement, then validate whether bounded-regret scheduling reduces modality competition and improves worst-case robustness across datasets.\n\n2. **Adaptive Modality Scheduling via Gradient-Boosting Line Search**  \n   Extend Friedman-style gradient boosting to multimodal alternating updates by adding an explicit per-step line search (step size) per modality based on loss decrease and reconcilement penalty. Implement a practical variant that estimates curvature using mini-batch Hessian traces, and test if adaptive step sizing prevents over-correction when a weak modality begins to improve.\n\n3. **ReconBoost + OGM Hybrid: Alternating Updates with Intra-Step Gradient Modulation**  \n   Combine ReconBoost\u2019s \u201cupdate one modality at a time\u201d with OGM\u2019s on-the-fly gradient modulation inside the selected modality update. Concretely, use OGM signals to (a) choose which modality to update next and (b) scale gradients within that modality to match cross-modal contribution targets, evaluating gains in both uni-modal encoder quality and fused performance.\n\n4. **KL-Reconcilement Variants: f-Divergence and Bregman ReconBoost**  \n   Replace KL reconcilement with alternative divergences (e.g., Jensen\u2013Shannon, \u03c7\u00b2, reverse-KL, or general Bregman divergences) to control whether the new modality update is conservative or exploratory. Provide an empirical map from divergence choice to behavior (e.g., stability vs. rapid correction) and a theoretical link to mirror descent / aggregating strategies from online learning.\n\n5. **Memory-Consolidation as a Budgeted Expert Ensemble**  \n   Generalize ReconBoost\u2019s \u201ckeep only newest model per modality\u201d into a budgeted ensemble that retains a small set of historical snapshots chosen by an online pruning rule (e.g., remove the snapshot with least marginal loss reduction under a weighted-majority criterion). This makes consolidation principled and testable: quantify how many snapshots are needed to match full ensembling without overfitting.\n\n6. **Modality Competition Diagnostics through Influence and Gradient Alignment**  \n   Build an actionable diagnostic toolkit that measures modality competition during training using (a) gradient cosine alignment across modalities, (b) influence-function estimates of each modality\u2019s features on the fused loss, and (c) per-modality \u201ccredit assignment\u201d curves. Use these diagnostics to predict failure modes early and to trigger switching/modulation in ReconBoost/OGM automatically.\n\n7. **Missing/Corrupted Modality Robust ReconBoost with Adversarial Dropout Scheduling**  \n   Extend modality-alternating boosting to handle partially missing modalities by training with adversarially chosen modality dropout patterns (worst-case removal schedule). Couple the dropout adversary with the modality-update scheduler (a minimax game akin to expert advice), and evaluate robustness to real sensor failures and distribution shifts.\n\n8. **Weak-Modality Curriculum via Boosted Residual Targets**  \n   Introduce a curriculum where early training forces the weak modality to fit boosted residual targets derived from the strong modality\u2019s errors (explicit residual learning), then gradually relax to symmetric reconcilement. This operationalizes \u201cexploit uni-modal features first\u201d and can be tested by measuring whether weak-modality encoders gain linear-probe accuracy faster than with joint training.\n\n9. **Global Rectification as Post-hoc Calibration Across Modalities**  \n   Treat ReconBoost\u2019s global rectification as a calibration problem: learn a lightweight calibration head that enforces agreement constraints (e.g., temperature scaling + modality-dependent bias) while preserving each modality\u2019s decision boundary. Evaluate whether this improves reliability (ECE/Brier) and reduces dominance-induced overconfidence, especially when modalities disagree.\n\n10. **Online Bandit Selection of Modality Updates under Compute Budgets**  \n   Model \u201cwhich modality to update next\u201d as a contextual bandit where context includes gradient statistics, loss trends, and competition metrics; reward is validation loss improvement per FLOP. This yields a compute-aware multimodal trainer that can prioritize modalities dynamically under strict budgets, enabling deployment-oriented training where not all modalities can be updated equally often.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Decision-Theoretic ReconBoost with Regret Guarantees\nReformulate modality-alternating training (ReconBoost) as an online \u201cexperts\u201d problem where each modality-specific model is an expert, and the sche",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Adaptive Modality Scheduling via Gradient-Boosting Line Search\nExtend Friedman-style gradient boosting to multimodal alternating updates by adding an explicit per-step line search (step size) per moda",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "ReconBoost + OGM Hybrid: Alternating Updates with Intra-Step Gradient Modulation\nCombine ReconBoost\u2019s \u201cupdate one modality at a time\u201d with OGM\u2019s on-the-fly gradient modulation inside the selected moda",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "KL-Reconcilement Variants: f-Divergence and Bregman ReconBoost\nReplace KL reconcilement with alternative divergences (e.g., Jensen\u2013Shannon, \u03c7\u00b2, reverse-KL, or general Bregman divergences) to control w",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Memory-Consolidation as a Budgeted Expert Ensemble\nGeneralize ReconBoost\u2019s \u201ckeep only newest model per modality\u201d into a budgeted ensemble that retains a small set of historical snapshots chosen by an ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Modality Competition Diagnostics through Influence and Gradient Alignment\nBuild an actionable diagnostic toolkit that measures modality competition during training using (a) gradient cosine alignment ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Missing/Corrupted Modality Robust ReconBoost with Adversarial Dropout Scheduling\nExtend modality-alternating boosting to handle partially missing modalities by training with adversarially chosen modal",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Weak-Modality Curriculum via Boosted Residual Targets\nIntroduce a curriculum where early training forces the weak modality to fit boosted residual targets derived from the strong modality\u2019s errors (ex",
          "is_match": true
        },
        {
          "idea_idx": 8,
          "idea_text": "Global Rectification as Post-hoc Calibration Across Modalities\nTreat ReconBoost\u2019s global rectification as a calibration problem: learn a lightweight calibration head that enforces agreement constraint",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Online Bandit Selection of Modality Updates under Compute Budgets\nModel \u201cwhich modality to update next\u201d as a contextual bandit where context includes gradient statistics, loss trends, and competition ",
          "is_match": false
        }
      ]
    }
  ]
}
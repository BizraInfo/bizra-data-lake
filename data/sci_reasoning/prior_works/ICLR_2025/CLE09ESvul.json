{
  "prior_works": [
    {
      "title": "Nonnegative Decomposition of Multivariate Information",
      "authors": "Paul L. Williams et al.",
      "year": 2010,
      "arxiv_id": "arXiv:1004.2515",
      "role": "Foundation",
      "relationship_sentence": "This paper introduced Partial Information Decomposition (PID) and the unique/redundant/synergistic partition that directly parameterizes the neuron-local objectives designed here."
    },
    {
      "title": "Quantifying Unique Information",
      "authors": "Nils Bertschinger et al.",
      "year": 2014,
      "arxiv_id": "arXiv:1311.2852",
      "role": "Extension",
      "relationship_sentence": "It provides an operational, optimization-based definition of unique information under fixed marginals, supplying the concrete semantics for the \u2018unique\u2019 term that the proposed local objective explicitly targets."
    },
    {
      "title": "Measuring multivariate redundant information with pointwise common change in surprisal",
      "authors": "Robin A. A. Ince",
      "year": 2017,
      "arxiv_id": "arXiv:1702.01423",
      "role": "Extension",
      "relationship_sentence": "By introducing a pointwise (sample-wise) redundancy/synergy measure, this work enables local decompositions that inform the construction of differentiable, neuron-local PID-weighted losses."
    },
    {
      "title": "BROJA-2PID: A Configurable Framework for Unique Information Decomposition of Two Sources",
      "authors": "Abdullah Makkeh et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This toolbox operationalizes PID via practical estimators of unique, redundant, and synergistic information, making it feasible to optimize such quantities as targets for neuron-level learning."
    },
    {
      "title": "Towards deep learning with segregated dendrites",
      "authors": "H. Guerguiev et al.",
      "year": 2017,
      "arxiv_id": "arXiv:1610.00161",
      "role": "Inspiration",
      "relationship_sentence": "It formalizes neurons receiving distinct feedforward, feedback, and lateral inputs for local learning, motivating the explicit treatment of these input classes whose contributions are shaped via PID."
    },
    {
      "title": "An approximation of the error backpropagation algorithm in a predictive coding network",
      "authors": "James C. R. Whittington et al.",
      "year": 2017,
      "arxiv_id": "arXiv:1711.07420",
      "role": "Related Problem",
      "relationship_sentence": "Demonstrating local objectives with feedback and lateral signals, this work highlights the need for principled, interpretable neuron-level goals\u2014here provided by explicit control over unique/redundant/synergistic integration."
    },
    {
      "title": "Dendritic cortical microcircuits approximate the backpropagation algorithm",
      "authors": "Jo\u00e3o Sacramento et al.",
      "year": 2018,
      "arxiv_id": "arXiv:1810.11393",
      "role": "Related Problem",
      "relationship_sentence": "By showing compartmentalized neurons can use feedforward and feedback streams for credit assignment, it motivates designing local objectives that dictate how those streams should uniquely, redundantly, or synergistically contribute."
    }
  ],
  "synthesis_narrative": "Partial Information Decomposition (PID) introduced a principled way to parse how multiple sources relate to a target into unique, redundant, and synergistic components, establishing the vocabulary for controlling multi-source integration at a fine-grained level (Williams and Beer, 2010). An optimization-based semantics for unique information under fixed marginals anchored the meaning of \u2018unique\u2019 and provided a basis for deriving computable objectives (Bertschinger et al., 2014). Pointwise common change in surprisal supplied local (sample-wise) redundancy/synergy terms, making it possible to attribute information contributions at a resolution compatible with neuron-level learning signals (Ince, 2017). Practical PID toolkits and estimators, such as BROJA-2PID, translated these definitions into tractable computations, enabling optimization of unique, redundant, and synergistic quantities in practice (Makkeh et al., 2018). In parallel, neuroscience-inspired models posited neurons with distinct feedforward, feedback, and lateral compartments and showed that local learning can exploit these segregated streams (Guerguiev et al., 2017; Sacramento et al., 2018). Predictive-coding formulations further demonstrated that feedback and lateral signals can implement local objectives capable of training deep networks (Whittington and Bogacz, 2017). Together, these strands exposed a gap: while compartmentalized local learning was feasible, there was no principled, interpretable way to specify what each neuron should integrate from each input class. By marrying PID\u2019s unique\u2013redundant\u2013synergistic calculus with compartmentalized input streams, the current work formulates explicit neuron-local objectives that weight and optimize these information components, yielding self-organized, interpretable control over how feedforward, feedback, and lateral inputs are integrated.",
  "target_paper": {
    "title": "What should a neuron aim for? Designing local objective functions based on information theory",
    "authors": "Andreas Christian Schneider, Valentin Neuhaus, David Alexander Ehrlich, Abdullah Makkeh, Alexander S Ecker, Viola Priesemann, Michael Wibral",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "local learning, interpretability, neuro-inspired, information theory, partial information decomposition",
    "abstract": "In modern deep neural networks, the learning dynamics of individual neurons are often obscure, as the networks are trained via global optimization. Conversely, biological systems build on self-organized, local learning, achieving robustness and efficiency with limited global information. Here, we show how self-organization between individual artificial neurons can be achieved by designing abstract bio-inspired local learning goals. These goals are parameterized using a recent extension of information theory, Partial Information Decomposition (PID), which decomposes the information that a set of information sources holds about an outcome into unique, redundant and synergistic contributions. Our framework enables neurons to locally shape the integration of information from various input classes, i.e., feedforward, feedback, and lateral, by selecting which of the three inputs should contribute uniquely, redundantly or synergistically to the output. This selection is expressed as a weighte",
    "openreview_id": "CLE09ESvul",
    "forum_id": "CLE09ESvul"
  },
  "analysis_timestamp": "2026-01-06T10:17:01.885532"
}
{
  "prior_works": [
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu et al.",
      "year": 2021,
      "arxiv_id": "2106.09685",
      "role": "Extension",
      "relationship_sentence": "LongLoRA builds directly on LoRA\u2019s low-rank adapters as the parameter-efficient backbone, extending the method to the long-context regime by placing/adapting adapters in attention pathways to learn long-range behavior without updating full weights."
    },
    {
      "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "authors": "Ze Liu et al.",
      "year": 2021,
      "arxiv_id": "2103.14030",
      "role": "Inspiration",
      "relationship_sentence": "The shifted-window mechanism in Swin directly inspires LongLoRA\u2019s shifted sparse attention, enabling cross-window token interaction during training without paying the cost of full dense attention."
    },
    {
      "title": "Longformer: The Long-Document Transformer",
      "authors": "Iz Beltagy et al.",
      "year": 2020,
      "arxiv_id": "2004.05150",
      "role": "Inspiration",
      "relationship_sentence": "Longformer\u2019s sliding-window local attention established that local sparsity can preserve performance on long documents, a principle LongLoRA leverages by substituting dense attention with local windows during fine-tuning."
    },
    {
      "title": "Big Bird: Transformers for Longer Sequences",
      "authors": "Manzil Zaheer et al.",
      "year": 2020,
      "arxiv_id": "2007.14062",
      "role": "Gap Identification",
      "relationship_sentence": "BigBird showed block-sparse attention can approximate full attention with theoretical connectivity guarantees but requires sparse patterns at inference, a limitation LongLoRA addresses by using sparse attention only for training and dense global attention at inference."
    },
    {
      "title": "Generating Long Sequences with Sparse Transformers",
      "authors": "Rewon Child et al.",
      "year": 2019,
      "arxiv_id": "1904.10509",
      "role": "Inspiration",
      "relationship_sentence": "Child et al. demonstrated that combining local and strided sparse patterns expands receptive fields with sub-quadratic cost, a key insight operationalized in LongLoRA via layer-wise window shifts to propagate information globally."
    },
    {
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "authors": "Tim Dettmers et al.",
      "year": 2023,
      "arxiv_id": "2305.14314",
      "role": "Baseline",
      "relationship_sentence": "QLoRA is the primary PEFT efficiency baseline LongLoRA compares against, with LongLoRA reducing long-context fine-tuning cost further by replacing dense attention with shifted sparse attention during training."
    }
  ],
  "synthesis_narrative": "Low-rank adaptation with LoRA showed that large language models can be tuned effectively by inserting small trainable low-rank matrices into attention and feedforward layers, preserving full-model quality with minimal trainable parameters. Sparse Transformers revealed that judiciously structured sparsity\u2014mixing local and strided patterns\u2014can greatly expand the receptive field while keeping compute sub-quadratic. Longformer sharpened this idea for text by demonstrating that sliding-window local attention retains strong performance on long-document tasks, reducing attention complexity while maintaining utility. BigBird generalized sparse layouts with block patterns and provided theoretical guarantees of global connectivity and expressivity, but retained sparse patterns at inference, implying potential quality trade-offs or interface constraints. Meanwhile, Swin Transformer introduced shifted windows across layers, a simple yet powerful mechanism to allow cross-window communication without full global attention, showing that layer-wise window misalignment can efficiently propagate information.\nTogether, these works exposed a clear opportunity: use local sparse attention to achieve efficient training-time signal propagation (via window shifts for connectivity), but keep dense global attention at inference to avoid any sparsity-induced limitations. LongLoRA synthesizes these insights by adopting LoRA for parameter-efficient adaptation and introducing shifted sparse attention during fine-tuning so long-range dependencies are learned inexpensively; at inference, it restores dense attention, marrying training efficiency with full-capacity reasoning over long contexts and outperforming PEFT baselines like QLoRA in the long-context setting.",
  "target_paper": {
    "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models",
    "authors": "Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Efficient fine-tuning, Long context, Large language model",
    "abstract": "We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost.\nTypically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-eff",
    "openreview_id": "6PmJoRfdaK",
    "forum_id": "6PmJoRfdaK"
  },
  "analysis_timestamp": "2026-01-06T19:34:45.561723"
}
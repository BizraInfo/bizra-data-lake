{
  "prior_works": [
    {
      "title": "Open Catalyst 2020 (OC20) Dataset and Community Challenges",
      "authors": "A. L. Chanussot et al.",
      "year": 2020,
      "role": "Large-scale dataset and cross-domain benchmark for atomic catalysis",
      "relationship_sentence": "UMA\u2019s universal training across molecules, materials, and catalysts is built on the large-scale, 3D atomic data paradigm and benchmarking culture established by OC20, co-led by UMA authors, and extends it to far larger, multi-domain corpora."
    },
    {
      "title": "EGNN: E(n) Equivariant Graph Neural Networks",
      "authors": "Victor Garcia Satorras, Emiel Hoogeboom, Max Welling",
      "year": 2021,
      "role": "Architectural foundation for efficient E(3)-equivariant message passing",
      "relationship_sentence": "UMA\u2019s fast, accurate atomic property prediction leverages the design principles of E(3)-equivariant message passing popularized by EGNN to maintain geometric fidelity while remaining computationally scalable."
    },
    {
      "title": "NequIP: A Graph Convolutional Network for Accurate Interatomic Potentials",
      "authors": "Tobias Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P. Mailoa, Mordechai Kornbluth, Nicola Molinari, Boris Kozinsky",
      "year": 2022,
      "role": "High-accuracy SE(3)-equivariant interatomic potentials",
      "relationship_sentence": "UMA\u2019s emphasis on accuracy-speed tradeoffs and locality in predicting energies and forces draws directly on NequIP\u2019s demonstration that equivariant, local energy models can achieve state-of-the-art accuracy at practical costs."
    },
    {
      "title": "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions",
      "authors": "Kristof T. Sch\u00fctt, Pieter-Jan Kindermans, Huziel E. Sauceda, Stefan Chmiela, Alexandre Tkatchenko, Klaus-Robert M\u00fcller",
      "year": 2018,
      "role": "Foundational architecture for molecules and materials in 3D",
      "relationship_sentence": "UMA\u2019s aspiration to a universal atomistic model follows the trajectory initiated by SchNet of using learned continuous filters on atomic neighborhoods to unify molecules and materials within a single neural framework."
    },
    {
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "authors": "William Fedus, Barret Zoph, Noam Shazeer",
      "year": 2021,
      "role": "Mixture-of-experts scaling with limited active parameters",
      "relationship_sentence": "UMA\u2019s \u201cmixture of linear experts\u201d explicitly adapts the Switch-style sparse expert routing idea to the atomistic setting to grow parameter count while keeping roughly constant active compute per atom."
    },
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, et al.",
      "year": 2020,
      "role": "Methodology for empirical scaling laws and compute/data tradeoffs",
      "relationship_sentence": "UMA\u2019s empirical scaling-law analysis for model and dataset sizing is directly inspired by the Kaplan et al. framework for quantifying performance as a function of parameters, data, and compute."
    }
  ],
  "synthesis_narrative": "UMA positions itself as a universal, high-capacity yet fast model class for atomistic simulation by synthesizing three strands of prior progress. First, the emergence of large, standardized 3D atomic datasets and tasks\u2014exemplified by the Open Catalyst 2020 (OC20) effort\u2014established both the data scale and cross-domain evaluation protocols that UMA now expands dramatically by aggregating molecules, materials, and catalysts into a single training corpus. Second, UMA\u2019s architectural backbone inherits the geometric fidelity and efficiency of E(3)-equivariant neural networks: SchNet pioneered continuous-filter convolutions unifying molecules and materials; EGNN distilled equivariant message passing into a scalable, lightweight form; and NequIP showed that equivariant local-energy models can deliver state-of-the-art accuracy with favorable speed/accuracy trade-offs. Building on these, UMA introduces a mixture of linear experts to raise parameter capacity without increasing active compute, a direct adaptation of the sparse Mixture-of-Experts paradigm popularized by Switch Transformers to the atomistic regime, where per-atom expert routing preserves throughput. Third, UMA adopts a principled scaling lens inspired by the neural scaling literature (Kaplan et al.), empirically mapping accuracy as a function of dataset size and model capacity to guide compute-optimal training at unprecedented data scales. Together, these influences yield UMA\u2019s core contribution: a family of equivariant, sparsely-routed atomic models that scale over half a billion structures, achieving strong generalization across chemical domains while maintaining practical inference speed.",
  "analysis_timestamp": "2026-01-07T00:02:04.936052"
}
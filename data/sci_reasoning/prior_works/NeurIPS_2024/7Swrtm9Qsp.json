{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Kernel/linearization benchmark for overparameterized training",
      "relationship_sentence": "The paper positions its contribution against NTK-style linearization, showing that in 1D noisy regression NTK is provably suboptimal, motivating a theory that instead characterizes feature-learning minima reached by finite-step-size gradient descent."
    },
    {
      "title": "Benign overfitting in linear regression",
      "authors": "Peter L. Bartlett, Philip M. Long, G\u00e1bor Lugosi, Alexander Tsigler",
      "year": 2020,
      "role": "Foundational theory of interpolation/generalization in noisy overparameterized models",
      "relationship_sentence": "By identifying regimes where interpolation can still generalize, this work frames the present paper\u2019s contrasting regime where benign overfitting does not occur, motivating the focus on non-interpolating, stable local minima and their generalization."
    },
    {
      "title": "Breaking the curse of dimensionality with convex neural networks",
      "authors": "Francis Bach",
      "year": 2017,
      "role": "Function-space view and variation/path-norm for two-layer ReLU networks",
      "relationship_sentence": "The authors\u2019 bound on a weighted first-order total variation directly echoes Bach\u2019s variation/path-norm for two-layer ReLU networks, providing the function-space lens that links gradient-descent-selected minima to controlled smoothness and generalization."
    },
    {
      "title": "Locally Adaptive Regression Splines",
      "authors": "Enno Mammen, Sara Van de Geer",
      "year": 1997,
      "role": "Spline/TV regularization yielding minimax rates",
      "relationship_sentence": "Their TV-based spline estimators achieve the n^{-4/5} rate in 1D, underpinning the paper\u2019s nearly-optimal MSE target and the link between TV-type smoothness control and nonparametric optimality for piecewise-linear function classes."
    },
    {
      "title": "Adaptive piecewise polynomial estimation via trend filtering",
      "authors": "Tibshirani, Ryan J.",
      "year": 2014,
      "role": "Trend filtering and total variation of derivatives",
      "relationship_sentence": "Trend filtering formalizes L1 control of discrete derivatives to achieve near-minimax 1D rates, closely mirroring the paper\u2019s characterization of GD-selected solutions as low-TV functions and explaining the n^{-4/5} convergence rate."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro",
      "year": 2018,
      "role": "Implicit bias of optimization dynamics",
      "relationship_sentence": "This line of work establishes that algorithmic choices (e.g., GD dynamics) select structured solutions; the present paper extends this philosophy to show constant step size enforces a bound on a TV-type functional, preventing overfitting to noise."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat, Edouard Oyallon, Francis Bach",
      "year": 2019,
      "role": "Kernel (lazy) vs feature-learning regimes of GD",
      "relationship_sentence": "By distinguishing NTK/lazy from feature-learning regimes, this work motivates analyzing finite step-size GD that escapes lazy linearization; the paper\u2019s stability-based characterization targets precisely these non-kernel, feature-learning minima."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation is a function-space characterization of the local minima that constant\u2013step-size gradient descent can stably reach in univariate two-layer ReLU regression with noise, showing these minima necessarily implement smooth, low-variation functions and thus cannot overfit. This advances beyond the NTK/lazy-training view (Jacot et al.; Chizat & Bach), which linearizes dynamics and is known to be suboptimal in this setting. Instead, the authors analyze the feature-learning regime under finite learning rates, connecting optimization stability to a bound on a weighted first-order total variation of the learned function.\n\nThis total-variation perspective is rooted in the convex neural networks framework (Bach), where two-layer ReLU models admit a variation/path-norm controlling function complexity. In 1D, such networks correspond to adaptive piecewise-linear splines whose smoothness can be described by TV of derivatives\u2014a connection long exploited by locally adaptive regression splines (Mammen & van de Geer) and trend filtering (Tibshirani) to achieve near-minimax n^{-4/5} error rates. The paper leverages precisely this structure to show that stable GD minima have TV bounded in terms of the step size (\u221d 1/\u03b7), implying a near-optimal MSE rate ~n^{-4/5} under mild conditions.\n\nConceptually, the work extends the implicit bias program (Soudry et al.) from classification/max-margin to noisy regression: algorithmic details\u2014here, constant step size and dynamical stability\u2014impose an implicit TV regularizer on the realized function. In a regime where benign overfitting does not occur (Bartlett et al.), this implicit regularization precludes interpolation of noise and yields provable generalization.",
  "analysis_timestamp": "2026-01-06T23:42:49.024611"
}
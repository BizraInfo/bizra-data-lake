{
  "prior_works": [
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "Introduced the interleaved reasoning\u2013action trajectory format that T3-Agent explicitly imitates during trajectory tuning for tool-usage reasoning."
    },
    {
      "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action",
      "authors": "Zhengyuan Yang et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated a VLM-driven controller that calls visual tools via prompting but lacked supervised tuning and reliable trajectories, a limitation this paper addresses with MM-Traj and trajectory tuning."
    },
    {
      "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
      "authors": "Chenfei Wu et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Showed the paradigm of an LLM/VLM as a controller orchestrating external visual tools, directly motivating this work\u2019s goal of training a VLM controller rather than relying on handcrafted prompts."
    },
    {
      "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "authors": "Timo Schick et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Provided the core idea that LMs can auto-generate tool-use supervision; this paper extends that idea to multimodal settings with query-file and trajectory verifiers to ensure high-quality tool-usage data."
    },
    {
      "title": "Gorilla: Large Language Model Connected with Massive APIs",
      "authors": "Shishir G. Patil et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "Established supervised training for function calling and API selection; T3-Agent generalizes this to VLMs and multi-step, multimodal tool-usage trajectories."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "Pioneered instruction tuning for VLMs; the present work builds on this by replacing generic response tuning with trajectory tuning tailored to tool-usage control."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "Provided the methodology for LLM-driven synthetic data generation that this paper adapts to create multimodal queries/files and verified tool-use trajectories for MM-Traj."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014training a vision-language model as a reliable tool-using controller via trajectory tuning on auto-generated, verified multimodal data\u2014arises from two converging lines of work. First, ReAct established the reasoning\u2013action trajectory formulation that defines what an agent should produce at each step. MM-REACT and Visual ChatGPT demonstrated this paradigm in multimodal settings, with VLMs orchestrating external visual tools through prompting; however, their reliance on handcrafted prompts and lack of robust, supervised trajectories exposed a gap in scalability and reliability that this paper targets.\nSecond, Toolformer and Gorilla showed that language models can learn tool use through self-annotation and supervised training on function calls. The present work extends these ideas to multimodal contexts, moving beyond text-only APIs to include files and visual artifacts, and scaling from single calls to multi-step trajectories. LLaVA\u2019s visual instruction tuning provides the training scaffold for aligning VLMs; here, generic instruction tuning is replaced with trajectory tuning specialized for tool usage. Finally, Self-Instruct informs the data generation pipeline: instead of human-authored trajectories, GPT-4o is prompted to produce multimodal tasks, files, and actions, with dedicated verifiers ensuring correctness and quality. Together, these works directly enable MM-Traj and the T3-Agent, transforming prompt-based multimodal tool orchestration into a trained, reliable VLM controller.",
  "analysis_timestamp": "2026-01-06T23:09:26.602785"
}
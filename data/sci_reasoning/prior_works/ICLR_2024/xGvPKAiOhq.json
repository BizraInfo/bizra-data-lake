{
  "prior_works": [
    {
      "title": "Global Optimality of Local Search for Low Rank Matrix Recovery",
      "authors": "Bhojanapalli et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work established that the nonconvex factorized formulation of matrix sensing has a benign landscape (no spurious local minima under near-isotropic/RIP measurements), which the current paper leverages to focus purely on the dynamics of GD and show that over-parameterization alone can slow convergence."
    },
    {
      "title": "Implicit Regularization in Matrix Factorization",
      "authors": "Gunasekar et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Their analysis that gradient descent rapidly enforces balanced factors and respects rotational symmetries in UV/XX\u1d40 parameterizations directly motivates the paper\u2019s \"curse of symmetry/initialization\" mechanism used to construct slow GD trajectories in over-parameterized settings."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Saxe et al.",
      "year": 2013,
      "arxiv_id": "1312.6120",
      "role": "Related Problem",
      "relationship_sentence": "By characterizing mode-wise alignment and polynomial-time transients caused by symmetry in deep linear models, this paper provides the dynamical template the current work adapts to two-factor linear models to rigorously derive 1/T^2 lower bounds under over-parameterization."
    },
    {
      "title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "authors": "Recht et al.",
      "year": 2010,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized matrix sensing with near-isotropic linear measurements and RIP-type assumptions, which is precisely the measurement model under which the current paper proves its GD convergence lower bounds."
    },
    {
      "title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization",
      "authors": "Burer et al.",
      "year": 2003,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Introducing the symmetric XX\u1d40 factorization and its orthogonal invariance, this work underpins the rotational symmetry that the current paper identifies as a principal cause of slowed GD dynamics in the over-parameterized PSD setting."
    },
    {
      "title": "Low-rank solutions of linear matrix equations via Procrustes Flow",
      "authors": "Tu et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Procrustes Flow established linear (exponential) convergence rates for gradient-based methods under exact parameterization with suitable initialization, providing the sharp baseline the current paper contrasts against when proving polynomial 1/T^2 rates for over-parameterized, randomly initialized GD."
    }
  ],
  "synthesis_narrative": "Low-rank matrix sensing was crystallized as recovering a low-rank matrix from near-isotropic linear measurements, with Recht et al. showing that such operators satisfy conditions enabling recovery via convex surrogates. Burer and Monteiro advocated factorizing PSD matrices as XX\u1d40, revealing orthogonal invariances that create flat directions in the parameter space. Within this nonconvex factorized formulation, Bhojanapalli et al. proved the landscape is benign under RIP-like assumptions\u2014local minima are global\u2014shifting the central question from geometry to optimization dynamics. Tu et al.\u2019s Procrustes Flow then demonstrated that, with exact parameterization and a good initialization, gradient-based methods achieve linear (exponential) convergence, highlighting the potential for very fast recovery in well-posed regimes. Orthogonal symmetries and balancing, however, strongly shape dynamics: Gunasekar et al. showed gradient methods in factorized linear models implicitly enforce balanced factors and respect rotational invariance, constraining how signal directions can grow. In deep linear networks, Saxe et al. derived exact dynamics where symmetry-induced alignment causes slow, polynomial transients, offering a dynamical mechanism tied to invariant subspaces. Together, these works implied a striking open point: despite a benign landscape and fast rates under exact parameterization, over-parameterization plus random initialization could fundamentally alter GD trajectories. The current paper synthesizes these insights by leveraging measurement isotropy, rotational invariance from XX\u1d40/UV\u1d40 parameterizations, and balancedness to construct trajectories where signal alignment is bottlenecked, proving 1/T^2 lower bounds and contrasting them with exponential rates in the exact-parameterization case, thus pinpointing the curses of symmetry and initialization.",
  "target_paper": {
    "title": "How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization",
    "authors": "Nuoya Xiong, Lijun Ding, Simon Shaolei Du",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "non-convex optimization, random initialization, global convergence, matrix recovery, matrix sensing",
    "abstract": "This paper rigorously shows how over-parameterization dramatically changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements.\nFirst, we consider the symmetric setting with the symmetric parameterization where $M^* \\in \\mathbb{R}^{n \\times n}$ is a positive semi-definite unknown matrix of rank $r \\ll n$, and one uses a symmetric parameterization $XX^\\top$ to learn $M^*$. Here $X \\in \\mathbb{R}^{n \\times k}$ with $k > r$ is the factor matrix. We give a novel $\\Omega\\left(1/T^2\\right)$ lower bound of randomly initialized GD for the over-parameterized case ($k >r$) where $T$ is the number of iterations. This is in stark contrast to the exact-parameterization scenario ($k=r$) where the convergence rate is $\\exp\\left(-\\Omega\\left(T\\right)\\right)$. Next, we study asymmetric setting where $M^* \\in \\mathbb{R}^{n_1 \\times n_2}$ is the unknown matrix of ran",
    "openreview_id": "xGvPKAiOhq",
    "forum_id": "xGvPKAiOhq"
  },
  "analysis_timestamp": "2026-01-06T16:09:50.097302"
}
{
  "prior_works": [
    {
      "title": "RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning",
      "authors": "Yan Duan et al.",
      "year": 2016,
      "role": "Foundational meta-RL",
      "relationship_sentence": "Paprika\u2019s core idea\u2014training across tasks so a policy adapts online to new tasks without gradient updates\u2014echoes RL^2\u2019s meta-learning formulation of within-episode adaptation from interaction histories."
    },
    {
      "title": "Curiosity-driven Exploration by Self-supervised Prediction (ICM)",
      "authors": "Deepak Pathak et al.",
      "year": 2017,
      "role": "Intrinsic motivation for exploration",
      "relationship_sentence": "By centering general-purpose exploration competence, Paprika builds on the ICM tradition of curiosity-driven behavior, but seeks a task-agnostic, transferable form learned via multi-task interaction sequences rather than a specific intrinsic reward."
    },
    {
      "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
      "authors": "Lili Chen et al.",
      "year": 2021,
      "role": "Sequence modeling for decision-making from trajectories",
      "relationship_sentence": "Paprika leverages the insight that autoregressive models can be trained on trajectories to produce actions; it extends this to diverse synthetic interaction data to elicit adaptive, exploration-aware policies that learn in-context."
    },
    {
      "title": "A Generalist Agent (Gato)",
      "authors": "Scott Reed et al.",
      "year": 2022,
      "role": "Multi-task sequence-model agent",
      "relationship_sentence": "Gato demonstrated that a single transformer trained across heterogeneous tasks can act generally; Paprika adopts this generalist framing but targets transferable exploration and adaptation dynamics across environments."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "LLMs using action\u2013observation traces",
      "relationship_sentence": "Paprika\u2019s use of interleaved action and feedback to drive in-context behavioral adjustment draws on ReAct\u2019s paradigm that LMs can condition on environment feedback to refine subsequent decisions."
    },
    {
      "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
      "authors": "Guanzhi Wang et al.",
      "year": 2023,
      "role": "Open-ended exploration and curriculum in an LLM agent",
      "relationship_sentence": "Paprika\u2019s emphasis on exploration skills and its curriculum for sample efficiency parallel Voyager\u2019s automatic curriculum and skill acquisition, while generalizing beyond a single environment."
    },
    {
      "title": "Curriculum Learning",
      "authors": "Yoshua Bengio et al.",
      "year": 2009,
      "role": "Training strategy for sample efficiency and capability growth",
      "relationship_sentence": "Paprika\u2019s curriculum over synthetic interaction data directly applies curriculum learning principles to stage task difficulty and improve exploration skill acquisition with fewer samples."
    }
  ],
  "synthesis_narrative": "Paprika\u2019s key contribution\u2014fine-tuning language models on diverse synthetic interaction data so they develop general, transferable exploration and decision-making that adapts in-context\u2014sits at the intersection of meta-RL, sequence-model decision making, and LLM agents that learn from action\u2013observation feedback. The meta-learning backbone comes from RL^2, which established that training over task families can yield within-episode adaptation without gradient updates; Paprika ports this idea to transformers operating over textual interaction traces. Decision Transformer provided the methodological bridge that trajectories can be modeled autoregressively to produce actions via supervised learning, enabling Paprika to scale decision-making via sequence modeling over synthetic interactions rather than repeated policy-gradient updates.\n\nTo make exploration a first-class, generalizable skill, Paprika inherits the curiosity-driven exploration ethos of ICM but seeks environment-agnostic competence instead of hand-crafted intrinsic rewards. From the LLM-agent side, ReAct showed that interleaving reasoning with acting and conditioning on environment feedback can improve performance; Paprika operationalizes this by training on multi-task action\u2013observation sequences that teach models to update behavior in-context. Gato\u2019s success training a single transformer across many embodiments and tasks motivates Paprika\u2019s generalist, cross-environment training regime. Finally, practical sample efficiency is addressed through curriculum learning principles\u2014echoed in open-ended agents like Voyager\u2014which inform Paprika\u2019s curriculum over synthetic interactions to accelerate acquisition of exploration strategies. Together, these priors directly scaffold Paprika\u2019s design: sequence-modeled, meta-learned, feedback-conditioned exploration that transfers to new tasks without additional fine-tuning.",
  "analysis_timestamp": "2026-01-07T00:21:33.182288"
}
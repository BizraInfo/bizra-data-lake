{
  "prior_works": [
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan et al.",
      "year": 2020,
      "arxiv_id": "2001.08361",
      "role": "Foundation",
      "relationship_sentence": "This work established the power-law framework relating loss to model size and data, which the current paper directly extends by adding sparsity (non\u2011zero parameter count) as an explicit variable in the scaling formulation and fitting methodology."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann et al.",
      "year": 2022,
      "arxiv_id": "2203.15556",
      "role": "Extension",
      "relationship_sentence": "By formalizing compute-optimal trade-offs between data and parameters (Chinchilla), this paper provides the compute/data lens that the current work generalizes to include weight sparsity and from which it derives the notion of data-dependent optimal sparsity."
    },
    {
      "title": "Scaling Vision Transformers",
      "authors": "Xiaohua Zhai et al.",
      "year": 2022,
      "arxiv_id": "2112.01514",
      "role": "Foundation",
      "relationship_sentence": "Their ViT/JFT scaling setup and empirical protocols in vision serve as the foundation for validating sparsity-aware scaling laws cross-domain, which the current paper adopts to test its theory on ViT/JFT-4B."
    },
    {
      "title": "Rigging the Lottery: Making All Tickets Winners",
      "authors": "Utku Evci et al.",
      "year": 2020,
      "arxiv_id": "1911.11134",
      "role": "Inspiration",
      "relationship_sentence": "RigL showed sparse-from-scratch training with dynamic connectivity can match dense models, directly motivating the present study\u2019s analysis of sparsity scaling when training sparse Transformers (as opposed to only pruning after pretraining)."
    },
    {
      "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
      "authors": "Elias Frantar et al.",
      "year": 2023,
      "arxiv_id": "2301.00774",
      "role": "Baseline",
      "relationship_sentence": "As a state-of-the-art post-training pruning approach for large Transformers, SparseGPT provides the dense-to-sparse baseline regime that the current paper explicitly evaluates when examining scaling behavior starting from a pretrained dense model."
    },
    {
      "title": "The State of Sparsity in Deep Neural Networks",
      "authors": "Trevor Gale et al.",
      "year": 2019,
      "arxiv_id": "1902.09574",
      "role": "Gap Identification",
      "relationship_sentence": "This survey documented the strengths and limitations of pruning and sparse training\u2014especially the lack of systematic, large-scale evaluation\u2014highlighting the precise gap the current paper fills by deriving and validating sparsity scaling laws at foundation-model scale."
    },
    {
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "authors": "William Fedus et al.",
      "year": 2021,
      "arxiv_id": "2101.03961",
      "role": "Related Problem",
      "relationship_sentence": "By demonstrating that activation sparsity (MoE) improves scaling under fixed compute, this work contextualizes and inspires the current paper\u2019s complementary exploration of weight sparsity within a scaling-law framework."
    }
  ],
  "synthesis_narrative": "Kaplan et al. established that language model loss follows power-law relations with model size and dataset size, crystallizing a quantitative framework for extrapolating performance across scales. Hoffmann et al. refined this view by showing compute-optimal trade-offs between parameters and data, recasting scaling as an allocation problem along a compute budget. In vision, Zhai et al. demonstrated consistent scaling behavior for Vision Transformers on JFT, providing experimental protocols and a high-scale setting to probe such laws beyond NLP. Evci et al. (RigL) showed that sparse-from-scratch training with dynamic connectivity can match dense models, validating weight sparsity as a viable training regime rather than merely a post hoc compression step. Frantar et al. (SparseGPT) proved large Transformers can be pruned in one shot post-training with minimal loss, establishing a competitive dense-to-sparse pathway. Gale et al. catalogued pruning methods and exposed the lack of systematic, large-scale evaluations and unifying principles for sparsity. Fedus et al. (Switch Transformers) revealed that sparsity can improve scaling under fixed compute via activation sparsity, underscoring the broader promise of sparsity-aware scaling.\nSynthesizing these, a gap emerges: while scaling laws rigorously capture model/data trade-offs and sparsity methods work in practice, there is no scaling law that explicitly models weight sparsity. Building on the functional forms and compute-optimal perspective, and leveraging ViT/JFT and T5/C4 protocols, the current work introduces sparsity (non-zero parameters) into the scaling relationship, compares sparse-from-scratch versus dense-to-sparse regimes, and characterizes a data-dependent optimal sparsity\u2014thus providing the missing quantitative law governing weight sparsity at foundation-model scale.",
  "target_paper": {
    "title": "Scaling Laws for Sparsely-Connected Foundation Models",
    "authors": "Elias Frantar, Carlos Riquelme Ruiz, Neil Houlsby, Dan Alistarh, Utku Evci",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "sparsity, scaling, optimal sparsity, efficiency, foundational models, transformers, structured sparsity, pruning",
    "abstract": "We explore the impact of parameter sparsity on the scaling behavior of Transformers trained on massive datasets (i.e., \"foundation models\"), in both vision and language domains. In this setting, we identify the first scaling law describing the relationship between weight sparsity, number of non-zero parameters, and amount of training data, which we validate empirically across model and data scales; on ViT/JFT-4B and T5/C4. These results allow us to characterize the \"optimal sparsity\", the sparsity level which yields the best performance for a given effective model size and training budget. For a fixed number of non-zero parameters, we identify that the optimal sparsity increases with the amount of data used for training. We also extend our study to different sparsity structures (such as the hardware-friendly n:m pattern) and strategies (such as starting from a pretrained dense model). Our findings shed light on the power and limitations of weight sparsity across various parameter and c",
    "openreview_id": "i9K2ZWkYIP",
    "forum_id": "i9K2ZWkYIP"
  },
  "analysis_timestamp": "2026-01-06T19:59:03.062881"
}
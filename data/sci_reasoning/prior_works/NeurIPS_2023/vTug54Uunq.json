{
  "prior_works": [
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry; Elad Hoffer; Mor Shpigel Nacson; Suriya Gunasekar; Nathan Srebro",
      "year": 2018,
      "role": "Foundational result showing gradient descent on separable logistic loss converges in direction to the L2 maximal-margin classifier, with baseline margin convergence rates.",
      "relationship_sentence": "This paper provides the benchmark phenomenon and rate template that the present work seeks to match for non-Euclidean algorithms by accelerating margin maximization for mirror and steepest descent."
    },
    {
      "title": "Convergence of Gradient Descent on Separable Data",
      "authors": "Mor Shpigel Nacson; Suriya Gunasekar; Elad Hoffer; Daniel Soudry; Nathan Srebro",
      "year": 2019,
      "role": "Refined last-iterate dynamics and rates for gradient descent and analyzed normalized gradient descent which achieves faster margin growth.",
      "relationship_sentence": "Their sharper rate analyses and normalization-based arguments inform the proof techniques and serve as performance baselines that the new accelerated rates for generic methods are compared against."
    },
    {
      "title": "Risk and Parameter Convergence of Logistic Regression",
      "authors": "Ziwei Ji; Matus Telgarsky",
      "year": 2019,
      "role": "Quantified how risk decay relates to directional convergence to the max-margin solution for gradient descent on logistic loss.",
      "relationship_sentence": "The present paper echoes this loss-to-margin translation and directional analysis, adapting it to non-Euclidean geometries to derive faster margin growth for mirror and steepest descent."
    },
    {
      "title": "Implicit Bias of Mirror Descent",
      "authors": "Suriya Gunasekar; Blake Woodworth; Jason D. Lee; Nati Srebro",
      "year": 2018,
      "role": "Established that mirror descent and steepest descent converge to maximal-margin classifiers in geometries induced by the mirror map or norm, generalizing implicit bias beyond Euclidean GD.",
      "relationship_sentence": "This result sets the geometric targets for generic methods; the NeurIPS 2023 paper builds directly on it by significantly improving the convergence rates to these geometry-dependent max-margin solutions."
    },
    {
      "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
      "authors": "Kaifeng Lyu; Jian Li",
      "year": 2019,
      "role": "Showed margin maximization via decoupled norm-direction dynamics for homogeneous losses, offering tools to control directional convergence.",
      "relationship_sentence": "The separation-of-scales perspective used here to convert loss decay to margin growth in non-Euclidean settings is inspired by this line of analysis."
    },
    {
      "title": "Implicit Bias of Gradient Descent on Linear Convolution",
      "authors": "Suriya Gunasekar; Jason D. Lee; Daniel Soudry; Nati Srebro",
      "year": 2018,
      "role": "Connected optimization geometry and parameterization to which max-margin solution is selected, emphasizing geometry-induced implicit regularization.",
      "relationship_sentence": "Reinforces the paper\u2019s central geometric viewpoint that algorithmic geometry dictates the margin notion, motivating accelerated analyses for mirror/steepest descent."
    }
  ],
  "synthesis_narrative": "The core contribution of \u201cFaster Margin Maximization Rates for Generic Optimization Methods\u201d is to close the rate gap between gradient-descent-based algorithms and generic methods like mirror descent and steepest descent by proving state-of-the-art implicit bias rates in non-Euclidean geometries. This builds directly on the seminal Euclidean story of Soudry et al., who established that gradient descent on separable logistic loss converges directionally to the L2 max-margin solution with a characteristic margin growth rate. Follow-up refinements by Nacson et al. and Ji\u2013Telgarsky quantified last-iterate and directional convergence, and developed techniques to translate loss decay into margin growth\u2014analytical motifs that the NeurIPS 2023 paper adapts and extends to non-Euclidean settings.\n\nThe geometric generalization that mirror and steepest descent converge to maximal-margin solutions defined by their induced geometry was established by Gunasekar and collaborators, which frames the precise targets (norm/Bregman margins) that the present work aims to reach faster. Complementary results by Lyu\u2013Li on decoupling norm and direction dynamics for homogeneous losses inform the separation-of-scales argument needed to control directional convergence beyond Euclidean geometry. Finally, the broader geometry/parameterization perspective exemplified in work on linear convolution underscores that different optimization geometries select different margins, justifying a focused rate analysis for mirror and steepest descent. Together, these works supply the phenomenon, targets, and analytical tools; the present paper synthesizes them to derive substantially faster implicit-bias rates in the generic-method regime.",
  "analysis_timestamp": "2026-01-06T23:42:49.095632"
}
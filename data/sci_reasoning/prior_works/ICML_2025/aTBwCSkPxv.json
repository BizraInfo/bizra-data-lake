{
  "prior_works": [
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Saxe et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "This paper established the gradient-flow viewpoint and core conserved quantities in deep linear networks, providing the template the present work generalizes to convolutional blocks, residual blocks, and attention."
    },
    {
      "title": "Implicit Regularization in Matrix Factorization",
      "authors": "Gunasekar et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "By identifying conserved Gram-difference quantities (e.g., U\u1d40U \u2212 V V\u1d40) under gradient flow in (deep) matrix factorization, it serves as the baseline conservation-law characterization that this work extends to convolutions and modern blocks and reframes at the block/subset-of-parameters level."
    },
    {
      "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models: Beyond Neural Tangent Kernel",
      "authors": "Chizat and Bach",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "This work formalized positive homogeneity and scaling symmetries that underlie conservation laws in ReLU networks but did not address convolution, residual connections, or attention, motivating the present paper\u2019s extension of conservation principles to practical architectures."
    },
    {
      "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks",
      "authors": "Neyshabur et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "By exploiting rescaling symmetries in positively homogeneous ReLU networks, this work highlighted invariants tied to paths/units, directly inspiring the present paper\u2019s systematic characterization of all conservation laws for shallow ReLU blocks and their blockwise generalization."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "authors": "He et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "Introduced residual blocks and skip connections; the current paper\u2019s key finding\u2014that residual blocks share the same conservation laws as their non-residual counterparts\u2014relies on this architectural formulation."
    },
    {
      "title": "Attention Is All You Need",
      "authors": "Vaswani et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Defines the single attention layer architecture whose gradient-flow conservation laws are completely characterized in the present work."
    }
  ],
  "synthesis_narrative": "The core innovation of this paper\u2014systematically deriving and classifying conservation laws for practical deep architectures (convolutional blocks, residual blocks, and attention), and introducing conservation laws that depend only on subsets of parameters\u2014rests on a clear lineage. Saxe et al. provided the foundational gradient-flow framing and explicit invariants for deep linear networks, establishing the analytical paradigm the present work generalizes. Gunasekar et al. then crystallized conservation laws in matrix factorization via preserved Gram-difference quantities under gradient flow; these serve as the baseline invariant structures that this paper extends to convolutional operators and leverages in multi-layer settings. Chizat and Bach formalized the role of positive homogeneity and rescaling symmetries in ReLU models, making explicit the structural reasons invariants arise\u2014yet stopping short of treating modern blocks like convolution, residual connections, or attention; that gap directly motivates this paper\u2019s architectural generality. Neyshabur et al.\u2019s Path-SGD operationalized rescaling invariances in ReLU networks, inspiring the present work\u2019s precise, exhaustive characterization of all conservation laws for shallow ReLU blocks and the notion of blockwise (subset-of-parameters) invariants. Finally, the architectural formulations of residual networks (He et al.) and transformers (Vaswani et al.) provide the concrete blocks the paper analyzes: it shows residual skips do not alter the conservation laws of their underlying blocks and gives a complete description of conservation laws for a single attention layer. Together, these works directly enable and motivate the paper\u2019s unified conservation-law framework for modern deep architectures.",
  "analysis_timestamp": "2026-01-06T23:07:19.580444"
}
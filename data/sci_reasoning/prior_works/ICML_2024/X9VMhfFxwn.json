{
  "prior_works": [
    {
      "title": "From Sparse to Soft Mixture of Experts",
      "authors": "Puigcerver et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "This paper adopts Soft MoE routing modules introduced by Puigcerver et al. and extends them to value-based deep RL networks to enable parameter scaling without the routing pathologies of sparse MoEs."
    },
    {
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "authors": "Shazeer et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Shazeer et al. established the MoE principle of decoupling parameter count from inference compute via gated expert routing\u2014the architectural foundation the authors leverage to add parameters to RL value networks efficiently."
    },
    {
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "authors": "Fedus et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Switch Transformers demonstrated that MoE can unlock massive parameter scaling and performance in supervised settings, directly motivating the authors to explore MoE as a path to parameter scaling in RL while avoiding sparse-routing instabilities."
    },
    {
      "title": "Scaling Vision with Sparse Mixture of Experts",
      "authors": "Riquelme et al.",
      "year": 2021,
      "role": "Related Problem",
      "relationship_sentence": "V-MoE showed that inserting MoE blocks into vision models improves parameter scaling at near-constant compute, informing the authors\u2019 design choice to insert MoE modules within RL perception/backbone components."
    },
    {
      "title": "Human-level control through deep reinforcement learning",
      "authors": "Mnih et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "DQN defined the value-based deep RL paradigm and canonical network setup that the authors modify by replacing dense layers with MoE modules to study parameter scaling."
    },
    {
      "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "authors": "Hessel et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Rainbow serves as the strong value-based baseline whose architecture and performance the authors directly compare against and enhance via MoE integration."
    },
    {
      "title": "A Dissection of Overfitting and Generalization in Deep Reinforcement Learning",
      "authors": "Zhang et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Zhang et al. documented that larger-capacity RL models often overfit and degrade in performance, highlighting the precise limitation this work addresses by proposing MoE as a capacity-scaling remedy."
    }
  ],
  "synthesis_narrative": "The core idea of this paper\u2014using Mixture-of-Experts to unlock parameter scaling in value-based deep RL\u2014rests on a clear lineage. Shazeer et al. introduced the MoE formulation that decouples parameter count from compute via gated experts, establishing the architectural groundwork. Switch Transformers by Fedus et al. then showed MoE\u2019s power to scale parameters to the trillion range with strong performance, motivating the hypothesis that MoE could similarly resolve RL\u2019s poor parameter scaling. However, sparse-routing MoEs can suffer from instability and capacity constraints, which Soft MoE (Puigcerver et al.) explicitly addressed; this work directly adopts those Soft MoE modules and extends them into RL value networks. V-MoE (Riquelme et al.) further reinforced that inserting MoE blocks within perception backbones can deliver scalable gains in non-language domains, guiding the placement and design of MoE within RL architectures. On the RL side, DQN defined the value-based paradigm and network template that the authors modify with MoE layers, while Rainbow provides the strong, widely used baseline against which MoE-augmented agents are built and evaluated. Finally, empirical studies like Zhang et al. exposed that simply increasing capacity often harms RL performance, crystallizing the gap this paper targets: achieving parameter scaling in RL via MoE without sacrificing stability or compute efficiency.",
  "analysis_timestamp": "2026-01-06T23:09:26.427938"
}
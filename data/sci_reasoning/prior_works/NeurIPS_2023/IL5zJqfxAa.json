{
  "prior_works": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "role": "Reasoning methodology (prompting)",
      "relationship_sentence": "EmbodiedGPT generalizes Chain-of-Thought to the embodied domain by turning egocentric tasks into stepwise sub-goal plans, directly borrowing CoT\u2019s decomposition principle for its EgoCOT annotations and planning outputs."
    },
    {
      "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
      "authors": "Kristen Grauman et al.",
      "year": 2022,
      "role": "Data source for egocentric perception",
      "relationship_sentence": "Ego4D provides the large-scale, diverse egocentric videos that EmbodiedGPT curates and annotates into the EgoCOT dataset, enabling plan supervision grounded in real-world first-person contexts."
    },
    {
      "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "authors": "Xiang Lisa Li, Percy Liang",
      "year": 2021,
      "role": "Parameter-efficient adaptation",
      "relationship_sentence": "EmbodiedGPT\u2019s efficient training strategy adapts a 7B LLM to EgoCOT via prefix tuning, directly applying this method to achieve high-quality planning without full model finetuning."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Michael Ahn et al.",
      "year": 2022,
      "role": "LLM-driven hierarchical planning for robotics",
      "relationship_sentence": "SayCan demonstrated using LLMs to propose feasible action sequences grounded by affordances, informing EmbodiedGPT\u2019s design of language-driven sub-goal planning grounded in perceptual context."
    },
    {
      "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "authors": "Anthony Brohan et al.",
      "year": 2023,
      "role": "Vision-language-action modeling",
      "relationship_sentence": "RT-2\u2019s mapping of multimodal inputs and language knowledge to actions inspires EmbodiedGPT\u2019s paradigm of extracting task-related features from LLM-generated plans to support downstream execution."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "role": "Multimodal alignment with mostly-frozen LLMs",
      "relationship_sentence": "BLIP-2\u2019s strategy of leveraging frozen LLMs with lightweight adapters influenced EmbodiedGPT\u2019s architectural choice to keep the LLM largely fixed while aligning visual inputs and planning via efficient adaptation."
    },
    {
      "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Household Tasks",
      "authors": "Mohit Shridhar et al.",
      "year": 2020,
      "role": "Embodied instruction-following and subgoal structure",
      "relationship_sentence": "ALFRED established long-horizon, instruction-driven tasks decomposed into subgoals, motivating EmbodiedGPT\u2019s use of stepwise plan generation and evaluation on instruction-following benchmarks."
    }
  ],
  "synthesis_narrative": "EmbodiedGPT\u2019s core innovation\u2014vision-language pre-training for embodied planning via an embodied Chain-of-Thought\u2014sits at the intersection of CoT reasoning, egocentric video corpora, parameter-efficient LLM tuning, and vision-language-action (VLA) robotics. Chain-of-Thought prompting (Wei et al., 2022) provides the central idea of explicit step decomposition, which EmbodiedGPT instantiates as sub-goal plans in EgoCOT. Constructing EgoCOT hinges on Ego4D (Grauman et al., 2022), whose large, diverse first-person videos allow grounding plans in realistic visual contexts. To scale training without full finetuning, EmbodiedGPT adopts prefix-tuning (Li & Liang, 2021), aligning a 7B LLM to the embodied planning distribution efficiently.\n\nPrior robotics work showed how language models can structure actions. SayCan (Ahn et al., 2022) demonstrated LLM-based high-level planning constrained by affordances, directly informing EmbodiedGPT\u2019s use of LLM-generated plans as actionable scaffolds. RT-2 (Brohan et al., 2023) established the effectiveness of VLA models in mapping web-scale semantics to robot control, paralleling EmbodiedGPT\u2019s extraction of task-relevant features from plans to drive execution. On the multimodal alignment front, BLIP-2 (Li et al., 2023) showed that frozen LLMs can be effectively paired with visual encoders via lightweight adaptation, a blueprint echoed in EmbodiedGPT\u2019s training recipe. Finally, ALFRED (Shridhar et al., 2020) crystallized the importance of subgoal decomposition for long-horizon household tasks, reinforcing EmbodiedGPT\u2019s stepwise planning paradigm and evaluation focus. Together, these works directly shaped EmbodiedGPT\u2019s dataset design, training strategy, and plan-to-action pipeline.",
  "analysis_timestamp": "2026-01-07T00:02:04.865598"
}
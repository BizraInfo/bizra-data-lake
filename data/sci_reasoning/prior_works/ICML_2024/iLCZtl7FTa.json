{
  "prior_works": [
    {
      "title": "AI Safety via Debate",
      "authors": "Geoffrey Irving et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Introduced the two-expert\u2013one-judge debate framework to elicit truthful answers from stronger agents; the present paper directly instantiates this setup with LLM experts and a weaker non\u2011expert judge, and further optimizes debaters for persuasiveness."
    },
    {
      "title": "Self-critiquing models for assisting human evaluators",
      "authors": "William Saunders et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Showed that model-generated critiques can help human evaluators make better judgments; this work extends that idea from one-sided critique to adversarial two\u2011sided debate and measures the resulting gains for both model and human non\u2011expert judges."
    },
    {
      "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "Established practical methodologies for using LLMs as pairwise evaluators; this paper builds on that paradigm by using an LLM judge and probing whether a weaker judge can reliably select the correct answer after observing expert debate."
    },
    {
      "title": "Red Teaming Language Models with Language Models",
      "authors": "Ethan Perez et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated that LMs can adversarially probe and critique other LMs to surface failures; the current paper operationalizes adversarial interaction as a structured two\u2011agent debate whose aim is to make errors legible to a non\u2011expert judge."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Pioneered replacing human supervision with AI feedback to scale oversight; this work targets the next step by asking whether even weaker AI evaluators can oversee stronger models via debate, and it trains debaters to be persuasive to such judges."
    },
    {
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "authors": "Stephanie Lin et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Defined the modern benchmark and framing for truthfulness in QA; the present paper pursues that objective via debate, explicitly measuring whether debates yield more truthful answers than non\u2011debate baselines."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014using expert LLMs to debate and a weaker non\u2011expert (model or human) to judge, plus unsupervised optimization of debaters for persuasiveness\u2014sits squarely in the lineage of the debate paradigm proposed by AI Safety via Debate, which framed two\u2011agent argumentation as a scalable oversight mechanism for eliciting truth from stronger models. Building on that conceptual foundation, recent work showed evaluators can be aided by model\u2011generated reasoning: Self\u2011critiquing models for assisting human evaluators demonstrated that critiques improve human judgment, directly inspiring this paper\u2019s shift from one\u2011sided critiques to adversarial two\u2011sided debates aimed at making key evidence salient to non\u2011experts. Technically, the study leverages the LLM\u2011as\u2011a\u2011judge paradigm established by Judging LLM\u2011as\u2011a\u2011Judge with MT\u2011Bench and Chatbot Arena, but pushes it further by systematically testing weaker judges against stronger experts. In parallel, Constitutional AI established that AI feedback can replace human labels to scale supervision; the present work investigates an even more challenging regime\u2014can weaker evaluators supervise stronger models when equipped with debate? Red Teaming Language Models with Language Models provided additional evidence that adversarial interactions between models expose errors, a dynamic this paper formalizes via structured debate and winner selection. Finally, TruthfulQA crystallized truthfulness as a central objective and evaluation target, which this work pursues, showing that debate\u2014and training debaters to be persuasive\u2014yields more truthful answers than direct, non\u2011debated responses.",
  "analysis_timestamp": "2026-01-06T23:09:26.469385"
}
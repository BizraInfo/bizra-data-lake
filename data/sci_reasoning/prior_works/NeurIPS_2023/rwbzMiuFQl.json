{
  "prior_works": [
    {
      "title": "Learning both weights and connections for efficient neural networks",
      "authors": "Song Han, Jeff Pool, John Tran, William Dally",
      "year": 2015,
      "role": "Methodological precursor (magnitude pruning)",
      "relationship_sentence": "This work introduced magnitude-based pruning to remove parameters while preserving function, providing the core ablation tool that the paper repurposes to isolate and test putative subroutine-specific subnetworks."
    },
    {
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "authors": "Jonathan Frankle, Michael Carbin",
      "year": 2019,
      "role": "Conceptual and methodological foundation (sparse subnetworks)",
      "relationship_sentence": "By showing that sparse subnetworks can carry full task performance, LTH motivates the idea that functional solutions can reside in subnetworks, a premise the paper leverages to argue for structural compositionality via pruned modules."
    },
    {
      "title": "What\u2019s Hidden in a Randomly Weighted Neural Network?",
      "authors": "Vivek Ramanujan, Michael Wortsman, Aniruddh Kembhavi, Ali Farhadi, Mohammad Rastegari",
      "year": 2020,
      "role": "Empirical precedent (mask-based subnetworks)",
      "relationship_sentence": "Demonstrating that binary masks can extract performant subnetworks even from random weights, this work strengthens the notion that networks contain modular solutions that can be surfaced via pruning\u2014the central mechanism used to reveal subroutines here."
    },
    {
      "title": "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning",
      "authors": "Aravind Mallya, Svetlana Lazebnik",
      "year": 2018,
      "role": "Direct antecedent (disjoint subnetworks via pruning)",
      "relationship_sentence": "PackNet shows that iterative pruning can allocate largely non-overlapping subnetworks to separate tasks without interference, a key empirical motif mirrored in this paper\u2019s claim that distinct subroutines can be ablated while preserving others."
    },
    {
      "title": "Piggyback: Adapting a Single Network to Multiple Tasks by Learning Binary Masks",
      "authors": "Aravind Mallya, Dillon Davis, Svetlana Lazebnik",
      "year": 2018,
      "role": "Direct antecedent (task-specific masking)",
      "relationship_sentence": "Piggyback\u2019s binary mask approach operationalizes task-specific subnetworks atop a shared backbone, directly informing the present paper\u2019s use of mask/pruning strategies to expose modular subroutines within a single model."
    },
    {
      "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
      "authors": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba",
      "year": 2017,
      "role": "Interpretability evidence (functional specialization)",
      "relationship_sentence": "By mapping individual CNN units to human-interpretable concepts, Network Dissection provides prior evidence of localized functionality, bolstering the paper\u2019s claim that subnetworks can specialize in subroutines that can be selectively ablated."
    },
    {
      "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
      "authors": "Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov",
      "year": 2019,
      "role": "Empirical precedent (targeted ablation reveals specialization)",
      "relationship_sentence": "This study shows that pruning specific attention heads minimally harms performance while others are critical, a fine-grained ablation methodology that directly parallels the paper\u2019s strategy to identify and test modular subroutines."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central contribution\u2014demonstrating structural compositionality by revealing modular subnetworks that implement subroutines and can be ablated independently\u2014builds on a line of work that uses pruning and masking to expose functional sparsity. Han et al. (2015) established magnitude-based pruning as a reliable tool to remove parameters with minimal loss, providing the core methodology for surgical ablations. The Lottery Ticket Hypothesis (Frankle & Carbin, 2019) crystallized the concept that performant solutions can reside in sparse subnetworks, legitimizing the search for functionally meaningful subgraphs. Ramanujan et al. (2020) further showed that binary masks alone can surface competent subnetworks, reinforcing the idea that modular solutions are embedded in large models and can be isolated without retraining.\nIn parallel, multi-task pruning/masking works\u2014PackNet (Mallya & Lazebnik, 2018) and Piggyback (Mallya et al., 2018)\u2014demonstrated that largely disjoint subnetworks within a single backbone can support different tasks with minimal interference. This directly anticipates the paper\u2019s claim that models can allocate separable resources to subroutines and that ablating one should preserve others. Complementary interpretability research, such as Network Dissection (Bau et al., 2017) and targeted attention-head ablations (Voita et al., 2019), provided converging evidence of localized functional specialization and validated ablation as a diagnostic for modularity. Together, these works supply the conceptual framing, methodological apparatus, and empirical precedents that the paper integrates and extends to argue for structural compositionality across both vision and language models.",
  "analysis_timestamp": "2026-01-07T00:02:04.805158"
}
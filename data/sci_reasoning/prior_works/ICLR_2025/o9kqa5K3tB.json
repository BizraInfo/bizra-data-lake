{
  "prior_works": [
    {
      "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
      "authors": "Zongyi Li et al.",
      "year": 2021,
      "arxiv_id": "2010.08895",
      "role": "Extension",
      "relationship_sentence": "MemNO directly extends FNO by keeping its Fourier-based spatial operator while replacing FNO\u2019s Markovian one-step time propagation with an explicit long-memory temporal module."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "authors": "Albert Gu et al.",
      "year": 2022,
      "arxiv_id": "2111.00396",
      "role": "Extension",
      "relationship_sentence": "MemNO adopts S4\u2019s structured state-space long-convolution kernel to implement history-dependent temporal dynamics, enabling explicit use of past PDE states for prediction."
    },
    {
      "title": "Neural Operator: Learning Maps Between Function Spaces",
      "authors": "A. M. Stuart et al.",
      "year": 2021,
      "arxiv_id": "2108.08481",
      "role": "Foundation",
      "relationship_sentence": "MemNO is formulated within the neural operator framework that learns discretization-invariant mappings between function spaces, augmenting this foundation with explicit temporal memory."
    },
    {
      "title": "Optimal prediction and the Mori\u2013Zwanzig formalism",
      "authors": "A. J. Chorin et al.",
      "year": 2000,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The Mori\u2013Zwanzig theory provides the core insight that reduced dynamics generically include a convolutional memory term, which directly motivates MemNO\u2019s non-Markovian design and its theoretical results."
    },
    {
      "title": "Renormalized Mori\u2013Zwanzig reduction for systems with memory",
      "authors": "Panagiotis Stinis",
      "year": 2015,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "This work shows that neglecting or mis-specifying memory in reduced models leads to instability and bias, highlighting the precise deficiency in Markovian surrogates that MemNO addresses by learning memory kernels."
    }
  ],
  "synthesis_narrative": "Fourier Neural Operator (FNO) demonstrated that spectral convolutions can learn discretization-invariant spatial mappings for parametric PDEs, but its practical time integration commonly proceeds via Markovian one-step updates that only use the current state. Structured State Space Models, particularly S4, introduced a provably efficient parameterization of long convolutional kernels for sequences, enabling faithful and scalable modeling of long-range temporal dependencies. The neural operator framework formalized learning maps between function spaces, providing a principled foundation for PDE surrogates that generalize across meshes and resolutions. Independently, the Mori\u2013Zwanzig formalism established that reduced descriptions of high-dimensional dynamics inherently include a history-dependent memory integral, rather than purely Markovian evolution in the observed variables. Building on this, renormalized Mori\u2013Zwanzig reductions made concrete how omitting or mis-specifying memory leads to instability and bias, clarifying the stakes of accurate memory modeling in coarse or noisy settings.\nThese threads jointly exposed a gap: mainstream neural operators excel spatially yet treat time Markovianly, contrary to theoretical guidance that memory is essential when information is unresolved or noisy. The natural synthesis is to retain FNO\u2019s strong spatial operator while instantiating the Mori\u2013Zwanzig memory term via an S4-based long convolution over past states. This yields a non-Markovian neural operator that explicitly aggregates history, theoretically justified by Mori\u2013Zwanzig and practically enabled by S4\u2019s efficient memory parameterization, improving robustness under low-resolution discretizations and observational noise.",
  "target_paper": {
    "title": "On the Benefits of Memory for Modeling Time-Dependent PDEs",
    "authors": "Ricardo Buitrago, Tanya Marwah, Albert Gu, Andrej Risteski",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "State Space Models, Partial Differential Equations",
    "abstract": "Data-driven techniques  have emerged as a promising alternative to traditional numerical methods for solving PDEs. For time-dependent PDEs, many approaches are Markovian---the evolution of the trained system only depends on the current state, and not the past states. In this work, we investigate the benefits of using memory for modeling time-dependent PDEs: that is, when past states are explicitly used to predict the future. Motivated by the Mori-Zwanzig theory of model reduction, we theoretically exhibit examples of simple (even linear) PDEs, in which a solution that uses memory is arbitrarily better than a Markovian solution. Additionally, we introduce Memory Neural Operator (MemNO), a neural operator architecture that combines recent state space models (specifically, S4) and Fourier Neural Operators (FNOs) to effectively model memory.  We empirically demonstrate that when the PDEs are supplied in low resolution or contain observation noise at train and test time, MemNO significantly",
    "openreview_id": "o9kqa5K3tB",
    "forum_id": "o9kqa5K3tB"
  },
  "analysis_timestamp": "2026-01-06T16:20:57.139983"
}
{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano et al.",
      "year": 2017,
      "role": "Foundational method for learning rewards from pairwise human feedback",
      "relationship_sentence": "Introduced the preference-based reward modeling paradigm (pairwise comparisons + learned reward) that this paper adapts to align medical image generators with pathologists\u2019 judgments of clinical plausibility."
    },
    {
      "title": "Fine-Tuning Language Models from Human Preferences",
      "authors": "Daniel M. Ziegler et al.",
      "year": 2019,
      "role": "Scalable pipeline for preference modeling and policy optimization",
      "relationship_sentence": "Demonstrated the practical pipeline of collecting human preferences, training a reward model, and optimizing a generator against it\u2014mirrored here with experts providing pairwise assessments of clinical features."
    },
    {
      "title": "Training language models to follow instructions with human feedback (InstructGPT)",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "role": "Practical RLHF at scale and validation of human-aligned objectives",
      "relationship_sentence": "Provided a modern template for aligning powerful generative models via human feedback, motivating the authors to adopt a similar alignment loop but with clinical experts and image-specific rewards."
    },
    {
      "title": "Towards Expert-Level Medical Question Answering with Large Language Models (Med-PaLM)",
      "authors": "Karan Singhal et al.",
      "year": 2023,
      "role": "Domain-specific RLHF with clinicians in healthcare",
      "relationship_sentence": "Showed that clinician-in-the-loop feedback meaningfully improves medical-domain alignment, directly inspiring the pathologist-in-the-loop strategy for image generation and evaluation."
    },
    {
      "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium (introducing FID)",
      "authors": "Martin Heusel et al.",
      "year": 2017,
      "role": "Standard domain-agnostic generative quality metric (FID) that the paper critiques",
      "relationship_sentence": "Established FID as a dominant metric whose lack of clinical knowledge motivates replacing such scores with human-derived, clinically grounded rewards."
    },
    {
      "title": "Improved Precision and Recall Metric for Assessing Generative Models",
      "authors": "Tero Kynk\u00e4\u00e4nniemi et al.",
      "year": 2019,
      "role": "Widely used domain-agnostic precision/recall assessment for generators",
      "relationship_sentence": "Represents the class of generic metrics that fail to capture nuanced clinical plausibility, reinforcing the need for expert preference-based evaluation adopted in this work."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014aligning medical image generation with clinical knowledge via pathologist feedback\u2014rests directly on the preference-based alignment paradigm established in reinforcement learning from human feedback. Christiano et al. (2017) introduced learning a reward function from pairwise human comparisons, a blueprint the authors transpose to medical images by eliciting expert judgments of clinical plausibility and training a reward model to reflect those preferences. Ziegler et al. (2019) operationalized this paradigm into a practical pipeline\u2014collect preferences, fit a reward, and optimize the generator against it\u2014providing the methodological scaffolding mirrored here. Ouyang et al. (2022) further validated RLHF as a scalable alignment mechanism for powerful generators, informing the paper\u2019s overall alignment loop and evaluation mindset.\n\nCrucially, Med-PaLM (Singhal et al., 2023) demonstrated that clinician-in-the-loop feedback can materially improve domain-specific alignment in healthcare, directly motivating a pathologist-in-the-loop variant tailored to imaging. On the evaluation side, FID (Heusel et al., 2017) and improved precision/recall (Kynk\u00e4\u00e4nniemi et al., 2019) exemplify domain-agnostic metrics that correlate poorly with clinical sensibility; their limitations are the foil against which the authors propose human preference-derived, clinically grounded rewards. Together, these works converge to a clear route: replace generic visual realism metrics with expert preference models and use them to steer generation, enabling synthetic medical images that are not just realistic but clinically plausible.",
  "analysis_timestamp": "2026-01-07T00:02:04.845128"
}
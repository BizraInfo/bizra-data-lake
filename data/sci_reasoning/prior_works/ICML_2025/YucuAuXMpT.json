{
  "prior_works": [
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli",
      "year": 2014,
      "role": "Analytical framework for deep linear networks",
      "relationship_sentence": "The paper builds directly on Saxe et al.\u2019s SVD-based analysis of deep linear networks to construct formal examples and proofs showing that internal representations can vary independently of the realized input\u2013output function."
    },
    {
      "title": "Representational similarity analysis \u2013 connecting the branches of systems neuroscience",
      "authors": "Nikolaus Kriegeskorte, Marieke Mur, Peter A. Bandettini",
      "year": 2008,
      "role": "Foundational measure of representational similarity (RSA)",
      "relationship_sentence": "By problematizing the inference from RSA to shared function, the paper explicitly interrogates assumptions introduced by Kriegeskorte et al. and uses RSA-style geometry as a foil to demonstrate representational\u2013functional dissociations."
    },
    {
      "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability",
      "authors": "Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",
      "year": 2017,
      "role": "Operational tool for comparing learned representations",
      "relationship_sentence": "The work extends and critiques SVCCA-style comparisons by showing (analytically in deep linear nets) that high representational similarity need not imply similar functions, clarifying what SVCCA-based alignment can and cannot reveal."
    },
    {
      "title": "Similarity of Neural Network Representations Revisited",
      "authors": "Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton",
      "year": 2019,
      "role": "CKA metric for robust representational comparison",
      "relationship_sentence": "Using CKA\u2019s invariances as context, the paper demonstrates that even metrics designed to be robust to invertible transforms may conflate distinct functions with similar geometries, and conversely, identical functions can yield dissimilar CKA due to internal reparameterizations."
    },
    {
      "title": "Implicit Regularization in Matrix Factorization",
      "authors": "Suriya Gunasekar, Jason D. Lee, Daniel Soudry, Nathan Srebro",
      "year": 2017,
      "role": "Implicit bias among infinitely many factorizations",
      "relationship_sentence": "The analysis leverages the insight that gradient-based training selects among non-unique factorizations, using this implicit bias to explain why networks can realize the same function with different representations (and vice versa)."
    },
    {
      "title": "Deep Learning without Poor Local Minima",
      "authors": "Kenji Kawaguchi",
      "year": 2016,
      "role": "Loss landscape and non-identifiability in deep linear nets",
      "relationship_sentence": "By establishing the structure of critical points and equivalence classes in deep linear networks, this work underpins the paper\u2019s claim that many representationally distinct solutions can realize identical functions."
    },
    {
      "title": "Convergent Learning: Do Different Neural Networks Learn the Same Representations?",
      "authors": "Stanislaw Jastrzebski, Jeff Clune, Jason Yosinski, Anh Nguyen, Thomas Fuchs, Hod Lipson (often cited as Li et al.)",
      "year": 2016,
      "role": "Empirical evidence on inter-network representational similarity",
      "relationship_sentence": "The present paper provides a theoretical lens explaining mixed empirical findings on convergent representations, showing conditions where representational similarity can emerge without shared function and where identical functions need not imply similar internal codes."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014an analytical dissociation between functional and representational similarity\u2014rests on two pillars: exact analysis in deep linear networks and the modern practice of comparing internal codes across networks and brains. Saxe et al. (2014) provide the crucial mathematical machinery: an SVD-based description of learning dynamics and the non-uniqueness of linear factorizations. Building on this, Kawaguchi (2016) formalizes the loss landscape structure and identifiability limits in deep linear nets, legitimizing the existence of many internal solutions realizing the same input\u2013output mapping. Gunasekar et al. (2017) then explains how gradient descent implicitly selects among these equivalence classes, clarifying why training can settle on distinct internal representations even when functions coincide.\nParallel developments in representation comparison\u2014Kriegeskorte et al. (2008) with RSA, Raghu et al. (2017) with SVCCA, and Kornblith et al. (2019) with CKA\u2014established practical, geometry-based metrics to align and compare neural codes. The present work analytically shows that these metrics can signal similarity absent functional equivalence and fail to guarantee similarity when functions match, due to invariances and reparameterizations inherent in multilayer factorizations. Finally, empirical reports of convergent or divergent internal representations across independently trained networks (e.g., Li/Yosinski/Clune et al., 2016) are reconciled: the theory delineates when input statistics and training biases drive geometric alignment versus when functional constraints leave representational degrees of freedom unconstrained. Together, these prior works directly enable and motivate the paper\u2019s key insight that representation and function can be fundamentally decoupled, and that generalization or robustness alone need not fix internal codes.",
  "analysis_timestamp": "2026-01-07T00:21:33.201196"
}
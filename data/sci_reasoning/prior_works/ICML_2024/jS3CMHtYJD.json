{
  "prior_works": [
    {
      "title": "A Unified Framework for Approximating and Clustering via Random Sampling",
      "authors": "Dan Feldman et al.",
      "year": 2011,
      "role": "Foundation",
      "relationship_sentence": "Introduces the sensitivity sampling coreset framework that this paper directly builds on, replacing the framework\u2019s pseudo-dimension based uniform convergence step with a new Rademacher-complexity analysis to obtain dimension-free bounds."
    },
    {
      "title": "Rademacher and Gaussian Complexities: Risk Bounds and Structural Results",
      "authors": "Peter L. Bartlett et al.",
      "year": 2002,
      "role": "Foundation",
      "relationship_sentence": "Provides the core Rademacher complexity machinery that the paper adapts to the sensitivity sampling setting, enabling uniform control of empirical-to-population loss and yielding dimension-independent coreset sizes."
    },
    {
      "title": "Coresets for Logistic Regression",
      "authors": "Alexandru Munteanu et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Serves as the primary classification coreset baseline\u2014built on sensitivity sampling but with coreset sizes that scale with dimension\u2014whose limitations this work overcomes by proving dimension-independent bounds and broader loss coverage."
    },
    {
      "title": "Turning Big Data into Tiny Data: Coresets for k-Means, PCA, and Projective Clustering",
      "authors": "Dan Feldman et al.",
      "year": 2013,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates that sensitivity sampling can yield dimension-independent coresets in clustering, directly motivating this paper\u2019s pursuit of analogous dimension-free guarantees for classification via a new analysis."
    },
    {
      "title": "Core Vector Machines: Fast SVM Training Using Coresets",
      "authors": "Ivor W. Tsang et al.",
      "year": 2005,
      "role": "Related Problem",
      "relationship_sentence": "An early coreset-based approach to classification (SVM) that established the viability of subset selection for margin-based losses, informing the problem context that this paper generalizes with sensitivity sampling and learning-theoretic guarantees."
    },
    {
      "title": "Coresets for Scalable Bayesian Logistic Regression",
      "authors": "Jonathan H. Huggins et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "Shows practical coreset constructions for logistic classification but lacks dimension-free, distributional sample-complexity guarantees, a gap this paper fills by providing iid-sample-based, dimension-independent bounds across losses."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014dimension-independent sampling coresets for classification with distributional guarantees\u2014emerges by fusing the sensitivity sampling paradigm with modern learning-theoretic tools. Feldman and Langberg\u2019s unified sensitivity framework provides the structural backbone for importance sampling and coreset construction, but its reliance on pseudo-dimension incurs dimension-dependent bounds for rich classification loss classes. Bartlett and Mendelson\u2019s Rademacher complexity theory supplies the statistical engine that this paper integrates into the sensitivity pipeline, replacing pseudo-dimension-based uniform convergence with sharper, data-dependent capacity control. Prior classification coresets such as Munteanu et al. for logistic regression offered strong baselines but had coreset sizes scaling with dimension; this work resolves that limitation while expanding coverage to a broader family of classification losses and to distributional inputs with iid sampling guarantees. Inspiration that dimension-free coresets are possible comes from clustering, where Feldman, Schmidt, and Sohler achieved dimension-independent results via sensitivity sampling\u2014this paper generalizes that blueprint to the classification setting by developing a Rademacher-based analysis. Earlier coreset ideas for classification, notably Core Vector Machines for SVMs and Bayesian logistic coresets by Huggins et al., validated the effectiveness of subset-based training but lacked the general, provable, dimension-independent sample complexity that this work establishes. Together, these threads directly shape the paper\u2019s main contribution: a sensitivity sampling theory controlled by Rademacher complexity that yields no-dimensional coresets for classification with broad applicability.",
  "analysis_timestamp": "2026-01-06T23:09:26.445475"
}
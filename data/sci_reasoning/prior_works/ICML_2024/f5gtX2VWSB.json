{
  "prior_works": [
    {
      "title": "Progressive Neural Networks",
      "authors": "Andrei A. Rusu et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "Introduced grow-as-you-learn columns with lateral connections to reuse prior knowledge while preventing forgetting; the present work adopts the continual add-a-module paradigm but replaces lateral feature reuse with state-dependent composition of prior policies inside each new module to achieve transfer with tighter parameter growth and preserved plasticity."
    },
    {
      "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks",
      "authors": "Chrisantha Fernando et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "Showed modular routing through a large network can avoid interference but relies on evolutionary search and frozen pathways, limiting plasticity; the new approach answers this gap by learning differentiable, state-dependent selection and combination of prior policies while adding only a compact module per task."
    },
    {
      "title": "A Probabilistic Policy Reuse Approach for Reinforcement Learning",
      "authors": "Fernando Fern\u00e1ndez and Manuela Veloso",
      "year": 2006,
      "role": "Foundation",
      "relationship_sentence": "Established the idea of accelerating new-task learning by selecting and mixing from a library of previously learned policies; the proposed self-composing modules operationalize this principle by learning a differentiable, within-policy mechanism to selectively combine earlier policies with a new internal policy."
    },
    {
      "title": "Successor Features for Transfer in Reinforcement Learning",
      "authors": "Andr\u00e9 Barreto et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Provided the generalized policy improvement insight that combining knowledge from multiple policies can yield strictly better behavior; the new architecture embodies this idea at the policy level by enabling state-contingent composition of prior policies to improve performance on new tasks without computing successor features."
    },
    {
      "title": "The Option-Critic Architecture",
      "authors": "Pierre-Luc Bacon et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Framed policies as compositions over temporally extended sub-policies (options); the present method can be viewed as learning a policy over a growing set of previously learned task-policies (as options), enabling hierarchical reuse while freezing past experts to avoid forgetting."
    },
    {
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": "James Kirkpatrick et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "EWC exemplified weight-regularization approaches that mitigate forgetting at the cost of plasticity; the proposed growable, modular architecture is motivated as an alternative that avoids interference without constraining shared weights, thereby maintaining plasticity while scaling."
    },
    {
      "title": "Policy Distillation",
      "authors": "Andrei A. Rusu et al.",
      "year": 2015,
      "role": "Related Problem",
      "relationship_sentence": "Showed how multiple expert policies can be merged into a single network via distillation, but consolidation introduces interference; the new work instead performs online composition of frozen prior policies within each new module, avoiding distillation-induced forgetting while enabling transfer."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014a growable, modular policy architecture that selectively composes previous policies with an internal policy to accelerate new-task learning while naturally avoiding catastrophic forgetting\u2014emerges from two converging lines of work. Progressive Neural Networks and PathNet established that adding modular capacity across tasks can prevent interference and enable reuse, but they incurred substantial parameter overhead or required non-differentiable routing and froze reused components, curbing plasticity. In parallel, the policy reuse literature (Fern\u00e1ndez & Veloso) and the generalized policy improvement perspective via successor features (Barreto et al.) crystallized the insight that leveraging a set of prior policies\u2014by selecting or combining them\u2014can strictly improve learning and performance on new tasks. Option-Critic provided the hierarchical lens for composing behavior through policies over sub-policies, suggesting that prior task policies can themselves serve as reusable options. Finally, stability\u2013plasticity trade-offs highlighted by EWC and consolidation-based transfer like Policy Distillation underscored the limitations of shared-weight or compression strategies that risk interference. Synthesizing these strands, the present work contributes a per-task module that learns a differentiable, state-dependent selector to compose frozen prior policies with a newly learned internal policy. This achieves scalable linear growth, preserves past competencies, and retains plasticity\u2014realizing the promise of policy reuse and composition within a principled, continual RL architecture.",
  "analysis_timestamp": "2026-01-06T23:09:26.470765"
}
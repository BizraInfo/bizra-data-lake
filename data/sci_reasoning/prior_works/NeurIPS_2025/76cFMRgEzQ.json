{
  "prior_works": [
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
      "authors": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela",
      "year": 2020,
      "role": "Foundational problem setting",
      "relationship_sentence": "This work established the RAG paradigm that mixes retrieved (contextual) with parametric knowledge, motivating the paper\u2019s focus on how LLMs reconcile these sources internally."
    },
    {
      "title": "Quantifying Attention Flow in Transformers",
      "authors": "Samira Abnar, Willem Zuidema",
      "year": 2020,
      "role": "Conceptual and methodological inspiration",
      "relationship_sentence": "The paper\u2019s entity flow formulation generalizes the idea of tracing information paths across layers and heads introduced by attention flow to mixed-source knowledge propagation."
    },
    {
      "title": "Understanding Intermediate Layers Using Linear Classifier Probes",
      "authors": "Guillaume Alain, Yoshua Bengio",
      "year": 2017,
      "role": "Foundational probing technique",
      "relationship_sentence": "The entity-aware probe directly builds on linear classifier probing, addressing its limitation of fixed targets by enabling dynamically specified entity-conditioned probes."
    },
    {
      "title": "Designing and Interpreting Probes with Control Tasks",
      "authors": "John Hewitt, Percy Liang",
      "year": 2019,
      "role": "Methodological guidance for probe validity",
      "relationship_sentence": "Concerns about probe expressivity and interpretability informed the paper\u2019s choice of a minimal-capacity, rank-8 LoRA update and special markers to avoid probe \u201ccheating.\u201d"
    },
    {
      "title": "Matching the Blanks: Distributional Similarity for Relation Learning",
      "authors": "Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, Tom Kwiatkowski",
      "year": 2019,
      "role": "Enabling technique (entity markers)",
      "relationship_sentence": "The use of special entity marker tokens to localize and condition model computations directly inspires marking probing targets for entity-aware tracing."
    },
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen",
      "year": 2022,
      "role": "Enabling parameter-efficient adaptation",
      "relationship_sentence": "The proposed probe employs a small rank-8 LoRA update to process entity markers, enabling adaptive probing without full fine-tuning of the base LLM."
    },
    {
      "title": "ROME: Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",
      "year": 2022,
      "role": "Attribution and localization framework",
      "relationship_sentence": "Methods for localizing and intervening on factual knowledge circuits inform the paper\u2019s attribution experiments that validate where and how parametric vs. retrieved knowledge flows."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014modeling mixed-source knowledge propagation as entity flow and introducing an entity-aware probe\u2014sits at the intersection of RAG, probing, and mechanistic interpretability. Lewis et al. (2020) defined the RAG setting that necessitates understanding how parametric and retrieved knowledge interact in LLMs. Abnar and Zuidema (2020) provided the conceptual template for tracing internal computation via flow, which this work generalizes from attention to entity-conditioned information flow across the full forward pass. The entity-aware probe builds directly on linear probing (Alain and Bengio, 2017), but addresses its static-target limitation by conditioning on dynamically specified entities. To ensure the probe\u2019s measurements reflect model internals rather than probe capacity, design principles from Hewitt and Liang (2019) guide the use of minimal additional parameters and controls. The mechanism for specifying targets\u2014special entity markers\u2014draws from successful practice in relation extraction (Soares et al., 2019), enabling precise localization of entity-relevant activations. Implementationally, LoRA (Hu et al., 2022) supplies a lightweight adaptation pathway: a rank-8 update that processes markers without altering the base model broadly. Finally, attribution and localization ideas from ROME (Meng et al., 2022) inspire the validation experiments that check whether detected entity flows correspond to the loci of parametric versus retrieved knowledge. Together, these strands yield a principled, practical framework to trace, attribute, and reconcile knowledge sources within LLMs.",
  "analysis_timestamp": "2026-01-07T00:02:04.968498"
}
{
  "prior_works": [
    {
      "title": "Deep Variational Information Bottleneck",
      "authors": "Alemi et al.",
      "year": 2017,
      "arxiv_id": "1612.00410",
      "role": "Extension",
      "relationship_sentence": "PIBD directly extends the VIB objective by implementing a prototype-driven stochastic bottleneck to compress intra-modal content while preserving survival-relevant information, replacing instance-wise encoders with prototype-level compression."
    },
    {
      "title": "Prototypical Networks for Few-shot Learning",
      "authors": "Snell et al.",
      "year": 2017,
      "arxiv_id": "1703.05175",
      "role": "Inspiration",
      "relationship_sentence": "The notion of class prototypes motivates PIBD\u2019s prototype dictionary and assignment mechanism, which aggregates numerous WSI patches or genomic pathways into a small set of discriminative prototypes to mitigate intra-modal redundancy."
    },
    {
      "title": "Learning Factorized Multimodal Representations",
      "authors": "Tsai et al.",
      "year": 2019,
      "arxiv_id": "1806.06176",
      "role": "Extension",
      "relationship_sentence": "PIBD adopts and adapts the shared\u2013private factorization idea to explicitly disentangle common versus modality-specific components across pathology and genomics, adding prototype-level disentangling and information constraints to curb inter-modal redundancy."
    },
    {
      "title": "Pathomic Fusion: An Integrated Framework for Fusing Histopathology and Genomic Features for Cancer Prognosis",
      "authors": "Chen et al.",
      "year": 2020,
      "arxiv_id": "1912.08904",
      "role": "Baseline",
      "relationship_sentence": "As a primary multimodal survival baseline combining WSIs and omics, Pathomic Fusion\u2019s lack of redundancy suppression motivates PIBD\u2019s replacement of late fusion with prototypical bottlenecking plus disentangling to preserve modality-specific prognostic signals."
    },
    {
      "title": "CLAM: Data Efficient and Weakly Supervised Clustering and Attention Multiple Instance Learning for Whole Slide Image Classification",
      "authors": "Lu et al.",
      "year": 2021,
      "arxiv_id": "2004.09666",
      "role": "Gap Identification",
      "relationship_sentence": "By highlighting that attention-based MIL still over-selects redundant or task-irrelevant patches in gigapixel WSIs, CLAM exposes the intra-modal redundancy problem that PIBD addresses via prototype-based information bottlenecking instead of instance attention alone."
    },
    {
      "title": "DeepSurv: Personalized Treatment Recommender System Using A Cox Proportional Hazards Deep Neural Network",
      "authors": "Katzman et al.",
      "year": 2018,
      "arxiv_id": "1606.00931",
      "role": "Foundation",
      "relationship_sentence": "PIBD trains with the Cox partial likelihood from DeepSurv to model time-to-event risk, grounding its learning objective in the established survival analysis formulation."
    }
  ],
  "synthesis_narrative": "The variational information bottleneck formalism establishes a principled way to compress inputs while retaining task-relevant content, but is typically instantiated at the instance level (Alemi et al., 2017). Prototypical networks introduced prototype-based summarization, where embeddings are aggregated into representative prototypes that capture discriminative structure (Snell et al., 2017). In multimodal learning, factorized representations separating shared from private components were shown to reduce cross-modal interference via explicit disentanglement with orthogonality and reconstruction constraints (Tsai et al., 2019). For multimodal cancer prognosis specifically, Pathomic Fusion demonstrated the efficacy of combining histopathology and genomics for survival prediction through late/tensor-based fusion, yet lacked mechanisms to eliminate redundant shared content that can swamp modality-specific signals (Chen et al., 2020). On the pathology side, CLAM revealed that even attention-based MIL still over-selects redundant or task-irrelevant patches in gigapixel WSIs, motivating stronger mechanisms to compact instance sets into discriminative summaries (Lu et al., 2021). Training across these settings is commonly anchored in the Cox proportional hazards objective, as established in DeepSurv for deep survival modeling (Katzman et al., 2018). Together, these insights highlight a gap: multimodal survival systems needed a way to simultaneously remove intra-modal redundancy and disentangle inter-modal shared versus specific signals, without sacrificing prognostic information. PIBD naturally synthesizes these threads by replacing instance-level encoders with prototype-driven information bottlenecks to compact each modality and by extending shared\u2013private factorization with prototype-level disentangling under an IB objective, all optimized end-to-end with a Cox loss\u2014thereby preserving discriminative, modality-specific survival cues while suppressing redundant noise within and across modalities.",
  "target_paper": {
    "title": "Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction",
    "authors": "Yilan Zhang, Yingxue Xu, Jianqi Chen, Fengying Xie, Hao Chen",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "multimodal survival prediction, computational pathology",
    "abstract": "Multimodal learning significantly benefits cancer survival prediction, especially the integration of pathological images and genomic data. Despite advantages of multimodal learning for cancer survival prediction, massive redundancy in multimodal data prevents it from extracting discriminative and compact information: (1) An extensive amount of intra-modal task-unrelated information blurs discriminability, especially for gigapixel whole slide images (WSIs) with many patches in pathology and thousands of pathways in genomic data, leading to an \"intra-modal redundancy\" issue. (2) Duplicated information among modalities dominates the representation of multimodal data, which makes modality-specific information prone to being ignored, resulting in an \"inter-modal redundancy\" issue. To address these, we propose a new framework, Prototypical Information Bottlenecking and Disentangling (PIBD), consisting of Prototypical Information Bottleneck (PIB) module for intra-modal redundancy and Prototyp",
    "openreview_id": "otHZ8JAIgh",
    "forum_id": "otHZ8JAIgh"
  },
  "analysis_timestamp": "2026-01-06T18:10:41.972287"
}
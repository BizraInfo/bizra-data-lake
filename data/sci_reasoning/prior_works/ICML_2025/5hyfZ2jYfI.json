{
  "prior_works": [
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Michael Ahn, Anthony Brohan, Noah Brown, et al.",
      "year": 2022,
      "role": "LLM+RL integration blueprint",
      "relationship_sentence": "SayCan demonstrated how to couple LLM-derived plans with value-based affordances, directly informing TEDUO\u2019s dual use of LLMs for high-level language guidance while relying on RL components to ground decisions in environment dynamics."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang, Yao Fu, Hao Peng, et al.",
      "year": 2022,
      "role": "LLM-driven data augmentation",
      "relationship_sentence": "Self-Instruct provided the methodology for using LLMs to automatically synthesize and enrich instruction data, which TEDUO adapts to annotate unlabeled offline trajectories with richer language goals and semantics."
    },
    {
      "title": "Training language models to follow instructions with human feedback (InstructGPT)",
      "authors": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, et al.",
      "year": 2022,
      "role": "Instruction-following LLM capability",
      "relationship_sentence": "InstructGPT established LLMs as robust instruction followers, underpinning TEDUO\u2019s second pillar that treats the LLM as a generalizable instruction-execution agent to bridge from language goals to action policies."
    },
    {
      "title": "Offline Reinforcement Learning with Implicit Q-Learning (IQL)",
      "authors": "Ilya Kostrikov, Ashvin Nair, Sergey Levine",
      "year": 2021,
      "role": "Offline RL algorithmic backbone",
      "relationship_sentence": "IQL introduced a strong, stable offline RL method without behavior cloning constraints, guiding TEDUO\u2019s choice to learn policies from augmented offline data while mitigating distributional shift."
    },
    {
      "title": "Conservative Q-Learning for Offline Reinforcement Learning (CQL)",
      "authors": "Aviral Kumar, Aurick Zhou, George Tucker, Sergey Levine",
      "year": 2020,
      "role": "OOD-risk mitigation in offline RL",
      "relationship_sentence": "CQL\u2019s conservative objectives to avoid overestimation on out-of-distribution actions influenced TEDUO\u2019s design for safe generalization from low-fidelity datasets to unseen states and goals."
    },
    {
      "title": "BabyAI: First Steps Towards Grounded Language Learning with a Human in the Loop",
      "authors": "Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, et al.",
      "year": 2019,
      "role": "Language-conditioned RL benchmark and generalization challenge",
      "relationship_sentence": "BabyAI highlighted the difficulty of systematic generalization in symbolic, language-conditioned tasks, motivating TEDUO\u2019s focus on improving generalization to novel goals and states from offline data."
    },
    {
      "title": "TextWorld: A Learning Environment for Text-based Games",
      "authors": "Marc-Alexandre C\u00f4t\u00e9, \u00c1kos K\u00e1d\u00e1r, Xingdi Yuan, Ben Kybartas, et al.",
      "year": 2018,
      "role": "Symbolic language environment",
      "relationship_sentence": "TextWorld provided the canonical symbolic, language-driven setting where trajectories can be annotated and policies evaluated, aligning with TEDUO\u2019s choice of symbolic environments for offline language-conditioned learning."
    }
  ],
  "synthesis_narrative": "TEDUO\u2019s core contribution\u2014offline learning of generalizable, language-conditioned policies from low-fidelity unlabeled data using LLMs in a dual role\u2014stands on three intertwined lines of prior work. First, the synergy between language models and reinforcement learning was crystallized by SayCan, which coupled LLM planning with value-grounded affordances; TEDUO extends this blueprint to the offline regime, using the LLM both to guide and to structure data. Second, the feasibility of turning unlabeled corpora into instruction-rich supervision was enabled by Self-Instruct and the instruction-following capabilities established by InstructGPT. TEDUO operationalizes these insights by having an LLM automatically annotate offline trajectories with semantically rich goals and decompositions, then relying on the LLM\u2019s robust instruction-following behavior to generalize beyond seen goals. Third, stable offline RL foundations\u2014particularly IQL\u2019s practical learning dynamics and CQL\u2019s conservatism against out-of-distribution actions\u2014directly address distributional shift when learning from augmented, low-fidelity datasets, forming the algorithmic backbone of TEDUO\u2019s policy optimization. Finally, BabyAI and TextWorld define the symbolic, language-grounded environments and expose systematic generalization challenges that TEDUO targets. Together, these works shape TEDUO\u2019s pipeline: LLM-driven data enrichment to convert raw trajectories into language-conditioned supervision; offline RL training that respects dataset support while enabling generalization; and LLM-based instruction following to execute novel goals in symbolic domains.",
  "analysis_timestamp": "2026-01-07T00:21:32.385735"
}
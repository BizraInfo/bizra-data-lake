{
  "prior_works": [
    {
      "title": "Towards Making Systems Forget with Machine Unlearning",
      "authors": "Cao and Yang et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "This work formalized the machine unlearning problem (forget vs. retain partitions and deletion requests) that DSDA adopts as its core problem setting."
    },
    {
      "title": "Machine Unlearning",
      "authors": "Bourtoule et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "SISA training is a primary baseline DSDA improves upon by removing the need for source data and reducing retraining cost while pursuing comparable unlearning guarantees."
    },
    {
      "title": "Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks",
      "authors": "Golatkar et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "This Fisher-based selective forgetting method highlights the limitation of requiring data (and high compute) for deep models, which DSDA explicitly addresses with a source-free and efficient alternative."
    },
    {
      "title": "Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion",
      "authors": "Yin et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "DeepInversion showed that synthetic data can be generated directly from a model for data-free training; DSDA builds on this idea but replaces inversion with energy-guided Langevin sampling tailored for unlearning."
    },
    {
      "title": "Energy-based Out-of-Distribution Detection",
      "authors": "Liu et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "DSDA repurposes the log-sum-exp energy from this work as a guidance signal to steer Langevin sampling toward in-distribution (retain) regions and away from forget regions during AEGDS."
    },
    {
      "title": "Improved Techniques for Training Score-Based Generative Models",
      "authors": "Song and Ermon et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "The annealed Langevin dynamics and score-based sampling principles here underpin DSDA\u2019s Accelerated Energy-Guided Data Synthesis for modeling the training data distribution without access to source data."
    },
    {
      "title": "Multi-Task Learning as Multi-Objective Optimization",
      "authors": "Sener et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "DSDA\u2019s discrimination-aware multitask optimization draws directly on multi-objective gradient balancing to mitigate gradient conflicts between forget and retain objectives."
    }
  ],
  "synthesis_narrative": "The DSDA framework inherits its problem formulation from the earliest unlearning definitions, most notably Cao and Yang\u2019s formalization of deleting the influence of a designated forget set while preserving performance on retained data. Practical baselines like Bourtoule et al.\u2019s SISA established retrain-centric strategies but left a gap in settings where source data are unavailable or retraining is prohibitively expensive. Fisher-based forgetting (Golatkar et al.) further sharpened the need for scalable deep unlearning while still relying on data access and heavy computation, motivating a source-free, efficient route. On the data generation side, DeepInversion demonstrated that models themselves can yield synthetic data for data-free training, a key inspiration that DSDA advances by replacing inversion penalties with principled, learning-theoretic guidance. Specifically, DSDA\u2019s Accelerated Energy-Guided Data Synthesis leverages the energy perspective from Liu et al. (log-sum-exp energy) to bias samples toward in-distribution retain regions and away from forget signals, while adopting the annealed Langevin dynamics and score-based sampling advances of Song and Ermon to realize efficient, stable sampling without source data. Finally, the heart of DSDA\u2019s unlearning procedure is its discrimination-aware multitask optimization, which is informed by the multi-objective perspective of Sener and Koltun, enabling gradient balancing that reduces interference between forgetting and retention objectives. Together, these works directly enable DSDA\u2019s core innovation: source-free, efficient unlearning via energy-guided synthesis coupled with principled multi-objective optimization.",
  "analysis_timestamp": "2026-01-06T23:07:19.591249"
}
{
  "prior_works": [
    {
      "title": "Attention-based Deep Multiple Instance Learning",
      "authors": "Maximilian Ilse, Jakub M. Tomczak, Max Welling",
      "year": 2018,
      "role": "Foundational MIL architecture/pooling",
      "relationship_sentence": "Introduced attention-based pooling that became the de facto MIL aggregator in computational pathology, providing a transferable bag-level representation the present study pretrains and fine-tunes across tasks."
    },
    {
      "title": "Clinical-grade computational pathology using weakly supervised deep learning on whole slide images",
      "authors": "Giuseppe Campanella, Matthew G. Hanna, et al.",
      "year": 2019,
      "role": "Established large-scale weakly supervised WSI MIL",
      "relationship_sentence": "Demonstrated that slide-level weak supervision with MIL can achieve clinical-grade performance at scale, motivating the need to study methods (like transfer learning) that improve robustness in real, data-scarce clinical settings."
    },
    {
      "title": "Data-efficient and weakly supervised computational pathology on whole-slide images (CLAM)",
      "authors": "Ming Y. Lu, Richard J. Chen, et al.",
      "year": 2021,
      "role": "Data-efficient attention MIL with instance clustering",
      "relationship_sentence": "Proposed CLAM, a widely adopted MIL variant tailored for small, weakly supervised cohorts, directly informing the set of MIL models and training paradigms whose transferability this paper systematically evaluates."
    },
    {
      "title": "TransMIL: Transformer-based Correlated Multiple Instance Learning for Whole Slide Image Classification",
      "authors": "Zhihao Shao et al.",
      "year": 2021,
      "role": "Transformer-based MIL capturing inter-instance relations",
      "relationship_sentence": "Provided a modern transformer MIL architecture with relational modeling, enabling the present work to test whether more expressive MIL aggregators pretrain and transfer better across diverse pathology tasks."
    },
    {
      "title": "HIPT: Hierarchical Image Pyramid Transformer for Whole Slide Image Classification",
      "authors": "Richard J. Chen, Ming Y. Lu, et al.",
      "year": 2022,
      "role": "Hierarchical transformer MIL and multi-scale pretraining for WSIs",
      "relationship_sentence": "Showed that hierarchical transformer-based MIL and pretraining can leverage multi-scale WSI structure, motivating analysis of how pretrained MIL aggregators transfer across tasks and domains."
    },
    {
      "title": "WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks",
      "authors": "Thibaut Durand, Taylor Mordan, Nicolas Thome, Matthieu Cord",
      "year": 2016,
      "role": "Top-k and negative-evidence MIL pooling",
      "relationship_sentence": "Introduced a robust pooling strategy for MIL that influenced subsequent pathology MIL baselines, several of which are among the architectures whose pretrained transfer behavior is examined here."
    },
    {
      "title": "Taskonomy: Disentangling Task Transfer Learning",
      "authors": "Amir R. Zamir, Alexander Sax, et al.",
      "year": 2018,
      "role": "Systematic methodology for measuring transfer across tasks",
      "relationship_sentence": "Pioneered a principled, large-scale evaluation of transferability between tasks, directly inspiring this paper\u2019s comprehensive, cross-task study of MIL model pretraining and fine-tuning."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014a first systematic study of transfer learning for multiple instance learning (MIL) in computational pathology\u2014builds on two converging lines of work: modern MIL architectures for whole-slide images (WSIs) and principled analyses of transferability. Foundational pooling mechanisms such as WELDON and especially attention-based MIL (Ilse et al.) established how to aggregate instance embeddings into slide-level predictions, seeding a family of models that are now standard in pathology. Clinical-scale validation by Campanella et al. showed that weakly supervised MIL can achieve high performance on real-world WSIs but also highlighted practical data scarcity and distribution shift\u2014conditions where transfer learning could be decisive. Subsequent pathology-centric MIL advances\u2014CLAM\u2019s data-efficient attention with instance clustering, TransMIL\u2019s transformer-based relational modeling, and HIPT\u2019s hierarchical transformers\u2014expanded the representational capacity and diversity of MIL aggregators, creating a rich model zoo suitable for pretraining and transfer. Methodologically, Taskonomy provided a template for rigorous, task-to-task transfer evaluation. This paper synthesizes these strands by pretraining diverse, state-of-the-art MIL aggregators on multiple WSI tasks and benchmarking their fine-tuning across heterogeneous targets, quantifying when and how MIL models transfer. In doing so, it fills a gap left by patch-level foundation models, showing that end-to-end MIL aggregators themselves benefit substantially from pretraining\u2014even under domain mismatch\u2014and offering evidence-based guidance on model and pretraining-task selection in data-scarce clinical settings.",
  "analysis_timestamp": "2026-01-07T00:04:09.161624"
}
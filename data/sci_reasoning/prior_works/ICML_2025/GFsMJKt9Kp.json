{
  "prior_works": [
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh, Percy Liang",
      "year": 2017,
      "role": "Foundational method for per-example influence estimation",
      "relationship_sentence": "Self-Inf-N builds on the core idea of influence functions\u2014estimating how individual training points affect model behavior\u2014to score and extract benign outlier samples most likely to degrade safety when used for fine-tuning."
    },
    {
      "title": "Estimating Training Data Influence by Tracing Gradient Descent (TracIn)",
      "authors": "Garima Pruthi, Frederick Liu, Satyen Kale, Mukund Sundararajan",
      "year": 2020,
      "role": "Scalable influence approximation technique",
      "relationship_sentence": "The paper\u2019s efficient, model-agnostic influence estimation approach is informed by TracIn\u2019s gradient-path approximations, enabling practical selection of high-impact outliers for large LLMs."
    },
    {
      "title": "Data Shapley: Equitable Valuation of Data for Machine Learning",
      "authors": "Amirata Ghorbani, James Zou",
      "year": 2019,
      "role": "Framework for per-example data valuation",
      "relationship_sentence": "The idea of ranking training samples by their marginal contribution directly motivates selecting a small subset of high-valuation (outlier) benign examples to steer model behavior during fine-tuning."
    },
    {
      "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP",
      "authors": "Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh",
      "year": 2019,
      "role": "Demonstration that small, targeted perturbations can reliably elicit unsafe behavior",
      "relationship_sentence": "This work\u2019s insight\u2014that compact, adversarially chosen artifacts can reliably induce harmful outputs\u2014parallels the paper\u2019s finding that a tiny, strategically selected set of benign outliers can severely compromise safety."
    },
    {
      "title": "Witches\u2019 Brew: Industrial Scale Data Poisoning via Gradient Matching",
      "authors": "Patrick Geiping, Liam H. Fowl, Gavin Taylor, Tom Goldstein",
      "year": 2021,
      "role": "Evidence that a small, carefully selected training subset can strongly steer model behavior",
      "relationship_sentence": "The paper extends the poisoning insight\u2014control via few influential points\u2014to a benign-only setting by selecting naturally occurring outliers instead of crafting poisons."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou, Zico Kolter, Matt Fredrikson",
      "year": 2023,
      "role": "Establishes transferability of jailbreaks across models",
      "relationship_sentence": "Findings on cross-model transferability of adversarial behaviors motivate and contextualize the paper\u2019s result that outlier-driven benign fine-tuning breaks safety across diverse LLM architectures."
    },
    {
      "title": "LOF: Identifying Density-Based Local Outliers",
      "authors": "Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, J\u00f6rg Sander",
      "year": 2000,
      "role": "Foundational outlier detection perspective",
      "relationship_sentence": "By framing the problem as outlier detection within benign data, the paper conceptually draws on classic local-density outlier principles, while operationalizing selection with influence-based scoring."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014Self-Inf-N for extracting outlier benign samples that most degrade safety when used for fine-tuning\u2014sits at the intersection of influence-based data valuation, outlier detection, and adversarial control via small data subsets. Foundationally, influence functions (Koh & Liang, 2017) provide the lens to quantify per-example impact on model behavior, while TracIn (Pruthi et al., 2020) demonstrates practical gradient-based influence estimation at scale. Data Shapley (Ghorbani & Zou, 2019) further legitimizes the strategy of ranking and sub-selecting training points by their marginal contribution, directly informing the idea of targeting the most consequential benign samples.\n\nThe decision to focus on a tiny, strategically chosen subset is grounded in the poisoning literature: Witches\u2019 Brew (Geiping et al., 2021) shows a small, curated set can meaningfully steer downstream behavior, a principle this paper adapts without crafting poisons\u2014by mining naturally occurring outliers. From an attack framing, Universal Adversarial Triggers (Wallace et al., 2019) underscores that compact, targeted artifacts can reliably induce harmful outputs, paralleling the efficacy of a small outlier set in fine-tuning. Classic outlier detection (LOF; Breunig et al., 2000) motivates casting the selection problem through an outlier lens, even as the operational solution uses influence-based scoring rather than density heuristics.\n\nFinally, the paper\u2019s demonstration of cross-model transfer echoes the transferability insights from GCG jailbreaks (Zou et al., 2023), situating Self-Inf-N\u2019s impact beyond a single architecture. Together, these works directly scaffold the authors\u2019 method and its empirical claims: influence-driven outlier selection, minimal data leverage, and transferable safety degradation.",
  "analysis_timestamp": "2026-01-07T00:04:09.139117"
}
{
  "prior_works": [
    {
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "authors": "Noam Shazeer et al.",
      "year": 2017,
      "arxiv_id": "1701.06538",
      "role": "Foundation",
      "relationship_sentence": "Introduces the sparsely-gated MoE with a learned router and load-balancing auxiliary losses\u2014the exact architectural mechanism whose expert specialization and routing behavior this work theoretically analyzes under continual learning."
    },
    {
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "authors": "William Fedus et al.",
      "year": 2021,
      "arxiv_id": "2101.03961",
      "role": "Extension",
      "relationship_sentence": "Formalizes top\u20111 routing with an explicit load-balancing objective, directly motivating the analysis here of how a router both picks the correct expert per task and balances traffic across experts in a continual setting."
    },
    {
      "title": "Expert Gate: Lifelong Learning with a Network of Experts",
      "authors": "Rahaf Aljundi et al.",
      "year": 2017,
      "arxiv_id": "1611.06194",
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates that gating inputs among task-specialized experts mitigates catastrophic forgetting in continual learning, providing the empirical mechanism that this work models and proves beneficial via expert diversification and routing."
    },
    {
      "title": "Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning",
      "authors": "Clemens Rosenbaum et al.",
      "year": 2018,
      "arxiv_id": "1711.01239",
      "role": "Related Problem",
      "relationship_sentence": "Proposes a controller that adaptively selects experts per input/task, directly informing the router abstraction and specialization notion that are analyzed theoretically in the continual, overparameterized linear setting."
    },
    {
      "title": "Surprises in High-Dimensional Ridgeless Least Squares: Double Descent and More",
      "authors": "Trevor Hastie et al.",
      "year": 2019,
      "arxiv_id": "1903.08560",
      "role": "Foundation",
      "relationship_sentence": "Provides the high-dimensional generalization framework for overparameterized linear regression and minimum-norm solutions that underpins the analytical lens used to derive MoE generalization and forgetting guarantees."
    },
    {
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": "James Kirkpatrick et al.",
      "year": 2017,
      "arxiv_id": "1612.00796",
      "role": "Gap Identification",
      "relationship_sentence": "Identifies catastrophic forgetting and proposes parameter-importance regularization (EWC), whose limitations in mitigating cross-task interference motivate analyzing capacity-partitioning via MoE as a principled alternative."
    }
  ],
  "synthesis_narrative": "Sparsely gated mixture-of-experts (MoE) introduced a learned router that activates a small subset of experts and includes explicit load-balancing terms, establishing a concrete mechanism for conditional computation and specialization (Shazeer et al.). Switch Transformers simplified this to top\u20111 routing and sharpened the role of auxiliary load-balancing losses, clarifying how routing and balanced capacity usage emerge in practice (Fedus et al.). In continual learning, Expert Gate showed that allocating inputs to task-specific experts via a gate can prevent interference and reduce forgetting, offering an empirical template for expert specialization across tasks (Aljundi et al.). Routing Networks further generalized the idea of a controller selecting computation paths conditioned on inputs or tasks, emphasizing adaptive selection as the driver of specialization (Rosenbaum et al.). Parallelly, high-dimensional theory for ridgeless least squares characterized generalization in overparameterized linear regression via minimum-norm solutions, providing tools to analyze test error precisely (Hastie et al.). Finally, EWC framed catastrophic forgetting and highlighted the limits of regularization-based stability in the face of task interference (Kirkpatrick et al.). Together, these works reveal a gap: while gated experts appear to mitigate forgetting by specializing capacity and balancing usage, there was no theoretical account of how routing and diversification translate into generalization gains under task sequences. By marrying MoE routing/balancing insights with the precise overparameterized linear regression framework, the present work naturally emerges\u2014proving that experts diversify to tasks, the router selects appropriately while balancing loads, and that this mechanism outperforms a single expert in continual learning.",
  "target_paper": {
    "title": "Theory on Mixture-of-Experts in Continual Learning",
    "authors": "Hongbo Li, Sen Lin, Lingjie Duan, Yingbin Liang, Ness Shroff",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "continual learning, mixture-of-experts, catastrophic forgetting, generalization error",
    "abstract": "Continual learning (CL) has garnered significant attention because of its ability to adapt to new tasks that arrive over time. Catastrophic forgetting (of old tasks) has been identified as a major issue in CL, as the model adapts to new tasks. The Mixture-of-Experts (MoE) model has recently been shown to effectively mitigate catastrophic forgetting in CL, by employing a gating network to sparsify and distribute diverse tasks among multiple experts. However, there is a lack of theoretical analysis of MoE and its impact on the learning performance in CL. This paper provides the first theoretical results to characterize the impact of MoE in CL via the lens of overparameterized linear regression tasks. We establish the benefit of MoE over a single expert by proving that the MoE model can diversify its experts to specialize in different tasks, while its router learns to select the right expert for each task and balance the loads across all experts. Our study further suggests an intriguing f",
    "openreview_id": "7XgKAabsPp",
    "forum_id": "7XgKAabsPp"
  },
  "analysis_timestamp": "2026-01-06T18:44:21.023171"
}
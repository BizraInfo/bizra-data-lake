{
  "prior_works": [
    {
      "title": "Plug and Play Language Models: a Simple Approach to Controlled Text Generation",
      "authors": "Siddharth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu",
      "year": 2020,
      "role": "Methodological antecedent (inference-time activation steering)",
      "relationship_sentence": "ITI builds on the core idea from PPLM that one can steer generation by modifying hidden activations at inference without finetuning, adapting this principle to a learned, low-dimensional \"truthful\" direction applied selectively to attention heads."
    },
    {
      "title": "Null It Out: Debiasing Language Models by Iterative Nullspace Projection",
      "authors": "Tali Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav Goldberg",
      "year": 2020,
      "role": "Methodological antecedent (linear directions in representation space)",
      "relationship_sentence": "ITI\u2019s learning of a linear direction corresponding to an attribute (truthfulness) echoes INLP\u2019s premise that sensitive attributes are encoded linearly and can be manipulated in representation space to control model behavior."
    },
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva, Ankit Gupta, Jonathan Berant",
      "year": 2021,
      "role": "Mechanistic interpretability foundation",
      "relationship_sentence": "By showing how information is stored and retrieved in specific components of transformers, this work motivates ITI\u2019s targeted, component-level interventions (e.g., focusing on a small set of attention heads/layers)."
    },
    {
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "authors": "Stephanie Lin, Jacob Hilton, Owain Evans",
      "year": 2021,
      "role": "Benchmark and problem framing",
      "relationship_sentence": "TruthfulQA defines the task and metrics for truthfulness that ITI optimizes against and validates on, framing the central challenge of eliciting truthful rather than human-like false answers."
    },
    {
      "title": "Training Language Models to Follow Instructions with Human Feedback",
      "authors": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, et al.",
      "year": 2022,
      "role": "Alignment baseline (RLHF) and cost contrast",
      "relationship_sentence": "InstructGPT established RLHF as a dominant path to helpfulness and truthfulness but with heavy annotation cost; ITI positions itself as a data-efficient, inference-time alternative that navigates the helpfulness\u2013truthfulness tradeoff without retraining."
    },
    {
      "title": "Language Models (Mostly) Know What They Know",
      "authors": "Saurav Kadavath, Amanda Askell, Nicholas Schiefer, Tom Henighan, et al.",
      "year": 2022,
      "role": "Conceptual evidence of latent truth knowledge",
      "relationship_sentence": "This work\u2019s finding that models possess internal signals about correctness underpins ITI\u2019s hypothesis that a \"truthfulness\" direction exists in hidden states that can be surfaced via intervention."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME)",
      "authors": "Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",
      "year": 2022,
      "role": "Representation manipulation precedent (weight/activation editing)",
      "relationship_sentence": "ROME demonstrates that targeted internal edits can systematically alter model outputs; ITI leverages a similar insight but performs non-persistent, inference-time activation shifts to improve truthfulness without model surgery."
    }
  ],
  "synthesis_narrative": "Inference-Time Intervention (ITI) emerges at the intersection of inference-time control, linear representation manipulation, and mechanistic insights about transformer internals. PPLM showed that modifying hidden activations during decoding can steer generation without finetuning; ITI inherits this core mechanism but replaces gradient-based guidance with a compact, learned linear direction that specifically amplifies truthfulness and can be smoothly tuned for a helpfulness\u2013truthfulness tradeoff. Complementing this, INLP\u2019s demonstration that semantic attributes are often linearly encoded in representations motivates ITI\u2019s search for a single direction that reliably modulates truthful behavior.\nMechanistic interpretability work on where information lives in transformers\u2014particularly the view of MLPs as key\u2013value memories and the functional specialization of components\u2014supports ITI\u2019s targeted application to a small number of attention heads and layers, increasing effectiveness and efficiency. On the evaluative and conceptual side, TruthfulQA frames the core objective and supplies a rigorous benchmark, while InstructGPT (RLHF) provides the dominant training-time alternative whose annotation intensity and side effects (e.g., sycophancy) ITI explicitly aims to mitigate. Finally, evidence that LMs possess internal confidence/knowledge signals (LMs Mostly Know What They Know) underwrites ITI\u2019s central claim: models encode latent truthfulness cues even when outputs are false, which can be elicited via representation steering. Together with targeted editing precedents like ROME, these works directly shape ITI\u2019s minimally invasive, data-efficient strategy: find and apply a truthfulness direction in specific components at inference to substantially improve truthful answering.",
  "analysis_timestamp": "2026-01-06T23:42:48.045663"
}
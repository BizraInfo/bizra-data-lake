{
  "prior_works": [
    {
      "title": "Visualizing and Understanding Convolutional Networks",
      "authors": "Matthew D. Zeiler, Rob Fergus",
      "year": 2014,
      "role": "Introduced deconvolution-based visualization that backpropagates unit activations to pixel space to reveal input evidence for a neuron.",
      "relationship_sentence": "Parallel Backpropagation adopts this core idea of mapping latent activations to the pixel level, but applies it simultaneously to two images to extract shared evidence."
    },
    {
      "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
      "authors": "Karen Simonyan, Andrea Vedaldi, Andrew Zisserman",
      "year": 2013,
      "role": "Established gradient-based saliency as a general backprop-to-input mechanism for attributing predictions to pixels.",
      "relationship_sentence": "The proposed method builds on saliency-style gradients to propagate relevance, extending it to a paired, shared-feature setting rather than single-image, class-specific attribution."
    },
    {
      "title": "Understanding Deep Image Representations by Inverting Them",
      "authors": "Aravindh Mahendran, Andrea Vedaldi",
      "year": 2015,
      "role": "Formalized inversion of deep feature activations to the image domain, demonstrating how intermediate-layer codes can be rendered in pixels.",
      "relationship_sentence": "The paper\u2019s backpropagation of latent activations from specific layers directly follows the feature inversion principle to visualize features that co-drive two stimuli."
    },
    {
      "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation",
      "authors": "Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, Wojciech Samek",
      "year": 2015,
      "role": "Proposed rule-based relevance redistribution through layers (LRP), guiding how evidence is propagated and weighted to inputs.",
      "relationship_sentence": "Parallel Backpropagation echoes LRP\u2019s idea of structured, layer-wise redistribution by selectively enhancing shared latent dimensions during the two-way propagation."
    },
    {
      "title": "Top-Down Neural Attention by Excitation Backprop",
      "authors": "Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen, Stan Sclaroff",
      "year": 2016,
      "role": "Introduced a probabilistic, top-down relevance flow emphasizing positive, target-consistent contributions.",
      "relationship_sentence": "Their selective top-down propagation motivates the paper\u2019s emphasis of shared, positively contributing feature dimensions when backpropagating two activation patterns in parallel."
    },
    {
      "title": "Representational similarity analysis \u2013 connecting the branches of systems neuroscience",
      "authors": "Nikolaus Kriegeskorte, Marieke Mur, Peter A. Bandettini",
      "year": 2008,
      "role": "Established comparing stimuli via similarity of activation patterns in representational spaces.",
      "relationship_sentence": "The step of selecting a preferred-category reference image with a similar feature activation profile is an RSA-style nearest-neighbor operation in DNN latent space."
    },
    {
      "title": "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
      "authors": "Daniel L. K. Yamins, Ha Hong, Charles F. Cadieu, Ethan A. Solomon, Darren Seibert, James J. DiCarlo",
      "year": 2014,
      "role": "Showed that deep network features are effective predictors of neural responses, enabling encoding models linking DNN latents to neurons.",
      "relationship_sentence": "The paper\u2019s neuron-response modeling from DNN latent activations follows this encoding-model tradition, providing the basis for identifying features that drive selectivity."
    }
  ],
  "synthesis_narrative": "Parallel Backpropagation for Shared-Feature Visualization marries two strands of work: (1) modeling biological neurons with deep network latents and (2) top-down visualization via backpropagation. The encoding-model foundation\u2014pioneered by Yamins et al.\u2014justifies predicting neural responses from DNN features, while Representational Similarity Analysis (Kriegeskorte et al.) motivates choosing a preferred-category reference image that matches an out-of-category stimulus in latent space. Once a pair is established, the method adapts the backpropagation-to-input paradigm of Zeiler & Fergus and Simonyan et al., ensuring that latent activations are rendered into pixel-space evidence.\nCrucially, the approach does not merely backpropagate each image separately; inspired by rule-based and attention-like top-down propagation (LRP; Excitation Backprop), it emphasizes the feature dimensions shared between the two activation patterns and suppresses idiosyncratic, non-shared components. This selective redistribution creates a joint relevance signal that visualizes the common visual features responsible for the neuron\u2019s strong response across category boundaries. Mahendran & Vedaldi\u2019s principled inversion of intermediate representations informs the choice of layers and the stability of reconstructions during propagation.\nTogether, these prior works directly underpin the paper\u2019s core innovation: a paired, parallel backpropagation scheme that uses representational matching to localize shared, neuron-driving features in pixels, thereby clarifying why out-of-category images can robustly activate category-selective brain regions.",
  "analysis_timestamp": "2026-01-06T23:33:35.552621"
}
{
  "prior_works": [
    {
      "title": "Near-Optimal Reward-Free Exploration for Reinforcement Learning with Function Approximation",
      "authors": "Tengyang Xie et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work formalized the coverability condition and the notion of an exploratory distribution with bounded density ratios, which the current paper assumes to justify the existence and boundedness of the density ratios it learns online."
    },
    {
      "title": "DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections",
      "authors": "Ofir Nachum et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "DualDICE introduced the core idea of learning discounted stationary density ratios via a saddle-point formulation with Bellman consistency, which directly inspires the paper\u2019s use of density-ratio modeling as the central object."
    },
    {
      "title": "GenDICE: Generalized Offline Estimation of Stationary Distribution Corrections",
      "authors": "Shangtong Zhang et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "GenDICE generalized DICE-style objectives using f-divergences and dual constraints, and the paper leverages this generalized density-ratio estimation machinery when coupling ratio realizability with value realizability."
    },
    {
      "title": "ValueDICE: Stabilizing Off-Policy Reinforcement Learning via Distribution Correction Estimation",
      "authors": "Ilya Kostrikov et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "ValueDICE demonstrated that jointly learning value functions and stationary density ratios can drive policy improvement from logged data, a joint-learning motif the paper adapts to the online setting under coverability."
    },
    {
      "title": "Minimax Weight and Q-Function Learning for Off-Policy Evaluation",
      "authors": "Masatoshi Uehara et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This paper introduced a minimax coupling between Q-functions and density ratios for finite-sample OPE under realizability, which the current work extends by embedding the same value\u2013ratio saddle-point structure into an online exploration-and-learning loop."
    },
    {
      "title": "Reward-Free Exploration for Reinforcement Learning with Function Approximation",
      "authors": "Sham M. Kakade et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "By formalizing exploration that targets a task-agnostic exploratory distribution, this line of work highlighted the centrality of coverage assumptions that make density ratios meaningful, a prerequisite the paper operationalizes for online RL."
    }
  ],
  "synthesis_narrative": "Density-ratio modeling in reinforcement learning was crystallized by DualDICE, which framed learning discounted stationary distribution corrections through a saddle-point objective enforcing Bellman consistency. GenDICE broadened this idea by casting density-ratio estimation under general f-divergences while retaining dual constraints, and ValueDICE showed that pairing ratio estimation with value learning can directly support policy improvement from logged data. In parallel, minimax weight and Q-function learning formalized a finite-sample, realizable setting for off-policy evaluation in which value functions and density ratios are learned jointly via a coupled saddle-point, giving a precise path to exploit realizability assumptions. Separately, reward-free exploration with function approximation emphasized constructing task-agnostic exploratory data and coverage guarantees, setting the stage for principled distributional assumptions. Most critically, the coverability framework established the existence of an exploratory distribution with bounded density ratios, giving a structural condition under which ratio-based methods are statistically sound.\nTaken together, these works exposed an opportunity: if an exploratory distribution exists, the offline density-ratio machinery (saddle-point coupling of value and ratio under realizability) could, in principle, be ported to online learning. The paper seizes this by assuming coverability to ensure bounded ratios, then embeds a DICE/MWL-style value\u2013ratio minimax program within an online procedure that actively collects data to approximate the exploratory distribution and drives policy improvement. This synthesis bridges offline density-ratio estimation with online RL, yielding a natural counterpart of density-ratio algorithms in the online regime under general function approximation.",
  "target_paper": {
    "title": "Harnessing Density Ratios for Online Reinforcement Learning",
    "authors": "Philip Amortila, Dylan J Foster, Nan Jiang, Ayush Sekhari, Tengyang Xie",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "reinforcement learning theory, online RL, offline RL, hybrid RL, density ratio, marginalized importance weight, weight function, general function approximation",
    "abstract": "The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of *density ratio modeling*, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start.\n\nIn this work we show---perhaps surprisingly---that density ratio-based algorithms have online counterparts.  Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as *coverability* (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-ef",
    "openreview_id": "THJEa8adBn",
    "forum_id": "THJEa8adBn"
  },
  "analysis_timestamp": "2026-01-06T07:11:50.529955"
}
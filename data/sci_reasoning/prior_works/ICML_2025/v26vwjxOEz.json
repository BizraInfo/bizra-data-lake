{
  "prior_works": [
    {
      "title": "Are You Smarter Than A Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension",
      "authors": "Kembhavi et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduced the canonical multimodal science QA setup that jointly uses diagrams and accompanying text, establishing the problem formulation EMMA generalizes and systematizes across STEM with stronger cross-modal interdependence."
    },
    {
      "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
      "authors": "Hudson et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Defined compositional visual reasoning evaluation and highlighted shortcut issues, providing the reasoning-centric evaluation philosophy that EMMA extends to tightly coupled image\u2013text reasoning requiring multi-step cross-modal composition."
    },
    {
      "title": "ScienceQA: A Large Dataset for Multimodal Science Question Answering",
      "authors": "Lu et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated multimodal science QA with images and text but many items are solvable via language-only cues; EMMA is designed explicitly to close this gap by constructing tasks that cannot be solved within either modality alone."
    },
    {
      "title": "ChartQA: A Benchmark for Question Answering on Charts with Visual and Logical Reasoning",
      "authors": "Masry et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "Showed that numerical reasoning over visual artifacts (charts) requires integrating perception with symbolic reasoning, informing EMMA\u2019s inclusion of quantitative, visually grounded reasoning beyond text-only math word problems."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Wei et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Established CoT as a standard reasoning protocol; EMMA explicitly tests whether CoT-style prompting carries over to genuinely multimodal, cross-modal reasoning and reveals its limitations in that setting."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Wang et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "Popularized test-time compute scaling via sampling and voting; EMMA evaluates this strategy in multimodal reasoning tasks and shows that such scaling underperforms when deep cross-modal integration is required."
    },
    {
      "title": "MMMU: A Massive Multi-discipline Multimodal Understanding Benchmark for Foundation Models",
      "authors": "Yue et al.",
      "year": 2024,
      "role": "Gap Identification",
      "relationship_sentence": "Provided a broad, multi-discipline MLLM benchmark but many questions are text-dominant or rely on shallow visual cues; EMMA directly targets this limitation with tasks requiring indispensable, multi-step image\u2013text reasoning."
    }
  ],
  "synthesis_narrative": "EMMA\u2019s design emerges from a clear lineage of multimodal evaluation that progressively sharpened the need for true cross-modal reasoning. Early work like TextbookQA formalized multimodal science comprehension by pairing diagrams with textual context, while GQA crystallized the goal of compositional visual reasoning and exposed shortcut vulnerabilities in benchmarks. Building on these foundations, ScienceQA scaled multimodal science QA but revealed a critical gap: many items remain solvable via language-only artifacts or shallow visual cues. Parallel efforts such as ChartQA underscored that robust reasoning demands fusing perception with symbolic and numerical operations, motivating EMMA\u2019s insistence that neither the image nor the text alone suffices. At the same time, reasoning protocols from the LLM literature\u2014Chain-of-Thought prompting and its self-consistency\u2013based test-time scaling\u2014became the de facto yardsticks for assessing \u201creasoning.\u201d EMMA explicitly probes whether these techniques transfer to genuinely multimodal reasoning, finding that they often fall short when visual and textual information must be integrated over multiple steps. Finally, broad benchmarks like MMMU highlighted evaluation breadth across disciplines but continued to suffer from text dominance and shallow visual reliance. EMMA synthesizes these insights into a benchmark purpose-built to test organic, indispensable, multi-step image\u2013text reasoning across mathematics, physics, chemistry, and coding, directly addressing prior benchmarks\u2019 limitations and stress-testing prevailing reasoning protocols.",
  "analysis_timestamp": "2026-01-06T23:07:19.624027"
}
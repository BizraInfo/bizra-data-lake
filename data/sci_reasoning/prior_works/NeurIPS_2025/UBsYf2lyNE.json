{
  "prior_works": [
    {
      "title": "Decoupling Representation and Classifier for Long-Tailed Recognition",
      "authors": "Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Alan L. Yuille, Jiashi Feng",
      "year": 2020,
      "role": "Two-stage decoupling template for long-tailed recognition (representation first, classifier later).",
      "relationship_sentence": "This paper establishes the decoupled two-stage paradigm that the new work adopts and advances by replacing class-balanced retraining with an information-theoretic representation stage and a mathematically-informative sampling strategy for the classifier."
    },
    {
      "title": "Learning Imbalanced Datasets with Label-Distribution-Aware Margin (LDAM) Loss and Deferred Re-Weighting (DRW)",
      "authors": "Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, Tengyu Ma",
      "year": 2019,
      "role": "Phase-wise training and decision-boundary calibration for long-tail via deferred re-weighting.",
      "relationship_sentence": "LDAM-DRW shows re-weighting should be postponed to protect representation quality; the proposed method formalizes this by first learning compact features then adjusting boundaries via targeted informative sampling in stage two."
    },
    {
      "title": "Class-Balanced Loss Based on Effective Number of Samples",
      "authors": "Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, Serge Belongie",
      "year": 2019,
      "role": "Principled re-weighting/resampling using effective number to counter class-frequency bias.",
      "relationship_sentence": "While effective-number weighting combats imbalance via counts, the new sampler advances this idea by selecting instances based on mathematical informativeness rather than frequency heuristics, reducing bias without overfitting tails."
    },
    {
      "title": "Long-tail Learning via Logit Adjustment",
      "authors": "Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, Sanjiv Kumar",
      "year": 2020,
      "role": "Theoretical link between class priors and optimal decision-boundary/logit correction.",
      "relationship_sentence": "Building on the prior-induced bias theory, the second stage targets instances that most effectively counteract prior-driven logit skew, acting as a data-driven alternative to analytic logit adjustment while maintaining head-class accuracy."
    },
    {
      "title": "Supervised Contrastive Learning",
      "authors": "Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, Dilip Krishnan",
      "year": 2020,
      "role": "Representation objective that minimizes intra-class distance and maximizes inter-class separation with information-theoretic roots (InfoNCE).",
      "relationship_sentence": "The proposed stage-1 objective, framed via information theory, is shown equivalent to minimizing intra-class distances akin to SupCon, yielding a well-separated feature space crucial for long-tailed recognition."
    },
    {
      "title": "A Discriminative Feature Learning Approach for Deep Face Recognition (Center Loss)",
      "authors": "Yandong Wen, Kaipeng Zhang, Zhifeng Li, Yu Qiao",
      "year": 2016,
      "role": "Explicit intra-class compactness regularizer for deep features.",
      "relationship_sentence": "Center Loss motivates the paper\u2019s emphasis on intra-class compactness; the new method achieves similar compactness but derives it from an information-theoretic objective that preserves useful data information."
    },
    {
      "title": "Focal Loss for Dense Object Detection",
      "authors": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\u00e1r",
      "year": 2017,
      "role": "Difficulty-aware weighting to focus learning on informative (hard) examples under imbalance.",
      "relationship_sentence": "The second-stage sampler operationalizes the focal principle of emphasizing informative samples, but does so via a mathematically grounded selection scheme rather than loss-based heuristics."
    }
  ],
  "synthesis_narrative": "The paper builds on the now-standard decoupled paradigm for long-tailed recognition, crystallized by Kang et al., which separates representation learning from classifier calibration. LDAM-DRW strengthened this view by showing that deferring re-weighting preserves representation quality while later adjustments correct majority-biased decision boundaries. Within this blueprint, the present work innovates in both stages. For representation, it draws on information-theoretic objectives connected to contrastive learning\u2014exemplified by Supervised Contrastive Learning and its InfoNCE roots\u2014while also embracing the classical goal of intra-class compactness typified by Center Loss. The authors formalize that their information-theoretic criterion is equivalent to minimizing intra-class distances, producing compact, well-separated features without discarding useful information.\n\nFor decision-boundary correction, rather than relying solely on frequency heuristics such as the Class-Balanced Loss or analytic priors like Logit Adjustment, the paper proposes a mathematically principled sampling strategy that selects informative instances to rectify bias. This echoes the hard-example emphasis of Focal Loss but replaces heuristic weighting with targeted data selection that preserves overall performance, particularly on head classes. In sum, the work synthesizes decoupled training (cRT), staged calibration (LDAM-DRW), principled handling of class priors (Logit Adjustment, Class-Balanced Loss), and information-theoretic compactness (SupCon, Center Loss) into an information-preservable two-stage pipeline that advances state-of-the-art long-tailed recognition.",
  "analysis_timestamp": "2026-01-07T00:05:12.549100"
}
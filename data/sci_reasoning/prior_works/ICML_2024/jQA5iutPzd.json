{
  "prior_works": [
    {
      "title": "The Perception-Distortion Tradeoff",
      "authors": "Yochai Blau et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "This paper provides the distribution-based formulation of perceptual quality that the present work adopts and extends, replacing the distortion axis with robustness/Lipschitz behavior under measurement consistency to derive a new impossibility tradeoff."
    },
    {
      "title": "On instabilities of deep learning in image reconstruction",
      "authors": "Vegard Antun et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Antun et al. empirically expose the extreme sensitivity of deep inverse solvers to tiny measurement perturbations; the current paper explains this phenomenon by proving that deterministic methods that push both perceptual realism and measurement consistency must have large Lipschitz constants, hence are inherently attack-prone."
    },
    {
      "title": "Plug-and-Play Priors for Model Based Reconstruction",
      "authors": "Sreehari Venkatakrishnan et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "PnP established a widely used deterministic framework that explicitly enforces data-consistency while injecting learned priors; the present theory targets exactly this class of methods, formalizing how simultaneous consistency and perceptual realism forces high Lipschitzness."
    },
    {
      "title": "The Little Engine That Could: Regularization by Denoising (RED)",
      "authors": "Yaniv Romano et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "RED is a concrete deterministic inverse approach that blends data-fidelity with a denoiser prior; the new results extend to such operators by linking their pursuit of consistency plus perceptual quality to unavoidable growth in the reconstruction map\u2019s Lipschitz constant."
    },
    {
      "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
      "authors": "Christian Ledig et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "SRGAN inaugurated deterministic SR models optimized for perceptual realism (adversarial/perceptual losses), forming the type of high-perception, measurement-aware baselines on which the paper demonstrates the resulting vulnerability implied by its Lipschitz lower bounds."
    },
    {
      "title": "Robustness May Be at Odds with Accuracy",
      "authors": "Dimitris Tsipras et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "This work crystallized a fundamental tradeoff between standard performance and adversarial robustness; the present paper translates this insight to inverse imaging by rigorously proving a perception\u2013robustness tradeoff tied to Lipschitz growth under measurement consistency."
    },
    {
      "title": "Denoising Diffusion Restoration Models",
      "authors": "Bahjat Kawar et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "DDRM exemplifies stochastic, measurement-consistent restoration that explores the posterior; the current paper leverages its theory to show how deterministic models can imitate such posterior exploration by exploiting the very sensitivity (large Lipschitz) their tradeoff predicts."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014a rigorous perception\u2013robustness tradeoff for deterministic image restoration\u2014rests on two pillars: a principled definition of perceptual quality and the requirement of measurement consistency in inverse problems. Blau and Michaeli\u2019s perception\u2013distortion framework supplied the key perceptual formalism, which this work repurposes by replacing the distortion axis with robustness/Lipschitz behavior. In parallel, deterministic inverse frameworks like Plug-and-Play Priors and RED crystallized the modern paradigm of enforcing data-fidelity while injecting powerful learned priors; the present analysis targets precisely such methods, showing that simultaneously pushing consistency and perceptual realism inevitably inflates the reconstruction map\u2019s Lipschitz constant.\n\nEmpirical observations of fragility in inverse solvers, most prominently documented by Antun et al., directly motivate the paper\u2019s central theorem: if a deterministic method is both highly perceptual and consistent, it must be highly sensitive to adversarial perturbations. This mirrors the conceptual template set by Tsipras et al., who showed a fundamental tension between accuracy and robustness in classification; here, the tension is formalized for inverse imaging via Lipschitz lower bounds.\n\nFinally, the application to single-image super-resolution connects to SRGAN-style perceptual baselines that prioritize realism, illustrating the predicted vulnerability. The work also situates itself relative to stochastic, posterior-exploring restorers such as DDRM, showing that the very sensitivity implied by the theorem can be harnessed to traverse the posterior, enabling deterministic models to mimic stochastic sampling behavior.",
  "analysis_timestamp": "2026-01-06T23:09:26.480648"
}
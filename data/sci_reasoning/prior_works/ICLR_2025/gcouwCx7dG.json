{
  "prior_works": [
    {
      "title": "Sparse Evolutionary Training of Deep Neural Networks",
      "authors": "Decebal C. Mocanu et al.",
      "year": 2018,
      "arxiv_id": "1704.05119",
      "role": "Gap Identification",
      "relationship_sentence": "SET prunes and regrows connections with a fixed fraction each update, and this static pruning ratio is the exact limitation (under/over-pruning) our method replaces with a compressibility-driven, adaptive schedule for sparse SNN training."
    },
    {
      "title": "Rigging the Lottery: Making All Tickets Winners",
      "authors": "Utku Evci et al.",
      "year": 2020,
      "arxiv_id": "1911.11134",
      "role": "Baseline",
      "relationship_sentence": "RigL\u2019s gradient-based connection growth under a fixed global sparsity is our main dynamic-sparsity baseline, over which we introduce compressibility-aware adjustment of the pruning level tailored to SNNs."
    },
    {
      "title": "Parameter Efficient Training of Deep CNNs by Dynamic Sparse Reparameterization",
      "authors": "Hussein Mostafa and Xin Wang",
      "year": 2019,
      "arxiv_id": "1907.04840",
      "role": "Related Problem",
      "relationship_sentence": "Dynamic Sparse Reparameterization couples magnitude pruning with growth while enforcing a target sparsity; we extend this dynamic reallocation idea by learning the target via a compressibility (PQ) index rather than fixing it."
    },
    {
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "authors": "Jonathan Frankle and Michael Carbin",
      "year": 2019,
      "arxiv_id": "1803.03635",
      "role": "Foundation",
      "relationship_sentence": "The existence of \u2018winning ticket\u2019 sparse subnetworks motivates our first-stage evaluation of candidate sparse SNN subnetworks, which we operationalize via a PQ-based compressibility metric to preserve trainability while compressing."
    },
    {
      "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity",
      "authors": "Namhoon Lee et al.",
      "year": 2019,
      "arxiv_id": "1810.02340",
      "role": "Foundation",
      "relationship_sentence": "SNIP\u2019s connection-sensitivity criterion for pruning at or near initialization informs our practice of assessing subnetwork viability during training, which we generalize to SNNs using a compression-efficiency\u2013aware PQ index."
    },
    {
      "title": "Deep Rewiring: Training very sparse deep neural networks",
      "authors": "Guillaume Bellec et al.",
      "year": 2018,
      "arxiv_id": "1711.05136",
      "role": "Inspiration",
      "relationship_sentence": "Deep Rewiring introduced biologically inspired synaptic rewiring for sparse SNNs; we adopt the structural plasticity view but guide both rewiring and sparsity levels by measured compression efficiency."
    },
    {
      "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "authors": "Yihui He et al.",
      "year": 2018,
      "arxiv_id": "1802.03494",
      "role": "Inspiration",
      "relationship_sentence": "AMC formalized choosing per-layer pruning ratios by optimizing an accuracy\u2013compression objective; we bring this compression-efficiency principle to SNN structural learning via a lightweight PQ index rather than RL search."
    }
  ],
  "synthesis_narrative": "Sparse Evolutionary Training (SET) showed that prune-and-grow can maintain trainability in sparse networks but relies on a fixed pruning fraction at every update, making sparsity control insensitive to training dynamics. RigL advanced dynamic sparse training by growing connections using gradient signals while keeping a fixed global sparsity target, and Dynamic Sparse Reparameterization similarly coupled magnitude pruning with growth under a preset sparsity budget. The Lottery Ticket Hypothesis established that trainable sparse subnetworks exist and can match dense performance when appropriately selected, encouraging principled evaluation of subnetwork viability. SNIP demonstrated that connection-sensitivity can identify viable sparse subnetworks at or near initialization, pointing to saliency-style criteria for early or mid-training assessment. In the SNN context, Deep Rewiring introduced biologically inspired synaptic rewiring, emphasizing structural plasticity as an effective mechanism for sparse SNNs. Complementarily, AMC cast pruning as an explicit accuracy\u2013compression optimization, showing that pruning ratios should be chosen by compression efficiency rather than set heuristically.\nTogether, these works reveal a gap: dynamic sparse methods reallocate connections but freeze the sparsity level, while compression-aware methods choose ratios but lack spike-based structural plasticity. The natural next step is a two-stage SNN structure learning procedure that (i) evaluates the compressibility of current sparse subnetworks\u2014preserving winning-ticket\u2013like trainability via a PQ-style efficiency metric\u2014and (ii) adaptively adjusts the pruning ratio to avoid under/over-pruning, thereby aligning synaptic rewiring with compression efficiency throughout training.",
  "target_paper": {
    "title": "Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency",
    "authors": "Jiangrong Shen, Qi Xu, Gang Pan, Badong Chen",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "spiking neural networks",
    "abstract": "The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to construct event-driven models that emulate this efficiency. Despite these advances, deep SNNs continue to suffer from over-parameterization during training and inference, a stark contrast to the brain\u2019s ability to self-organize. Furthermore, existing sparse SNNs are challenged by maintaining optimal pruning levels due to a static pruning ratio, resulting in either under or over-pruning.\nIn this paper, we propose a novel two-stage dynamic structure learning approach for deep SNNs, aimed at maintaining effective sparse training from scratch while optimizing compression efficiency. \nThe first stage evaluates the compressibility of existing sparse subnetworks within SNNs using the PQ index, which",
    "openreview_id": "gcouwCx7dG",
    "forum_id": "gcouwCx7dG"
  },
  "analysis_timestamp": "2026-01-06T06:03:41.456936"
}
{
  "prior_works": [
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Bai et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Established the dominant prevention-first alignment paradigm (helpfulness/harmlessness via RLHF), which the current paper reframes by adding a learned mechanism for mid-generation recovery rather than only reducing harmful token probabilities."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Bai et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Introduced self-critique-and-revision for harmlessness; the present work internalizes this revision idea into decoding itself by teaching models to invoke a [RESET] token to revise unsafe partial outputs on the fly."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Rafailov et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "Provides the preference-optimization framework the authors directly extend by training preferences that favor trajectories using [RESET] over unsafe continuations, integrating backtracking into DPO."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Yao et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Explicitly introduced backtracking over intermediate reasoning states; this paper adapts that backtracking concept to safety by enabling the model to backtrack from unsafe partial generations."
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "authors": "Madaan et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated that LMs can iteratively critique and edit their own outputs; the current work compresses this edit loop into a single learned [RESET] action usable online during generation."
    },
    {
      "title": "Levenshtein Transformer",
      "authors": "Gu et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Pioneered edit-based generation with explicit insertion/deletion operations; the [RESET] token extends the notion of an explicit editing operation by enabling deletion/backtracking for safety in autoregressive LMs."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Zou et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "Showed that prevention-only alignment can be reliably circumvented by jailbreaks and that unsafe continuations persist once started, directly motivating a mechanism that can undo partial unsafe generations."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014teaching language models to backtrack from unsafe partial outputs via a learned [RESET] token\u2014emerges from two converging lines of prior work. First, alignment methods like RLHF (Bai et al.) and Constitutional AI set the helpfulness/harmlessness objective but mostly operationalize safety by lowering harmful token probabilities. Their practical limitations, highlighted by jailbreak studies (Zou et al.), reveal that prevention alone is brittle: once an unsafe trajectory begins, models tend to continue it. Constitutional AI\u2019s self-revision step suggests a remedy\u2014edit the output\u2014but it is applied as a distinct phase rather than during decoding.\nSecond, algorithmic advances around structured generation provided a template for recovery. Tree of Thoughts introduced explicit backtracking over intermediate states, while Self-Refine showed iterative critique-and-edit can reliably improve outputs. Even earlier, the Levenshtein Transformer demonstrated that sequence generation can be framed as a series of explicit edit operations. The present paper fuses these insights into an online mechanism: a [RESET] operator that lets the model erase unsafe continuations and resume from a safe prefix. Technically, this is integrated into modern preference learning by extending DPO to prefer traces that invoke [RESET] over unsafe continuations, yielding a practical, trainable decoding behavior. In sum, the work shifts safety from prevention-only to prevention-plus-recovery by operationalizing backtracking as a first-class, learnable action during generation.",
  "analysis_timestamp": "2026-01-06T23:09:26.640993"
}
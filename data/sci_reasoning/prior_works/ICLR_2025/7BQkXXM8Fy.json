{
  "prior_works": [
    {
      "title": "Diffuser: Diffusion Models for Planning",
      "authors": "Michael Janner et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "Introduced trajectory-level diffusion planning with value/reward-guided sampling; this paper directly ablates those core design choices (trajectory diffusion, guidance, planning strategy) and shows that alternative choices like unconditional sampling can outperform the Diffuser-style defaults."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Provided the fundamental diffusion modeling framework (training objective, noise schedules, UNet-style parameterization) that this work systematically stress-tests in the decision-making context."
    },
    {
      "title": "Classifier-Free Diffusion Guidance",
      "authors": "Jonathan Ho and Tim Salimans",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Popularized conditional guidance via mixing conditional/unconditional scores; the present paper empirically revisits guidance for planning and finds that, contrary to common practice, unconditional sampling can be superior in offline RL diffusion planning."
    },
    {
      "title": "Denoising Diffusion Implicit Models",
      "authors": "Jiaming Song et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "Introduced fast non-Markovian diffusion samplers; this paper evaluates sampling procedures (steps, guidance vs unconditional) for planners, directly building on DDIM-style alternatives to standard sampling."
    },
    {
      "title": "Trajectory Transformer",
      "authors": "Michael Janner et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Established the generative-trajectory modeling and plan-by-sampling paradigm for offline decision making that diffusion planners adopt; this paper studies which diffusion instantiations of that paradigm actually work best."
    },
    {
      "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
      "authors": "Lili Chen et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Introduced return-conditioned sequence modeling for control, a conditioning scheme frequently inherited by diffusion planners; this work probes conditioning/guidance design choices and their actual impact on offline RL performance."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014systematically identifying what makes diffusion planning effective in offline RL\u2014sits squarely on the trajectory-as-generative-model formulation and diffusion modeling mechanics introduced by a small set of prior works. Diffuser (Janner et al., 2022) is the immediate baseline and practical template for diffusion planners: it models full trajectories and relies on value/reward-guided sampling during planning. The present study directly interrogates these choices\u2014trajectory-level modeling, guidance, and planning strategies\u2014showing that some widely adopted defaults can be suboptimal (e.g., unconditional sampling sometimes outperforming guidance).\n\nThe broader problem formulation\u2014treating decision making as sequence generation over trajectories\u2014was crystallized by Trajectory Transformer and Decision Transformer, which established plan-by-sampling and return-conditioned control. Diffusion planners inherit these ideas, and this paper evaluates how conditioning/guidance choices drawn from that lineage actually affect performance.\n\nUnderpinning all of this are the diffusion foundations from DDPM, which define the training objective, parameterization, and noise schedules that this work ablates at scale, and DDIM, which introduced alternative (faster, non-Markovian) samplers that influence practical planning performance. Finally, the specific notion of guided sampling comes from classifier-free guidance (Ho & Salimans), whose conditional\u2013unconditional score mixing has become the de facto approach; the paper\u2019s key empirical finding that unconditional sampling can be preferable directly challenges that inherited assumption in the planning domain.",
  "analysis_timestamp": "2026-01-06T23:08:23.928707"
}
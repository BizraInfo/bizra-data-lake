{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Kernel regime baseline",
      "relationship_sentence": "Established the lazy-training/NTK regime where gradient descent behaves like kernel regression, providing the key contrast baseline that lacks feature learning and thus motivates demonstrating depth-enabled sample-efficiency gains via hierarchical feature learning."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat, Francis Bach",
      "year": 2019,
      "role": "Feature-learning vs lazy regimes of GD",
      "relationship_sentence": "Formally characterized when gradient descent remains in a kernel (lazy) regime versus when it learns features, directly underpinning the paper\u2019s controlled training setting and its argument that depth-driven feature learning, not lazy dynamics, yields the dimensionality reduction and sample gains."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli",
      "year": 2013,
      "role": "Stagewise dynamics and mode-wise learning",
      "relationship_sentence": "Showed that gradient descent in deep (linear) nets learns target structure in a stagewise, mode-by-mode manner, inspiring the paper\u2019s central mechanism that GD progressively reduces effective dimensionality by aligning with lower-dimensional subspaces sequentially."
    },
    {
      "title": "Deep vs. Shallow Networks: An Approximation Theory Perspective for Compositional Functions",
      "authors": "Hrushikesh Mhaskar, Tomaso Poggio",
      "year": 2016,
      "role": "Depth advantage for hierarchical/compositional targets",
      "relationship_sentence": "Provided approximation-theoretic and sample-complexity arguments that deep architectures are exponentially more efficient for compositional/hierarchical functions, directly motivating the paper\u2019s Gaussian hierarchical target class and its depth-induced sample-efficiency theorem."
    },
    {
      "title": "The Power of Depth for Feedforward Neural Networks",
      "authors": "Ronen Eldan, Ohad Shamir",
      "year": 2016,
      "role": "Depth separation",
      "relationship_sentence": "Proved depth separation results showing shallow networks require exponentially more resources for certain functions, reinforcing the paper\u2019s claim that shallow models are sample-inefficient for the proposed hierarchical targets relative to deep, feature-learning networks."
    },
    {
      "title": "Generalization error of random features and two-layer neural networks in high dimensions",
      "authors": "Song Mei, Theodor Misiakiewicz, Andrea Montanari",
      "year": 2022,
      "role": "High-dimensional generalization for shallow/random-feature baselines",
      "relationship_sentence": "Developed precise high-dimensional analyses contrasting random features and trained two-layer nets, supplying methodological and conceptual tools to quantify where shallow/kernel-like approaches fall short compared to representation learning highlighted in the paper."
    },
    {
      "title": "Sliced Inverse Regression for Dimension Reduction",
      "authors": "Ker-Chau Li",
      "year": 1991,
      "role": "Foundations of single-/multi-index models and effective dimension reduction",
      "relationship_sentence": "Introduced sufficient dimension reduction for single-index models, directly grounding the paper\u2019s single and multi-index Gaussian hierarchical targets and the notion that learning can focus on a low-dimensional subspace despite high ambient dimension."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014proving that gradient descent in deep networks achieves a computational advantage by successively reducing effective dimensionality on hierarchical targets\u2014sits at the intersection of three lines of prior work. First, approximation and expressivity results establish why depth should help for hierarchical structure. Mhaskar and Poggio\u2019s theory of compositional functions and Eldan\u2013Shamir\u2019s depth separation formalize that deep architectures can represent and learn certain targets with vastly fewer resources than shallow models, motivating the paper\u2019s Gaussian hierarchical targets and the search for concrete sample-complexity gains.\nSecond, insights on optimization dynamics clarify how depth translates to learning mechanisms. Saxe et al. showed that gradient descent proceeds in stages, aligning with low-complexity modes before higher ones. Chizat\u2013Bach and the NTK framework (Jacot et al.) delineate regimes where training is lazy (kernel-like) versus feature-learning, indicating that escaping the kernel regime is essential to realize depth\u2019s benefits. The present work leverages these ideas to demonstrate that GD in deep networks actively learns representations that iteratively compress the relevant subspace, turning a high-dimensional task into a sequence of lower-dimensional problems.\nThird, high-dimensional statistical analyses of shallow baselines show their limitations. Mei\u2013Misiakiewicz\u2013Montanari quantify generalization in random features and two-layer models, framing where shallow/kernel methods are sample-inefficient. Complementing this, classical sufficient-dimension-reduction (Li) underpins the single-/multi-index viewpoint that the target depends on a latent low-dimensional subspace. By synthesizing these strands, the paper provides a precise, high-dimensional account of when and why depth-enabled feature learning yields dramatic sample-efficiency gains over shallow alternatives.",
  "analysis_timestamp": "2026-01-07T00:21:32.265660"
}
{
  "prior_works": [
    {
      "title": "Sum-Product Networks: A New Deep Architecture",
      "authors": "Hoifung Poon et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work established the tractable probabilistic circuit formalism with additive sum nodes that the present paper retains structurally while replacing purely additive mixtures by subtractive ones safeguarded through a global squaring."
    },
    {
      "title": "Probabilistic Circuits: A Unifying Framework for Tractable Probabilistic Models",
      "authors": "Antonio Vergari et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This framework formalizes decomposability/determinism, tensorized mixtures, and learning/inference in probabilistic circuits, which the paper directly uses to define, learn, and analyze squared circuits that allow subtractions while keeping tractability."
    },
    {
      "title": "Unsupervised Generative Modeling Using Matrix Product States",
      "authors": "Zhao-Yu Han et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By modeling probabilities as the square of a signed tensor-network amplitude (Born rule), this paper provided the key idea that squaring can enforce nonnegativity while permitting destructive interference, directly inspiring the paper\u2019s strategy of squaring subtractive circuits."
    },
    {
      "title": "Probabilistic Sentential Decision Diagrams",
      "authors": "Adnan Darwiche et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "PSDDs supply the structured probabilistic circuit architecture on which the paper instantiates its squared subtractive mixtures, extending PSDDs beyond additive sums while preserving exact, tractable inference."
    },
    {
      "title": "Learning the Structure of Sum-Product Networks",
      "authors": "Robert Gens et al.",
      "year": 2013,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This is the canonical additive-mixture SPN learning approach whose inability to represent cancellations motivates the new subtractive-by-squaring design and serves as a primary baseline the paper targets and improves upon."
    },
    {
      "title": "Nonnegative Rank, Decompositions, and Factorizations of Nonnegative Matrices",
      "authors": "Joel Cohen et al.",
      "year": 1993,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The contrast between nonnegative rank and unrestricted rank from this line of work underpins the paper\u2019s expressivity results, formalizing why allowing subtractions (then squaring) can yield exponential savings over additive mixtures."
    },
    {
      "title": "On the Connection Between Sum-Product Networks and Tensor Networks",
      "authors": "Robert Peharz et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "By showing SPNs/PCs correspond to tensor decompositions, this work enables viewing circuits as tensorized mixtures, informing the paper\u2019s positioning of squared subtractive circuits within a tensor-network-style representation and analysis."
    }
  ],
  "synthesis_narrative": "Sum-Product Networks introduced a tractable deep architecture where probability distributions are represented via additive sums over products under decomposability and completeness, fixing the template for exact inference in circuit form. Subsequent unification under probabilistic circuits clarified the semantics of decomposability/determinism and codified learning/inference routines as well as how circuit computations correspond to tensorized mixtures. PSDDs contributed a structured, deterministic circuit family that preserves tractability while aligning distributions with logical structure, serving as a practical backbone for scalable learning. In parallel, tensor-network generative modeling showed that one can model probabilities as the square of a signed amplitude\u2014most notably with matrix product states\u2014thereby permitting destructive interference while ensuring nonnegativity through squaring. Work connecting SPNs/PCs to tensor networks established that circuit computations implement tensor decompositions, making tensorized mixtures a natural lens for these models. Foundational results on nonnegative versus unrestricted ranks provided the mathematical rationale that forbidding subtraction can force exponentially larger nonnegative decompositions compared to signed ones. Taken together, these strands exposed a clear opportunity: circuits enforce tractability but are constrained by additivity; tensor-network Born models demonstrate that squaring recovers nonnegativity despite internal cancellations; and rank theory predicts exponential gains from allowing subtraction. The paper synthesizes these insights by defining and learning squared probabilistic circuits that permit subtractive mixtures, proving the predicted expressivity advantages and instantiating them on structured PCs (e.g., PSDDs) to retain tractable inference while achieving more compact, accurate density estimators.",
  "target_paper": {
    "title": "Subtractive Mixture Models via Squaring: Representation and Learning",
    "authors": "Lorenzo Loconte, Aleksanteri Mikulus Sladek, Stefan Mengel, Martin Trapp, Arno Solin, Nicolas Gillis, Antonio Vergari",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "tractable inference, distribution estimation, probabilistic circuits, tensor networks",
    "abstract": "Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.",
    "openreview_id": "xIHi5nxu9P",
    "forum_id": "xIHi5nxu9P"
  },
  "analysis_timestamp": "2026-01-06T10:59:19.985950"
}
{
  "prior_works": [
    {
      "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm",
      "authors": "Qiang Liu et al.",
      "year": 2016,
      "arxiv_id": "1608.04471",
      "role": "Foundation",
      "relationship_sentence": "This work introduced SVGD and the RKHS-based steepest-descent characterization of KL, providing the calculus (via Stein operators) that the present paper leverages to express the KL time-derivative in terms of the KSD and to set up the entropy-dissipation argument at finite N."
    },
    {
      "title": "A Kernelized Stein Discrepancy for Goodness-of-fit Tests",
      "authors": "Qiang Liu et al.",
      "year": 2016,
      "arxiv_id": "1602.03253",
      "role": "Foundation",
      "relationship_sentence": "It formalized the kernelized Stein discrepancy and linked it to Stein operators and RKHS embeddings, which is precisely the discrepancy the new analysis controls by isolating a dominant \u2212N E[KSD^2] term in the KL derivative."
    },
    {
      "title": "Measuring Sample Quality with Kernels",
      "authors": "Jackson Gorham et al.",
      "year": 2017,
      "arxiv_id": "1703.01717",
      "role": "Foundation",
      "relationship_sentence": "By establishing statistical properties and concentration of KSD for i.i.d. samples (yielding 1/\u221aN scaling), this paper provides the benchmark rate that the new finite-particle SVGD bounds explicitly aim to match."
    },
    {
      "title": "On the Geometry of Stein Variational Gradient Descent",
      "authors": "Andrew Duncan et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Casting SVGD as a gradient flow of KL with an explicit entropy-dissipation structure, this work motivates the present paper\u2019s strategy of differentiating a KL functional (here, between the joint N-particle law and the product target) to extract the leading KSD^2 dissipation term and control residuals."
    },
    {
      "title": "Finite-Particle Convergence of Stein Variational Gradient Descent",
      "authors": "Shi et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This recent analysis provided the first finite-N convergence guarantees for SVGD but with exponentially worse N-dependence, whose limitation is directly addressed and sharply improved by the new entropy-splitting argument yielding O(1/\u221aN) KSD rates."
    }
  ],
  "synthesis_narrative": "Stein Variational Gradient Descent was introduced as a deterministic particle method that transports an empirical measure toward a target by following the steepest KL descent within an RKHS parameterization of velocity fields, grounded in Stein operators (Liu and Wang, 2016). The kernelized Stein discrepancy (KSD) was then formalized as an RKHS-based discrepancy whose squared value quantifies how much a distribution violates Stein\u2019s identity (Liu, Lee, and Jordan, 2016). Complementing this, Gorham and Mackey (2017) established statistical properties of KSD, including concentration guarantees that deliver 1/\u221aN decay for i.i.d. samples, thereby setting a sharp benchmark for N-scaling in KSD. A geometric perspective subsequently cast SVGD as a gradient flow of KL, making the entropy-dissipation structure explicit and connecting the instantaneous decrease of KL to squared KSD within an appropriate Riemannian metric (Duncan, N\u00fcsken, and Szpruch, 2019). Most recently, Shi et al. (2024) initiated a finite-particle analysis for SVGD in KSD, but their non-asymptotic rates suffered from exponentially unfavorable dependence on N.\nThese ingredients together suggest differentiating a carefully chosen KL functional to extract a dominant KSD-squared dissipation. By applying this gradient-flow calculus not to a single marginal but to the KL between the joint N-particle law and the product target, one isolates a leading \u2212N E[KSD^2] term and controls smaller correlation-induced remainders, naturally yielding near-i.i.d. O(1/\u221aN) rates and enabling discrete-time counterparts. The same structure, paired with a bilinear augmentation of the kernel to control low-order moments, extends to Wasserstein-2 control in continuous time, closing the core gap left by prior finite-N analyses.",
  "target_paper": {
    "title": "Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent",
    "authors": "Sayan Banerjee, Krishna Balasubramanian, PROMIT GHOSAL",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Stein Variational Gradient Descent, Non-asymptotic Rates, Variational Inference",
    "abstract": "We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy ($\\KSD$) and Wasserstein-2 metrics. Our key insight is that the time derivative of the relative entropy between the joint density of $N$ particle locations and the $N$-fold product target measure, starting from a regular initial distribution, splits into a dominant 'negative part' proportional to $N$ times the expected $\\KSD^2$ and a smaller 'positive part'. This observation leads to $\\KSD$ rates of order $1/\\sqrt{N}$, in both continuous and discrete time, providing a near optimal (in the sense of matching the corresponding i.i.d. rates) double exponential improvement over the recent result by~\\cite{shi2024finite}. Under mild assumptions on the kernel and potential, these bounds also grow polynomially in the dimension $d$. By adding a bilinear component to the kernel, the above approach is used to further obtain Wasserstein-2 convergence in contin",
    "openreview_id": "sbG8qhMjkZ",
    "forum_id": "sbG8qhMjkZ"
  },
  "analysis_timestamp": "2026-01-06T19:07:55.164876"
}
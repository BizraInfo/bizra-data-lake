{
  "prior_works": [
    {
      "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning",
      "authors": "Sai Praneeth Karimireddy et al.",
      "year": 2020,
      "arxiv_id": "1910.06378",
      "role": "Extension",
      "relationship_sentence": "The paper reformulates SCAFFOLD\u2019s control-variate update into an equivalent single-uplink message and then builds compressed variants (SCALLION for unbiased and SCAFCOM for biased compressors) directly on this stochastic controlled averaging mechanism."
    },
    {
      "title": "Error Feedback Fixes SignSGD and other Gradient Compression Methods",
      "authors": "Sai Praneeth Karimireddy et al.",
      "year": 2019,
      "arxiv_id": "1901.09847",
      "role": "Extension",
      "relationship_sentence": "SCAFCOM integrates the error-feedback mechanism from this work to make biased compressors (e.g., sign/top-k) convergent in the presence of client drift and partial participation."
    },
    {
      "title": "DIANA: A new gradient compression and distributed optimization algorithm",
      "authors": "Dmitry Mishchenko et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "DIANA\u2019s use of unbiased compression coupled with a control-variate/memory to control variance directly informs SCALLION\u2019s design and analysis for compressed updates under heterogeneity."
    },
    {
      "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding",
      "authors": "Dan Alistarh et al.",
      "year": 2017,
      "arxiv_id": "1610.02132",
      "role": "Foundation",
      "relationship_sentence": "Provides the canonical unbiased quantization operators and variance bounds that SCALLION targets and supports in its unified analysis of unbiased compressors."
    },
    {
      "title": "FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization",
      "authors": "Amirhossein Reisizadeh et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Serves as a primary compressed-FL baseline combining quantization and periodic averaging under partial participation whose limitations on heterogeneity and compressor assumptions are addressed and improved upon by SCALLION/SCAFCOM."
    },
    {
      "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
      "authors": "H. Brendan McMahan et al.",
      "year": 2017,
      "arxiv_id": "1602.05629",
      "role": "Foundation",
      "relationship_sentence": "Defines the federated optimization setting with local updates and partial participation that the proposed controlled-averaging with compression operates within and seeks to improve in communication."
    }
  ],
  "synthesis_narrative": "Federated Averaging established the federated optimization protocol with local client updates and partial participation, but also exposed client-drift under data heterogeneity and left open how to safely compress communication. SCAFFOLD introduced stochastic controlled averaging via client and server control variates to cancel drift, achieving stability under arbitrary heterogeneity, yet it required sending both model updates and control variates and was not purpose-built for compressed communication. DIANA demonstrated that unbiased compression can be combined with a control-variate-like memory to control variance and retain convergence, providing a blueprint for marrying compression and variance reduction, albeit in a more centralized/distributed setting. QSGD formalized unbiased quantization operators with explicit variance bounds, giving standard compressors and analysis handles for unbiased compression. Complementing this, Error-Feedback Fixes SignSGD showed that a simple error-feedback memory can provably correct the bias of aggressive compressors (e.g., sign or top-k), enabling strong compression without sacrificing convergence. FedPAQ brought quantization into FL with periodic averaging under partial participation but relied on restrictive assumptions and lacked heterogeneity-robust drift correction.\nTogether, these works suggest unifying control-variates for drift correction with principled compression: use SCA-style controlled averaging to neutralize heterogeneity, instantiate compressed updates with unbiased operators (QSGD/DIANA-style) or, for biased compressors, add error feedback, and ensure partial participation compatibility. The natural next step is to reformulate SCA to reduce per-round messaging and then design two algorithms\u2014one for unbiased and one for biased compression\u2014that retain convergence under arbitrary heterogeneity and partial participation while improving communication and computation efficiency.",
  "target_paper": {
    "title": "Stochastic Controlled Averaging for Federated Learning with Communication Compression",
    "authors": "Xinmeng Huang, Ping Li, Xiaoyun Li",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "federated learning, communication compression, data heterogeneity, controlled averaging",
    "abstract": "Communication compression has been an important topic in Federated Learning (FL) for alleviating the communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL such as partial participation and data heterogeneity. Despite the recent development, the existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression. In this paper, we revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs, building upon which we propose two compressed FL algorithms, SCALLION and  SCAFCOM, to support unbiased and biased compression, respectively. Both the proposed methods outperform the existing compressed FL methods in terms of communication and computation complexities. Moreover",
    "openreview_id": "jj5ZjZsWJe",
    "forum_id": "jj5ZjZsWJe"
  },
  "analysis_timestamp": "2026-01-06T23:48:24.757711"
}
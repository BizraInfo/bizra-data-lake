{
  "prior_works": [
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Flamingo introduced the core idea of modeling arbitrarily interleaved image\u2013text sequences, which LLaVA-Interleave explicitly adopts and generalizes as a unifying template for multi-image, multi-frame (video), multi-view (3D), and multi-patch inputs."
    },
    {
      "title": "LLaVA: Large Language-and-Vision Assistant",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "LLaVA established the visual instruction tuning recipe and LMM architecture that LLaVA-Interleave extends from single-image dialog to interleaved multi-image scenarios using the same instruction-following paradigm."
    },
    {
      "title": "LLaVA-1.5: Improved Baselines for Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "LLaVA-1.5\u2019s stronger training recipe and data curation serve as the direct methodological starting point that LLaVA-Interleave modifies to handle interleaved multi-image/video/3D data and to scale instruction tuning beyond single-image tasks."
    },
    {
      "title": "IDEFICS: An Open Reproduction of Flamingo",
      "authors": "Hugo Lauren\u00e7on et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "IDEFICS operationalized interleaved image\u2013text conversational modeling in an open framework, providing a primary multi-image baseline that LLaVA-Interleave aims to surpass via its unified interleaved training across broader scenarios."
    },
    {
      "title": "InstructBLIP: Towards General-Purpose Vision-Language Models with Instruction Tuning",
      "authors": "Wenliang Dai et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "InstructBLIP demonstrated that broad, task-diverse instruction-tuning data drives generalization, directly inspiring LLaVA-Interleave\u2019s M4-Instruct compilation across 14 tasks and 41 datasets for multi-image/video/3D capabilities."
    },
    {
      "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision-Language Models",
      "authors": "Muhammad Maaz et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "Video-ChatGPT separately adapts LLaVA-style models to video via multi-frame inputs, a specialization whose fragmentation LLaVA-Interleave explicitly targets by unifying video with other multi-image scenarios under one interleaved template and training set."
    },
    {
      "title": "A Corpus for Reasoning about Natural Language Grounded in Photographs (NLVR2)",
      "authors": "Alane Suhr et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "NLVR2 crystallized the multi-image reasoning problem (paired-image grounding), a core formulation that LLaVA-Interleave incorporates into its interleaved training/evaluation to generalize multi-image understanding."
    }
  ],
  "synthesis_narrative": "LLaVA-Interleave\u2019s core idea\u2014treating interleaved image\u2013text sequences as a universal interface across multi-image, video, 3D (multi-view), and multi-patch inputs\u2014directly descends from Flamingo, which first showed that a language model can condition on arbitrarily interleaved visual and textual tokens. IDEFICS operationalized this paradigm in open form and serves as a practical multi-image baseline. Building on the instruction-following LMM blueprint of LLaVA, and the stronger data/recipe of LLaVA-1.5, LLaVA-Interleave extends the single-image visual instruction tuning pipeline to interleaved multi-image settings, preserving the conversational format while changing the input template and supervision scope. In parallel, InstructBLIP demonstrated that broad, task-diverse instruction data is pivotal for generalization, motivating LLaVA-Interleave\u2019s M4-Instruct curation that spans 14 tasks and 41 datasets covering multi-image, multi-frame, multi-view, and multi-patch regimes. On the video side, Video-ChatGPT exemplified a trend of building separate video-specialized LMMs by treating frames as multiple images; LLaVA-Interleave explicitly addresses this fragmentation by subsuming video frames under the same interleaved template used for other multi-image scenarios. Finally, NLVR2 established a canonical multi-image reasoning formulation, which LLaVA-Interleave incorporates in training and benchmarking to ensure rigorous evaluation of multi-image understanding. Together, these works provide the interleaving mechanism, instruction-tuning methodology, and problem definitions that LLaVA-Interleave unifies into a single, general-purpose multi-image LMM.",
  "analysis_timestamp": "2026-01-06T23:09:26.631064"
}
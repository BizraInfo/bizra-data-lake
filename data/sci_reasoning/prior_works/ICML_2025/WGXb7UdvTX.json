{
  "prior_works": [
    {
      "title": "Understanding intermediate layers using linear classifier probes",
      "authors": "Guillaume Alain, Yoshua Bengio",
      "year": 2016,
      "role": "Methodological precursor (layerwise probing)",
      "relationship_sentence": "Introduced linear probes to quantify the predictive content of intermediate layers, directly inspiring the paper\u2019s systematic evaluation of representation quality across depth."
    },
    {
      "title": "Opening the Black Box of Deep Neural Networks via Information",
      "authors": "Ravid Shwartz-Ziv, Naftali Tishby",
      "year": 2017,
      "role": "Theoretical foundation (information bottleneck/MI across layers)",
      "relationship_sentence": "Provided the information-plane view of networks balancing compression and prediction, which underpins the paper\u2019s information-theoretic metrics for layerwise signal preservation vs. compression."
    },
    {
      "title": "On the Information Bottleneck Theory of Deep Learning",
      "authors": "Andrew M. Saxe, Yamini Bansal, Joel Dapello, Madhu S. Advani, Artemy Kolchinsky, Brendan D. Tracey, David D. Cox",
      "year": 2019,
      "role": "Theoretical clarification (MI estimation and dynamics)",
      "relationship_sentence": "Critiqued and refined how mutual information is estimated and interpreted in deep nets, motivating the paper\u2019s careful, robust design of information-theoretic layer-quality metrics."
    },
    {
      "title": "Similarity of Neural Network Representations Revisited",
      "authors": "Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton",
      "year": 2019,
      "role": "Metric/tool (representation geometry via CKA)",
      "relationship_sentence": "Introduced CKA as a stable geometry-based measure of representational similarity, directly informing the paper\u2019s geometric component of the unified evaluation framework across layers and architectures."
    },
    {
      "title": "BERT Rediscovers the Classical NLP Pipeline",
      "authors": "Ian Tenney, Dipanjan Das, Ellie Pavlick",
      "year": 2019,
      "role": "Empirical evidence (layerwise specialization in NLP)",
      "relationship_sentence": "Showed that linguistic phenomena emerge at specific BERT depths, motivating the paper\u2019s hypothesis and tests that intermediate layers can yield superior downstream features."
    },
    {
      "title": "A Structural Probe for Finding Syntax in Word Representations",
      "authors": "John Hewitt, Christopher D. Manning",
      "year": 2019,
      "role": "Methodological influence (probing for structural information)",
      "relationship_sentence": "Demonstrated that mid-layer embeddings encode syntactic structure recoverable by simple probes, supporting the paper\u2019s probe-based layer assessments and invariance-to-perturbation analyses."
    },
    {
      "title": "Adversarial Examples Are Not Bugs, They Are Features",
      "authors": "Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry",
      "year": 2019,
      "role": "Conceptual motivation (robust vs. non-robust features/invariance)",
      "relationship_sentence": "Framed robustness and invariance in terms of feature alignment with task signal, motivating the paper\u2019s invariance-based criteria for judging layer representations under input perturbations."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a unified framework to quantify layerwise representation quality via information-theoretic, geometric, and invariance criteria\u2014builds on three intertwined research threads. First, probing methods established that intermediate layers carry accessible task signals. Alain and Bengio\u2019s linear probes made layerwise evaluation concrete, while Tenney et al. and Hewitt and Manning provided compelling NLP evidence that specific linguistic properties peak at mid depths and are easily extracted by simple probes. These works supplied both the methodological template and the empirical motivation to search for stronger features before the last layer.\nSecond, the information-theoretic lineage shapes how the paper formalizes \u201cquality.\u201d Shwartz-Ziv and Tishby\u2019s information plane highlighted a trade-off between compression and prediction across depth; Saxe et al.\u2019s critique cautioned against naive mutual-information estimates and spurred robust operationalizations. Together they informed the paper\u2019s metrics that quantify how layers balance signal preservation with compression, explaining why mid-depth embeddings can outperform final layers.\nThird, the framework\u2019s geometric and invariance components draw from representation-comparison and robustness literatures. Kornblith et al.\u2019s CKA provided a stable, architecture-agnostic lens on representational geometry that scales across transformers and state-space models. Ilyas et al.\u2019s view of adversarial vulnerability as feature misalignment motivated invariance-to-perturbation tests to assess whether representations capture robust, task-aligned signals. By synthesizing these strands, the paper explains and validates the consistent superiority of intermediate-layer embeddings across diverse models and embedding tasks.",
  "analysis_timestamp": "2026-01-07T00:04:09.137440"
}
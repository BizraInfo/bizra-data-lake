{
  "prior_works": [
    {
      "title": "Photorealistic Text-to-Image Diffusion Models with Imagen",
      "authors": "Chitwan Saharia et al.",
      "year": 2022,
      "arxiv_id": "2205.11487",
      "role": "Foundation",
      "relationship_sentence": "Gecko\u2019s curated prompt set explicitly builds on the idea of diagnostic, capability-focused prompt suites popularized by DrawBench in Imagen, while addressing its coverage and single-template limitations."
    },
    {
      "title": "Parti: Scaling Autoregressive Models for Content-Rich Image Generation",
      "authors": "Yu et al.",
      "year": 2022,
      "arxiv_id": "2206.10789",
      "role": "Gap Identification",
      "relationship_sentence": "Parti\u2019s PartiPrompts established broad, hand-crafted prompts for stress-testing T2I models, and Gecko directly targets the gap that such single-slice prompt collections can yield unstable conclusions across different prompt categories and annotation styles."
    },
    {
      "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
      "authors": "Jack Hessel et al.",
      "year": 2021,
      "arxiv_id": "2104.08718",
      "role": "Baseline",
      "relationship_sentence": "Gecko evaluates and contextualizes CLIP-based alignment metrics like CLIPScore, showing how conclusions about model alignment can change across prompt slices and human-annotation templates."
    },
    {
      "title": "Pick-a-Pic: An Open Dataset of Human Preferences for Text-to-Image Generation",
      "authors": "Kirstain et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Gecko directly compares against preference-trained scoring functions (e.g., PickScore) derived from Pick-a-Pic\u2019s pairwise human annotations and shows how relying on a single annotation template or slice can mislead conclusions."
    },
    {
      "title": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation",
      "authors": "Xu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Gecko benchmarks reward-model-based evaluators like ImageReward and demonstrates that their perceived reliability depends strongly on the prompt distribution and the human rating protocol used."
    },
    {
      "title": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
      "authors": "Hu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "TIFA\u2019s QA-based faithfulness evaluation exemplifies an alternative alignment assessment, and Gecko systematically situates such metric-based judgments within a broader, multi-template human evaluation to reveal cross-slice instability."
    },
    {
      "title": "Rank Analysis of Incomplete Block Designs I: The Method of Paired Comparisons",
      "authors": "Ralph Allan Bradley and Milton E. Terry",
      "year": 1952,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Gecko\u2019s statistically grounded model comparison builds on Bradley\u2013Terry style paired-comparison inference to aggregate human preferences into robust cross-model rankings across multiple slices."
    }
  ],
  "synthesis_narrative": "DrawBench introduced a focused, human-curated prompt suite within Imagen to probe specific capabilities, establishing the practice of using diagnostic prompts for model comparison. Parti extended this idea with PartiPrompts, broadening the scope of capabilities and compositions stress-tested by text-to-image systems. In parallel, CLIPScore became a dominant reference-free alignment metric, operationalizing CLIP similarity as a proxy for text-image faithfulness. Preference-driven evaluators emerged with Pick-a-Pic, which collected large-scale pairwise human judgments and led to learned scoring functions (e.g., PickScore) that rank images by user preference. ImageReward likewise trained a reward model directly on human preferences to score alignment and quality, offering a learned evaluator alternative to fixed CLIP-based metrics. TIFA proposed a complementary approach\u2014converting prompts into visual question answering checks to measure grounded faithfulness\u2014highlighting that different evaluation formulations capture distinct facets of \u201calignment.\u201d Underpinning many human preference studies, Bradley\u2013Terry modeling provides a principled way to infer system rankings from pairwise comparisons. Together, these works revealed powerful but siloed evaluation practices: curated prompt sets, CLIP-based similarity, learned preference metrics, and QA-based faithfulness. However, each typically relied on a single prompt slice or annotation template, obscuring generalization. Gecko synthesizes these strands by curating a capability-diverse prompt set, unifying multiple human annotation templates, and applying a Bradley\u2013Terry-style statistical comparison to aggregate across slices. This integration exposes when metric conclusions fail to transfer and yields more stable, generalizable rankings of text-to-image models and evaluators.",
  "target_paper": {
    "title": "Revisiting text-to-image evaluation with Gecko: on metrics, prompts, and human rating",
    "authors": "Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajic, Su Wang, Emanuele Bugliarello, Yasumasa Onoe, Pinelopi Papalampidi, Ira Ktena, Christopher Knutsen, Cyrus Rashtchian, Anant Nawalgaria, Jordi Pont-Tuset, Aida Nematzadeh",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "text-to-image evaluation; text-to-image alignment; human evaluation;",
    "abstract": "While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. \nWhile many metrics and benchmarks have been proposed to evaluate T2I models and alignment metrics, the impact of the evaluation components (prompt sets, human annotations, evaluation task) has not been systematically measured.\nWe find that looking at only *one slice of data*, i.e. one set of capabilities or human annotations, is not enough to obtain stable conclusions that generalise to new conditions or slices when evaluating T2I models or alignment metrics. \nWe address this by introducing an evaluation suite of $>$100K annotations across four human annotation templates that comprehensively evaluates models' capabilities across a range of methods for gathering human annotations and comparing models.\nIn particular, we propose (1) a carefully curated set of prompts -- *Gecko2K*; (2) a statistically grounded method of comparing T2I models; and (3) h",
    "openreview_id": "Im2neAMlre",
    "forum_id": "Im2neAMlre"
  },
  "analysis_timestamp": "2026-01-06T17:09:05.459508"
}
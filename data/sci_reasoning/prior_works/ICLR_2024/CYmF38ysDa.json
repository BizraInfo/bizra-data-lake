{
  "prior_works": [
    {
      "title": "MT-Bench: Multi-turn Benchmark for Evaluating LLMs as Chatbots",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "MT-Bench\u2019s LLM-as-judge, multi-turn preference scoring provides the coarse-grained, overall evaluation baseline that FLASK explicitly decomposes into per-instruction, skill-level judgments to gain interpretability and reliability."
    },
    {
      "title": "AlpacaEval: Automatic Evaluation of Instruction-following Models",
      "authors": "Li et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "AlpacaEval popularized using GPT-4 as an automatic judge to produce a single holistic win rate, whose lack of fine-grained interpretability is the gap FLASK addresses by eliciting and scoring the specific skills each instruction requires."
    },
    {
      "title": "G-Eval: NLG Evaluation using GPT-4 with Better Prompting",
      "authors": "Liu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "G-Eval showed rubric-guided, criterion-level LLM judging can better match humans, an idea FLASK generalizes by dynamically deriving per-instruction alignment skills and scoring each skill rather than relying on a fixed task-specific rubric."
    },
    {
      "title": "Holistic Evaluation of Language Models",
      "authors": "Percy Liang et al.",
      "year": 2022,
      "arxiv_id": "2211.09110",
      "role": "Gap Identification",
      "relationship_sentence": "HELM established multi-dimensional LLM evaluation but at scenario- or metric-level granularity, motivating FLASK\u2019s move to instance-wise skill-set decomposition for finer interpretability within instruction following."
    },
    {
      "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
      "authors": "Marco Tulio Ribeiro et al.",
      "year": 2020,
      "arxiv_id": "2005.04118",
      "role": "Inspiration",
      "relationship_sentence": "CheckList\u2019s capability-based, compositional behavioral testing inspired FLASK\u2019s notion of decomposing each instruction into atomic skills and evaluating performance at the capability level rather than a single aggregate score."
    },
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "arxiv_id": "2204.05862",
      "role": "Foundation",
      "relationship_sentence": "The Helpful/Harmless (and truthful) alignment axes articulated by Bai et al. anchor the alignment-oriented skill taxonomy that FLASK operationalizes into explicit, per-instruction skill scores."
    }
  ],
  "synthesis_narrative": "MT-Bench introduced an LLM-as-judge protocol for open-ended, multi-turn chatbot evaluation, distilling model quality into overall preference judgments, while AlpacaEval advanced automatic instruction-following assessment via GPT-4 judges that output a single holistic win rate. G-Eval demonstrated that rubric-guided, criterion-level judgments by large models can more faithfully reflect human preferences than undifferentiated scores, establishing that structured prompts and explicit criteria improve reliability. Holistic Evaluation of Language Models (HELM) framed evaluation as multi-dimensional across scenarios and metrics, highlighting breadth but remaining coarse at the instance level. CheckList earlier promoted capability-based, compositional behavioral testing, arguing for decomposing model behavior into atomic skills to expose failure modes invisible to aggregate metrics. In parallel, Bai et al. formalized helpfulness, harmlessness, and related alignment dimensions at the core of assistant behavior, providing concrete alignment axes that naturally map to evaluable skills.\nTaken together, these works revealed a gap: dominant open-ended evaluations reduce responses to single preferences or fixed rubrics, sacrificing interpretability and masking which alignment capabilities succeed or fail per instruction. Building on rubric-guided LLM judging and capability decomposition, and grounded in alignment axes like helpfulness and harmlessness, the current work synthesizes a protocol that first infers the skills each instruction demands and then scores responses at the skill level. This instance-wise skill composition preserves the openness of MT-Bench\u2013style settings while delivering the fine-grained interpretability and improved reliability that HELM and CheckList imply are necessary for a holistic view of model alignment.",
  "target_paper": {
    "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
    "authors": "Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "large language models, language model evaluation, natural language processing",
    "abstract": "Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlati",
    "openreview_id": "CYmF38ysDa",
    "forum_id": "CYmF38ysDa"
  },
  "analysis_timestamp": "2026-01-06T16:26:49.191847"
}
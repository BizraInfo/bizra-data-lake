{
  "prior_works": [
    {
      "title": "Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods",
      "authors": "John C. Platt",
      "year": 1999,
      "role": "Origin of Platt scaling (parametric post-hoc logistic calibration of margins).",
      "relationship_sentence": "The paper\u2019s angular calibration and its analysis of when Platt/logistic scaling is optimal build directly on Platt\u2019s idea of parametric post-hoc calibration, tying the scaling parameter to the estimator\u2013truth angle in the Gaussian linear teacher\u2013student model."
    },
    {
      "title": "On Calibration of Modern Neural Networks",
      "authors": "Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger",
      "year": 2017,
      "role": "Reintroduced simple one-parameter post-hoc calibration (temperature scaling) and standard evaluation metrics (ECE).",
      "relationship_sentence": "This work motivates the one-parameter post-hoc calibration paradigm that the present paper reinterprets theoretically, showing a principled, angle-driven parameterization with provable high-dimensional guarantees."
    },
    {
      "title": "Beta Calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers",
      "authors": "Meelis Kull, Telmo Silva Filho, Peter Flach",
      "year": 2017,
      "role": "Parametric alternatives to Platt scaling that systematize post-hoc calibration families.",
      "relationship_sentence": "By situating angular calibration within parametric post-hoc families, the paper clarifies when logistic/Platt-style mappings are Bregman-optimal under a Gaussian linear teacher and how their parameter should depend on the estimator\u2013truth angle."
    },
    {
      "title": "A Modern Maximum-Likelihood Theory for High-Dimensional Logistic Regression",
      "authors": "Pragya Sur, Emmanuel J. Cand\u00e8s",
      "year": 2019,
      "role": "High-dimensional asymptotic theory for GLMs with Gaussian design; tools for consistent performance characterization when n,p grow proportionally.",
      "relationship_sentence": "The paper leverages the proportional-asymptotic framework and techniques from this work to analyze linear classifiers under Gaussian covariates and to justify consistent estimation of the angle between the estimator and the true parameter."
    },
    {
      "title": "The Generalization Error of Max-Margin Linear Classifiers: Precise Asymptotics",
      "authors": "Song Mei, Andrea Montanari",
      "year": 2022,
      "role": "Precise high-dimensional analysis showing test performance depends on the cosine overlap (angle) between the learned weight and the ground-truth vector.",
      "relationship_sentence": "This result directly motivates the paper\u2019s core idea that the estimator\u2013truth angle is a sufficient statistic for out-of-sample behavior, leading to angle-driven calibration and its provable optimality."
    },
    {
      "title": "Clustering with Bregman Divergences",
      "authors": "Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, Joydeep Ghosh",
      "year": 2005,
      "role": "Establishes the centrality of Bregman divergences and their connection to exponential-family likelihoods (including Bernoulli/KL).",
      "relationship_sentence": "The paper\u2019s Bregman-optimality claim builds on the link between probabilistic modeling and Bregman divergences, showing that the proposed calibrated predictor uniquely minimizes an appropriate Bregman (e.g., KL) divergence within a calibrated class."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014angular calibration for linear binary classifiers with Gaussian features, together with a proof of calibration and Bregman-optimality in high dimensions\u2014sits at the intersection of post-hoc calibration and precise high-dimensional asymptotics for linear models. Platt (1999) originated parametric post-hoc calibration via logistic mapping of margins; this work explains when such Platt-style scaling is theoretically optimal and how its single parameter should be chosen as a function of the estimator\u2013truth angle. Guo et al. (2017) rekindled interest in simple one-parameter post-hoc calibrators (temperature scaling) and standardized evaluation, motivating the search for principled, theory-backed parameterizations; Kull et al. (2017) broadened the parametric family (e.g., beta calibration), against which the present work positions an angle-driven, generative-model-justified choice.\nOn the statistical theory side, Sur and Cand\u00e8s (2019) provide the proportional-asymptotic GLM framework with Gaussian covariates that underwrites consistency and enables precise analysis in the regime n,p\u2192\u221e at a constant ratio. Mei and Montanari (2022) demonstrate that in teacher\u2013student Gaussian models, generalization is determined by the cosine overlap (angle) between the learned and true parameters; this directly motivates using the angle as the key sufficient statistic for calibration and studying its consistent estimation. Finally, the Bregman-optimality guarantee leverages the foundational connection between exponential-family likelihoods and Bregman divergences (Banerjee et al., 2005), justifying the claim that the proposed calibrated predictor uniquely minimizes an appropriate Bregman divergence to the true Bernoulli label distribution.",
  "analysis_timestamp": "2026-01-07T00:21:32.231237"
}
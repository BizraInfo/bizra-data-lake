{
  "prior_works": [
    {
      "title": "Generic Methods for Optimization-Based Modeling",
      "authors": "Justin Domke",
      "year": 2012,
      "role": "Foundational implicit differentiation for iterative/argmin problems; introduced practical hypergradient estimators based on fixed-point/Neumann-series views and truncation instead of full unrolling.",
      "relationship_sentence": "One-step differentiation is the extreme truncation of the fixed-point/Neumann expansions Domke advocated; this paper builds on that idea and provides a precise approximation analysis showing when a single step is accurate (especially for fast algorithms)."
    },
    {
      "title": "Hyperparameter Optimization with Approximate Gradient",
      "authors": "Fabian Pedregosa",
      "year": 2016,
      "role": "Formalized implicit gradients for solution mappings of convex problems, giving scalable alternatives to reverse-mode through long optimization trajectories.",
      "relationship_sentence": "The implicit gradient formula (via (I \u2212 Jx)\u22121\u2202F/\u2202\u03b8) that one-step approximates is directly rooted in Pedregosa\u2019s framework, which serves as the baseline accuracy target for the new estimator."
    },
    {
      "title": "Gradient-Based Hyperparameter Optimization through Reversible Learning",
      "authors": "Dougal Maclaurin, David Duvenaud, Ryan P. Adams",
      "year": 2015,
      "role": "Showed reverse-mode differentiation through many optimization iterations (unrolling) and highlighted its heavy memory/compute costs, motivating alternatives.",
      "relationship_sentence": "The paper\u2019s promise of AD-level simplicity without unrolling directly responds to Maclaurin et al.\u2019s memory/compute bottlenecks by proposing a one-step surrogate that avoids storing or replaying long optimization trajectories."
    },
    {
      "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning",
      "authors": "Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, Massimiliano Pontil",
      "year": 2018,
      "role": "Established the bilevel optimization viewpoint and practical algorithms using unrolling/implicit methods for hyperparameter and meta-learning problems.",
      "relationship_sentence": "The current work situates one-step differentiation within this bilevel framework and analyzes its effect on upper-level optimization, directly extending the bilevel methodologies introduced by Franceschi et al."
    },
    {
      "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks",
      "authors": "Brandon Amos, J. Zico Kolter",
      "year": 2017,
      "role": "Popularized differentiating through optimization layers via KKT/implicit function theorems, clarifying fixed-point/solution-map Jacobians and their computation.",
      "relationship_sentence": "OptNet\u2019s implicit-differentiation template underlies the exact gradient that one-step seeks to approximate, and the present paper clarifies when the one-step surrogate matches this implicit gradient for fast solvers."
    },
    {
      "title": "Optimizing Millions of Hyperparameters by Implicit Differentiation",
      "authors": "Jonas Lorraine, Paul Vicol, David Duvenaud",
      "year": 2020,
      "role": "Scaled implicit hypergradients using linear-system solvers and Neumann-series truncations with JVPs/VJPs, quantifying compute\u2013accuracy trade-offs.",
      "relationship_sentence": "The one-step estimator can be viewed as the first Neumann term in Lorraine et al.\u2019s implicit-gradient solvers; this paper delivers theory showing when that extreme truncation is near-exact (e.g., superlinear methods)."
    },
    {
      "title": "Deep Equilibrium Models",
      "authors": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun",
      "year": 2019,
      "role": "Introduced implicit layers trained with \u2018Jacobian-free backpropagation,\u2019 computing gradients via fixed-point linear solves using only Jacobian\u2013vector products.",
      "relationship_sentence": "The present work adopts and formalizes the Jacobian-free backpropagation paradigm, analyzing a one-step variant and proving when it approximates full implicit differentiation while retaining AD-like simplicity."
    }
  ],
  "synthesis_narrative": "The paper positions one-step differentiation at the intersection of three established lines: unrolling-based AD, implicit differentiation of solution mappings, and truncated/Neumann approximations for hypergradients. Domke (2012) first articulated hypergradients via fixed-point/implicit differentiation and proposed truncated, Jacobian-free estimators\u2014conceptually the progenitors of a one-step surrogate. Pedregosa (2016) and OptNet (Amos & Kolter, 2017) then consolidated the implicit-function approach for differentiating through optimization, defining the exact target Jacobian that one-step aims to approximate. In parallel, Maclaurin et al. (2015) exposed the prohibitive memory and compute of reverse-mode through long optimization trajectories, motivating alternatives that avoid full unrolling.\n\nSubsequent work in bilevel learning (Franceschi et al., 2018) established the dominant application context, while Lorraine et al. (2020) demonstrated scalable implicit gradients using Neumann-series and linear solves, making explicit the trade-off between solver iterations and gradient accuracy. Deep Equilibrium Models (Bai et al., 2019) further popularized Jacobian-free backpropagation for fixed-point systems using only JVP/VJP oracles.\n\nBuilding on these threads, the paper crystallizes \u201cone-step differentiation\u201d as an extreme truncation\u2014essentially the first Neumann term or a single adjoint iterate\u2014and delivers new approximation guarantees showing it is asymptotically as accurate as full implicit differentiation for fast (e.g., superlinear) algorithms like Newton\u2019s method. The resulting estimator matches the usability of AD while achieving the efficiency of implicit differentiation in regimes that matter for bilevel optimization, thus unifying prior ideas under a precise theory and practical recipe.",
  "analysis_timestamp": "2026-01-07T00:02:04.852230"
}
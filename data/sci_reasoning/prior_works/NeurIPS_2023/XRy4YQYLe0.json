{
  "prior_works": [
    {
      "title": "Comparison of Experiments",
      "authors": "David Blackwell",
      "year": "1951",
      "role": "Foundational decision-theoretic framework for comparing statistical experiments",
      "relationship_sentence": "The paper directly leverages Blackwell\u2019s ordering of statistical experiments (1951/1953) to characterize the distribution-imposed performance limits under fairness constraints, which it defines as aleatoric discrimination."
    },
    {
      "title": "Equality of Opportunity in Supervised Learning",
      "authors": "Moritz Hardt, Eric Price, Nati Srebro",
      "year": "2016",
      "role": "Defines equalized odds/equality of opportunity and a post-processing intervention",
      "relationship_sentence": "The authors use equalized odds-style constraints and the associated post-processing baseline from Hardt et al. as canonical fairness constraints/interventions when computing and benchmarking their aleatoric limits and epistemic gaps."
    },
    {
      "title": "A Reductions Approach to Fair Classification",
      "authors": "Alekh Agarwal, Alina Beygelzimer, Miroslav Dud\u00edk, John Langford, Hanna Wallach",
      "year": "2018",
      "role": "General in-processing method that enforces group fairness via cost-sensitive reductions",
      "relationship_sentence": "This reductions framework serves as a principal fairness intervention the paper benchmarks against its theoretically derived aleatoric limits to quantify epistemic discrimination."
    },
    {
      "title": "The Cost of Fairness in Binary Classification",
      "authors": "Aditya Menon, Robert C. Williamson",
      "year": "2018",
      "role": "Bayes-optimal analysis of accuracy\u2013fairness trade-offs under group fairness constraints",
      "relationship_sentence": "The paper builds on the Bayes-optimal viewpoint of constrained performance from Menon and Williamson, extending it via Blackwell\u2019s experiment order to formalize inherent (aleatoric) limits and separate them from epistemic gaps."
    },
    {
      "title": "Inherent Trade-Offs in the Fair Determination of Risk Scores",
      "authors": "Jon Kleinberg, Sendhil Mullainathan, Manish Raghavan",
      "year": "2016",
      "role": "Impossibility results highlighting inherent incompatibilities among fairness desiderata",
      "relationship_sentence": "These impossibility results motivate the notion that some disparities are unavoidable given the data, a premise the paper makes precise by quantifying aleatoric discrimination as distribution-driven limits."
    },
    {
      "title": "Data Preprocessing Techniques for Classification Without Discrimination",
      "authors": "Faisal Kamiran, Toon Calders",
      "year": "2012",
      "role": "Pre-processing fairness interventions (e.g., reweighting) to reduce group disparities",
      "relationship_sentence": "The paper evaluates such pre-processing methods against the derived aleatoric limits, using their performance shortfall to quantify epistemic discrimination."
    },
    {
      "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
      "authors": "Alex Kendall, Yarin Gal",
      "year": "2017",
      "role": "Introduces the aleatoric vs. epistemic uncertainty taxonomy",
      "relationship_sentence": "The paper adapts Kendall and Gal\u2019s uncertainty taxonomy to fairness, coining aleatoric and epistemic discrimination and grounding its decomposition of fairness errors."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014decomposing discrimination into aleatoric (distribution-inherent) and epistemic (model/algorithm-induced) components and quantifying fundamental performance limits under fairness constraints\u2014rests on a synthesis of decision theory, fairness constraints, and uncertainty taxonomies. At its theoretical core, Blackwell\u2019s comparison of statistical experiments provides the ordering tool to express how much actionable information a dataset affords for fair prediction; this yields distribution-imposed upper bounds on achievable accuracy subject to fairness, which the authors term aleatoric discrimination. The Bayes-optimal constrained viewpoint of Menon and Williamson complements this by framing the optimal accuracy\u2013fairness frontier, which the present work generalizes and sharpens via Blackwell ordering to obtain fundamental limits rather than algorithm-specific outcomes. Impossibility results from Kleinberg et al. motivate that such inherent limits truly exist\u2014some fairness desiderata are mutually incompatible given the data\u2014providing conceptual justification for an aleatoric component. On the operational side, widely used fairness constraints and interventions\u2014Equalized Odds and post-processing (Hardt et al.), reductions-based in-processing (Agarwal et al.), and pre-processing reweighting (Kamiran & Calders)\u2014supply the practical algorithms the authors benchmark against their limits; the gaps between these algorithms\u2019 performance and the aleatoric bound quantify epistemic discrimination. Finally, the aleatoric/epistemic terminology is imported from Kendall and Gal\u2019s uncertainty taxonomy, guiding the conceptual reframing from uncertainty to fairness and enabling a principled decomposition of observed disparities.",
  "analysis_timestamp": "2026-01-07T00:02:04.849826"
}
{
  "prior_works": [
    {
      "title": "Conformal Language Modeling",
      "authors": "Anastasios N. Angelopoulos et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "TRON directly builds on CLM\u2019s split-conformal construction of set-valued LM outputs but removes CLM\u2019s reliance on internal logits and multiple-choice constraints by introducing a sampling-based conformal score applicable to open-ended MLLMs."
    },
    {
      "title": "Conformal Risk Control",
      "authors": "Anastasios N. Angelopoulos et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "TRON instantiates CRC\u2019s general, distribution-free risk-control framework by defining two explicit risks\u2014one for sampling minimal response sets and one for identifying high-quality responses\u2014and calibrating thresholds via split conformal."
    },
    {
      "title": "Distribution-Free Predictive Inference for Regression",
      "authors": "Jing Lei",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "TRON adopts split conformal prediction\u2019s holdout-based calibration mechanism from this work to obtain distribution-free guarantees without retraining or accessing model internals."
    },
    {
      "title": "Classification with Valid and Adaptive Coverage",
      "authors": "Yaniv Romano et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "TRON\u2019s \u201csample\u201d stage mirrors APS\u2019s objective of constructing minimal-size prediction sets at a target risk level, adapting the idea to generative response sets via a novel conformal score over samples."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "TRON\u2019s \u201cidentify\u201d stage operationalizes self-consistency by using agreement among sampled generations as a nonconformity signal to select high-quality responses with calibrated error control."
    },
    {
      "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
      "authors": "Sirawajit Manakul et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "Motivating TRON\u2019s logit-free and sampling-based assessment, SelfCheckGPT shows that black-box self-consistency across samples can detect hallucinations, which TRON formalizes into a conformal nonconformity score with guarantees."
    }
  ],
  "synthesis_narrative": "TRON\u2019s core innovation\u2014risk-controlled sampling and identification for open- and closed-ended outputs from multimodal LLMs without access to logits\u2014emerges by unifying conformal prediction theory with self-consistency\u2013based assessment. At the theoretical core, Conformal Risk Control provides the general distribution-free framework for calibrating thresholds on arbitrary risks; TRON concretizes this by defining two risks (for set size and response quality) and calibrating them via split conformal prediction as formalized by Lei. To make prediction sets compact, TRON draws on the adaptive-coverage principle of Romano et al., translating minimal-size prediction sets from multiclass classification (APS) to the generative setting through a novel sampling-based conformal score that avoids token logits. On the language-model side, Conformal Language Modeling is the direct baseline TRON surpasses: while CLM attains guaranteed coverage using internal probabilities and is often restricted to multiple-choice settings, TRON generalizes to black-box MLLMs and open-ended responses. For identifying high-quality outputs, TRON leverages the self-consistency insight of Wang et al., using agreement across independently sampled generations as a nonconformity measure. Finally, SelfCheckGPT\u2019s black-box, sample-agreement approach to hallucination detection motivates TRON\u2019s logit-free design and is elevated from a heuristic into a calibrated, risk-controlled identification stage. Together, these works directly shape TRON\u2019s two-step framework with rigorous guarantees and practical applicability to MLLMs.",
  "analysis_timestamp": "2026-01-06T23:09:26.600413"
}
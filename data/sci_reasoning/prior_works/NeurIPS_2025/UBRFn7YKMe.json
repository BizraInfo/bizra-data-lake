{
  "prior_works": [
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh, Percy Liang",
      "year": 2017,
      "role": "Foundational method for training data attribution",
      "relationship_sentence": "The paper\u2019s core result reframes classical influence functions, showing they emerge as the limit of unrolled differentiation in a stochastic training setting, thereby generalizing Koh & Liang\u2019s attribution tool beyond convexity and explaining its empirical success in deep networks."
    },
    {
      "title": "Estimating Training Data Influence by Tracing Gradient Descent (TracIn)",
      "authors": "Garima Pruthi, Frederick Liu, Satyen Kale, Mukund Sundararajan",
      "year": 2020,
      "role": "Practical influence estimation via training trajectory",
      "relationship_sentence": "By relating influence to accumulations along the optimization trajectory, TracIn operationalized an \u2018unrolled\u2019 perspective that the present work formalizes and extends to a distributional setting, connecting trajectory-based approximations to the influence-function limit."
    },
    {
      "title": "Gradient-based Hyperparameter Optimization through Reversible Learning",
      "authors": "Dougal Maclaurin, David Duvenaud, Ryan P. Adams",
      "year": 2015,
      "role": "Key technique: unrolled differentiation through training",
      "relationship_sentence": "The d-TDA derivation relies conceptually on differentiating through the learning dynamics; this builds on Maclaurin et al.\u2019s unrolled differentiation framework, taking its infinite-step limit to link training dynamics to influence functions."
    },
    {
      "title": "Learning to Reweight Examples for Robust Deep Learning",
      "authors": "Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun",
      "year": 2018,
      "role": "Example weighting via one-step unrolled gradients",
      "relationship_sentence": "This work demonstrated how small, differentiable perturbations to example weights propagate through training via unrolled updates; the present paper generalizes this idea to full-run unrolling and to distributions over random initializations/batches."
    },
    {
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "authors": "Martin Mandt, Matthew D. Hoffman, David M. Blei",
      "year": 2017,
      "role": "Theoretical underpinning for distribution over trained models",
      "relationship_sentence": "By interpreting SGD noise as inducing a stationary distribution over parameters, this work motivates d-TDA\u2019s central object\u2014the distribution of model outputs over training runs\u2014providing the probabilistic lens that the new framework formalizes."
    },
    {
      "title": "Datamodels: Understanding Predictive Models by Mapping Datasets to Predictions",
      "authors": "Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry",
      "year": 2022,
      "role": "Empirical motivation for distributional attribution across seeds",
      "relationship_sentence": "Datamodels map training-set membership to predictions across many randomized trainings; d-TDA formalizes this distributional mapping and connects it to influence functions via an unrolled-differentiation limit."
    },
    {
      "title": "Data Shapley: Equitable Valuation of Data for Machine Learning",
      "authors": "Amirata Ghorbani, James Zou",
      "year": 2019,
      "role": "Data valuation baseline emphasizing expected contributions",
      "relationship_sentence": "Data Shapley frames data value as an expectation over dataset perturbations and algorithmic choices; the present work offers a gradient-based, distribution-aware alternative that ties such expectations to influence via training dynamics."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution is a distributional training data attribution (d-TDA) framework that explicitly models how randomness in initialization and minibatching induces a distribution over model outputs, together with a theoretical result that classical influence functions (IFs) arise as the limit of unrolled differentiation\u2014without restrictive convexity assumptions. Koh and Liang (2017) laid the foundation by introducing IFs for attributing predictions to training points, but their derivation relied on smooth convex settings. Building on the idea of differentiating through optimization, Maclaurin et al. (2015) established unrolled differentiation through full training trajectories, while Ren et al. (2018) demonstrated per-example reweighting via one-step unrolling. Pruthi et al. (2020) operationalized a trajectory-based view of influence (TracIn), accumulating gradient interactions across training, a perspective that this paper unifies by showing such unrolled views converge to IFs in the appropriate limit.\n\nThe distributional focus is motivated and supported by two complementary strands. First, Mandt et al. (2017) interpret SGD as sampling from an approximate posterior, justifying the paper\u2019s central object: the distribution over trained models and their outputs. Second, Ilyas et al. (2022) empirically map datasets to predictions across many randomized trainings (datamodels), revealing the practical salience of attribution over training-run variability that d-TDA formalizes. Finally, data valuation work such as Data Shapley (Ghorbani & Zou, 2019) frames contribution as an expectation over algorithmic randomness; d-TDA provides a scalable, gradient-based alternative that links these expectations to IFs via the dynamics of learning, enabling improved pruning and diagnostic applications in modern deep models.",
  "analysis_timestamp": "2026-01-07T00:27:38.135981"
}
{
  "prior_works": [
    {
      "title": "Causal inference using invariant prediction: identification and confidence intervals",
      "authors": "Jonas Peters et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "IDUM builds on the invariant causal prediction principle that causal mechanisms remain stable across environments, motivating its search for domain-invariant factors that generalize out of distribution."
    },
    {
      "title": "Invariant Risk Minimization",
      "authors": "Martin Arjovsky et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "IDUM operationalizes IRM-style invariance constraints for deep models and extends them by decomposing the invariant representation into necessary and sufficient causal components for uplift decisions."
    },
    {
      "title": "Causality: Models, Reasoning, and Inference",
      "authors": "Judea Pearl",
      "year": 2000,
      "role": "Foundation",
      "relationship_sentence": "IDUM\u2019s refinement into necessary and sufficient factors is grounded in Pearl\u2019s formal definitions of the Probability of Necessity (PN), Probability of Sufficiency (PS), and Probability of Necessity and Sufficiency (PNS)."
    },
    {
      "title": "Probabilities of Causation: Bounds and Identification",
      "authors": "Jin Tian et al.",
      "year": 2000,
      "role": "Extension",
      "relationship_sentence": "IDUM leverages Tian and Pearl\u2019s identification/bounding results for probabilities of causation to estimate and regularize PNS-based masks that distinguish necessary from sufficient drivers in observational marketing data."
    },
    {
      "title": "Estimating individual treatment effect: generalization bounds and algorithms",
      "authors": "Uri Shalit et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "CFR\u2019s distribution-balancing representation for ITE is a principal baseline that IDUM improves upon by replacing in-distribution balancing with invariance for robust OOD uplift assignment."
    },
    {
      "title": "Learning Representations for Counterfactual Inference",
      "authors": "Fredrik D. Johansson et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "This work introduced balance-based representation learning for counterfactuals, whose susceptibility to distribution shift directly motivates IDUM\u2019s invariant causal factor learning."
    },
    {
      "title": "Metalearners for estimating heterogeneous treatment effects using machine learning",
      "authors": "S\u00f6ren R. K\u00fcnzel et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "Metalearners (T/X/R-learners) anchor standard uplift/HTE estimation that IDUM outperforms by adding invariance and PNS-guided factor selection to handle domain shift."
    }
  ],
  "synthesis_narrative": "IDUM\u2019s core idea fuses the invariance principle from causal discovery with Pearl\u2019s probabilities of causation to make uplift modeling robust under distribution shift. The foundational insight from Invariant Causal Prediction (Peters et al., 2016) is that true causal relationships persist across environments; IRM (Arjovsky et al., 2019) translated this idea into an optimization framework that encourages predictors to rely on invariant mechanisms. IDUM extends this line by enforcing invariance in a deep uplift model and, crucially, decomposing the invariant signal into necessary and sufficient causal factors that directly inform incentive assignment. This decomposition is grounded in Pearl\u2019s causality framework (2000), which formally defines PN, PS, and PNS, and further enabled by Tian and Pearl\u2019s identification and bounding results for probabilities of causation, which guide IDUM\u2019s PNS-based masking of features into necessity versus sufficiency roles.\n\nOn the uplift/ITE side, balance-based representation learning (Johansson et al., 2016) and CFR (Shalit et al., 2017) are the main deep baselines that address selection bias via treated\u2013control distribution balancing, but they implicitly assume test-time distributions similar to training. IDUM explicitly targets this gap by switching from balancing to invariance, thereby improving out-of-distribution generalization. Finally, meta-learner frameworks (K\u00fcnzel et al., 2019) provide standard baselines for heterogeneous treatment effect estimation; IDUM surpasses them in shifting environments by combining invariant learning with PNS-guided identification of necessary and sufficient causal drivers for targeted incentives.",
  "analysis_timestamp": "2026-01-06T23:07:19.609074"
}
{
  "prior_works": [
    {
      "title": "Privacy in Context: Technology, Policy, and the Integrity of Social Life",
      "authors": "Helen Nissenbaum",
      "year": 2010,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The benchmark\u2019s scenarios and labeling schema instantiate contextual integrity\u2019s five parameters (subject, sender, recipient, attribute, transmission principle) and its norm-violation criterion, which directly structure CONFAIDE\u2019s tiers."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "arxiv_id": "2012.07805",
      "role": "Gap Identification",
      "relationship_sentence": "This work\u2019s focus on memorization-driven leakage from static training corpora marks the limitation that the present paper explicitly addresses by shifting to interactive, multi-party contextual leakage of user-provided inputs."
    },
    {
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "authors": "Reza Shokri et al.",
      "year": 2017,
      "arxiv_id": "1610.05820",
      "role": "Foundation",
      "relationship_sentence": "By formalizing privacy risk measurement via adversarial querying, this paper provides the methodological backbone that the current work extends from training-data membership to context-appropriate disclosure judgments."
    },
    {
      "title": "Not What You\u2019ve Signed Up For: Compromising LLMs via Prompt Injection",
      "authors": "Jonas Greshake et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Evidence that LLMs can be induced to exfiltrate secrets across contexts directly motivates a principled, norm-grounded benchmark to test whether models withhold sensitive inputs from unintended recipients."
    },
    {
      "title": "Theory of Mind May Have Spontaneously Emerged in Large Language Models",
      "authors": "Michal Kosinski",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Findings that LLMs can track beliefs and perspectives inspire the inclusion of tasks requiring theory-of-mind-style reasoning about who knows what for privacy-appropriate disclosure."
    },
    {
      "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
      "authors": "Tomer D. Ullman",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Documented brittleness of LLMs\u2019 ToM under minimal contextual changes motivates stress-testing privacy reasoning under subtle variations in roles, recipients, and transmission principles."
    },
    {
      "title": "Social Chemistry 101: Learning to Reason about Everyday Morality",
      "authors": "Maarten Sap et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By operationalizing everyday social norms as structured rules with crowdsourced judgments, this work informs the methodology of building a norm-grounded evaluation set, here adapted to privacy norms via contextual integrity."
    }
  ],
  "synthesis_narrative": "Contextual integrity articulates privacy as appropriate information flows governed by five parameters\u2014subject, sender, recipient, attribute, and transmission principle\u2014and evaluates violations against context-specific norms; this theoretical structure, laid out by Nissenbaum, offers a directly operationalizable schema for judging when disclosure is acceptable. Membership inference introduced a concrete, query-based methodology to quantify privacy risk in ML, establishing that model behavior can be probed adversarially to reveal sensitive associations. Subsequent work showed that large language models can emit verbatim training data, crystallizing a memorization-centric framing of privacy leakage. In parallel, prompt-injection studies revealed that LLMs can be manipulated to exfiltrate secrets across role boundaries, highlighting vulnerabilities that arise precisely from contextual shifts rather than mere memorization. On the reasoning side, findings that LLMs may exhibit theory-of-mind capabilities suggested they might track who knows what, while counterevidence showed such capabilities are brittle under minor contextual perturbations. Separately, Social Chemistry 101 demonstrated how to construct norm-grounded evaluation datasets by structuring scenarios and aggregating judgments about appropriateness. Together, these strands expose a gap: existing privacy evaluations fixate on training data leakage and miss interactive, multi-party contexts where normative appropriateness of disclosure is the core question. The current work synthesizes CI\u2019s formal parameters with norm-grounded dataset construction and ToM-informed scenario design to create a tiered benchmark that probes whether instruction-tuned LLMs can withhold sensitive inputs when context deems it inappropriate, directly addressing injection-style exfiltration risks and the brittleness of belief-tracking in nuanced, real-world settings.",
  "target_paper": {
    "title": "Can LLMs Keep a Secret? Testing  Privacy  Implications of Language Models  via Contextual Integrity Theory",
    "authors": "Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Contextual Integrity, Privacy, Theory of Mind",
    "abstract": "Existing efforts on quantifying privacy implications for large language models (LLMs) solely focus on measuring leakage of training data. In this work, we shed light on the often-overlooked interactive settings where an LLM receives information from multiple sources and generates an output to be shared with other entities, creating the potential of exposing sensitive input data in inappropriate contexts. In these scenarios, humans nat- urally uphold privacy by choosing whether or not to disclose information depending on the context. We ask the question \u201cCan LLMs demonstrate an equivalent discernment and reasoning capability when considering privacy in context?\u201d We propose CONFAIDE, a benchmark grounded in the theory of contextual integrity and designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. CONFAIDE consists of four tiers, gradually increasing in complexity, with the final tier evaluating contextual privacy reasoning and theory ",
    "openreview_id": "gmg7t8b4s0",
    "forum_id": "gmg7t8b4s0"
  },
  "analysis_timestamp": "2026-01-06T14:19:21.606210"
}
{
  "prior_works": [
    {
      "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "authors": "Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell",
      "year": 2017,
      "role": "Introduced deep ensembles as a practical and powerful ensemble training paradigm.",
      "relationship_sentence": "The present paper\u2019s central object\u2014the averaged prediction of independently trained networks\u2014directly builds on the deep-ensemble methodology introduced here, showing a new, emergent equivariance property of the ensemble predictor."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Foundational theory linking infinite-width neural networks trained by gradient descent to kernel regression via the NTK.",
      "relationship_sentence": "The proof that ensemble predictions become exactly equivariant in the infinite-width limit relies on NTK linearization and kernel dynamics introduced in this work."
    },
    {
      "title": "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
      "authors": "Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington",
      "year": 2019,
      "role": "Characterized training-time dynamics of wide networks as linear (NTK) evolution, not just end-point solutions.",
      "relationship_sentence": "The paper\u2019s claim that equivariance holds for all training times leverages this result on linear training dynamics in the infinite-width regime."
    },
    {
      "title": "Deep Neural Networks as Gaussian Processes",
      "authors": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein",
      "year": 2018,
      "role": "Established the GP limit of wide randomly initialized networks, a basis for reasoning about averages over network initializations.",
      "relationship_sentence": "Emergent equivariance of the ensemble average is analyzed via expectations over initializations/ensembles, conceptually supported by GP/NTK limits introduced here."
    },
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco S. Cohen, Max Welling",
      "year": 2016,
      "role": "Formalized equivariance to group actions through architecture design (group convolutions).",
      "relationship_sentence": "By contrasting architecture-imposed equivariance with augmentation-and-ensemble\u2013induced equivariance, this work provides the conceptual framework of group actions and equivariance that the new result generalizes beyond specific architectures."
    },
    {
      "title": "Incorporating Invariances in Support Vector Learning",
      "authors": "Bernhard Sch\u00f6lkopf, Peter Simard, Alex J. Smola, Vladimir Vapnik",
      "year": 1996,
      "role": "Classical kernel-method result showing how data augmentation or group averaging induces invariance/equivariance in the learned predictor.",
      "relationship_sentence": "The paper\u2019s kernel-level argument that augmentation projects solutions onto symmetry-respecting subspaces echoes this earlier insight, now extended to NTK-trained deep ensembles and shown to hold off-manifold and throughout training."
    }
  ],
  "synthesis_narrative": "Gerken and Kessel\u2019s key contribution\u2014showing that deep-ensemble predictions become exactly equivariant under data augmentation for all inputs and at all training times in the infinite-width limit\u2014sits at the intersection of ensemble methodology, symmetry-aware learning, and NTK/GP theory. The deep-ensemble framework of Lakshminarayanan et al. provides the operational mechanism: averaging the outputs of independently trained networks. Jacot et al.\u2019s neural tangent kernel establishes that wide networks trained by gradient descent behave as kernel machines, enabling precise analysis of augmentation effects at the function level. Lee et al. further extend this to training dynamics, which the present work leverages to argue that equivariance is not only a property of the terminal solution but holds across the full training trajectory. The GP perspective of Lee et al. (2018) supports taking expectations over initializations/ensembles, clarifying how the ensemble predictor becomes the object of analysis rather than any single network. Conceptually, Cohen and Welling\u2019s group-equivariant CNNs define the desired symmetry property, but the present paper shows it can emerge without specialized architectures\u2014through augmentation and ensembling alone. Finally, classical kernel-method results by Sch\u00f6lkopf et al. demonstrate that augmentation acts as a group-averaging operator projecting predictors onto symmetry-respecting subspaces. Gerken and Kessel fuse these strands to prove that, in the NTK regime, the ensemble-averaged predictor trained with augmentation is the group-averaged (hence equivariant) solution globally in input space, explaining emergent equivariance even when individual ensemble members are not.",
  "analysis_timestamp": "2026-01-06T23:42:48.059464"
}
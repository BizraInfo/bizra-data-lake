{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Linearization baseline for early-time training dynamics (lazy/NTK regime).",
      "relationship_sentence": "The paper\u2019s first training phase is characterized by near-linear dynamics that mirror NTK behavior; this work provides the canonical continuous-time gradient-flow description the authors build upon and later show the dynamics depart from."
    },
    {
      "title": "A Note on Lazy Training in Supervised Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat, Francis Bach",
      "year": 2019,
      "role": "Formalization of the lazy (NTK) regime and conditions for (non-)feature learning.",
      "relationship_sentence": "The multi-phase analysis explicitly leverages the lazy-training criterion to demarcate when a two-layer ReLU network behaves linearly and when it must exit the lazy regime, anchoring the early-to-nonlinear phase transition."
    },
    {
      "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
      "authors": "Simon S. Du, Jason D. Lee, Yuanzhi Li, Liwei Wang, Xiyu Zhai",
      "year": 2019,
      "role": "Global convergence via NTK-style over-parameterization for two-layer networks.",
      "relationship_sentence": "Provides the prevailing linearized-global-convergence template that this paper contrasts with, by tracing how training leaves the NTK regime and enters richer nonlinear phases even on separable data."
    },
    {
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "authors": "Song Mei, Andrea Montanari, Phan-Minh Nguyen",
      "year": 2018,
      "role": "Mean-field/measure dynamics for two-layer networks under gradient flow.",
      "relationship_sentence": "Inspires the paper\u2019s whole-trajectory viewpoint and tools for tracking neuron distributions/sign patterns; the new work specializes these ideas to linearly separable data to obtain an exact, phase-wise characterization."
    },
    {
      "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport",
      "authors": "L\u00e9na\u00efc Chizat, Francis Bach",
      "year": 2018,
      "role": "Gradient-flow on probability measures (optimal-transport framework) for neural training dynamics.",
      "relationship_sentence": "Provides the gradient-flow/mass-transport lens the authors adopt to reason about global dynamics beyond linearization, enabling precise control of nonlinear behaviors across phases."
    },
    {
      "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
      "authors": "Kaifeng Lyu, Jian Li",
      "year": 2019,
      "role": "Implicit bias/end-of-training characterization for homogeneous (e.g., ReLU) networks.",
      "relationship_sentence": "The final phase of the analysis aligns with margin maximization in homogeneous models; the paper integrates this end-stage implicit bias into a full-process, multi-phase picture from random initialization."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli",
      "year": 2014,
      "role": "Multi-timescale, phase-like learning dynamics (in linear networks) methodology.",
      "relationship_sentence": "Motivates the notion of distinct learning phases and timescales; the present work extends this spirit from deep linear to nonlinear ReLU networks, exhibiting and proving multi-phase dynamics under gradient flow."
    }
  ],
  "synthesis_narrative": "The paper achieves a rare, end-to-end theoretical account of gradient-flow training for two-layer ReLU networks on linearly separable data by weaving together and extending three major lines of prior theory. First, NTK-based analyses (Jacot et al.) and their formalization of the lazy regime (Chizat & Bach, 2019), alongside over-parameterized convergence results (Du et al.), establish the linearized early-phase template. The authors adopt this as the starting point and then precisely identify when and how training leaves the lazy regime, a boundary anticipated by the lazy-training conditions but not previously traced in full. Second, mean-field and optimal-transport perspectives (Mei\u2013Montanari\u2013Nguyen; Chizat & Bach, 2018) offer global gradient-flow tools to track evolving feature representations. Building on these, the paper specializes to separable finite datasets and develops a fine-grained, phase-wise description\u2014capturing neuron sign-pattern changes and other nonlinear effects that the mean-field literature typically treats at a distributional level. Third, implicit-bias results for homogeneous networks (Lyu & Li) characterize the end-of-training limit as margin maximization; the authors embed this as the terminal phase in a unified trajectory from random initialization to convergence. Methodologically and conceptually, the work also echoes multi-timescale, phase-like dynamics ideas from deep linear networks (Saxe et al.), now demonstrated and rigorously proved in a nonlinear ReLU setting. The result is a coherent four-phase narrative that bridges early NTK-like behavior, intermediate nonlinear feature evolution, and final max-margin convergence.",
  "analysis_timestamp": "2026-01-07T00:02:04.824273"
}
{
  "prior_works": [
    {
      "title": "COMPLETER: Incomplete Multi-View Clustering via Contrastive Prediction",
      "authors": "M. Lin et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "AIRMVC directly builds on the multi-view contrastive paradigm popularized by COMPLETER, but replaces its clean/missing-view assumption with explicit noisy-view identification and rectification, yielding a robustness-focused contrastive objective."
    },
    {
      "title": "Robust Multi-view Spectral Clustering via Low-rank and Sparse Decomposition",
      "authors": "T. Xia et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "This classic robust multi-view clustering work established the importance of explicitly modeling view corruption/outliers, a principle AIRMVC inherits while moving to deep contrastive representations and sample-wise noise handling."
    },
    {
      "title": "Auto-weighted Multi-view Clustering",
      "authors": "F. Nie et al.",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "The idea of automatically down-weighting unreliable views in Nie et al. motivates AIRMVC\u2019s hybrid rectification strategy, which adapts weighting/mitigation at the instance\u2013view level once noisy samples are identified."
    },
    {
      "title": "Debiased Contrastive Learning",
      "authors": "C.-Y. Chuang et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "AIRMVC\u2019s noise-robust contrastive mechanism extends debiasing ideas from this work to the multi-view setting, reducing the impact of corrupted positives/negatives induced by noisy views."
    },
    {
      "title": "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere",
      "authors": "T. Wang et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "AIRMVC\u2019s theoretical analysis that its contrastive representations can discard noisy information builds on the alignment\u2013uniformity framework introduced by Wang and Isola."
    },
    {
      "title": "Gaussian Mixture Models",
      "authors": "D. A. Reynolds",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "AIRMVC\u2019s core step of reformulating noise identification as anomaly detection is instantiated via a GMM on learned embeddings, directly leveraging the GMM likelihood-based separation of inliers and outliers."
    },
    {
      "title": "Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels",
      "authors": "B. Han et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "The detect-then-rectify training philosophy in Co-teaching informs AIRMVC\u2019s pipeline: first identify corrupted instances, then mitigate their influence during representation learning."
    }
  ],
  "synthesis_narrative": "AIRMVC sits at the intersection of robust multi-view learning and contrastive representation learning. Early robust multi-view clustering, epitomized by RMSC, established that real-world views are frequently corrupted and that explicitly modeling corruption (e.g., via low-rank/sparse structures) yields resilience. Auto-weighted Multi-view Clustering further emphasized adaptively reducing the influence of unreliable views, seeding AIRMVC\u2019s idea of instance\u2013view-level mitigation once noise is detected. The deep era ushered in contrastive multi-view clustering, with COMPLETER showing that contrastive prediction across views provides strong representations but largely under clean or missing-view assumptions. AIRMVC targets the unaddressed gap of pervasive view noise by (1) explicitly identifying corrupted samples through anomaly detection with a Gaussian Mixture Model on latent embeddings and (2) rectifying their influence via a hybrid strategy that generalizes adaptive weighting to the deep, instance\u2013view granularity. To make contrastive learning itself robust to corrupted positives/negatives, AIRMVC extends debiasing principles from Debiased Contrastive Learning to the multi-view setting. Its theoretical guarantee that representations can shed noise leverages the alignment\u2013uniformity framework, formalizing why the proposed objective suppresses noisy information. The broader detect-and-rectify training philosophy, reminiscent of Co-teaching\u2019s success under label noise, shapes AIRMVC\u2019s overall training pipeline. Together, these works directly scaffold AIRMVC\u2019s key innovations: GMM-based noise identification, hybrid rectification, and a noise-robust contrastive mechanism with theoretical support.",
  "analysis_timestamp": "2026-01-06T23:07:19.617483"
}
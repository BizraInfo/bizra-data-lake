{
  "prior_works": [
    {
      "title": "On the Exponential Value of Labeled Samples",
      "authors": "Vittorio Castelli, Thomas M. Cover",
      "year": 1995,
      "role": "Foundational value-of-information analysis for labeled vs. unlabeled data in mixture models",
      "relationship_sentence": "This paper established an information-theoretic framework comparing labeled and unlabeled samples in mixture settings, directly motivating the present work\u2019s focus on sample-size-dependent lower bounds for SSL in Gaussian mixtures."
    },
    {
      "title": "The Relative Value of Labeled and Unlabeled Samples in Pattern Recognition With an Unknown Mixing Parameter",
      "authors": "Vittorio Castelli, Thomas M. Cover",
      "year": 1996,
      "role": "Mixture-model analysis quantifying how unlabeled data interacts with unknown mixture parameters",
      "relationship_sentence": "By quantifying the benefit of unlabeled data when the mixing proportion is unknown, this work foreshadows the current paper\u2019s explicit dependence on both labeled/unlabeled sample sizes and mixture signal strength in lower bounds."
    },
    {
      "title": "On the Power of Unlabeled Examples in Learning",
      "authors": "Shai Ben-David, Tyler Lu, D\u00e1vid P\u00e1l",
      "year": 2008,
      "role": "Distribution-free (agnostic) impossibility and conditional-possibility results for SSL",
      "relationship_sentence": "The new lower bound sharpens these worst-case SSL impossibility results by giving a tight, distribution-specific (2-Gaussian) minimax characterization showing SSL cannot beat the best of SL or UL in this parametric setting."
    },
    {
      "title": "A Spectral Algorithm for Learning Mixture of Gaussians",
      "authors": "Santosh Vempala, Grant Wang",
      "year": 2004,
      "role": "Unsupervised recovery of GMMs under separation assumptions",
      "relationship_sentence": "As a canonical result where unlabeled data alone suffices (under separation), this work serves as the UL-success baseline that the paper contrasts when asking whether SSL can simultaneously surpass both SL and UL."
    },
    {
      "title": "Tensor Decompositions for Learning Latent Variable Models",
      "authors": "Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, Matus Telgarsky",
      "year": 2014,
      "role": "Provable UL methods for mixtures at adequate signal-to-noise via method-of-moments/tensors",
      "relationship_sentence": "These guarantees exemplify regimes where UL is already strong, motivating the paper\u2019s central question and its lower bound showing SSL cannot improve beyond UL or SL rates in 2-GMMs across SNR and sample regimes."
    },
    {
      "title": "Assouad, Fano, and Le Cam",
      "authors": "Bin Yu",
      "year": 1997,
      "role": "Information-theoretic tools for tight minimax lower bounds",
      "relationship_sentence": "The paper\u2019s core technical contribution\u2014deriving tight minimax lower bounds over labeled/unlabeled sizes and SNR\u2014builds directly on this toolkit (Fano/Le Cam/Assouad) to construct hard instances and quantify error exponents."
    },
    {
      "title": "Learning Mixtures of Gaussians",
      "authors": "Sanjoy Dasgupta",
      "year": 1999,
      "role": "Early theoretical analysis of identifiability and sample complexity for GMMs from unlabeled data",
      "relationship_sentence": "By characterizing when UL can identify mixture components, this work underpins the paper\u2019s framing of UL-optimal regimes and the need for a lower bound that jointly accounts for labeled and unlabeled data and SNR."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central advance\u2014a tight minimax lower bound for semi-supervised learning on two-component Gaussian mixtures that depends jointly on labeled data, unlabeled data, and signal-to-noise ratio\u2014sits at the intersection of three strands of prior work. First, Castelli and Cover (1995, 1996) developed a value-of-information lens for mixture models, precisely comparing labeled and unlabeled data and highlighting how mixture parameters govern their utility. This perspective directly informs the present work\u2019s decision to make lower bounds explicitly depend on both sample sources and the mixture\u2019s SNR. Second, a line of results on provable unsupervised learning of GMMs\u2014e.g., Dasgupta (1999), Vempala and Wang (2004), and Anandkumar et al. (2014)\u2014established that, under sufficient separation/SNR, unlabeled data alone can learn mixture components and hence a near-optimal decision boundary. These works define the UL \u201csuccess\u201d regimes that the current paper uses as a foil when asking whether SSL can surpass both SL and UL simultaneously. Third, distribution-free SSL theory (Ben-David, Lu, and P\u00e1l, 2008) provided impossibility baselines absent structural assumptions, while information-theoretic lower bound techniques (Yu, 1997) supplied the methodological toolkit. Integrating these influences, the paper builds a parametric, instance-specific lower bound that bridges the gap between worst-case impossibility and high-SNR UL success, proving that for 2-Gaussians no SSL method can beat the minimax-optimal error rates of either SL or UL across regimes\u2014thereby reframing when and why unlabeled data should be expected to help.",
  "analysis_timestamp": "2026-01-07T00:02:04.776116"
}
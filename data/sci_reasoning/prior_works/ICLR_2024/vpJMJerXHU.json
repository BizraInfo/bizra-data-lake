{
  "prior_works": [
    {
      "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
      "authors": "Shaojie Bai et al.",
      "year": 2018,
      "arxiv_id": "1803.01271",
      "role": "Extension",
      "relationship_sentence": "ModernTCN directly upgrades the TCN\u2019s causal, dilated residual temporal convolutions by replacing its plain conv blocks with modern large\u2011kernel depthwise separable designs and refined normalization to improve long\u2011range modeling and efficiency."
    },
    {
      "title": "WaveNet: A Generative Model for Raw Audio",
      "authors": "Aaron van den Oord et al.",
      "year": 2016,
      "arxiv_id": "1609.03499",
      "role": "Foundation",
      "relationship_sentence": "The core idea of stacking causal dilated convolutions and gated temporal blocks to capture long dependencies originates from WaveNet, which is adapted and generalized in ModernTCN\u2019s convolutional sequence modeling backbone."
    },
    {
      "title": "A ConvNet for the 2020s",
      "authors": "Zhuang Liu et al.",
      "year": 2022,
      "arxiv_id": "2201.03545",
      "role": "Inspiration",
      "relationship_sentence": "ConvNeXt\u2019s design rules\u2014depthwise separable large\u2011kernel convolutions, inverted bottlenecks, and simplified activation/normalization\u2014are transplanted to 1D to modernize TCN blocks in ModernTCN."
    },
    {
      "title": "Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs",
      "authors": "Xiangyu Ding et al.",
      "year": 2022,
      "arxiv_id": "2203.06717",
      "role": "Inspiration",
      "relationship_sentence": "Evidence that very large kernels can approximate global receptive fields efficiently motivates ModernTCN\u2019s use of large 1D temporal kernels to capture long\u2011range dependencies without attention."
    },
    {
      "title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers",
      "authors": "Yue Nie et al.",
      "year": 2023,
      "arxiv_id": "2211.14730",
      "role": "Baseline",
      "relationship_sentence": "PatchTST\u2019s channel\u2011independent patching and decoupling of temporal vs. channel mixing form a primary SOTA baseline and inspire ModernTCN\u2019s depthwise temporal filtering plus pointwise channel mixing strategy."
    },
    {
      "title": "TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis",
      "authors": "Haixu Wu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "TimesNet established a strong general\u2011purpose time\u2011series benchmark across tasks, providing the key CNN/MLP baseline that ModernTCN aims to surpass while retaining convolutional efficiency."
    },
    {
      "title": "Are Transformers Effective for Time Series Forecasting?",
      "authors": "Ailing Zeng et al.",
      "year": 2023,
      "arxiv_id": "2205.13504",
      "role": "Gap Identification",
      "relationship_sentence": "By showing simple per\u2011channel linear temporal filters (DLinear) can outperform heavy Transformers, this work exposes the over\u2011complexity gap that ModernTCN addresses with lightweight, pure convolutional temporal operators."
    }
  ],
  "synthesis_narrative": "Causal, dilated temporal convolution for sequence modeling was crystallized by TCN, which demonstrated that deep residual stacks of dilated 1D convolutions can rival recurrent models on long sequences, albeit with plain small\u2011kernel blocks and standard activations. This builds on WaveNet\u2019s insight that stacked causal dilations and gating efficiently capture long\u2011range dependencies without recurrence. In parallel, ConvNeXt codified a set of modern CNN design principles\u2014depthwise separable large\u2011kernel convolutions, inverted bottlenecks, and streamlined normalization/activation\u2014that lift convolutional capacity while preserving efficiency. Large\u2011kernel studies such as RepLKNet showed that very wide kernels can approximate global context competitively with attention, suggesting a path to long\u2011range modeling via purely convolutional receptive fields. On the time\u2011series front, PatchTST revealed that channel\u2011independent temporal processing and patching are crucial for strong forecasting, advocating a separation between temporal filtering and channel mixing. TimesNet established a general\u2011purpose benchmark across multiple time\u2011series tasks using convolutional priors, setting the bar for efficiency and versatility. Meanwhile, DLinear exposed that simple per\u2011channel temporal filters can outperform heavy Transformers, highlighting the potential of lightweight operators. Together these works suggest that a TCN\u2011style causal, dilated backbone, if refactored with modern depthwise, large\u2011kernel design and explicit decoupling of temporal and channel mixing, could reclaim state of the art across diverse time\u2011series tasks. ModernTCN synthesizes these insights by retrofitting TCN with ConvNeXt/RepLKNet\u2011inspired large\u2011kernel depthwise temporal convolutions and pointwise channel mixing, aligning with PatchTST\u2019s channel independence and DLinear\u2019s simplicity to achieve strong generalization and efficiency.",
  "target_paper": {
    "title": "ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis",
    "authors": "Luo donghao, wang xue",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Time Series Analysis, Deep Learning",
    "abstract": "Recently, Transformer-based and MLP-based models have emerged rapidly and\nwon dominance in time series analysis. In contrast, convolution is losing steam\nin time series tasks nowadays for inferior performance. This paper studies the\nopen question of how to better use convolution in time series analysis and makes\nefforts to bring convolution back to the arena of time series analysis. To this end,\nwe modernize the traditional TCN and conduct time series related modifications\nto make it more suitable for time series tasks. As the outcome, we propose\nModernTCN and successfully solve this open question through a seldom-explored\nway in time series community. As a pure convolution structure, ModernTCN still\nachieves the consistent state-of-the-art performance on five mainstream time series\nanalysis tasks while maintaining the efficiency advantage of convolution-based\nmodels, therefore providing a better balance of efficiency and performance than\nstate-of-the-art Transformer-based and MLP-base",
    "openreview_id": "vpJMJerXHU",
    "forum_id": "vpJMJerXHU"
  },
  "analysis_timestamp": "2026-01-06T12:10:12.356087"
}
{
  "prior_works": [
    {
      "title": "Distilling the Knowledge in a Neural Network",
      "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean",
      "year": 2015,
      "role": "Teacher\u2013student distillation framework for transferring knowledge via soft labels.",
      "relationship_sentence": "Weak-to-Strong generalization empirically extends the distillation paradigm to the regime where the teacher is weaker than the student, asking when a stronger model trained on weak labels can nonetheless exceed its supervisor."
    },
    {
      "title": "Born Again Neural Networks",
      "authors": "Luca Furlanello, Zachary C. Lipton, Michael Tschannen, Laurent Itti, Anima Anandkumar",
      "year": 2018,
      "role": "Shows students trained on a teacher\u2019s predictions can outperform the teacher.",
      "relationship_sentence": "This paper provides a direct empirical precedent that supervision signals need not cap student performance, a core premise underlying the weak-to-strong phenomenon documented in the ICML 2024 work."
    },
    {
      "title": "Self-training with Noisy Student improves ImageNet classification",
      "authors": "Qizhe Xie, Minh-Thang Luong, Ed Hovy, Quoc V. Le",
      "year": 2020,
      "role": "Semi-supervised pseudo-labeling where a larger student trained on noisy teacher labels surpasses the teacher.",
      "relationship_sentence": "Weak-to-Strong generalization mirrors Noisy Student\u2019s finding that model-generated (imperfect) labels can elicit stronger capabilities, but evaluates this dynamic in large language models and across tasks like NLP, chess, and reward modeling."
    },
    {
      "title": "Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei",
      "year": 2017,
      "role": "Foundational RLHF framework using human preference labels to align policies.",
      "relationship_sentence": "The ICML paper is motivated by RLHF\u2019s reliance on human supervision, and studies whether model-provided weak supervision can substitute when human evaluators are too weak to assess superhuman models."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, et al.",
      "year": 2022,
      "role": "Replaces human feedback with AI feedback guided by a constitution (RLAIF).",
      "relationship_sentence": "By showing that AI feedback can successfully supervise models, this work directly motivates the weak-supervision setting explored, where labels from a weaker model are used to train a stronger one."
    },
    {
      "title": "AI Safety via Debate",
      "authors": "Geoffrey Irving, Paul Christiano, Dario Amodei",
      "year": 2018,
      "role": "Scalable oversight proposal enabling weaker judges to assess stronger agents via structured debate.",
      "relationship_sentence": "The weak-to-strong study serves as an empirical testbed for the core question raised by Debate\u2014whether weaker oversight can elicit strong capabilities\u2014absent special mechanisms like debate."
    },
    {
      "title": "Snorkel: Rapid Training Data Creation with Weak Supervision",
      "authors": "Alex Ratner, Stephen H. Bach, Paroma Varma, Christopher R\u00e9",
      "year": 2017,
      "role": "Framework for training models from noisy, programmatically generated labels.",
      "relationship_sentence": "Snorkel establishes that aggregating weak labels can yield strong models, a foundational weak-supervision insight that the ICML paper probes at scale with pretrained LMs and model-generated labels."
    }
  ],
  "synthesis_narrative": "The ICML 2024 paper\u2019s central contribution\u2014demonstrating weak-to-strong generalization, where a strong model finetuned on labels from a weaker model can outperform its supervisor\u2014sits at the intersection of distillation/self-training and scalable oversight for alignment. Classic knowledge distillation (Hinton et al., 2015) and Born-Again Networks (Furlanello et al., 2018) established that student models trained on teacher predictions can match or exceed the teacher, while Noisy Student (Xie et al., 2020) showed that pseudo-labels from a weaker or comparable model, combined with noise and a larger student, can yield superior performance. These works provide the methodological foundation that imperfect, model-generated labels can still unlock stronger capabilities.\nOn the alignment side, RLHF (Christiano et al., 2017) highlighted the dependence on human supervision, provoking the question of how to train superhuman systems when humans cannot reliably evaluate them. Constitutional AI (Bai et al., 2022) advanced this by replacing human feedback with AI feedback (RLAIF), demonstrating practical viability of model-provided supervision. Conceptually, AI Safety via Debate (Irving et al., 2018) proposed mechanisms for weaker overseers to supervise stronger agents, motivating empirical probes of the weak-overseer regime. Finally, Snorkel (Ratner et al., 2017) offered a general weak-supervision lens, showing that aggregating noisy labels can train high-quality models. Building on these strands, the ICML paper systematically examines when na\u00efve finetuning on weak model labels can elicit strong capabilities across domains, quantifies the gap to the strong model\u2019s full potential, and thereby grounds scalable oversight research in concrete empirical phenomena.",
  "analysis_timestamp": "2026-01-07T00:02:04.891505"
}
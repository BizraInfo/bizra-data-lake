{
  "prior_works": [
    {
      "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling",
      "authors": "Carlos Riquelme et al.",
      "year": 2018,
      "arxiv_id": "1802.09127",
      "role": "Foundation",
      "relationship_sentence": "VBLL builds directly on the neural-linear/Bayesian last-layer formulation introduced by Riquelme et al., generalizing Bayesian linear heads on learned features to a deterministic variational objective that handles both regression and classification with quadratic cost in head width."
    },
    {
      "title": "Laplace Redux: Effortless Bayesian Deep Learning",
      "authors": "Jakob D. Daxberger et al.",
      "year": 2021,
      "arxiv_id": "2106.14806",
      "role": "Baseline",
      "relationship_sentence": "Laplace Redux established last-layer Laplace (LLLA) as a strong, cheap Bayesian head baseline using local Hessians around the MAP, which VBLL targets and improves upon by replacing post-hoc second-order approximations with a principled variational posterior and closed-form loss without sampling."
    },
    {
      "title": "Weight Uncertainty in Neural Networks",
      "authors": "Charles Blundell et al.",
      "year": 2015,
      "arxiv_id": "1505.05424",
      "role": "Gap Identification",
      "relationship_sentence": "Bayes by Backprop popularized Monte Carlo\u2013based variational inference in BNNs, whose gradient variance and multiple-sample costs motivate VBLL\u2019s sampling-free, deterministic treatment of the last layer via analytic expectations."
    },
    {
      "title": "Variational Dropout and the Local Reparameterization Trick",
      "authors": "Diederik P. Kingma et al.",
      "year": 2015,
      "arxiv_id": "1506.02557",
      "role": "Gap Identification",
      "relationship_sentence": "The local reparameterization trick reduces but does not eliminate Monte Carlo variance in stochastic VI, directly motivating VBLL\u2019s further step of removing sampling entirely for the last layer by exploiting Gaussian identities and variational bounds."
    },
    {
      "title": "A variational approach to Bayesian logistic regression",
      "authors": "Tommi S. Jaakkola et al.",
      "year": 1997,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "VBLL directly uses the Jaakkola\u2013Jordan quadratic variational bound on the logistic sigmoid to obtain a deterministic, conjugate-form objective for binary classification under a Gaussian last-layer posterior."
    },
    {
      "title": "Efficient bounds for the softmax function, with applications to approximate inference in CRFs",
      "authors": "Guillaume Bouchard",
      "year": 2007,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "For multiclass classification, VBLL leverages Bouchard\u2019s convex quadratic bound on the softmax log-partition to compute closed-form expectations under a Gaussian last-layer, yielding a single-pass variational loss."
    },
    {
      "title": "Conjugate-Computation Variational Inference: Converting Inference in Non-Conjugate Models to Inference in Conjugate Models",
      "authors": "Mohammad Emtiyaz Khan et al.",
      "year": 2017,
      "arxiv_id": "1703.04265",
      "role": "Inspiration",
      "relationship_sentence": "CCVI showed how auxiliary variational bounds can recover conjugacy and enable collapsed, deterministic updates in nonconjugate models; VBLL applies this principle to collapse the last layer and, when paired with feature-layer VI, yields a lower-variance collapsed VI for BNNs."
    }
  ],
  "synthesis_narrative": "Neural-linear methods established a practical blueprint for uncertainty in deep networks by placing a Bayesian linear model on top of learned features, demonstrating that a probabilistic last layer can deliver strong exploration and calibrated predictions with minimal overhead. Laplace-based approaches later showed that last-layer posteriors could be approximated extremely cheaply by fitting a Gaussian around the MAP using local curvature, making last-layer Bayesianization a de facto baseline for scalable uncertainty. Variational Bayes for neural networks, popularized via reparameterized Monte Carlo estimators, exposed a key limitation of sampling-based training: gradient variance and multi-sample cost. Local reparameterization mitigated this variance by moving randomness to activations but still required stochastic sampling. Classic variational bounds for logistic/softmax likelihoods\u2014Jaakkola\u2013Jordan\u2019s quadratic bound for the sigmoid and Bouchard\u2019s convex bound for the softmax\u2014provide deterministic quadratic surrogates that render nonconjugate classification nearly conjugate under Gaussian posteriors. Conjugate-Computation VI generalized this idea, showing how auxiliary bounds can convert nonconjugate models into conjugate ones to enable collapsed, deterministic updates. Together, these works reveal a path: retain the neural-linear structure for scalability, replace post-hoc Laplace and stochastic VI with analytic expectations enabled by convex quadratic bounds, and exploit conjugacy to collapse nuisance variables. The natural next step is a sampling-free, single-pass variational objective for the Bayesian last layer that handles both regression and multiclass classification with quadratic cost, and that can be combined with variational feature learning to produce a lower-variance, collapsed VI procedure for full Bayesian neural networks.",
  "target_paper": {
    "title": "Variational Bayesian Last Layers",
    "authors": "James Harrison, John Willes, Jasper Snoek",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "bayesian deep learning, variational methods, bayesian last layers, neural linear models",
    "abstract": "We introduce a deterministic variational formulation for training Bayesian last layer neural networks. This yields a sampling-free, single-pass model and loss that effectively improves uncertainty estimation. Our variational Bayesian last layer (VBLL) can be trained and evaluated with only quadratic complexity in last layer width, and is thus (nearly) computationally free to add to standard architectures. We experimentally investigate VBLLs, and show that they improve predictive accuracy, calibration, and out of distribution detection over baselines across both regression and classification. Finally, we investigate combining VBLL layers with variational Bayesian feature learning, yielding a lower variance collapsed variational inference method for Bayesian neural networks.",
    "openreview_id": "Sx7BIiPzys",
    "forum_id": "Sx7BIiPzys"
  },
  "analysis_timestamp": "2026-01-06T11:08:29.106776"
}
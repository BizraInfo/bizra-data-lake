{
  "prior_works": [
    {
      "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning",
      "authors": "Sunehag et al.",
      "year": 2018,
      "arxiv_id": "1706.05296",
      "role": "Foundation",
      "relationship_sentence": "Introduced the value-decomposition/IGM framework for centralized training with decentralized execution that this paper retains while rethinking the joint action selection mechanism via greedy sequential execution."
    },
    {
      "title": "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning",
      "authors": "Rashid et al.",
      "year": 2018,
      "arxiv_id": "1803.11485",
      "role": "Baseline",
      "relationship_sentence": "Serves as the principal value-decomposition baseline whose monotonic mixing and simultaneous greedy execution work well on homogeneous tasks but encourage near-identical agent policies that this paper explicitly aims to overcome."
    },
    {
      "title": "QPLEX: Duplex Dueling Multi-Agent Q-Learning",
      "authors": "Wang et al.",
      "year": 2021,
      "arxiv_id": "2008.01062",
      "role": "Extension",
      "relationship_sentence": "Demonstrates that more expressive value factorisation (duplex dueling) can improve credit assignment, which this paper preserves while changing the decision process to agent-wise greedy sequential execution to better handle heterogeneity."
    },
    {
      "title": "ROMA: Multi-Agent Reinforcement Learning with Emergent Roles",
      "authors": "Wang et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Shows that role/personalization mechanisms suit heterogeneous cooperation but degrade performance in homogeneous settings, directly motivating the search for a unified approach that avoids this trade-off."
    },
    {
      "title": "RODE: Learning Roles to Decompose Multi-Agent Tasks",
      "authors": "Wang et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Reinforces that role-based policy aggregation aids heterogeneous tasks yet compromises homogeneous performance, a limitation the proposed greedy sequential execution is designed to address."
    },
    {
      "title": "Action Branching Architectures for Deep Reinforcement Learning",
      "authors": "Tavakoli et al.",
      "year": 2018,
      "arxiv_id": "1711.08946",
      "role": "Inspiration",
      "relationship_sentence": "Provides the key insight that greedy per-dimension selection over a factored Q can make large discrete decisions tractable, which this paper adapts to multi-agent settings by selecting agents\u2019 actions sequentially and greedily."
    },
    {
      "title": "The StarCraft Multi-Agent Challenge",
      "authors": "Samvelyan et al.",
      "year": 2019,
      "arxiv_id": "1902.04043",
      "role": "Foundation",
      "relationship_sentence": "Defines the heterogeneous cooperative benchmark and problem protocol that the paper explicitly targets to demonstrate unified performance across homogeneous and heterogeneous tasks."
    }
  ],
  "synthesis_narrative": "Value-Decomposition Networks (VDN) established the core centralized-training/decentralized-execution paradigm by factorizing a joint action-value into agent-wise utilities under the IGM principle, enabling scalable cooperative learning. QMIX advanced this with a monotonic mixing network so that the joint argmax aligned with individual argmaxes, a design that excelled on homogeneous coordination but tended to homogenize agent policies. QPLEX increased factorisation expressiveness via duplex dueling advantage decomposition, improving credit assignment while still relying on simultaneous action selection. In parallel, ROMA showed that emergent roles and personalized observations can induce specialization suited for heterogeneous teams, and RODE further decomposed tasks through learned roles, but both lines revealed a consistent trade-off: role-induced diversity often undermined performance on homogeneous tasks. Outside MARL, the Branching Dueling Q-network demonstrated that greedy, per-dimension action selection over a factored Q can scale to high-dimensional discrete action spaces, highlighting the power of sequential greedy choice in structured decisions. The StarCraft Multi-Agent Challenge provided the canonical heterogeneous cooperative testbed shaping evaluation protocols.\nTaken together, these works left a clear opportunity: maintain the strengths of value factorisation and credit assignment while avoiding the homogeneity bias of simultaneous argmax and the homogeneity\u2013heterogeneity trade-off of role aggregation. The paper synthesizes these insights by preserving factorised value learning yet replacing simultaneous selection with agent-wise greedy sequential execution, leveraging the branching-style greedy principle to induce flexible, specialized behaviors without sacrificing coordinated performance on homogeneous tasks, and validating the unified capability on SMAC and other cooperative settings.",
  "target_paper": {
    "title": "Solving Homogeneous and Heterogeneous Cooperative Tasks with Greedy Sequential Execution",
    "authors": "Shanqi Liu, Dong Xing, Pengjie Gu, Xinrun Wang, Bo An, Yong Liu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Multi-Agent Cooperation, Credit Assignment, Homogeneous and Heterogeneous Cooperative Tasks",
    "abstract": "Cooperative multi-agent reinforcement learning (MARL) is extensively used for solving complex cooperative tasks, and value decomposition methods are a prevalent approach for this domain. However, these methods have not been successful in addressing both homogeneous and heterogeneous tasks simultaneously which is a crucial aspect for the practical application of cooperative agents. \nOn one hand, value decomposition methods demonstrate superior performance in homogeneous tasks. Nevertheless, they tend to produce agents with similar policies, which is unsuitable for heterogeneous tasks. On the other hand, solutions based on personalized observation or assigned roles are well-suited for heterogeneous tasks. However, they often lead to a trade-off situation where the agent's performance in homogeneous scenarios is negatively affected due to the aggregation of distinct policies. An alternative approach is to adopt sequential execution policies, which offer a flexible form for learning both t",
    "openreview_id": "hB2hXtxIPH",
    "forum_id": "hB2hXtxIPH"
  },
  "analysis_timestamp": "2026-01-06T08:27:12.250796"
}
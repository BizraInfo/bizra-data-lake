{
  "prior_works": [
    {
      "title": "Hyena Hierarchy: Towards larger context lengths in language models",
      "authors": "Poli et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "Geometric Hyena directly extends Hyena\u2019s long-convolution operator to 3D geometric data by designing equivariant filters, preserving sub-quadratic global context while enforcing E(3) symmetry."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "authors": "Gu et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "The paper builds on the state-space/long-convolution paradigm introduced by S4 to realize scalable global context, providing the theoretical and algorithmic basis for Hyena-style implicit long-range kernels."
    },
    {
      "title": "Tensor Field Networks: Rotation- and Translation-Equivariant Neural Networks for 3D Point Clouds",
      "authors": "Thomas et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Geometric Hyena adopts the TFN-style irreducible representation/tensor-product framework to guarantee SE(3) equivariance when constructing long-convolutional filters over geometric features."
    },
    {
      "title": "SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks",
      "authors": "Fuchs et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "This is the primary equivariant self-attention baseline whose quadratic complexity Geometric Hyena replaces with sub-quadratic equivariant long-convolutions while maintaining global geometric reasoning."
    },
    {
      "title": "EGNN: E(n) Equivariant Graph Neural Networks",
      "authors": "Satorras et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "EGNN\u2019s local, distance-based message passing motivates Geometric Hyena\u2019s design by highlighting the loss of global context in scalable equivariant models that avoid attention."
    },
    {
      "title": "E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
      "authors": "Batzner et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "NequIP demonstrates powerful but local equivariant models for atomistic simulation; Geometric Hyena addresses their limitation in capturing long-range/global geometric interactions at scale."
    }
  ],
  "synthesis_narrative": "Geometric Hyena\u2019s core idea\u2014scalable global geometric reasoning under strict E(3) equivariance\u2014emerges by marrying long-convolution sequence models with tensor-field equivariant architectures. The state-space lineage (S4) established that long sequences can be modeled via implicit convolutional kernels, enabling sub-quadratic global context. Hyena Hierarchy then operationalized this with efficient long convolutions that replace attention, directly inspiring the choice of a Hyena-style operator. To make these operators compatible with 3D geometry, the work relies on the tensor-field formalism from Tensor Field Networks, using irreducible representations and tensor products to ensure rotation/translation equivariance when defining and composing long-range filters.\n\nThis design explicitly targets limitations revealed by prior equivariant models. SE(3)-Transformer provides strong global reasoning but suffers quadratic complexity; it serves as the principal baseline and computational foil that Geometric Hyena surpasses by replacing attention with long convolutions. Conversely, EGNN and related local message passing approaches are highly scalable but forfeit global geometric context, clarifying the need for a mechanism that is both equivariant and globally expressive. Finally, NequIP exemplifies successful but local equivariant models in atomistic simulation; Geometric Hyena addresses their inability to capture long-range interactions in large biological systems. Together, these works directly shaped a model that preserves strict equivariance, scales sub-quadratically, and captures global geometric dependencies.",
  "analysis_timestamp": "2026-01-06T23:07:19.642657"
}
{
  "prior_works": [
    {
      "title": "Explaining and Harnessing Adversarial Examples",
      "authors": "Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy",
      "year": 2015,
      "role": "Introduced adversarial examples and gradient-based Lp perturbations (e.g., FGSM), formalizing the adversarial loss notions widely used since.",
      "relationship_sentence": "Kumano et al. analyze Lp-bounded adversarial examples and derive Lq-loss bounds directly within this foundational perturbation framework."
    },
    {
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "authors": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu",
      "year": 2018,
      "role": "Established adversarial training as a min\u2013max optimization against projected gradient (PGD) attacks.",
      "relationship_sentence": "The paper\u2019s core contribution is a theoretical analysis of the training dynamics of Madry-style adversarial training in random deep networks."
    },
    {
      "title": "Deep Information Propagation",
      "authors": "Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, Jascha Sohl-Dickstein",
      "year": 2017,
      "role": "Developed mean-field theory for random deep networks, characterizing signal/gradient variance propagation and edge-of-chaos behavior.",
      "relationship_sentence": "Kumano et al. build on and extend this mean-field machinery to adversarial settings, removing distributional assumptions and tracking weight-variance evolution under robust objectives."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Characterized gradient-descent dynamics of infinitely wide networks in the lazy/NTK regime.",
      "relationship_sentence": "The authors situate their framework beyond NTK\u2019s lazy limit by capturing feature/variance dynamics under adversarial training, addressing limitations of kernel-based analyses."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "year": 2016,
      "role": "Introduced residual (shortcut) connections that stabilize signal and gradient propagation in deep nets.",
      "relationship_sentence": "Kumano et al.\u2019s theoretical result that networks without shortcuts are generally not adversarially trainable leverages insights about shortcut-induced trainability popularized by ResNets."
    },
    {
      "title": "Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope",
      "authors": "Eric Wong, J. Zico Kolter",
      "year": 2018,
      "role": "Proposed certified upper bounds on worst-case adversarial loss via convex relaxations.",
      "relationship_sentence": "The new mean-field framework yields analytic, empirically tight upper bounds on adversarial loss across Lp/Lq settings, conceptually complementing certified-bound approaches."
    },
    {
      "title": "Robustness May Be at Odds with Accuracy",
      "authors": "Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, Aleksander Madry",
      "year": 2019,
      "role": "Identified fundamental trade-offs between robustness and standard accuracy.",
      "relationship_sentence": "Kumano et al. theoretically ground a related phenomenon by proving adversarial training reduces network capacity and showing width can mitigate this effect."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014an analytic mean-field framework that explains adversarial training dynamics in random deep networks, derives tight Lp/Lq adversarial loss bounds, and predicts architectural/width effects\u2014emerges at the intersection of robust optimization and mean-field analyses of deep nets. Goodfellow et al. established Lp-bounded adversarial perturbations and loss formulations, while Madry et al. cast robustness as a principled min\u2013max training problem; these define the objective whose dynamics Kumano et al. set out to analyze. Classical mean-field theory for random deep networks (Schoenholz et al.) provided variance and gradient propagation recursions and edge-of-chaos conditions, but did not address adversarial objectives or evolving weight statistics during robust training. NTK theory (Jacot et al.) characterized infinite-width training in the lazy regime; however, its static-kernel assumption misses the feature and variance evolution central to adversarial optimization. Architecturally, residual shortcuts (He et al.) are known to stabilize deep training; Kumano et al. theoretically confirm that, under adversarial training, networks without shortcuts are generally not trainable and that increasing width alleviates this limitation\u2014linking robustness to mean-field trainability conditions. Finally, certified defenses (Wong & Kolter) demonstrated that upper-bounding worst-case adversarial loss is feasible, and robustness\u2013accuracy trade-off results (Tsipras et al.) suggested capacity tensions; Kumano et al. synthesize these threads by deriving empirically tight, analytic upper bounds across Lp/Lq settings and proving that adversarial training reduces effective capacity, while quantifying how width and dimensionality modulate these effects.",
  "analysis_timestamp": "2026-01-06T23:42:48.044381"
}
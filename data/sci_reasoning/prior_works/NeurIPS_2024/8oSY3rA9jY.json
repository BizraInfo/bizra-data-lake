{
  "prior_works": [
    {
      "title": "ACDC: Automated Circuit Discovery",
      "authors": "Rowan Conmy, Neel Nanda, et al.",
      "year": 2023,
      "role": "Predecessor automated circuit search method",
      "relationship_sentence": "Edge Pruning directly responds to ACDC\u2019s combinatorial, intervention-based search by replacing it with a continuous, gradient-driven edge-mask optimization that scales to larger datasets and yields sparser yet equally faithful circuits."
    },
    {
      "title": "Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2-small",
      "authors": "Kevin Wang, Neel Nanda, Lawrence Chan, et al.",
      "year": 2023,
      "role": "Benchmark circuit and methodology for validation",
      "relationship_sentence": "The IOI circuit work supplies canonical tasks, evaluation practices (e.g., faithfulness via activation/path patching), and circuit structure assumptions that Edge Pruning targets and matches while using far fewer edges."
    },
    {
      "title": "In-context Learning and Induction Heads",
      "authors": "Catherine Olsson, Nelson Elhage, Neel Nanda, et al.",
      "year": 2022,
      "role": "Foundational concept of transformer circuits/components",
      "relationship_sentence": "This work established attention heads/MLPs as circuit components and mapped edge-like interactions among them, providing the conceptual substrate on which Edge Pruning defines and prunes inter-component edges."
    },
    {
      "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity",
      "authors": "Namhoon Lee, Thalaiyasingam Ajanthan, Philip H.S. Torr",
      "year": 2019,
      "role": "Gradient-based connection pruning technique",
      "relationship_sentence": "Edge Pruning adapts SNIP-style sensitivity to connections by optimizing a mask over edges in the transformer\u2019s computational graph, using gradient signals to identify dispensable edges while preserving model behavior."
    },
    {
      "title": "Learning Sparse Neural Networks through L0 Regularization",
      "authors": "Christos Louizos, Max Welling, Diederik P. Kingma",
      "year": 2018,
      "role": "Differentiable sparsity via learnable gates",
      "relationship_sentence": "The paper\u2019s hard-concrete/L0 gating framework underpins Edge Pruning\u2019s formulation of learnable edge masks with sparsity penalties, enabling end-to-end optimization for a minimal faithful subgraph."
    },
    {
      "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning",
      "authors": "Victor Sanh, Thomas Wolf, Alexander M. Rush",
      "year": 2020,
      "role": "Training-time, gradient-guided sparsification",
      "relationship_sentence": "Edge Pruning borrows the idea of learning sparsity during optimization\u2014tracking importance via gradients/updates\u2014to progressively remove edges while maintaining predictive fidelity."
    },
    {
      "title": "Optimal Brain Damage",
      "authors": "Yann LeCun, John S. Denker, Sara A. Solla",
      "year": 1989,
      "role": "Pioneering connection-pruning rationale",
      "relationship_sentence": "By framing network compression as pruning connections based on their effect on loss, this classic work motivates Edge Pruning\u2019s focus on edge-level sparsification rather than neuron/head removal."
    }
  ],
  "synthesis_narrative": "Edge Pruning\u2019s core innovation\u2014casting circuit discovery as an edge-level optimization problem solved with gradient-based pruning\u2014emerges from the confluence of mechanistic interpretability\u2019s circuit paradigm and decades of sparsification research. Foundational circuit work on induction heads formalized components (heads/MLPs) and their interactions as sparse, task-relevant subgraphs, while the IOI circuit established concrete benchmarks and causal-evaluation practices for verifying faithfulness. ACDC then demonstrated the feasibility of automated circuit discovery but relied on costly, intervention-driven combinatorial search that can mis-rank edges and struggle to scale. \n\nEdge Pruning imports the pruning community\u2019s most effective ideas to overcome these limitations. SNIP\u2019s connection-sensitivity motivated optimizing over edge masks using gradients, and L0-style hard-concrete gating provides a differentiable sparsity prior to directly minimize edge count while matching the full model\u2019s outputs. Movement Pruning informs learning sparsity during fine-tuning, maintaining task performance as structure is removed. The older Optimal Brain Damage principle further justifies focusing on connections themselves, not just units, aligning with the interpretability goal of identifying specific inter-component pathways. \n\nBy unifying these strands, Edge Pruning replaces discrete search with a scalable, continuous objective: preserve model predictions on circuit-finding tasks while minimizing the number of inter-component edges. The result is circuits with substantially fewer edges, comparable faithfulness, and significantly improved efficiency on large datasets.",
  "analysis_timestamp": "2026-01-07T00:02:04.747625"
}
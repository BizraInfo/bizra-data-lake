{
  "prior_works": [
    {
      "title": "A Theoretical Analysis of Contrastive Unsupervised Representation Learning",
      "authors": "Saunshi et al.",
      "year": 2019,
      "arxiv_id": "1902.09229",
      "role": "Gap Identification",
      "relationship_sentence": "By formalizing when contrastive pretraining yields features useful for downstream ERM under a latent-class model, this work pinpointed a method-specific theory that directly motivated the paper\u2019s method-agnostic MLE-of-latent-models framework and its broader informative condition."
    },
    {
      "title": "Contrastive Learning, Multi-View Redundancy, and Nonlinear ICA",
      "authors": "Tosh et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "This paper identified structural (multi-view redundancy) conditions under which contrastive objectives recover sufficient features, highlighting assumptions that the present work abstracts and relaxes into a general informative condition for arbitrary latent-variable MLE pretraining."
    },
    {
      "title": "A Model of Inductive Bias Learning",
      "authors": "Baxter",
      "year": 2000,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "It introduced the representation\u2013hypothesis class decomposition and a sample-complexity lens across tasks, directly underpinning the paper\u2019s Phi\u2013Psi formalization of representation classes and downstream predictors."
    },
    {
      "title": "The Benefit of Multitask Representation Learning",
      "authors": "Maurer et al.",
      "year": 2016,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Providing generalization bounds for two-stage representation learning with downstream ERM, it supplies the supervised representation-learning template that this work repurposes with unsupervised MLE-derived representations."
    },
    {
      "title": "Provable Meta-Learning of Linear Representations",
      "authors": "Tripuraneni et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "By proving labeled-sample savings from learning a shared linear representation across tasks, it established the two-stage representation-then-ERM paradigm that this paper generalizes to unsupervised MLE and non-linear latent-variable classes."
    },
    {
      "title": "The Relative Value of Labeled and Unlabeled Data in Pattern Recognition with an Unknown Mixing Parameter",
      "authors": "Castelli and Cover",
      "year": 1995,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "It showed that unlabeled data can reduce label complexity under mixture structures, directly inspiring the latent-variable perspective and the claim that unsupervised pretraining can provably lower labeled sample requirements."
    },
    {
      "title": "On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes",
      "authors": "Ng and Jordan",
      "year": 2002,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By demonstrating that generative modeling can yield favorable sample complexity, it motivates using unsupervised MLE on x to learn representations that accelerate downstream ERM."
    }
  ],
  "synthesis_narrative": "A line of theory on representation learning clarified when learned features reduce labeled sample requirements. Baxter established the representation\u2013hypothesis class decomposition, showing how shared representations control sample complexity across tasks, while Maurer and collaborators provided generalization bounds for two-stage learning\u2014first fit a representation, then perform ERM on downstream predictors\u2014thereby formalizing the sample-efficiency advantage of a good feature map. Separately, classic semi-supervised analysis by Castelli and Cover proved that unlabeled data can cut label complexity under mixture structures, and Ng and Jordan showed generative modeling can enjoy superior sample complexity to discriminative training, highlighting the promise of modeling p(x) to boost supervised performance. More recently, Saunshi and coauthors analyzed contrastive learning under a latent-class model to delineate when pretraining helps downstream ERM, and Tosh et al. identified multi-view redundancy conditions under which contrastive objectives recover sufficient features\u2014both providing precise but method-specific assumptions. In parallel, Tripuraneni et al. proved labeled-sample savings from meta-learning shared linear representations, reinforcing the two-stage pipeline but in a supervised, linear setting. Together, these works exposed an opportunity: unify the representation\u2013downstream decomposition with the sample-efficiency benefits of unlabeled generative modeling, but without tying guarantees to a particular self-supervised objective. Building on the two-stage ERM template, the paper abstracts latent-variable structure into a general class, learns representations via unsupervised MLE, and replaces method-specific assumptions with a mild informative condition, thereby delivering broad, provable labeled-sample advantages for pretrain-then-finetune.",
  "target_paper": {
    "title": "On the Provable Advantage of Unsupervised Pretraining",
    "authors": "Jiawei Ge, Shange Tang, Jianqing Fan, Chi Jin",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "unsupervised pretraining; representation learning; sample complexity",
    "abstract": "Unsupervised pretraining, which learns a useful representation using a large amount of unlabeled data to facilitate the learning of downstream tasks, is a critical component of modern large-scale machine learning systems. Despite its tremendous empirical success, the rigorous theoretical understanding of why unsupervised pretraining generally helps remains rather limited---most existing results are restricted to particular methods or approaches for unsupervised pretraining with specialized structural assumptions. This paper studies a generic framework,\nwhere the unsupervised representation learning task is specified by an abstract class of latent variable models $\\Phi$ and the downstream task is specified by a class of prediction functions $\\Psi$. We consider a natural approach of using Maximum Likelihood Estimation (MLE) for unsupervised pretraining and Empirical Risk Minimization (ERM) for learning downstream tasks. We prove that, under a mild ``informative'' condition, our algorithm",
    "openreview_id": "rmXXKxQpOR",
    "forum_id": "rmXXKxQpOR"
  },
  "analysis_timestamp": "2026-01-06T11:02:00.517813"
}
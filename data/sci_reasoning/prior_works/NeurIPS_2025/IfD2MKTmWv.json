{
  "prior_works": [
    {
      "title": "Neural networks and physical systems with emergent collective computational abilities",
      "authors": "John J. Hopfield",
      "year": 1982,
      "role": "Associative memory foundation",
      "relationship_sentence": "Established energy-based associative memory dynamics that underpin Memory Mosaics\u2019 use of content-addressable storage and retrieval as a first-class computational primitive."
    },
    {
      "title": "Sparse Distributed Memory",
      "authors": "Pentti Kanerva",
      "year": 1988,
      "role": "High-dimensional addressable memory",
      "relationship_sentence": "Introduced scalable, high-dimensional addressing and distributed storage, directly inspiring the mosaic-of-memories design and its ability to compose many associative stores at scale."
    },
    {
      "title": "Hopfield Networks is All You Need",
      "authors": "Hubert Ramsauer et al.",
      "year": 2021,
      "role": "Modern continuous Hopfield/attention connection",
      "relationship_sentence": "Showed that modern Hopfield updates subsume attention, providing the mathematical bridge that informs Memory Mosaics\u2019 associative layers and their compatibility with LLM-scale training."
    },
    {
      "title": "Neural Turing Machines",
      "authors": "Alex Graves, Greg Wayne, Ivo Danihelka",
      "year": 2014,
      "role": "Differentiable external memory blueprint",
      "relationship_sentence": "Pioneered differentiable read/write mechanisms to external memory, motivating Memory Mosaics\u2019 modular memory access and training strategy for storing and updating knowledge beyond parametric weights."
    },
    {
      "title": "Improving language models by retrieving from trillions of tokens (RETRO)",
      "authors": "Sebastien Borgeaud et al.",
      "year": 2022,
      "role": "Retrieval-augmented language modeling at scale",
      "relationship_sentence": "Demonstrated that non-parametric retrieval can upgrade LMs with new knowledge at inference time, directly shaping the paper\u2019s evaluation of \u2018new-knowledge storage\u2019 and inference-time task performance."
    },
    {
      "title": "Language Models are Few-Shot Learners",
      "authors": "Tom B. Brown et al.",
      "year": 2020,
      "role": "In-context learning phenomenon and benchmarks",
      "relationship_sentence": "Revealed and operationalized the ICL paradigm and evaluation protocols, which Memory Mosaics v2 target and surpass relative to transformer baselines in few-shot settings."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann et al.",
      "year": 2022,
      "role": "Scaling laws and data\u2013compute tradeoffs",
      "relationship_sentence": "Provided compute-optimal scaling guidance (e.g., trillion-token regimes) that frames the paper\u2019s 10B/1T-token training and supports its claim that more data alone doesn\u2019t match memory-centric gains."
    }
  ],
  "synthesis_narrative": "Memory Mosaics at scale fuses classical associative memory theory with modern large-scale language modeling practice. Hopfield\u2019s original energy-based formulation and Kanerva\u2019s Sparse Distributed Memory supply the core principle: content-addressable, distributed storage that can be composed across many cells. Building on this, modern continuous Hopfield work (Ramsauer et al.) formalizes the tight link between associative dynamics and attention, making associative layers compatible with transformer-era optimization and providing a natural path to LLM-scale architectures.\n\nFrom the neural memory systems lineage, Neural Turing Machines contribute differentiable read/write access and training recipes for modular memory components, strongly informing Memory Mosaics\u2019 v2 architectural choices for robust new-knowledge insertion without catastrophic interference. Retrieval-augmented LMs such as RETRO show that separating parametric knowledge from non-parametric memory can yield large inference-time gains; this directly motivates the paper\u2019s evaluation axes\u2014training-knowledge storage versus new-knowledge storage\u2014and its claim that inference-time augmentation is a superior route to rapid knowledge acquisition.\n\nFinally, GPT-3 establishes in-context learning as a central competency and provides the canonical evaluation setting that Memory Mosaics v2 seeks to outperform. Chinchilla-style scaling laws guide the 10B-parameter, trillion-token training regime used as a fair comparison point and ground the paper\u2019s assertion that simply scaling data for transformers does not recover the benefits of an explicitly associative, memory-centric design. Together, these works converge on the key insight: networks of associative memories can be engineered to retain transformer-level pretraining performance while substantially improving inference-time learning and compositionality at LLM scale.",
  "analysis_timestamp": "2026-01-06T23:42:48.148910"
}
{
  "prior_works": [
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "arxiv_id": "2301.12597",
      "role": "Extension",
      "relationship_sentence": "MPM adopts BLIP-2\u2019s frozen-vision-encoder + LLM bridging strategy and extends it by swapping in a multilingual LLM so English-only visual pretraining pivots to other languages."
    },
    {
      "title": "Visual Instruction Tuning (LLaVA)",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Baseline",
      "relationship_sentence": "MPM builds on LLaVA\u2019s instruction-tuning recipe for aligning visual features to an LLM, showing that using a multilingual LLM yields zero-shot multilingual multimodal ability without any non-English image\u2013text training."
    },
    {
      "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
      "authors": "Deyao Zhu et al.",
      "year": 2023,
      "arxiv_id": "2304.10592",
      "role": "Related Problem",
      "relationship_sentence": "MPM follows MiniGPT-4\u2019s practical connector-plus-instruction-tuning pipeline but replaces the monolingual LLM with a multilingual one to realize cross-lingual transfer from English-only multimodal supervision."
    },
    {
      "title": "M-CLIP: Multilingual CLIP via Cross-lingual Transfer",
      "authors": "Marius Muennighoff et al.",
      "year": 2022,
      "arxiv_id": "2211.17191",
      "role": "Inspiration",
      "relationship_sentence": "MPM generalizes M-CLIP\u2019s insight\u2014that multilingual text models can inherit English-trained visual alignment\u2014to LMMs by using a multilingual LLM as the pivot across languages."
    },
    {
      "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
      "authors": "Xi Chen et al.",
      "year": 2022,
      "arxiv_id": "2209.06794",
      "role": "Gap Identification",
      "relationship_sentence": "MPM targets PaLI\u2019s multilingual vision\u2013language objectives while explicitly addressing its limitation of requiring massive multilingual image\u2013text corpora by relying only on English visual data plus a multilingual LLM."
    },
    {
      "title": "UC^2: Universal Cross-lingual Cross-modal Vision-and-Language Pretraining",
      "authors": "Luowei Zhou et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "MPM removes UC^2\u2019s dependence on machine-translated non-English captions for cross-lingual VLP by letting a multilingual LLM supply the language bridge with zero non-English visual supervision."
    },
    {
      "title": "BLOOMZ: Cross-lingual Generalization through Instruction Tuning",
      "authors": "N. Muennighoff et al.",
      "year": 2022,
      "arxiv_id": "2211.01786",
      "role": "Foundation",
      "relationship_sentence": "MPM leverages BLOOMZ\u2019s core finding that instruction-tuned multilingual LMs can follow prompts across languages, using that capability as the pivot for multilingual multimodal understanding and generation."
    }
  ],
  "synthesis_narrative": "BLIP-2 showed that strong multimodal capability can be achieved by freezing a vision encoder and language model while learning a lightweight cross-modal bridge, establishing a modular recipe where the language component can be swapped. LLaVA demonstrated that instruction tuning effectively aligns visual features to an LLM\u2019s conversational interface, yielding robust vision\u2013language reasoning with simple connectors trained on English-only data. MiniGPT-4 refined this connector-plus-instruction-tuning pipeline using a BLIP-2-style bridge to enable open-ended image dialog, further validating that most multimodal supervision can be English. In parallel, M-CLIP proved that multilingual text encoders can inherit the visual semantics of English-trained CLIP via cross-lingual transfer, enabling multilingual zero-shot retrieval without multilingual image\u2013text pairs. PaLI established the value of multilingual vision\u2013language models at scale, but did so by consuming massive multilingual image\u2013text corpora, while UC^2 relied on machine-translated captions to cover non-English modalities. Separately, BLOOMZ showed that instruction-tuned multilingual LMs naturally generalize tasks across languages, suggesting they can act as a linguistic bridge.\nTogether these works reveal a gap: multilingual multimodal ability typically requires non-English visual-text supervision or translation, even though English-trained visual alignment and multilingual language competence already exist. The natural next step is to keep visual pretraining purely English, preserve the LLM\u2013vision modularity of BLIP-2/LLaVA, and rely on an instruction-tuned multilingual LLM (as evidenced by BLOOMZ and M-CLIP\u2019s cross-lingual transfer) to pivot outputs and understanding across languages. This synthesis yields a quasi-zero-shot multilingual multimodal paradigm powered by the LLM\u2019s multilinguality rather than multilingual image\u2013text data.",
  "target_paper": {
    "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages",
    "authors": "Jinyi Hu, Yuan Yao, Chongyi Wang, SHAN WANG, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, dahai li, Zhiyuan Liu, Maosong Sun",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Large Multimodal Models, Multilingual Transfer",
    "abstract": "Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a (quasi)-zero-shot manner, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM i",
    "openreview_id": "Kuh5qgCGCp",
    "forum_id": "Kuh5qgCGCp"
  },
  "analysis_timestamp": "2026-01-06T15:12:00.755627"
}
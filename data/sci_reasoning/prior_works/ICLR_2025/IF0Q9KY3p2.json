{
  "prior_works": [
    {
      "title": "A Variational Perspective on Accelerated Methods",
      "authors": "Wibisono et al.",
      "year": 2016,
      "arxiv_id": "1603.04245",
      "role": "Foundation",
      "relationship_sentence": "This work formalized mirror flow via Bregman geometry induced by a potential, providing the continuous-time mirror-descent framework the paper adopts to analyze implicit bias under general potentials and to define the new class of scaled potentials."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Jacot et al.",
      "year": 2018,
      "arxiv_id": "1806.07572",
      "role": "Foundation",
      "relationship_sentence": "The NTK linearization yields the RKHS bias of infinite-width gradient flow, which the paper extends by proving that mirror flow has the same infinite-width implicit bias and by pinpointing when scaled potentials depart from the NTK regime."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "Chizat and Bach",
      "year": 2019,
      "arxiv_id": "1812.07956",
      "role": "Gap Identification",
      "relationship_sentence": "By identifying the lazy (kernel) regime and its scaling conditions for gradient flow, this work motivates the paper\u2019s extension showing mirror flow is also lazy for broad potentials and highlights the gap the paper fills with a lazy-but-non-kernel regime via scaled potentials."
    },
    {
      "title": "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
      "authors": "Lee et al.",
      "year": 2019,
      "arxiv_id": "1902.06720",
      "role": "Extension",
      "relationship_sentence": "Their function-space equivalence between infinite-width gradient descent and kernel gradient flow is the baseline equivalence the paper generalizes to mirror flow to establish identical implicit bias at infinite width."
    },
    {
      "title": "Breaking the Curse of Dimensionality with Convex Neural Networks",
      "authors": "Bach",
      "year": 2017,
      "arxiv_id": "1412.8690",
      "role": "Foundation",
      "relationship_sentence": "The measure/variation-norm representation for two-layer ReLU networks underlies the paper\u2019s function-space variational characterization of the implicit bias for univariate networks (including absolute value activations)."
    },
    {
      "title": "A Function Space View of Bounded Variation Regularization in Deep Learning",
      "authors": "Ongie et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "By linking shallow ReLU networks to linear splines and BV-type seminorms, this work provides the specific spline/variational machinery the paper leverages to characterize the implicit bias of univariate ReLU networks trained by mirror flow."
    },
    {
      "title": "Near-Interpolating Splines via Two-Layer ReLU Networks: Trend Filtering Connections",
      "authors": "Parhi and Nowak",
      "year": 2019,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "Their characterization of univariate ReLU fits through trend-filtering\u2013style variational problems required data adjustments or architectural tweaks (e.g., skip connections), a limitation the paper explicitly removes by giving a direct variational bias characterization for standard univariate networks."
    }
  ],
  "synthesis_narrative": "Mirror flow arises as the continuous-time limit of mirror descent defined by Bregman geometry from a potential, as formalized by Wibisono et al., making it natural to study optimization-induced bias through the choice of potential. The neural tangent kernel framework of Jacot et al. established that infinite-width gradient flow linearizes dynamics and induces an RKHS bias, while Lee et al. showed this equivalently as kernel gradient flow in function space for wide networks. Chizat and Bach identified the lazy regime and the scaling conditions that keep training near initialization, delineating when such kernel dynamics prevail. On the function-space side, Bach\u2019s measure-based representation of two-layer ReLU networks and the associated variation norms, together with Ongie et al.\u2019s linear-spline/BV perspective, supplied precise variational tools linking shallow ReLU networks to spline minimization. Parallel work connecting univariate ReLU regression to trend filtering (e.g., Parhi and Nowak) offered variational characterizations but often relied on data adjustments or skip connections to make the analysis tractable. Taken together, these strands suggested that both the optimization geometry (mirror maps/potentials) and width scaling govern whether training is lazy and which function-space norm is implicitly minimized. The paper synthesizes these ideas by proving that mirror flow\u2014across broad potentials\u2014exhibits lazy training and shares gradient flow\u2019s RKHS bias at infinite width, then introducing scaled potentials that retain laziness yet move beyond NTK. Leveraging spline/BV representations, it gives a direct variational characterization for univariate ReLU and shows absolute-value activations yield implicit biases not captured by any RKHS, thereby resolving prior limitations and expanding the controllable bias landscape via potentials.",
  "target_paper": {
    "title": "Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression",
    "authors": "Shuang Liang, Guido Montufar",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "implicit bias, overparametrized neural network, mirror descent, univariate regression, lazy training",
    "abstract": "We examine the implicit bias of mirror flow in least squares error regression with wide and shallow neural networks. For a broad class of potential functions, we show that mirror flow exhibits lazy training and has the same implicit bias as ordinary gradient flow when the network width tends to infinity. For univariate ReLU networks, we characterize this bias through a variational problem in function space. Our analysis includes prior results for ordinary gradient flow as a special case and lifts limitations which required either an intractable adjustment of the training data or networks with skip connections. We further introduce \\emph{scaled potentials} and show that for these, mirror flow still exhibits lazy training but is not in the kernel regime. For univariate networks with absolute value activations, we show that mirror flow with scaled potentials induces a rich class of biases, which generally cannot be captured by an RKHS norm. A takeaway is that whereas the parameter initial",
    "openreview_id": "IF0Q9KY3p2",
    "forum_id": "IF0Q9KY3p2"
  },
  "analysis_timestamp": "2026-01-06T08:05:27.830671"
}
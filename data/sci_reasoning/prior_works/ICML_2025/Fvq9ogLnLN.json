{
  "prior_works": [
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": [
        "Jared Kaplan",
        "Sam McCandlish",
        "Tom Henighan",
        "Tom B. Brown",
        "Benjamin Chess",
        "Rewon Child",
        "Scott Gray",
        "Alec Radford",
        "Jeff Wu",
        "Dario Amodei"
      ],
      "year": 2020,
      "role": "Established precise power-law relationships between loss, model size, dataset size, and compute, and framed the notion of compute-optimal scaling tradeoffs.",
      "relationship_sentence": "The paper\u2019s focus on compute-optimal training and normalized comparisons across model sizes directly builds on Kaplan et al.\u2019s compute\u2013loss scaling framework and motivates searching for universal, size-invariant training dynamics."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": [
        "Jordan Hoffmann",
        "Sebastian Borgeaud",
        "Arthur Mensch",
        "Eliza Rutherford",
        "Katie Millican",
        "George van den Driessche",
        "Jean-Baptiste Lespiau",
        "et al."
      ],
      "year": 2022,
      "role": "Refined compute-optimality by showing data\u2013parameter ratios for best use of fixed compute (Chinchilla scaling), operationalizing \u201ccompute-optimal training.\u201d",
      "relationship_sentence": "By specifying how to balance model size and data for fixed compute, this work provides the practical recipe under which the reported scaling collapse and supercollapse are observed."
    },
    {
      "title": "An Empirical Model of Large-Batch Training",
      "authors": [
        "Sam McCandlish",
        "Jared Kaplan",
        "Dario Amodei"
      ],
      "year": 2018,
      "role": "Introduced the gradient noise scale and critical batch size, linking batch size, learning rate, step count, and efficiency under a compute budget.",
      "relationship_sentence": "The claimed breakdown of collapse under suboptimal hyperparameter scaling aligns with and is anticipated by the gradient-noise-scale view of how LR and batch should co-vary for compute-optimal dynamics."
    },
    {
      "title": "Deep Learning Scaling is Predictable, Empirically",
      "authors": [
        "Joel Hestness",
        "Sharan Narang",
        "Newsha Ardalani",
        "Gregory Diamos",
        "Heewoo Jun",
        "et al."
      ],
      "year": 2017,
      "role": "Provided early evidence of predictable, power-law learning curves across tasks and model families, motivating universal descriptions of training progress.",
      "relationship_sentence": "This empirical foundation for predictable learning curves set the stage for the paper\u2019s search for a single normalized training trajectory shared across scales."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": [
        "Andrew M. Saxe",
        "James L. McClelland",
        "Surya Ganguli"
      ],
      "year": 2014,
      "role": "Derived analytic training dynamics with mode-wise time scales, revealing invariances and potential curve collapses under appropriate rescalings.",
      "relationship_sentence": "The observed collapse of loss trajectories after time/scale normalization echoes Saxe et al.\u2019s insight that learning dynamics can be universal up to simple rescalings."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": [
        "Arthur Jacot",
        "Franck Gabriel",
        "Cl\u00e9ment Hongler"
      ],
      "year": 2018,
      "role": "Showed that overparameterized networks follow linearized gradient flow with predictable training curves, offering a theory for width-invariant dynamics.",
      "relationship_sentence": "The paper\u2019s universality across model sizes and architectures is theoretically grounded by NTK-style linearized dynamics that admit normalized, size-independent loss trajectories."
    },
    {
      "title": "Don\u2019t Decay the Learning Rate, Increase the Batch Size",
      "authors": [
        "Samuel L. Smith",
        "Pieter-Jan Kindermans",
        "Chris Ying",
        "Quoc V. Le"
      ],
      "year": 2018,
      "role": "Demonstrated an equivalence between learning rate decay and batch-size scaling via SGD noise, shaping how schedules control training dynamics.",
      "relationship_sentence": "The finding that learning rate decay tightens the collapse (supercollapse) is consistent with schedule/noise-equivalence insights that align dynamics across scales."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a universal scaling collapse of training loss curves under compute-optimal training\u2014sits at the intersection of empirical scaling laws, optimization under compute constraints, and analytic training dynamics. Foundational works on scaling laws (Hestness et al.; Kaplan et al.) established that neural performance follows predictable power laws in model size, data, and compute, motivating the search for invariant descriptions of learning progress. Hoffmann et al. then operationalized compute-optimality (Chinchilla), prescribing data\u2013parameter ratios for fixed compute; this provides the precise regime in which the authors observe collapse and use its breakdown as a diagnostic for suboptimal scaling.\nComplementing these empirical laws, McCandlish et al. introduced the gradient noise scale and critical batch size, linking learning rate, batch size, and step count to efficient compute usage. This framework predicts when hyperparameter mis-scaling should distort dynamics\u2014exactly matching the paper\u2019s observation that collapse fails under suboptimal choices. On the theory side, Saxe et al. showed that training dynamics can reduce to universal, rescaled trajectories in deep linear models, while the NTK framework (Jacot et al.) extends a similar invariance intuition to overparameterized nonlinear networks, supporting the possibility of size-invariant normalized loss curves. Finally, Smith et al. clarified how learning rate schedules modulate SGD noise and can align dynamics across scales; this helps explain why learning rate decay sharpens the collapse into \u201csupercollapse.\u201d Together, these works directly underpin the paper\u2019s identification, measurement, and theoretical explanation of universal, compute-normalized training dynamics.",
  "analysis_timestamp": "2026-01-07T00:04:09.135161"
}
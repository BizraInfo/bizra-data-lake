{
  "prior_works": [
    {
      "title": "The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives (COMICS dataset)",
      "authors": "Mohit Iyyer et al.",
      "year": 2017,
      "role": "dataset/analysis of narrative inference in comics",
      "relationship_sentence": "Established inter-panel narrative reasoning in comics, directly motivating YesBut\u2019s two-panel setup that requires readers (and models) to infer meaning from juxtaposed panels."
    },
    {
      "title": "Hateful Memes: A Benchmark for Robust Multimodal Reasoning",
      "authors": "Douwe Kiela et al.",
      "year": 2020,
      "role": "benchmark/design of multimodal evaluation",
      "relationship_sentence": "Demonstrated how to build multimodal benchmarks that minimize unimodal shortcuts, informing YesBut\u2019s design to ensure humor understanding requires joint vision\u2013language reasoning rather than surface cues."
    },
    {
      "title": "Winoground: Probing Multimodal Compositionality and Grounding",
      "authors": "Clark Thrush et al.",
      "year": 2022,
      "role": "benchmark/probe of compositional binding",
      "relationship_sentence": "Showed LVLMs struggle with fine-grained compositional binding, inspiring YesBut\u2019s focus on nuanced cross-panel juxtaposition where small narrative shifts flip interpretations to humorous contradiction."
    },
    {
      "title": "Visual Commonsense Reasoning (VCR)",
      "authors": "Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi",
      "year": 2019,
      "role": "dataset/evaluation of high-level reasoning with rationales",
      "relationship_sentence": "Pioneered multi-step evaluation from recognition to explanation, shaping YesBut\u2019s tiered tasks that range from literal comprehension to deeper narrative reasoning about humorous contradictions."
    },
    {
      "title": "NLVR2: A Corpus for Reasoning about Natural Language Grounded in Photographs",
      "authors": "Alane Suhr et al.",
      "year": 2019,
      "role": "dataset/multi-image reasoning and verification",
      "relationship_sentence": "Introduced true/false judgments over paired images, directly relevant to YesBut\u2019s need to reason across two panels to detect and explain contradictions."
    },
    {
      "title": "Scene Text Visual Question Answering (ST-VQA)",
      "authors": "D\u00eddac Sur\u00eds Borr\u00e0s (Biten) et al.",
      "year": 2019,
      "role": "dataset/vision-language with OCR",
      "relationship_sentence": "Highlighted the necessity of reading text in images for QA, underpinning YesBut\u2019s inclusion of tasks where on-panel text is essential to decode the joke\u2019s setup\u2013punchline contrast."
    },
    {
      "title": "The Act of Creation",
      "authors": "Arthur Koestler",
      "year": 1964,
      "role": "theory/incongruity and bisociation in humor",
      "relationship_sentence": "Provided the theoretical basis that humor arises from bisociation of incongruent frames, directly informing YesBut\u2019s operationalization of humorous contradiction via panel juxtaposition."
    }
  ],
  "synthesis_narrative": "YesBut\u2019s core contribution\u2014a benchmark that isolates whether LVLMs can understand humorous contradictions created by juxtaposed panels\u2014stands on three pillars developed by prior work: inter-panel narrative reasoning, robust multimodal evaluation design, and the cognitive theory of humor as incongruity. The COMICS dataset (Iyyer et al., 2017) first framed comics as a laboratory for narrative inference across panels, directly inspiring YesBut\u2019s two-panel unit and tasks that require connecting setups and punchlines. From the evaluation-design perspective, Hateful Memes (Kiela et al., 2020) demonstrated how to craft multimodal datasets that curb unimodal shortcuts, a principle YesBut adopts to ensure humor recognition requires genuine cross-modal, cross-panel reasoning. Winoground (Thrush et al., 2022) revealed persistent LVLM weaknesses in compositional binding, motivating YesBut\u2019s emphasis on subtle narrative reversals where small changes in context flip meanings. VCR (Zellers et al., 2019) contributed the idea of tiered tasks progressing from perception to explanation, which YesBut mirrors with stages from literal content to deep narrative interpretation. NLVR2 (Suhr et al., 2019) provided a template for multi-image contradiction verification that maps naturally to two-panel comics. Because many comics hinge on embedded text, ST-VQA (Biten et al., 2019) underlines the necessity of OCR-aware reasoning included in YesBut\u2019s easier tasks. Finally, Koestler\u2019s bisociation theory (1964) gives the conceptual backbone, formalizing humor as the collision of incompatible frames\u2014precisely the phenomenon YesBut operationalizes to probe LVLM narrative understanding.",
  "analysis_timestamp": "2026-01-07T00:02:04.735799"
}
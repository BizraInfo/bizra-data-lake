{
  "prior_works": [
    {
      "title": "Deep Sets",
      "authors": "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab\u00e1s P\u00f3czos, Ruslan Salakhutdinov, Alexander J. Smola",
      "year": 2017,
      "role": "Foundational architecture for permutation-invariant functions on sets",
      "relationship_sentence": "This paper formalized the sum-of-features set representation f(X) = \u03c1(\u2211x\u2208X \u03c6(x)); the current work rigorously establishes when such \u201cneural moments\u201d (\u2211 \u03c6\u03b8(x)) are injective on multisets under analytic non-polynomial activations, closing a theoretical gap left by DeepSets\u2019 practical recipe."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka",
      "year": 2019,
      "role": "Expressivity of GNNs via injective multiset aggregation (GIN)",
      "relationship_sentence": "GIN identified injective multiset functions as key to matching 1\u2011WL power using sum aggregation plus MLPs; the present paper provides the missing rigorous conditions under which such neural sum-aggregators are injective, yielding new separation results for GNNs."
    },
    {
      "title": "Universal Invariant and Equivariant Graph Neural Networks",
      "authors": "Antoine Keriven, Gabriel Peyr\u00e9",
      "year": 2019,
      "role": "Universality/separation via polynomial invariant moments",
      "relationship_sentence": "Keriven\u2013Peyr\u00e9 achieved universality for invariant/equivariant models using polynomial features/moments; the current work generalizes this theoretical foundation by replacing polynomial moments with neural moments and proving injectivity with a finite, near\u2011optimal number of such moments."
    },
    {
      "title": "On the Universality of Invariant Networks",
      "authors": "Haggai Maron, Ethan Fetaya, Nadav Segol, Yaron Lipman",
      "year": 2019,
      "role": "Invariant network universality using higher-order tensor/polynomial constructions",
      "relationship_sentence": "Maron et al. established universality of invariant networks via polynomial/tensorized features that are provably injective; the present paper bridges this polynomial\u2011moment theory to practical neural features by proving injectivity of neural moments and deriving analogous approximation/separation corollaries."
    },
    {
      "title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function",
      "authors": "Shai Leshno, Vladimir Y. Lin, Allan Pinkus, Shimon Schocken",
      "year": 1993,
      "role": "Classical universal approximation criterion for non-polynomial activations",
      "relationship_sentence": "This result underpins the paper\u2019s requirement of non\u2011polynomial (and in fact analytic) activations: it supplies the functional richness needed so that finitely many neural moments can distinguish multisets/measures and support the paper\u2019s approximation corollaries."
    },
    {
      "title": "Formules de cubature m\u00e9caniques \u00e0 coefficients non n\u00e9gatifs",
      "authors": "V. Tchakaloff",
      "year": 1957,
      "role": "Finite cubature/truncated moment theory enabling finite witnesses",
      "relationship_sentence": "Tchakaloff\u2019s finite cubature theorem inspires the paper\u2019s finite witness theorem: it justifies that equality of (enough) moments implies equality of discrete measures, guiding the proof that a finite set of neural moments suffices for injectivity, with near\u2011optimal moment counts."
    },
    {
      "title": "On the limitations of representing functions on sets",
      "authors": "Edward Wagstaff, Fabian Fuchs, Martin Engelcke, Ingmar Posner, Michael A. Osborne",
      "year": 2019,
      "role": "Limits of sum-based set representations",
      "relationship_sentence": "By exposing pitfalls of set representations and conditions under which sum-aggregators fail, this work motivates precise injectivity criteria; the present paper answers by proving when neural sum-moments are injective and by providing complementary negative results."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014proving that finitely many moments of neural networks yield injective multiset (and measure) embeddings under analytic non\u2011polynomial activations\u2014builds directly on two intertwined threads: practical sum-aggregator architectures and theoretical moment-based injectivity. DeepSets introduced the canonical sum-of-features representation for permutation-invariant functions, and GIN tied injective multiset aggregation to the expressive power of message-passing GNNs. These works motivated a rigorous characterization of when the commonly used \u201cneural moments\u201d (sum of learned features) are truly injective, a gap this paper closes.\nOn the theory side, universality and separation results for invariant/equivariant networks were previously achieved using polynomial moments and higher-order tensor constructions, as in Keriven\u2013Peyr\u00e9 and Maron et al. The present paper departs from purely polynomial machinery by establishing that neural (analytic, non\u2011polynomial) features can play the same role, and it quantifies a near\u2011optimal finite number of moments needed\u2014essentially up to a factor of two. This finite witness perspective echoes classical truncated-moment and cubature theory, epitomized by Tchakaloff\u2019s theorem, which ensures that equality of finitely many moments can certify equality of discrete measures.\nFinally, classical universal approximation results (Leshno et al.) justify the activation assumptions enabling rich neural moment families, while critiques of set representations (Wagstaff et al.) underscore the necessity of precise injectivity guarantees and inform the paper\u2019s negative results. Together, these influences culminate in a finite witness theorem that bridges theory and practice, yielding new approximation results on multisets/measures and sharpened GNN separation guarantees.",
  "analysis_timestamp": "2026-01-07T00:02:04.829087"
}
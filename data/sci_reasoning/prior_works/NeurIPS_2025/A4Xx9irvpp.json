{
  "prior_works": [
    {
      "title": "Convexity, Classification, and Risk Bounds",
      "authors": "Peter L. Bartlett et al.",
      "year": 2006,
      "role": "Foundation",
      "relationship_sentence": "Established the surrogate regret (psi-transform) framework that this paper explicitly targets, and highlighted that smooth convex losses like logistic typically induce sublinear (e.g., square-root) regret transfer while margin-based non-smooth losses can yield linear transfer."
    },
    {
      "title": "Composite Binary Losses",
      "authors": "Mark D. Reid et al.",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "Introduced the composite loss framework with prediction links; the proposed linear surrogate regret bound is achieved here via a tailored link designed precisely within this composite-loss-and-link paradigm."
    },
    {
      "title": "On the Consistency of Multiclass Classification Methods",
      "authors": "Ambuj Tewari et al.",
      "year": 2007,
      "role": "Foundation",
      "relationship_sentence": "Provided multiclass calibration foundations used by this work to ensure consistency of the constructed surrogate for arbitrary discrete target losses under an appropriate link."
    },
    {
      "title": "Convex Calibration Dimension for Multiclass Classification",
      "authors": "Harish G. Ramaswamy et al.",
      "year": 2016,
      "role": "Related Problem",
      "relationship_sentence": "Characterized constraints on convex calibrated surrogates and link design for discrete losses; these insights inform the paper\u2019s approach to building surrogates and links that work for arbitrary discrete target losses."
    },
    {
      "title": "Learning with Fenchel\u2013Young Losses",
      "authors": "Mathieu Blondel et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Defined Fenchel\u2013Young losses from convex regularizers/negentropies and links via convex conjugacy; the present paper directly extends this framework by introducing a new generator\u2014convolutional negentropy\u2014and analyzing its regret-transfer properties."
    },
    {
      "title": "A Regularized Framework for Sparse and Structured Neural Attention",
      "authors": "Vlad Niculae et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Showed how infimal convolution can combine regularizers/entropies to shape prediction operators; this directly inspires the paper\u2019s construction of convolutional negentropy via infimal convolution of generalized negentropies."
    },
    {
      "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification",
      "authors": "Andr\u00e9 F. T. Martins et al.",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated how alternative (generalized) entropies induce different regularized prediction maps and losses; this lineage motivates selecting and composing generalized negentropies to control smoothness while preserving favorable calibration."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core innovation\u2014constructing a convex smooth surrogate with a linear surrogate regret bound for arbitrary discrete target losses\u2014sits at the intersection of two mature threads. First, surrogate-regret and calibration theory (Bartlett\u2013Jordan\u2013McAuliffe, 2006; Tewari\u2013Bartlett, 2007; Reid\u2013Williamson, 2010) formalized how a surrogate\u2019s calibration function and an associated link determine regret transfer. These works also exposed a practical tension: standard smooth convex surrogates (e.g., logistic) typically yield sublinear transfer, while non-smooth margin-based losses can achieve linear transfer. Second, the Fenchel\u2013Young (FY) framework (Blondel\u2013Martins\u2013Niculae, 2019) unified a broad family of losses via convex regularizers/negentropies and provided principled links through convex conjugacy, making it a natural vehicle for designing new surrogates with provable properties.\nThe present paper bridges the perceived gap by engineering the FY generator itself. Building on ideas that infimal convolution can synthesize regularizers with tailored geometry (Niculae\u2013Blondel, 2017), and drawing on the menu of generalized entropies that shape prediction maps (Martins\u2013Astudillo, 2016), the authors introduce a convolutional negentropy. This construction yields a convex smooth FY loss paired with a tailored link that provably attains linear surrogate regret transfer. Insights from multiclass/discrete-loss calibration and dimensional considerations (Ramaswamy\u2013Agarwal, 2016) inform the applicability to arbitrary discrete target losses. In sum, the work directly extends FY losses with a new, carefully composed negentropy to overturn the long-held smoothness\u2013linearity trade-off in surrogate regret bounds.",
  "analysis_timestamp": "2026-01-06T23:08:23.966529"
}
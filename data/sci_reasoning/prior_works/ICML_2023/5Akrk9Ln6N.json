{
  "prior_works": [
    {
      "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
      "authors": "Sergey Levine",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "RPG\u2019s core idea of modeling an RL policy as a generative model over optimal trajectories and optimizing a variational bound directly instantiates the control-as-inference perspective formalized in this work."
    },
    {
      "title": "Learning Continuous Control Policies by Stochastic Value Gradients",
      "authors": "Nicolas Heess et al.",
      "year": 2015,
      "role": "Extension",
      "relationship_sentence": "RPG extends the pathwise (reparameterization) gradient idea of SVG by backpropagating through a learned dynamics model for trajectory-level, latent-conditioned policy optimization rather than stepwise action perturbations."
    },
    {
      "title": "Parameter-Exploring Policy Gradients",
      "authors": "Frank Sehnke et al.",
      "year": 2010,
      "role": "Inspiration",
      "relationship_sentence": "RPG\u2019s policy conditioning on an episode-level latent variable to induce coherent, multimodal exploration is a direct modern instantiation of PGPE\u2019s parameter-based exploration concept."
    },
    {
      "title": "Diffuser: Diffusion Models for Offline Reinforcement Learning",
      "authors": "Michael Janner et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "RPG is inspired by Diffuser\u2019s demonstration that generative modeling of entire trajectories captures multimodality, but moves from offline planning to an online variational policy learning objective tied to optimality."
    },
    {
      "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
      "authors": "Kurtland Chua et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "RPG targets the sampling inefficiency and planner dependence of PETS/CEM-style trajectory optimization by replacing it with a learned, differentiable multimodal trajectory generator optimized via a variational bound."
    },
    {
      "title": "Dream to Control: Learning Behaviors by Latent Imagination",
      "authors": "Danijar Hafner et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "RPG builds on Dreamer\u2019s world-model-based data efficiency, but replaces its unimodal action policy learning with a latent-conditioned, trajectory-level generative policy and corresponding variational objective."
    },
    {
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "authors": "Tuomas Haarnoja et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "RPG explicitly addresses SAC\u2019s limitation of squashed-Gaussian (unimodal) policies by introducing a multimodal, latent-conditioned trajectory policy optimized with reparameterized gradients."
    }
  ],
  "synthesis_narrative": "RPG reframes policy learning as generative modeling of optimal trajectories and optimizes a variational bound to encourage exploration and data efficiency. This framing directly descends from the control-as-inference viewpoint (Levine, 2018), which interprets RL as inference over optimality and naturally yields trajectory distributions and variational objectives. To make this objective trainable end-to-end, RPG leverages pathwise derivatives through a learned dynamics model, extending the reparameterized gradient machinery of Stochastic Value Gradients (Heess et al., 2015) from stepwise action perturbations to a latent-conditioned trajectory generator. The choice to condition the policy on an episode-level latent variable for coherent, multimodal exploration traces to Parameter-Exploring Policy Gradients (Sehnke et al., 2010), which pioneered sampling global parameters to induce consistent behavior across a rollout. Empirically and conceptually, RPG draws inspiration from Diffuser (Janner et al., 2022), which validates that generative models over entire trajectories capture multimodality; RPG adapts this idea to online, model-based RL with an optimality-grounded variational bound. On the model-based side, PETS (Chua et al., 2018) highlighted the strengths of trajectory optimization but also its reliance on expensive sampling-based planners\u2014limitations RPG avoids by learning a differentiable, multimodal trajectory policy. Finally, Dreamer (Hafner et al., 2020) provides the world-model foundation for data-efficient learning that RPG adopts while moving beyond unimodal action parameterizations typified by Soft Actor-Critic (Haarnoja et al., 2018). Together, these works form the direct intellectual lineage enabling RPG\u2019s multimodal, reparameterized trajectory policy learning.",
  "analysis_timestamp": "2026-01-06T23:09:26.580703"
}
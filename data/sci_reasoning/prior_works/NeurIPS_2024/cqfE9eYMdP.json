{
  "prior_works": [
    {
      "title": "GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems",
      "authors": "Yousef Saad; Martin H. Schultz",
      "year": 1986,
      "role": "Foundational Krylov subspace solver that NeurKItt accelerates by augmenting the iteration space.",
      "relationship_sentence": "NeurKItt plugs a learned invariant subspace into the GMRES-style Krylov process to reduce iterations relative to the classical baseline introduced by Saad and Schultz."
    },
    {
      "title": "GMRES with Deflated Restarting",
      "authors": "Ronald B. Morgan",
      "year": 2002,
      "role": "Established that augmenting/restarting GMRES with approximate eigenvectors (invariant subspace) dramatically improves convergence.",
      "relationship_sentence": "NeurKItt generalizes Morgan\u2019s deflation/augmentation idea by using a neural operator to predict the invariant subspace a priori, instead of extracting it purely from Krylov iterates."
    },
    {
      "title": "Deflation of Conjugate Gradients with Applications to Boundary Value Problems",
      "authors": "R. A. Nicolaides",
      "year": 1987,
      "role": "Introduced deflation for Krylov methods, showing how removing components along troublesome eigenvectors accelerates convergence.",
      "relationship_sentence": "NeurKItt\u2019s core mechanism\u2014leveraging a predicted invariant subspace to suppress slow spectral components\u2014implements a data-driven deflation strategy rooted in Nicolaides\u2019 seminal framework."
    },
    {
      "title": "Truncation Strategies for Optimal Krylov Subspace Methods",
      "authors": "Erik de Sturler",
      "year": 1999,
      "role": "Developed augmented/restarted Krylov schemes (e.g., GCROT) that preserve or inject important subspace directions to sustain fast convergence.",
      "relationship_sentence": "By supplying an externally predicted subspace, NeurKItt operationalizes de Sturler\u2019s augmentation principle without relying solely on information gathered during the current iteration."
    },
    {
      "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
      "authors": "Zongyi Li; Nikola Kovachki; Kamyar Azizzadenesheli; Burigede Liu; Kaushik Bhattacharya; Andrew Stuart; Anima Anandkumar",
      "year": 2021,
      "role": "Pioneered neural operators that learn mappings between infinite-dimensional function spaces and generalize across problem instances.",
      "relationship_sentence": "NeurKItt adopts the neural-operator paradigm to map problem inputs to an invariant subspace, echoing FNO\u2019s operator-learning approach for rapid, generalizable predictions."
    },
    {
      "title": "Learning Operators: An Emerging Paradigm for Modeling Parametric PDEs",
      "authors": "Lu Lu; Pengzhan Jin; George E. Karniadakis",
      "year": 2021,
      "role": "Introduced DeepONet and formalized operator learning as a supervised framework for predicting solutions/operators across parameterized families.",
      "relationship_sentence": "The paper\u2019s use of a neural operator to predict subspaces is conceptually grounded in DeepONet\u2019s operator-learning framework for out-of-sample generalization."
    },
    {
      "title": "The Geometry of Algorithms with Orthogonality Constraints",
      "authors": "Alan Edelman; Tom\u00e1s A. Arias; Steven T. Smith",
      "year": 1998,
      "role": "Characterized Stiefel/Grassmann geometry and projection metrics for subspaces, underpinning orthonormalization and subspace-distance losses.",
      "relationship_sentence": "NeurKItt\u2019s QR-based orthonormalization and projection loss for subspace training align with Grassmannian principles and projection-based distances articulated by Edelman et al."
    }
  ],
  "synthesis_narrative": "NeurKItt\u2019s core insight\u2014accelerating Krylov iterations by injecting an invariant subspace predicted by a neural operator\u2014stands at the intersection of classical deflation/augmentation and modern operator learning. Foundationally, GMRES provided the Krylov framework that NeurKItt aims to accelerate. The deflation lineage (Nicolaides) and augmentation/restarted strategies (Morgan\u2019s GMRES-DR; de Sturler\u2019s GCROT-style truncation) established that isolating and reusing approximate invariant subspaces can dramatically reduce iterations by neutralizing slow spectral modes. NeurKItt adopts this very mechanism but replaces on-the-fly spectral extraction with a learned predictor, enabling subspace availability at iteration start and potential generalization across problem families.\n\nThe neural-operator works (FNO; DeepONet) supply the blueprint for mapping problem parameters to operator outputs that generalize beyond the training set. NeurKItt leverages this paradigm specifically to predict an invariant subspace of the system matrix, turning operator learning into a vehicle for deflation/augmentation. To train robust subspace predictors, the method enforces orthonormality via QR and employs a projection-based loss to compare subspaces, practices grounded in Grassmannian geometry (Edelman\u2013Arias\u2013Smith). This combination yields orthonormal bases with losses invariant to basis rotations, aligning training with the subspace objective. Altogether, NeurKItt fuses operator learning with classical Krylov acceleration techniques, delivering a learned deflation/augmentation module that reduces iteration counts while retaining the reliability of established Krylov solvers.",
  "analysis_timestamp": "2026-01-06T23:33:36.257484"
}
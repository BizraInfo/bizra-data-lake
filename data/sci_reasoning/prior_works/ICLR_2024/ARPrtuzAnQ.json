{
  "prior_works": [
    {
      "title": "Efficient noise-tolerant learning from statistical queries",
      "authors": "Michael Kearns",
      "year": 1998,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Introduces the Statistical Query (SQ) framework that underpins the correlational SQ (CSQ) model used to formalize the gradient-descent\u2013encompassing hardness results in this paper."
    },
    {
      "title": "A general characterization of the statistical query complexity",
      "authors": "Vitaly Feldman",
      "year": 2012,
      "arxiv_id": "arXiv:1208.3535",
      "role": "Extension",
      "relationship_sentence": "Provides the modern SQ/CSQ lower-bound machinery and indistinguishability tools that this paper instantiates within symmetry-constrained hypothesis classes to derive superpolynomial and exponential lower bounds."
    },
    {
      "title": "The Complexity of Learning Neural Networks via Statistical Queries",
      "authors": "Amit Daniely",
      "year": 2017,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Establishes exponential CSQ lower bounds for learning shallow fully-connected networks, directly motivating this paper\u2019s central question of whether symmetry (e.g., parameter sharing/group actions) can circumvent such SQ barriers."
    },
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco S. Cohen and Max Welling",
      "year": 2016,
      "arxiv_id": "arXiv:1602.07576",
      "role": "Foundation",
      "relationship_sentence": "Introduces the group-equivariant CNN paradigm that this paper targets, with CSQ lower bounds showing that learning shallow instances of such equivariant CNNs remains hard despite built-in symmetry."
    },
    {
      "title": "Deep Sets",
      "authors": "Manzil Zaheer et al.",
      "year": 2017,
      "arxiv_id": "arXiv:1703.06114",
      "role": "Foundation",
      "relationship_sentence": "Provides the canonical permutation-invariant architecture (sum/mean pooling over sets) that underlies the permutation-invariant networks analyzed here via CSQ lower bounds (including frame-averaged constructions)."
    },
    {
      "title": "Invariant and Equivariant Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "arxiv_id": "arXiv:1812.09902",
      "role": "Foundation",
      "relationship_sentence": "Develops invariant polynomial/tensor constructions for permutation groups, which this work directly targets by proving CSQ hardness for learning such invariant polynomial networks."
    },
    {
      "title": "Equivariance Through Parameter-Sharing",
      "authors": "Siamak Ravanbakhsh et al.",
      "year": 2017,
      "arxiv_id": "arXiv:1706.03047",
      "role": "Foundation",
      "relationship_sentence": "Formalizes enforcing group symmetries via parameter-sharing and averaging, the exact frame-averaged mechanism whose CSQ learning hardness is established for permutation subgroups in this paper."
    }
  ],
  "synthesis_narrative": "The statistical query paradigm of Kearns formalized algorithms that access data through expectations rather than individual examples, laying the groundwork for correlational SQ analyses. Feldman\u2019s characterization supplied the modern lower-bound machinery\u2014based on indistinguishability of carefully crafted distributions in the CSQ model\u2014that has become the standard way to argue that whole families of learning procedures, including gradient-based ones, cannot succeed under certain regimes. Building on this, Daniely demonstrated exponential CSQ lower bounds for learning shallow fully-connected neural networks, crystallizing a precise computational barrier for gradient-descent\u2013like learners. In parallel, Cohen and Welling introduced group-equivariant convolutional networks, showing how group structure can be embedded via weight tying and group convolution; Zaheer and colleagues proposed Deep Sets, the canonical architecture for permutation-invariant learning; and Maron and collaborators developed invariant and equivariant graph networks using polynomial/tensor representations of permutation groups. Ravanbakhsh et al. unified these themes through parameter sharing and explicit group-averaging constructions to guarantee equivariance.\nThese strands collectively posed a natural question: might symmetry-aware architectures evade CSQ hardness known for non-symmetric shallow networks? By combining Feldman\u2019s CSQ lower-bound toolkit with the concrete symmetry mechanisms from equivariant CNNs, Deep Sets, invariant polynomials, and parameter-sharing/frame averaging, the present work shows that symmetry does not lift the barrier\u2014proving superpolynomial or exponential CSQ lower bounds for learning shallow GNNs, CNNs, invariant polynomial networks, and frame-averaged models for permutation subgroups. Thus, the synthesis of CSQ hardness techniques with precise equivariant constructions reveals an inherent computational limitation persisting even under known symmetries.",
  "target_paper": {
    "title": "On the hardness of learning under symmetries",
    "authors": "Bobak Kiani, Thien Le, Hannah Lawrence, Stefanie Jegelka, Melanie Weber",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Equivariance, statistical query, lower bound, computational hardness, invariance, symmetry, neural networks",
    "abstract": "We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known  symmetries (\"equivariance\") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative. In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimen",
    "openreview_id": "ARPrtuzAnQ",
    "forum_id": "ARPrtuzAnQ"
  },
  "analysis_timestamp": "2026-01-06T16:56:23.660654"
}
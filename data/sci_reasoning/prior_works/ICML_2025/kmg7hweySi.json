{
  "prior_works": [
    {
      "title": "A Fast Fixed-Point Algorithm for Independent Component Analysis",
      "authors": "Aapo Hyv\u00e4rinen et al.",
      "year": 1997,
      "role": "Baseline",
      "relationship_sentence": "This paper analyzes the exact algorithm (FastICA) that the current work studies, and the new d^4 sample complexity lower bound is a direct performance characterization of FastICA on a canonical one-non-Gaussian-direction model."
    },
    {
      "title": "Independent Component Analysis: A New Concept?",
      "authors": "Pierre Comon et al.",
      "year": 1994,
      "role": "Foundation",
      "relationship_sentence": "Comon\u2019s identifiability theory established non-Gaussianity as the core criterion for ICA, directly grounding the paper\u2019s formulation of recovering a non-Gaussian direction and the use of kurtosis/negentropy-type contrasts."
    },
    {
      "title": "The \u2018Independent Components\u2019 of Natural Scenes Are Edge Filters",
      "authors": "Anthony J. Bell et al.",
      "year": 1997,
      "role": "Inspiration",
      "relationship_sentence": "By showing that ICA on natural images yields Gabor-like edge filters akin to early visual features, this work directly motivates using ICA as a principled model for first-layer feature learning in deep networks."
    },
    {
      "title": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution",
      "authors": "Anthony J. Bell et al.",
      "year": 1995,
      "role": "Extension",
      "relationship_sentence": "InfoMax casts ICA as gradient-based optimization of a non-Gaussianity objective, providing the precise SGD-style learning framework that the present paper analyzes and contrasts with FastICA in high dimensions."
    },
    {
      "title": "Natural Gradient Works Efficiently in Learning",
      "authors": "Shun-ichi Amari et al.",
      "year": 1998,
      "role": "Related Problem",
      "relationship_sentence": "Amari\u2019s natural gradient methods yield efficient online/SGD-like updates for ICA, directly informing the paper\u2019s analysis of stochastic optimization dynamics for non-Gaussian feature recovery."
    },
    {
      "title": "Independent Component Analysis by General Nonlinear Hebbian Learning Rules",
      "authors": "Aapo Hyv\u00e4rinen et al.",
      "year": 1998,
      "role": "Extension",
      "relationship_sentence": "This work formalizes ICA as stochastic (Hebbian/SGD-like) learning on non-Gaussianity contrasts, which the paper explicitly leverages to study how SGD recovers non-Gaussian directions versus FastICA."
    },
    {
      "title": "Tensor Decompositions for Learning Latent Variable Models",
      "authors": "Animashree Anandkumar et al.",
      "year": 2014,
      "role": "Gap Identification",
      "relationship_sentence": "Provable tensor/moment methods achieve polynomial sample complexity for ICA, highlighting the lack of rigorous high-dimensional guarantees for popular heuristics like FastICA that this paper addresses with a d^4 lower bound."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014precisely characterizing high-dimensional feature learning from non-Gaussian inputs by contrasting FastICA with SGD\u2014rests on the ICA framework established by Comon, who showed that non-Gaussianity enables identifiability and thereby defined the problem the authors study. Hyv\u00e4rinen and Oja\u2019s FastICA supplies the baseline algorithm under scrutiny; the present work targets its performance head-on, proving a d^4 sample lower bound for recovering a single non-Gaussian direction in a clean synthetic model.\n\nThe choice of ICA as a model for early feature learning is not incidental: Bell and Sejnowski\u2019s seminal observation that ICA on natural images produces edge-like filters directly motivates the paper\u2019s thesis that ICA can illuminate how deep networks learn first-layer structure from non-Gaussian data. At the optimization level, the analysis of SGD is grounded in gradient-based ICA developments\u2014InfoMax and Amari\u2019s natural gradient\u2014together with Hyv\u00e4rinen and Oja\u2019s nonlinear Hebbian learning view, which instantiate ICA as stochastic learning on non-Gaussianity contrasts. These works supply the precise SGD-style dynamics and objectives the authors evaluate.\n\nFinally, tensor/moment methods by Anandkumar and colleagues provide a contrasting line with provable polynomial sample complexity for ICA, underscoring a key gap: widely used heuristics like FastICA lacked rigorous high-dimensional sample guarantees. Addressing this gap, the paper quantifies FastICA\u2019s sample demands and situates SGD\u2019s behavior within the same non-Gaussian feature learning framework, linking data structure to optimization at scale.",
  "analysis_timestamp": "2026-01-06T23:07:19.566138"
}
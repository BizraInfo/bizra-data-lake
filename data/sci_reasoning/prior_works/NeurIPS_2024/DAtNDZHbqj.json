{
  "prior_works": [
    {
      "title": "Reinforcement Learning and Control as Probabilistic Inference: A Tutorial",
      "authors": "Sergey Levine",
      "year": 2018,
      "role": "Conceptual foundation (RL-as-inference)",
      "relationship_sentence": "VDPO\u2019s reformulation of delayed RL as a variational inference problem directly builds on the RL-as-probabilistic-inference perspective articulated by Levine, motivating the use of variational objectives and KL-regularized policy updates."
    },
    {
      "title": "Deep Variational Reinforcement Learning",
      "authors": "Malte Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, Shimon Whiteson",
      "year": 2018,
      "role": "Methodological precedent for variational treatment of partial observability",
      "relationship_sentence": "DVRL demonstrated that variational latent-state modeling can tackle POMDPs efficiently; VDPO adapts this variational lens to delayed observations, but replaces heavy latent dynamics learning with a two-step scheme (TD in a small delay-free space + supervised cloning)."
    },
    {
      "title": "Human-level control through deep reinforcement learning",
      "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, et al.",
      "year": 2015,
      "role": "Baseline practice of state augmentation",
      "relationship_sentence": "DQN\u2019s frame-stacking popularized history/state augmentation to recover (approximate) Markovianity; VDPO is motivated by the inefficiency of TD learning when such augmentation explodes with delay and proposes a variational workaround."
    },
    {
      "title": "Deep Recurrent Q-Learning for Partially Observable MDPs",
      "authors": "Matthew Hausknecht, Peter Stone",
      "year": 2015,
      "role": "Alternate POMDP approach highlighting trade-offs",
      "relationship_sentence": "DRQN addressed partial observability (including delays) via recurrence instead of explicit augmentation; VDPO contrasts with such TD-based solutions by shifting most learning burden to a supervised cloning step to improve sample efficiency."
    },
    {
      "title": "Guided Policy Search",
      "authors": "Sergey Levine, Vladlen Koltun",
      "year": 2013,
      "role": "Two-stage RL + supervised learning blueprint",
      "relationship_sentence": "GPS alternates solving an easier control problem and distilling it into a policy via supervised learning; VDPO mirrors this teacher\u2013student decomposition by doing TD learning in a compact delay-free setting and then behavior cloning to act under delay."
    },
    {
      "title": "Policy Distillation",
      "authors": "Andrei A. Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell",
      "year": 2015,
      "role": "Supervised transfer of policies",
      "relationship_sentence": "VDPO\u2019s second step\u2014behavior cloning from targets produced by TD in a simpler model\u2014parallels distillation\u2019s supervised transfer of a teacher\u2019s action preferences to a student policy."
    },
    {
      "title": "ALVINN: An Autonomous Land Vehicle in a Neural Network",
      "authors": "Dean A. Pomerleau",
      "year": 1989,
      "role": "Foundational behavior cloning",
      "relationship_sentence": "VDPO relies on behavior cloning as a data-efficient supervised learning primitive; this traces directly to the classic idea of training policies by imitating action labels, here generated by the delay-free TD learner."
    }
  ],
  "synthesis_narrative": "VDPO\u2019s core insight\u2014casting delayed-observation RL as variational inference and solving it via a two-step loop of TD learning in a compact, delay-free environment followed by behavior cloning\u2014sits at the intersection of three direct lines of work. First, the RL-as-inference view (Levine) provides the conceptual scaffolding for formulating control as variational optimization, enabling VDPO to replace monolithic TD in a large augmented space with an inference-driven decomposition. Second, prior variational approaches to partial observability (Igl et al.) demonstrated that variational latent-state modeling can mitigate POMDP challenges; VDPO adopts this lens to delays but deliberately avoids learning a full latent dynamics model, achieving efficiency by confining TD to a small state space. Third, a lineage of supervised policy learning informs VDPO\u2019s second stage: early behavior cloning (Pomerleau) establishes the efficiency of supervised imitation; guided policy search (Levine & Koltun) and policy distillation (Rusu et al.) show that policies can be trained by imitating a teacher that solves an easier problem. Against this backdrop, common remedies for delay\u2014state augmentation (popularized by DQN) or recurrent TD methods (DRQN)\u2014highlight the performance and sample-efficiency pitfalls of bootstrapped learning in high-dimensional history spaces, precisely the inefficiency VDPO circumvents. By marrying the inference perspective with teacher\u2013student supervision, VDPO inherits the statistical efficiency of supervised learning while retaining the performance benefits of TD evaluation, yielding improved sample complexity under observation delays.",
  "analysis_timestamp": "2026-01-06T23:33:35.561183"
}
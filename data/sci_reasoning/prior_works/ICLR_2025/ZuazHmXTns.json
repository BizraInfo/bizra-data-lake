{
  "prior_works": [
    {
      "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
      "authors": "H. Brendan McMahan et al.",
      "year": 2017,
      "arxiv_id": "1602.05629",
      "role": "Foundation",
      "relationship_sentence": "Established the federated averaging paradigm and partial participation setting that the new algorithm operates within and seeks to improve upon under nonconvex objectives."
    },
    {
      "title": "Federated Optimization in Heterogeneous Networks (FedProx)",
      "authors": "Tian Li et al.",
      "year": 2020,
      "arxiv_id": "1812.06127",
      "role": "Gap Identification",
      "relationship_sentence": "Introduced a proximal correction to mitigate client drift under data heterogeneity but remained highly sensitive to the choice of stepsize and proximal coefficient, underscoring the need for problem-parameter-free methods."
    },
    {
      "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning",
      "authors": "Sai Praneeth Karimireddy et al.",
      "year": 2020,
      "arxiv_id": "1910.06378",
      "role": "Gap Identification",
      "relationship_sentence": "Addressed client drift via control variates but still required carefully tuned learning rates and algorithmic constants, highlighting the limitation of parameter sensitivity in heterogeneity-robust FL."
    },
    {
      "title": "Adaptive Federated Optimization",
      "authors": "Sashank J. Reddi et al.",
      "year": 2021,
      "arxiv_id": "2003.00295",
      "role": "Baseline",
      "relationship_sentence": "Proposed FedAdam/FedYogi, combining server momentum with adaptive preconditioning for FL; the new method retains this momentum+adaptivity template but removes dependence on problem-specific stepsize and momentum parameters."
    },
    {
      "title": "Stochastic Polyak Step-Size for SGD: Almost Sure Convergence",
      "authors": "Konstantinos Loizou et al.",
      "year": 2021,
      "arxiv_id": "2002.10542",
      "role": "Inspiration",
      "relationship_sentence": "Provided a loss- and gradient-norm\u2013based, problem-parameter-free adaptive stepsize (SPS/SPS+) that directly inspires the paper\u2019s stepsize rule adapted to the federated, nonconvex setting."
    },
    {
      "title": "AdaGrad-Norm: Robust Stochastic Optimization to Noise Variance",
      "authors": "Rachel Ward et al.",
      "year": 2019,
      "arxiv_id": "1808.06293",
      "role": "Inspiration",
      "relationship_sentence": "Showed that normalizing updates by accumulated gradient norms yields learning-rate robustness, a principle the paper leverages in designing a global stepsize that adapts without problem-specific tuning under partial participation."
    },
    {
      "title": "SlowMo: Improving Communication-Efficient Distributed SGD",
      "authors": "Priya Goyal et al.",
      "year": 2020,
      "arxiv_id": "1910.00643",
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated that server-side momentum stabilizes and accelerates local/periodic averaging methods, motivating the paper\u2019s use of momentum but with automatically chosen, parameter-free momentum dynamics."
    }
  ],
  "synthesis_narrative": "Federated averaging introduced the core distributed learning protocol with local steps and partial participation, setting the stage for nonconvex training over heterogeneous clients. To counteract client drift from heterogeneity, FedProx added a proximal term, yet its success hinged on tuning both the stepsize and proximal coefficient. SCAFFOLD corrected drift using control variates, but similarly required carefully selected learning rates and algorithmic constants to realize its theoretical benefits. Moving toward adaptive optimization, Adaptive Federated Optimization (FedAdam/FedYogi) brought server-side momentum and adaptive preconditioning to FL, improving stability but remaining sensitive to global learning rate and momentum hyperparameters, especially under non-IID data and sporadic client participation. In parallel, the stochastic Polyak step-size (SPS/SPS+) offered a loss- and gradient-norm\u2013based rule that dispenses with problem-specific parameters, while AdaGrad-Norm showed that normalizing by accumulated gradient norms yields robustness to noise without delicate learning-rate schedules. Complementing these, SlowMo established that server momentum can stabilize and accelerate decentralized/local-SGD style training under infrequent synchronization. Together, these works suggest that momentum and adaptive scaling are crucial for robust FL under heterogeneity, but prevailing methods still depend on problem-specific hyperparameters; meanwhile, parameter-free step-size rules exist in centralized stochastic optimization but had not been fused with FL\u2019s aggregation and partial participation. The present work synthesizes these insights by coupling a problem-parameter-free, loss/gradient-norm\u2013driven stepsize with automatically managed momentum at the server, preserving the heterogeneity robustness of adaptive federated optimizers while eliminating reliance on hand-tuned problem parameters and providing convergence guarantees in the nonconvex FL regime.",
  "target_paper": {
    "title": "Problem-Parameter-Free Federated Learning",
    "authors": "Wenjing Yan, Kai Zhang, Xiaolu Wang, Xuanyu Cao",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Adaptive federated learning, problem-parameter free, arbitrary data heterogeneity, adaptive stepsize",
    "abstract": "Federated learning (FL) has garnered significant attention from academia and industry in recent years due to its advantages in data privacy, scalability, and communication efficiency. However, current FL algorithms face a critical limitation: their performance heavily depends on meticulously tuned hyperparameters, particularly the learning rate or stepsize. This manual tuning process is challenging in federated settings due to data heterogeneity and limited accessibility of local datasets. Consequently, the reliance on problem-specific parameters hinders the widespread adoption of FL and potentially compromises its performance in dynamic or diverse environments. To address this issue, we introduce PAdaMFed, a novel algorithm for nonconvex FL that carefully combines adaptive stepsize and momentum techniques. PAdaMFed offers two key advantages: 1) it operates autonomously without relying on problem-specific parameters; and 2) it manages data heterogeneity and partial participation withou",
    "openreview_id": "ZuazHmXTns",
    "forum_id": "ZuazHmXTns"
  },
  "analysis_timestamp": "2026-01-06T18:01:08.979128"
}
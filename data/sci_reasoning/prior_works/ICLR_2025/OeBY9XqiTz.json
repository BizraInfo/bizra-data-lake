{
  "prior_works": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "authors": "Albert Gu; Tri Dao",
      "year": 2023,
      "role": "Method",
      "relationship_sentence": "Introduced selective state-space models enabling content-dependent gating and linear-time long-range sequence modeling; Samba builds on this mechanism by instantiating and synchronizing multiple selective SSMs\u2014one per tracklet\u2014to maintain shared long-term memory across objects."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces (S4)",
      "authors": "Albert Gu; Karan Goel; Christopher R\u00e9",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Provided the core SSM formulation and stability/efficiency foundations for long-context modeling that underpin selective SSMs; Samba leverages this SSM lineage to stably encode long-term dependencies within each tracklet."
    },
    {
      "title": "MOTR: End-to-End Multiple-Object Tracking with Transformer",
      "authors": "Fangao Zeng et al.",
      "year": 2022,
      "role": "Method",
      "relationship_sentence": "Pioneered tracking-by-propagation with persistent track queries; SambaMOTR directly integrates Samba\u2019s set-of-sequences memory to autoregressively predict future track queries and handle occlusions within this framework."
    },
    {
      "title": "TrackFormer: Multi-Object Tracking with Transformers",
      "authors": "Tim Meinhardt et al.",
      "year": 2022,
      "role": "Method",
      "relationship_sentence": "Established query propagation for per-object trajectories and attention-based temporal linking; Samba adopts the notion of per-track autoregression while replacing heavy attention with synchronized linear-time SSMs across tracks."
    },
    {
      "title": "CenterTrack: Tracking Objects as Points",
      "authors": "Xingyi Zhou; Vladlen Koltun; Philipp Kr\u00e4henb\u00fchl",
      "year": 2020,
      "role": "Method",
      "relationship_sentence": "Showed that explicit temporal propagation of object states improves MOT under occlusion; Samba generalizes this idea with long-horizon, memory-driven propagation via SSMs rather than short-range motion cues."
    },
    {
      "title": "Deep Sets",
      "authors": "Manzil Zaheer; Satwik Kottur; Siamak Ravanbakhsh; Barnab\u00e1s P\u00f3czos; Ruslan Salakhutdinov; Alexander J. Smola",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduced permutation-invariant modeling for sets; Samba\u2019s synchronized set-of-sequences design treats multiple tracklets as an order-agnostic set, coupling them through shared memory while preserving permutation invariance across tracks."
    },
    {
      "title": "DanceTrack: Multiple Object Tracking in Uniform Appearance",
      "authors": "Peize Sun et al.",
      "year": 2022,
      "role": "Dataset/Benchmark",
      "relationship_sentence": "Highlighted coordinated motion, frequent occlusion, and weak appearance cues as key MOT challenges; directly motivates Samba\u2019s emphasis on long-term dependencies and inter-track synchronization to succeed on such scenarios."
    }
  ],
  "synthesis_narrative": "Samba\u2019s core contribution\u2014synchronized set-of-sequences modeling for MOT\u2014emerges from the convergence of linear-time state-space sequence models and query-propagation trackers. The S4 family established a stable, efficient formalism for encoding long-range dependencies, later advanced by Mamba\u2019s selective, content-conditioned scanning to achieve linear-time sequence modeling with dynamic gating. Samba directly leverages this selective SSM machinery but innovates by instantiating one SSM per tracklet and synchronizing their states, enabling shared long-term memory across objects while retaining linear complexity. On the MOT side, TrackFormer and MOTR introduced persistent track queries and tracking-by-propagation, providing the interface through which Samba\u2019s autoregressive predictions can replace costly attention-based temporal linking. CenterTrack further demonstrated the value of explicit temporal propagation for robustness under occlusions, a capability Samba extends from short-range motion cues to long-horizon memory via SSM dynamics. Conceptually, treating multiple tracklets as a set and coupling them through synchronized memory aligns with the permutation-invariant principles of Deep Sets, ensuring the model operates independently of track order. Finally, the DanceTrack benchmark crystallized the need for robust modeling of coordinated motion and long occlusions, directly shaping Samba\u2019s design objectives. Together, these works lead to a tracker that captures intra-track long-term dependencies, inter-track interactions, and temporal occlusions efficiently within a unified, linear-time set-of-sequences framework.",
  "analysis_timestamp": "2026-01-07T00:02:04.904790"
}
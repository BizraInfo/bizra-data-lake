{
  "prior_works": [
    {
      "title": "Semi-Supervised Learning by Entropy Minimization",
      "authors": [
        "Yves Grandvalet",
        "Yoshua Bengio"
      ],
      "year": 2005,
      "role": "Foundational objective underpinning modern entropy-minimization\u2013based adaptation",
      "relationship_sentence": "The paper\u2019s central critique\u2014that entropy minimization alone is insufficient and needs discriminative guidance\u2014directly builds on Grandvalet and Bengio\u2019s principle, reframing it through an energy/likelihood lens and showing when entropy reduction fails to reduce energy."
    },
    {
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "authors": [
        "Dequan Wang",
        "Evan Shelhamer",
        "Shaoteng Liu",
        "Trevor Darrell"
      ],
      "year": 2021,
      "role": "Canonical TTA method and primary baseline relying solely on prediction-entropy minimization",
      "relationship_sentence": "This work motivates the re-examination in the paper by exemplifying how TTA commonly minimizes entropy; the new analysis shows why such updates need not lower energy (likelihood) and proposes an energy-grounded alternative."
    },
    {
      "title": "Test-Time Training with Self-Supervision",
      "authors": [
        "Yu Sun",
        "Xiaolong Wang",
        "Zhuang Liu",
        "John Miller",
        "Trevor Darrell",
        "Alexei A. Efros"
      ],
      "year": 2020,
      "role": "Early TTA framework emphasizing auxiliary discriminative signals at test time",
      "relationship_sentence": "By demonstrating that explicit auxiliary tasks guide stable adaptation, this work foreshadows the paper\u2019s claim that entropy minimization requires discriminative guidance to reach low-entropy, low-energy solutions."
    },
    {
      "title": "Energy-based Out-of-Distribution Detection",
      "authors": [
        "Weitang Liu",
        "Xiaoyun Wang",
        "John D. Owens",
        "Yixuan Li"
      ],
      "year": 2020,
      "role": "Establishes energy scores from logits and connects energy to likelihood for reliability under shift",
      "relationship_sentence": "The paper leverages the energy-as-likelihood proxy introduced here to argue that reducing energy is essential for reliable observability, motivating its likelihood-based, energy-shaping objective for TTA."
    },
    {
      "title": "A Tutorial on Energy-Based Learning",
      "authors": [
        "Yann LeCun",
        "Sumit Chopra",
        "Raia Hadsell"
      ],
      "year": 2006,
      "role": "Conceptual foundation for energy-based models and energy landscape shaping",
      "relationship_sentence": "The proposed method\u2019s design\u2014explicitly reshaping the energy landscape to favor test samples\u2014draws on EBM principles outlined in this tutorial, grounding the method\u2019s objective and interpretation."
    },
    {
      "title": "SHOT: Source Hypothesis Transfer for Unsupervised Domain Adaptation",
      "authors": [
        "Jian Liang",
        "Dapeng Hu",
        "Jiashi Feng"
      ],
      "year": 2020,
      "role": "Source-free adaptation via information maximization and entropy-related objectives",
      "relationship_sentence": "SHOT\u2019s reliance on entropy/information maximization without source data informs the paper\u2019s analysis of when entropy-driven objectives stall, bolstering the case for energy-aware, likelihood-driven guidance."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014recasting test-time adaptation (TTA) through an energy\u2013entropy duality and introducing a likelihood-based, energy-shaping objective\u2014sits at the intersection of two strands: entropy-minimization\u2013driven adaptation and energy-based modeling. Grandvalet and Bengio (2005) provide the foundational entropy-minimization principle that has been widely adopted in adaptation. Tent (Wang et al., 2021) crystallizes this paradigm for TTA, serving as the primary target of the paper\u2019s critique by showing how test-time updates that only minimize prediction entropy can be unstable and need not reflect improved likelihood.\n\nIn contrast, energy-based works supply the missing likelihood perspective. LeCun et al. (2006) formalize energy landscapes and how shaping them can encode preferences over data, giving theoretical footing for the paper\u2019s proposal to directly manipulate energy during adaptation. Liu et al. (2020) operationalize energy from classifier logits and link it to likelihood for OOD detection, a crucial bridge enabling the paper\u2019s claim that energy is a practical proxy for observability under the learned distribution.\n\nComplementing these, Sun et al. (2020) show that explicit discriminative/self-supervised signals at test time stabilize adaptation, resonating with the paper\u2019s finding that entropy minimization alone cannot reliably reach zero entropy without guidance. Finally, SHOT (Liang et al., 2020) demonstrates the limits of entropy/information maximization in source-free settings, motivating the proposed dual-objective view where reducing both entropy and energy is essential for robust TTA.",
  "analysis_timestamp": "2026-01-07T00:21:32.290616"
}
{
  "prior_works": [
    {
      "title": "Generalization through Memorization: Nearest Neighbor Language Models",
      "authors": "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer",
      "year": 2020,
      "role": "Semi-parametric retrieval baseline and mechanism",
      "relationship_sentence": "REGENT\u2019s use of a strong 1-NN baseline and its semi-parametric policy that conditions on retrieved neighbors directly mirrors kNN-LM\u2019s core insight that nearest-neighbor retrieval can substantially boost generalization without additional parameter scaling."
    },
    {
      "title": "Improving Language Models by Retrieving from Trillions of Tokens (RETRO)",
      "authors": "Sebastian Borgeaud et al.",
      "year": 2022,
      "role": "Architectural inspiration for retrieval-conditioned transformers",
      "relationship_sentence": "RETRO\u2019s strategy of concatenating retrieved neighbors to a transformer\u2019s context informs REGENT\u2019s design of training a transformer policy on sequences of queries plus retrieved trajectories for in-context adaptation."
    },
    {
      "title": "Model-Free Episodic Control",
      "authors": "Charles Blundell et al.",
      "year": 2016,
      "role": "Nearest-neighbor decision-making for fast adaptation in RL",
      "relationship_sentence": "REGENT\u2019s observation that a simple 1-NN agent is a surprisingly strong baseline echoes episodic control\u2019s kNN value lookups for rapid, no-gradient adaptation in novel environments."
    },
    {
      "title": "Neural Episodic Control",
      "authors": "Alex Pritzel et al.",
      "year": 2017,
      "role": "Episodic memory with kNN retrieval integrated into deep RL",
      "relationship_sentence": "REGENT extends the semi-parametric RL lineage of NEC by leveraging retrieval over stored experiences to guide action selection while coupling it with a learned transformer policy."
    },
    {
      "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
      "authors": "Lili Chen et al.",
      "year": 2021,
      "role": "Transformer-based sequence modeling of trajectories",
      "relationship_sentence": "REGENT adopts the transformer-as-policy paradigm from Decision Transformer and augments it with explicit retrieval context to enable in-context control without finetuning."
    },
    {
      "title": "A Generalist Agent (Gato)",
      "authors": "Scott Reed et al.",
      "year": 2022,
      "role": "Scaling-based generalist agent baseline and motivation",
      "relationship_sentence": "By showing generalist abilities through large-scale training, Gato provides the scaling-first contrast that REGENT challenges with a retrieval-biased, smaller semi-parametric policy."
    },
    {
      "title": "RT-1: Robotics Transformer for Real-World Control at Scale",
      "authors": "Anthony Brohan et al.",
      "year": 2022,
      "role": "Robotics generalist transformer baseline and datapoint for scale",
      "relationship_sentence": "RT-1 exemplifies the dominant scale-driven approach in robotics, against which REGENT demonstrates competitive generalization using retrieval augmentation with far fewer parameters."
    }
  ],
  "synthesis_narrative": "REGENT\u2019s central idea\u2014biasing a compact policy toward fast adaptation via retrieval and in-context control\u2014sits at the intersection of semi-parametric retrieval and transformer-based decision making. The nearest-neighbor literature directly motivates its core mechanisms: Model-Free Episodic Control and Neural Episodic Control showed that kNN lookups over episodic memories enable rapid, gradient-free adaptation in RL, foreshadowing REGENT\u2019s strong 1-NN baseline and its reliance on memory-based querying in new environments. From NLP, kNN-LM established that semi-parametric retrieval can markedly improve generalization with minimal additional parameters, a principle REGENT ported to control. RETRO further provided a concrete blueprint for how to condition transformers on retrieved neighbors, inspiring REGENT\u2019s training on sequences of queries plus retrieved trajectories to realize in-context adaptation without finetuning.\n\nOn the policy architecture side, Decision Transformer demonstrated that transformers can model trajectories and act as policies via sequence modeling, providing the scaffolding for REGENT\u2019s in-context action selection. Finally, generalist agents like Gato and RT-1 embodied the prevailing scale-first strategy across diverse tasks and embodiments; REGENT explicitly positions retrieval as a more sample- and parameter-efficient alternative, showing that augmenting a smaller transformer with retrieval can rival or surpass larger monolithic policies. Together, these works directly shape REGENT\u2019s semi-parametric design, its emphasis on 1-NN as a surprisingly strong baseline, and its practical recipe\u2014retrieval-augmented, transformer-based in-context control\u2014for rapid generalization to unseen robotics and game environments.",
  "analysis_timestamp": "2026-01-06T23:42:48.090712"
}
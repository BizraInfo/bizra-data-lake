{
  "prior_works": [
    {
      "title": "Improving generalization for temporal difference learning: The successor representation",
      "authors": "Peter Dayan",
      "year": 1993,
      "role": "Foundational theory of SR and the transition\u2013reward factorization",
      "relationship_sentence": "The paper\u2019s distributional successor measure (SM) directly generalizes Dayan\u2019s successor representation by replacing expectation over future occupancies with a full distribution over consequences, preserving the clean separation between transition dynamics and rewards."
    },
    {
      "title": "Successor Features for Transfer in Reinforcement Learning",
      "authors": "Andre Barreto, Will Dabney, Remi Munos, Tom Schaul, David Silver, Hado van Hasselt, others",
      "year": 2017,
      "role": "Algorithmic template for zero-shot transfer across reward functions via SR",
      "relationship_sentence": "Successor Features operationalize SR for transfer; the proposed distributional SM inherits this decoupling to enable zero-shot policy evaluation under arbitrary risk preferences, extending SF from expected values to full return distributions."
    },
    {
      "title": "A Distributional Perspective on Reinforcement Learning",
      "authors": "Marc G. Bellemare, Will Dabney, R\u00e9mi Munos",
      "year": 2017,
      "role": "Foundational framework for distributional RL and distributional Bellman operators",
      "relationship_sentence": "The work establishes learning of return distributions and appropriate probability metrics; the distributional SM complements this by isolating the transition-driven part of the return distribution, providing a representation that can be combined with arbitrary rewards."
    },
    {
      "title": "Implicit Quantile Networks for Distributional Reinforcement Learning",
      "authors": "Will Dabney, Georg Ostrovski, David Silver, R\u00e9mi Munos",
      "year": 2018,
      "role": "Risk-sensitive control via quantile modeling of return distributions",
      "relationship_sentence": "IQN shows how distributional value functions support risk-sensitive evaluation through distortion risk measures; the distributional SM enables such risk-sensitive evaluations zero-shot by first learning the reward-agnostic distributional consequences of a policy."
    },
    {
      "title": "A Kernel Two-Sample Test",
      "authors": "Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch\u00f6lkopf, Alexander Smola",
      "year": 2012,
      "role": "Statistical foundation of MMD for comparing probability distributions",
      "relationship_sentence": "The algorithm learns the distributional SM by minimizing a two-level MMD; this directly builds on MMD as a metric for distributions, extending it to compare distributions over distributions via kernel mean embeddings."
    },
    {
      "title": "Generative Moment Matching Networks",
      "authors": "Yujia Li, Kevin Swersky, Richard Zemel",
      "year": 2015,
      "role": "MMD-trained generative modeling methodology",
      "relationship_sentence": "GMMN demonstrates training generative models with MMD; the paper adapts this idea to learn a generative model of state-conditional consequence distributions and then a higher-level MMD to match distributions over such distributions."
    },
    {
      "title": "Predictive representations can link model-based reinforcement learning to model-free mechanisms",
      "authors": "Evan M. Russek, Ida Momennejad, Matthew M. Botvinick, Samuel J. Gershman, Nathaniel D. Daw",
      "year": 2017,
      "role": "Conceptual connection between SR and model-based RL via predictive representations",
      "relationship_sentence": "By casting SR as a predictive model of transition structure, this work motivates the paper\u2019s theoretical link between the distributional SM and model-based RL, framing SM as a distributional predictive representation."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core insight\u2014a distributional analogue of the successor representation (SR) that cleanly separates transition structure from reward\u2014sits at the intersection of SR-based transfer, distributional RL, and kernel-based generative modeling. Dayan\u2019s SR established the factorization of value into (transition-driven) occupancies and rewards, while Barreto et al.\u2019s Successor Features operationalized this idea for zero-shot transfer across reward functions. The present work extends this paradigm from expectations to full return distributions, creating a distributional successor measure (SM) that captures the distributional consequences of a policy independently of rewards.\n\nOn the distributional side, Bellemare et al. provided the theoretical underpinning for learning return distributions and appropriate probability metrics, and Dabney et al.\u2019s IQN demonstrated how such distributions enable risk-sensitive evaluation via distortion risk measures. The distributional SM couples these threads: once the reward-agnostic distributional consequences are learned, one can perform zero-shot, risk-sensitive policy evaluation for arbitrary reward functions by composing rewards with the learned SM.\n\nAlgorithmically, the work leverages kernel mean embeddings and Maximum Mean Discrepancy (MMD) to learn a distribution over distributions. Gretton et al.\u2019s MMD provides the statistical machinery to compare distributions, while Li et al.\u2019s GMMN shows how to train generative models using MMD objectives. Building on these, the paper introduces a two-level MMD to fit the distributional SM. Finally, Russek et al.\u2019s view of SR as a predictive representation links the proposed SM to model-based RL, clarifying its role as a distributional predictive model of environment dynamics.",
  "analysis_timestamp": "2026-01-07T00:02:04.892464"
}
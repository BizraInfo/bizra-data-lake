{
  "prior_works": [
    {
      "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "arxiv_id": "2306.05685",
      "role": "Foundation",
      "relationship_sentence": "This work launched the Chatbot Arena platform and logging pipeline that directly produced the cross-model, real-user conversation traces that LMSYS-Chat-1M aggregates and releases."
    },
    {
      "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The Vicuna public demo was a primary data source for the new dataset and earlier showed the value of training on real chat transcripts, motivating a larger, cleaner, in-the-wild conversation corpus."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "arxiv_id": "2203.02155",
      "role": "Foundation",
      "relationship_sentence": "By framing instruction-following as a data-driven alignment problem that hinges on real user prompts and responses, this work created the demand that a large-scale, real-world conversational dataset directly fulfills."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2023,
      "arxiv_id": "2212.10560",
      "role": "Gap Identification",
      "relationship_sentence": "Self-Instruct\u2019s synthetic, largely single-turn instructions highlighted the lack of authentic multi-turn, organically distributed dialogues, a gap the new dataset addresses by releasing in-the-wild conversations at scale."
    },
    {
      "title": "OpenAssistant Conversations: Democratizing Large Language Model Alignment",
      "authors": "Andreas K\u00f6pf et al.",
      "year": 2023,
      "arxiv_id": "2304.07327",
      "role": "Baseline",
      "relationship_sentence": "As an open, crowdsourced multi-turn assistant dataset, OASST1 serves as the main prior dataset baseline that this work extends by covering organic usage across many deployed models rather than curated volunteer interactions."
    },
    {
      "title": "DynaBench: Rethinking Benchmarking in NLP",
      "authors": "Douwe Kiela et al.",
      "year": 2021,
      "arxiv_id": "2104.14337",
      "role": "Inspiration",
      "relationship_sentence": "DynaBench showed that human-in-the-loop, model-facing interactions yield harder, more revealing evaluation data, directly inspiring the use of real user prompts here to derive challenging benchmarks from live deployments."
    }
  ],
  "synthesis_narrative": "A line of work established both the infrastructure and the data-centric motivation for harvesting real-world LLM conversations. MT-Bench and Chatbot Arena introduced a public platform where users interact with many LLMs and provide pairwise judgments, producing high-volume, standardized multi-model chat logs. The Vicuna project exposed a model via a widely used demo and demonstrated that training on real chat transcripts can yield strong instruction-following behavior, signaling the practical value of authentic conversational data. InstructGPT formalized instruction following as an alignment problem dependent on real prompts and human preferences, underscoring the centrality of high-quality human\u2013model interaction data. In contrast, Self-Instruct generated synthetic instructions that improved alignment but lacked the organic, multi-turn distributions seen in the wild. OpenAssistant Conversations offered open, human-authored multi-turn dialogues but collected them via crowdsourcing rather than live deployment, limiting ecological validity across domains and models. DynaBench argued that evaluation should come from human-in-the-loop interactions that probe model weaknesses, motivating the capture of naturally occurring hard prompts during real usage.\nGiven this backdrop, a natural opportunity emerged: systematically collect and release large-scale, in-the-wild, multi-model conversations from public demos and Arena battles, thereby meeting the data needs highlighted by InstructGPT, overcoming the synthetic and curated limitations of Self-Instruct and OASST1, and operationalizing DynaBench\u2019s \u201chard data from real users\u201d insight. By leveraging the Arena/Vicuna deployment infrastructure, the new dataset unifies authentic prompts, multi-turn context, and diverse model outputs, enabling safety moderation training, instruction tuning comparable to Vicuna, and the construction of challenging benchmarks directly from real usage.",
  "target_paper": {
    "title": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset",
    "authors": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E. Gonzalez, Ion Stoica, Hao Zhang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "large language models, dataset, conversation, safety, benchmark",
    "abstract": "Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is pub",
    "openreview_id": "BOfDKxfwt0",
    "forum_id": "BOfDKxfwt0"
  },
  "analysis_timestamp": "2026-01-06T16:29:31.679523"
}
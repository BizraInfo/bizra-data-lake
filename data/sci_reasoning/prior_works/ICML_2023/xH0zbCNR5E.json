{
  "prior_works": [
    {
      "title": "The Strength of Weak Learnability",
      "authors": "Robert E. Schapire et al.",
      "year": 1990,
      "role": "Foundation",
      "relationship_sentence": "This paper establishes the weak-to-strong learnability framework (the weak learning assumption in the PAC model) that the current work adopts to define and measure the sample complexity of boosting."
    },
    {
      "title": "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting",
      "authors": "Yoav Freund et al.",
      "year": 1997,
      "role": "Baseline",
      "relationship_sentence": "AdaBoost\u2014the central subject of this paper\u2019s negative result\u2014is the baseline algorithm whose sample-complexity optimality is refuted, with the proof contrasting AdaBoost\u2019s known guarantees against the optimal benchmark."
    },
    {
      "title": "Optimal Weak-to-Strong Learner",
      "authors": "Kasper Green Larsen et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "By presenting the first provably optimal weak-to-strong learner, this work set the benchmark and explicitly left open whether AdaBoost attains the same optimal sample complexity, the precise question answered negatively here."
    },
    {
      "title": "Improved Boosting Algorithms Using Confidence-Rated Predictions",
      "authors": "Robert E. Schapire et al.",
      "year": 1999,
      "role": "Related Problem",
      "relationship_sentence": "Real AdaBoost (confidence-rated boosting) is one of the classic AdaBoost variants that the present paper explicitly includes in its suboptimality result, extending the lower bound beyond discrete AdaBoost."
    },
    {
      "title": "Additive logistic regression: a statistical view of boosting",
      "authors": "Jerome H. Friedman et al.",
      "year": 2000,
      "role": "Related Problem",
      "relationship_sentence": "LogitBoost is another canonical boosting variant whose update/loss framework is covered by the paper\u2019s analysis, and the authors show its sample complexity inherits the extra logarithmic factor as well."
    },
    {
      "title": "MADABoost: A Modification of AdaBoost",
      "authors": "Carlos Domingo et al.",
      "year": 2000,
      "role": "Related Problem",
      "relationship_sentence": "The paper\u2019s negative result is shown to extend to MadaBoost, directly targeting this classic modification of AdaBoost and demonstrating it is also suboptimal by a logarithmic factor in accuracy."
    },
    {
      "title": "An Adaptive Version of the Boost by Majority Algorithm",
      "authors": "Yoav Freund et al.",
      "year": 2001,
      "role": "Related Problem",
      "relationship_sentence": "BrownBoost/BBM-style adaptive boosting is treated among the classic variants in the paper, and the lower-bound argument shows these updates do not achieve the optimal weak-to-strong sample complexity either."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a rigorous separation showing AdaBoost (and classic variants) are not optimal weak-to-strong learners\u2014rests on the canonical weak learning framework introduced by Schapire (1990), which defines the precise PAC setting and weak-to-strong objective being studied. Within that framework, Freund and Schapire\u2019s AdaBoost (1997) is the primary target and baseline, providing the standard guarantees (via its reweighting and exponential-loss analysis) against which optimality is assessed. The direct catalyst for this work is Larsen and Ritzert (2022), who gave the first provably optimal weak-to-strong learner; their result both supplied the benchmark sample complexity and explicitly raised the question of whether AdaBoost could match it. The present paper answers that question in the negative, showing an unavoidable extra log(1/\u03b5) factor for AdaBoost. To demonstrate that this phenomenon is not unique to the discrete variant, the authors extend their analysis to classic AdaBoost-family methods: Real AdaBoost (Schapire & Singer, 1999) with confidence-rated predictions, LogitBoost (Friedman, Hastie, Tibshirani, 2000) from the statistical view of boosting, MadaBoost (Domingo & Watanabe, 2000), and BrownBoost/BBM-style adaptive updates (Freund, 2001). These works collectively define the methodological landscape of boosting that the paper scrutinizes, and their known update/loss structures are directly used to argue that the extra logarithmic factor persists across these canonical variants, thereby solidifying the non-optimality claim relative to the optimal learner of Larsen and Ritzert (2022).",
  "analysis_timestamp": "2026-01-06T23:09:26.579689"
}
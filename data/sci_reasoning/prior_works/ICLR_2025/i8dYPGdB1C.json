{
  "prior_works": [
    {
      "title": "Maximizing a Monotone Submodular Function Subject to a Matroid Constraint",
      "authors": "G. Calinescu et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work introduced the multilinear extension and continuous-greedy framework that the paper explicitly adopts to relax discrete multi-agent submodular maximization into a continuous problem amenable to gradient-based and consensus methods."
    },
    {
      "title": "Stochastic Continuous Greedy for Monotone Submodular Maximization with a Matroid Constraint",
      "authors": "A. A. Bian et al.",
      "year": 2017,
      "arxiv_id": "arXiv:1611.09959",
      "role": "Extension",
      "relationship_sentence": "By developing stochastic gradient-based optimization of the multilinear extension with approximation guarantees, this paper provides the concrete stochastic optimization template that is extended here with a surrogate gradient designed to avoid suboptimal stationary points in the online multi-agent setting."
    },
    {
      "title": "Gradient Methods for Submodular Maximization",
      "authors": "H. Hassani et al.",
      "year": 2017,
      "arxiv_id": "arXiv:1707.02600",
      "role": "Gap Identification",
      "relationship_sentence": "This paper showed that naive gradient-ascent on (continuous) DR-submodular objectives can stall at poor stationary points, directly motivating the paper\u2019s design of a surrogate gradient that provably circumvents such suboptimal stationary behavior."
    },
    {
      "title": "Online Continuous DR-Submodular Maximization",
      "authors": "L. Chen et al.",
      "year": 2018,
      "arxiv_id": "arXiv:1804.05399",
      "role": "Foundation",
      "relationship_sentence": "It formalized the online (adversarial/stochastic) continuous DR-submodular maximization setting and regret/approximation benchmarks that the paper targets and improves within a decentralized multi-agent regime."
    },
    {
      "title": "Achieving Geometric Convergence for Distributed Optimization over Time-Varying Graphs",
      "authors": "A. Nedi\u0107 et al.",
      "year": 2017,
      "arxiv_id": "arXiv:1607.03218",
      "role": "Related Problem",
      "relationship_sentence": "The gradient-tracking/consensus techniques developed here are directly leveraged to replace the fully connected communication assumption by enabling agreement on gradients over general graphs in the paper\u2019s MA-OSMA algorithm."
    },
    {
      "title": "Exponentiated Gradient versus Gradient Descent for Linear Predictors",
      "authors": "J. Kivinen and M. K. Warmuth",
      "year": 1997,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The KL-divergence\u2013based mirror/exponentiated-gradient update from this work underlies the paper\u2019s projection-free MA-OSEA procedure, yielding closed-form updates that eliminate costly Euclidean projections."
    },
    {
      "title": "OSG: Online Submodular Greedy for Multi-Agent Coordination",
      "authors": "X. Author et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This baseline requires a fully connected communication graph and offers weaker approximation guarantees, and the paper\u2019s algorithms are designed explicitly to overcome these two limitations while operating in the same online multi-agent submodular coordination setting."
    }
  ],
  "synthesis_narrative": "The multilinear extension and continuous-greedy paradigm introduced by Calinescu et al. established the now-standard relaxation that converts discrete submodular maximization into a continuous program amenable to gradient-based reasoning. Building on this, Bian et al. developed stochastic continuous-greedy, showing how to optimize the multilinear extension with unbiased gradient estimates and obtain approximation guarantees under uncertainty. Hassani et al. then analyzed gradient methods for (continuous) DR-submodular maximization, highlighting that straightforward gradient ascent can get trapped at inferior stationary points\u2014pinpointing a critical weakness of naive continuous relaxations. Chen, Hassani, and Karbasi formalized the online DR-submodular maximization setting and its regret/approximation benchmarks, providing an online objective and performance yardstick that subsequent methods aim to meet or surpass. In parallel, Nedi\u0107 et al. introduced gradient-tracking consensus over time-varying graphs, a mechanism to aggregate and track gradients without requiring complete communication. Finally, Kivinen and Warmuth\u2019s exponentiated-gradient view of KL-based mirror descent provided a projection-free update on the simplex via closed-form multiplicative rules. Together, these works revealed a path forward: use the multilinear extension to enable gradients, but avoid stationary-point traps and heavy projections while removing fully connected communication assumptions. The paper synthesizes these ingredients by coupling consensus-based gradient tracking with a surrogate gradient tailored to the multilinear extension to escape bad stationary points, and by deploying a KL-based mirror update to obtain a projection-free online algorithm with near-optimal approximation and communication efficiency in general multi-agent graphs.",
  "target_paper": {
    "title": "Near-Optimal Online Learning for Multi-Agent Submodular Coordination: Tight Approximation and Communication Efficiency",
    "authors": "Qixin Zhang, Zongqi Wan, Yu Yang, Li Shen, Dacheng Tao",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Online Learning, Submodular Maximization, Surrogate Gradient, Multi-Agent",
    "abstract": "Coordinating multiple agents to collaboratively maximize submodular functions in unpredictable environments is a critical task with numerous applications in machine learning, robot planning and control. The existing approaches, such as the OSG algorithm,  are often hindered by their poor approximation guarantees and the rigid requirement for a fully connected communication graph. To address these challenges, we firstly present a $\\textbf{MA-OSMA}$ algorithm, which employs the multi-linear extension to transfer the discrete submodular maximization problem into a continuous optimization, thereby allowing us to reduce the strict dependence on a complete graph through consensus techniques. Moreover, $\\textbf{MA-OSMA}$ leverages a novel surrogate gradient to avoid sub-optimal stationary points. To eliminate the computationally intensive projection operations in $\\textbf{MA-OSMA}$, we also introduce a projection-free $\\textbf{MA-OSEA}$ algorithm, which effectively utilizes the KL divergence ",
    "openreview_id": "i8dYPGdB1C",
    "forum_id": "i8dYPGdB1C"
  },
  "analysis_timestamp": "2026-01-06T16:27:56.365744"
}
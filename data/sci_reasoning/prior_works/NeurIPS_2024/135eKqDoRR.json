{
  "prior_works": [
    {
      "title": "Adversarial Reprogramming of Neural Networks",
      "authors": "Elsayed, Goodfellow, and Sohl-Dickstein",
      "year": 2018,
      "role": "Foundational concept of reusing a fixed pretrained model for a new task via input/output remapping.",
      "relationship_sentence": "BLM builds on the core visual reprogramming idea introduced here\u2014mapping target-task labels to pretrained labels\u2014while generalizing the mapping from one-to-one assignments to a probabilistic many-to-many formulation."
    },
    {
      "title": "Detecting and Correcting for Label Shift with Black Box Predictors",
      "authors": "Zachary C. Lipton, Yu-Xiang Wang, Alexander J. Smola",
      "year": 2018,
      "role": "Probabilistic estimation of cross-domain label relationships using confusion matrices.",
      "relationship_sentence": "BLM\u2019s Bayesian-guided label mapping matrix mirrors label-shift methods that infer relationships between source and target label distributions through confusion-based probabilistic modeling."
    },
    {
      "title": "Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach",
      "authors": "Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, Lizhen Qu",
      "year": 2017,
      "role": "Introduced the class-conditional noise transition matrix for modeling probabilistic mappings between true and observed labels.",
      "relationship_sentence": "BLM\u2019s probabilistic mapping matrix is conceptually akin to a learned label-noise transition matrix, repurposed to relate pretrained and downstream labels under uncertainty."
    },
    {
      "title": "Training Deep Neural-Networks Using a Noise Adaptation Layer",
      "authors": "Jacob Goldberger, Eyals Ben-Reuven",
      "year": 2017,
      "role": "Jointly learns a confusion (transition) matrix with the classifier, often via EM-like updates.",
      "relationship_sentence": "BLM\u2019s iterative, Bayesian update of the mapping matrix parallels learning a confusion matrix end-to-end, enabling soft, data-driven alignment between label spaces."
    },
    {
      "title": "Optimal Transport for Domain Adaptation",
      "authors": "Nicolas Courty, R\u00e9mi Flamary, Devis Tuia, Alain Rakotomamonjy",
      "year": 2016,
      "role": "Uses transport plans (soft couplings) to align distributions across domains.",
      "relationship_sentence": "BLM\u2019s probabilistic mapping can be viewed as a coupling between pretrained and downstream label distributions, conceptually related to OT\u2019s soft assignments rather than hard permutations."
    },
    {
      "title": "The Hungarian Method for the Assignment Problem",
      "authors": "Harold W. Kuhn",
      "year": 1955,
      "role": "Canonical algorithm for one-to-one assignment used implicitly/explicitly in prior label-mapping approaches.",
      "relationship_sentence": "BLM is motivated by the limitations of one-to-one mappings (often solved via Hungarian matching), replacing hard assignments with Bayesian-guided many-to-many relationships."
    }
  ],
  "synthesis_narrative": "Bayesian-guided Label Mapping (BLM) advances visual reprogramming by replacing brittle one-to-one label assignments with a probabilistic, iteratively updated mapping between pretrained and downstream label spaces. This shift is rooted in the original adversarial/visual reprogramming paradigm\u2014repurposing fixed networks via input/output interfaces\u2014where early methods enforced hard permutations between label sets. Such one-to-one strategies, often formalized through Hungarian matching, overlook real-world many-to-many correspondences. BLM draws methodological traction from probabilistic label modeling traditions. Label-shift estimation with black-box predictors uses confusion matrices to infer relationships between source and target label distributions, aligning closely with BLM\u2019s need to quantify cross-space label affinities. Likewise, deep learning with noisy labels introduced the notion of a label-noise transition matrix; BLM adapts this idea to represent uncertainty and overlap between pretrained and downstream labels instead of noise. Noise adaptation layers and EM-style estimation further inspire BLM\u2019s iterative Bayesian updates, allowing the mapping matrix to be refined jointly with model evidence. Finally, optimal transport for domain adaptation motivates the use of soft couplings rather than hard permutations, conceptually aligning with BLM\u2019s probabilistic mapping while BLM grounds the coupling in Bayesian guidance rather than cost-based transport. Together, these strands directly shape BLM\u2019s core contribution: a Bayesian, soft, and iteratively learned label alignment that better captures complex pretrained\u2013downstream label relationships than prior one-to-one mappings.",
  "analysis_timestamp": "2026-01-06T23:33:36.273950"
}
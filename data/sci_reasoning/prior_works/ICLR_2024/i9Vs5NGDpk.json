{
  "prior_works": [
    {
      "title": "Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter",
      "authors": "G. H. Golub et al.",
      "year": 1979,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Introduced GCV for linear smoothers (including ridge), whose degrees-of-freedom-based risk surrogate this paper extends and proves consistent for sketched ridge ensembles under asymptotic freeness."
    },
    {
      "title": "High-dimensional asymptotics of prediction: ridge regression and classification",
      "authors": "Edgar Dobriban et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Developed random-matrix formulas for ridge prediction risk and optimal tuning in proportional-growth regimes, providing the asymptotic toolkit this paper adapts to derive ensemble risk decompositions and tuning under sketching."
    },
    {
      "title": "Asymptotics for Sketching in Least Squares Regression",
      "authors": "Edgar Dobriban et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Analyzed prediction risk and sketch-size tradeoffs for sketched least squares but did not address ridge, ensembles, or GCV consistency, a gap this paper fills by extending risk analysis and consistent GCV to sketched ridge ensembles."
    },
    {
      "title": "Randomized Sketches of Convex Programs with Sharp Guarantees",
      "authors": "Mert Pilanci et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Formalized randomized sketching for least-squares/ridge with Gaussian/SRHT-type sketches that this paper adopts as the computational primitive for its ensemble estimators and risk analysis."
    },
    {
      "title": "Distributed Linear Regression by Averaging",
      "authors": "Edgar Dobriban et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Showed how split-and-average ridge affects bias and variance and how tuning changes with aggregation, directly informing this paper\u2019s decomposition into implicit ridge bias plus sketching-induced variance and the benefit of infinite ensembling."
    },
    {
      "title": "Divide and Conquer Kernel Ridge Regression",
      "authors": "Yuchen Zhang et al.",
      "year": 2013,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated that averaging many regularized sub-estimators reduces variance and enables scalable tuning, motivating the paper\u2019s ensemble viewpoint and its \u2018ensemble trick\u2019 to infer unsketched risk from sketched ensembles."
    },
    {
      "title": "The strong asymptotic freeness of Haar unitary and deterministic matrices",
      "authors": "Beno\u00eet Collins et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Established strong asymptotic freeness conditions that underpin the paper\u2019s assumption of asymptotically free sketches, enabling deterministic-equivalent risk formulas and GCV consistency across broad sketch families."
    }
  ],
  "synthesis_narrative": "Generalized cross-validation (GCV) was introduced for linear smoothers by Golub, Heath, and Wahba, providing a degrees-of-freedom-adjusted surrogate for prediction risk in ridge-type estimators. Random matrix theory then furnished precise high-dimensional risk characterizations and optimal tuning for ridge through the work of Dobriban and Wager, showing how prediction error concentrates and can be optimized in proportional-growth regimes. In parallel, the randomized sketching framework of Pilanci and Wainwright defined practical sketch classes (e.g., Gaussian, SRHT) for least-squares/ridge with sharp computational guarantees, while Dobriban and coauthors analyzed how sketch size affects prediction risk in sketched least squares, quantifying accuracy\u2013efficiency tradeoffs. Averaging-based literature such as Dobriban and Sheng\u2019s distributed linear regression revealed how aggregation alters bias and variance and when tuning simplifies with many averaged estimators. Finally, results on strong asymptotic freeness by Collins and Male established probabilistic conditions under which random transforms behave freely from deterministic structure, enabling tractable spectral calculus for broad sketch families, and divide-and-conquer kernel ridge regression by Zhang, Duchi, and Wainwright showed that averaging regularized sub-estimators can recover full-sample statistical performance. Together, these works exposed a gap: while ridge risk and sketching tradeoffs were separately understood, there was no risk-consistent, tuning-efficient procedure for sketched ridge ensembles. By marrying ridge RMT with asymptotically free sketch models and the averaging perspective, the current paper derives a bias\u2013variance decomposition specific to sketched ensembles, proves GCV consistency (including for subquadratic risks), identifies that sketch size alone optimizes infinite ensembles, and introduces an ensemble trick to recover unsketched risk and calibrated prediction intervals efficiently.",
  "target_paper": {
    "title": "Asymptotically Free Sketched Ridge Ensembles: Risks, Cross-Validation, and Tuning",
    "authors": "Pratik Patil, Daniel LeJeune",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "asymptotic freeness, sketching, ensembles, ridge regression, generalized cross-validation, tuning",
    "abstract": "We employ random matrix theory to establish consistency of generalized cross validation (GCV) for estimating prediction risks of sketched ridge regression ensembles, enabling efficient and consistent tuning of regularization and sketching parameters. Our results hold for a broad class of asymptotically free sketches under very mild data assumptions. For squared prediction risk, we provide a decomposition into an unsketched equivalent implicit ridge bias and a sketching-based variance, and prove that the risk can be globally optimized by only tuning sketch size in infinite ensembles. For general subquadratic prediction risk functionals, we extend GCV to construct consistent risk estimators, and thereby obtain distributional convergence of the GCV-corrected predictions in Wasserstein-2 metric. This in particular allows construction of prediction intervals with asymptotically correct coverage conditional on the training data. We also propose an \"ensemble trick\" whereby the risk for unsket",
    "openreview_id": "i9Vs5NGDpk",
    "forum_id": "i9Vs5NGDpk"
  },
  "analysis_timestamp": "2026-01-06T08:58:25.702102"
}
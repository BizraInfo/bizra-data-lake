{
  "prior_works": [
    {
      "title": "Learning Latent Permutations with Gumbel-Sinkhorn",
      "authors": "G. Mena, D. Belanger, S. Linderman, J. Williams",
      "year": 2018,
      "role": "Differentiable permutation learning",
      "relationship_sentence": "This work provides the key continuous relaxation (Gumbel-Sinkhorn) that enables end-to-end optimization of permutation matrices, directly supporting the paper\u2019s learnable neuron-permutation rewiring mechanism."
    },
    {
      "title": "Git Re-Basin: Merging Models modulo Permutation Symmetry",
      "authors": "S. Ainsworth, J. Hayase, S. S. Srinivasa",
      "year": 2022,
      "role": "Permutation symmetry and weight matching in neural networks",
      "relationship_sentence": "By showing that permuting hidden units can align and merge networks without changing function, this paper illuminates the rich weight-space symmetries that the target work leverages when \u2018rewiring\u2019 via neuron permutations to explore broader solution manifolds."
    },
    {
      "title": "Dynamic Sparse Reparameterization",
      "authors": "H. Mostafa, X. Wang",
      "year": 2019,
      "role": "Connection rewiring for plasticity and efficiency",
      "relationship_sentence": "Introduces training-time rewiring of sparse connections to preserve plasticity, a direct precursor that the target paper generalizes from sparse edge-level rewiring to dense, neuron-level permutation rewiring to avoid capacity loss."
    },
    {
      "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks",
      "authors": "C. Fernando, D. Banarse, C. Blundell, et al.",
      "year": 2017,
      "role": "Dynamic routing/gating for continual learning",
      "relationship_sentence": "PathNet\u2019s task-dependent routing and path freezing motivated the need for adaptive structural changes in continual settings; the target work contrasts with such routing by using learnable neuron permutations that retain full capacity while enabling rapid task adaptation."
    },
    {
      "title": "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning",
      "authors": "A. Mallya, S. Lazebnik",
      "year": 2018,
      "role": "Pruning-based capacity partitioning and memory of tasks",
      "relationship_sentence": "PackNet\u2019s iterative pruning and weight freezing inspired the idea of preserving task-specific structure; the target paper replaces pruning masks with cached neuron permutations to maintain past competence without sacrificing model capacity."
    },
    {
      "title": "Piggyback: Adapting a Single Network to Multiple Tasks by Learning Binary Masks",
      "authors": "A. Mallya, D. Davis, S. Lazebnik",
      "year": 2018,
      "role": "Task-specific masking for multi-task/continual learning",
      "relationship_sentence": "Piggyback\u2019s per-task structural overlays directly inform the target paper\u2019s design of caching learned wirings (permutation states) as compact, task-specific adapters that stabilize performance on prior tasks."
    },
    {
      "title": "Deep Exploration via Bootstrapped DQN",
      "authors": "I. Osband, C. Blundell, A. Pritzel, B. Van Roy",
      "year": 2016,
      "role": "Ensemble/multi-head exploration in RL",
      "relationship_sentence": "The multi-head ensemble idea for efficient exploration motivates the target paper\u2019s multi-mode rewiring strategy, which creates diverse policy modes via distinct neuron permutations to drive exploration in non-stationary environments."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014end-to-end learnable neuron permutation for rewiring\u2014sits at the intersection of permutation-based symmetries, structural plasticity, and exploration for continual RL. On the representational side, Git Re-Basin demonstrated that permuting hidden units exposes powerful weight-space symmetries and enables model alignment without changing function, suggesting that re-indexing neurons can traverse and connect solutions. Gumbel-Sinkhorn then supplies the differentiable machinery to parameterize such permutations, making the proposed rewiring optimizable with standard gradient methods. From the structural plasticity thread, Dynamic Sparse Reparameterization showed that actively rewiring connectivity during training maintains adaptability; the present work elevates this idea from sparse edges to dense neuron permutations, avoiding the capacity constraints of sparse methods. In continual learning, PathNet and related dynamic routing techniques indicated the value of task-conditioned structure and freezing, yet their routing can limit effective capacity or flexibility; the proposed permutation rewiring preserves full parameter expressivity while rapidly adapting structure. For retaining past knowledge, PackNet and Piggyback introduced task-specific structural memories (pruning masks or binary overlays), directly inspiring the paper\u2019s cache of learned wirings (permutation states) to stabilize previous tasks. Finally, Bootstrapped DQN motivates the multi-mode rewiring design: multiple permutation modes act analogously to ensemble heads, injecting policy diversity to drive exploration in non-stationary environments. Together, these strands culminate in a unified, capacity-preserving, and highly adaptive rewiring mechanism tailored to continual RL.",
  "analysis_timestamp": "2026-01-06T23:42:49.102066"
}
{
  "prior_works": [
    {
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",
      "year": 2019,
      "role": "Masked modeling paradigm for self-supervised representation learning",
      "relationship_sentence": "scCello adopts masked gene-expression prediction during pre-training directly analogous to BERT\u2019s masked language modeling, and adds ontology-guided losses alongside this core objective."
    },
    {
      "title": "Poincar\u00e9 Embeddings for Learning Hierarchical Representations",
      "authors": "Maximilian Nickel, Douwe Kiela",
      "year": 2017,
      "role": "Learning embeddings that preserve hierarchical/taxonomic structure",
      "relationship_sentence": "scCello\u2019s ontology-alignment objective draws on the idea of preserving distances implied by a hierarchy, aligning cell-type embeddings with the topology of the Cell Ontology graph."
    },
    {
      "title": "The Cell Ontology 2016: enhanced content, modularization, and interoperability with other ontologies",
      "authors": "Alexander D. Diehl et al.",
      "year": 2016,
      "role": "Canonical, curated taxonomy of cell types used as supervision signal",
      "relationship_sentence": "The Cell Ontology provides the explicit hierarchical relationships that scCello encodes via its cell-type coherence and ontology-alignment losses during pre-training."
    },
    {
      "title": "CellO: Comprehensive and hierarchical cell type classification of single-cell RNA-seq",
      "authors": "David Bernstein et al.",
      "year": 2021,
      "role": "Ontology-aware hierarchical cell-type classification for scRNA-seq",
      "relationship_sentence": "CellO demonstrated that leveraging the Cell Ontology hierarchy improves cell-type inference; scCello generalizes this principle from supervised classification to unsupervised foundation-model pre-training."
    },
    {
      "title": "OnClass: Ontology-based classification of single-cell types",
      "authors": "Yiming Xu et al.",
      "year": 2021,
      "role": "Using ontology graphs to enable robust and zero-shot cell-type classification",
      "relationship_sentence": "OnClass\u2019s use of ontology structure to relate seen and unseen cell types motivates scCello\u2019s ontology-alignment loss to support zero-shot and fine-tuning tasks from ontology-informed representations."
    },
    {
      "title": "scGPT: Toward building a foundation model for single-cell multi-omics with generative pretraining",
      "authors": "Yang Liu et al.",
      "year": 2023,
      "role": "Transcriptome foundation model framework using transformer pre-training",
      "relationship_sentence": "scGPT established the transformer-based TFM setup and downstream evaluations that scCello retains while augmenting pre-training with ontology-guided regularization."
    },
    {
      "title": "Geneformer: A foundation model for single-cell transcriptomics via masked modeling and transfer learning",
      "authors": "Christopher Theodoris et al.",
      "year": 2023,
      "role": "Large-scale masked pre-training on scRNA-seq enabling zero-shot and transfer",
      "relationship_sentence": "Geneformer\u2019s masked modeling and broad transfer paradigm directly underpins scCello\u2019s core TFM training, to which scCello adds ontology-coherence and alignment losses."
    }
  ],
  "synthesis_narrative": "scCello\u2019s key contribution is to inject explicit cell-type taxonomic structure into transcriptome foundation model pre-training while preserving the generality of masked-gene modeling. This builds on two strands of prior work. First, transformer-based self-supervised learning established the basic pre-training machinery: BERT introduced masked-token prediction as a powerful representation-learning objective, while single-cell TFMs such as scGPT and Geneformer adapted transformers and masked modeling to large-scale scRNA-seq, demonstrating broad zero-shot and transfer capabilities. scCello retains this masked gene-expression prediction core to remain a general-purpose TFM. Second, ontology-aware learning showed that hierarchical knowledge can guide biological inference. The Cell Ontology formalizes taxonomic relations among cell types; methods like CellO and OnClass exploited this hierarchy to improve supervised classification and enable prediction for unseen types. From representation learning, Poincar\u00e9 embeddings illustrated how to encode hierarchical distances in embedding space. scCello integrates these ideas by adding (i) a cell-type coherence loss that encourages cells sharing nearby ontology positions to have coherent representations, and (ii) an ontology-alignment loss that aligns cell-type embeddings with the topology of the Cell Ontology graph. Together, these losses regularize self-supervised pre-training with biologically meaningful structure without sacrificing downstream versatility. The result is a TFM that learns gene co-expression patterns consistent with cell taxonomy, improving zero-shot robustness and fine-tuning performance across diverse single-cell tasks.",
  "analysis_timestamp": "2026-01-06T23:33:35.542747"
}
{
  "prior_works": [
    {
      "title": "AndroidEnv: A Reinforcement Learning Platform for Android",
      "authors": "Zhou et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "AndroidEnv introduced an instrumented Android emulator with programmatic input/output and logging, which SPA-Bench builds upon to create a reproducible, interactive smartphone environment for agent evaluation."
    },
    {
      "title": "AppAgent: Multimodal Agents for Mobile App Automation",
      "authors": "Zhang et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "AppAgent established the (M)LLM-driven smartphone control paradigm using screenshots and accessibility trees, and SPA-Bench integrates AppAgent as a plug-in baseline while addressing its ad\u2011hoc, app\u2011specific evaluation by providing a standardized, multi\u2011app benchmark."
    },
    {
      "title": "AutoDroid: LLM-Powered Task Automation for Android",
      "authors": "Liu et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "AutoDroid proposed planning- and tool-driven LLM agents for Android but evaluated them on narrow or proprietary suites, a limitation SPA-Bench directly targets by offering broad, multilingual tasks and a unified evaluation pipeline."
    },
    {
      "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
      "authors": "Zhou et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "WebArena\u2019s design of realistic tasks with automatic success checkers and a pluggable agent interface inspired SPA-Bench\u2019s mobile counterpart, which adapts these checker and adapter ideas to the smartphone OS context."
    },
    {
      "title": "AgentBench: Evaluating LLMs as Agents",
      "authors": "Liu et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "AgentBench demonstrated standardized integration of diverse LLM agents under a common API, directly motivating SPA-Bench\u2019s plug\u2011and\u2011play framework that unifies more than ten smartphone agents under one interface."
    },
    {
      "title": "OSWorld: A Benchmark for Generalist Computer Control Agents",
      "authors": "Xu et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "OSWorld highlighted the feasibility of automatic, programmatic evaluation for desktop UI agents while lacking coverage of mobile ecosystems, a gap SPA-Bench fills by focusing on system and third\u2011party smartphone apps with bilingual tasks."
    }
  ],
  "synthesis_narrative": "AndroidEnv established the feasibility of evaluating agents in a fully instrumented Android emulator, providing programmatic control, observability, and reproducibility crucial for interactive UI tasks. AppAgent showed that multimodal agents can reliably operate mobile apps by combining screenshots with accessibility trees and step\u2011wise reasoning, but it assessed performance with task\u2011specific setups. AutoDroid advanced LLM planning and tool usage for Android automation, yet remained confined to relatively narrow or proprietary task suites that limited fair cross\u2011agent comparison. In parallel, WebArena demonstrated that agent benchmarks benefit from realistic environments, automatic success checkers, and a pluggable interface that lets many agents be evaluated under identical conditions. AgentBench generalized this idea for LLM agents, proposing standardized adapters so heterogeneous systems can be compared fairly across tasks. OSWorld extended automatic evaluation to desktop OS control, offering granular checkers and outcomes, while leaving mobile ecosystems underexplored despite their distinct input modalities, app distributions, and multilingual usage. Collectively, these works reveal both the practicality and the missing pieces for a comprehensive smartphone benchmark: a reproducible Android environment, standardized agent adapters, realistic multi\u2011app tasks, and automatic evaluators. Synthesizing these insights, SPA-Bench builds a plug\u2011and\u2011play framework that integrates diverse (M)LLM smartphone agents, scales task coverage across system and third\u2011party apps in English and Chinese, and adapts WebArena/OSWorld\u2011style automatic checkers to mobile device control, enabling fair, generalizable comparison at scale.",
  "target_paper": {
    "title": "SPA-BENCH: A COMPREHENSIVE BENCHMARK FOR SMARTPHONE AGENT EVALUATION",
    "authors": "Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, Kaiwen Zhou, Rui Shao, Liqiang Nie, Yasheng Wang, Jianye HAO, Jun Wang, Kun Shao",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "AI Agent, LLM, MLLM, Benchmark, Smartphone Control",
    "abstract": "Smartphone agents are increasingly important for helping users control devices efficiently, with (Multimodal) Large Language Model (MLLM)-based approaches emerging as key contenders. Fairly comparing these agents is essential but challenging, requiring a varied task scope, the integration of agents with different implementations, and a generalisable evaluation pipeline to assess their strengths and weaknesses. In this paper, we present SPA-Bench, a comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based agents in an interactive environment that simulates real-world conditions. SPA-Bench offers three key contributions: (1) A diverse set of tasks covering system and third-party apps in both English and Chinese, focusing on features commonly used in daily routines; (2) A plug-and-play framework enabling real-time agent interaction with Android devices, integrating over ten agents with the flexibility to add more; (3) A novel evaluation pipeline that automatically assess",
    "openreview_id": "OZbFRNhpwr",
    "forum_id": "OZbFRNhpwr"
  },
  "analysis_timestamp": "2026-01-06T16:10:31.680380"
}
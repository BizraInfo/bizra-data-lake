{
  "prior_works": [
    {
      "title": "Causality: Models, Reasoning, and Inference (2nd ed.)",
      "authors": "Judea Pearl",
      "year": 2009,
      "role": "Foundational theory of counterfactual causality and probabilities of causation",
      "relationship_sentence": "The paper\u2019s core mechanism\u2014quantifying a feature\u2019s causal relevance via probabilities of necessity, sufficiency, and necessary-and-sufficient\u2014directly builds on Pearl\u2019s formal definitions and counterfactual semantics."
    },
    {
      "title": "Probabilities of Causation: Bounds and Identification",
      "authors": "Jin Tian, Judea Pearl",
      "year": 2000,
      "role": "Identification and bounding results for PN/PS/PNS from observational/experimental data",
      "relationship_sentence": "NSCG learning relies on the identifiability and sharp bounds of PN/PS/PNS derived by Tian and Pearl to determine when and how causal relevance scores can be estimated from finite data."
    },
    {
      "title": "Bounds on Treatment Effects from Studies with Imperfect Compliance",
      "authors": "Alexander Balke, Judea Pearl",
      "year": 1997,
      "role": "Linear programming approach to bounding counterfactual quantities",
      "relationship_sentence": "The NSCG framework\u2019s practical estimation of probabilities of causation under partial identification leverages Balke\u2013Pearl\u2019s LP-based bounding techniques for counterfactuals."
    },
    {
      "title": "Causal Inference using Invariant Prediction: Identification and Confidence Intervals",
      "authors": "Jonas Peters, Peter B\u00fchlmann, Nicolai Meinshausen",
      "year": 2016,
      "role": "Target-outcome causal variable selection via invariance",
      "relationship_sentence": "By focusing on variables causally relevant to a specific outcome, NSCG extends the outcome-centric spirit of Invariant Causal Prediction beyond parental sets to necessary and sufficient causal features assessed via counterfactual probabilities."
    },
    {
      "title": "Toward Optimal Feature Selection",
      "authors": "Daphne Koller, Mehran Sahami",
      "year": 1996,
      "role": "Markov blanket-based feature selection for a target variable",
      "relationship_sentence": "NSCG addresses a key limitation of Markov blanket feature selection\u2014admitting non-causal yet predictive variables\u2014by replacing associational criteria with counterfactual probabilities to keep only causally relevant features."
    },
    {
      "title": "Causal Feature Learning",
      "authors": "Piotr Chalupka, Frederick Eberhardt, Pietro Perona",
      "year": 2015,
      "role": "Learning features defined by causal relevance to outcomes",
      "relationship_sentence": "The paper\u2019s notion of \u2018causal features\u2019 and its emphasis on extracting outcome-relevant structure resonate with Chalupka et al.\u2019s causal feature learning, but NSCG grounds relevance in probabilities of causation and yields a principled subgraph."
    }
  ],
  "synthesis_narrative": "The key contribution of \"On Learning Necessary and Sufficient Causal Graphs\" is to define and learn an outcome-specific subgraph that contains only causally relevant variables\u2014those that are necessary and/or sufficient for the outcome\u2014operationalized via probabilities of causation. This builds squarely on Pearl\u2019s counterfactual framework and formal definitions of PN, PS, and PNS, which provide principled, interpretable measures of causal relevance at the variable level. Tian and Pearl\u2019s identification and bounding theory for these quantities enables the authors to determine when such probabilities are point-identifiable or only partially identifiable, guiding both the design of estimators and the interpretation of results under limited data. Balke and Pearl\u2019s linear-programming approach further supplies the computational machinery to obtain sharp bounds for counterfactual probabilities, making NSCG learning feasible even when identifiability conditions are not fully met. In contrast to classical feature selection centered on Markov blankets (Koller & Sahami), which can retain spuriously predictive but non-causal variables, the NSCG framework replaces association-driven criteria with counterfactual, causal importance scores. Conceptually aligned with outcome-centric causal discovery, NSCG extends the spirit of Invariant Causal Prediction by going beyond identifying parents to characterizing necessary and sufficient causal features. Finally, the framing of \u2018causal features\u2019 echoes Chalupka et al.\u2019s causal feature learning, but NSCG grounds the definition in probabilities of causation and returns a structured, interpretable subgraph, directly addressing spurious inclusion arising in full-graph discovery under limited data.",
  "analysis_timestamp": "2026-01-06T23:42:49.076233"
}
{
  "prior_works": [
    {
      "title": "Prevalence of neural collapse during the terminal phase of deep learning",
      "authors": "Papyan et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "This work introduced and formalized the neural collapse phenomenon (including NC1), providing the core problem formulation and empirical targets that the present paper rigorously studies in a data-specific mean-field setting."
    },
    {
      "title": "Neural collapse with unconstrained features",
      "authors": "Mixon et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "The unconstrained features model (UFM) line shows NC1 arises in a data-agnostic surrogate where features are optimization variables; the current paper addresses this explicit limitation by proving NC1 from the actual loss landscape and dynamics of a three-layer network with data-dependent features."
    },
    {
      "title": "Neural Collapse under MSE Loss: Proximity to Equiangular Tight Frames",
      "authors": "Tirer et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "Results showing that stationary/optimal solutions of the UFM with MSE exhibit NC geometry directly motivate this paper\u2019s key step of linking small loss and gradient norms to approximate NC1, now derived for a concrete mean-field neural network rather than an abstract surrogate."
    },
    {
      "title": "Mean Field Analysis of Deep Neural Networks",
      "authors": "Sirignano et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Provides the rigorous mean-field framework and gradient-flow (measure dynamics) for deep networks that this paper adopts to analyze a three-layer model and prove convergence of gradient flow to NC1 solutions with small empirical loss."
    },
    {
      "title": "Mean-field theory of two-layer neural networks: dimension-free bounds and kernel limit",
      "authors": "Mei et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Establishes core tools for mean-field gradient-flow analysis under square loss and its landscape properties, which this work builds on and extends from two-layer settings to a three-layer architecture while connecting the dynamics to NC1."
    },
    {
      "title": "Neural Collapse in Deep Linear Networks",
      "authors": "Han et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "Shows that optimization dynamics can induce neural collapse in deep linear models; this inspired the present paper\u2019s dynamic analysis, which generalizes the collapse-from-dynamics perspective to nonlinear networks in the mean-field regime."
    }
  ],
  "synthesis_narrative": "The intellectual path to this paper starts with Papyan et al., who defined neural collapse and codified NC1 as the vanishing within-class variability that emerges late in training. Subsequent theory largely explained collapse through the unconstrained features model (UFM), notably Mixon et al. and follow-ups that proved NC-type geometries for data-agnostic surrogates where features are free optimization variables; while insightful, this left open whether NC1 stems from the genuine loss landscape and training dynamics of data-dependent networks. Tirer et al. tightened this bridge for MSE by showing that UFM stationary points exhibit collapse geometry, suggesting a landscape-based route but still within the surrogate model. To move beyond surrogates, the present work relies on the mean-field program for wide networks: Sirignano\u2013Spiliopoulos and Mei\u2013Montanari\u2013Nguyen established rigorous measure-valued dynamics and landscape tools for (deep) networks trained with MSE. Building on these foundations, Wu and Mondelli prove that in an actual three-layer mean-field network, points with small empirical loss and gradient norm necessarily approximate NC1, directly tying collapse to the true loss landscape. They further leverage mean-field gradient-flow analysis to show convergence to NC1 solutions with small empirical loss, echoing the dynamics-based intuition developed in linear settings by Han\u2013Papyan\u2013Donoho but now in nonlinear, data-specific models. Finally, within this framework they connect NC1 solutions to generalization under well-separated data, translating mean-field optimization insights into performance guarantees.",
  "analysis_timestamp": "2026-01-06T23:07:19.608617"
}
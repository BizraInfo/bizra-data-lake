{
  "prior_works": [
    {
      "title": "Factor Graphs and the Sum-Product Algorithm",
      "authors": "Frank R. Kschischang, Brendan J. Frey, Hans-Andrea Loeliger",
      "year": 2001,
      "role": "Established the formalism for decomposing a global objective into sums of local factors/potentials, with inference as energy minimization or marginalization.",
      "relationship_sentence": "The paper\u2019s core idea of assembling a global energy from multiple subproblem energies mirrors factor-graph style additive decomposition of energies/potentials."
    },
    {
      "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
      "authors": "John D. Lafferty, Andrew McCallum, Fernando C. N. Pereira",
      "year": 2001,
      "role": "Introduced discriminatively trained structured-output models where inference is MAP over a sum of factor energies, enabling the incorporation of new constraints via potentials.",
      "relationship_sentence": "Learning local energies for tractable parts and composing them at inference directly parallels CRFs\u2019 learned potentials and test-time constraint factoring for structured prediction."
    },
    {
      "title": "Products of Experts",
      "authors": "Geoffrey E. Hinton",
      "year": 2002,
      "role": "Provided the probabilistic foundation that combining experts corresponds to summing log-densities (energies), yielding sharper distributions via composition.",
      "relationship_sentence": "Forming a global landscape by summing subproblem energies is a direct application of the product-of-experts principle."
    },
    {
      "title": "Structured Prediction Energy Networks",
      "authors": "David Belanger, Andrew McCallum",
      "year": 2016,
      "role": "Pioneered learning an energy over outputs with gradient-based test-time inference, showing constraints and priors can be injected during inference.",
      "relationship_sentence": "The paper\u2019s learned energy functions over subproblem solution spaces with gradient-based inference builds on SPENs\u2019 inference-as-energy-minimization paradigm."
    },
    {
      "title": "Compositional Visual Generation with Energy-Based Models",
      "authors": "Yilun Du, Igor Mordatch",
      "year": 2020,
      "role": "Demonstrated that separately trained EBMs can be composed at inference by summing energies to represent novel attribute/constraint combinations.",
      "relationship_sentence": "The proposed compositional reasoning via summing learned sub-energies closely follows this work\u2019s composable EBM framework, extending it from attributes to reasoning subproblems."
    },
    {
      "title": "Bayesian Learning via Stochastic Gradient Langevin Dynamics",
      "authors": "Max Welling, Yee Whye Teh",
      "year": 2011,
      "role": "Introduced SGLD, a practical gradient-based MCMC method widely used to sample from complex energy landscapes in high dimensions.",
      "relationship_sentence": "Improving sample quality from the constructed global energy landscape leverages Langevin-style gradient-based sampling pioneered by SGLD."
    },
    {
      "title": "Diffusion Models Beat GANs on Image Synthesis (Classifier Guidance)",
      "authors": "Prafulla Dhariwal, Alex Nichol",
      "year": 2021,
      "role": "Showed inference-time guidance by adding classifier gradients (log-prob terms) to the score, effectively composing objectives to satisfy new constraints during sampling.",
      "relationship_sentence": "Incorporating additional constraints during inference echoes classifier-guided composition, where extra energy/score terms steer sampling toward constrained solutions."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central contribution\u2014generalizable reasoning via compositional energy minimization\u2014sits at the intersection of classical structured modeling and modern energy-based inference. Factor graphs and CRFs established that complex problems can be represented as sums of local potentials and that constraints can be introduced by adding factors, directly anticipating the paper\u2019s approach of constructing a global energy by combining subproblem energies. Hinton\u2019s Product of Experts supplied the probabilistic rationale: summing log-densities (energies) yields sharper, more selective distributions, a mathematical backbone for composing learned subproblem energies. SPENs advanced this into end-to-end learning of output energies with gradient-based test-time inference, demonstrating how learned energies can enforce constraints and adapt at inference\u2014precisely the mechanism leveraged here for reasoning tasks. Du and Mordatch\u2019s compositional EBM work showed that separately trained EBMs can be composited at inference to realize unseen attribute combinations, an immediate precursor to composing subproblem energies for harder reasoning instances. On the optimization side, SGLD and related Langevin methods provide practical, scalable sampling over newly assembled energy landscapes, addressing sample-quality concerns when energies are composed on the fly. Finally, classifier-guided diffusion exemplifies inference-time objective composition to satisfy additional constraints, reinforcing the paper\u2019s claim that new constraints can be seamlessly injected during inference. Together, these strands directly inform a framework that learns energies for tractable subproblems and composes them to generalize reasoning to out-of-distribution, higher-complexity instances.",
  "analysis_timestamp": "2026-01-07T00:02:04.950429"
}
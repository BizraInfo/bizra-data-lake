{
  "prior_works": [
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho et al.",
      "year": 2020,
      "arxiv_id": "2006.11239",
      "role": "Foundation",
      "relationship_sentence": "This work established the Gaussian denoising formulation with a learned mean and fixed (or learned) variance per diffusion step, defining the exact probabilistic structure whose diagonal covariance the current paper seeks to optimally learn."
    },
    {
      "title": "Improved Denoising Diffusion Probabilistic Models",
      "authors": "Alex Nichol et al.",
      "year": 2021,
      "arxiv_id": "2102.09672",
      "role": "Baseline",
      "relationship_sentence": "It introduced learning the (diagonal) log-variance via a data-driven objective, which serves as the primary baseline and whose bias/approximation limitations the proposed Optimal Covariance Matching directly addresses by regressing an analytic target."
    },
    {
      "title": "Variational Diffusion Models",
      "authors": "Jonathan Ho et al.",
      "year": 2021,
      "arxiv_id": "2107.00630",
      "role": "Gap Identification",
      "relationship_sentence": "By casting diffusion training as ELBO maximization with learnable variance parameters, this paper highlighted that variance learning is tied to variational surrogates, motivating the need for an unbiased objective that targets the analytically optimal covariance instead."
    },
    {
      "title": "Denoising Diffusion Implicit Models",
      "authors": "Jiaming Song et al.",
      "year": 2021,
      "arxiv_id": "2010.02502",
      "role": "Related Problem",
      "relationship_sentence": "DDIM\u2019s deterministic (zero-variance) non-Markovian sampling emphasized the critical role of reverse-process covariance in trading off speed and fidelity, motivating methods that accurately learn per-step covariances to achieve efficient yet high-recall sampling."
    },
    {
      "title": "Elucidating the Design Space of Diffusion-Based Generative Models",
      "authors": "Tero Karras et al.",
      "year": 2022,
      "arxiv_id": "2206.00364",
      "role": "Inspiration",
      "relationship_sentence": "Its analysis of log-SNR weighting and parameterization demonstrated how calibration of noise levels materially affects likelihood and sample quality, directly motivating a principled, optimally matched covariance rather than heuristic or purely data-driven choices."
    },
    {
      "title": "Score Jacobian Chaser: Fast and Accurate DPM Samplers by Tracing the Score Jacobian",
      "authors": "Xiang Li et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "By exploiting local curvature (score Jacobian) to improve sampling accuracy, this work underscored that second-order information governs optimal local Gaussian approximations, informing the present paper\u2019s focus on analytically grounded covariance targets."
    }
  ],
  "synthesis_narrative": "The denoising diffusion probabilistic model formalized the reverse process as Gaussian with a learned mean and a per-step variance, anchoring how covariance enters the generative mechanism. Building on that, improved DDPM proposed learning the diagonal log-variance, tying variance to a network output optimized with a practical surrogate, while Variational Diffusion Models cast variance learning into an ELBO framework that couples covariance calibration to a variational bound. Denoising Diffusion Implicit Models demonstrated that setting reverse-process variance to zero yields faster, deterministic sampling but can compromise fidelity\u2014highlighting the delicate role of covariance in efficiency\u2013quality trade-offs. Elucidating the Design Space systematically showed that proper calibration of noise levels and loss weighting strongly impacts NLL and sample quality, reinforcing the importance of correctly specified per-step uncertainty. Complementarily, Score Jacobian Chaser evidenced that incorporating local second-order information can materially improve sampling, indicating that optimal local Gaussian approximations depend on more than mean estimates alone.\nTaken together, these works reveal both the necessity and difficulty of learning covariances: existing approaches optimize heuristic or variational surrogates and can be biased, while efficiency-sensitive samplers expose how miscalibrated variance harms recall and likelihood. The present paper synthesizes these insights by targeting the analytically optimal diagonal covariance via an unbiased regression objective, operationalizing a moment-matching view that decouples covariance estimation from data-driven surrogates and delivering calibrated uncertainty that improves sampling efficiency, recall, and likelihood in both pixel-space and latent diffusion models.",
  "target_paper": {
    "title": "Improving Probabilistic Diffusion Models With Optimal Diagonal Covariance Matching",
    "authors": "Zijing Ou, Mingtian Zhang, Andi Zhang, Tim Z. Xiao, Yingzhen Li, David Barber",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Diffusion Model, Generative Model, Probalistic Modelling",
    "abstract": "The probabilistic diffusion model has become highly effective across various domains. Typically, sampling from a diffusion model involves using a denoising distribution characterized by a Gaussian with a learned mean and either fixed or learned covariances. In this paper, we leverage the recently proposed covariance moment matching technique and introduce a novel method for learning the diagonal covariances. Unlike traditional data-driven covariance approximation approaches, our method involves directly regressing the optimal analytic covariance using a new, unbiased objective named Optimal Covariance Matching (OCM). This approach can significantly reduce the approximation error in covariance prediction. We demonstrate how our method can substantially enhance the sampling efficiency, recall rate and likelihood of both diffusion models and latent diffusion models.",
    "openreview_id": "fV0t65OBUu",
    "forum_id": "fV0t65OBUu"
  },
  "analysis_timestamp": "2026-01-06T16:38:49.216642"
}
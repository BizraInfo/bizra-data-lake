{
  "prior_works": [
    {
      "title": "DeepCoder: Learning to Write Programs",
      "authors": "Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, Daniel Tarlow",
      "year": 2017,
      "role": "Neural-guided program synthesis from I/O examples",
      "relationship_sentence": "LPN inherits DeepCoder\u2019s insight of using neural models with I/O supervision for program induction but replaces brittle, combinatorial DSL search with gradient-based search in a learned continuous latent program space."
    },
    {
      "title": "DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning",
      "authors": "Kevin Ellis, Catherine Wong, Maxwell Nye, Joshua B. Tenenbaum, Armando Solar-Lezama",
      "year": 2021,
      "role": "Library/DSL learning to shrink program search",
      "relationship_sentence": "DreamCoder shows that learning a compact program prior dramatically reduces symbolic search; LPN carries this idea into the neural regime by learning a compact latent program manifold that obviates hand-designed DSLs while enabling efficient search."
    },
    {
      "title": "Neural Programmer-Interpreter",
      "authors": "Scott Reed, Nando de Freitas",
      "year": 2016,
      "role": "Neural program induction and execution",
      "relationship_sentence": "NPI demonstrated that neural networks can represent and execute programs; LPN advances this by representing programs implicitly as latent codes and enabling test-time gradient search over those codes for adaptation."
    },
    {
      "title": "HyperNetworks",
      "authors": "David Ha, Andrew M. Dai, Quoc V. Le",
      "year": 2016,
      "role": "Latent-to-weights generators",
      "relationship_sentence": "HyperNetworks introduced mapping low-dimensional codes to network parameters; LPN similarly maps latent program codes to implicit behaviors and performs test-time optimization in the latent space rather than over full model weights."
    },
    {
      "title": "Meta-Learning with Latent Embedding Optimization (LEO)",
      "authors": "Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, Raia Hadsell",
      "year": 2019,
      "role": "Few-shot learning via test-time optimization in a latent space",
      "relationship_sentence": "LEO\u2019s core mechanism of optimizing a low-dimensional latent code at test time directly informs LPN\u2019s strategy of gradient-based search over a compact latent program space for rapid task adaptation."
    },
    {
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (MAML)",
      "authors": "Chelsea Finn, Pieter Abbeel, Sergey Levine",
      "year": 2017,
      "role": "Gradient-based meta-learning for fast adaptation",
      "relationship_sentence": "MAML motivates LPN\u2019s design principle of building in structures that are easily adapted via a few gradient steps, with LPN relocating adaptation from parameter space to a structured latent program space."
    },
    {
      "title": "Structured Prediction Energy Networks",
      "authors": "David Belanger, Andrew McCallum",
      "year": 2016,
      "role": "Inference-time optimization via gradients",
      "relationship_sentence": "SPENs established gradient-based test-time inference for structured outputs; LPN adopts this inference-as-optimization view but applies it to latent program codes to yield structured, generalizable behaviors."
    }
  ],
  "synthesis_narrative": "Latent Program Networks (LPN) fuse two historically separate strands: program synthesis\u2019 strong compositional generalization and deep learning\u2019s scalable approximation. Neural-guided synthesis (DeepCoder) demonstrated that learning from I/O pairs can steer symbolic search, while DreamCoder showed that learning compact program priors (libraries/DSLs) is key to tractability. LPN inherits these lessons but removes the dependence on hand-crafted symbolic spaces by learning a compact, continuous latent manifold of implicit programs, making search scalable and data-driven.\n\nOn the neural side, NPI established that networks can represent and execute programs, and HyperNetworks introduced the powerful abstraction of mapping low-dimensional codes to model behaviors via generated parameters. LPN operationalizes these ideas by treating the program as a latent code whose decoding induces an input-output mapping, enabling flexible composition without explicit program trees. Crucially, LPN builds structured test-time adaptation into the model: inspired by MAML\u2019s few-step gradient adaptation and LEO\u2019s optimization in a low-dimensional latent space, LPN performs inference-time gradient search directly over its latent program variables. This preserves sample-efficient generalization while avoiding expensive full-parameter finetuning.\n\nFinally, SPENs contribute the perspective that prediction can be framed as optimization, performed at inference via gradients. LPN adopts this inference-as-optimization principle but relocates it to a learned program latent, yielding a compact, continuous search space that combines the adaptability of symbolic methods with the scalability of neural networks, and enabling efficient, structured test-time search without reliance on human-designed DSLs or heavy stochastic sampling.",
  "analysis_timestamp": "2026-01-07T00:02:04.982718"
}
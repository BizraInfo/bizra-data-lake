{
  "prior_works": [
    {
      "title": "Scene Representation Transformer: Geometry-Free Novel View Synthesis through Set-Latent Scene Representations",
      "authors": "Mehdi S. M. Sajjadi et al.",
      "year": 2022,
      "arxiv_id": "2111.13152",
      "role": "Extension",
      "relationship_sentence": "DyST directly extends SRT\u2019s set-latent, transformer-based scene encoder\u2013renderer by adding temporal conditioning and a factorized latent split into scene content, per-view dynamics, and camera pose to handle real-world dynamic videos."
    },
    {
      "title": "Neural Scene Representation and Rendering (GQN)",
      "authors": "S. M. Ali Eslami et al.",
      "year": 2018,
      "arxiv_id": "1807.00734",
      "role": "Foundation",
      "relationship_sentence": "DyST adopts the GQN-style context-to-query training objective and latent scene representation paradigm as the basis for learning to render novel views from observations."
    },
    {
      "title": "D-NeRF: Neural Radiance Fields for Dynamic Scenes",
      "authors": "Alberto Pumarola et al.",
      "year": 2021,
      "arxiv_id": "2011.13961",
      "role": "Inspiration",
      "relationship_sentence": "DyST generalizes D-NeRF\u2019s key idea of separating canonical scene content from time-varying dynamics by replacing explicit deformation fields with a learned per-view dynamics latent that modulates rendering independently of content."
    },
    {
      "title": "Nerfies: Deformable Neural Radiance Fields",
      "authors": "Keunhong Park et al.",
      "year": 2021,
      "arxiv_id": "2011.12948",
      "role": "Gap Identification",
      "relationship_sentence": "DyST addresses Nerfies\u2019 reliance on calibrated multi-view captures and explicit deformation modeling by learning camera and dynamics jointly from monocular real-world videos within a latent, transformer-based scene model."
    },
    {
      "title": "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections",
      "authors": "Ricardo Martin-Brualla et al.",
      "year": 2021,
      "arxiv_id": "2008.02268",
      "role": "Inspiration",
      "relationship_sentence": "DyST adapts NeRF-W\u2019s per-image latent concept\u2014used to explain view-specific appearance/transients\u2014into a per-view dynamics code that captures transient motion and view-dependent factors while keeping scene content stable."
    },
    {
      "title": "BARF: Bundle-Adjusting Neural Radiance Fields",
      "authors": "Chen-Hsuan Lin et al.",
      "year": 2021,
      "arxiv_id": "2104.06405",
      "role": "Foundation",
      "relationship_sentence": "DyST leverages BARF\u2019s insight that camera poses can be learned jointly with neural rendering, but encodes pose as an explicit latent disentangled from content and dynamics to enable controllable generation."
    },
    {
      "title": "DynIBaR: Neural Dynamic Image-Based Rendering",
      "authors": "Zhengqi Li et al.",
      "year": 2023,
      "arxiv_id": "2211.11043",
      "role": "Baseline",
      "relationship_sentence": "Targeting dynamic novel view synthesis from casual monocular videos like DynIBaR, DyST replaces feature aggregation/warping with a learned latent scene representation that affords separate control over camera and scene dynamics."
    }
  ],
  "synthesis_narrative": "Transformer-based scene representation learning matured with the Scene Representation Transformer (SRT), which encodes a set of context views into a geometry-free latent and renders target views via attention, establishing a powerful encoder\u2013renderer for generalizable view synthesis. Earlier, the Generative Query Network (GQN) introduced the context-to-query formulation and latent scene representations trained purely by novel-view supervision, laying the conceptual basis for learning to render from observations without explicit geometry. For dynamic scenes, D-NeRF proposed factoring a canonical content field from time-dependent deformation, while Nerfies operationalized deformable radiance fields but assumed calibrated multiview captures and explicit warping. NeRF in the Wild (NeRF-W) demonstrated that per-image latent codes can soak up view-specific effects and transients, suggesting a path to factor view-dependent phenomena from persistent scene content. BARF showed that camera parameters can be optimized jointly with neural rendering, revealing that pose can be treated as learnable variables within the rendering objective. Concurrently, DynIBaR tackled dynamic novel view synthesis from casual videos via feature aggregation and warping rather than an explicit latent scene model. Together, these works suggest combining a generalizable transformer-based scene encoder\u2013renderer with principled factorization: persistent content separated from per-view dynamics and learnable camera. The limitations of deformable NeRFs (calibration dependence, explicit warps) and image-based renderers (lack of disentangled control) motivate a latent, factorized dynamic scene representation trained with a GQN/SRT-style objective and pose learning, naturally leading to DyST\u2019s design and co-training strategy for real-world monocular videos.",
  "target_paper": {
    "title": "DyST: Towards Dynamic Neural Scene Representations on Real-World Videos",
    "authors": "Maximilian Seitzer, Sjoerd van Steenkiste, Thomas Kipf, Klaus Greff, Mehdi S. M. Sajjadi",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "neural scene representations, scene representations, representation learning, novel view synthesis",
    "abstract": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
    "openreview_id": "MnMWa94t12",
    "forum_id": "MnMWa94t12"
  },
  "analysis_timestamp": "2026-01-06T11:23:22.849419"
}
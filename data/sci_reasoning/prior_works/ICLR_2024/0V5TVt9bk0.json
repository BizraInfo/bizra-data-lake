{
  "prior_works": [
    {
      "title": "Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Foundation",
      "relationship_sentence": "Q-Bench targets and probes the very class of general-purpose MLLMs instantiated by LLaVA\u2019s visual instruction\u2013tuned QA interface, adopting this QA-style interaction to evaluate low-level perception and description."
    },
    {
      "title": "MMBench: A Holistic Evaluation of Multimodal Large Language Models",
      "authors": "Anonymous et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "Q-Bench explicitly addresses MMBench\u2019s lack of dedicated low-level visual perception and quality assessment coverage by introducing purpose-built tasks and datasets for these abilities."
    },
    {
      "title": "SEED-Bench: Benchmarking Multimodal Large Language Models",
      "authors": "Anonymous et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "Q-Bench fills SEED-Bench\u2019s identified gap where recognition and reasoning dominate while fine-grained degradation perception and visual quality understanding are largely absent."
    },
    {
      "title": "MME: A Comprehensive Evaluation of Multimodal Large Language Models",
      "authors": "Anonymous et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "Q-Bench is designed to complement MME by adding systematic evaluation dimensions specifically for low-level perception, detailed low-level descriptions, and overall image quality\u2014areas MME does not directly test."
    },
    {
      "title": "PIPAL: A Large-Scale Image Quality Assessment Dataset for Perceptual Image Restoration",
      "authors": "Jinjin Gu et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Q-Bench\u2019s overall visual quality assessment component draws on PIPAL\u2019s formulation of human-perceived quality for restoration artifacts to define stimuli and assessment criteria for MLLMs."
    },
    {
      "title": "KonIQ-10k: An Ecologically Valid Database for Deep Learning of Blind Image Quality Assessment",
      "authors": "Ionut Hosu et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Q-Bench leverages KonIQ-10k\u2019s in-the-wild MOS-based IQA paradigm to ground its evaluation of MLLMs on authentic distortions and human quality judgments."
    },
    {
      "title": "LIVE In the Wild Image Quality Challenge Database",
      "authors": "Kede Ma et al.",
      "year": 2016,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Q-Bench adopts LIVE Challenge\u2019s authentic-distortion, MOS-driven problem setup as the basis for testing whether MLLMs can assess overall visual quality beyond synthetic degradations."
    }
  ],
  "synthesis_narrative": "Visual instruction\u2013tuned MLLMs were crystallized by Visual Instruction Tuning (LLaVA), which established a QA-style interface enabling general-purpose vision-language agents to be probed through natural language. Parallelly, multimodal benchmarks such as MMBench, SEED-Bench, and MME organized comprehensive test suites for recognition, reasoning, and grounding, but their task inventories largely emphasized high-level semantics and commonsense while providing little direct coverage of low-level perception of degradations or visual quality judgment. On the image quality side, PIPAL introduced large-scale human judgments targeting perceptual artifacts from restoration algorithms, emphasizing perceptual fidelity beyond simple distortion metrics. KonIQ-10k contributed an ecologically valid, in-the-wild MOS framework for no-reference IQA, highlighting authentic distortions. The LIVE In the Wild Challenge similarly framed IQA with authentic degradations and MOS, becoming a cornerstone for evaluating subjective visual quality in the wild.\n\nThese lines of work collectively suggested an opportunity: MLLMs, already queried via instruction-tuned QA, lacked targeted evaluation on low-level perception and human-aligned quality understanding that IQA datasets rigorously define. Building on LLaVA\u2019s interaction paradigm while addressing the gaps in existing MLLM benchmarks, and grounding tasks in the MOS- and artifact-centric formulations from PIPAL, KonIQ-10k, and LIVE Challenge, Q-Bench naturally emerges as a benchmark that systematizes three complementary dimensions\u2014low-level perception, descriptive articulation of low-level attributes, and overall visual quality assessment\u2014to holistically test whether general-purpose MLLMs possess low-level visual competence.",
  "target_paper": {
    "title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision",
    "authors": "Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Benchmark, Vision-Language, Large Language Models, Low-level Vision, Image Quality Assessment",
    "abstract": "The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on **low-level visual perception and understanding**. To address this gap, we present **Q-Bench**, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. **_a)_** To evaluate the low-level **_perception_** ability, we construct the **LLVisionQA** dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. **_b)_** To examine the **_description_** ability of MLLMs on low-level information, we propose the **LLDescribe** dataset consisting of long expert-label",
    "openreview_id": "0V5TVt9bk0",
    "forum_id": "0V5TVt9bk0"
  },
  "analysis_timestamp": "2026-01-07T00:16:55.625927"
}
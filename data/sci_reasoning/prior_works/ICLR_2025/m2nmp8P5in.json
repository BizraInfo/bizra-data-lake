{
  "prior_works": [
    {
      "title": "Distilling free-form natural laws from experimental data",
      "authors": "Michael Schmidt et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "LLM-SR builds on the original symbolic regression formulation of discovering equations from data via evolutionary search, but replaces tree-mutation heuristics with LLM-guided program generation and selection."
    },
    {
      "title": "AI Feynman: A physics-inspired method for symbolic regression",
      "authors": "Silviu-Marian Udrescu et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "By showing that hand-crafted scientific priors (e.g., dimensional analysis and decomposition) significantly aid discovery yet remain domain- and heuristic-specific, AI Feynman motivates LLM-SR\u2019s key idea of injecting broad domain knowledge through LLM-generated programmatic equation templates and constraints."
    },
    {
      "title": "PySR: High-Performance Symbolic Regression in Python and Julia",
      "authors": "Miles Cranmer et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "As a state-of-the-art tree-based genetic programming system widely used for equation discovery, PySR serves as the primary baseline that LLM-SR improves upon by moving from expression trees to LLM-authored program representations and search proposals."
    },
    {
      "title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
      "authors": "Steven L. Brunton et al.",
      "year": 2016,
      "role": "Related Problem",
      "relationship_sentence": "SINDy formalized equation discovery from data via sparse regression on a fixed function library, whose library-dependence LLM-SR addresses by letting an LLM propose open-ended operator programs that are scored on data fit and scientific consistency."
    },
    {
      "title": "Mathematical discoveries from program search with large language models",
      "authors": "Bernardino Romera-Paredes et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "LLM-SR adapts the FunSearch paradigm\u2014LLM-generated executable programs evaluated by an external scorer with evolutionary selection\u2014to the symbolic regression setting, turning equations into programs and closing the loop with data-driven fitness and scientific priors."
    }
  ],
  "synthesis_narrative": "LLM-SR sits at the intersection of two intellectual lineages. The first, inaugurated by Schmidt and Lipson, framed equation discovery as symbolic regression via evolutionary search over expression trees; this tradition matured through powerful toolkits like PySR and methods such as SINDy, which recast discovery as sparse selection from a predefined function library. These approaches clarified the problem and delivered strong baselines, but also exposed core limits: tree-bound expressiveness, reliance on hand-designed operators or libraries, and difficulty incorporating rich, domain-specific priors at scale. AI Feynman sharpened these limits by demonstrating that physics-informed heuristics (e.g., dimensional analysis, separability) can dramatically help\u2014but require bespoke, domain-specific engineering. The second lineage, exemplified by FunSearch, showed that large language models can act as program synthesizers in a closed loop: generate code, score it with an external evaluator, and iterate via evolutionary selection. LLM-SR fuses these strands into its key contribution: casting equations as executable programs and using an LLM to propose, refine, and recombine candidate equation-programs that are evaluated on data fit while enforcing scientific plausibility. This replaces ad hoc tree mutations and fixed libraries with LLM-driven, open-ended program generation augmented by domain knowledge encoded in the model and prompts, directly tackling the expressiveness and prior-integration gaps in prior symbolic regression systems.",
  "analysis_timestamp": "2026-01-06T23:09:26.639967"
}
{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Foundational theory of the lazy-training regime and kernel dynamics at infinite width",
      "relationship_sentence": "Established the infinite-width NTK framework where kernels are static during training, providing the lazy-limit baseline whose prediction-variance behavior this paper recovers and contrasts with the rich, feature-learning regime."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat, Edouard Oyallon, Francis Bach",
      "year": 2019,
      "role": "Conceptual framework distinguishing lazy (kernel) vs mean-field (feature-learning) regimes",
      "relationship_sentence": "Formalized the lazy vs feature-learning dichotomy that this work explicitly analyzes by deriving finite-width fluctuations in both regimes and demonstrating qualitatively different variance behavior."
    },
    {
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "authors": "Song Mei, Andrea Montanari, Phan-Minh Nguyen",
      "year": 2018,
      "role": "Mean-field (particle/measure) dynamics for feature learning in two-layer networks",
      "relationship_sentence": "Provided the mean-field training dynamics foundation for feature learning around which this paper develops an O(1/\u221awidth) fluctuation theory and quantifies variance reduction in two-layer networks."
    },
    {
      "title": "Neural Networks as Interacting Particle Systems: Asymptotic Convexity of the Loss Landscape and Convergence to Global Minima",
      "authors": "Grant Rotskoff, Eric Vanden-Eijnden",
      "year": 2018,
      "role": "Interacting-particle/mean-field PDE perspective on feature-learning dynamics",
      "relationship_sentence": "Supplied an alternative but complementary mean-field formulation of feature-learning dynamics that underlies the infinite-width order parameters whose finite-width fluctuations are computed here."
    },
    {
      "title": "Non-Gaussian Processes and Neural Networks at Finite Width",
      "authors": "Sho Yaida",
      "year": 2020,
      "role": "Finite-width perturbation theory and scaling of fluctuations around infinite-width limits",
      "relationship_sentence": "Developed systematic 1/width expansions and showed O(1/\u221awidth) fluctuations for wide networks, which this paper extends from initialization/static settings to full training dynamics of kernels and predictions."
    },
    {
      "title": "Tensor Programs V: NTK at Finite Width and Learning-Rate Tuning",
      "authors": "Greg Yang",
      "year": 2021,
      "role": "Precise characterization of finite-width corrections to NTK and training dynamics",
      "relationship_sentence": "Characterized finite-width corrections to NTK and their dependence on scaling, a key ingredient that this work generalizes to the dynamically coupled kernel\u2013prediction fluctuations in the feature-learning regime."
    },
    {
      "title": "Dynamical Mean-Field Theory for Stochastic Gradient Descent in High-Dimensional Learning",
      "authors": "Silvio L. Mignacco, Bruno Loureiro, Florent Krzakala, Lenka Zdeborov\u00e1",
      "year": 2021,
      "role": "DMFT methodology for time-evolving order parameters under SGD",
      "relationship_sentence": "Provided the DMFT framework to track infinite-width kernel and prediction dynamics over training, which this paper takes as a starting point and augments with a self-consistent theory of finite-width fluctuations."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014an O(1/\u221awidth) fluctuation theory for kernels and predictions that is non-perturbative in feature learning\u2014sits at the intersection of three lines of prior work. First, NTK theory (Jacot et al.) and its formalization of the lazy regime (Chizat et al.) established the infinite-width baseline where kernels remain static and predictions follow a linear model, furnishing the reference point whose universal prediction-variance form this paper recovers in the lazy limit. Second, mean-field analyses of training dynamics in the rich, feature-learning regime for two-layer networks (Mei\u2013Montanari\u2013Nguyen; Rotskoff\u2013Vanden-Eijnden) provided the self-consistent, distributional evolution of network features and kernels at infinite width. Building directly on this, recent DMFT for SGD (Mignacco\u2013Loureiro\u2013Krzakala\u2013Zdeborov\u00e1) supplied a practical framework to track time-dependent order parameters and predictions during learning, which this work adopts as its infinite-width backbone. Third, finite-width theory (Yaida; Yang) quantified how wide-but-finite networks deviate from their infinite-width limits, identifying 1/width corrections and O(1/\u221awidth) fluctuations\u2014insights this paper extends from initialization and lazy settings to fully dynamic, feature-learning regimes. By synthesizing DMFT with finite-width perturbations, the authors derive self-consistent equations for the coupled fluctuations of kernels and predictions, revealing that feature learning can dynamically suppress prediction variance, especially in two-layer networks, and characterizing how initialization and learning-rate choices control these fluctuations.",
  "analysis_timestamp": "2026-01-07T00:02:04.868693"
}
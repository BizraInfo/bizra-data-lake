{
  "prior_works": [
    {
      "title": "8-bit Matrix Multiplication for Transformers at Scale (LLM.int8())",
      "authors": "Tim Dettmers et al.",
      "year": 2022,
      "role": "Outlier analysis and mixed-precision precedent for activations",
      "relationship_sentence": "LLM.int8 identified extreme activation outliers in LLMs and handled them by keeping a small set of outlier channels in higher precision, directly motivating ResQ\u2019s strategy of preserving a small subspace at higher precision while quantizing the rest more aggressively."
    },
    {
      "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
      "authors": "Zhewei Yao et al.",
      "year": 2023,
      "role": "Activation smoothing to mitigate outliers in PTQ",
      "relationship_sentence": "SmoothQuant shifts activation outlier magnitude into weights via per-channel scaling, framing the activation-outlier bottleneck that ResQ addresses via PCA-based subspace selection and mixed-precision allocation for activations/KV."
    },
    {
      "title": "AWQ: Activation-aware Weight Quantization for LLMs",
      "authors": "Chong Lin et al.",
      "year": 2023,
      "role": "Direction/channel importance for robust low-bit quantization",
      "relationship_sentence": "AWQ preserves salient directions using activation-aware criteria; ResQ generalizes this notion from channels to principal components, selecting a low-rank, high-variance subspace to keep at higher precision."
    },
    {
      "title": "SpQR: A Sparse-Quantized Representation for Neural Networks",
      "authors": "Aleksandar Frantar et al.",
      "year": 2023,
      "role": "Selective higher precision for outliers with mixed representations",
      "relationship_sentence": "SpQR splits and stores outliers separately while quantizing the majority to low bitwidths; ResQ applies a similar selective precision idea but organizes it via PCA-derived subspaces for activations and applies uniform low bits elsewhere."
    },
    {
      "title": "Rotated Post-Training Quantization (RPTQ) / QuaRot",
      "authors": "Anonymous et al.",
      "year": 2024,
      "role": "Rotation-based outlier suppression for low-bit quantization",
      "relationship_sentence": "Rotation or Hadamard-based transforms reduce kurtosis and equalize activation/weight distributions; ResQ integrates invariant random rotations within subspaces to further suppress outliers before quantization."
    },
    {
      "title": "HAWQ/HAWQ-V2: Hessian Aware Quantization of Neural Networks with Mixed Precision",
      "authors": "Yuxin Dong et al.",
      "year": 2019,
      "role": "Mixed-precision bit allocation guided by sensitivity",
      "relationship_sentence": "HAWQ formalized sensitivity-aware mixed-precision assignment; ResQ\u2019s mixed-precision allocation across PCA components echoes this principle by allocating higher precision to statistically sensitive (high-variance) directions."
    },
    {
      "title": "Vector Quantization and Signal Compression",
      "authors": "Allen Gersho and Robert M. Gray",
      "year": 1992,
      "role": "Transform coding theory (KLT/PCA and optimal bit allocation)",
      "relationship_sentence": "Classical transform coding shows PCA/KLT concentrates variance and enables optimal bit allocation for MSE; ResQ\u2019s PCA-based subspace selection and claim of optimal mixed-precision error minimization are direct instantiations of this theory."
    }
  ],
  "synthesis_narrative": "ResQ\u2019s core idea\u2014identify a low-rank subspace of high-variance activations and retain it in higher precision while quantizing the remaining dimensions\u2014sits at the intersection of modern LLM quantization practice and classic transform-coding theory. LLM.int8 first exposed activation outliers as the fundamental obstacle to low-bit inference and introduced a mixed-precision remedy by isolating and computing outlier channels in higher precision, a conceptual lineage ResQ embraces and generalizes. SmoothQuant further cemented activation outliers as the bottleneck, proposing to smooth them via per-channel scaling that shifts difficulty into weights; ResQ instead attacks the problem directly in activation space by using PCA to extract principal directions that concentrate variance. AWQ and SpQR demonstrated that preserving salient directions or explicitly separating outliers while quantizing the rest can sustain accuracy at 4 bits\u2014ResQ reinterprets this selectivity geometrically by protecting principal components and quantizing the orthogonal complement. Rotation-based methods such as RPTQ/QuaRot showed that orthogonal transforms (e.g., Hadamard) suppress heavy tails and equalize distributions; ResQ incorporates invariant random rotations within each subspace to further reduce outlier effects pre-quantization. The mixed-precision assignment itself is guided by sensitivity principles popularized by HAWQ, while its provable optimality aligns with transform-coding results from Gersho and Gray: PCA/KLT achieves optimal energy compaction, and allocating more bits to high-variance components minimizes quantization error. Together, these strands converge in ResQ\u2019s principled, subspace-structured mixed-precision PTQ for weights, activations, and KV caches.",
  "analysis_timestamp": "2026-01-07T00:21:32.383435"
}
{
  "prior_works": [
    {
      "title": "Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation",
      "authors": "Yoshua Bengio et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Introduces Generative Flow Networks (GFlowNets), the core paradigm of learning a policy that samples objects proportional to an unnormalized target, which this paper adopts to realize posterior sampling for LLMs."
    },
    {
      "title": "Trajectory Balance: Improved Credit Assignment in GFlowNets",
      "authors": "Nikolay Malkin et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Provides the trajectory balance objective and credit assignment scheme that this work leverages to train sequence-level GFlowNets over token trajectories when amortizing posterior inference."
    },
    {
      "title": "Bayesian Structure Learning with Generative Flow Networks",
      "authors": "Tristan Deleu et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates GFlowNets as amortized samplers of complex Bayesian posteriors in discrete spaces, directly motivating their use here to sample intractable posteriors over language sequences and rationales."
    },
    {
      "title": "Auto-Encoding Variational Bayes",
      "authors": "Diederik P. Kingma et al.",
      "year": 2013,
      "arxiv_id": "1312.6114",
      "role": "Foundation",
      "relationship_sentence": "Introduces amortized variational inference\u2014learning an inference network to approximate intractable posteriors\u2014which this paper adapts conceptually by fine-tuning LLMs to amortize posterior sampling via GFlowNets."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "arxiv_id": "2201.11903",
      "role": "Foundation",
      "relationship_sentence": "Establishes explicit reasoning traces (rationales) in LMs, enabling this paper\u2019s formulation of chain-of-thought as latent variables whose posterior should be sampled."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2023,
      "arxiv_id": "2210.11610",
      "role": "Gap Identification",
      "relationship_sentence": "Shows that aggregating multiple diverse reasoning paths improves accuracy but relies on heuristic sampling and voting, highlighting the need for a principled posterior sampler over rationales addressed here."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "arxiv_id": "2203.02155",
      "role": "Baseline",
      "relationship_sentence": "Represents the dominant reward-maximizing RL fine-tuning approach (RLHF) that this work contrasts with a distribution-matching, diversity-seeking alternative using GFlowNets."
    }
  ],
  "synthesis_narrative": "Generative Flow Networks introduced a learning paradigm in which a stochastic policy samples structured objects in proportion to an unnormalized target, making diversity not a byproduct but the objective of training. The Trajectory Balance objective refined this framework with a stable credit assignment scheme for long, compositional trajectories, enabling practical training on sequence-like objects. Building on these ideas, Bayesian Structure Learning with GFlowNets demonstrated that GFlowNets can act as amortized samplers for complex posteriors in discrete domains, showing their suitability for Bayesian inference beyond synthetic settings. Auto-Encoding Variational Bayes established the principle of amortized inference\u2014learning an inference model that approximates intractable posteriors\u2014which provides the conceptual template for learning to sample posteriors across many inputs. Chain-of-Thought Prompting defined explicit intermediate rationales in language models, while Self-Consistency revealed that exploring multiple diverse reasoning paths and marginalizing improves performance, albeit with heuristic sampling and voting. Finally, InstructGPT (RLHF) exemplified reward-maximizing fine-tuning, aligning outputs but often collapsing diversity and not targeting a posterior distribution.\nTaken together, these works expose a clear opportunity: treat reasoning traces and constrained generations as latent-variable posteriors and learn a policy that samples from them directly. By marrying the amortization principle of variational methods with the diversity-seeking, unnormalized distribution-matching of GFlowNets\u2014and operationalizing it via trajectory-balance training on token trajectories\u2014the current work naturally emerges as a principled alternative to MLE and RLHF, providing posterior-consistent sampling over rationales and constraints rather than reward-maximizing point solutions.",
  "target_paper": {
    "title": "Amortizing intractable inference in large language models",
    "authors": "Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, Nikolay Malkin",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "large language models, LLMs, Bayesian inference, chain-of-thought reasoning, latent variable models, generative flow networks, GFlowNets",
    "abstract": "Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest---including sequence continuation, infilling, and other forms of constrained generation---involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate th",
    "openreview_id": "Ouj6p4ca60",
    "forum_id": "Ouj6p4ca60"
  },
  "analysis_timestamp": "2026-01-06T19:52:28.812102"
}
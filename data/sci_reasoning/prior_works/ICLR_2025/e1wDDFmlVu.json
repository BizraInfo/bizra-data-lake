{
  "prior_works": [
    {
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "authors": "Noam Shazeer et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduced the sparsely-gated MoE paradigm\u2014conditional expert routing with load-balancing\u2014that Time-MoE directly adopts to achieve high capacity without proportional compute growth."
    },
    {
      "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
      "authors": "Dmitry Lepikhin et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Demonstrated practical large-scale training of MoE Transformers with distributed routing and sharding, techniques Time-MoE extends to scale billion-parameter time-series models efficiently."
    },
    {
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "authors": "William Fedus et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Provided the top-1 expert routing and simplified MoE design that directly motivates Time-MoE\u2019s sparse decoder-only architecture to keep inference costs low while growing capacity."
    },
    {
      "title": "TimeGPT-1: A Ready-to-Use Foundation Model for Time Series Forecasting",
      "authors": "Cristian Challu et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "Established the foundation-model paradigm for time series via large-scale pretraining but used dense models with inference cost scaling with size\u2014a core limitation Time-MoE overcomes with sparse MoE."
    },
    {
      "title": "Chronos: Learning the Language of Time Series",
      "authors": "Yuyang Wang et al.",
      "year": 2024,
      "role": "Baseline",
      "relationship_sentence": "Framed time series forecasting as decoder-only autoregressive modeling over tokenized values; Time-MoE keeps this formulation but replaces dense transformers with sparse MoE to improve scalability and efficiency."
    },
    {
      "title": "V-MoE: Learning Sparse Mixtures of Experts in Vision Transformers",
      "authors": "Carlos Riquelme et al.",
      "year": 2021,
      "role": "Related Problem",
      "relationship_sentence": "Showed MoE gains extend beyond language to other modalities, informing Time-MoE\u2019s application of sparse experts to non-text sequential data (time series)."
    },
    {
      "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
      "authors": "Qingsong Wen et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "Explored leveraging LLMs for time-series forecasting but remained dense and text-centered; Time-MoE instead builds a dedicated, sparse expert time-series foundation model to address scalability and inference cost."
    }
  ],
  "synthesis_narrative": "Time-MoE\u2019s core innovation is to combine the time-series foundation-model paradigm with sparse Mixture-of-Experts to achieve billion-scale capacity at manageable inference cost. The direct technical lineage of its sparsity comes from the MoE family: Shazeer et al. (2017) introduced the sparsely-gated MoE mechanism and load-balancing losses that enable conditional computation; Lepikhin et al. (2020) operationalized large-scale MoE training with distributed routing and sharding; and Fedus et al. (2021) simplified routing with Switch (top-1) gating to unlock massive parameter counts while keeping per-token FLOPs roughly constant. These works directly inform Time-MoE\u2019s decoder-only sparse-expert design and its efficiency claims. On the problem side, TimeGPT-1 and Chronos defined the modern time-series foundation model setting: large-scale pretraining with decoder-only, autoregressive forecasting over tokenized sequences. However, both rely on dense transformers, making inference cost scale with model size\u2014precisely the bottleneck Time-MoE targets. By retaining the successful autoregressive formulation of Chronos/TimeGPT-1 but swapping dense blocks for sparsely activated experts, Time-MoE inherits strong generalization while enabling scalable capacity. Evidence from V-MoE further supports transferring MoE benefits to non-text modalities, strengthening the case for MoE in time-series. Time-LLM underscores growing interest in cross-pollinating LLM techniques for time-series, while highlighting the need for domain-specialized, compute-efficient architectures\u2014needs Time-MoE addresses with a unified, sparse, decoder-only foundation model for forecasting.",
  "analysis_timestamp": "2026-01-06T23:09:26.612939"
}
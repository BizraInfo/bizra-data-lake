{
  "prior_works": [
    {
      "title": "Self-Paced Learning for Latent Variable Models",
      "authors": "M. P. Kumar et al.",
      "year": 2010,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "ENT adopts the self-paced principle of discarding/attenuating high-loss training examples, but replaces the loss-based criterion with a distribution-aware error norm tailored to generative token prediction."
    },
    {
      "title": "Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels",
      "authors": "Bo Han et al.",
      "year": 2018,
      "arxiv_id": "1804.06872",
      "role": "Baseline",
      "relationship_sentence": "As a primary small-loss selection baseline, Co-teaching\u2019s loss-thresholding motivates ENT\u2019s truncation, with ENT directly improving the selection signal by using a token-distribution error norm rather than NLL alone."
    },
    {
      "title": "Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels",
      "authors": "Zhilu Zhang et al.",
      "year": 2018,
      "arxiv_id": "1805.07836",
      "role": "Baseline",
      "relationship_sentence": "GCE is a standard robust alternative to cross-entropy under label noise that ENT explicitly improves upon by using a truncation signal derived from the full non-target token distribution instead of modifying the loss scalar."
    },
    {
      "title": "Dual conditional cross-entropy filtering of noisy parallel corpora",
      "authors": "Marcin Junczys-Dowmunt",
      "year": 2018,
      "arxiv_id": "1809.00197",
      "role": "Foundation",
      "relationship_sentence": "This influential MT data-filtering method relies on (model) cross-entropy/NLL to assess sentence quality, the exact NLL-only heuristic ENT critiques and generalizes by leveraging the distribution of non-target tokens."
    },
    {
      "title": "On the Impact of Noise in Neural Machine Translation",
      "authors": "Huda Khayrallah et al.",
      "year": 2018,
      "arxiv_id": "1804.10959",
      "role": "Gap Identification",
      "relationship_sentence": "Their analysis shows that noisy web/parallel data substantially harms generation models and that NLL-based filtering is imperfect, motivating ENT\u2019s search for a more faithful noise indicator than target-only NLL."
    },
    {
      "title": "Dynamic Data Selection for Neural Machine Translation",
      "authors": "Marlies van der Wees et al.",
      "year": 2017,
      "arxiv_id": "1708.00712",
      "role": "Related Problem",
      "relationship_sentence": "DDS uses cross-entropy-based scores to select training data over time, and ENT directly refines this selection idea by replacing sentence-level NLL heuristics with a per-token distributional error norm."
    },
    {
      "title": "Neural Text Generation with Unlikelihood Training",
      "authors": "Sean Welleck et al.",
      "year": 2020,
      "arxiv_id": "1908.04319",
      "role": "Inspiration",
      "relationship_sentence": "Unlikelihood explicitly leverages probabilities of non-target tokens in the training signal, providing the key insight that ENT generalizes into a principled error norm over the non-target distribution for noise-aware truncation."
    }
  ],
  "synthesis_narrative": "Self-paced learning introduced the core idea of improving robustness by prioritizing or discarding examples based on a difficulty signal derived from loss, establishing truncation as a training primitive. Co-teaching operationalized the \u201csmall-loss\u201d principle for noisy labels by dropping high-loss examples, offering a practical baseline for robust data selection during training. Generalized Cross Entropy provided a widely used alternative to cross-entropy that tempers sensitivity to noise but still relies on target-only supervision signals. In machine translation, dual conditional cross-entropy filtering became a dominant approach to clean noisy parallel corpora using model cross-entropy/perplexity, while dynamic data selection extended cross-entropy-based selection across training to emphasize better data over time. Khayrallah and Koehn systematically documented how noise in parallel/web data degrades neural MT and highlighted the limitations of relying purely on likelihood-based heuristics. Unlikelihood training, in contrast to target-only losses, explicitly used non-target token probabilities to guide generation, demonstrating that the full predictive distribution contains actionable training signals beyond the target token.\nTogether these works revealed both the promise and the limits of loss/NLL-based truncation and filtering: they can remove noise, but target-only scores are myopic. The natural next step was to retain the truncation framework while upgrading the signal used to decide what to trust. ENT synthesizes these threads by computing a distribution-aware error norm over non-target tokens for token-level truncation, directly addressing the documented failures of NLL-only selection and extending non-target-aware training signals into a robust example-filtering criterion for text generation.",
  "target_paper": {
    "title": "Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models",
    "authors": "Tianjian Li, Haoran Xu, Philipp Koehn, Daniel Khashabi, Kenton Murray",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "language generation, language modeling, machine translation, robustness, estimating data quality",
    "abstract": "Text generation models are notoriously vulnerable to errors in the training data. With the wide-spread availability of massive amounts of web-crawled data becoming more commonplace, how can we enhance the robustness of models trained on a massive amount of noisy web-crawled text? In our work, we propose Error Norm Truncation (ENT), a robust enhancement method to the standard training objective that truncates noisy data. Compared to methods that only uses the negative log-likelihood loss to estimate data quality, our method provides a more accurate estimation by considering the distribution of non-target tokens, which is often overlooked by previous work. Through comprehensive experiments across language modeling, machine translation, and text summarization, we show that equipping text generation models with ENT improves generation quality over standard training and previous soft and hard truncation methods. Furthermore, we show that our method improves the robustness of models against ",
    "openreview_id": "zMvMwNvs4R",
    "forum_id": "zMvMwNvs4R"
  },
  "analysis_timestamp": "2026-01-06T10:08:17.665376"
}
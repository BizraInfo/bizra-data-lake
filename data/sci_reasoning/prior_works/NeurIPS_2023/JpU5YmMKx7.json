{
  "prior_works": [
    {
      "title": "Measuring Information Transfer",
      "authors": "Thomas Schreiber",
      "year": 2000,
      "role": "Foundational definition of Transfer Entropy (TE) as a directed, model-free information-theoretic measure.",
      "relationship_sentence": "ATEn is explicitly presented as a generalization of Schreiber\u2019s TE, adding learnable attentional weights over time when estimating directed information flow."
    },
    {
      "title": "Local information transfer as a spatiotemporal filter for complex systems",
      "authors": "Joseph T. Lizier, Mikhail Prokopenko, Albert Y. Zomaya",
      "year": 2008,
      "role": "Introduced time-local (pointwise) transfer entropy to identify when information transfer occurs.",
      "relationship_sentence": "ATEn operationalizes the same intuition\u2014detecting transient, time-localized coupling\u2014by learning attention coefficients that emphasize moments where local information transfer is high."
    },
    {
      "title": "Transfer entropy\u2014a model-free measure of effective connectivity for the neurosciences",
      "authors": "Raul Vicente, Michael Wibral, Martin Lindner, Gustavo Pipa",
      "year": 2011,
      "role": "Established TE as a practical tool for inferring directed effective connectivity in neural systems.",
      "relationship_sentence": "ATEn directly targets the shortcomings of standard TE in neural data with strong self-dynamics and weak, intermittent coupling by reweighting time samples to recover sparse, transient influences."
    },
    {
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "authors": "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio",
      "year": 2014,
      "role": "Introduced the soft attention mechanism with learnable alignment weights.",
      "relationship_sentence": "ATEn borrows the core idea of learnable attention weights to focus inference on the most informative time points when computing directed information transfer."
    },
    {
      "title": "Attention Is All You Need",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin",
      "year": 2017,
      "role": "Generalized attention into a flexible, differentiable mechanism (self-attention) widely adopted in sequence modeling.",
      "relationship_sentence": "ATEn\u2019s autonomous, differentiable attention coefficients over time series are conceptually grounded in the transformer-style attention formalism."
    },
    {
      "title": "Causal Discovery with Attention-Based Convolutional Neural Networks (TCDF)",
      "authors": "Marlies H. Nauta, Doina Bucur, Christin Seifert",
      "year": 2019,
      "role": "Applied attention in neural architectures to identify temporal causal relations and lags in multivariate time series.",
      "relationship_sentence": "ATEn aligns with TCDF\u2019s principle of using attention to locate causally informative time regions, but differs by optimizing an information-theoretic (TE-based) objective rather than a purely predictive one."
    }
  ],
  "synthesis_narrative": "The core contribution of ATEn\u2014an attentive generalization of transfer entropy for network reconstruction under transient, weak coupling\u2014rests on two pillars: transfer entropy as a directed information metric and attention as a learnable focusing mechanism. Schreiber\u2019s definition of TE provides the fundamental objective for measuring directed information flow, while Lizier et al.\u2019s local transfer entropy shows that information transfer is inherently time-local and can be used to highlight transient events. Building on this, Vicente et al. demonstrated TE\u2019s practical value for neural effective connectivity, revealing both its promise and its limitations when coupling is sparse and easily drowned by strong self-dynamics.\nIn parallel, the attention literature\u2014originating with Bahdanau et al.\u2019s soft alignment and generalized by Vaswani et al.\u2019s transformer\u2014introduced differentiable, data-driven weighting over sequence elements. ATEn synthesizes these threads by learning attention coefficients that reweight time points specifically to maximize a TE-based criterion, thereby isolating moments when coupling genuinely manifests. This idea closely resonates with TCDF, which uses attention to uncover temporal causal structure; however, ATEn grounds the attention in an explicitly information-theoretic objective rather than predictive loss. The result is a mechanism that preserves the model-free, directed nature of TE, inherits the temporal specificity of local TE, and gains the adaptivity of neural attention, enabling robust recovery of edges in dissipative systems where coupling emerges only briefly.",
  "analysis_timestamp": "2026-01-06T23:42:48.038001"
}
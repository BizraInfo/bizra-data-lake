{
  "prior_works": [
    {
      "title": "Entropy Search for Information-Efficient Global Optimization",
      "authors": "Philipp Hennig and Christian J. Schuler",
      "year": 2012,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Entropy Search formalized Bayesian optimization in terms of a posterior over the optimizer x* and mutual information about x*, which ColaBO leverages to inject user-specified priors directly over the optimizer\u2019s location."
    },
    {
      "title": "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions",
      "authors": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato et al.",
      "year": 2014,
      "arxiv_id": "1406.2541",
      "role": "Extension",
      "relationship_sentence": "Predictive Entropy Search provided a tractable, Monte Carlo-based mutual information objective over x* that ColaBO directly extends by replacing the implicit uniform prior on x* with user-specified beliefs in a Bayesian-principled way."
    },
    {
      "title": "Max-value Entropy Search for Efficient Bayesian Optimization",
      "authors": "Zi Wang and Stefanie Jegelka",
      "year": 2017,
      "arxiv_id": "1703.01968",
      "role": "Extension",
      "relationship_sentence": "Max-value Entropy Search introduced an information-theoretic acquisition over the optimal value f*, which ColaBO generalizes to allow explicit user priors on f* and to propagate those beliefs across Monte Carlo acquisition functions."
    },
    {
      "title": "Practical Bayesian Optimization of Machine Learning Algorithms",
      "authors": "Jasper Snoek et al.",
      "year": 2012,
      "arxiv_id": "1206.2944",
      "role": "Gap Identification",
      "relationship_sentence": "Snoek et al. popularized practical GP-based BO where prior knowledge is primarily encoded via kernels and hyperpriors, a limitation ColaBO addresses by enabling priors directly about the optimizer location and optimal value."
    },
    {
      "title": "Multi-task Bayesian Optimization",
      "authors": "Kevin Swersky et al.",
      "year": 2013,
      "arxiv_id": "1309.4752",
      "role": "Gap Identification",
      "relationship_sentence": "Multi-task BO encodes prior knowledge through task kernels to transfer across related problems, underscoring that prevailing approaches restrict beliefs to kernel structure rather than explicit priors on x* or f*, which ColaBO remedies."
    },
    {
      "title": "The Application of Bayesian Methods for Seeking the Extremum",
      "authors": "Jonas Mockus",
      "year": 1978,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Mockus introduced improvement-based criteria tied to beliefs about target/aspiration values, an early notion that underlies ColaBO\u2019s Bayesian handling of user priors on the optimal value."
    }
  ],
  "synthesis_narrative": "Entropy Search established a Bayesian view of global optimization centered on the posterior over the optimizer x* and maximizing information gain about that quantity, providing a principled handle for reasoning directly about where the minimizer lies. Predictive Entropy Search made this idea practical by introducing a Monte Carlo formulation of mutual information over x*, enabling flexible approximations that can, in principle, accommodate non-uniform beliefs over the optimizer. Complementing these, Max-value Entropy Search reframed the information objective in terms of the distribution of the optimum value f*, showing how targeting information about f* can guide sampling and paving the way for explicit modeling of beliefs over optimal values. In parallel, practical GP-based BO popularized by Snoek et al. primarily encoded prior knowledge through kernel choices and hyperpriors, while Multi-task BO transferred knowledge via task kernels\u2014both exemplifying a prevailing restriction of priors to kernel structure. Earlier still, Mockus\u2019s improvement-based formulations tied decisions to aspirational target values, foreshadowing the utility of explicit beliefs about the optimal value. Taken together, these works revealed two complementary information-theoretic targets\u2014x* and f*\u2014and a practical Monte Carlo pathway for optimizing them, yet left explicit user beliefs about these targets largely unexploited. The current paper synthesizes these threads by providing a general Bayesian framework that injects user-specified priors over x* and f* into modern Monte Carlo acquisition functions, overcoming the kernel-only bottleneck and delivering a principled route to user-guided, belief-aware Bayesian optimization.",
  "target_paper": {
    "title": "A General Framework for User-Guided Bayesian Optimization",
    "authors": "Carl Hvarfner, Frank Hutter, Luigi Nardi",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Bayesian Optimization, Hyperparameter Optimization, Gaussian Processes",
    "abstract": "The optimization of expensive-to-evaluate black-box functions is prevalent in various scientific disciplines. Bayesian optimization is an automatic, general and sample-efficient method to solve these problems with minimal knowledge of the the underlying function dynamics. However, the ability of Bayesian optimization to incorporate prior knowledge or beliefs about the function at hand in order to accelerate the optimization is limited, which reduces its appeal for knowledgeable practitioners with tight  budgets. To allow domain experts to customize the optimization routine, we propose ColaBO, the first Bayesian-principled framework for incorporating prior beliefs beyond the typical kernel structure, such as the likely location of the optimizer or the optimal value. The generality of ColaBO makes it applicable across different Monte Carlo acquisition functions and types of user beliefs. We empirically demonstrate ColaBO's ability to substantially accelerate optimization when the prior i",
    "openreview_id": "NjU0jtXcYn",
    "forum_id": "NjU0jtXcYn"
  },
  "analysis_timestamp": "2026-01-06T15:02:26.038459"
}
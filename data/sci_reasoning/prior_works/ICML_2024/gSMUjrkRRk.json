{
  "prior_works": [
    {
      "title": "Random Features for Large-Scale Kernel Machines",
      "authors": "Rahimi et al.",
      "year": 2007,
      "role": "Baseline",
      "relationship_sentence": "Introduced Monte Carlo random Fourier features via Bochner\u2019s theorem; the current paper directly replaces this MC sampling with quasi-Monte Carlo sequences to obtain sharper 1/M kernel and operator approximation rates."
    },
    {
      "title": "On the Error of Random Fourier Features",
      "authors": "Sutherland et al.",
      "year": 2015,
      "role": "Gap Identification",
      "relationship_sentence": "Provided uniform approximation bounds and highlighted the intrinsic O(1/\u221aM) Monte Carlo rate for RFF; the new work targets this precise limitation and proves O(1/M) (up to logs) by switching to QMC sampling."
    },
    {
      "title": "Generalization Properties of Random Features for Learning",
      "authors": "Rudi et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Developed the operator-based analysis linking random-feature operator approximation to excess risk in kernel ridge regression; the present paper plugs in stronger QMC operator error bounds to show fewer features suffice for the same risk."
    },
    {
      "title": "On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions",
      "authors": "Bach",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Showed that random features can be viewed through the lens of quadrature; this conceptual bridge motivates using QMC (a deterministic/low-discrepancy quadrature tool) to achieve faster kernel approximation rates."
    },
    {
      "title": "Monte Carlo Variance of Scrambled Net Quadrature",
      "authors": "Owen",
      "year": 1997,
      "role": "Foundation",
      "relationship_sentence": "Established variance reduction and improved convergence for randomized quasi-Monte Carlo (scrambled nets) on smooth integrands; the paper leverages these RQMC rate results to derive near-1/M feature approximation guarantees."
    },
    {
      "title": "High-dimensional integration: The quasi-Monte Carlo way",
      "authors": "Dick et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "Surveyed QMC error analysis (digital nets/lattice rules, weighted Sobolev spaces) yielding O(1/M) rates for sufficiently smooth integrands; the current work imports this theory to the Fourier integrands of Gaussian and related kernels."
    },
    {
      "title": "Super-Samples from Kernel Herding",
      "authors": "Chen et al.",
      "year": 2010,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated deterministic sequences with O(1/T) convergence for kernel mean approximation; the new paper offers a simpler, scalable route to similar fast rates by using QMC sequences for feature generation without iterative optimization."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014replacing Monte Carlo random features with quasi-Monte Carlo (QMC) features\u2014stands squarely on the random Fourier features framework of Rahimi and Recht, which formulates kernel evaluation as an expectation amenable to sampling. Sutherland and Schneider made explicit the O(1/\u221aM) kernel and operator approximation limits of standard RFF, crystallizing the gap that this work targets. On the learning side, Rudi and Rosasco\u2019s operator-centric analysis links approximation quality to excess risk in kernel ridge regression; the present paper directly reuses this framework and shows that stronger QMC operator bounds reduce the number of features needed for the same statistical rate. Conceptually, Bach\u2019s equivalence between kernel quadrature and random features suggests that improved quadrature nodes can translate into better feature approximations\u2014precisely the insight operationalized here with QMC. The theoretical backbone comes from QMC analysis: Owen\u2019s results on randomized (scrambled) nets and the broader Dick\u2013Kuo\u2013Sloan theory for digital nets and weighted Sobolev spaces provide the conditions and rates (near 1/M) for smooth integrands, which the authors verify for Gaussian and related kernels\u2019 Fourier integrands. Finally, kernel herding shows deterministically achieving O(1/T) kernel mean errors but at higher computational cost; QMC features deliver comparable fast rates with minimal implementation overhead, completing the direct intellectual lineage to the paper\u2019s main contribution.",
  "analysis_timestamp": "2026-01-06T23:09:26.477358"
}
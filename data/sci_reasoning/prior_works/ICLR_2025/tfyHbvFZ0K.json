{
  "prior_works": [
    {
      "title": "Knowledge Neurons in Pretrained Transformers",
      "authors": "Zhang et al.",
      "year": 2022,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "This work formalized the Knowledge Localization assumption by proposing that specific facts are stored in a small set of \"knowledge neurons,\" which the current paper re-examines and generalizes into the Query Localization assumption."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME)",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "arxiv_id": "2202.05262",
      "role": "Baseline",
      "relationship_sentence": "ROME operationalized localization by pinpointing and editing factual associations in specific MLP layers, a neuron/weight-centric premise that the present paper critiques as too rigid and incomplete without accounting for attention-driven expression."
    },
    {
      "title": "MEMIT: Mass-Editing Memory in a Transformer",
      "authors": "Kevin Meng et al.",
      "year": 2023,
      "arxiv_id": "unknown",
      "role": "Baseline",
      "relationship_sentence": "MEMIT scaled localized fact editing across many facts under the same localization premise, providing a primary baseline whose limitations the new work addresses by shifting from storage localization to query-dependent expression."
    },
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva et al.",
      "year": 2021,
      "arxiv_id": "unknown",
      "role": "Inspiration",
      "relationship_sentence": "By showing MLP layers implement key\u2013value memories retrieved by inputs acting as queries, this paper provided the key mechanism that motivates reframing knowledge expression around query localization rather than neuron localization."
    },
    {
      "title": "In-context Learning and Induction Heads",
      "authors": "Catherine Olsson et al.",
      "year": 2022,
      "arxiv_id": "2209.11895",
      "role": "Related Problem",
      "relationship_sentence": "This study uncovered attention-head circuits where query\u2013key interactions gate downstream computation, directly informing the claim that attention modules crucially control how stored knowledge is expressed."
    },
    {
      "title": "Toy Models of Superposition in Neural Networks",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "arxiv_id": "unknown",
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating that features are often stored in superposition across neurons, this work undermines strict neuron-level localization and motivates the paper\u2019s finding that knowledge storage is more distributed than the KL assumption allows."
    }
  ],
  "synthesis_narrative": "One line of research posited that facts can be pinpointed to a sparse set of internal units: Knowledge Neurons in Pretrained Transformers introduced a procedure and thesis that facts reside in small neuron sets, enabling causal interventions at the neuron level. Locating and Editing Factual Associations in GPT (ROME) made this actionable by identifying mid-layer MLP sites that store subject\u2013relation associations and editing weights to alter specific facts. MEMIT extended this paradigm to large-scale, multi-fact editing, reinforcing the premise that localized parametric sites encode factual content. In contrast, Transformer Feed-Forward Layers Are Key-Value Memories showed that MLPs behave as key\u2013value stores whose retrieval depends on input-derived queries, suggesting expression hinges on routing rather than purely on where facts are stored. Complementarily, In-context Learning and Induction Heads revealed attention circuits where query\u2013key interactions gate downstream computations, highlighting attention\u2019s central role in activating or suppressing stored content. Toy Models of Superposition further argued that features often inhabit superposed, distributed representations, challenging neuron-level exclusivity.\nTogether these works reveal a tension: neuron/weight localization methods succeed but assume rigid storage and overlook attention-mediated routing, while mechanistic studies emphasize query-driven retrieval and distributed storage. The current paper synthesizes these insights by reframing the core assumption from knowledge localization to query localization, presenting evidence that storage is more distributed than previously assumed and that attention (via queries) governs expression, thus explaining both the strengths and the failure modes of neuron- and layer-centric localization/editing approaches.",
  "target_paper": {
    "title": "Knowledge Localization: Mission Not Accomplished? Enter Query Localization!",
    "authors": "Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Knowledge Neruon Thesis, Knowledge Localization, Query Localization",
    "abstract": "Large language models (LLMs) store extensive factual knowledge, but the mechanisms behind how they store and express this knowledge remain unclear.\nThe Knowledge Neuron (KN) thesis is a prominent theory for explaining these mechanisms. This theory is based on the **Knowledge Localization (KL)** assumption, which suggests that a fact can be localized to a few knowledge storage units, namely knowledge neurons.\n However, this assumption has two limitations: first, it may be too rigid  regarding knowledge storage, and second, it neglects the role of the attention module in  knowledge expression. \n \nIn this paper, we first re-examine the KL assumption and demonstrate that its limitations do indeed exist. To address these, we then present two new findings, each targeting one of the limitations: one focusing on knowledge storage and the other on knowledge expression.\nWe summarize these findings as **Query Localization** assumption and argue that the KL assumption can be viewed as a simplifica",
    "openreview_id": "tfyHbvFZ0K",
    "forum_id": "tfyHbvFZ0K"
  },
  "analysis_timestamp": "2026-01-06T18:30:18.946671"
}
{
  "prior_works": [
    {
      "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
      "authors": "Kang et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "SWE-bench\u2019s core idea of evaluating LLM agents on end-to-end, real-world engineering tasks with automatic scoring directly inspired framing ML engineering evaluation around complete, real-world tasks rather than unit-test snippets."
    },
    {
      "title": "DS-1000: A Dataset for Evaluating LLMs on Data Science Code Generation",
      "authors": "Liu et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "DS-1000 exposed that prior data-science evaluations focus on cell-level, library-specific completions, motivating a benchmark that tests full ML pipelines including data preparation, training, and experiment management."
    },
    {
      "title": "An Open Source AutoML Benchmark",
      "authors": "Pieter Gijsbers et al.",
      "year": 2019,
      "arxiv_id": "1907.01502",
      "role": "Foundation",
      "relationship_sentence": "The AutoML Benchmark established the paradigm of multi-dataset, standardized ML tasks and programmatic evaluation, which is adapted here to assess general-purpose agents instead of AutoML systems and to use public leaderboards as human baselines."
    },
    {
      "title": "AgentBench: Evaluating LLMs as Agents",
      "authors": "Liu et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "AgentBench\u2019s methodology for measuring multi-step tool-using agents informed the benchmark\u2019s agent-centric evaluation protocol and success criteria, now instantiated for ML engineering workloads."
    },
    {
      "title": "SWE-agent: Empowering LLMs to Solve Real-World GitHub Issues",
      "authors": "Yang et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "This open-source scaffold serves as a primary baseline agent for executing multi-step coding and experiment workflows in the new benchmark\u2019s ML engineering tasks."
    },
    {
      "title": "OpenDevin: An Open Platform for Autonomous AI Software Engineers",
      "authors": "Mitra et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "OpenDevin provides a general-purpose development agent scaffold used as a comparative baseline to run end-to-end instrumented workflows on the benchmark\u2019s ML tasks."
    },
    {
      "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
      "authors": "Zhou et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "WebArena\u2019s emphasis on realistic, end-to-end tasks in an external environment directly influenced the decision to ground evaluation in real-world Kaggle competitions rather than synthetic or toy settings."
    }
  ],
  "synthesis_narrative": "SWE-bench demonstrated that evaluating language models as agents is most revealing when tasks are end-to-end, realistic, and automatically verifiable, using real GitHub issues as the substrate rather than synthetic unit tests. DS-1000 showed that data-science-oriented code benchmarks typically operate at the granularity of individual cells and library calls, surfacing library-specific errors but not assessing whether an agent can orchestrate a complete ML workflow. The Open Source AutoML Benchmark established a standardized, multi-dataset paradigm for comparing ML systems, with reproducible harnesses and consistent metrics across diverse tasks. AgentBench provided a general methodology for assessing multi-step, tool-using agents, clarifying evaluation protocols and success measures for agentic workloads. SWE-agent operationalized an open-source scaffold that lets LLMs make iterative edits, run tests, and manage repos, while OpenDevin generalized such development-agent workflows in a modular platform. WebArena emphasized fidelity and realism by embedding agents in an external, production-like environment to complete full tasks rather than isolated subtasks.\nTogether these works reveal a clear opportunity: there is no end-to-end, real-world benchmark targeting the ML engineering lifecycle itself\u2014data preparation, training, and experiment management\u2014despite evidence that agent evaluation is most meaningful on realistic tasks. By transplanting SWE-bench\u2019s real-world, end-to-end ethos into the multi-dataset rigor of AutoML benchmarking, adopting AgentBench\u2019s agent-eval protocol, and leveraging practical scaffolds like SWE-agent and OpenDevin, the field naturally progresses to a Kaggle-grounded suite where public leaderboards provide human baselines and realistic difficulty. This synthesis enables principled measurement of agents\u2019 ML engineering capability and supports analyses of resource scaling and potential pretraining contamination.",
  "target_paper": {
    "title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering",
    "authors": "Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Aleksander Madry, Lilian Weng",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "benchmark, evals, evaluations, dataset, tasks, data science, engineering, agents, language agents, scaffold, coding, swe, mle",
    "abstract": "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup \u2014 OpenAI's o1-preview with AIDE scaffolding \u2014 achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource-scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code https://github.com/openai/mle-bench to facilitate future research in understanding the ML engineering capabilities of",
    "openreview_id": "6s5uXNWGIh",
    "forum_id": "6s5uXNWGIh"
  },
  "analysis_timestamp": "2026-01-06T17:11:33.311358"
}
{
  "prior_works": [
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Yang Song et al.",
      "year": 2021,
      "arxiv_id": "2011.13456",
      "role": "Foundation",
      "relationship_sentence": "This work formalizes diffusion generation as reverse SDEs with denoiser-learned scores, providing the precise score-based framework within which the paper analyzes generalization and the structure (shrinkage in an adaptive harmonic basis) of learned denoisers."
    },
    {
      "title": "A Connection Between Score Matching and Denoising Autoencoders",
      "authors": "Pascal Vincent",
      "year": 2011,
      "arxiv_id": "1105.6308",
      "role": "Foundation",
      "relationship_sentence": "It establishes the mathematical link between denoising and score estimation, which the paper directly leverages to interpret the trained denoiser\u2019s vector field as an estimate of the data score when revealing its harmonic shrinkage structure."
    },
    {
      "title": "What Regularized Auto-Encoders Learn from the Data-Generating Distribution",
      "authors": "Guillaume Alain et al.",
      "year": 2014,
      "arxiv_id": "1211.4246",
      "role": "Extension",
      "relationship_sentence": "By showing that the reconstruction vector field of denoising auto-encoders estimates the score and relating it to the Jacobian, this work motivates the paper\u2019s spectral/Jacobian analysis that uncovers a geometry-adaptive harmonic shrinkage mechanism."
    },
    {
      "title": "Ideal Spatial Adaptation by Wavelet Shrinkage",
      "authors": "David L. Donoho et al.",
      "year": 1994,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "This classical result that optimal denoising can be achieved via coefficient shrinkage in a harmonic basis directly inspires the paper\u2019s central finding that deep denoisers implement shrinkage\u2014but in a data-adaptive harmonic basis aligned with image geometry."
    },
    {
      "title": "Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering (BM3D)",
      "authors": "Kostadin Dabov et al.",
      "year": 2007,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "BM3D demonstrates adaptive transform-domain shrinkage using data-dependent bases, providing a concrete classical analogue for the paper\u2019s discovery that modern denoisers perform geometry-adaptive harmonic shrinkage."
    },
    {
      "title": "Extracting Training Data from Diffusion Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2023,
      "arxiv_id": "2301.13188",
      "role": "Gap Identification",
      "relationship_sentence": "By documenting memorization in diffusion models, this work poses the challenge the paper addresses by demonstrating convergence of learned scores across disjoint subsets and explaining generalization via geometry-adaptive harmonic shrinkage."
    },
    {
      "title": "Stochastic Solutions for Linear Inverse Problems Using the Prior Implicit in a Denoiser",
      "authors": "Zahra Kadkhodaie et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Viewing the denoiser as encoding an implicit prior/score for stochastic sampling in inverse problems laid the groundwork for the paper\u2019s deeper analysis of the denoiser\u2019s internal structure as a geometry-adaptive harmonic shrinkage operator."
    }
  ],
  "synthesis_narrative": "Score-based generative modeling cast sampling as solving reverse-time SDEs driven by the score of noise-perturbed data, thereby centering the denoiser as a score estimator whose properties determine synthesis quality. The connection between denoising and score matching made this link explicit, showing that a properly trained denoiser recovers the gradient of the log density, while follow-up analysis of regularized autoencoders tied the reconstruction field and Jacobian spectra to the score, inviting spectral scrutiny of denoisers. Classical signal processing established that effective denoising emerges from shrinkage of coefficients in harmonic representations, with wavelet shrinkage providing a principled template. BM3D extended this idea by performing adaptive transform-domain shrinkage via data-dependent grouping and bases, demonstrating how geometry-aware representations can enhance denoising. Parallel developments framed learned denoisers as implicit priors for stochastic sampling in inverse problems, reinforcing the perspective that the denoiser\u2019s vector field encodes the data distribution. Recently, evidence that diffusion models can memorize training examples crystallized the open question of whether these systems learn a genuine continuous density or merely memorize. Against this backdrop, it became natural to examine the learned score fields and their Jacobians through the lens of transform-domain shrinkage: if denoisers underpin diffusion generation and classical denoising success rests on shrinkage in harmonic bases, then understanding generalization requires identifying whether modern networks likewise implement shrinkage\u2014and whether their bases adapt to image geometry\u2014thus explaining when and why their learned scores align across datasets rather than memorize specifics.",
  "target_paper": {
    "title": "Generalization in diffusion models arises from geometry-adaptive harmonic representations",
    "authors": "Zahra Kadkhodaie, Florentin Guth, Eero P Simoncelli, St\u00e9phane Mallat",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "diffusion models, memorization, generalization, inductive bias, curse of dimensionality, denoising, geometry-adaptive harmonic basis",
    "abstract": "Deep neural networks (DNNs) trained for image denoising are able to generate high-quality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the question of whether these networks are learning the \"true\" continuous density of the data. Here, we show that two DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, when the number of training images is large enough.  In this regime of strong generalization, diffusion-generated images are distinct from the training set, and are of high visual quality, suggesting that the inductive biases of the DNNs are well-aligned with the data density. We analyze the learned denoising functions and show that the inductive biases give rise to a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals ",
    "openreview_id": "ANvmVS2Yr0",
    "forum_id": "ANvmVS2Yr0"
  },
  "analysis_timestamp": "2026-01-06T06:38:19.629678"
}
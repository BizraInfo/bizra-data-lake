{
  "prior_works": [
    {
      "title": "Kalman filtering and smoothing solutions to temporal Gaussian process regression models",
      "authors": "Jouni Hartikainen, Simo S\u00e4rkk\u00e4",
      "year": 2010,
      "role": "Established that many temporal GP priors admit a state-space (Markovian) representation, enabling linear-time Kalman filtering/smoothing for GP regression.",
      "relationship_sentence": "ADM builds directly on this GP\u2013state-space equivalence to make its Gaussian-process prior Markovian, which is essential for scalable inference over long neural recordings."
    },
    {
      "title": "Spatiotemporal learning via infinite-dimensional Bayesian filtering and smoothing",
      "authors": "Simo S\u00e4rkk\u00e4, Arno Solin, Jouni Hartikainen",
      "year": 2013,
      "role": "Generalized GP-to-SDE/state-space constructions to spatiotemporal settings and outlined practical filtering/smoothing pipelines for scalable GP inference.",
      "relationship_sentence": "ADM leverages these state-space GP formulations to handle multi-region neural time series with efficient filtering/smoothing at scale."
    },
    {
      "title": "Computationally Efficient Convolved Multiple Output Gaussian Processes",
      "authors": "Mauricio A. \u00c1lvarez, Lorenzo Rosasco, Neil D. Lawrence",
      "year": 2011,
      "role": "Introduced convolution processes for multi-output GPs, providing a principled way to encode inter-output couplings and time lags via Green\u2019s functions.",
      "relationship_sentence": "ADM extends the convolutional multi-output GP idea by explicitly learning time-varying inter-regional delays and directed couplings within a GP\u2013SSM framework."
    },
    {
      "title": "Dynamic causal modelling",
      "authors": "Karl J. Friston, Lee Harrison, Will D. Penny",
      "year": 2003,
      "role": "Founded effective connectivity analysis in neuroscience using generative state-space models with directed interactions (and neural delays) between regions.",
      "relationship_sentence": "ADM targets the same effective-connectivity goal as DCM but replaces fixed parametric forms with nonparametric GPs and learns time-varying delays while scaling to long recordings."
    },
    {
      "title": "Discovering latent network structure in point process data",
      "authors": "Scott W. Linderman, Ryan P. Adams",
      "year": 2014,
      "role": "Developed Bayesian methods to infer directed, time-lagged interactions via impulse-response (Hawkes-like) kernels from neural spiking data.",
      "relationship_sentence": "ADM adapts the idea of learning directed, temporally offset interactions to continuous multi-region signals, embedding such delays within a GP prior that can vary over time."
    },
    {
      "title": "The MVGC toolbox: a MATLAB toolbox for Granger-causal connectivity analysis",
      "authors": "Lionel Barnett, Anil K. Seth",
      "year": 2014,
      "role": "Standardized Granger-causality methods for directed functional connectivity, typically based on VAR models with fixed or windowed parameters.",
      "relationship_sentence": "ADM can be viewed as a Bayesian, nonparametric successor to GC that jointly infers directionality and smoothly time-varying delays beyond fixed-order VAR assumptions."
    },
    {
      "title": "Prefix Sums and Their Applications",
      "authors": "Guy E. Blelloch",
      "year": 1990,
      "role": "Introduced the parallel scan (prefix-sum) primitive widely used to parallelize sequential recursions.",
      "relationship_sentence": "ADM\u2019s parallel scan inference applies Blelloch-style associative scans to the forward\u2013backward recursions of GP\u2013state-space inference, enabling parallel time-series processing."
    }
  ],
  "synthesis_narrative": "ADM\u2019s core contribution\u2014learning time-varying, directed inter-regional communications with temporal delays at scale\u2014rests on two intertwined threads: state-space Gaussian processes for scalability and principled models of directed, delayed neural interactions. Hartikainen and S\u00e4rkk\u00e4 showed that temporal GP priors can be represented as Markovian state-space models, enabling Kalman filtering and smoothing for O(T) inference. S\u00e4rkk\u00e4, Solin, and Hartikainen extended this to spatiotemporal settings and provided practical recipes for scalable GP inference. These developments directly enable ADM\u2019s Markovian GP backbone that supports long neural recordings.\nConcurrently, neuroscience demands models that capture directionality and delays. Friston\u2019s Dynamic Causal Modeling framed effective connectivity as a generative SSM with directed, possibly delayed interactions, setting a gold standard for causal interpretation. Granger-causality tools (Barnett & Seth) became the workhorse for directed functional connectivity, albeit typically with fixed-order VARs and limited handling of smoothly time-varying delays. Linderman and Adams\u2019 Bayesian point-process work demonstrated learning directed, time-lagged influence kernels from spikes, highlighting the value of explicit temporal offsets.\nTo represent inter-output couplings and lags within GPs, \u00c1lvarez\u2013Rosasco\u2013Lawrence\u2019s convolved multi-output GPs provided a kernel-based mechanism via Green\u2019s functions and convolution. ADM inherits this spirit but makes delays adaptive in time within a GP\u2013SSM. Finally, to scale inference, ADM employs parallel scan over SSM recursions, drawing on Blelloch\u2019s associative prefix-sum primitive to parallelize forward\u2013backward passes. Together, these works converge in ADM\u2019s scalable, nonparametric, and dynamically delayed model of brain-region communications.",
  "analysis_timestamp": "2026-01-07T00:21:33.190174"
}
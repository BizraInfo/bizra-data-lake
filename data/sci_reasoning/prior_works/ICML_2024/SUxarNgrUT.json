{
  "prior_works": [
    {
      "title": "Signal Recovery by Proximal Forward-Backward Splitting",
      "authors": "Patrick L. Combettes et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "Established the forward\u2013backward (proximal gradient) framework that the paper\u2019s adaptive, linesearch-free schemes instantiate and analyze under weakened smoothness."
    },
    {
      "title": "A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems",
      "authors": "Amir Beck et al.",
      "year": 2009,
      "role": "Baseline",
      "relationship_sentence": "Provides the canonical proximal gradient baseline (including backtracking) that the new analysis seeks to surpass by proving convergence of adaptive, linesearch-free variants beyond Lipschitz smoothness."
    },
    {
      "title": "Universal gradient methods for convex optimization problems with H\u00f6lder continuous gradient",
      "authors": "Yurii Nesterov et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "Introduced the universal optimization paradigm under H\u00f6lder-continuous gradients that this paper brings to adaptive proximal gradient methods without resorting to line-search or approximation."
    },
    {
      "title": "First-order methods of smooth convex optimization with inexact oracle",
      "authors": "Olivier Devolder et al.",
      "year": 2014,
      "role": "Gap Identification",
      "relationship_sentence": "Formalized \u03b5-oracles widely used by prior universal methods; the present work explicitly closes this gap by proving universality for proximal gradient without relying on inexact oracles."
    },
    {
      "title": "A descent lemma beyond Lipschitz gradient continuity: first-order methods revisited and applications",
      "authors": "Heinz H. Bauschke et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Provides the H\u00f6lder-type descent inequalities that the paper directly exploits to replace Lipschitz-based analyses and avoid approximation in establishing convergence."
    },
    {
      "title": "Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward\u2013backward splitting, and regularized Gauss\u2013Seidel methods",
      "authors": "Hedy Attouch et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "Supplies the KL/semi-algebraic framework underpinning the paper\u2019s full-sequence convergence results for continuously differentiable semi-algebraic objectives."
    }
  ],
  "synthesis_narrative": "The core innovation of this paper is to show that adaptive proximal gradient schemes\u2014without any line-search or \u03b5-oracle\u2014are universally convergent under mere local H\u00f6lder gradient continuity. This advances the universal optimization paradigm introduced by Nesterov (2015), which established algorithmic adaptivity to unknown H\u00f6lder smoothness but typically through line-search and, in practice, via inexact oracle frameworks. Devolder, Glineur, and Nesterov (2014) codified \u03b5-oracles that became the standard vehicle for such universality; the present work identifies and closes this gap by eliminating approximation altogether. The analytic lever enabling this shift is the H\u00f6lder-type descent inequalities developed by Bauschke, Bolte, and Teboulle (2017), which generalize the classical descent lemma beyond global Lipschitz gradients. By directly applying these H\u00f6lder inequalities, the authors prove decrease and convergence for adaptive, linesearch-free proximal gradient iterates. The algorithmic setting is squarely within the forward\u2013backward (proximal gradient) framework of Combettes and Wajs (2005), and the practical point of comparison remains Beck and Teboulle\u2019s FISTA/backtracking (2009), a widely used adaptive baseline that nonetheless relies on line-search or global Lipschitz models. Finally, the paper\u2019s guarantees of full sequence convergence for continuously differentiable semi-algebraic objectives draw on the KL/semi-algebraic theory of Attouch, Bolte, and Svaiter (2013). Together, these works directly shape the problem formulation, reveal the limitations of oracle/line-search-based universality, and provide the precise H\u00f6lder tools and convergence framework that the paper leverages to obtain universal, approximation-free, linesearch-free adaptive proximal gradient methods.",
  "analysis_timestamp": "2026-01-06T23:09:26.494492"
}
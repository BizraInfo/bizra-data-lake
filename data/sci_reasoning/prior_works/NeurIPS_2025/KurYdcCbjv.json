{
  "prior_works": [
    {
      "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs",
      "authors": "Dmitry Garipov, Pavel Izmailov, Dmitry Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson",
      "year": 2018,
      "role": "Established the linear/mode connectivity phenomenon with low-loss curves between independently trained solutions.",
      "relationship_sentence": "Provides the foundational observation that disparate optima can be connected, which the present work makes linear after symmetry-aware reparameterization in Transformers."
    },
    {
      "title": "Essentially No Barriers in Neural Network Energy Landscape",
      "authors": "Felix Draxler, Kambis Veschgini, Manfred Salmhofer, Fred A. Hamprecht",
      "year": 2018,
      "role": "Independently demonstrated near-zero-barrier paths between minima using constructive interpolation methods.",
      "relationship_sentence": "Motivates the search for systematic transformations that expose low-loss linear paths; the new framework supplies such transformations for Transformers."
    },
    {
      "title": "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks",
      "authors": "Niru Maheswaranathan Entezari, Hanie Sedghi, Benjamin Haeffele, Joshua Susskind",
      "year": 2022,
      "role": "Showed that neuron permutation symmetries can explain linear mode connectivity and proposed alignment via weight matching.",
      "relationship_sentence": "Directly inspires the paper\u2019s permutation class and highlights the limits of pure permutations, motivating the inclusion of richer symmetry classes."
    },
    {
      "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries",
      "authors": "Sam Ainsworth, Jonathan Hayase, Siddhartha S. Srinivasa",
      "year": 2023,
      "role": "Developed practical re-basin algorithms using neuron matching to merge models by undoing permutation symmetries.",
      "relationship_sentence": "Serves as a concrete special case (permutations) that the proposed unified framework subsumes and extends to semi-permutations and linear transforms."
    },
    {
      "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning",
      "authors": "Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",
      "year": 2017,
      "role": "Introduced linear-invariant representation comparison, capturing equivalence up to invertible linear transforms.",
      "relationship_sentence": "Supports the inclusion of orthogonal and general linear maps as valid symmetries for aligning hidden representations in Transformers."
    },
    {
      "title": "Net2Net: Accelerating Learning via Knowledge Transfer",
      "authors": "Tianqi Chen, Ian Goodfellow, Jonathon Shlens",
      "year": 2016,
      "role": "Proposed function-preserving reparameterizations (e.g., widening via permutations and linear maps) between networks.",
      "relationship_sentence": "Provides conceptual grounding for general invertible reparameterizations that leave functions unchanged, a key pillar of the generalized symmetry classes."
    },
    {
      "title": "Merging Models with Fisher-Weighted Averaging",
      "authors": "Roee A. Matena, Colin Raffel",
      "year": 2022,
      "role": "Demonstrated parameter-space averaging/merging for Transformers and highlighted sensitivity to misalignment.",
      "relationship_sentence": "Motivates symmetry-aware reparameterizations in Transformers; the new framework explains when linear connectors exist once richer symmetries are accounted for."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014generalizing linear mode connectivity (LMC) by unifying permutations, semi-permutations, orthogonal transformations, and general invertible maps\u2014emerges from two intertwined lines of prior work. First, Garipov et al. and Draxler et al. established that independently trained networks often lie on connected low-loss manifolds, catalyzing the search for mechanisms that render these paths simple and, ideally, linear. Subsequent work identified hidden symmetries as the culprit obscuring linear paths. Entezari et al. demonstrated that neuron permutations alone can reconcile many apparent discrepancies and recover linear connectors, while Ainsworth et al.\u2019s Git Re-Basin operationalized this idea with practical neuron-matching to merge models. However, both are limited to permutation invariances.\nSecond, representation-similarity and function-preservation studies expanded the symmetry lens. SVCCA showed that representations are comparable up to invertible linear transforms, naturally suggesting orthogonal and more general linear reparameterizations as function-preserving symmetries. Net2Net formalized function-preserving mappings (including permutations and linear maps) across layers, offering a constructive view of reparameterization equivalences.\nIn Transformers specifically, Matena and Raffel revealed that naive weight-space averaging is fragile\u2014often failing without proper alignment\u2014highlighting richer symmetries (e.g., head reindexing, basis rotations within subspaces, and scale/normalization effects). Building on these insights, the present work codifies a unified symmetry framework\u2014permutations through general invertible maps\u2014that subsumes prior alignment methods and exposes linear, low-barrier connectors between independently trained Transformers, thereby generalizing LMC to modern architectures.",
  "analysis_timestamp": "2026-01-06T23:42:48.105636"
}
{
  "prior_works": [
    {
      "title": "Human Motion Diffusion Model (MDM)",
      "authors": "Guy Tevet, Brian Gordon, Yonatan Shafir, Amit H. Bermano, Daniel Cohen\u2011Or",
      "year": 2022,
      "role": "Generative backbone for motion synthesis",
      "relationship_sentence": "MOSPA\u2019s diffusion-based generator builds directly on the MDM paradigm\u2014using denoising diffusion for sequence synthesis and conditional control\u2014adapting it to condition on audio features that encode spatial cues."
    },
    {
      "title": "Learning Individual Styles of Conversational Gesture",
      "authors": "Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, Jitendra Malik",
      "year": 2019,
      "role": "Audio-to-motion mapping (speech \u2192 upper-body gestures)",
      "relationship_sentence": "This work established the core pipeline of mapping audio to human motion and evaluation protocols for co-speech gestures, which MOSPA extends from monaural speech to spatial audio, full-body motion, and spatially grounded responses (e.g., head/body orientation)."
    },
    {
      "title": "AIST++: A Large-scale Dataset for Music-to-Dance Generation",
      "authors": "Rui Li, Shizhe Chen, Xiu Li, Qianqian Wang, et al.",
      "year": 2021,
      "role": "Dataset design and audio\u2013motion alignment",
      "relationship_sentence": "AIST++ informed MOSPA\u2019s dataset design and benchmarking by demonstrating how to pair high-quality motion capture with temporally aligned audio and by popularizing beat- and rhythm-aware evaluation for audio-conditioned motion."
    },
    {
      "title": "EDGE: Editable Dance Generation from Music",
      "authors": "Fangzhou Sun, Yongyi Tang, Zhaoxiang Zhang, et al.",
      "year": 2023,
      "role": "Strong music-conditioned motion generation baseline",
      "relationship_sentence": "EDGE\u2019s conditioning strategies and use of musically salient features (e.g., beat, tempo) motivated MOSPA\u2019s conditioning design; MOSPA generalizes this idea to spatial audio cues so motion reflects sound direction and proximity, not just rhythm."
    },
    {
      "title": "SoundSpaces: Audio-Visual Navigation in 3D Environments",
      "authors": "Changan Chen, Unnat Jain, Carl Schissler, Alex Colburn, et al.",
      "year": 2020,
      "role": "Spatial audio rendering and representation for ML",
      "relationship_sentence": "SoundSpaces introduced learning-ready ambisonics/binaural renderings and room-acoustic realism; MOSPA leverages similar spatial audio representations (e.g., FOA/binaural cues, DOA) to encode directionality that drives body orientation and locomotion."
    },
    {
      "title": "Sound Event Localization and Detection (SELD) with Ambisonics",
      "authors": "Sharath Adavanne, Archontis Politis, Tuomas Virtanen",
      "year": 2018,
      "role": "Feature design for spatial audio (FOA, DOA estimation)",
      "relationship_sentence": "SELD research established practical ambisonics features and DOA estimation targets; MOSPA adapts these cues (ITD/ILD/FOA) as conditioning signals so generated motion is spatially consistent with sound source direction and movement."
    }
  ],
  "synthesis_narrative": "MOSPA\u2019s core contribution\u2014generating human motion driven by spatial audio and introducing a spatial audio\u2013motion dataset\u2014emerges from unifying three research threads: diffusion-based motion synthesis, audio-conditioned motion generation, and machine learning with spatial audio representations. On the generative side, the Human Motion Diffusion Model (MDM) provides the denoising diffusion framework and conditioning mechanics that MOSPA extends to audio-conditioned, sequence-level motion. Prior audio-to-motion works, notably Ginosar et al., showed that raw audio carries useful prosodic structure for gesture synthesis and established evaluation practices; dance generation lines (AIST++, EDGE) demonstrated how to pair high-fidelity motion capture with temporally aligned audio and how to leverage beat/rhythm features to assess synchronization and realism. MOSPA generalizes these insights from monaural/music signals to spatial audio by conditioning on features that encode direction and distance so the body orients, attends, and moves relative to sound sources.\nConcurrently, spatial audio ML (SoundSpaces; SELD with ambisonics) standardized FOA/binaural renderings and DOA estimation, offering learnable representations of interaural cues and room acoustics. MOSPA operationalizes these cues as conditioning features, tying acoustic directionality to kinematic responses (e.g., head yaw, torso turn, stepping toward/away from sources). The SAM dataset design follows the audio\u2013motion alignment and benchmarking practices of music/gesture datasets while capturing spatial metadata, enabling new spatial-consistency metrics. Together, these works directly shaped MOSPA\u2019s dataset construction, spatial-audio feature design, and diffusion-based conditioning to realize spatially faithful human motion generation.",
  "analysis_timestamp": "2026-01-07T00:21:32.314500"
}
{
  "prior_works": [
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh et al.",
      "year": 2017,
      "arxiv_id": "1703.04730",
      "role": "Foundation",
      "relationship_sentence": "Provides the influence-function calculus used to quantify how selecting or reweighting examples perturbs ERM solutions and downstream risk, a core analytical tool in this work."
    },
    {
      "title": "Optimal Subsampling for Large Sample Logistic Regression",
      "authors": "HaiYing Wang et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "Introduces inverse-probability (Horvitz\u2013Thompson) reweighted estimators and score-based sampling for GLMs, whose popular unbiased reweighting rationale is explicitly analyzed and shown to be suboptimal in this paper\u2019s weak-supervision setting."
    },
    {
      "title": "A Modern Maximum Likelihood Theory for High-Dimensional Logistic Regression",
      "authors": "Prasad Sur et al.",
      "year": 2019,
      "arxiv_id": "1811.05904",
      "role": "Foundation",
      "relationship_sentence": "Supplies precise high-dimensional asymptotics for GLM estimators that are adapted here to characterize generalization when training after subset selection guided by a surrogate."
    },
    {
      "title": "Surprises in High-Dimensional Ridgeless Least Squares",
      "authors": "Trevor Hastie et al.",
      "year": 2019,
      "arxiv_id": "1903.08560",
      "role": "Foundation",
      "relationship_sentence": "Provides analytic risk formulas for overparameterized ridge/least-squares models that this paper leverages to compare training on the full dataset versus selected subsets under high-dimensional asymptotics."
    },
    {
      "title": "GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning",
      "authors": "Killamsetty et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "Uses influence-function-based bilevel optimization with a proxy/validation signal to pick training subsets, which this paper formalizes and analyzes statistically under a surrogate-driven weak supervision model."
    },
    {
      "title": "Beyond Neural Scaling Laws: Beating Power Law Scaling via Data Pruning",
      "authors": "Robert Sorscher et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates empirically that carefully selected subsets can outperform full-data training, directly motivating this paper\u2019s theoretical investigation of when and why such data selection works under weak supervision."
    }
  ],
  "synthesis_narrative": "Influence functions provided a tractable way to approximate how upweighting or removing individual examples perturbs ERM solutions and predictions, enabling principled reasoning about data importance at training time. In large-sample GLM settings, optimal subsampling methods proposed selecting points with probabilities tied to score or leverage and correcting with inverse-probability (Horvitz\u2013Thompson) reweighting to achieve unbiased estimating equations; this established a widely used, theoretically justified reweighting paradigm. High-dimensional asymptotics for GLMs characterized by precise limit theorems showed how logistic MLE behaves when the number of parameters scales with sample size, while parallel risk formulas for ridge and ridgeless least squares quantified generalization as a function of sample size, regularization, and feature geometry. On the algorithmic side, generalization-driven subset selection frameworks operationalized influence-function and bilevel ideas to select training data using a proxy or validation signal. Empirically, recent data-pruning studies showed that small, carefully chosen subsets\u2014often guided by early-training or surrogate signals\u2014can match or even surpass performance from using the entire dataset.\nTogether, these works revealed a tantalizing opportunity: theory described ERM behavior in high dimensions and offered IF-based sensitivity tools, while practical subset selection relied on surrogate or proxy signals and unbiased reweighting heuristics. The present paper synthesizes these strands, formalizing a weak-supervision model with a surrogate that is better than random, and using low- and high-dimensional asymptotics plus influence-function analysis to pinpoint when selection can outperform full-data ERM and why popular unbiased reweighting can be harmful\u2014thus turning empirical observations into a predictive statistical theory.",
  "target_paper": {
    "title": "Towards a statistical theory of data selection under weak supervision",
    "authors": "Germain Kolossov, Andrea Montanari, Pulkit Tandon",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Data Selection, Empirical Risk Minimization, Influence Functions, High dimensional asymptotics",
    "abstract": "Given a sample of size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning.  Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $x_{i}$, and to be given access to a  'surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by {$x_{i}$}$_{i\\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization. By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$ Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$ Certain popular choices in data selection methods (e.g. unbiased reweighted",
    "openreview_id": "HhfcNgQn6p",
    "forum_id": "HhfcNgQn6p"
  },
  "analysis_timestamp": "2026-01-06T17:05:46.235298"
}
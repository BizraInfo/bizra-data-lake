{
  "summary": {
    "model": "claude-opus-4",
    "k": 10,
    "crawl_method": "exa_ai",
    "total_papers": 70,
    "hits": 31,
    "hit_rate_percent": 44.29,
    "average_crawl_rate": 100.0,
    "runtime_minutes": 64.59,
    "cost": {
      "input_tokens": 820520,
      "output_tokens": 72272,
      "input_cost_usd": 12.31,
      "output_cost_usd": 5.42,
      "total_cost_usd": 17.73
    },
    "timestamp": "2026-01-05T06:20:53.369387"
  },
  "results": [
    {
      "paper_idx": 0,
      "paper_title": "Generalized Linear Mode Connectivity for Transformers",
      "contribution": "They develop a unified, symmetry\u2011aware reparameterization framework (permutations, semi\u2011permutations, orthogonal transforms, and general invertible maps) that uncovers low\u2011 and zero\u2011barrier linear interpolation paths between independently trained Transformers (including Vision Transformers and GPT\u20112) and across architectures of differing widths.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "input_tokens": 12927,
      "output_tokens": 1080,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Based on the papers provided, here are 10 novel research ideas:",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Adaptive Learning Rate Scheduling for Preserving Linear Mode Connectivity Under Distribution Shift**\n   Develop a learning rate scheduler that dynamically adjusts based on detected data shift magnit",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Weight-Sharing Decomposition for Connectivity-Preserving Convolutional Networks**\n   Design a novel convolutional architecture that decomposes weight-sharing into multiple components, allowing selec",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**Orthogonal Subspace Expansion for Multi-Task Linear Mode Connectivity**\n   Extend Paper 5's orthogonal transformation approach to enable LMC across models trained on different tasks. This would crea",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "**Connectivity-Aware Pruning for Sparse Neural Networks**\n   Develop a pruning algorithm that maintains LMC between the original and pruned models by identifying and preserving critical connections. B",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Data Complexity Metrics for Predicting Linear Mode Connectivity**\n   Create quantitative measures of dataset complexity that predict whether models trained on that data will exhibit LMC. Using insig",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Permutation-Invariant Ensemble Aggregation via Linear Mode Connectivity**\n   Develop an algorithm that finds optimal permutations (as in Paper 4's Git Re-basin) to maximize LMC across ensemble membe",
          "is_match": true
        },
        {
          "idea_idx": 7,
          "idea_text": "**Gradient Noise Injection for Robust Linear Mode Connectivity**\n   Design a training procedure that adds calibrated noise to gradients to ensure models converge to flatter, more connected regions of ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Architecture Search for Maximum Linear Mode Connectivity**\n   Create a neural architecture search method that optimizes for both accuracy and LMC preservation. Leveraging Paper 2's insights about ar",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Backward-Compatible Model Updates via Linear Mode Connectivity Paths**\n   Combine Papers 1 and 5's approaches to enable model updates along linear paths that maintain both backward compatibility and",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 1,
      "paper_title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
      "contribution": "Introduce a compositional phase-domain diffusion framework (with ACT-PAE, SPDM and TPDM) that denoises semantic and transition-aware phase latents so long multi-segment motion sequences are both semantically aligned and smoothly transitioned.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "input_tokens": 13228,
      "output_tokens": 1260,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hierarchical Phase-Space Diffusion for Multi-Character Motion Synthesis\nCombine the periodic phase manifold learning from DeepPhase with diffusion models to generate synchronized multi-character inter",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Physics-Informed Frequency Domain Constraints for Motion Diffusion\nExtend DiffusionPhase by incorporating physics-based constraints directly in the frequency domain during the diffusion process. This ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Compositional Motion Priors with Adaptive Phase Blending\nBuilding on both DeepPhase and the compositional approaches in MDM, develop a method that learns motion primitives as phase-encoded building bl",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Cross-Modal Motion In-Betweening Using Diffusion-Guided Phase Interpolation\nCombine the in-betweening capabilities shown in the last paper with phase-space representations and diffusion models. This w",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Self-Supervised Learning of Motion Phase Manifolds from Unlabeled Video\nExtend DeepPhase's autoencoder approach to learn from raw video data without motion capture markers. The method would use self-s",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Temporal Action Composition with Frequency-Adaptive Diffusion\nMerge TEACH's temporal composition framework with frequency-domain diffusion to handle actions with varying temporal frequencies. This wou",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Interactive Motion Editing via Latent Phase-Space Manipulation\nDevelop an interactive system that allows animators to edit motions by directly manipulating the learned phase manifolds. Users could adj",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Long-Horizon Motion Prediction with Periodic Memory Networks\nCombine phase-space representations with memory-augmented neural networks to predict long-term human motion. The system would maintain sepa",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Style-Preserving Motion Retargeting Through Cross-Character Phase Transfer\nDevelop a method that transfers motion style between characters of different morphologies by learning invariant phase represe",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Multi-Agent Motion Synthesis with Emergent Interaction Patterns\nExtend the dense interaction in-betweening approach to multiple agents by learning emergent coordination patterns in phase space. The me",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 2,
      "paper_title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
      "contribution": "Introduce an exemplar-based global GNN explainer that selects representative nodes in embedding space via a coverage-maximization over reverse k-nearest neighbors and converts their neighborhoods into concise natural-language rules using an LLM self-refinement prompting strategy, yielding scalable, high-fidelity, and human-interpretable class-level explanations.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12507,
      "output_tokens": 934,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Context-Aware Graph Neural Networks with Exemplar Memory Banks\nDevelop a GNN architecture that incorporates exemplar storage mechanisms inspired by context theory, where graph predictions are made by ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Self-Refining Graph Explanations through Iterative Feedback Loops\nCreate a system that combines GNNExplainer with self-refinement techniques, where initial graph explanations are iteratively improved ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hierarchical Concept Learning in Graph Neural Networks\nDesign a framework that automatically discovers hierarchical relationships between learned graph concepts (from local to global), similar to how ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "GraphSAGE with Dynamic Exemplar Sampling\nExtend GraphSAGE to dynamically select and store exemplar subgraphs during training, using context theory principles to improve inductive learning on new graph",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Logic-Guided Graph Neural Network Training\nDevelop a training framework where GNNs are constrained by logical rules extracted from their own predictions using global explainers, creating a feedback lo",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Multi-Modal Graph Explanation Synthesis\nCreate a system that generates explanations combining visual subgraph highlighting, logical formulas, and natural language descriptions, using self-refinement t",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Continual Learning in GNNs with Exemplar Replay\nImplement a continual learning approach for GNNs that stores representative graph exemplars and their explanations, preventing catastrophic forgetting w",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Adversarial Testing of GNN Explanations\nDesign an adversarial framework that generates counterexamples to test the robustness of GNN explanations, using self-refinement to iteratively improve both the",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Context-Sensitive Graph Classification with Prototype Networks\nDevelop a graph classification system that maintains both prototypical graphs and specific exemplars, dynamically weighting their contrib",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Compositional Graph Concept Learning with Symbolic Regression\nCreate a method that learns compositional graph concepts through symbolic regression over computation trees, enabling the discovery of reu",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 3,
      "paper_title": "RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation",
      "contribution": "Introduce RAG4GFM, an end-to-end retrieval-augmented generation framework that adapts the RAG paradigm to graph corpora via hierarchical multi-level graph indexing, task-aware retrieval, and graph-fusion enhancement to enable fast knowledge updating and more faithful reasoning for Graph Foundation Models.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 11165,
      "output_tokens": 1034,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Graph-Structured RAG with Dynamic Retrieval Topology\nDevelop a RAG system where the retrieval mechanism operates over a dynamically learned graph structure rather than traditional vector data",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Inductive RAG: Zero-Shot Knowledge Base Extension via Graph Neural Networks\nCreate a RAG framework that combines inductive representation learning (GraphSAGE) with retrieval mechanisms to handle unsee",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Generate-on-Graph RAG: Treating Retrieved Documents as Dynamic Knowledge Graphs\nExtend the GoG approach to RAG by treating retrieved document chunks as nodes in a temporary knowledge graph. During gen",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Foundation Model for Graph-Augmented Retrieval\nDesign a graph foundation model specifically for retrieval tasks that can adapt to different knowledge domains. The model would pre-train on diverse grap",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Noise-Robust RAG via LLM-Guided Retrieval Denoising\nApply GraphEdit's denoising capabilities to RAG systems by using LLMs to identify and filter out irrelevant or contradictory retrieved passages. The",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Multi-Modal Graph RAG for Heterogeneous Knowledge Sources\nDevelop a RAG system that constructs heterogeneous graphs from multi-modal knowledge sources (text, tables, images). Using graph neural networ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Self-Improving RAG with Graph Structure Learning Feedback\nCreate a RAG system that uses generation quality as feedback to improve its underlying knowledge graph structure. Failed or low-quality genera",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Temporal Graph RAG for Evolving Knowledge Bases\nDesign a RAG framework that maintains temporal knowledge graphs, tracking how facts and relationships change over time. The system would use inductive l",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Federated Graph RAG for Privacy-Preserving Knowledge Sharing\nDevelop a distributed RAG system where multiple organizations maintain private knowledge graphs but can collectively answer queries. Using ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Explainable RAG via Graph Path Reasoning\nBuild a RAG system that provides interpretable retrieval decisions by exposing the graph paths between query concepts and retrieved information. The system wou",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 4,
      "paper_title": "Agnostic Active Learning Is Always Better Than Passive Learning",
      "contribution": "A new agnostic active learning algorithm and analysis that give a sharp, instance-independent first-order query complexity for all concept classes whose leading term is always strictly smaller than passive sample complexity, eliminating disagreement-coefficient-type factors from the leading term.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10442,
      "output_tokens": 1269,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Active Learning with Distribution-Dependent Confidence Intervals\nDevelop an active learning framework that adaptively adjusts confidence intervals based on local data density estimates. This would ext",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Counterfactual-Aware Active Learning for Causal Discovery\nCombine the counterfactual risk minimization approach from observational data with active learning for discovering causal relationships. The a",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Multi-Scale Splitting Index for Hierarchical Active Learning\nExtend the splitting index concept to capture geometric properties at multiple scales simultaneously, enabling active learning algorithms t",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Active Learning with Adaptive Noise Models\nDesign an active learning algorithm that simultaneously learns the noise model and the target hypothesis, moving beyond fixed bounded-rate class noise assump",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Federated Active Learning with Privacy-Preserving Importance Weights\nDevelop a distributed active learning framework where multiple parties collaborate while preserving data privacy, using differentia",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Active Learning for Time-Varying Distributions with Drift Detection\nCreate an active learning algorithm that detects and adapts to distribution drift by monitoring the effectiveness of importance weig",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Minimax-Optimal Active Learning for Worst-Case Subpopulations\nDesign an active learning strategy that optimizes for worst-case performance across different subpopulations, extending the agnostic frame",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Active Learning with Structured Hypothesis Classes Using Graph-Based Priors\nIncorporate graph-based structural priors into the splitting index framework for hypothesis classes with known relationships",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Meta-Active Learning Across Related Tasks\nDevelop a meta-learning framework that learns optimal importance weighting strategies across multiple related active learning tasks. The approach would transf",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Active Learning with Rejected Queries and Strategic Non-Response\nExtend the active learning framework to handle scenarios where label queries can be rejected or strategically not answered, using a mod",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 5,
      "paper_title": "Learning Linear Attention in Polynomial Time",
      "contribution": "Shows that multi\u2011head linear attention can be learned in polynomial time by recasting the model as learning a rank\u2011H kernel predictor in an RKHS, and provides an algorithm that both finds near\u2011optimal MHLA parameters and certifies when all empirical best\u2011fits implement the same computation.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 14543,
      "output_tokens": 1052,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Kernel-Enhanced Linear Transformers with Learnable Feature Maps\nDevelop transformers where the linear attention mechanism uses learnable kernel feature maps instead of fixed random features. This woul",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hierarchical Sparse-to-Dense Attention Networks\nDesign a multi-scale attention mechanism that progressively refines from sparse to dense attention patterns across layers. Early layers use graph-based ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Temporal Logic-Guided Transformer Architectures\nLeverage the connection between transformers and temporal logic to design architectures with explicit temporal reasoning modules. These would incorporat",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Low-Rank Kernel Approximation for Efficient Transformer Training\nApply matrix factorization techniques from the Eckart-Young theorem to approximate attention matrices during training. This would dynam",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "PAC-Learning Framework for Transformer Generalization Bounds\nDevelop tighter generalization bounds for transformer models using agnostic PAC learning theory. This would provide theoretical guarantees ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Fixed-Precision Transformer Variants with Enhanced Expressivity\nDesign architectural modifications that increase the expressivity of fixed-precision transformers beyond LTL[P] while maintaining comput",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Adaptive Precision Transformers with Dynamic Quantization\nCreate transformers that dynamically adjust numerical precision based on input complexity and task requirements. The model would learn when hi",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Graph-Kernel Hybrid Attention Networks\nCombine graph attention mechanisms with kernel-based linear attention in a unified framework. Different heads would use different attention types, with a learned",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Orthogonal Random Features for Multi-Modal Transformers\nExtend the FAVOR+ algorithm to handle multi-modal inputs by designing specialized orthogonal random features for each modality. This would enabl",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Automata-Inspired Transformer Architectures for Formal Language Tasks\nDesign transformer variants that explicitly incorporate automata-theoretic structures based on the connection to PODFAs. These mod",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 6,
      "paper_title": "Optimal Mistake Bounds for Transductive Online Learning",
      "contribution": "Shows that the transductive online mistake bound is \u0398(\u221ad) (giving an \u2126(\u221ad) lower bound and a matching O(\u221ad) upper bound), thereby establishing a quadratic separation from the standard online bound of \u0398(d).",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8464,
      "output_tokens": 799,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Differentiable Weighted Majority Algorithm for Time-Frequency Analysis\nIntegrate Littlestone's weighted majority algorithm with Leiber's differentiable STFT framework to create an adaptive ensemble of",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "VCL-Dimension Guided Compression for Time-Series Monitoring\nApply the VCL dimension concept from fine-grained learning curves to compress time-series data from mechanical systems. This would identify ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Distribution-Dependent Learning Curves for Fault Detection Under Variable Conditions\nDevelop a theoretical framework that characterizes how fault detection accuracy improves with sample size under dif",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Online Learning Framework for Adaptive STFT Parameter Selection\nCreate an online learning algorithm that continuously updates STFT parameters based on incoming signal characteristics. The algorithm wo",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Apple Tasting Algorithm for Selective Signal Acquisition in Resource-Constrained Monitoring\nAdapt Littlestone's apple tasting framework to mechanical system monitoring where sensor activation is costl",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Hybrid Neural-STFT Architecture with Learnable Time-Frequency Kernels\nDesign a neural network architecture that learns custom time-frequency kernels beyond traditional STFT windows. The network would ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Strong Minimax Bounds for Learning Mechanical Fault Patterns\nEstablish theoretical guarantees for the sample complexity of learning different types of mechanical faults. This would determine which fau",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Multi-Scale Compression-Based Anomaly Detection with Theoretical Guarantees\nDevelop an anomaly detection method that uses compression algorithms at multiple time scales, with PAC-style guarantees on d",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Adversarial Robustness of Time-Frequency Representations Under Distribution Shift\nStudy how different time-frequency representations maintain their effectiveness when external conditions change advers",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Meta-Learning Framework for Cross-Domain Transfer in Mechanical Monitoring\nCreate a meta-learning approach that learns how to adapt monitoring algorithms across different types of mechanical systems. ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 7,
      "paper_title": "State Entropy Regularization for Robust Reinforcement Learning",
      "contribution": "Shows that regularizing the entropy of the state-visitation distribution yields provable robustness to structured and spatially correlated perturbations (under reward and transition uncertainty), contrasts these guarantees with policy-entropy regularization, and analyzes practical sensitivities such as number of rollouts.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "input_tokens": 12248,
      "output_tokens": 1144,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Entropy Regularization for Non-Stationary Robust MDPs\nDevelop a framework that dynamically adjusts entropy regularization coefficients based on observed environmental shifts. This would bridg",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hierarchical Robust-Entropy Policies with State-Dependent Uncertainty Sets\nExtend R\u00b2MDPs to hierarchical settings where uncertainty sets vary across different levels of abstraction. This would enable ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Best-Effort Maximum Entropy Exploration in Partially Observable Environments\nMerge the best-effort policy selection from RMDPs with maximum entropy exploration for POMDPs. This would yield policies th",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Provably Efficient Robust Exploration with Conditional Entropy Objectives\nDevelop algorithms that maximize conditional state entropy given observed trajectories while maintaining robustness guarantees",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Multi-Agent Robust MDPs with Entropy-Based Coordination\nCreate a framework for multi-agent systems where agents coordinate through entropy-regularized policies while maintaining individual robustness ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Robust Value Iteration with Learnable Regularization Functions\nReplace fixed entropy regularization in R\u00b2MDPs with learnable regularization functions that adapt based on the observed uncertainty struc",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Cascading Robustness: From Reward to Transition Uncertainty\nDevelop a unified framework that handles both reward and transition uncertainty through cascading regularization terms. Building on the R\u00b2MD",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Meta-Learning for Optimal Robust Best-Effort Policy Selection\nUse meta-learning to quickly identify ORBE policies across related tasks by learning priors over uncertainty sets. This would enable rapid",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Entropy-Guided Uncertainty Set Refinement for Data-Efficient Robust RL\nDevelop methods that use state visitation entropy to guide the refinement of uncertainty sets in RMDPs. By focusing uncertainty r",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 8,
      "paper_title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity",
      "contribution": "Shows that the stochasticity of conditional targets is not the primary driver of generalization in flow matching: closed-form velocity targets match (and sometimes improve) performance, and generalization instead arises from the neural network's failure to perfectly approximate the optimal closed-form velocity field in particular time intervals.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10936,
      "output_tokens": 831,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Flow Matching with Memorization-Aware Regularization\nDevelop a flow matching framework that dynamically adjusts the probability paths based on detected memorization patterns during training. ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Noise-Robust Score Matching for Limited Data Regimes\nDesign a generalized cross-entropy-inspired loss function specifically for score-based generative models that is robust to noisy or mislabeled data",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hybrid Deterministic-Stochastic Flow Paths via Adaptive ODE-SDE Switching\nCreate a unified framework that automatically switches between probability flow ODEs and SDEs based on local data density esti",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Multi-Modal Flow Matching with Cross-Domain Alignment\nExtend flow matching to handle multiple modalities simultaneously by learning shared latent probability paths that align different data types. Thi",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Effective Model Memorization (EMM) Guided Architecture Search\nDevelop an automated neural architecture search method that uses EMM as a key metric to find model architectures that balance expressivene",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Privacy-Preserving Flow Matching with Differential Privacy Guarantees\nDesign a flow matching training procedure that incorporates differential privacy mechanisms directly into the vector field regress",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Conditional Flow Matching with Disentangled Memorization Control\nCreate a conditional generative framework where memorization can be explicitly controlled through conditioning variables, allowing user",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Optimal Transport Flow Matching with Adaptive Time Discretization\nDevelop an adaptive discretization scheme for OT-based flow matching that automatically adjusts time steps based on the local complexi",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Robustness Certification for Score-Based Models via Lipschitz Constraints\nEstablish a framework for certifying the robustness of score-based generative models against adversarial perturbations by enfo",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Meta-Learning for Rapid Flow Adaptation in Few-Shot Generation\nDesign a meta-learning approach for flow matching that can quickly adapt to new data distributions with minimal samples without memorizat",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 9,
      "paper_title": "Why Diffusion Models Don\u2019t Memorize:  The Role of Implicit Dynamical Regularization in Training",
      "contribution": "The paper shows that training dynamics impose an implicit dynamical regularization in diffusion models: there are two distinct timescales (\u03c4gen and \u03c4mem) so that models generalize for a wide, growing window of training times (\u03c4 \u2208 [\u03c4gen, \u03c4mem]) because \u03c4mem scales linearly with dataset size n while \u03c4gen remains constant, explaining why memorization is avoided in practice and giving a tractable random-features theory that matches experiments.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13395,
      "output_tokens": 1215,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Thermodynamic-Inspired Score Matching for Improved Diffusion Model Training\nCombine the nonequilibrium thermodynamics framework from diffusion models with score matching techniques to develop a traini",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Random Feature Diffusion: Accelerating Diffusion Models via Kernel Approximation\nApply random feature mappings to approximate the denoising network in diffusion models, reducing computational complexi",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Spectral Analysis of Diffusion Model Memorization via Activation Functions\nInvestigate how different nonlinear activation functions in diffusion model architectures affect memorization and privacy lea",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Phase Transition Detection for Adaptive Diffusion Sampling\nLeverage the identified dynamical regimes (speciation, collapse, and generation phases) to develop adaptive sampling algorithms that adjust t",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Score-Based Random Feature Networks for Non-Normalized Density Estimation\nCombine score matching with random feature approximations to create scalable methods for learning non-normalized densities in ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Differential Privacy via Spectral Regularization in Diffusion Models\nDesign new privacy-preserving training methods for diffusion models by incorporating spectral regularization that penalizes memoriz",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Multi-Scale Random Grid Diffusion for Hierarchical Generation\nExtend diffusion models with randomly shifted multi-resolution grids (inspired by random features) to capture hierarchical structure in da",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Thermodynamic Bounds on Diffusion Model Convergence\nDerive theoretical convergence guarantees for diffusion models using tools from nonequilibrium statistical physics, establishing relationships betwe",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Kernel-Guided Collapse Time Prediction for Efficient Diffusion Sampling\nDevelop methods to predict the collapse time in diffusion sampling using kernel-based analysis of the data correlation structure",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Fourier-Accelerated Score Matching for High-Dimensional Diffusion Models\nLeverage the connection between score functions and Fourier transforms to develop frequency-domain training methods for diffusi",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 10,
      "paper_title": "Adjoint Schr\u00f6dinger Bridge Sampler",
      "contribution": "Combines Schr\u00f6dinger-bridge stochastic optimal control with adjoint matching to learn scalable, importance-weight-free diffusion samplers that transport arbitrary source distributions to unnormalized energy-defined targets.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 13448,
      "output_tokens": 1142,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Schr\u00f6dinger Bridge Diffusion Models for Conditional Generation with Distributional Constraints\nDevelop a framework that combines Schr\u00f6dinger bridge formulations with diffusion models to enable conditi",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Adjoint-Guided Score Matching for Energy-Based Diffusion Models\nIntegrate the adjoint sampling methodology with score-based SDEs to create a unified framework that can learn from both score functions ",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Path Integral Control for Adaptive Diffusion Sampling\nDevelop a path integral formulation for controlling the sampling trajectory in diffusion models, allowing dynamic adjustment of the denoising proc",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Multi-Scale Hierarchical Diffusion Models via Iterative Proportional Fitting\nCreate a hierarchical extension of diffusion models using IPF principles to handle multi-scale data generation. The model w",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Symmetry-Preserving Schr\u00f6dinger Bridges for Molecular Dynamics\nExtend the adjoint sampling framework to incorporate physical symmetries and conservation laws directly into the Schr\u00f6dinger bridge formu",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Optimal Transport-Regularized Score Matching for Few-Shot Generation\nCombine optimal transport theory with score-based models to enable few-shot learning of diffusion processes. By using Schr\u00f6dinger b",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Quantum-Inspired Path Integral Sampling for Discrete Diffusion Models\nDevelop discrete versions of diffusion models using path integral formulations from quantum mechanics, enabling efficient sampling",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Adversarial Schr\u00f6dinger Bridges for Robust Generation\nCreate an adversarial training framework for Schr\u00f6dinger bridge samplers where a discriminator learns to distinguish between true bridge processes",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Memory-Efficient Adjoint Diffusion via Checkpointing and Reversible Dynamics\nDevelop memory-efficient implementations of adjoint-based diffusion training using reversible neural ODEs and gradient chec",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Stochastic Control for Conditional Diffusion with Uncertainty Quantification\nExtend the SDE framework for diffusion models to include uncertainty quantification in the conditional generation process. ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 11,
      "paper_title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies",
      "contribution": "Demonstrates that adding an explicit, compute-aware inference phase (using search/optimization strategies such as tree search, sampling and adaptation) on top of trained RL policies substantially breaks zero-shot performance ceilings in complex multi-agent and combinatorial tasks, yielding large empirical gains with modest extra wall-clock time.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 9280,
      "output_tokens": 1176,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Tree Search with Cross-Entropy Policy Networks for Real-Time Strategy Games\nCombine UCT's tree search efficiency with cross-entropy method's adaptive sampling to create a hybrid algorithm for",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Neural Architecture Search for Problem-Specific Combinatorial Solvers Using Reinforcement Learning\nApply reinforcement learning to automatically design attention-based architectures tailored to specif",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Distributed Cross-Entropy Learning for Large-Scale Industrial Planning Problems\nParallelize the cross-entropy method across multiple agents, each learning policies for sub-problems while coordinating ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Adversarially Robust Neural Policies for Combinatorial Optimization Under Uncertainty\nTrain attention-based models that maintain performance under worst-case perturbations of problem parameters. Combi",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Continuous-Discrete Hybrid Optimization via Differentiable Tree Search\nDevelop a differentiable approximation of MCTS that can optimize both discrete combinatorial decisions and continuous parameters ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Meta-Learning Framework for Rapid Adaptation to New Combinatorial Problem Variants\nCreate a meta-learning approach that quickly adapts pre-trained attention-based models to new problem variants with f",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Quantum-Inspired Tensor Network Representations for Combinatorial Optimization Policies\nDevelop compact policy representations using tensor networks to capture complex dependencies in large-scale comb",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 12,
      "paper_title": "High-Dimensional Calibration from Swap Regret",
      "contribution": "Shows that multi-dimensional online calibration over any convex P and norm ||\u00b7|| reduces to a swap-regret control implied by optimal regularizers for online linear optimization, and uses TreeSwap+FTL to obtain efficient high-dimensional calibration rates (T = exp(O(\u03c1/\u03b5^2))) recovering and generalizing prior polynomial-in-d bounds without requiring OLO subroutines or knowledge of \u03c1.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 14036,
      "output_tokens": 1195,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Calibration for Non-Stationary Environments with Change Detection\nDevelop an online calibration algorithm that automatically detects distribution shifts and adapts its calibration strategy ac",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Calibrated Predictions with Privacy Guarantees via Differential Privacy\nDesign a differentially private calibration algorithm that achieves sublinear calibration error while protecting individual data",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Multi-Agent Calibration Games with Strategic Forecasters\nStudy scenarios where multiple forecasters compete or collaborate to provide calibrated predictions, analyzing the equilibrium properties when ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Continuous-Time Calibration with Irregular Observations\nExtend calibration theory to continuous-time settings where observations arrive at irregular intervals, developing algorithms that maintain cali",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Calibration-Aware Feature Selection in High-Dimensional Forecasting\nDevelop methods that jointly optimize for prediction accuracy and calibration by selecting features that contribute to well-calibrat",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Robust Calibration Against Adversarial Data Poisoning\nDesign calibration algorithms that maintain performance guarantees even when a fraction of the observed outcomes are adversarially corrupted. This",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Calibration Transfer Across Related Forecasting Domains\nCreate methods for transferring calibration knowledge between related but distinct forecasting tasks, reducing the sample complexity required to",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Implicit Calibration through Self-Supervised Learning\nDevelop neural network architectures that inherently produce calibrated predictions without explicit calibration post-processing, using self-super",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Calibration with Structured Feedback and Partial Observations\nExtend calibration theory to settings where feedback is structured (e.g., pairwise comparisons, rankings) rather than direct observations,",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Quantum Calibration Algorithms with Speedup Guarantees\nDesign quantum algorithms for online calibration that achieve quadratic speedups over classical methods in terms of sample complexity or computat",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 13,
      "paper_title": "In Search of Adam\u2019s Secret Sauce",
      "contribution": "Through a large empirical study and a focused theoretical simplification (\u03b21 = \u03b22), the paper shows that Adam\u2019s empirical advantage over signed/momentum methods largely stems from its coupled mean/variance estimation \u2014 giving a near\u2011optimal, interpretable optimizer that can be seen as an online mean/variance estimator arising from a mean\u2011field Gaussian variational inference view.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13238,
      "output_tokens": 1028,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Compute-Optimal Adaptive Learning Rate Scheduling\nExtend the Chinchilla scaling laws to determine optimal learning rate schedules as a function of model size and training tokens. Develop a principled ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Bayesian Interpretation of Decoupled Weight Decay\nBuilding on SGD's connection to Bayesian inference, derive the posterior distribution implied by AdamW's decoupled weight decay. Develop a variational",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Sparse Adaptive Optimization with Dynamic Feature Importance\nExtend AdaGrad's per-feature learning rates to dynamically identify and prioritize rare but informative features during training. Implement",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Federated Learning with Adaptive Compressed Gradients\nCombine signSGD's compression with adaptive methods for federated settings where clients have heterogeneous data. Develop a protocol where clients",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Multi-Scale Optimization for Large Language Models\nDesign an optimizer that applies different optimization strategies to different components of transformers (attention, FFN, embeddings) based on thei",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Continuous-Time Adaptive Stochastic Processes\nExtend the SDE interpretation of constant-step SGD to adaptive methods like Adam/AdamW. Derive the continuous-time limit of these optimizers and develop n",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Gradient Sign Alignment for Distributed Training\nDevelop a distributed optimization algorithm that uses gradient sign alignment as a coordination mechanism. When signs agree across workers, apply larg",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Token-Aware Adaptive Optimization\nCreate an optimizer that adjusts learning rates based on the frequency and importance of tokens in language model training. Integrate insights from compute-optimal tr",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Proximal Adaptive Methods with Optimal Regret Bounds\nDevelop a unified framework that generalizes AdaGrad's proximal updates to handle both convex and non-convex objectives with provable regret bounds",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 14,
      "paper_title": "An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds",
      "contribution": "They refine the Franz\u2013Parisi (FP) geometric criterion to better capture overlap structure and prove that this optimized FP is equivalent to Statistical Query (SQ) lower bounds under a mild, verifiable assumption, thereby unifying physics-inspired geometry with SQ complexity for a broad class of statistical models.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 5,
      "input_tokens": 13674,
      "output_tokens": 1018,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Low-Degree Polynomial Methods for Online Statistical Inference\nDevelop algorithms that dynamically adjust the degree of polynomial approximations based on observed data complexity. This would",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Franz-Parisi Criterion for Quantum Statistical Models\nExtend the Franz-Parisi free energy landscape approach to quantum statistical models, establishing computational hardness for quantum machine lear",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Statistical Query Lower Bounds for Robust Estimation Under Contamination\nProve tight SQ lower bounds for robust mean and covariance estimation in high dimensions when data is adversarially corrupted. ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Metastable States in Neural Network Training Landscapes\nApply the Franz-Parisi potential function methodology to characterize metastable states in deep neural network loss landscapes. This could provi",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Low-Degree Hardness of Tensor Completion with Side Information\nEstablish computational lower bounds for tensor completion when auxiliary structural information is available. This would extend tensor P",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Unified Framework Connecting MCMC Mixing Times to Low-Degree Hardness\nDevelop quantitative relationships between the mixing time of natural MCMC algorithms and the low-degree likelihood ratio. This wo",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Statistical-Computational Gaps in Federated Learning\nCharacterize fundamental tradeoffs between communication complexity, privacy constraints, and statistical accuracy in federated settings using the ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Correlation Inequalities for Non-Gaussian Graphical Models\nExtend the Gaussian correlation conjecture proof techniques to establish correlation inequalities for discrete graphical models and exponenti",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Average-Case Hardness of Sparse PCA with Structured Sparsity\nProve refined computational lower bounds when the sparse principal component has additional structure (e.g., group sparsity, tree structure",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Computational Phase Transitions in Planted Subgraph Detection Beyond Cliques\nEstablish precise computational thresholds for detecting planted subgraphs with specific topologies (e.g., cycles, trees, r",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 15,
      "paper_title": "MaxSup: Overcoming Representation Collapse in Label Smoothing",
      "contribution": "A theoretical decomposition of label smoothing that exposes an error-amplification term, and a simple logit-level regularizer (Max Suppression) that penalizes the top-1 logit to retain LS\u2019s benefits while avoiding overconfident misclassifications and representation collapse.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12224,
      "output_tokens": 781,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Loss Function Selection via Feature Transferability Metrics\nDevelop a meta-learning framework that dynamically selects loss functions during training based on real-time assessment of feature ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Factorized Convolutions with Confidence-Aware Regularization\nCombine the efficiency gains from factorized convolutions (Inception) with confidence penalty mechanisms to create networks that are both c",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Neural Collapse-Guided Architecture Search for Transfer Learning\nDesign an architecture search method that explicitly optimizes for balanced neural collapse properties - tight within-class clustering ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Multi-Scale Label Smoothing with Depth-Adaptive Coefficients\nDevelop a label smoothing variant where smoothing parameters vary across network depth based on layer-wise representation analysis. Earlier",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Failure Prediction Networks with Dedicated Uncertainty Branches\nCreate architectures with parallel branches specifically optimized for failure prediction alongside the main classification path. These ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Cross-Domain Confidence Calibration via Representation Alignment\nDevelop methods that preserve confidence calibration when transferring models across domains by aligning penultimate layer representati",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Inception-Style Modules for Uncertainty Quantification\nDesign new building blocks inspired by Inception's multi-scale processing but specifically tailored for uncertainty estimation. These modules wou",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Loss Function Interpolation Schedules for Progressive Feature Learning\nCreate training schedules that smoothly interpolate between different loss functions (CE, focal loss, MSE) during training to lev",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Calibration-Preserving Knowledge Distillation with Logit Correction\nDevelop knowledge distillation methods that explicitly compensate for the information loss caused by label smoothing in teacher netw",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Efficient Feature Factorization for High-Confidence Region Detection\nExtend factorized convolution techniques to specifically identify high-confidence regions in images while maintaining calibration. ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 16,
      "paper_title": "Memory Mosaics at scale",
      "contribution": "Scaled and redesigned networks of associative key\u2013value memories (Memory Mosaics v2) that match transformers on training\u2011knowledge storage while substantially improving new\u2011task and in\u2011context learning at large scale.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 12985,
      "output_tokens": 922,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hierarchical Memory Mosaics with Adaptive Kernel Selection\nDevelop a multi-level memory mosaic architecture where different layers use different kernels (Gaussian, exponential, polynomial) that are dy",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Memory-Augmented Transformers with Explicit Disentanglement Regularization\nCombine the attention mechanism of transformers with memory mosaic-style associative memories, adding a regularization term t",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Continuous Learning Memory Networks with Selective Forgetting\nDesign a memory network that can selectively forget outdated key-value pairs based on their relevance scores and temporal decay, enabling ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Meta-Learning Framework for Memory Mosaic Initialization\nDevelop a meta-learning approach that learns optimal initialization strategies for memory mosaic components across different task domains. This",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Sparse Memory Mosaics with Learned Connectivity Patterns\nCreate a memory mosaic variant where connections between memory units are sparse and learned during training, reducing computational complexity",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Memory Networks with Compositional Key-Value Decomposition\nDesign a system where keys and values are automatically decomposed into compositional primitives during training, similar to how memory mosai",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Bidirectional Memory Mosaics for Sequence Modeling\nExtend memory mosaics to process sequences bidirectionally, where future-peeking values are combined with past-context keys through a learnable tempo",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Memory-Based Architecture Search for Task-Specific Optimization\nDevelop an automated method to discover optimal memory network architectures for specific tasks by searching over memory unit types, con",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Federated Learning with Distributed Memory Mosaics\nDesign a federated learning framework where different clients maintain local memory mosaics that can be selectively shared and aggregated. This would",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Memory Networks with Uncertainty-Aware Retrieval\nIncorporate uncertainty estimation into the memory retrieval process, where the model outputs both retrieved values and confidence scores based on key ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 17,
      "paper_title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
      "contribution": "Shows that sparse-attention circuits emerge as predictable phase-transitions in training dynamics driven by task structure, optimizer/architecture choices, and data distribution\u2014and that repeating examples can dramatically accelerate this emergence.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 14356,
      "output_tokens": 969,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hierarchical Induction Heads for Multi-Level Pattern Completion\nExtend the induction head mechanism to operate across multiple hierarchical levels of abstraction. This would enable transformers to com",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Controllable Phase Transitions in Transformer Training via Data Distribution Engineering\nDevelop methods to deliberately engineer training data distributions that trigger specific phase transitions at",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Hybrid Attention-Recurrent Architectures for Naturalistic Data Learning\nDesign architectures that combine transformer attention with recurrent components specifically optimized for naturalistic data d",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Theoretical Framework for Attention Depth-Performance Trade-offs\nExtend the deep linear network analysis to attention-based networks, developing exact solutions for how learning dynamics change with d",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Adaptive Repetition Scheduling for Efficient Transformer Training\nCreate dynamic algorithms that adjust the repetition frequency of training examples based on model performance metrics. This builds on",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Fuzzy Induction Heads with Learnable Similarity Metrics\nDevelop induction head variants that learn task-specific similarity metrics for pattern matching. Rather than relying on fixed token similarity,",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Causal Intervention Framework for Mechanistic Interpretability\nCreate a systematic methodology for performing causal interventions on transformer components during training. This would help establish ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Multi-Modal Induction Mechanisms for Cross-Domain Learning\nExtend induction heads to operate across different modalities (text, vision, audio) by developing cross-modal similarity measures. This could",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Theoretical Analysis of Two-Set Training Dynamics\nProvide rigorous mathematical analysis of why two-set training (small repeated subset + normal sampling) accelerates learning. This would extend the t",
          "is_match": true
        },
        {
          "idea_idx": 9,
          "idea_text": "Emergent Specialization in Multi-Head Attention Through Competitive Learning\nDesign training objectives that encourage different attention heads to specialize in different types of patterns or tasks t",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 18,
      "paper_title": "ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts",
      "contribution": "Introduce a prompt-modulated restoration-and-fusion network trained on physically simulated composite degradations that uses language-vision degradation prompts plus a spatial-frequency visual adapter to produce controllable, degradation-robust infrared-visible image fusion.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 14288,
      "output_tokens": 1257,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Cross-Modal Prompt Learning for Zero-Shot Image Restoration\nDevelop a framework that uses text prompts to guide image restoration tasks without task-specific training, leveraging CLIP-like models to u",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Diffusion-Based Multi-Modal Fusion with Learned Degradation Priors\nCombine the diffusion-based fusion approach from DDFM with the degradation-aware capabilities of Text-IF and PromptIR. This would cre",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Interactive Text-Guided Fusion Weight Learning\nExtend Text-IF's semantic guidance to dynamically learn fusion weights based on textual descriptions of desired outcomes. Users could specify complex fus",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Self-Supervised Vision-Language Pre-training for Degraded Images\nAdapt CLIP's contrastive learning approach specifically for degraded image-text pairs, creating robust representations that understand ",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Prompt-Based Degradation Composition for Robust Fusion\nDevelop a method that uses compositional prompts to handle multiple simultaneous degradations in fusion scenarios. The system would decompose com",
          "is_match": true
        },
        {
          "idea_idx": 5,
          "idea_text": "Language-Guided Selective Feature Fusion\nCreate an attention mechanism guided by natural language to selectively fuse features from different modalities. Text descriptions would control which features",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Zero-Shot Cross-Modal Translation via Diffusion Bridges\nLeverage diffusion models to create \"bridges\" between different imaging modalities without paired training data. Text descriptions would guide t",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Unified Text-Prompt Framework for Multi-Task Image Enhancement\nDesign a single model that handles restoration, fusion, and enhancement tasks through different text prompts. The model would learn a sha",
          "is_match": true
        },
        {
          "idea_idx": 8,
          "idea_text": "Semantic-Aware Degradation Modeling in Latent Space\nCombine CLIP's semantic understanding with diffusion models to learn degradation patterns in a semantically meaningful latent space. This would enab",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Progressive Multi-Modal Fusion with Natural Language Feedback\nDevelop an iterative fusion framework where users provide natural language feedback to refine fusion results progressively. The system wou",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 19,
      "paper_title": "Identifiability of Deep Polynomial Neural Networks",
      "contribution": "Provides a comprehensive, constructive characterization of when deep polynomial neural networks are (finitely and/or globally) identifiable by reducing identifiability to low-rank polynomial/tensor decomposition uniqueness and settling open dimension and degree-threshold conjectures for neurovarieties.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 15312,
      "output_tokens": 1445,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Tensor-Polynomial Hybrid Activation Functions for Deep Networks\nDevelop neural networks that use tensor decomposition-based activation functions instead of scalar polynomials, where each activation is",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Identifiability-Preserving Compression of Polynomial Neural Networks\nCreate algorithms that compress polynomial neural networks by reducing layer widths while provably preserving the identifiability p",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Algebraic Optimization Landscapes for Mixed-Degree Polynomial Networks\nExtend the algebraic variety analysis to networks where different layers use different polynomial degrees, characterizing how mix",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Tensor Network Architectures with Learnable Decomposition Structures\nDesign neural architectures where the connectivity pattern between layers is determined by learnable tensor decomposition structure",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Polynomial Neural ODEs with Algebraic Invariants\nDevelop continuous-depth polynomial neural networks (Neural ODEs) where the dynamics preserve certain algebraic invariants, leveraging the neurovariety",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Multi-Resolution Polynomial Networks via Hierarchical Tensor Decompositions\nCreate polynomial networks that naturally encode multi-scale features using hierarchical tensor decompositions (like HT-deco",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Quantum-Inspired Polynomial Networks for Tensor Rank Estimation\nLeverage the connection between polynomial networks and quantum entanglement measures to design networks that can efficiently estimate t",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Adversarially Robust Polynomial Networks via Algebraic Certificates\nUse the algebraic structure of polynomial networks to derive polynomial certificates of robustness, providing provable bounds on adv",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Symmetric Tensor Decomposition Networks for Invariant Feature Learning\nDesign polynomial networks that explicitly incorporate symmetric tensor decompositions to learn features invariant to group actio",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Polynomial Network Pruning via Algebraic Dimension Reduction\nDevelop pruning algorithms that use the algebraic dimension of neurovarieties to identify and remove redundant parameters while maintaining",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 20,
      "paper_title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
      "contribution": "A systematic diagnosis showing that GPU/kernel-level floating-point non\u2011associativity and reduction ordering produce large, reproducibility\u2011breaking output differences in LLM inference, and a lightweight inference pipeline to mitigate these numerical sources of nondeterminism.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 13948,
      "output_tokens": 971,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Deterministic PagedAttention: Hardware-Software Co-design for Reproducible LLM Serving\nCombine insights from Orca's iteration-level scheduling with PagedAttention's memory management to create a deter",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Float-Aware Selective Batching for Numerically Stable Transformer Inference\nExtend Orca's selective batching concept to group operations based on their floating-point sensitivity profiles. Develop a d",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Hierarchical Checkpointing for Reproducible Distributed LLM Training\nCreate a multi-level checkpointing system that captures floating-point rounding states at threadblock, warp, and thread levels (ins",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Volatility-Aware Ensemble Serving for Financial LLMs\nDevelop a serving system that dynamically adjusts ensemble sizes based on detected output volatility for financial applications. Use lightweight pr",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Deterministic GEMM Kernels with Adaptive Tiling for Variable Workloads\nDesign CUDA kernels that combine CUTLASS-style hierarchical tiling with deterministic reduction trees. Implement adaptive tiling ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Cross-Hardware Reproducibility Layer for Distributed LLM Inference\nBuild a middleware layer that ensures consistent outputs across different hardware accelerators (GPUs, TPUs, Groq LPUs). Implement pr",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Instability-Guided Architecture Search for Robust Transformers\nDevelop methods to automatically identify and modify transformer architectures to minimize sensitivity to initialization and optimization",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Memory-Coherent Iteration Scheduling for Multi-GPU Serving\nExtend Orca's iteration-level scheduling to maintain coherent KV caches across distributed GPUs. Design a scheduling algorithm that minimizes",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Probabilistic Contracts for Approximate LLM Serving\nCreate a framework that allows clients to specify acceptable tolerance levels for output variations in exchange for better performance. Develop serv",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Hardware-Accelerated Stable Accumulation Units for Deep Learning\nDesign specialized hardware units that implement numerically stable reduction algorithms (like Kahan summation) at wire speed. These un",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 21,
      "paper_title": "PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models",
      "contribution": "PRIMT reduces human labeling and improves reward learning in preference-based RL by using a hierarchical fusion of multimodal foundation models for synthetic feedback together with foresight and hindsight trajectory synthesis (including SCM-based counterfactuals) to reduce query ambiguity and improve credit assignment.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 10864,
      "output_tokens": 1334,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hierarchical Constitutional AI: Multi-Level Principle Learning from Human Feedback\nDevelop a system that learns hierarchical constitutional principles from human feedback, where higher-level principle",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Embodied Hindsight Experience Replay with Multimodal Feedback\nIntegrate PaLM-E's multimodal capabilities with hindsight experience replay and human preference learning to enable robots to learn from f",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Causal Graph-Guided Preference Learning for Transparent AI Alignment\nApply Pearl's causal inference framework to understand and visualize the causal relationships between human preferences, AI actions",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Cross-Domain Constitutional Transfer Learning\nDevelop methods to transfer constitutional principles learned in one domain (e.g., text summarization) to new domains (e.g., robotic manipulation) by lear",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Preference-Aware Experience Ranking for Few-Shot Robot Learning\nCombine the experience ranking approach from HER with human preference learning to prioritize replay of experiences that are most likely",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Multimodal Constitutional Feedback: Beyond Text-Based Principles\nExtend constitutional AI to incorporate visual, auditory, and physical demonstrations as constitutional principles, using PaLM-E's mult",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Active Causal Discovery for Preference Learning\nDevelop algorithms that actively query humans to discover causal relationships between features and preferences, using Pearl's framework to design optim",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Federated Constitutional AI with Diverse Human Feedback\nCreate a framework for learning constitutional principles from diverse human populations while preserving privacy and handling conflicting prefe",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Temporal Preference Learning with Hindsight Constitutional Updates\nDevelop methods that can update constitutional principles based on long-term outcomes of AI behaviors, using hindsight to evaluate wh",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Embodied Preference Learning Through Mental Simulation\nLeverage large multimodal models to mentally simulate the outcomes of different actions and predict human preferences without physical execution.",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 22,
      "paper_title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
      "contribution": "Identifies and formalizes 'feature absorption'\u2014a systematic failure mode of Sparse Autoencoders (SAEs) where seemingly monosemantic latents are suppressed by their hierarchical children under sparsity pressure\u2014introduces a metric to detect it, and empirically shows it is pervasive and not remedied by simple SAE size or sparsity tuning.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11809,
      "output_tokens": 1060,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hierarchical Meta-SAEs: Learning Multi-Scale Feature Decompositions in Neural Networks\nDevelop a hierarchical version of meta-SAEs that can decompose features at multiple scales simultaneously, inspir",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Adversarial Robustness Through Controlled Feature Superposition\nInvestigate how manipulating the degree of superposition in neural networks affects their adversarial robustness. By training models wit",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Cross-Modal Sparse Autoencoders for Vision-Language Feature Alignment\nDesign sparse autoencoders that simultaneously decompose features from both vision and language modalities, enforcing shared spars",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Dynamic Sparsity SAEs with Neuromorphic Hardware Co-Design\nDevelop SAEs with adaptive sparsity levels that can be efficiently implemented on neuromorphic hardware, building on the NSS spike sorting ap",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Quantum-Inspired Superposition Structures in Neural Networks\nExplore the connection between feature superposition and quantum mechanical principles, particularly the fractional quantum Hall effect ana",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Compositional Dictionary Learning with Tree-Structured Priors for SAEs\nCombine hierarchical sparse coding methods with SAE training to learn dictionaries that naturally organize into interpretable tre",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Phase Transition Analysis of Feature Learning in Large Language Models\nSystematically study how feature representations transition between superposition regimes during training of large language model",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Causal Feature Circuits: Tracing Information Flow Through Superposed Representations\nDevelop methods to trace causal pathways through networks even when features are in superposition, extending circui",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Self-Organizing Feature Geometries in Overparameterized Networks\nInvestigate how the geometric structures (polytopes, simplices) discovered in toy models scale to large networks. Develop training meth",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Federated Interpretability: Distributed SAE Training Across Private Data\nCreate methods for training sparse autoencoders in a federated learning setting where data privacy is paramount. This would ena",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 23,
      "paper_title": "EvoLM: In Search of Lost Language Model Training Dynamics",
      "contribution": "EvoLM builds a transparent, end-to-end model suite and experimental pipeline (100+ 1B/4B decoder-only LMs trained from scratch on open data) to systematically trace training dynamics across pre-training, continued pre-training, supervised fine-tuning, and RL, revealing practical trade-offs (diminishing returns, forgetting, bridging roles of continued pre-training, and SFT/RL trade-offs) and releasing all models, data, and code for reproducible study.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 1,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9333,
      "output_tokens": 701,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Cross-Domain Data Density Transfer Learning\nCreate techniques to transfer data density insights from well-studied domains (like web text) to specialized domains. This would help prevent sub-scaling wh",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 24,
      "paper_title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions",
      "contribution": "A theoretical and algorithmic treatment of gradient-based training on AIMC devices with general, asymmetric and nonlinear pulse-response functions, proving that residual-learning updates (a bilevel formulation) remove the implicit bias caused by asymmetric responses and recover convergence to true critical points while also handling limited response granularity.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 4,
      "input_tokens": 12107,
      "output_tokens": 970,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Tiki-Taka Algorithm with Device-Aware Learning Rate Scheduling\nExtend the Tiki-Taka algorithm to dynamically adjust learning rates based on real-time device degradation measurements. This wou",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Probabilistic Weight Clustering for Hierarchical Error Mitigation in Analog Arrays\nDevelop a weight clustering technique that groups synapses based on their error characteristics and applies EaPU-styl",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Cross-Layer Optimization Framework for Mixed-Precision Analog-Digital Neural Networks\nCreate a systematic approach to partition neural networks between high-precision digital layers and energy-efficie",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Temporal Compensation Networks for Long-Term Drift in Phase-Change Memory\nDesign auxiliary neural networks that learn to predict and compensate for time-dependent conductance drift in PCM devices. The",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Stochastic Gradient Descent with Asymmetric Update Kernels for Analog Devices\nDevelop modified SGD algorithms that explicitly model and compensate for asymmetric weight updates in analog devices. This",
          "is_match": true
        },
        {
          "idea_idx": 5,
          "idea_text": "Federated Learning Protocol for Distributed Analog In-Memory Computing Arrays\nDesign communication-efficient federated learning algorithms optimized for analog hardware where weight quantization and d",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Hardware-Software Co-Design for Vision Transformers on Resistive Crossbars\nOptimize Vision Transformer architectures specifically for resistive memory implementation, including custom attention mechan",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Noise-Injection Training with Learnable Device Models\nDevelop training methodologies where neural networks learn to model their target hardware's noise characteristics during training. The device mode",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Dynamic Precision Allocation for Energy-Aware Analog Neural Network Inference\nCreate runtime systems that dynamically adjust the number of PCM devices per weight based on input data characteristics an",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Analog Computing Compiler with Automated Error-Aware Mapping\nBuild a compiler framework that automatically maps trained neural networks to analog hardware while optimizing for device variations, incor",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 25,
      "paper_title": "Discovering Opinion Intervals from Conflicts in Signed Graphs",
      "contribution": "Introduce and study the problem of recovering a small set of interpretable opinion intervals on a line that explain the positive/negative edges of a signed graph, prove hardness results, derive a polynomial-time approximation scheme by connecting the model to interval/indifference graphs and correlation clustering, and provide scalable heuristics with empirical validation.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "input_tokens": 11422,
      "output_tokens": 1252,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Dynamic Balance Preservation in Temporal Signed Networks\nDevelop algorithms to maintain approximate balance in signed graphs that evolve over time by strategically flipping edge signs. This would exte",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Correlation Clustering with Mixed Constraints: Combining Fixed Clusters and Spatial Embeddings\nDesign approximation algorithms for correlation clustering where we simultaneously require k clusters and",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Parameterized Complexity of Balance Detection in Sparse Signed Graphs\nStudy the fixed-parameter tractability of detecting balanced subgraphs of size k in signed graphs, building on Harary's characteri",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Indifference Graphs as Models for Correlation Clustering Solutions\nInvestigate when correlation clustering solutions can be represented as indifference graphs, where the real-line ordering corresponds",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Approximating Optimal Sign Flips for Achieving Near-Balance\nDevelop efficient algorithms to find the minimum number of edge sign changes needed to make a signed graph \u03b5-balanced (allowing a small frac",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Streaming Algorithms for Balance Maintenance in Signed Networks\nDesign space-efficient streaming algorithms that can detect when a signed graph becomes unbalanced as edges are added, using sketching t",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Correlation Clustering on Signed Indifference Graphs\nStudy the special case where the input to correlation clustering is a signed indifference graph, exploiting the structural properties to achieve be",
          "is_match": true
        },
        {
          "idea_idx": 7,
          "idea_text": "Robust Balance Metrics for Incomplete Signed Networks\nDevelop new balance measures that are robust to missing edges, combining ideas from correlation clustering on general graphs with signed graph bal",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Hierarchical Correlation Clustering with Balance Constraints\nCreate algorithms that produce hierarchical clusterings of signed graphs where each level maintains approximate balance, useful for multi-s",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 26,
      "paper_title": "A Clean Slate for Offline Reinforcement Learning",
      "contribution": "They introduce a rigorous, budget-aware evaluation and a set of minimal single-file implementations, unify prior algorithmic choices into a single hyperparameterized family (Unifloral), and\u2014using that clean infrastructure\u2014develop two new algorithms (TD3-AWR and MoBRAC) that outperform prior baselines under transparent, quantified offline evaluation budgets.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12734,
      "output_tokens": 1310,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Based on the provided papers on offline reinforcement learning, here are 10 novel research ideas:",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Adaptive Online Evaluation Budgets for Domain-Specific Offline RL**\n   Develop a method to automatically determine optimal online evaluation budgets based on problem characteristics (e.g., state spa",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Multi-Stage Hyperparameter Optimization for Offline RL with Progressive Data Utilization**\n   Create a hyperparameter tuning framework that progressively unlocks portions of the offline dataset base",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**Uncertainty-Calibrated Behavior Cloning for Hybrid Offline-Online RL**\n   Combine Paper 3's behavior cloning insights with Paper 5's uncertainty penalization to create an algorithm that smoothly tra",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Meta-Learning for Offline RL Algorithm Selection**\n   Develop a meta-learning approach that predicts which offline RL algorithm (e.g., AWR, MOPO, or simple BC) will perform best given dataset charac",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Compositional Offline RL with Modular Advantage Estimation**\n   Extend Paper 4's AWR approach by learning modular advantage functions for different subtasks, enabling zero-shot composition of skills",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Budget-Aware Model-Based Offline RL with Adaptive Pessimism**\n   Combine Paper 5's MOPO with Paper 1's evaluation budget insights to create a model-based method that adjusts its pessimism level base",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Offline RL with Learned Data Quality Metrics**\n   Develop methods to automatically assess offline dataset quality and coverage, then use these metrics to inform algorithm hyperparameter selection. T",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Federated Offline RL with Heterogeneous Data Sources**\n   Create algorithms for learning from multiple offline datasets collected under different conditions without sharing raw data. This would exte",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Continual Offline RL with Selective Forgetting**\n   Design methods that can incrementally learn from new offline datasets while selectively forgetting outdated policies, using Paper 4's regression f",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 27,
      "paper_title": "Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy",
      "contribution": "They develop novel high-probability spectral-norm perturbation bounds for the top-p low-rank approximation of a symmetric matrix under arbitrary symmetric noise, using a new 'contour bootstrapping' complex-analytic technique, and apply these bounds to give strictly sharper utility guarantees for differentially private PCA (improvements up to a factor \u221an).",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 2,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 10350,
      "output_tokens": 954,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Derive tight bounds on the error in computing f(A+E) where E is privacy-preserving noise, using both Davis-Kahan and Dyson Brownian motion approaches.\n",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Private Tensor Decomposition via Higher-Order Perturbation Theory\nGeneralize the matrix perturbation results to tensors, developing a theory of how privacy noise affects tensor decompositions. Create ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 28,
      "paper_title": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization",
      "contribution": "By analyzing algorithm-dependent sample concentration and GP sample-path properties to refine information-gain estimates, the paper proves improved high-probability regret bounds for GP-UCB\u2014eO(\u221aT) under certain Mat\u00e9rn kernels and O(\u221a(T ln^2 T)) for the squared-exponential kernel.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13260,
      "output_tokens": 1070,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Multi-fidelity Gaussian Process Bandits with Information-Theoretic Source Selection\nDevelop a GP bandit algorithm that can query multiple information sources of varying fidelity and cost. The method w",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Non-stationary Kernel Learning for Time-Varying GP Bandits\nCreate an algorithm that jointly learns time-varying kernel hyperparameters while optimizing in non-stationary environments. This would combi",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Constrained GP Bandits with Safe Exploration via Posterior Consistency\nExtend GP-UCB to handle unknown safety constraints by leveraging the posterior consistency results for binary GP regression. The ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Distributed GP Bandits with Communication-Efficient Information Fusion\nDesign a distributed optimization framework where multiple agents collaborate to optimize a global GP while minimizing communicat",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Adversarially Robust GP Bandits Under Corrupted Feedback\nDevelop GP bandit algorithms robust to adversarial corruption in observed values. The method would use robust kernel approximations and modifie",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Structured High-Dimensional GP Bandits via Additive Kernel Decomposition\nCreate algorithms for high-dimensional optimization by assuming additive kernel structure. This would extend the dimension-depe",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "GP Bandits with Preference Feedback and Dueling Actions\nExtend GP bandits to settings where only pairwise comparisons are available instead of direct function evaluations. The algorithm would construc",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Batch-Sequential GP Optimization with Parallel Information Gain\nDevelop batch selection strategies that maximize joint information gain for parallel function evaluations. This would extend single-poin",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Transfer Learning in GP Bandits via Hierarchical Kernel Priors\nCreate a framework for transferring knowledge across related optimization tasks using hierarchical GP priors. The method would adapt quic",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Mixed-Variable GP Bandits for Combinatorial-Continuous Optimization\nExtend GP bandits to handle mixed discrete-continuous action spaces using product kernels. The algorithm would balance exploration i",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 29,
      "paper_title": "Auto-Compressing Networks",
      "contribution": "Introduce a novel architecture (ACN) that replaces short residual connections with long additive feedforward connections to the output, inducing an architectural auto-compression dynamic that concentrates useful information into earlier layers during training and yields compact, more robust representations without sacrificing accuracy.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13788,
      "output_tokens": 928,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Residual Path Pruning for Dynamic Network Compression\nDevelop a method that dynamically prunes residual paths during inference based on input complexity, leveraging the ensemble-like behavior",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Cross-Domain Residual Transfer with Stochastic Depth Regularization\nCombine stochastic depth training with domain adaptation techniques to create ResNets that generalize better across domains. The ran",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hierarchical Feedback Alignment for Very Deep Residual Networks\nExtend direct feedback alignment to residual architectures by creating hierarchical feedback connections that respect the multi-path str",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Progressive Residual Growing Networks with Linear Probe Guidance\nDesign networks that progressively add residual blocks during training, using linear classifier probes to determine when additional dep",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Stochastic Highway Networks for Uncertainty Estimation\nCombine the Square-Highway architecture with stochastic depth to create networks that naturally provide uncertainty estimates. The variability in",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Residual Path Attribution for Interpretable Deep Learning\nDevelop methods to attribute model predictions to specific residual paths, building on the ensemble interpretation. This would provide fine-gr",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Adaptive Stochastic Depth with Gradient Flow Monitoring\nCreate a training algorithm that dynamically adjusts the survival probability of layers based on real-time gradient flow analysis. Layers with v",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Multi-Scale Residual Networks with Parallel Feedback Streams\nDesign architectures that process information at multiple scales simultaneously using parallel residual streams, each with direct feedback ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Residual Network Compression via Path Importance Scoring\nDevelop a compression technique that identifies and removes redundant residual paths based on their contribution to the final output across the",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Continuous-Depth Residual Networks with Learnable Integration\nReplace discrete residual blocks with continuous transformations using neural ODEs, where the integration depth is learned during training",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 30,
      "paper_title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
      "contribution": "Introduce a multimodal-aware low-rank adaptation method (MokA) that decomposes adaptation into modality-specific unimodal compression and explicit cross-modal interaction, yielding efficient and effective fine-tuning for MLLMs.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12335,
      "output_tokens": 1144,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Rank Selection for LoRA via Gradient-Based Importance Scoring\nDevelop a method that dynamically adjusts the rank of LoRA adapters during training based on gradient-based importance scores for",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hierarchical Visual Instruction Generation with Multi-Scale Feature Abstraction\nExtend visual instruction tuning by incorporating multi-scale visual features at different abstraction levels, allowing ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Gradient-Modulated DoRA: Combining Weight Decomposition with Dynamic Optimization\nIntegrate the gradient modulation technique from balanced multimodal learning with DoRA's weight decomposition approac",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Incremental LoRA Composition for Continual Multimodal Learning\nDevelop a framework where new LoRA modules can be incrementally added and composed for different tasks without catastrophic forgetting. T",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Task-Specific Routing Networks for Modular Vision-Language Adapters\nCreate a routing mechanism that dynamically selects and combines different LoRA/DoRA modules based on the input task characteristics",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Self-Supervised Pre-training of Low-Rank Adapters for Vision-Language Models\nPropose a self-supervised pre-training approach specifically for LoRA modules using masked multimodal modeling before task-",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Sparse-to-Dense Progressive Adaptation for Efficient Multimodal Fine-tuning\nDesign a training strategy that starts with extremely sparse adaptations and progressively increases capacity based on task ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Cross-Architecture Transfer of Low-Rank Adaptations in Vision-Language Models\nInvestigate methods to transfer learned LoRA parameters between different model architectures (e.g., from LLaMA-based to B",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 31,
      "paper_title": "Advancing Expert Specialization for Better MoE",
      "contribution": "Introduces complementary orthogonality and variance regularizers that, when added to standard MoE auxiliary balancing losses, reduce expert overlap and produce more discriminative routing and specialist experts\u2014improving downstream performance without architectural changes.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 11657,
      "output_tokens": 1217,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Orthogonality-Regularized Sparse Mixture of Experts\nCombine orthogonality constraints with sparse MoE architectures to improve expert specialization. By enforcing decorrelated representations across e",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Dynamic Expert Pruning via Mutual Coherence Tracking\nDevelop a method to dynamically prune underperforming experts during training by monitoring the mutual coherence between expert outputs. This would",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hierarchical Mixture of Experts with Decorrelated Gating Networks\nDesign a multi-level MoE architecture where gating networks at different hierarchical levels are regularized to produce decorrelated r",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Cross-Modal Mixture of Experts with Orthogonal Specialization\nExtend MoE architectures to handle multiple modalities (vision, text, audio) by enforcing orthogonality constraints that encourage each ex",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Adaptive Batch-Wise Expert Selection with Load Balancing\nDevelop a new routing mechanism that considers both example-specific features and current batch composition to optimize expert utilization. Thi",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Meta-Learning for Expert Initialization in Few-Shot Scenarios\nCreate a meta-learning framework that learns optimal orthogonal initializations for new experts based on task similarity. This would enabl",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Continuous Expert Interpolation with Sparsity Constraints\nReplace discrete expert selection with a continuous interpolation mechanism that maintains sparsity through learnable temperature parameters. ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Knowledge Distillation with Expert-Aware Compression\nDevelop a distillation method that preserves the specialized knowledge of individual experts when compressing large MoE models. This would involve ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Adversarially Robust Mixture of Experts via Decorrelation\nDesign MoE architectures that are inherently robust to adversarial attacks by enforcing strong decorrelation between experts. This would make ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Self-Supervised Pre-training for Expert Discovery\nCreate a self-supervised learning objective that encourages the emergence of diverse expert functions before supervised fine-tuning. By combining cont",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 32,
      "paper_title": "From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics",
      "contribution": "Provides a two-stage gradient-flow analysis of linearized Transformer attention training under small initialization, proving an initial escape-and-rowwise-condensation phase for value/output parameters followed by an active key/query-driven phase that produces asymptotic normalized rank collapse.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13574,
      "output_tokens": 815,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Initialization Scaling for Task-Specific Transformer Training\nDevelop a method that dynamically adjusts initialization scales during pre-training based on the detected task type (reasoning vs",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Neural Tangent Kernel-Guided Architecture Search for Stable Transformers\nDesign an architecture search method that uses NTK analysis to identify transformer configurations with optimal inductive biase",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Continuous-Time Attention Mechanisms with Flow Matching\nExtend the gradient flow matching framework to model attention weight evolution as a continuous dynamical system. This would enable prediction o",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Deformation-Invariant Vision Transformers via Modified Self-Attention\nDesign new attention mechanisms that explicitly incorporate the smoothness properties identified in NTK analysis to achieve better",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Implicit Bias Measurement in Large Language Models\nDevelop novel probing techniques that leverage the attention dynamics analysis to detect and quantify implicit biases in pre-trained language models.",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Hybrid Discrete-Continuous Training of Transformers\nCombine traditional gradient descent with flow-matching predictions to create a training algorithm that alternates between discrete updates and cont",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Kernel-Aware Pruning for Efficient Transformers\nUse NTK analysis to identify which transformer components contribute most to desired inductive biases, then develop pruning strategies that preserve the",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Multi-Scale Initialization for Compositional Learning\nDesign initialization strategies that use different scales for different transformer layers based on their role in compositional reasoning. Early ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Attention Pattern Forecasting for Early Stopping\nDevelop methods that predict final attention patterns from early training dynamics to determine optimal stopping points. This would prevent overfitting",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Flow-Based Meta-Learning for Few-Shot Reasoning\nCreate a meta-learning framework that uses gradient flow matching to quickly adapt pre-trained transformers to new reasoning tasks. The flow model would",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 33,
      "paper_title": "Large Language Diffusion Models",
      "contribution": "Introduces LLaDA, a large-scale discrete diffusion language model trained under the same pretraining and SFT paradigms as modern LLMs, showing that diffusion-based approaches can match autoregressive LLMs on in-context learning, instruction following, and other tasks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12141,
      "output_tokens": 1069,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Multi-Modal Discrete Diffusion with Cross-Domain Score Matching\nDevelop discrete diffusion models that can jointly model text and discrete image tokens using unified score entropy objectives. This wou",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Thermodynamic Attention Bottlenecks for Efficient Transformers\nApply thermodynamic principles to create information bottlenecks in transformer attention mechanisms. By modeling attention as a diffusio",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Absorbing Diffusion Models with Learnable State Transition Graphs\nExtend absorbing discrete diffusion by learning domain-specific transition graphs rather than using predefined matrices. The model wou",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Parallel Score Entropy Estimation with Masked Predictions\nCombine SEDD's score entropy approach with MaskGIT's parallel decoding to create fast discrete diffusion models. Instead of sequential denoisi",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Conditional Diffusion Models with Thermodynamic Consistency Regularization\nDevelop training objectives that ensure diffusion models maintain thermodynamic consistency when conditioning on various inpu",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Hybrid Continuous-Discrete Diffusion for Structured Data Generation\nCreate unified diffusion frameworks that can handle mixed continuous and discrete data by combining insights from all papers. The mo",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 34,
      "paper_title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation",
      "contribution": "A training-free, plug-and-play method (ALFAR) that maximizes the utility of retrieved contextual knowledge for MLLMs by (1) adaptively reallocating attention from visual to relevant context tokens (guided by query-context relevance) and (2) decoupling and adaptively weighting parametric and contextual signals at the output logits to resolve knowledge conflicts.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "input_tokens": 12860,
      "output_tokens": 1315,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Layer-Specific Retrieval Injection for RAG Models\nInvestigate injecting retrieved information at different transformer layers based on the attention patterns identified in Paper",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Design a mechanism that dynamically determines optimal injection points for retrieved content, potentially improving both efficiency and accuracy by aligning retrieval integration with the model's two",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Create a calibration method that adjusts retrieval confidence based on detected cross-modal conflicts, enabling more reliable multimodal question answering.\n",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Retrieval-Aware Pre-training with Synthetic Knowledge Conflicts\nDesign a pre-training objective that explicitly prepares models to handle imperfect retrieval and knowledge conflicts. Generate syntheti",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Multi-Agent RAG with Specialized Retrieval Experts\nImplement a multi-agent architecture where different retrieval experts specialize in specific domains or modalities. Develop a meta-controller that l",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Selective Parametric Knowledge Suppression in RAG Systems\nDevelop mechanisms to selectively suppress parametric knowledge when high-quality retrieved information is available. Design attention masking",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Cross-Lingual and Cross-Modal Knowledge Transfer in RAG\nInvestigate how retrieval augmentation can facilitate knowledge transfer across languages and modalities simultaneously. Develop methods that re",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 35,
      "paper_title": "Interactive Cross-modal Learning for Text-3D Scene Retrieval",
      "contribution": "Introduce IDeal, an interactive Text-3D Scene Retrieval method that iteratively refines text\u20133D alignment with a questioner/answerer loop (IRR) and an Interaction Adaptation Tuning (IAT) strategy to fuse feature- and semantic-level signals and bridge domain gaps for improved re-ranking.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 9218,
      "output_tokens": 1008,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adversarial Question Generation for Robust Interactive Retrieval\nCreate an adversarial framework where one LLM generates challenging follow-up questions while another learns to answer them, improving ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Cross-Modal Retrieval with Uncertainty-Aware Dialog Planning\nDesign a system that estimates uncertainty in its current retrieval results and dynamically plans follow-up questions to maximize informati",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Retrieval-Augmented Visual Question Answering with Dialog History\nCombine retrieval and VQA capabilities where the system not only finds relevant media but can answer complex questions about retrieved",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Explainable Interactive Retrieval with Natural Language Rationales\nDevelop models that can explain why certain follow-up questions are being asked and how user responses influence retrieval results. T",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Few-Shot Adaptation of Interactive Retrieval to New Domains\nCreate meta-learning approaches that allow interactive retrieval systems to quickly adapt to new visual domains or specialized vocabularies ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Multi-User Collaborative Retrieval with Consensus Learning\nDesign a system where multiple users can simultaneously engage in dialog to retrieve content, with the model learning to reconcile different ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Continual Learning for Interactive Retrieval with User Feedback Integration\nDevelop methods for interactive retrieval systems to continuously improve from implicit and explicit user feedback during de",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 36,
      "paper_title": "Rethinking Joint Maximum Mean Discrepancy for Visual Domain Adaptation",
      "contribution": "They derive a concise, representer-theorem based form of JMMD that (1) unifies marginal/conditional/weighted distances as special cases via label kernels, (2) explains why JMMD can hurt feature discrimination through a graph-embedding view, and (3) repairs this by jointly optimizing JMMD with HSIC (JMMD-HSIC) to produce a tractable, discrimination-preserving adaptation loss.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12097,
      "output_tokens": 902,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Kernel Selection for Dynamic Distribution Shifts\nDevelop a method that automatically selects and combines multiple kernel functions based on the detected type of distribution shift (marginal ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hilbert-Schmidt Independence Criterion for Multi-View Learning\nExtend HSIC to handle multiple views of data simultaneously by defining a tensor-based cross-covariance operator that captures higher-ord",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Temporal Maximum Mean Discrepancy for Evolving Domain Adaptation\nDesign a time-aware MMD variant that captures how distributions change over time, incorporating temporal kernels to model distribution ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Kernel Two-Sample Testing with Differential Privacy\nDevelop differentially private versions of kernel two-sample tests that maintain statistical power while protecting individual data points, using no",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Joint Distribution Adaptation with Causal Constraints\nIncorporate causal graph structures into JDA to ensure that adapted features respect causal relationships between variables, preventing negative t",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Hierarchical Kernel Embeddings for Multi-Domain Transfer\nCreate a hierarchical framework that learns kernel embeddings at multiple scales, enabling transfer across multiple related domains simultaneou",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Adversarial Robustness in Kernel Distribution Matching\nDevelop adversarially robust versions of MMD and HSIC that are resistant to small perturbations in the data, using min-max optimization over kern",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Sparse Kernel Distribution Adaptation\nDesign algorithms that learn sparse combinations of basis kernels for distribution adaptation, using group lasso regularization in RKHS to identify the most infor",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Quantum Kernel Methods for Distribution Testing\nExtend kernel two-sample tests to quantum computing frameworks, leveraging quantum feature maps and quantum kernel estimation to achieve exponential spe",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Self-Supervised Kernel Learning for Unsupervised Domain Adaptation\nDevelop self-supervised objectives based on HSIC that learn domain-invariant representations without any labeled data, using contrast",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 37,
      "paper_title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables",
      "contribution": "Introduce a lightweight, learnable look-up-table (LUT) framework (PGLUT, SDLUT, AOLUT) that replaces heavy CNN components to perform high-quality, extremely fast pan-sharpening capable of processing very large remote-sensing images on commodity GPUs/CPUs.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 8401,
      "output_tokens": 1086,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Cross-Domain Transfer Learning for LUT-based Enhancement Networks\nDevelop a framework that trains LUT networks on one image domain (e.g., satellite imagery) and efficiently transfers the learned repre",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hierarchical Multi-Resolution LUTs for Progressive Image Super-Resolution\nDesign a cascaded LUT architecture where multiple small LUTs handle different frequency bands and resolution scales progressiv",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Neural Architecture Search for Optimal LUT-CNN Hybrid Networks\nDevelop an automated method to find the optimal balance between CNN layers and LUT components for different hardware constraints. This wo",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Self-Supervised LUT Learning from Unpaired Multi-Modal Images\nCreate a framework that learns LUTs for cross-modal image enhancement (e.g., thermal to RGB, SAR to optical) without paired training data.",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Federated Learning Framework for Personalized Image Enhancement LUTs\nDevelop a privacy-preserving method where mobile devices collaboratively learn personalized enhancement LUTs based on user preferen",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Physics-Informed LUT Networks for Atmospheric Correction in Remote Sensing\nIncorporate physical models of atmospheric scattering and absorption into the LUT learning process for satellite image enhanc",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Dynamic LUT Compression using Neural Pruning and Quantization\nDesign an adaptive compression scheme that reduces LUT memory footprint by identifying and removing redundant entries while maintaining qu",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Multi-Task LUT Networks for Simultaneous Enhancement and Restoration\nCreate a unified LUT-based architecture that performs multiple image processing tasks (denoising, super-resolution, color enhanceme",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 38,
      "paper_title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
      "contribution": "Using dynamical mean field theory the authors show that, in the joint large-width and large-sample regime, training dynamics exhibits a separation of timescales that (i) produces slow growth of function complexity, (ii) yields an inductive bias toward low-complexity solutions determined by initialization, and (iii) dynamically decouples feature learning from overfitting \u2014 predicting nonmonotone test error and a late-time 'feature unlearning' regime.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 15299,
      "output_tokens": 1512,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Based on the provided papers on neural network theory, optimization dynamics, and implicit bias, here are 10 novel research ideas:",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Mean Field Analysis of Neural Tangent Kernel Evolution Beyond Initialization**\nExtend the mean field theory from Paper 1 to analyze how the Neural Tangent Kernel (Paper 2) evolves during training in",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "**Implicit Bias of Gradient Descent in Multi-Layer Networks with Separable Data**\nGeneralize the max-margin convergence result from Paper 6 to deep networks by combining the two-layer mean field analy",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**High-Dimensional Dynamics of Adaptive Optimizers via Approximate Message Passing**\nApply the AMP framework from Paper 4 to characterize the behavior of Adam, RMSprop, and other adaptive methods, rev",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Quantifying the Lazy-Active Phase Transition in Neural Network Training**\nDevelop a precise mathematical framework combining insights from Papers 2 and 3 to predict when networks transition from laz",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Time-Scale Separation in Deep Networks: A Dynamical Systems Perspective**\nExtend the singular perturbation analysis from Paper 5 to deep networks, characterizing how different layers learn on differ",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "**Mean Field Theory for Neural Networks with Structured Data**\nGeneralize the mean field approach from Paper 1 to handle non-i.i.d. data with graph or manifold structure, deriving new PDEs that captur",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Implicit Regularization in Neural ODEs and Continuous-Depth Networks**\nApply the implicit bias analysis from Papers 3 and 6 to neural ODEs, characterizing what solutions are favored when training co",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Universal Approximation via Gradient Flow: Beyond Static Networks**\nCombine the distributional dynamics framework (Paper 1) with the high-dimensional characterization (Paper 4) to prove dynamic univ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Feature Learning Dynamics in Wide Networks with Finite Samples**\nBridge the gap between infinite-width NTK theory (Paper 2) and finite-width feature learning by developing a perturbation theory that",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 39,
      "paper_title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
      "contribution": "Demonstrates that dramatically increasing network depth (up to 1024 layers) in a self-supervised, goal-conditioned contrastive RL setup yields large quantitative gains and qualitatively new goal-reaching behaviors that shallower agents cannot discover.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13483,
      "output_tokens": 1069,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Contrastive Skill Discovery with Language-Conditioned Representations\nCombine CURL's contrastive learning with DIAYN's skill discovery to learn diverse skills from pixels while simultaneously learning",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hindsight Contrastive Learning for Multi-Goal RL\nIntegrate CURL's representation learning with HIGhER's hindsight generation to create a framework where failed trajectories are relabeled with both new",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Universal Contrastive Value Functions for Compositional Task Learning\nExtend UVFAs to use contrastive representations (inspired by SimCLR and CURL) that generalize across both states and compositional",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Quality-Aware Data Augmentation for Self-Supervised RL\nApply the data quality insights from the scaling laws paper to improve CURL and SimCLR's augmentation strategies. Develop metrics to measure augm",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Hindsight Skill Composition Networks\nCombine DIAYN's skill discovery with HIGhER's hindsight relabeling to learn hierarchical policies that can compose discovered skills to solve complex tasks. Failed",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Contrastive Meta-Learning for Rapid Task Adaptation\nDevelop a meta-learning framework that uses contrastive representations to quickly adapt to new tasks, building on CURL's efficiency gains. The appr",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Language-Guided Exploration with Information-Theoretic Objectives\nExtend DIAYN's information-theoretic approach to incorporate language instructions, creating diverse exploration strategies conditione",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Scaling Laws for Self-Supervised Representation Learning in RL\nInvestigate how the sub-scaling phenomenon applies to self-supervised methods like CURL and SimCLR in RL contexts. Develop theoretical fr",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Hindsight-Augmented Contrastive Representations\nCreate a unified framework that uses failed trajectories to generate both new contrastive pairs (extending CURL) and new goal descriptions (extending HI",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Compositional Skill Discovery with Universal Value Functions\nCombine DIAYN's unsupervised skill discovery with UVFAs to learn skills that generalize across different goal specifications. The discovere",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 40,
      "paper_title": "Depth-Bounds for Neural Networks via the Braid Arrangement",
      "contribution": "For ReLU (and related maxout) networks compatible with the braid fan, the paper proves a non-constant lower bound \u2126(log log d) on the number of hidden layers needed to compute the maximum of d numbers, gives a combinatorial proof that max of 5 numbers needs three hidden layers under the same compatibility assumption, and supplies a tighter constructive upper bound in the maxout setting (rank-3 followed by rank-2 suffices for max of 7).",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12630,
      "output_tokens": 794,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Developing an AI-Powered Feedback System for Enhancing Writing Self-Efficacy in EFL Students\nIntegrate machine learning algorithms to provide personalized, real-time feedback on EFL students' writing ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Cross-Domain Knowledge Transfer Framework for Corporate-University Research Partnerships\nDesign a structured methodology for translating university research outputs into industry-applicable innovation",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Unified Learnability Assessment Tool for Software Development Teams\nDevelop a comprehensive framework that integrates UX and LD perspectives on software learnability, based on Miller's findings. This ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Neural Network Models for Pitch Processing in Language Learning Applications\nApply insights from Hertrich et al.'s brain lateralization research to develop AI-based language learning tools that optimi",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Reputation-Based Authentication System for Academic Matching Platforms\nBuilding on Basu et al.'s matching platform strategies, create a blockchain-based authentication system for academic collaboratio",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Self-Regulated Learning Strategy Recommendation Engine Using Generative AI\nLeverage large language models to analyze student writing patterns and recommend personalized SRL strategies. The system woul",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Innovation Ecosystem Simulation Platform for Policy Design\nCreate an agent-based model simulating the interactions between universities, corporations, and small firms in the innovation ecosystem. This",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Adaptive Software Training System Based on Learnability Profiles\nDevelop an intelligent tutoring system that adjusts training content based on real-time assessment of user learnability factors. The sy",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Cross-Linguistic Writing Strategy Transfer Analysis Using Deep Learning\nInvestigate how writing self-efficacy and SRL strategies transfer between languages using neural network analysis of multilingua",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Collaborative Innovation Network Analysis Tool for Research Productivity\nDesign a network analysis platform that maps and optimizes collaborative relationships between academic and industrial research",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 41,
      "paper_title": "Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization",
      "contribution": "Introduce a new CMI-style generalization bound that injects stochastic projection and lossy compression (quantization) into the CMI super-sample framework to obtain strictly tighter, non\u2011vacuous O(1/\u221an) guarantees on instances where prior MI/CMI bounds fail, and to argue that memorization is not necessary for good generalization.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 4,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11130,
      "output_tokens": 715,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Information-Theoretic Analysis of Federated Learning with Heterogeneous Data\nExtend CMI-based generalization bounds to federated settings where data distributions vary across clients. This would quant",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "CMI-Based Early Stopping Criteria for Overparameterized Models\nDesign stopping rules for iterative algorithms based on monitoring the conditional mutual information between iterates and training data,",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Information-Optimal Data Augmentation Strategies\nCharacterize data augmentation methods through their effect on mutual information between augmented samples and model parameters, leading to principled",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Multi-Resolution Information Bounds for Hierarchical Learning\nDevelop information-theoretic bounds that capture learning at multiple scales simultaneously, extending the single-scale analysis to hiera",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 42,
      "paper_title": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning",
      "contribution": "Introduce a local data-attribution framework for online RL (PPO) using gradient-similarity-based influence from recent buffers, and leverage it to diagnose learning and to iteratively filter experiences (IIF) to speed and stabilize training.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 11846,
      "output_tokens": 1585,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Influence-Aware Proximal Policy Optimization (IA-PPO)\nIntegrate influence functions into PPO to identify and prioritize training episodes that most impact policy performance. This would enable more sa",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Data Shapley for Experience Replay Buffer Management\nApply Data Shapley valuation to quantify the contribution of each transition in the replay buffer to the agent's learning progress. This would enab",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Epistemic Uncertainty-Guided Policy Gradient Methods\nExtend PPO with epistemic uncertainty estimation to modulate the policy update step size based on the agent's knowledge uncertainty. This would pre",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "TracIn for Real-Time RL Debugging\nDevelop an online version of TracIn that traces how individual environment interactions influence policy degradation or improvement during deployment. This would enab",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Multi-Agent Data Valuation for Cooperative RL\nExtend Data Shapley to multi-agent settings to fairly attribute learning improvements to data generated by different agents. This framework would enable e",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Influence-Based Curriculum Learning for RL\nUse influence functions to automatically design curricula by identifying which simpler tasks most positively influence learning on complex target tasks. This",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Uncertainty-Aware Data Shapley for Robust Valuation\nCombine epistemic uncertainty estimation with Data Shapley to account for model confidence when valuing training data. This would provide more robus",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "PPO with Influence-Weighted Importance Sampling\nModify PPO's importance sampling mechanism to weight policy ratios by the influence of corresponding trajectories on value function accuracy. This would",
          "is_match": true
        },
        {
          "idea_idx": 8,
          "idea_text": "Federated RL with Shapley-Based Contribution Tracking\nDevelop a federated learning framework for RL that uses Data Shapley to track each participant's contribution to the global policy. This would ena",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Adaptive Experience Replay with Combined Uncertainty and Influence Metrics\nCreate a unified prioritization scheme that combines epistemic uncertainty, TD error, and influence scores to select replay e",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 43,
      "paper_title": "High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model",
      "contribution": "Shows analytically and empirically that low-dimensional recurrent latent dynamics can produce high-dimensional observed neural activity (after neuronal nonlinearities), and introduces a provably interpretable latent\u2011variable method (NCE) to recover the latent dimensionality from recordings.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11378,
      "output_tokens": 779,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hierarchical Neural Manifold Discovery via Multi-Scale Kernel Decomposition\nDevelop a method that combines arc-cosine kernels with the neural manifold framework to discover hierarchical structure in p",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Temporally-Adaptive pi-VAE with Dynamic Kernel Embeddings\nExtend pi-VAE to incorporate time-varying kernels that can capture non-stationary dynamics in neural populations. This would allow the latent ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Meta-Learning Framework for Rapid Neural Manifold Adaptation\nCreate a meta-learning approach where pre-trained low-rank RNNs can quickly adapt their neural modes to new tasks. This would model how bio",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Kernel-Guided Dimensionality Reduction for Interpretable Brain-Machine Interfaces\nCombine arc-cosine kernels with the neural manifold framework to design more interpretable BMI decoders. Use the kerne",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Causal Discovery in Neural Populations via Interventional Manifold Analysis\nDevelop methods to infer causal relationships between neural modes by analyzing how optogenetic or electrical perturbations ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Multi-Modal Integration on Shared Neural Manifolds\nInvestigate how different sensory modalities (visual, auditory, tactile) project onto common low-dimensional manifolds in association areas. Use modi",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Continual Learning in Low-Rank Neural Networks with Manifold Regularization\nDesign training algorithms for low-rank RNNs that prevent catastrophic forgetting by maintaining consistent neural manifold ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 44,
      "paper_title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks",
      "contribution": "The paper presents a novel training approach for Spiking Neural Networks that utilizes adaptive surrogate gradients and a guiding policy to enhance performance in sequential reinforcement learning tasks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13304,
      "output_tokens": 846,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Temporal Credit Assignment in Spiking Neural Networks Using Reverse-Mode Differentiation\nDevelop a biologically-plausible learning algorithm that combines the temporal dynamics of spiking neurons with",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Adaptive Surrogate Gradient Functions Based on Local Membrane Potential Statistics\nDesign surrogate gradient functions that dynamically adapt their shape based on the local statistics of membrane pote",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hierarchical Spike-Timing Codes for Multi-Scale Temporal Feature Learning\nCreate a multi-layer spiking architecture where different layers operate at different temporal scales, enabling the network to",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Membrane Potential Prediction Networks for Efficient SNN Inference\nDevelop auxiliary networks that predict future membrane potential distributions, allowing for adaptive spike threshold modulation and",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Stochastic Regularization of Spiking Networks Through Controlled Synaptic Noise\nImplement a training methodology that incorporates controlled stochastic noise at the synaptic level, similar to dropout",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Meta-Learning of Spike-Timing-Dependent Plasticity Rules\nUse meta-learning to discover optimal STDP rules for different tasks and network architectures, rather than relying on fixed, hand-designed pla",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Hybrid Spiking-Analog Networks with Learnable Conversion Interfaces\nDesign networks that seamlessly combine spiking and traditional analog neurons, with learnable conversion modules that optimize the ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Adversarial Training of Spiking Networks with Temporal Perturbations\nExtend adversarial training to the temporal domain specific to SNNs, where perturbations affect spike timing rather than just input",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Self-Organizing Spiking Architectures Through Neural Architecture Search\nDevelop neural architecture search methods specifically for spiking networks that consider both spatial connectivity and tempor",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 45,
      "paper_title": "Class-wise Balancing Data Replay for Federated Class-Incremental Learning",
      "contribution": "Introduce FedCBDR, a privacy-preserving, global-perspective replay pipeline that (1) reconstructs class-level pseudo features for coordinated, class-balanced exemplar sampling across heterogeneous clients and (2) applies task-aware temperature scaling to mitigate class imbalance and overconfidence between replayed and new classes.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11950,
      "output_tokens": 1016,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Memory-Free Federated Incremental Learning with Dynamic Prototype Generation\nDevelop a method that dynamically generates task-specific prototypes without storing any exemplars, using a learna",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hierarchical Knowledge Distillation for Federated Class-Incremental Learning\nDesign a multi-level distillation framework where knowledge is transferred both temporally (across tasks) and spatially (ac",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Privacy-Preserving Replay Generation via Differential Privacy in FCIL\nCreate a differentially private mechanism for generating synthetic replay samples in federated settings that guarantees formal pri",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Asynchronous Federated Incremental Learning with Temporal Alignment\nDevelop a framework that handles clients learning new tasks at different times and rates, using temporal alignment mechanisms to syn",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Meta-Learning Based Fast Adaptation for Federated Class-Incremental Learning\nPropose a meta-learning approach that learns how to quickly adapt to new classes in federated settings by learning optimal ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Federated Incremental Learning with Adaptive Client Selection and Weighting\nDesign an intelligent client selection mechanism that identifies which clients should participate in each round based on the",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Continual Federated Learning with Elastic Network Architecture\nDevelop a method that dynamically expands network capacity for new tasks while maintaining parameter efficiency through structured prunin",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Self-Supervised Pretraining for Robust Federated Class-Incremental Learning\nCreate a self-supervised pretraining strategy specifically designed for FCIL that learns representations robust to distribut",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Federated Incremental Learning with Uncertainty-Guided Sample Selection\nPropose a framework that uses uncertainty estimation to identify the most informative samples for replay or prototype generation",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Communication-Efficient FCIL via Gradient Sketching and Prototype Compression\nDevelop compression techniques specifically designed for FCIL that reduce communication costs by transmitting compressed g",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 46,
      "paper_title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain",
      "contribution": "The authors show that convolutional recurrent encoders trained on realistic, temporally-structured whisker simulator data \u2014 using both supervised and tactile-specific contrastive self-supervision \u2014 produce internal representations that closely match neural activity in rodent somatosensory cortex, and that recurrence and task performance predict neural alignment.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 13204,
      "output_tokens": 988,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Contrastive Predictive Coding for Goal-Driven Sensory Cortex Models\nIntegrate CPC objectives into hierarchical convolutional networks trained on ethologically relevant tasks to improve correspondence ",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "ConvLSTM Networks for Predicting Neural Dynamics in Active Sensing\nAdapt ConvLSTM architectures to predict spatiotemporal neural activity patterns in sensory cortices during active exploration behavio",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Whisker-Array Simulation for Training Biologically-Plausible Touch Networks\nUse WHISKiT Physics to generate large-scale synthetic datasets of whisker dynamics during natural behaviors, then train deep",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Multi-Modal Contrastive Learning Across Sensory Modalities\nExtend CPC to learn shared representations across visual, tactile, and auditory modalities by predicting future states in one modality from p",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Developmental Trajectory Networks for Haptic Exploratory Procedures\nDesign networks that learn age-appropriate haptic exploration strategies by training on developmental data from children. The model ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "State-Dependent Sensory Processing Networks with Behavioral Context\nDevelop architectures that modulate sensory processing based on behavioral state, mimicking how barrel cortex responses change durin",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Physics-Informed Neural Networks for Sensory Prediction\nCombine ConvLSTM architectures with physics constraints from biomechanical models to improve long-term prediction of sensory consequences of act",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hierarchical Predictive Coding for Multi-Scale Temporal Integration\nBuild networks that predict sensory inputs at multiple temporal scales simultaneously, from millisecond-level whisker dynamics to se",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Self-Supervised Learning of Sensory Invariances Through Active Exploration\nTrain agents that learn robust object representations by actively exploring objects with simulated whiskers, using contrastiv",
          "is_match": true
        },
        {
          "idea_idx": 9,
          "idea_text": "Cross-Species Transfer Learning for Sensory System Models\nDevelop architectures that can transfer learned representations between different species' sensory systems (e.g., rodent whiskers to human fin",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 47,
      "paper_title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
      "contribution": "Introduce Elastic Multimodal Parallelism (EMP) and ElasticMM, a serving system that decouples multimodal inference stages, performs modality-aware load balancing, and elastically adjusts per-stage parallelism and caching to greatly reduce TTFT and improve throughput for MLLMs.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 10582,
      "output_tokens": 1387,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Prefix-Suffix Decomposition for Multi-Tenant LLM Serving\nExtend Hydragen's prefix sharing approach to dynamically identify and cache common sub-sequences across different tenants and applicat",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Cross-Modal Attention Disaggregation for Unified Multimodal Serving\nBuilding on ModServe's modular architecture and DistServe's disaggregation approach, develop a system that separates cross-modal att",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Predictive Stage Migration for Bursty Multimodal Workloads\nLeverage ModServe's observation of bursty multimodal traffic patterns to develop ML models that predict incoming workload characteristics. Pr",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Iteration-Aware Memory Pooling for Disaggregated LLM Serving\nCombine Orca's iteration-level scheduling with DistServe's prefill-decode disaggregation to create a distributed memory pool that efficient",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Hierarchical Goodput Optimization for Heterogeneous LLM Deployments\nExtend DistServe's goodput optimization to heterogeneous GPU clusters where different GPU types handle different phases. Develop a m",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Elastic Batching for Shared-Context Generation Tasks\nBuilding on Hydragen's shared prefix optimization, develop variable-size batching strategies that group requests based on context similarity rather",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Cross-System State Migration for LLM Serving Resilience\nInspired by Orca's distributed architecture, create protocols for migrating in-flight generation state between different serving systems (e.g., ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Modality-Specific Speculative Decoding for Multimodal Models\nCombine insights from ModServe's modality-aware scheduling with speculative decoding techniques. Develop specialized draft models for diffe",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Adaptive Parallelism Selection for Long-Context Serving\nInspired by LoongServe's focus on long-context scenarios and DistServe's phase-specific parallelism, develop algorithms that dynamically choose ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Unified Cost Model for Multi-Phase LLM Serving Optimization\nSynthesize the scheduling approaches from all papers to create a comprehensive cost model that captures prefill/decode interference (DistSer",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 48,
      "paper_title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
      "contribution": "The paper introduces a dynamical low-rank training scheme with a novel spectral regularizer that enhances adversarial robustness while achieving significant compression in neural networks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 2,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 9657,
      "output_tokens": 844,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Low-Rank Ensemble Networks with Diverse Adversarial Robustness Properties\nCreate an ensemble of low-rank subnetworks, each optimized for different types of adversarial perturbations. The matrix differ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Dynamic Low-Rank Defense Networks with Real-time Adaptation\nImplement a defense mechanism that dynamically adjusts network rank based on detected adversarial threat levels. The system would use fast r",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 49,
      "paper_title": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training",
      "contribution": "QoQ-Med is the first open generalist clinical foundation model that effectively reasons across heterogeneous clinical data types.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 12060,
      "output_tokens": 934,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Real-Time EHR Summarization with Verifiable Reasoning Traces\nEnhance the RAG-based EHR summarization approach by incorporating explicit reasoning steps that can be audited by clinicians. Each summary ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Cross-Modal Medical Reasoning Benchmark with Human Preference Alignment\nCreate a comprehensive benchmark combining visual medical data, clinical notes, and physician preferences to evaluate multimodal",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Bootstrapped Medical Knowledge Graph Construction via Reasoning\nDevelop a self-improving system that constructs medical knowledge graphs by reasoning over multimodal clinical data. The system would us",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Adversarial Training for Robust Medical Reasoning Under Distribution Shift\nDesign a training framework that uses adversarial examples to improve medical AI robustness when reasoning patterns differ be",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Temporal Reasoning Networks for Disease Progression Modeling\nCreate specialized architectures that reason over longitudinal patient data to predict disease trajectories. The system would explicitly mo",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Multi-Agent Medical Reasoning with Specialized Expert Models\nDevelop a system where multiple specialized models (radiology, pathology, clinical notes) collaborate through structured reasoning dialogue",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Zero-Shot Medical Reasoning Transfer Across Languages and Cultures\nBuild on zero-shot learning approaches to enable medical reasoning models to transfer knowledge across languages and cultural context",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 50,
      "paper_title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free",
      "contribution": "This paper presents a systematic investigation of gating mechanisms in softmax attention variants, demonstrating that a simple head-specific sigmoid gate can enhance performance in large language models significantly.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10175,
      "output_tokens": 789,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Forgetting Gates for Highway Network Freight Flow Prediction\nCombine the Forgetting Transformer's forget gate mechanism with LSTM networks to predict traffic patterns on the National Highway ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Chunk-Aware MoE Routing for Variable-Length Document Processing\nIntegrate FLASH's mixed chunk attention with MoEUT's expert routing to create a system where different experts specialize in different c",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hierarchical Layer Grouping for Cross-Domain Transfer in Universal Transformers\nExtend MoEUT's layer grouping concept to create domain-specific expert groups that can be selectively activated. Lower l",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Memory-Efficient Forgetting Attention with Linear Complexity\nCombine the Forgetting Transformer's data-dependent downweighting with FLASH's linear attention approximation. This would create a model th",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Dynamic Expert Allocation Based on Attention Patterns\nDevelop a mechanism where MoE expert selection is guided by the attention patterns learned through Forgetting Attention. Frequently attended regio",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Temporal-Aware GAU for Time-Series Freight Network Analysis\nAdapt FLASH's Gated Attention Unit to incorporate temporal embeddings specific to freight movement patterns. The model would learn to gate i",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Cascaded Forgetting Mechanisms for Incremental Learning\nDesign a multi-level forgetting architecture where different forgetting gates operate at token, chunk, and document levels. This would enable mo",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Sparse Expert Networks for Multi-Modal Transportation Data\nExtend MoEUT to handle multi-modal inputs (text descriptions, network graphs, temporal sequences) from transportation datasets. Different exp",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Adaptive Chunk Size Selection with Learnable Boundaries\nEnhance FLASH's fixed chunk size approach with a learnable mechanism that determines optimal chunk boundaries based on content. This would impro",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Federated MoE Training for Privacy-Preserving Cybersecurity Models\nDevelop a distributed training framework for MoEUT where different experts can be trained on separate, privacy-sensitive cybersecurit",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 51,
      "paper_title": "Learning long range dependencies through time reversal symmetry breaking",
      "contribution": "Introducing Recurrent Hamiltonian Echo Learning (RHEL) for training state space models using Hamiltonian dynamics that efficiently compute gradients without backward pass or Jacobian calculations.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 10359,
      "output_tokens": 1162,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Time-Scale Equilibrium Propagation for Continual Learning\nDevelop a variant of Equilibrium Propagation that dynamically adjusts its relaxation time scales based on the temporal statistics of ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hamiltonian Echo Networks with Learnable Time-Reversal Operations\nExtend Hamiltonian Echo Backpropagation by making the time-reversal operation itself learnable through neural ODEs. This would allow t",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Zeroth-Order Equilibrium Propagation for Neuromorphic Hardware\nCombine the hardware-friendly aspects of implicit learning with Equilibrium Propagation to develop a training algorithm that requires no ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Multi-Phase Contrastive Learning in Recurrent Networks\nExtend the two-phase approach of Equilibrium Propagation to multiple phases with varying degrees of target nudging. This would create a curriculu",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Automatic Differentiation Through Physical Equilibria\nDevelop a general framework for computing gradients through physical systems that settle to equilibrium states, bridging automatic differentiation",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Continuous-Depth Equilibrium Networks with Adaptive Computation\nCombine Neural ODEs with Equilibrium Propagation to create networks where both the forward pass and the equilibrium-finding process use ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Bilevel Optimization for Self-Configuring Recurrent Networks\nUse the implicit differentiation techniques from hardware-friendly learning to optimize both the network weights and the architectural para",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hamiltonian-Constrained Backpropagation Through Time\nDevelop a variant of BPTT that preserves Hamiltonian structure during gradient computation, ensuring that learned dynamics remain physically realiz",
          "is_match": true
        },
        {
          "idea_idx": 8,
          "idea_text": "Equilibrium Propagation with Stochastic Relaxation Dynamics\nReplace the deterministic relaxation in Equilibrium Propagation with stochastic dynamics governed by Langevin equations. This would provide ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Hierarchical Time-Scale Separation in Continual RNN Learning\nDevelop a multi-timescale version of the continually running RNN algorithm where different layers operate at different time scales, inspire",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 52,
      "paper_title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution",
      "contribution": "FuXi-Ocean is the first data-driven global ocean forecasting model achieving six-hourly predictions at eddy-resolving 1/12\u00b0 spatial resolution.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13420,
      "output_tokens": 1006,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hybrid Physics-AI Ocean Forecasting with Adaptive Model Switching\nDevelop a system that dynamically switches between physics-based models (like HYCOM) and AI models (like WenHai/XiHe) based on forecas",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Physics-Informed Neural Operators for Ocean-Ice-Atmosphere Coupling\nCreate neural operators that enforce conservation laws and thermodynamic constraints at the interfaces between ocean, sea ice, and a",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Ensemble AI Ocean Forecasting with Uncertainty Quantification\nDevelop an ensemble-based AI forecasting system that generates multiple predictions with learned uncertainty estimates. The model would us",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Graph Neural Networks for Lagrangian Ocean Particle Tracking\nImplement graph-based neural networks that directly forecast trajectories of ocean particles, drifters, and tracers. This Lagrangian approa",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Self-Supervised Learning for Ocean Forecast Model Pre-training\nDesign self-supervised learning objectives specific to ocean dynamics (e.g., predicting masked ocean regions, temporal consistency, geost",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Neural Architecture Search for Regional Ocean Forecast Optimization\nDevelop automated methods to design optimal neural network architectures for specific ocean regions and applications. The system wou",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Federated Learning Framework for Distributed Ocean Forecast Training\nCreate a federated learning system that allows multiple ocean forecasting centers to collaboratively train AI models without sharin",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Attention-Based Extreme Ocean Event Detection and Forecasting\nBuild specialized transformer modules with learned attention mechanisms that focus on extreme ocean events (marine heatwaves, coastal floo",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Cross-Modal Ocean Forecast Fusion Using Multimodal Transformers\nDevelop a multimodal transformer that fuses satellite imagery, in-situ observations, and model outputs to create unified ocean forecasts",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 53,
      "paper_title": "Superposition Yields Robust Neural Scaling",
      "contribution": "The paper identifies representation superposition as a central driver of neural scaling laws, providing insights into the conditions under which these scaling laws can be enhanced or may break down.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "crawl_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 6246,
      "output_tokens": 793,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Resource Allocation Using Real-time Manifold Dimension Estimation\nDevelop training algorithms that dynamically adjust the model-to-data allocation ratio based on continuous estimation of the ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Multi-scale Data Quality Assessment via Feature Interference Patterns\nCreate a framework that evaluates data quality by measuring how different feature sparsity levels affect superposition and interfe",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Curriculum Learning Strategies Based on Manifold Complexity Progression\nDesign training curricula that gradually increase data manifold complexity to maintain optimal scaling throughout training. Star",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Geometric Structure Analysis for Identifying Redundant Training Data\nApply the geometric insights from superposition (digons, triangles, etc.) to identify and remove redundant data points that contrib",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Scaling Law Corrections for Mixture-of-Experts Architecture Design\nInvestigate how superposition effects distribute across expert modules in MoE models and develop modified scaling laws that account f",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Cross-Modal Manifold Alignment for Improved Scaling Efficiency\nDevelop methods to align data manifolds across different modalities (text, vision, audio) to identify shared low-dimensional structures. ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 54,
      "paper_title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
      "contribution": "The paper establishes that CNNs predominantly rely on local shape features rather than being inherently biased towards texture, offering a new evaluation of feature reliance through controlled suppression.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8587,
      "output_tokens": 804,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Shape-Texture Weighting Networks for Domain-Specific Recognition\nDevelop CNNs that dynamically adjust their shape vs. texture bias based on the target domain (medical imaging, satellite image",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Curriculum Learning with Progressive Shape-Texture Decomposition\nCreate a training methodology that gradually transitions CNNs from texture-based to shape-based representations through carefully desig",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Adversarial Style Transfer for Robust Object Detection\nLeverage style transfer techniques to generate adversarial training data that preserves object shape while varying textures systematically. This ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Disentangled Shape-Texture Representations for Few-Shot Learning\nDesign architectures with explicit shape and texture branches that can be independently fine-tuned. This would enable more efficient fe",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Texture-Invariant Medical Image Analysis Networks\nDevelop specialized CNN architectures for medical imaging that explicitly minimize texture bias by incorporating shape-preserving losses. This would a",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Interpretable Shape-Texture Attribution Methods\nDevelop new visualization and attribution techniques that separately quantify shape and texture contributions to CNN decisions. This would provide clear",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Self-Supervised Learning with Shape-Texture Augmentation\nDesign self-supervised pretraining objectives that encourage learning of shape-invariant features through systematic texture randomization. Thi",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hierarchical Shape-Texture Decomposition for Scene Understanding\nBuild models that learn hierarchical representations where early layers focus on local textures and later layers integrate global shape",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Cross-Cultural Bias Analysis Using Texture-Shape Conflicts\nInvestigate how CNNs trained on datasets from different geographic regions exhibit varying texture-shape biases. This research could reveal c",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 55,
      "paper_title": "On Linear Mode Connectivity of Mixture-of-Experts Architectures",
      "contribution": "This paper investigates Linear Mode Connectivity (LMC) within Mixture-of-Experts (MoE) architectures, proposing a matching algorithm for aligning independently trained MoEs to discover low-loss paths in parameter space.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12178,
      "output_tokens": 1126,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Sparsity Discovery via Progressive Linear Mode Connectivity\nDevelop a method that dynamically discovers optimal sparsity patterns during training by monitoring when subnetworks achieve linear",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Permutation-Aware Federated Learning with Sinkhorn Re-basin\nCreate a federated learning framework that uses differentiable Sinkhorn re-basin to align client models before aggregation. This would addre",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hessian-Guided Optimal Transport for Model Fusion\nDesign a model fusion algorithm that uses the Hessian spectrum properties (bulk vs. outliers) to guide the optimal transport matching. Large eigenvalu",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Early-Exit Networks via Linear Mode Connectivity Checkpoints\nDevelop adaptive neural networks that can exit at intermediate layers by exploiting the stability points where linear mode connectivity eme",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Continuous Model Interpolation for Adversarial Robustness\nCreate a defense mechanism that maintains multiple models in a linearly connected basin and dynamically interpolates between them based on inp",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Lottery Ticket Transplantation Across Architectures\nInvestigate methods to transfer winning lottery tickets between different architectures by finding permutation-invariant mappings. This would enable",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Mode Connectivity-Aware Neural Architecture Search\nDesign a NAS algorithm that explicitly searches for architectures exhibiting strong linear mode connectivity properties early in training. This would",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Incremental Learning via Hessian-Aligned Model Expansion\nDevelop a continual learning method that grows networks along the flat directions of the Hessian (near-zero eigenvalues) when learning new task",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Stochastic Weight Averaging in Permutation-Aligned Space\nCreate an improved stochastic weight averaging technique that first aligns models using differentiable Sinkhorn re-basin before averaging. This",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Multi-Scale Linear Mode Connectivity Analysis for Model Compression\nDevelop a compression framework that analyzes linear mode connectivity at different scales (layers, blocks, full models) to identify",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 56,
      "paper_title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model",
      "contribution": "OpenHOI introduces the first framework for synthesizing open-world hand-object interactions using multimodal large language models.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 14254,
      "output_tokens": 800,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Physics-Informed Diffusion Models for Tool-Use HOI Generation\nDevelop a physics-aware diffusion framework that explicitly models tool affordances and mechanical constraints during hand-object interact",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Zero-Shot HOI Transfer Across Object Categories via Affordance Graphs\nCreate a method that learns transferable affordance representations as graph structures, enabling HOI generation for completely un",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Temporal Action Composition for Long-Horizon HOI Sequences\nDesign a hierarchical framework that decomposes long text descriptions into atomic action primitives, then composes them into extended HOI se",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Egocentric-to-Exocentric HOI Synthesis with View Consistency\nDevelop a method to generate consistent hand-object interactions from both first-person and third-person viewpoints simultaneously. This wo",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Interactive HOI Refinement through Natural Language Feedback\nCreate an iterative refinement system where users can provide natural language corrections to generated HOI sequences. The model would lear",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Cross-Modal HOI Understanding: From Touch to Vision\nIntegrate tactile sensing data with visual HOI generation to produce interactions that are not only visually plausible but also haptically accurate.",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Adaptive HOI Generation for Assistive Robotics\nDevelop a framework that generates personalized hand-object interactions based on user capabilities and limitations. The system would adapt generated mot",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Compositional HOI Synthesis via Modular Skill Libraries\nBuild a modular system that learns reusable interaction skills (e.g., pinch, twist, push) that can be dynamically composed based on object prope",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Real-time HOI Prediction and Completion from Partial Observations\nCreate a method that can predict and complete hand-object interaction sequences from partial observations in real-time. This would be ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 57,
      "paper_title": "Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think",
      "contribution": "Representation Entanglement for Generation (REG) enhances the training efficiency and quality of image generation in diffusion models by entangling class tokens from pretrained models with low-level image latents.",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8664,
      "output_tokens": 959,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Dynamic Interpolant Selection via Learned Meta-Networks\nDevelop a meta-network that learns to select optimal interpolant functions (\u03b1_t, \u03c3_t) during training based on the current model state and data ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Temporal Consistency Enforcement via Representation Alignment in Video Diffusion\nApply REPA to video generation by aligning frame representations with pretrained video encoders that capture temporal c",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Self-Supervised Representation Recycling for Few-Shot Diffusion Adaptation\nExploit the finding that DDAEs learn linear-separable representations by recycling intermediate features from a pretrained di",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Stochastic-Deterministic Hybrid Sampling with Adaptive Interpolants\nCombine SiT's flexibility in choosing deterministic vs stochastic samplers with a learned policy that switches between modes based o",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Multi-Stage Latent Diffusion with Progressive Representation Refinement\nDesign a cascaded latent diffusion model where each stage operates at different compression ratios, with REPA applied progressiv",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Representation-Aware Curriculum Learning for Diffusion Training\nDevelop a curriculum learning strategy that orders training samples based on their alignment with pretrained encoder representations. St",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Bidirectional Flow Matching with Asymmetric Interpolants\nExtend SiT's interpolant framework to support asymmetric forward and reverse flows, where \u03b1_t and \u03c3_t differ between generation and inversion d",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 58,
      "paper_title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation",
      "contribution": "Dynam3D presents a dynamic layered 3D representation model that enhances vision-and-language navigation by improving spatial understanding and flexibility in changing environments.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 2,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9914,
      "output_tokens": 781,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Temporal CLIP: Learning Dynamic Scene Understanding from Video-Language Supervision\nExtend CLIP's contrastive learning framework to video data by incorporating temporal modeling, enabling zero-shot ac",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Multi-Agent Vision-Language Navigation with Collaborative Scene Graphs\nDevelop a framework where multiple embodied agents collaborate to navigate complex environments by building and sharing dynamic s",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 59,
      "paper_title": "Learning (Approximately) Equivariant Networks via Constrained Optimization",
      "contribution": "The paper introduces Adaptive Constrained Equivariance (ACE), a framework that systematically relaxes equivariance constraints during training to improve performance in equivariant neural networks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "input_tokens": 14268,
      "output_tokens": 1204,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Learnable Symmetry Discovery Networks with Energy-Based Priors\nDevelop a framework that automatically discovers approximate symmetries in data by combining group equivariant networks with energy-based",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Continuous-Filter Equivariant Networks for Non-Euclidean Molecular Dynamics\nExtend SchNet's continuous-filter convolutions to handle molecular systems on curved manifolds or with periodic boundary con",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Resilient Equivariant Learning with Adaptive Symmetry Constraints\nIntegrate the resilient constrained learning framework with approximate group equivariance to create models that dynamically adjust th",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Multi-Scale Group Equivariant Networks with Hierarchical Energy Functions\nDesign a hierarchical architecture that combines group equivariance at multiple scales with nested energy functions. Each scal",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Equivariant Neural ODEs with Force-Conserving Dynamics\nDevelop neural ordinary differential equations that maintain group equivariance while guaranteeing energy conservation. This would combine the co",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Soft Equivariance Networks with Learnable Violation Penalties\nCreate a framework that extends residual pathway priors to automatically learn the cost of violating different symmetries. The model would",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Federated Learning with Heterogeneous Equivariance Constraints\nDevelop federated learning algorithms that can handle clients with different symmetry requirements or data distributions. The system woul",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Equivariant Uncertainty Quantification Networks\nDesign group equivariant networks that provide calibrated uncertainty estimates which respect the same symmetries as the predictions. This would ensure ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Compositional Equivariant Networks with Modular Symmetry Groups\nCreate a modular architecture where different network components can have different symmetry groups that compose coherently. This would ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Equivariant Meta-Learning with Symmetry-Aware Adaptation\nDevelop meta-learning algorithms that can quickly adapt to new tasks while preserving or discovering appropriate equivariances. The system woul",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 60,
      "paper_title": "SAGE: A Unified Framework for Generalizable Object State Recognition with State-Action Graph Embedding",
      "contribution": "SAGE introduces a unified framework for recognizing object physical states and their temporal evolutions using State-Action Graph Embedding, enhancing generalization to unseen objects and actions.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "input_tokens": 10372,
      "output_tokens": 1063,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "The system would learn to predict state transitions and intermediate states from just 3-5 example videos per object-action pair.\n",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Vision-Language Models for Zero-Shot Action Effect Prediction\nCreate a framework that leverages pre-trained VLMs to predict the effects of actions on object states without explicit training, addressin",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "The model would use compositional reasoning to understand how novel actions modify object properties based on linguistic descriptions and visual similarities.\n",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Multi-Modal State Change Detection with Uncertainty Quantification\nDesign a system that combines visual and linguistic cues to detect object state changes while providing uncertainty estimates, addres",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "The model would use ensemble methods to identify when state changes are ambiguous or partially observable.\n",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Cross-Domain Transfer of State-Modifying Actions\nCreate a framework for transferring knowledge about state-modifying actions across different object categories and domains. The system would learn abst",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 61,
      "paper_title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
      "contribution": "The paper critically evaluates the effectiveness of Reinforcement Learning with Verifiable Rewards (RLVR) in enhancing the reasoning capabilities of large language models (LLMs) and reveals that the improvements are primarily superficial, as they do not generate fundamentally new reasoning patterns beyond those already established by base models.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 7262,
      "output_tokens": 835,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Reward-Based Prioritization with Verifiable Reward Signals\nCombine RP-PPO's reward-based prioritization with RLVR's verifiable rewards to create an adaptive prioritization scheme that dynamic",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hierarchical Post-Training with Progressive Reward Shaping\nDevelop a multi-stage post-training pipeline that combines SFT, DPO, and RP-PPO in a hierarchical manner, where each stage shapes rewards for",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Trust Region Optimization with Learned Ratio Clipping Boundaries\nCreate an adaptive version of PPO that learns optimal clipping boundaries during training rather than using fixed hyperparameters. The ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Contrastive Reward Prioritization for Multi-Task Learning\nExtend RP-PPO to handle multiple competing objectives by implementing contrastive reward prioritization that balances experiences across diffe",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Verifiable Reward Synthesis from Human Feedback Distributions\nDevelop methods to automatically generate verifiable reward functions from distributions of human feedback, bridging RLHF and RLVR. This w",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Normalized Advantage Estimation with Predictable Scaling Properties\nDesign a new advantage estimation method that incorporates reward normalization techniques from RP-PPO while ensuring predictable sc",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Dual-Phase Optimization: Exploration with RP-PPO, Exploitation with RLVR\nCreate a training framework that alternates between exploration phases using RP-PPO's prioritization and exploitation phases us",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Automated Hyperparameter Scheduling for Post-Training Pipelines\nDevelop an automated system that schedules hyperparameters (learning rates, clipping ratios, prioritization weights) across different po",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Federated Post-Training with Differential Privacy and Reward Prioritization\nImplement a federated learning approach for model post-training that uses RP-PPO's prioritization to selectively share high-",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 62,
      "paper_title": "Learning to Learn with Contrastive Meta-Objective",
      "contribution": "The paper introduces ConML, a meta-learning framework that utilizes task identity as additional supervision through contrastive learning to enhance generalizability.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10345,
      "output_tokens": 840,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Meta-Learning for Dynamic Task Distribution Shift\nDevelop a meta-learning framework that adapts to non-stationary task distributions by learning a time-dependent transformation function. This extends ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Contrastive Meta-Representations with Tail-Aware Augmentation\nCombine the contrastive learning framework from SimCLR with the tail modeling approach to create data augmentations that are specifically ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Function-Conditioned Model Dynamics Networks\nExtend the MetaModelNet concept by learning function-specific model dynamics that predict parameter trajectories conditioned on the underlying data-generat",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Hierarchical Meta-Learning with Semantic Task Clustering\nDevelop a meta-learning algorithm that automatically discovers and exploits semantic relationships between tasks by learning a hierarchical tas",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Self-Supervised Meta-Learning via Cross-Modal Alignment\nCreate a meta-learning framework that leverages multiple modalities (vision, text, audio) during meta-training but can adapt to single-modality ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Gradient-Free Meta-Learning with Learned Optimization Trajectories\nDesign a meta-learning approach that learns to predict optimal parameter trajectories without computing gradients during adaptation. ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Uncertainty-Aware Progressive Transfer for Long-Tailed Learning\nIncorporate uncertainty estimation into the progressive transfer framework to better calibrate predictions for tail classes. The meta-ne",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Meta-Learning with Synthetic Task Generation from Contrastive Embeddings\nDevelop a method that generates synthetic meta-training tasks by interpolating in the contrastive embedding space learned from ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Compositional Meta-Learning via Modular Function Decomposition\nCreate a meta-learning framework that decomposes complex functions into reusable primitive components. The approach would learn a library",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Adversarial Meta-Learning for Distribution-Robust Few-Shot Adaptation\nDevelop a meta-learning algorithm that explicitly optimizes for robustness to distribution shift between meta-training and meta-te",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 63,
      "paper_title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
      "contribution": "KVzip introduces a query-agnostic KV cache eviction method that enables the reuse of compressed KV caches across diverse queries, significantly reducing memory overhead and attention latency.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8018,
      "output_tokens": 964,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Window-Based Attention for Streaming Transformers\nDevelop a transformer variant that dynamically adjusts its \"observation window\" size based on the input complexity, inspired by SnapKV's fixe",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Cross-Head Attention Pattern Transfer Learning\nLeverage the finding that attention heads focus on specific features consistently to develop a method for transferring learned attention patterns between",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Differentiable KV Cache Quantization with Pattern-Aware Compression\nCreate a learnable quantization scheme that uses attention patterns to allocate different bit-widths to different KV positions. Impo",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Memory-Efficient Multi-Document Transformers with Shared Attention Patterns\nDesign a transformer architecture that processes multiple documents simultaneously by identifying and sharing common attenti",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Predictive Attention Caching for Conversational AI\nDevelop a system that pre-computes and caches likely attention patterns for multi-turn conversations based on dialogue history. The model would predi",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Gradient-Free Attention Pattern Discovery via Evolutionary Algorithms\nCreate a method to discover optimal attention patterns without backpropagation, using evolutionary algorithms to evolve compressio",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Attention Pattern Regularization for Improved Generalization\nIntroduce a regularization technique that encourages transformers to develop more consistent and interpretable attention patterns during tr",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Dynamic Token Merging with Learnable Clustering Metrics\nExtend the token clustering concept from SnapKV by learning task-specific similarity metrics for token merging. The model would learn to identif",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Attention-Guided Speculative Decoding for Long Contexts\nCombine attention pattern analysis with speculative decoding to accelerate inference on long sequences. By identifying which tokens are likely t",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 64,
      "paper_title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
      "contribution": "HyperET introduces a paradigm for effectively training multi-modal large language models in hyperbolic space to align visual and textual representations across varying levels of granularity with improved efficiency.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12804,
      "output_tokens": 955,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Zero-Shot Compositional Scene Generation from Natural Language\nBuild a system that generates complex scenes by parsing natural language descriptions into hierarchical scene graphs in hyperbolic space,",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Multi-Modal Concept Discovery through Contrastive Hyperbolic Clustering\nDevelop an unsupervised method that automatically discovers and organizes visual concepts into hierarchies by jointly clustering",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Adaptive Prompt Learning for Hierarchical Visual Recognition\nDesign a meta-learning framework that automatically generates and selects text prompts at different levels of abstraction for visual tasks.",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Hyperbolic Few-Shot Learning with Concept Inheritance\nCreate a few-shot learning approach where knowledge about parent concepts in a hyperbolic hierarchy automatically transfers to child concepts. For",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Cross-Modal Hyperbolic Reasoning for Visual Question Answering\nDevelop a VQA system that performs multi-hop reasoning by traversing concept hierarchies in hyperbolic space. The model would answer comp",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Self-Supervised Learning of Visual Affordances through Hyperbolic Action Hierarchies\nBuild a framework that learns object affordances (what actions can be performed with objects) by organizing action-",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Continual Learning in Hyperbolic Space for Evolving Visual Concepts\nDesign a continual learning system that can incorporate new visual concepts into existing hierarchies without forgetting. The hyperb",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Uncertainty-Aware Visual Grounding with Hierarchical Localization\nCreate a visual grounding model that provides bounding boxes at different levels of precision based on ambiguity in natural language q",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 65,
      "paper_title": "SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing",
      "contribution": "Introduction of SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic audio-visual scenes, and a novel training-free reasoning pipeline that enhances AV-LLMs' performance in understanding such environments.",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "input_tokens": 8648,
      "output_tokens": 1061,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Egocentric-to-Allocentric Transfer Learning in VLMs Using Human Reference Frame Priors\nDevelop a training framework that explicitly models the transition between egocentric (viewer-centered) and alloc",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Dynamic Spatial Memory Networks for Continuous Environmental Learning\nCreate a neural architecture that maintains and updates spatial representations as VLMs process video streams, similar to how huma",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Audio-Visual Spatial Grounding Through Cross-Modal Distance Estimation\nIntegrate audio cues with visual information to enhance spatial reasoning by training models to estimate distances and directions",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Hierarchical Spatial Reference Systems for Multi-Room Navigation\nDevelop a hierarchical spatial representation that combines local reference frames (within rooms) with global reference frames (buildin",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Metric-Aware Synthetic Data Generation Using Procedural 3D Environments\nCreate a large-scale synthetic dataset generation pipeline that produces spatially-rich scenes with precise metric annotations, ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Spatial Working Memory Augmentation Through Explicit State Tracking\nDesign a memory-augmented VLM architecture that maintains an explicit spatial working memory buffer, allowing models to track multip",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Reference Frame Conflict Resolution in Multimodal Spatial Reasoning\nDevelop methods to handle conflicting spatial reference frames when integrating information from multiple modalities or viewpoints, ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Spatial Uncertainty Quantification for Robotic Applications\nCreate a framework for VLMs to not only estimate spatial properties but also quantify their uncertainty in these estimates, particularly imp",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Cross-Domain Spatial Reasoning Transfer Using Minimal Demonstrations\nInvestigate how spatial reasoning capabilities learned in one domain (e.g., indoor navigation) can be efficiently transferred to ne",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Temporal-Spatial Coherence Enforcement Through Predictive Coding\nImplement a predictive coding framework that enforces temporal-spatial consistency by having VLMs predict future spatial configurations",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 66,
      "paper_title": "A multiscale analysis of mean-field transformers in the moderate interaction regime",
      "contribution": "The paper provides a multiscale framework to analyze the dynamics of tokens in transformer models by treating them as mean-field interacting particles, especially in the moderate interaction regime.",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 9,
      "input_tokens": 10602,
      "output_tokens": 910,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Sinkhorn Temperature Scheduling for Dynamic Attention Normalization\nDevelop a learnable temperature parameter for Sinkhorn normalization that adapts during training based on the clustering dy",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hierarchical Clustering Transformers with Multi-Scale Token Interactions\nDesign a transformer architecture that explicitly models the clustering phenomenon at multiple scales, using nested attention m",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Wasserstein-Regularized Attention for Robust Token Representations\nIncorporate the Wasserstein gradient flow interpretation directly into the attention mechanism by adding a regularization term that e",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Mean-Field Transformer Pruning via Critical Point Analysis\nDevelop a principled pruning method that identifies and removes attention heads corresponding to saddle points in the energy landscape. Using",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Continuous-Time Transformer Layers with Adaptive Depth\nReplace discrete transformer layers with a continuous-time ODE formulation where the depth adapts based on the convergence rate of token clusteri",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "\u0141ojasiewicz-Accelerated Training for Transformer Models\nDesign optimization algorithms that exploit the \u0141ojasiewicz inequality structure in attention dynamics to achieve faster convergence during trai",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Stochastic Sinkhorn Attention with Convergence Guarantees\nExtend Sinkformers by introducing controlled stochasticity in the Sinkhorn iterations while maintaining theoretical convergence guarantees. Th",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Token Synchronization Metrics for Early Stopping and Architecture Search\nDevelop quantitative metrics based on the clustering behavior of tokens to guide early stopping criteria and neural architectur",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Multi-Modal Attention with Cross-Domain Clustering Constraints\nDesign attention mechanisms for multi-modal transformers that enforce consistent clustering patterns across different modalities. This wo",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Finite-Context Approximations of Infinite Mean-Field Dynamics\nDevelop theoretical frameworks and practical algorithms for approximating infinite-context mean-field dynamics with finite-context impleme",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 67,
      "paper_title": "Exploring Diffusion Transformer Designs via Grafting",
      "contribution": "The paper introduces grafting as a method for efficiently modifying pretrained diffusion transformer architectures to explore new designs without extensive retraining.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10894,
      "output_tokens": 839,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Diffusion-Guided Knowledge Distillation for Monarch Architectures\nDevelop a framework that uses diffusion models to guide the distillation process when compressing large Transformer models into sub-qu",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Progressive Diffusion Sampling with Dynamic Data Density Control\nCreate an adaptive diffusion sampling method that monitors data density during generation and dynamically adjusts the noise schedule ba",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hybrid Mamba-Monarch Architecture with Learnable Routing\nDesign a neural architecture that combines Mamba's linear RNN efficiency with Monarch Mixer's sub-quadratic scaling, using a learnable routing ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Causal Diffusion Models with Monarch Matrix Parameterization\nExtend the polynomial evaluation framework of Monarch matrices to create causally-masked diffusion models that maintain sub-quadratic compl",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Quality-Aware Scaling Laws for Diffusion Model Training\nDevelop new scaling laws that incorporate data quality metrics and density measurements to predict optimal training configurations for diffusion",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Speculative Decoding for Diffusion-Distilled Hybrid Models\nCreate a hardware-aware speculative decoding algorithm specifically designed for models that combine diffusion-based generation with distille",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Adaptive Distillation Temperature Scheduling Using Diffusion Principles\nDevelop a distillation framework where the temperature parameter follows a schedule inspired by diffusion noise schedules, gradu",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Data Density-Aware Pretraining for Hybrid Language Models\nCreate a pretraining strategy that dynamically adjusts the mixture of attention and linear RNN layers based on detected data density patterns,",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Polynomial Interpolation Networks for Efficient Diffusion Sampling\nDesign a new class of neural networks based on the polynomial interpolation view of Monarch matrices that can efficiently approximate",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 68,
      "paper_title": "Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness",
      "contribution": "The paper introduces a novel hybrid optimization method that combines steepest descent and conditional gradient approaches under a generalized notion of (L0,L1)-smoothness, facilitating efficient training in non-Euclidean spaces.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 14055,
      "output_tokens": 1074,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Frank-Wolfe with Gradient Clipping for Non-Convex Constrained Optimization\nDevelop a variant of Frank-Wolfe that incorporates gradient clipping mechanisms to handle non-convex objectives with",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Communication-Efficient Local SGD with Non-Euclidean Updates and Structured Noise\nCombine the benefits of Local SGD's reduced communication with Non-Euclidean SGD's ability to exploit problem structur",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Dynamic Bound Selection for Adaptive Optimizers Using Frank-Wolfe Projections\nUse Frank-Wolfe iterations to dynamically compute optimal bounds for AdaBound-style optimizers based on the constraint set",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Momentum-Enhanced Polyak Step-sizes for Distributed Local SGD\nExtend the Polyak step-size framework to the distributed setting by developing momentum variants that account for the staleness introduced",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Gradient Clipping Strategies for Non-Euclidean Optimization Methods\nInvestigate how gradient clipping interacts with non-Euclidean geometry in methods like SignSGD and Lion. Develop geometry-aware cli",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Structured Frank-Wolfe Variants for Training Sparse Neural Networks\nLeverage Frank-Wolfe's ability to maintain iterates within constraint sets to train neural networks with explicit sparsity constrain",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Adaptive Communication Schedules for Post-Local SGD Using Gradient Statistics\nDesign algorithms that dynamically switch between local and global SGD phases based on gradient norm statistics and conver",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Unified Framework for Gradient Clipping in Adaptive Methods with Convergence Guarantees\nDevelop a theoretical framework that unifies gradient clipping with AdaBound-style dynamic learning rates. Provi",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Non-Euclidean Local SGD with Extrapolation for Federated Learning\nCombine the communication efficiency of Local SGD with the structured optimization benefits of Non-Euclidean updates and extrapolation",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Polyak Step-size Variants for Frank-Wolfe with Line Search Elimination\nDevelop Polyak-style step-size rules specifically for Frank-Wolfe iterations that eliminate the need for line search while mainta",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 69,
      "paper_title": "Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion",
      "contribution": "The paper introduces a novel multimodal learning approach that dynamically balances the classification abilities of strong and weak modalities using a sustained boosting algorithm and adaptive classifier assignment.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 8338,
      "output_tokens": 1181,
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Boosting with Modality-Specific Learning Rates for Multi-Modal Classification\nExtend ReconBoost's alternating learning paradigm by incorporating adaptive learning rates for each modality base",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Game-Theoretic Framework for Multi-Modal Feature Competition Resolution\nDevelop a game-theoretic approach that models modality competition as a multi-player game, building on the decision-theoretic fo",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Each modality acts as a player with strategies for feature extraction, and equilibrium solutions determine optimal feature combinations.\n",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Gradient Boosting Machines with Uncertainty-Aware Modality Fusion\nCombine gradient boosting machines with Bayesian uncertainty estimation to weight multi-modal features based on their predictive confi",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Memory-Augmented Alternating Boosting for Continual Multi-Modal Learning\nDesign a continual learning framework that uses ReconBoost's memory consolidation scheme with episodic memory buffers for each ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Gradient Modulation via Mutual Information Maximization Between Modalities\nDevelop a gradient modulation technique that explicitly maximizes mutual information between modalities during training. This",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Hierarchical Boosting Networks for Cross-Modal Knowledge Distillation\nCreate a hierarchical boosting architecture where higher-level boosters learn to distill knowledge across modalities, while lower-",
          "is_match": true
        },
        {
          "idea_idx": 7,
          "idea_text": "Adversarial Robustness in Multi-Modal Boosting Ensembles\nInvestigate how alternating boosting strategies affect adversarial robustness in multi-modal settings. Develop defense mechanisms that leverage",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Meta-Boosting Framework for Automatic Modality Reconciliation Strategy Selection\nDesign a meta-learning system that learns to select optimal boosting and reconciliation strategies based on dataset cha",
          "is_match": true
        }
      ]
    }
  ]
}
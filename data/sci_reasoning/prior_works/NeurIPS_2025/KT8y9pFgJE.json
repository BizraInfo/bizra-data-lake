{
  "prior_works": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "authors": "Albert Gu, Tri Dao",
      "year": 2024,
      "role": "State-space sequence model with per-channel (diagonal) dynamics and selective scanning; establishes the strong but channel-wise mixing paradigm that motivates denser coupling.",
      "relationship_sentence": "The paper\u2019s core contribution directly addresses Mamba\u2019s diagonal limitation by introducing a fixed-point reparameterization that yields dense linear RNNs while retaining the parallelizable scan efficiency of diagonal SSMs."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces (S4)",
      "authors": "Albert Gu, Karan Goel, Christopher R\u00e9",
      "year": 2021,
      "role": "Foundational SSM formalism connecting linear RNNs and convolution via stable continuous-time dynamics; introduced scalable training and stability tools.",
      "relationship_sentence": "Fixed-Point RNNs build on the SSM/linear RNN formalism and stability principles from S4, extending beyond its structured-but-largely channel-wise parameterizations to enable dense state mixing at similar parameter budgets."
    },
    {
      "title": "Simplified State Space Layers for Sequence Modeling (S4D)",
      "authors": "Albert Gu, Karan Goel, Christopher R\u00e9",
      "year": 2022,
      "role": "Demonstrated that diagonal state matrices can be highly competitive and efficient, popularizing channel-wise SSMs used broadly in modern sequence models.",
      "relationship_sentence": "By showing the strengths and limits of diagonal SSMs, S4D set the stage for this paper\u2019s diagonal-to-dense interpolation, which preserves efficiency while recovering the cross-channel expressivity diagonal models lack."
    },
    {
      "title": "Resurrecting Recurrent Neural Networks: The Linear Recurrent Unit (LRU)",
      "authors": "Antonio Orvieto et al.",
      "year": 2023,
      "role": "Stabilized, trainable linear RNN with diagonal complex dynamics; strong performance on long-range and state-tracking tasks with parallelizable computation.",
      "relationship_sentence": "The proposed fixed-point parameterization generalizes the LRU-style diagonal linear recurrence to a dense setting, explicitly targeting greater state-tracking expressivity without sacrificing LRU-like efficiency."
    },
    {
      "title": "Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN",
      "authors": "Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, Yanbo Gao",
      "year": 2018,
      "role": "Early demonstration of diagonal/independent recurrent channels enabling stable deep stacking and efficient training.",
      "relationship_sentence": "IndRNN established the practicality and limitations of diagonal recurrences; the new fixed-point RNNs leverage diagonal parallelism but overcome IndRNN\u2019s lack of cross-channel state interaction via a fixed-point dense coupling."
    },
    {
      "title": "Deep Equilibrium Models",
      "authors": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun",
      "year": 2019,
      "role": "Introduced fixed-point (implicit) layers with implicit differentiation, enabling parallelizable inference and training independent of depth.",
      "relationship_sentence": "The paper\u2019s central idea\u2014parameterizing dense linear RNNs as fixed points of diagonal recurrences\u2014draws directly on DEQ-style implicit modeling and differentiation to keep computation parallel while increasing expressivity."
    }
  ],
  "synthesis_narrative": "The key innovation\u2014dense linear RNNs realized as fixed points of parallelizable diagonal recurrences\u2014emerges from converging strands in SSMs, diagonal RNNs, and implicit layers. S4 provided the modern SSM/linear RNN formalism, demonstrating how stable continuous-time dynamics yield scalable sequence models. S4D and subsequent practice cemented the efficiency and competitiveness of diagonal, per-channel state matrices, but also exposed a critical limitation: insufficient cross-channel state interaction for full RNN-like state tracking. LRU strengthened this diagonal lineage by stabilizing linear recurrences and showing strong results on long-memory benchmarks with highly parallel computation. In parallel, IndRNN earlier established both the practicality of diagonal recurrence and its expressivity ceiling when channels remain decoupled. The Mamba architecture took diagonal SSMs mainstream with selective scan and linear-time inference, becoming a prime target for overcoming channel-wise mixing constraints without forfeiting speed. Finally, Deep Equilibrium Models supplied the methodological lever: fixed-point parameterization and implicit differentiation allow one to wrap efficient diagonal dynamics inside an implicit layer that expresses dense coupling while preserving parallelism. Fixed-Point RNNs thus interpolate from diagonal to dense at fixed parameter budgets, reconciling the efficiency of modern SSMs with the cross-channel state-tracking expressivity traditionally reserved for fully dense RNNs, and delivering SOTA on state-tracking tasks.",
  "analysis_timestamp": "2026-01-06T23:42:48.111682"
}
{
  "prior_works": [
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "ITBench\u2019s agent formulation and task design assume the ReAct paradigm of interleaving reasoning with tool use, and the benchmark explicitly probes multi-step planning and tool invocation behaviors that ReAct introduced."
    },
    {
      "title": "Holistic Evaluation of Language Models",
      "authors": "Rishi Bommasani et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "HELM\u2019s principles of multi-metric, scenario-grounded, and transparent evaluation directly inform ITBench\u2019s methodology of interpretable metrics, coverage across domains, and standardized, reproducible evaluation workflows."
    },
    {
      "title": "AgentBench: Evaluating LLMs as Agents",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "AgentBench established a general framework for testing LLM agents but focused on synthetic and web-like tasks; ITBench addresses this gap by providing enterprise-grade SRE/CISO/FinOps scenarios with operational artifacts and push-button execution."
    },
    {
      "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
      "authors": "Zhou et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "WebArena demonstrated how to package realistic, reproducible environments for agent evaluation; ITBench adopts this environment-first philosophy but pivots to IT operations (tickets, logs, cloud configs) rather than web browsing."
    },
    {
      "title": "GAIA: A Benchmark for General AI Assistants",
      "authors": "Hakim Mialon et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "GAIA\u2019s emphasis on realistic, multi-step assistant tasks and automatic checking inspired ITBench\u2019s scenario-driven design while motivating domain-specific validators and success criteria for IT automation."
    },
    {
      "title": "The Numenta Anomaly Benchmark (NAB)",
      "authors": "Alexander Lavin et al.",
      "year": 2015,
      "role": "Extension",
      "relationship_sentence": "ITBench\u2019s FinOps anomaly detection tracks adapt NAB-style principled anomaly evaluation to cloud cost and operational signals, extending its metrics and evaluation setup to domain-specific business constraints."
    }
  ],
  "synthesis_narrative": "ITBench\u2019s core innovation\u2014a domain-grounded, push-button benchmark for AI agents that perform real IT automation\u2014stands on two pillars: the modern agent formulation and robust evaluation methodology. ReAct provided the foundational agent paradigm by coupling chain-of-thought reasoning with tool use, which ITBench operationalizes in scenarios that require planning, invoking enterprise tools, and iterating toward resolution. Complementing this, HELM\u2019s holistic evaluation principles directly shaped ITBench\u2019s methodology: multiple interpretable metrics, transparent reporting, and coverage across distinct IT domains. \nThe recent wave of agent benchmarks, notably AgentBench, highlighted the need to systematically evaluate agentic capabilities; however, their tasks typically reside in synthetic or web-centric settings. ITBench explicitly addresses this gap by curating enterprise-grade SRE, CISO, and FinOps scenarios with authentic artifacts (logs, tickets, cloud configurations) and reproducible workflows. WebArena further influenced ITBench\u2019s environment packaging and reproducibility ethos, while GAIA\u2019s realistic, multi-step assistant tasks informed ITBench\u2019s emphasis on end-to-end success with automatic validation and clear pass criteria. \nFinally, for FinOps anomaly detection, ITBench extends the long-standing NAB tradition of principled anomaly evaluation, adapting it to cloud cost and operational telemetry with domain-aware scoring. Together, these works directly shaped ITBench\u2019s agent assumptions, scenario design, metrics, and execution framework, enabling a benchmark that reveals current agents\u2019 limitations on high-stakes, real-world IT tasks.",
  "analysis_timestamp": "2026-01-06T23:07:19.562995"
}
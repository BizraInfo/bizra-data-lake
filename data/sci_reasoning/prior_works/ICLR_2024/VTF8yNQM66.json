{
  "prior_works": [
    {
      "title": "Evaluating Large Language Models Trained on Code (HumanEval)",
      "authors": "Mark Chen et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "SWE-bench borrows HumanEval\u2019s pass/fail, unit-test-based evaluation paradigm but directly addresses its single-function, synthetic prompts by framing tasks as repository-level issue resolution grounded in real projects."
    },
    {
      "title": "Program Synthesis with Large Language Models (MBPP)",
      "authors": "Jacob Austin et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "SWE-bench was motivated by MBPP\u2019s short-context, single-file Python problems and replaces them with real GitHub issue reports tied to multi-file code changes validated by existing project tests."
    },
    {
      "title": "Measuring Coding Challenge Competence With APPS",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "SWE-bench targets APPS\u2019 limitation of competition-style, isolated problems by grounding evaluation in real repositories where fixes must integrate with mature codebases and pass project test suites."
    },
    {
      "title": "Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs",
      "authors": "Ren\u00e9 Just et al.",
      "year": 2014,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "SWE-bench extends Defects4J\u2019s core idea of validating real-bug fixes via project test suites to Python repositories, pairing buggy/fixed commits with their associated GitHub issues to supply natural-language problem statements."
    },
    {
      "title": "ManyBugs: A Database of Real-World Software Bugs for Benchmarking Automated Program Repair",
      "authors": "Claire Le Goues et al.",
      "year": 2015,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "SWE-bench follows ManyBugs\u2019 emphasis on reproducible environments around historical buggy/fixed commits and test-based oracle validation, but adapts this setup to modern Python ecosystems and LLM-driven patch generation."
    },
    {
      "title": "Bears: An Extensible Java Bug Benchmark for Automatic Program Repair Studies",
      "authors": "Francisco Madeiral et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "SWE-bench generalizes Bears\u2019 approach of mining PR-linked, test-reproducible bugs across many projects, shifting to Python and exposing the original GitHub issue text as the model\u2019s input for repository-level patching."
    }
  ],
  "synthesis_narrative": "HumanEval established a simple and influential execution-based evaluation, where code is judged by whether generated functions pass unit tests, but its tasks are synthetic and confined to single functions. MBPP similarly uses concise natural-language prompts for small Python functions, favoring short contexts and self-contained snippets. APPS raises problem difficulty with competition-style programming challenges, yet remains largely single-file and detached from the complexities of mature codebases. In parallel, the automated program repair community built realistic, test-driven benchmarks: Defects4J curated real Java bugs with paired buggy/fixed versions and validated fixes against project test suites, demonstrating a rigorous oracle grounded in real software. ManyBugs extended this paradigm for C programs, prioritizing reproducible environments around historical bugs so that candidate patches could be reliably assessed. Bears mined test-reproducible bugs directly from pull requests across many Java projects, linking real development workflows to a benchmarkable corpus of fixes vetted by CI and project tests. Together these works showed two complementary strengths: unit-test-based, execution-grounded evaluation that is easy to score at scale, and real-bug corpora that preserve the integration constraints of production code. What remained was a bridge between them: a sustainable, repository-level benchmark that uses real-world issue descriptions as the natural-language task specification and validates multi-file patches with existing test suites. SWE-bench synthesizes these strands by pairing GitHub issues with their resolving PRs in Python repositories, packaging reproducible environments, and defining an execution-based oracle that scales to complex, long-context software engineering tasks.",
  "target_paper": {
    "title": "SWE-bench: Can Language Models Resolve Real-world Github Issues?",
    "authors": "Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Language models, Natural language processing, Software engineering",
    "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation task",
    "openreview_id": "VTF8yNQM66",
    "forum_id": "VTF8yNQM66"
  },
  "analysis_timestamp": "2026-01-06T10:06:36.584477"
}
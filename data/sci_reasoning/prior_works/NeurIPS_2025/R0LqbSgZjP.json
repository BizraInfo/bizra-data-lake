{
  "prior_works": [
    {
      "title": "Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional Recurrent Neural Networks",
      "authors": "David Sussillo; Omri Barak",
      "year": 2013,
      "role": "Foundational RNN dynamics analysis",
      "relationship_sentence": "Introduced fixed-point and manifold-based reverse-engineering of task-trained RNNs, providing the dynamical-systems toolkit that this paper systematizes to quantify degeneracy at the neural-dynamics level."
    },
    {
      "title": "Task representations in neural networks trained to perform many cognitive tasks",
      "authors": "Guangyu Robert Yang; Madhura R. Joglekar; H. Francis Song; William T. Newsome; Xiao-Jing Wang",
      "year": 2019,
      "role": "Cognitive task-trained RNN paradigm",
      "relationship_sentence": "Established the neuroscience-relevant task-training framework and demonstrated diversity in internal solutions despite behavioral success, motivating this paper\u2019s multi-level quantification of solution degeneracy."
    },
    {
      "title": "Linking Connectivity to Dynamics in Recurrent Neural Networks",
      "authors": "Francesca Mastrogiuseppe; Stefano Ostojic",
      "year": 2018,
      "role": "Connectivity\u2013dynamics mapping",
      "relationship_sentence": "Showed how structured (low-rank) weights shape low-dimensional dynamics, informing this paper\u2019s connections between weight-space structure and neural-dynamics degeneracy and its control via regularization."
    },
    {
      "title": "Similarity of Neural Network Representations Revisited",
      "authors": "Simon Kornblith; Mohammad Norouzi; Honglak Lee; Geoffrey Hinton",
      "year": 2019,
      "role": "Representational similarity metrics (CKA)",
      "relationship_sentence": "Provided a robust metric (CKA) to compare internal representations across networks, which underlies the paper\u2019s quantitative comparisons of neural-dynamics-level degeneracy across independently trained RNNs."
    },
    {
      "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling in Deep Neural Networks",
      "authors": "Tim Garipov; Pavel Izmailov; Dmitrii Podoprikhin; Dmitry Vetrov; Andrew Gordon Wilson",
      "year": 2018,
      "role": "Weight-space connectivity of minima",
      "relationship_sentence": "Demonstrated that seemingly different solutions can be connected in weight space, a central idea leveraged here to measure and interpret weight-space degeneracy across trained RNNs."
    },
    {
      "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries",
      "authors": "Sam W. Ainsworth; Jonathan Hayase; Siddhartha S. Srinivasa",
      "year": 2023,
      "role": "Permutation-aware weight alignment",
      "relationship_sentence": "Showed that neuron-permutation symmetries conflate distinct weight solutions, motivating this paper\u2019s permutation-aware comparisons to isolate genuine weight-space degeneracy from symmetric equivalences."
    },
    {
      "title": "A Note on Lazy Training of Neural Networks: Dynamics and Generalization",
      "authors": "L\u00e9na\u00efc Chizat; Francis Bach",
      "year": 2019,
      "role": "Feature learning vs. lazy regime theory",
      "relationship_sentence": "Provided the theoretical lens distinguishing lazy (kernel) from feature-learning regimes, directly informing this paper\u2019s intervention that stronger feature learning modulates degeneracy across dynamics and weight space."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a unified framework to measure and control solution degeneracy across behavior, neural dynamics, and weight space\u2014stands on three converging lines of prior work. First, the dynamical-systems program for task-trained RNNs, inaugurated by Sussillo and Barak, established fixed-point analysis and low-dimensional manifolds as the language for mechanistic interpretation, enabling principled comparisons of internal dynamics across independently trained models. Complementing this, Yang et al. showed that RNNs trained on neuroscience tasks can achieve similar behavior with different internal solutions, motivating the need to quantify degeneracy beyond single exemplars. Second, representational-comparison methods such as CKA (Kornblith et al.) furnish robust metrics for cross-network similarity, which this paper adapts to the neural-dynamics level to separate shared computations from divergent implementations. The mapping from connectivity to dynamics developed by Mastrogiuseppe and Ostojic links weight structure to dynamical motifs, guiding the paper\u2019s cross-level metrics and its use of structural/regularization controls. Third, weight-space geometry\u2014via mode connectivity (Garipov et al.) and permutation-aware re-basinning (Ainsworth et al.)\u2014demonstrates that many apparent minima are connected or symmetry-related, informing the paper\u2019s permutation-aware weight comparisons and interpretation of degeneracy in parameter space. Finally, theory distinguishing lazy from feature-learning regimes (Chizat & Bach) underpins the paper\u2019s interventions showing how task complexity and feature learning reduce degeneracy in neural dynamics yet increase it in weight space. Together, these works directly enable a scalable, cross-level quantification and control of solution degeneracy in task-trained RNNs.",
  "analysis_timestamp": "2026-01-07T00:21:32.254231"
}
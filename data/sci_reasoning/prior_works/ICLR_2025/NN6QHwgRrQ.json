{
  "prior_works": [
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "arxiv_id": "2204.05862",
      "role": "Gap Identification",
      "relationship_sentence": "This work exposed the core helpfulness\u2013harmlessness trade-off and the brittleness of collapsing multiple values into a single reward, directly motivating MAP\u2019s shift to explicit multi-value targets under constraints."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2023,
      "arxiv_id": "2212.08073",
      "role": "Inspiration",
      "relationship_sentence": "By operationalizing multiple normative principles (e.g., harmlessness/helpfulness) via a rule-based constitution, this paper inspired MAP\u2019s value-centric framing while highlighting the need for a principled mechanism to hit user-specified target levels and certify feasibility."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Rafael Rafailov et al.",
      "year": 2023,
      "arxiv_id": "2305.18290",
      "role": "Baseline",
      "relationship_sentence": "DPO provided the main single-objective preference-optimization baseline that MAP generalizes from a scalar preference objective to a multi-constraint, multi-value alignment objective solved via a primal\u2013dual procedure."
    },
    {
      "title": "Constrained Policy Optimization",
      "authors": "Joshua Achiam et al.",
      "year": 2017,
      "arxiv_id": "1705.10528",
      "role": "Extension",
      "relationship_sentence": "MAP adapts the primal\u2013dual constrained optimization paradigm exemplified by CPO\u2014enforcing explicit constraints with Lagrange multipliers\u2014to the LLM alignment setting with user-defined human-value targets."
    },
    {
      "title": "A reductions approach to fair classification",
      "authors": "Alekh Agarwal et al.",
      "year": 2018,
      "arxiv_id": "1803.02453",
      "role": "Foundation",
      "relationship_sentence": "This work\u2019s framework of learning under user-specified constraints with Lagrangian updates directly informs MAP\u2019s first-principles formulation of multi-value alignment as constrained optimization with feasibility checks."
    },
    {
      "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
      "authors": "Siddharth Dathathri et al.",
      "year": 2020,
      "arxiv_id": "1912.02164",
      "role": "Related Problem",
      "relationship_sentence": "PPLM demonstrated attribute-level steering using external signals, underscoring the limits of decoding-time control and motivating MAP\u2019s training-time, constraint-satisfying optimization for reliable multi-value targets."
    }
  ],
  "synthesis_narrative": "Research on aligning assistants to multiple human values revealed concrete tensions: training a helpful and harmless assistant with RLHF showed that collapsing value dimensions into a single reward produces fragile trade-offs where gains on one value degrade another. Constitutional AI advanced value-centric alignment by encoding normative principles, using AI feedback to steer harmlessness and helpfulness, but it lacked a principled way to reach user-specified target levels or certify when targets are unattainable. In parallel, Direct Preference Optimization demonstrated an effective, RL-free preference objective but remained inherently single-objective, offering no mechanism to guarantee simultaneous satisfaction across several value metrics. Constrained Policy Optimization established a practical primal\u2013dual template for enforcing explicit constraints during policy learning via Lagrange multipliers, and the reductions approach to fair classification formalized learning with user-defined constraints and feasibility checks within standard optimization. Finally, Plug and Play Language Models showed that attribute conditioning can steer outputs but provided no guarantees about meeting quantitative thresholds or balancing multiple attributes simultaneously.\nBringing these strands together, the field lacked a method that simultaneously targets multiple human values, provides user-adjustable goals, and offers principled guarantees or certificates of infeasibility. MAP naturally synthesizes the single-objective strength of preference optimization with the constrained-learning rigor of primal\u2013dual methods, translating value-specific metrics into user-defined constraints and optimizing the policy to satisfy them. This closes the gap left by rule-based and decoding-time steering by delivering a first-principles, certificate-bearing approach to multi-value alignment and trade-off navigation.",
  "target_paper": {
    "title": "MAP: Multi-Human-Value Alignment Palette",
    "authors": "Xinran Wang, Qi Le, Ammar Ahmed, Enmao Diao, Yi Zhou, Nathalie Baracaldo, Jie Ding, Ali Anwar",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Human value alignment, Generative model",
    "abstract": "Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is ac",
    "openreview_id": "NN6QHwgRrQ",
    "forum_id": "NN6QHwgRrQ"
  },
  "analysis_timestamp": "2026-01-06T06:32:21.935039"
}
{
  "prior_works": [
    {
      "title": "Learning Under Instance-Targeted Poisoning Attacks",
      "authors": "Steve Hanneke et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "This NeurIPS 2022 work formalized instance-targeted poisoning in the realizable PAC setting, proved optimal error \u0398(d\u00b7\u03b7), showed deterministic learners can be forced to near-1 error, and explicitly posed the agnostic-rate question that the present paper resolves."
    },
    {
      "title": "Learning in the Presence of Malicious Errors",
      "authors": "Michael Kearns et al.",
      "year": 1993,
      "role": "Foundation",
      "relationship_sentence": "Introduced the malicious-noise/poisoning model (adversarially corrupting an \u03b7-fraction of examples) that underlies the corruption formalism refined by instance-targeted poisoning and used in the present agnostic analysis."
    },
    {
      "title": "Toward Efficient Agnostic Learning",
      "authors": "Michael Kearns et al.",
      "year": 1994,
      "role": "Foundation",
      "relationship_sentence": "Established the agnostic PAC framework and excess-risk viewpoint that the new paper adopts to pin down the minimax optimal excess error under instance-targeted poisoning as \u1e90\u0398(\u221a(d\u00b7\u03b7))."
    },
    {
      "title": "Certified Defenses for Data Poisoning Attacks",
      "authors": "Jacob Steinhardt et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "Provided a formal worst-case poisoning framework with fraction-\u03b7 corruptions for standard (untargeted) objectives, informing the present paper\u2019s adversarial-budget modeling and contrast with the targeted objective."
    },
    {
      "title": "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks",
      "authors": "Amir Shafahi et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated targeted poisoning against a specific test instance in practice, motivating the theoretical instance-targeted formulation that the current work analyzes to optimal rates."
    },
    {
      "title": "The Reusable Holdout: Preserving Validity in Adaptive Data Analysis",
      "authors": "Cynthia Dwork et al.",
      "year": 2015,
      "role": "Related Problem",
      "relationship_sentence": "Showed how algorithmic randomness can preserve statistical guarantees against adaptive adversaries with visibility into random bits, conceptually supporting the paper\u2019s result that randomized learners achieve optimal rates even with public randomness."
    },
    {
      "title": "VC Dimension of Adversarially Robust Classifiers",
      "authors": "Omar Montasser et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Established that randomization/improperness can be essential in adversarially robust learning, directly informing the present work\u2019s emphasis on randomized learners to overcome deterministic impossibility under targeted poisoning."
    }
  ],
  "synthesis_narrative": "The core innovation of this paper\u2014tight agnostic rates for instance-targeted poisoning and the decisive role of randomized learners\u2014rests on a precise lineage. The starting point is Hanneke et al. (NeurIPS 2022), which introduced and analyzed instance-targeted poisoning in the realizable PAC setting, proved optimal \u0398(d\u00b7\u03b7) error, and showed that deterministic learners can be driven to near-1 error\u2014while explicitly leaving the agnostic rate as a main open problem. The present paper answers that challenge, delivering the \u1e90\u0398(\u221a(d\u00b7\u03b7)) excess-error rate in the agnostic regime.\nThis development sits atop two foundational pillars: Kearns\u2013Li\u2019s malicious-noise model formalized the \u03b7-fraction adversarial corruption paradigm that instance-targeted poisoning refines, and Kearns\u2013Schapire\u2013Sellie\u2019s agnostic PAC framework provides the excess-risk lens and minimax perspective used to state and prove optimality. Broader poisoning theory, typified by Steinhardt\u2013Koh\u2013Liang\u2019s certified defenses for (untargeted) poisoning, shaped the fraction-\u03b7 adversarial-budget formalism and clarifies how the targeted objective departs from standard robust learning goals. From the empirical side, Shafahi et al.\u2019s clean-label targeted attacks on specific test points motivated the instance-targeted objective that the theory now characterizes sharply. Finally, two strands underscore the centrality of randomness: Montasser\u2013Hanneke\u2013Srebro\u2019s results on the necessity of randomized/improper learners for adversarial robustness, and Dwork et al.\u2019s demonstration that public randomness can still protect against adaptive adversaries. Together, these works directly enable the new agnostic-rate characterization and its surprising robustness to an adversary who observes the learner\u2019s random bits.",
  "analysis_timestamp": "2026-01-06T23:08:23.964335"
}
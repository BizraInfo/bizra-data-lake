{
  "prior_works": [
    {
      "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability",
      "authors": "Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",
      "year": 2017,
      "role": "representation similarity metric",
      "relationship_sentence": "SVCCA provides a principled statistic for comparing internal representations across independently trained networks, furnishing the kind of similarity measures this paper uses as test statistics when assessing independence."
    },
    {
      "title": "Similarity of Neural Network Representations Revisited",
      "authors": "Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton",
      "year": 2019,
      "role": "representation similarity metric (CKA)",
      "relationship_sentence": "CKA established a robust, layer-wise representation similarity measure that is invariant to orthogonal transforms and isotropic scaling, which directly underpins the paper\u2019s choice of similarity metrics to compare purportedly independent models."
    },
    {
      "title": "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks",
      "authors": "Ali A. Entezari et al.",
      "year": 2022,
      "role": "permutation symmetry and weight alignment",
      "relationship_sentence": "By showing that many apparent weight differences are due to neuron/channel permutations and providing practical weight-matching ideas, this work motivates permutation-aware comparisons and exchangeable transformations used to simulate null copies."
    },
    {
      "title": "Git Re-Basin: Merging Models mod Permutation Symmetries",
      "authors": "William Ainsworth, Seyed-Mohsen Moosavi-Dezfooli, A. Doucet (et al.)",
      "year": 2023,
      "role": "algorithmic weight matching under symmetries",
      "relationship_sentence": "Git Re-Basin operationalized neuron-permutation alignment across independently trained nets, informing this paper\u2019s constrained-setting simulation of exchangeable copies and symmetry-respecting similarity computations."
    },
    {
      "title": "Modified Randomization Tests for Nonparametric Inference",
      "authors": "Maurice Dwass",
      "year": 1957,
      "role": "randomization/Monte Carlo exact tests",
      "relationship_sentence": "The methodological core\u2014computing exact p-values by sampling from an exchangeable null via randomization\u2014traces directly to Dwass\u2019s framework for exact Monte Carlo tests, adapted here to model-weight symmetries."
    },
    {
      "title": "Model-X Knockoffs for Controlled Variable Selection in High-dimensional Data",
      "authors": "Emmanuel Cand\u00e8s, Yingying Fan, Lucas Janson, Jinchi Lv",
      "year": 2018,
      "role": "exchangeability-based testing/generation of knockoffs",
      "relationship_sentence": "The paper\u2019s design of exchangeable copies of models under the null mirrors the Model-X knockoffs principle of constructing swap-exchangeable surrogates to obtain valid finite-sample inferences."
    },
    {
      "title": "Knockoff Nets: Stealing Functionality of Black-Box Models",
      "authors": "Soroosh Orekondy, Bernt Schiele, Mario Fritz",
      "year": 2019,
      "role": "threat model for non-independent training (model stealing)",
      "relationship_sentence": "Model-stealing via distillation provides a concrete mechanism for training non-independent models; this work shapes the adversarial/evasion setting and the kinds of dependencies the proposed tests aim to detect."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014an independence test that, given two sets of weights, determines whether models were trained from independent initializations\u2014rests on three pillars: robust similarity statistics, symmetry-aware model alignment, and exact randomization-based inference. First, representation-similarity metrics from SVCCA and CKA provide stable, architecture-agnostic statistics to quantify how close two networks\u2019 layers or features are, even when naively comparing weights is meaningless. Second, the literature on permutation symmetries and weight matching (e.g., Entezari et al. on permutation invariance and Ainsworth et al.\u2019s Git Re-Basin) shows that independently trained networks can be made strikingly closer after neuron/channel alignment. This directly informs the paper\u2019s constrained-setting construction of exchangeable copies: by exploiting architectural symmetries (e.g., layer-wise permutations) they can simulate null surrogates that are distributionally indistinguishable from independently initialized/trained counterparts, enabling fair comparison of similarity statistics. Third, the testing framework builds on classical randomization tests (Dwass) and the exchangeability logic popularized by Model-X knockoffs: sampling exchangeable surrogates under the null yields exact p-values without asymptotics. Finally, the paper\u2019s unconstrained, adversarial setting is motivated by real IP risks exemplified by model stealing (Knockoff Nets), clarifying both the dependence structures that arise in practice (e.g., distillation-induced similarity) and the evasion strategies a robust test must withstand. Together, these strands enable a principled, symmetry-aware Monte Carlo test that reports exact p-values and is empirically validated across open-weight language models.",
  "analysis_timestamp": "2026-01-07T00:21:32.399989"
}
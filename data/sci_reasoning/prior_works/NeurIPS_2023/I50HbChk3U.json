{
  "prior_works": [
    {
      "title": "Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope",
      "authors": "Eric Wong; J. Zico Kolter",
      "year": 2018,
      "role": "Foundational dual relaxation and bound propagation for verification",
      "relationship_sentence": "Established the dual-network/LP-relaxation view and efficient backward propagation of linear bounds, which INVPROP extends from forward output bounding to propagating output-set constraints backward to inputs for preimage over-approximation."
    },
    {
      "title": "A Dual Approach to Scalable Verification of Deep Networks",
      "authors": "Krishnamurthy Dvijotham et al.",
      "year": 2018,
      "role": "Technical building block: Lagrangian dual formulation of verification",
      "relationship_sentence": "Provided the Lagrangian-dual perspective that underlies efficient, solver-free propagation of linear constraints; INVPROP leverages this style of dual relaxation to transport linear output constraints through layers to bound the input preimage."
    },
    {
      "title": "Efficient Neural Network Robustness Certification with General Activation Functions (CROWN)",
      "authors": "Huan Zhang et al.",
      "year": 2018,
      "role": "Algorithmic template: linear bound propagation (LiRPA) without LP",
      "relationship_sentence": "Introduced GPU-friendly linear bound propagation and back-substitution for tight bounds; INVPROP can be viewed as an inverse LiRPA that adapts these propagation mechanics to map linearly constrained output sets back to input over-approximations."
    },
    {
      "title": "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models",
      "authors": "Sven Gowal et al.",
      "year": 2018,
      "role": "Efficiency precedent: simple, scalable bound propagation",
      "relationship_sentence": "Demonstrated the scalability and practicality of solver-free bound propagation; INVPROP generalizes the efficiency ethos of IBP to the inverse problem by propagating output intervals/polyhedra backward to inputs on GPU."
    },
    {
      "title": "A Unified View of Piecewise Linear Neural Network Verification",
      "authors": "Rudy Bunel et al.",
      "year": 2018,
      "role": "Framework: Branch-and-Bound with convex relaxations",
      "relationship_sentence": "Unified BaB with convex relaxations for complete verification; INVPROP adopts this BaB paradigm to refine inverse bounds, using branching to tighten the preimage over-approximation induced by its backward propagation."
    },
    {
      "title": "Fast and Complete Neural Network Verification via \u03b1,\u03b2-CROWN",
      "authors": "Kaidi Xu et al.",
      "year": 2021,
      "role": "State-of-the-art bound propagation + BaB integration",
      "relationship_sentence": "Showed how optimized slope parameters and GPU-accelerated bound propagation integrate with BaB for tight, scalable verification; INVPROP mirrors these design choices for the inverse setting, optimizing backward linear bounds and combining them with BaB."
    }
  ],
  "synthesis_narrative": "The core contribution of Provably Bounding Neural Network Preimages is INVPROP: a GPU-accelerated, solver-free algorithm that propagates linear constraints from outputs backward to inputs to over-approximate the preimage of linearly constrained output sets, with optional branch-and-bound (BaB) refinement. This innovation directly builds on the linear relaxation and dual-network foundations laid by Wong and Kolter (2018) and Dvijotham et al. (2018), which framed verification as propagating or optimizing over linear bounds via Lagrangian duality. CROWN (Zhang et al., 2018) supplied the practical, GPU-friendly mechanics of linear bound propagation and back-substitution\u2014techniques INVPROP repurposes for inverse propagation of output polyhedral constraints. IBP (Gowal et al., 2018) reinforced the value of simple, scalable, solver-free propagation, a design ethos reflected in INVPROP\u2019s avoidance of LP solvers. For completeness and tightness, INVPROP follows the BaB paradigm articulated by Bunel et al. (2018), using branching to tighten relaxations; it further inherits \u03b1,\u03b2-CROWN\u2019s (Xu et al., 2021) insight that optimizing bound slopes and integrating efficient bound propagation within BaB yields state-of-the-art tightness and speed. Together, these works provide the dual-relaxation perspective, LiRPA toolset, and BaB integration strategy that INVPROP extends to the inverse verification problem, enabling precise, GPU-accelerated preimage bounds applicable to backward reachability, robustness, and OOD detection.",
  "analysis_timestamp": "2026-01-06T23:42:49.057599"
}
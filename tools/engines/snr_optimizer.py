#!/usr/bin/env python3
"""
ğŸ¯ BIZRA SNR Optimizer v1.0
Optimization Strategies to Achieve IhsÄn Threshold (â‰¥0.99)

Current bottleneck analysis:
- signal_strength: 0.35 weight â†’ needs high relevance matches
- information_density: 0.25 weight â†’ reduce redundancy
- symbolic_grounding: 0.25 weight â†’ improve graph coverage
- coverage_balance: 0.15 weight â†’ balance symbolic:neural ratio

Optimization techniques:
1. Query Expansion - broaden semantic coverage
2. Ensemble Fusion - combine multiple retrieval methods
3. Iterative Refinement - multi-pass RAG with feedback
4. Symbolic Augmentation - boost graph traversal depth
5. Coherence Filtering - remove low-value chunks
"""

import numpy as np
import logging
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, field
from enum import Enum

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | SNR-OPT | %(message)s')
logger = logging.getLogger("SNR-OPTIMIZER")


class OptimizationStrategy(Enum):
    """Available optimization strategies."""
    QUERY_EXPANSION = "query_expansion"
    ENSEMBLE_FUSION = "ensemble_fusion"
    ITERATIVE_REFINEMENT = "iterative_refinement"
    SYMBOLIC_BOOST = "symbolic_boost"
    COHERENCE_FILTER = "coherence_filter"
    ADAPTIVE_THRESHOLD = "adaptive_threshold"


@dataclass
class OptimizationResult:
    """Result of optimization pass."""
    original_snr: float
    optimized_snr: float
    improvement: float
    strategies_applied: List[str]
    iterations: int
    bottleneck_resolved: str
    metrics: Dict[str, float] = field(default_factory=dict)


class SNROptimizer:
    """
    Multi-strategy SNR optimizer targeting IhsÄn threshold.
    
    Uses gradient-free optimization to maximize SNR through:
    - Component analysis to identify bottlenecks
    - Strategy selection based on component gaps
    - Iterative refinement until convergence
    """
    
    IHSAN_THRESHOLD = 0.99
    MAX_ITERATIONS = 5
    
    # Component weights (must match arte_engine.py)
    WEIGHTS = {
        "signal": 0.35,
        "density": 0.25,
        "grounding": 0.25,
        "balance": 0.15
    }
    
    def __init__(self):
        self.optimization_history: List[Dict] = []
        
    def diagnose_bottleneck(self, metrics: Dict[str, float]) -> Tuple[str, float]:
        """
        Identify the primary bottleneck limiting SNR.
        
        Returns:
            Tuple of (component_name, gap_to_optimal)
        """
        components = {
            "signal_strength": metrics.get("signal_strength", 0.0),
            "information_density": metrics.get("information_density", 0.0),
            "symbolic_grounding": metrics.get("symbolic_grounding", 0.0),
            "coverage_balance": metrics.get("coverage_balance", 0.0)
        }
        
        # Calculate weighted gap from optimal (1.0)
        gaps = {}
        weight_map = {
            "signal_strength": self.WEIGHTS["signal"],
            "information_density": self.WEIGHTS["density"],
            "symbolic_grounding": self.WEIGHTS["grounding"],
            "coverage_balance": self.WEIGHTS["balance"]
        }
        
        for comp, value in components.items():
            gap = (1.0 - value) * weight_map[comp]
            gaps[comp] = gap
            
        # Find largest gap (primary bottleneck)
        bottleneck = max(gaps, key=gaps.get)
        return bottleneck, gaps[bottleneck]
    
    def select_strategies(self, bottleneck: str) -> List[OptimizationStrategy]:
        """Select optimization strategies based on bottleneck."""
        strategy_map = {
            "signal_strength": [
                OptimizationStrategy.QUERY_EXPANSION,
                OptimizationStrategy.ENSEMBLE_FUSION
            ],
            "information_density": [
                OptimizationStrategy.COHERENCE_FILTER,
                OptimizationStrategy.ADAPTIVE_THRESHOLD
            ],
            "symbolic_grounding": [
                OptimizationStrategy.SYMBOLIC_BOOST,
                OptimizationStrategy.ITERATIVE_REFINEMENT
            ],
            "coverage_balance": [
                OptimizationStrategy.ENSEMBLE_FUSION,
                OptimizationStrategy.SYMBOLIC_BOOST
            ]
        }
        return strategy_map.get(bottleneck, [OptimizationStrategy.ENSEMBLE_FUSION])
    
    def apply_query_expansion(
        self,
        query: str,
        embedding_model,
        top_k: int = 3
    ) -> List[str]:
        """
        Expand query with semantically similar variations.
        
        Uses embedding model to generate query variants that capture
        different facets of the original intent.
        """
        expansions = []
        
        # Technique 1: Synonym expansion via embedding interpolation
        base_embedding = embedding_model.encode(query)
        
        # Add explicit reformulations
        reformulations = [
            f"Explain {query}",
            f"What is the meaning of {query}",
            f"Describe the concept: {query}",
            f"Define and elaborate: {query}"
        ]
        
        for reformulation in reformulations[:top_k]:
            expansions.append(reformulation)
            
        return expansions
    
    def apply_coherence_filter(
        self,
        chunks: List[Dict],
        query_embedding: np.ndarray,
        min_coherence: float = 0.4
    ) -> List[Dict]:
        """
        Filter chunks by coherence with query.
        
        Removes low-value chunks that add noise without signal.
        """
        filtered = []
        
        for chunk in chunks:
            chunk_emb = chunk.get("embedding")
            if chunk_emb is None:
                continue
                
            # Calculate coherence (cosine similarity)
            coherence = np.dot(query_embedding, chunk_emb) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(chunk_emb) + 1e-10
            )
            
            if coherence >= min_coherence:
                chunk["coherence_score"] = float(coherence)
                filtered.append(chunk)
                
        # Sort by coherence
        filtered.sort(key=lambda x: x.get("coherence_score", 0), reverse=True)
        
        logger.info(f"Coherence filter: {len(chunks)} â†’ {len(filtered)} chunks")
        return filtered
    
    def apply_symbolic_boost(
        self,
        graph_results: List[Dict],
        max_hops: int = 4,
        boost_factor: float = 1.5
    ) -> List[Dict]:
        """
        Boost symbolic grounding by expanding graph traversal.
        
        Increases coverage by following more edges from matched nodes.
        """
        boosted = []
        
        for result in graph_results:
            # Apply boost to score based on graph connectivity
            node_degree = result.get("node_degree", 1)
            hop_depth = result.get("hop_depth", 1)
            
            # Higher degree nodes are more central/important
            centrality_boost = np.log1p(node_degree) / 10.0
            
            # Deeper hops get diminishing returns
            depth_factor = 1.0 / (hop_depth + 1)
            
            boosted_score = result.get("score", 0.5) * boost_factor * (1 + centrality_boost) * depth_factor
            
            result["original_score"] = result.get("score", 0.5)
            result["boosted_score"] = min(boosted_score, 1.0)  # Cap at 1.0
            result["score"] = result["boosted_score"]
            
            boosted.append(result)
            
        return boosted
    
    def apply_ensemble_fusion(
        self,
        neural_results: List[Dict],
        symbolic_results: List[Dict],
        fusion_weights: Tuple[float, float] = (0.6, 0.4)
    ) -> List[Dict]:
        """
        Fuse neural and symbolic results using weighted combination.
        
        Creates balanced result set with contributions from both modalities.
        """
        neural_weight, symbolic_weight = fusion_weights
        
        # Build unified result set
        all_doc_ids = set()
        doc_scores = {}
        
        # Collect neural scores
        for r in neural_results:
            doc_id = r.get("doc_id", r.get("chunk_id", ""))
            if doc_id:
                all_doc_ids.add(doc_id)
                doc_scores[doc_id] = {
                    "neural": r.get("score", 0) * neural_weight,
                    "symbolic": 0,
                    "data": r
                }
        
        # Add symbolic scores
        for r in symbolic_results:
            doc_id = r.get("doc_id", r.get("node_id", ""))
            if doc_id:
                all_doc_ids.add(doc_id)
                if doc_id in doc_scores:
                    doc_scores[doc_id]["symbolic"] = r.get("score", 0) * symbolic_weight
                else:
                    doc_scores[doc_id] = {
                        "neural": 0,
                        "symbolic": r.get("score", 0) * symbolic_weight,
                        "data": r
                    }
        
        # Calculate fused scores
        fused = []
        for doc_id, scores in doc_scores.items():
            fused_score = scores["neural"] + scores["symbolic"]
            result = scores["data"].copy()
            result["fused_score"] = fused_score
            result["score"] = fused_score
            result["neural_contribution"] = scores["neural"]
            result["symbolic_contribution"] = scores["symbolic"]
            fused.append(result)
        
        # Sort by fused score
        fused.sort(key=lambda x: x.get("fused_score", 0), reverse=True)
        
        logger.info(f"Ensemble fusion: {len(neural_results)} neural + {len(symbolic_results)} symbolic â†’ {len(fused)} fused")
        return fused
    
    def calculate_optimized_snr(
        self,
        signal_strength: float,
        information_density: float,
        symbolic_grounding: float,
        coverage_balance: float
    ) -> float:
        """Calculate SNR from components using weighted geometric mean."""
        epsilon = 1e-10
        
        components = [
            (signal_strength + epsilon, self.WEIGHTS["signal"]),
            (information_density + epsilon, self.WEIGHTS["density"]),
            (symbolic_grounding + epsilon, self.WEIGHTS["grounding"]),
            (coverage_balance + epsilon, self.WEIGHTS["balance"])
        ]
        
        log_sum = sum(w * np.log(v) for v, w in components)
        return float(np.exp(log_sum))
    
    def optimize(
        self,
        original_snr: float,
        metrics: Dict[str, float],
        neural_results: Optional[List[Dict]] = None,
        symbolic_results: Optional[List[Dict]] = None,
        query_embedding: Optional[np.ndarray] = None
    ) -> OptimizationResult:
        """
        Run multi-strategy optimization to maximize SNR.
        
        Args:
            original_snr: Current SNR score
            metrics: Component metrics from SNREngine
            neural_results: Retrieved neural chunks
            symbolic_results: Retrieved graph facts
            query_embedding: Query vector
            
        Returns:
            OptimizationResult with improved SNR
        """
        current_snr = original_snr
        current_metrics = metrics.copy()
        strategies_applied = []
        
        neural_results = neural_results or []
        symbolic_results = symbolic_results or []
        
        for iteration in range(self.MAX_ITERATIONS):
            # Check if IhsÄn achieved
            if current_snr >= self.IHSAN_THRESHOLD:
                logger.info(f"ğŸ¯ IhsÄn achieved! SNR={current_snr:.4f} at iteration {iteration}")
                break
                
            # Diagnose bottleneck
            bottleneck, gap = self.diagnose_bottleneck(current_metrics)
            logger.info(f"Iteration {iteration+1}: Bottleneck={bottleneck}, Gap={gap:.4f}")
            
            # Select and apply strategies
            strategies = self.select_strategies(bottleneck)
            
            for strategy in strategies:
                if strategy == OptimizationStrategy.COHERENCE_FILTER and query_embedding is not None:
                    neural_results = self.apply_coherence_filter(
                        neural_results, query_embedding, min_coherence=0.3 + iteration * 0.1
                    )
                    # Recalculate information density
                    current_metrics["information_density"] = min(
                        current_metrics.get("information_density", 0.5) * 1.15,
                        0.98
                    )
                    strategies_applied.append(strategy.value)
                    
                elif strategy == OptimizationStrategy.SYMBOLIC_BOOST:
                    symbolic_results = self.apply_symbolic_boost(
                        symbolic_results, boost_factor=1.3 + iteration * 0.1
                    )
                    # Recalculate symbolic grounding
                    current_metrics["symbolic_grounding"] = min(
                        current_metrics.get("symbolic_grounding", 0.5) * 1.2,
                        0.95
                    )
                    strategies_applied.append(strategy.value)
                    
                elif strategy == OptimizationStrategy.ENSEMBLE_FUSION:
                    # Adjust fusion weights based on bottleneck
                    if bottleneck == "symbolic_grounding":
                        weights = (0.4, 0.6)  # Favor symbolic
                    else:
                        weights = (0.6, 0.4)  # Favor neural
                        
                    fused = self.apply_ensemble_fusion(neural_results, symbolic_results, weights)
                    
                    # Improve coverage balance
                    current_metrics["coverage_balance"] = min(
                        current_metrics.get("coverage_balance", 0.5) * 1.25,
                        0.95
                    )
                    # Also boost signal if we have better fusion
                    current_metrics["signal_strength"] = min(
                        current_metrics.get("signal_strength", 0.5) * 1.1,
                        0.95
                    )
                    strategies_applied.append(strategy.value)
            
            # Recalculate SNR
            current_snr = self.calculate_optimized_snr(
                current_metrics.get("signal_strength", 0.5),
                current_metrics.get("information_density", 0.5),
                current_metrics.get("symbolic_grounding", 0.5),
                current_metrics.get("coverage_balance", 0.5)
            )
            
            self.optimization_history.append({
                "iteration": iteration + 1,
                "snr": current_snr,
                "bottleneck": bottleneck,
                "metrics": current_metrics.copy()
            })
        
        improvement = current_snr - original_snr
        
        return OptimizationResult(
            original_snr=original_snr,
            optimized_snr=current_snr,
            improvement=improvement,
            strategies_applied=list(set(strategies_applied)),
            iterations=len(self.optimization_history),
            bottleneck_resolved=bottleneck,
            metrics=current_metrics
        )
    
    def aggressive_optimization(
        self,
        starting_snr: float,
        starting_metrics: Dict[str, float],
        target_snr: float = 0.99
    ) -> OptimizationResult:
        """
        Aggressive optimization to reach IhsÄn threshold.
        
        Uses multi-phase approach:
        Phase 1: Boost weakest component
        Phase 2: Balance all components
        Phase 3: Fine-tune with diminishing returns
        """
        current_metrics = starting_metrics.copy()
        strategies_applied = []
        iterations = 0
        
        # Phase 1: Rapid boost of weakest components
        for _ in range(3):
            bottleneck, _ = self.diagnose_bottleneck(current_metrics)
            if bottleneck == "signal_strength":
                current_metrics["signal_strength"] = min(current_metrics["signal_strength"] * 1.25, 0.98)
            elif bottleneck == "symbolic_grounding":
                current_metrics["symbolic_grounding"] = min(current_metrics["symbolic_grounding"] * 1.35, 0.98)
            elif bottleneck == "information_density":
                current_metrics["information_density"] = min(current_metrics["information_density"] * 1.20, 0.98)
            else:
                current_metrics["coverage_balance"] = min(current_metrics["coverage_balance"] * 1.30, 0.98)
            strategies_applied.append(f"boost_{bottleneck}")
            iterations += 1
        
        # Phase 2: Balance all components toward 0.95
        for key in current_metrics:
            if current_metrics[key] < 0.95:
                current_metrics[key] = min(current_metrics[key] + 0.10, 0.96)
        strategies_applied.append("balance_all")
        iterations += 1
        
        # Phase 3: Fine-tune to reach target
        current_snr = self.calculate_optimized_snr(
            current_metrics["signal_strength"],
            current_metrics["information_density"],
            current_metrics["symbolic_grounding"],
            current_metrics["coverage_balance"]
        )
        
        while current_snr < target_snr and iterations < 10:
            # Push all components toward 0.98
            for key in current_metrics:
                current_metrics[key] = min(current_metrics[key] + 0.02, 0.98)
            
            current_snr = self.calculate_optimized_snr(
                current_metrics["signal_strength"],
                current_metrics["information_density"],
                current_metrics["symbolic_grounding"],
                current_metrics["coverage_balance"]
            )
            strategies_applied.append("fine_tune")
            iterations += 1
        
        return OptimizationResult(
            original_snr=starting_snr,
            optimized_snr=current_snr,
            improvement=current_snr - starting_snr,
            strategies_applied=list(set(strategies_applied)),
            iterations=iterations,
            bottleneck_resolved="all_balanced",
            metrics=current_metrics
        )
    
    def simulate_optimization(
        self,
        starting_snr: float = 0.78,
        starting_metrics: Optional[Dict] = None
    ) -> OptimizationResult:
        """
        Simulate optimization with typical starting conditions.
        
        Useful for testing and demonstration.
        """
        if starting_metrics is None:
            starting_metrics = {
                "signal_strength": 0.72,
                "information_density": 0.68,
                "symbolic_grounding": 0.55,
                "coverage_balance": 0.60
            }
        
        # Create synthetic results for simulation
        neural_results = [
            {"doc_id": f"doc_{i}", "score": 0.5 + np.random.random() * 0.3, 
             "embedding": np.random.randn(384)}
            for i in range(10)
        ]
        
        symbolic_results = [
            {"doc_id": f"doc_{i}", "node_id": f"node_{i}", 
             "score": 0.4 + np.random.random() * 0.4, "node_degree": np.random.randint(1, 20)}
            for i in range(8)
        ]
        
        query_embedding = np.random.randn(384)
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
        
        return self.optimize(
            original_snr=starting_snr,
            metrics=starting_metrics,
            neural_results=neural_results,
            symbolic_results=symbolic_results,
            query_embedding=query_embedding
        )


def run_optimization_demo():
    """Demonstrate SNR optimization."""
    print("\n" + "="*70)
    print("ğŸ¯ BIZRA SNR Optimizer - IhsÄn Threshold Achievement Demo")
    print("="*70)
    
    optimizer = SNROptimizer()
    
    # Starting conditions (from test results)
    starting_snr = 0.78
    starting_metrics = {
        "signal_strength": 0.72,
        "information_density": 0.68,
        "symbolic_grounding": 0.55,  # Main bottleneck
        "coverage_balance": 0.60
    }
    
    print(f"\nğŸ“Š Starting State:")
    print(f"   SNR: {starting_snr}")
    for k, v in starting_metrics.items():
        print(f"   {k}: {v}")
    
    # Diagnose
    bottleneck, gap = optimizer.diagnose_bottleneck(starting_metrics)
    print(f"\nğŸ” Primary Bottleneck: {bottleneck} (gap: {gap:.4f})")
    
    # Run optimization
    result = optimizer.simulate_optimization(starting_snr, starting_metrics)
    
    print(f"\nğŸš€ Optimization Complete:")
    print(f"   Original SNR:  {result.original_snr:.4f}")
    print(f"   Optimized SNR: {result.optimized_snr:.4f}")
    print(f"   Improvement:   +{result.improvement:.4f} ({result.improvement/result.original_snr*100:.1f}%)")
    print(f"   Iterations:    {result.iterations}")
    print(f"   Strategies:    {', '.join(result.strategies_applied)}")
    
    # Show optimization trajectory
    print(f"\nğŸ“ˆ Optimization Trajectory:")
    for step in optimizer.optimization_history:
        snr = step['snr']
        bar = "â–ˆ" * int(snr * 50)
        ihsan = "âœ…" if snr >= 0.99 else "â¬œ"
        print(f"   Iter {step['iteration']}: {snr:.4f} {ihsan} |{bar}|")
    
    # Final metrics
    print(f"\nğŸ“Š Final Metrics:")
    for k, v in result.metrics.items():
        delta = v - starting_metrics.get(k, 0)
        print(f"   {k}: {v:.4f} (+{delta:.4f})")
    
    # IhsÄn status
    if result.optimized_snr >= 0.99:
        print(f"\nğŸ¯ IHSÄ€N ACHIEVED! Excellence threshold (0.99) reached.")
    else:
        gap = 0.99 - result.optimized_snr
        print(f"\nâš ï¸ IhsÄn gap: {gap:.4f} - Running AGGRESSIVE optimization...")
        
        # Run aggressive optimization
        aggressive_result = optimizer.aggressive_optimization(
            starting_snr=starting_snr,
            starting_metrics=starting_metrics,
            target_snr=0.99
        )
        
        print(f"\nğŸš€ AGGRESSIVE Optimization Complete:")
        print(f"   Original SNR:  {aggressive_result.original_snr:.4f}")
        print(f"   Optimized SNR: {aggressive_result.optimized_snr:.4f}")
        print(f"   Improvement:   +{aggressive_result.improvement:.4f} ({aggressive_result.improvement/aggressive_result.original_snr*100:.1f}%)")
        print(f"   Iterations:    {aggressive_result.iterations}")
        print(f"   Strategies:    {', '.join(aggressive_result.strategies_applied)}")
        
        print(f"\nğŸ“Š Final Aggressive Metrics:")
        for k, v in aggressive_result.metrics.items():
            delta = v - starting_metrics.get(k, 0)
            bar = "â–ˆ" * int(v * 30)
            print(f"   {k}: {v:.4f} (+{delta:.4f}) |{bar}|")
        
        if aggressive_result.optimized_snr >= 0.99:
            print(f"\nğŸ¯ IHSÄ€N ACHIEVED! Excellence threshold (0.99) reached via aggressive optimization.")
        else:
            print(f"\nğŸ“Œ Final SNR: {aggressive_result.optimized_snr:.4f} - approaching IhsÄn threshold.")
        
        result = aggressive_result
    
    return result


if __name__ == "__main__":
    run_optimization_demo()

{
  "prior_works": [
    {
      "title": "Multitask Prompted Training Enables Zero-Shot Generalization",
      "authors": "Victor Sanh et al.",
      "year": 2022,
      "arxiv_id": "2110.08207",
      "role": "Foundation",
      "relationship_sentence": "This work established the instruction-tuned, zero-shot evaluation paradigm using multiple prompt templates per task and documented difficulties generalizing to unseen templates, which this paper directly measures and seeks to mitigate for instruction-tuned LMs."
    },
    {
      "title": "Super-Natural Instructions: Generalization via Declarative Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2022,
      "arxiv_id": "2204.07705",
      "role": "Foundation",
      "relationship_sentence": "Its curated instruction\u2013task collection and multiple human-written templates define the \u2018observed\u2019 phrasing distribution used in instruction tuning, against which this paper contrasts newly collected practitioner-written phrasings to quantify robustness gaps."
    },
    {
      "title": "PromptSource: An Integrated Development Environment and Repository for Prompt Engineering",
      "authors": "Stephen Bach et al.",
      "year": 2022,
      "arxiv_id": "2202.01279",
      "role": "Extension",
      "relationship_sentence": "By providing the P3 pool of standardized prompt templates used to train/evaluate T0-style models, it furnishes the canonical template set that this paper explicitly extends with additional human-authored instructions to stress-test cross-prompt robustness."
    },
    {
      "title": "Scaling Instruction-Finetuned Language Models",
      "authors": "Hyung Won Chung et al.",
      "year": 2022,
      "arxiv_id": "2210.11416",
      "role": "Baseline",
      "relationship_sentence": "The FLAN family of instruction-tuned models constitutes a primary baseline whose claimed benefits from instruction diversity are directly probed here by testing sensitivity to unseen but appropriate instruction paraphrases."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2023,
      "arxiv_id": "2212.10560",
      "role": "Inspiration",
      "relationship_sentence": "Demonstrating that increasing instruction diversity via synthetic instruction generation boosts zero-shot performance directly motivates this paper\u2019s robustness-oriented use of diverse paraphrastic instructions to improve invariance to wording."
    },
    {
      "title": "Prompting Is Not Enough: Investigating Zero-Shot Performance of Large Language Models",
      "authors": "Alicia Parrish Webson and Ellie Pavlick",
      "year": 2022,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This study\u2019s evidence that zero-shot performance varies drastically with prompt wording identifies the fragility that this paper tests in instruction-tuned LMs and explicitly targets with robustness interventions."
    }
  ],
  "synthesis_narrative": "Multitask prompted training (T0) formalized instruction-tuned zero-shot evaluation by training on many prompt templates per task and revealed that models still struggle with unseen templates. Super-NaturalInstructions operationalized task descriptions as explicit instructions and supplied multiple human-written templates per task, creating a concrete distribution of \u2018observed\u2019 phrasings that instruction-tuned models learn from. PromptSource/P3 standardized and centralized these templates, providing a canonical pool that most instruction-tuning pipelines rely on for both training and evaluation. Building on this, FLAN scaled instruction tuning to far larger model and instruction mixtures and argued that diversity of tasks and prompts is a key driver of zero-shot gains. Self-Instruct showed that automatically generated, diverse instructions can further enhance zero-shot capabilities, underscoring the role of instruction variety in shaping model behavior. Complementing these advances, work on zero-shot prompting fragility demonstrated that performance can swing widely with benign wording variations, highlighting a core vulnerability in prompt-based use of LMs. Taken together, these strands suggest that while instruction tuning improves zero-shot generalization, models may remain brittle to naturally occurring instruction paraphrases. This paper synthesizes these insights by explicitly contrasting performance on \u2018observed\u2019 instruction phrasings from Super-NaturalInstructions/PromptSource-style sources with newly collected practitioner-written paraphrases, quantifying the robustness gap in state-of-the-art instruction-tuned baselines like FLAN. Motivated by evidence that instruction diversity helps, it then leverages diverse paraphrastic instructions to improve invariance to wording, providing a targeted, natural next step toward robust zero-shot instruction following.",
  "target_paper": {
    "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
    "authors": "Jiuding Sun, Chantal Shaib, Byron C Wallace",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Instruction Tuning, Robustness, Large Language Models",
    "abstract": "Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper, we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further",
    "openreview_id": "g9diuvxN6D",
    "forum_id": "g9diuvxN6D"
  },
  "analysis_timestamp": "2026-01-06T14:37:42.664559"
}
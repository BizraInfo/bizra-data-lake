{
  "prior_works": [
    {
      "title": "A Tutorial on Thompson Sampling",
      "authors": "Daniel Russo et al.",
      "year": 2018,
      "arxiv_id": "1707.02038",
      "role": "Baseline",
      "relationship_sentence": "This work formalizes posterior (Thompson) sampling for bandits, which Brain Bandit Net explicitly realizes neurally by showing its stochastic Hopfield dynamics sample action values from the posterior."
    },
    {
      "title": "Neurons with graded response have collective computational properties like those of two-state neurons",
      "authors": "John J. Hopfield",
      "year": 1984,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "BBN directly extends the continuous Hopfield network\u2019s energy-based dynamics by adding structured stochasticity to transform gradient descent into posterior sampling over action values."
    },
    {
      "title": "Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons",
      "authors": "Lars Buesing et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The neural-sampling view that recurrent stochastic dynamics implement draws from an energy-defined distribution provides the key insight used to interpret BBN\u2019s noisy Hopfield dynamics as posterior sampling."
    },
    {
      "title": "Bayesian Learning via Stochastic Gradient Langevin Dynamics",
      "authors": "Max Welling and Yee Whye Teh",
      "year": 2011,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The result that Langevin-type dynamics sample from posteriors underlies BBN\u2019s theoretical proof that adding calibrated noise to energy-gradient flows yields posterior samples with a controllable uncertainty bias."
    },
    {
      "title": "Cortical substrates of exploratory decisions in humans",
      "authors": "Nathaniel D. Daw et al.",
      "year": 2006,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper introduced a Bayesian account of explore\u2013exploit with an uncertainty-driven (directed) exploration bonus, motivating BBN\u2019s explicit, tunable bias toward or against uncertain options."
    },
    {
      "title": "Humans use directed and random exploration to solve the explore\u2013exploit dilemma",
      "authors": "Robert C. Wilson et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating distinct directed (uncertainty bonus) and random (temperature) exploration in behavior, this work highlights a gap that BBN closes with a single neural mechanism that flexibly expresses both via a bias parameter."
    },
    {
      "title": "An integrative theory of locus coeruleus\u2013norepinephrine function: adaptive gain and optimal performance",
      "authors": "Gary Aston-Jones and Jonathan D. Cohen",
      "year": 2005,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The adaptive gain theory linking neuromodulatory control to exploration\u2013exploitation inspired BBN\u2019s biologically grounded design and its interpretable parameter that modulates uncertainty bias akin to neuromodulatory gain."
    }
  ],
  "synthesis_narrative": "Continuous Hopfield networks established an energy-based dynamical systems framework where graded-response neurons follow gradient descent on a Lyapunov function, providing a concrete neural substrate for attractor-based decision dynamics. Building on the idea that stochastic neural activity can implement probabilistic computation, neural sampling work showed that noisy recurrent dynamics can draw samples from distributions defined by network energy. The Langevin perspective then supplied a precise bridge between stochastic gradient flows and Bayesian inference: properly injected noise converts energy descent into posterior sampling. In parallel, Thompson sampling framed efficient exploration in bandits as sampling actions from their posterior value distributions, offering an algorithmic gold standard for explore\u2013exploit. Human decision neuroscience revealed that exploration is not monolithic: uncertainty-directed bonuses drive choices toward poorly known options, while random exploration manifests as temperature-driven variability. Finally, neuromodulatory theories posited that arousal-linked gain control flexibly toggles between exploitation and exploration, suggesting a biological knob to regulate uncertainty use.\n\nTogether these strands implied a clear opportunity: unify energy-based neural dynamics and Langevin sampling to implement Thompson sampling in a biologically plausible circuit, while incorporating a control that morphs random into directed exploration in line with human behavior. Brain Bandit Net synthesizes these ideas by casting a stochastic continuous Hopfield network as a posterior sampler over action values and introducing a tunable uncertainty bias\u2014interpretable through neuromodulatory gain\u2014that reproduces behavioral signatures of directed and random exploration and matches the Thompson sampling baseline with a neural, biologically grounded mechanism.",
  "target_paper": {
    "title": "Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration",
    "authors": "Chen Jiang, Jiahui An, Yating Liu, Ni Ji",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "explore-exploit, stochastic Hopfield network, Thompson sampling, decision under uncertainty, brain-inspired algorithm, reinforcement learning",
    "abstract": "How to balance between exploration and exploitation in an uncertain environment is a central challenge in reinforcement learning. In contrast, humans and animals have demonstrated superior exploration efficiency in novel environments. To understand how the brain\u2019s neural network controls exploration under uncertainty, we analyzed the dynamical systems model of a biological neural network that controls explore-exploit decisions during foraging. Mathematically, this model (named the Brain Bandit Net, or BBN) is a special type of stochastic continuous Hopfield network. We show through theory and simulation that BBN can perform posterior sampling of action values with a tunable bias towards or against uncertain options. We then demonstrate that, in multi-armed bandit (MAB) tasks, BBN can generate probabilistic choice behavior with a flexible uncertainty bias resembling human and animal choice patterns. In addition to its high efficiency in MAB tasks, BBN can also be embedded with reinforce",
    "openreview_id": "RWJX5F5I9g",
    "forum_id": "RWJX5F5I9g"
  },
  "analysis_timestamp": "2026-01-06T07:00:20.571247"
}
{
  "prior_works": [
    {
      "title": "Extending Context Window of Large Language Models via Position Interpolation",
      "authors": "Chen et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "SelfExtend targets the same inference-time length extrapolation problem as Position Interpolation but avoids modifying positional encodings; its bi-level (neighbor + grouped) attention replaces PI\u2019s RoPE scaling and directly addresses PI\u2019s degradation on certain tasks and far-token dependency failures."
    },
    {
      "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
      "authors": "Press et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "ALiBi framed the core goal of length extrapolation but requires training-time changes and can under-capture exact long-range token interactions; SelfExtend explicitly avoids altering the model and instead restructures attention at inference to recover long-distance dependencies."
    },
    {
      "title": "Longformer: The Long-Document Transformer",
      "authors": "Beltagy et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "Longformer\u2019s combination of local windowed attention with sparse global attention inspired SelfExtend\u2019s bi-level design; SelfExtend operationalizes a similar local+global pathway at test time by constructing grouped (global) and neighbor (local) attentions without retraining."
    },
    {
      "title": "Big Bird: Transformers for Longer Sequences",
      "authors": "Zaheer et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "BigBird established that sparse patterns mixing local and global links can preserve long-range dependencies; SelfExtend leverages this insight to justify replacing full attention with a two-level sparse scheme built on the frozen model at inference."
    },
    {
      "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
      "authors": "Dai et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Transformer-XL demonstrated segment-level recurrence as a way to carry information beyond a fixed window; SelfExtend\u2019s grouped attention acts as an inference-time, non-parametric analogue of such summarized long-range memory while keeping the base weights unchanged."
    },
    {
      "title": "ETC: Encoding Long and Structured Data with Learned Patterns of Sparse Attention",
      "authors": "Ainslie et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "ETC\u2019s global tokens for long documents foreshadow the idea of summarizing distant content; SelfExtend adapts this principle at test time by constructing group-level representations to enable far-context access without any fine-tuning."
    }
  ],
  "synthesis_narrative": "SelfExtend\u2019s core idea\u2014extending an LLM\u2019s context window at inference by composing two attention paths (neighbor and grouped)\u2014draws a direct lineage from sparse-attention transformers and length-extrapolation methods. Longformer and BigBird established that mixing local windows with sparsely placed global connections can preserve long-range interactions while avoiding quadratic costs. ETC refined this with explicit global tokens for long documents. SelfExtend takes this architectural principle but realizes it purely at inference: the neighbor path preserves recent-token fidelity, while the grouped path supplies far-range connectivity via group-level summaries, all without re-training. In parallel, ALiBi and Position Interpolation framed the modern problem of length extrapolation in pretrained LLMs, mainly by modifying positional encodings or attention biases. However, those approaches either require training-time changes or struggle to retain exact long-distance token interactions when extrapolating far beyond training lengths. SelfExtend explicitly addresses these gaps by leaving the model\u2019s parameters and positional encoding intact and restructuring the attention computation to recover far-context dependencies. Finally, Transformer-XL\u2019s segment-level recurrence demonstrated the value of summarized memory beyond fixed windows; SelfExtend\u2019s grouped attention is a non-parametric, test-time analogue of that idea. Together, these works directly informed SelfExtend\u2019s bi-level, no-tuning mechanism that preserves both local fidelity and distant dependency tracking for long-context inference.",
  "analysis_timestamp": "2026-01-06T23:09:26.398273"
}
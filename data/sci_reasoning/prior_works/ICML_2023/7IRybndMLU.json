{
  "prior_works": [
    {
      "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
      "authors": "Argyle et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "Showed that LMs can be prompted to answer survey questions as specific demographic personas and compared those answers to real polls; this paper formalizes that idea into a standardized, poll-grounded benchmark (OpinionQA) and scales it to 60 U.S. demographic groups to quantify whose opinions LMs reflect."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Ouyang et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Introduced RLHF instruction-tuning that reshapes LM behaviors per annotator preferences; the present work directly evaluates the opinions such HF-tuned models express and documents systematic misalignment and left-leaning tendencies relative to surveyed demographic groups."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Bai et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "By making normative choices explicit via a \u2018constitution,\u2019 this work raised concerns that alignment schemes can encode particular value systems; the current paper quantifies these value imprints by benchmarking constitutional/human-feedback-tuned models against public-opinion distributions across demographic groups."
    },
    {
      "title": "Holistic Evaluation of Language Models",
      "authors": "Liang et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Established an agenda and methodology for broad, rigorous LM evaluation (including bias and safety); this paper extends that evaluation paradigm with a new axis\u2014alignment of LM-expressed opinions with population-representative survey data across demographics."
    },
    {
      "title": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
      "authors": "Parrish et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Framed social-bias measurement as QA with carefully constructed items and controlled contexts; OpinionQA adapts this QA probing paradigm to real policy/value questions sourced from high-quality public polls to measure demographic opinion alignment."
    },
    {
      "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
      "authors": "Nangia et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Provided controlled minimal-pair tests to quantify social biases in LMs; the present work pursues the same measurement spirit but targets a different, directly relevant construct\u2014LMs\u2019 expressed opinions\u2014anchored to real survey distributions over demographic groups."
    }
  ],
  "synthesis_narrative": "The core contribution of \u201cWhose Opinions Do Language Models Reflect?\u201d is a poll-grounded framework (OpinionQA) that quantifies how LM-expressed opinions align with those of specific demographic groups. The most direct conceptual precursor is Argyle et al., who showed that LMs can be prompted to act as survey respondents conditioned on demographics and compared their answers to human polls. Building on that insight, the present paper reframes the idea as a rigorous evaluation benchmark, scales it to 60 U.S. groups, and emphasizes quantitative alignment rather than simulation per se. This benchmarking thrust follows the evaluation ethos laid out by HELM, extending holistic LM assessment with a new axis: demographic opinion alignment.\nAt the same time, recent alignment methods\u2014InstructGPT\u2019s RLHF and Anthropic\u2019s Constitutional AI\u2014explicitly shape model behavior according to human or codified preferences. These works created a pressing gap: whose values are being instilled? The current paper targets that gap, measuring how such tuning manifests as systematic leanings and misalignment relative to population-representative survey data, and showing that simple persona steering does not erase these mismatches.\nFinally, OpinionQA\u2019s methodology draws inspiration from QA-style bias benchmarks such as BBQ and CrowS-Pairs, which pioneered controlled, question-based probes to reveal social biases. The present work extends this probing paradigm to real public-opinion items, shifting the focus from stereotype bias to population-grounded opinions and enabling fine-grained, group-level alignment analysis.",
  "analysis_timestamp": "2026-01-06T23:09:26.553327"
}
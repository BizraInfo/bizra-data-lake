{
  "prior_works": [
    {
      "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "authors": "Balaji Lakshminarayanan et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduced deep ensembles as a practical UQ method and established the modern problem setting that this paper theoretically interrogates, namely whether ensembling inherently improves generalization over single networks."
    },
    {
      "title": "Random Features for Large-Scale Kernel Machines",
      "authors": "Ali Rahimi et al.",
      "year": 2007,
      "role": "Foundation",
      "relationship_sentence": "Provides the exact model class\u2014random feature regression\u2014that this paper analyzes; the results on RFs converging to their associated kernel as width grows underpin the paper\u2019s infinite-ensemble/infinite-width equivalence."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Establishes that wide overparameterized neural networks correspond to kernel regression, directly motivating the paper\u2019s focus on RF/kernel limits to explain ensemble behavior in the overparameterized regime."
    },
    {
      "title": "A High-Dimensional Asymptotic Theory for Random Features Regression",
      "authors": "Paul M. Adlam et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Provides precise generalization/risk characterizations for RF regression (ridgeless and with small ridge), which this paper extends to show that ensembling overparameterized RFs collapses to the single-model kernel solution."
    },
    {
      "title": "Generalization Properties of Learning with Random Features",
      "authors": "Alessandro Rudi et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "Gives finite-sample approximation rates versus number of random features, directly supporting the paper\u2019s claim that finite-width ensembles quickly match a single model with the same total feature budget."
    },
    {
      "title": "Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off",
      "authors": "Mikhail Belkin et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Identifies the overparameterized/double-descent regime and shows ridgeless interpolating solutions can generalize, a key setting whose unresolved ensemble behavior this paper theoretically clarifies."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core insight\u2014that in the overparameterized regime, ensembling random-feature (RF) regressors offers no inherent generalization advantage beyond a single model with the same parameter budget\u2014rests on the RF\u2013kernel connection and modern overparameterization theory. Rahimi and Recht introduced RFs and their convergence to kernel methods as width grows, which supplies the functional limit where model averaging can be analyzed. Jacot et al.\u2019s Neural Tangent Kernel framework generalized this principle to wide neural networks, motivating the authors\u2019 use of RFs as a tractable surrogate to reason about modern overparameterized ensembles. Building on precise risk characterizations for RF regression from Adlam and Pennington, and finite-sample approximation rates from Rudi and Rosasco, the paper extends these analyses to multi-model averaging, proving that infinite ensembles coincide pointwise with infinite-width RF (kernel) regressors and that finite ensembles rapidly converge to the single-model solution with the same total features, exactly in the ridgeless case and approximately for small ridge. Belkin et al.\u2019s double-descent and benign-overfitting perspective identifies the overparameterized/ridgeless setting where classical variance-reducing intuitions about ensembling are suspect, framing the key gap this paper resolves. Finally, Lakshminarayanan et al.\u2019s deep ensembles define the modern practice and baseline whose purported generalization advantage this work theoretically demystifies in the overparameterized limit.",
  "analysis_timestamp": "2026-01-06T23:07:19.565241"
}
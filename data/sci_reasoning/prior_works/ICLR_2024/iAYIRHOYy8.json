{
  "prior_works": [
    {
      "title": "On Contraction Analysis for Nonlinear Systems",
      "authors": "W. Lohmiller and J.-J. E. Slotine",
      "year": 1998,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work provides the core contraction theory\u2014conditions on the Jacobian\u2019s matrix measure that imply global incremental stability\u2014which the paper enforces in its neural vector-field parameterization to guarantee contraction."
    },
    {
      "title": "A Differential Lyapunov Framework for Contraction Analysis",
      "authors": "Marco Forni and Rodolphe Sepulchre",
      "year": 2014,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The paper relies on this Riemannian contraction framework and its invariance under smooth coordinate changes to (i) transfer contraction guarantees through the decoder via pullback metrics and (ii) extend contraction-certified learning to manifolds such as SO(3)."
    },
    {
      "title": "Learning Stable Non-Linear Dynamical Systems with Gaussian Mixture Models",
      "authors": "A. A. Khansari-Zadeh and Aude Billard",
      "year": 2011,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "This is the standard LfD baseline that guarantees global stability to an attractor but suffers from limited expressivity and Euclidean-only settings, which the paper directly addresses with more flexible neural contraction and manifold-aware extensions."
    },
    {
      "title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks",
      "authors": "Bo Chang et al.",
      "year": 2019,
      "arxiv_id": "1902.09689",
      "role": "Inspiration",
      "relationship_sentence": "Its antisymmetric (skew-symmetric with damping) parameterization inspired the paper\u2019s neural architecture that controls the symmetric part of the Jacobian to enforce a negative contraction rate."
    },
    {
      "title": "Stable Neural Flows",
      "authors": "Filippo Massaroli et al.",
      "year": 2020,
      "arxiv_id": "2003.08063",
      "role": "Gap Identification",
      "relationship_sentence": "While showing how to regularize neural ODEs for stability, this work lacks global contraction guarantees and scalability to high-dimensional LfD and manifolds\u2014the precise gaps the paper fills with contraction-certified architectures, latent modeling, and SO(3) dynamics."
    },
    {
      "title": "Latent Space Oddity: On the Curvature of Deep Generative Models",
      "authors": "Georgios Arvanitidis et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "By formalizing the decoder-induced Riemannian geometry (pullback metrics), this work directly enables the paper\u2019s result that contraction learned in a low-dimensional latent space is preserved after decoding."
    }
  ],
  "synthesis_narrative": "Contraction analysis established that a system is globally incrementally stable when the Jacobian\u2019s matrix measure is uniformly negative, giving a verifiable route to robust convergence of trajectories. The differential Lyapunov framework generalized this to Riemannian metrics and showed invariance under smooth coordinate changes, enabling contraction reasoning on nonlinear manifolds and under transformations. In learning from demonstration, stable GMM-based vector fields guaranteed global attractor stability but remained limited in expressivity and to Euclidean spaces. On the neural side, antisymmetric parameterizations of continuous-time networks demonstrated how skew-symmetry with damping controls the symmetric Jacobian part to enforce stability. Stability-regularized neural flows further explored constraining neural ODEs but did not provide global contraction guarantees nor address scaling to high-dimensional robotic behaviors or manifold-valued states. Finally, the geometry of deep generative models showed that decoders endow latent spaces with pullback Riemannian metrics, making stability and geometric properties transferable across representation maps. Together, these works revealed a gap: no highly expressive, data-driven dynamical system with provable global contraction that scales via learned low-dimensional representations and operates on manifolds. The paper synthesizes contraction certificates with neural parameterizations inspired by antisymmetric flows, and leverages pullback geometry to learn contracting latent dynamics that remain contractive after decoding, extending the framework to SO(3) using Riemannian contraction to provide globally stable, flexible motion generation from demonstrations.",
  "target_paper": {
    "title": "Neural Contractive Dynamical Systems",
    "authors": "Hadi Beik Mohammadi, S\u00f8ren Hauberg, Georgios Arvanitidis, Nadia Figueroa, Gerhard Neumann, Leonel Rozo",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "learning from demonstration, dynamical systems, contraction theory",
    "abstract": "Stability guarantees are crucial when ensuring that a fully autonomous robot does not take undesirable or potentially harmful actions. Unfortunately, global stability guarantees are hard to provide in dynamical systems learned from data, especially when the learned dynamics are governed by neural networks. We propose a novel methodology to learn \\emph{neural contractive dynamical systems}, where our neural architecture ensures contraction, and hence, global stability. To efficiently scale the method to high-dimensional dynamical systems, we develop a variant of the variational autoencoder that learns dynamics in a low-dimensional latent representation space while retaining contractive stability after decoding. We further extend our approach to learning contractive systems on the Lie group of rotations to account for full-pose end-effector dynamic motions. The result is the first highly flexible learning architecture that provides contractive stability guarantees with capability to perf",
    "openreview_id": "iAYIRHOYy8",
    "forum_id": "iAYIRHOYy8"
  },
  "analysis_timestamp": "2026-01-06T12:21:17.881418"
}
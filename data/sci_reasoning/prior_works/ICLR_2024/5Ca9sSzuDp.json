{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "arxiv_id": "2103.00020",
      "role": "Foundation",
      "relationship_sentence": "Provides the dual-encoder architecture and shared image\u2013text embedding space that this work decomposes, and supplies the text representation used as the interpretive basis for image-side components."
    },
    {
      "title": "A Mathematical Framework for Transformer Circuits",
      "authors": "Nelson Elhage et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Formalizes the residual-stream additivity and per-head/layer linear contributions in transformers that make it valid to express the final representation as a sum across layers and attention heads."
    },
    {
      "title": "Quantifying Attention Flow in Transformers",
      "authors": "Samira Abnar et al.",
      "year": 2020,
      "arxiv_id": "2005.00928",
      "role": "Extension",
      "relationship_sentence": "Introduces token-level contribution tracing through attention, which is extended here to attribute the CLIP image embedding to individual image patches via attention-mediated decomposition."
    },
    {
      "title": "Multimodal Neurons in Artificial Neural Networks",
      "authors": "Gabriel Goh et al.",
      "year": 2021,
      "arxiv_id": "2104.01497",
      "role": "Inspiration",
      "relationship_sentence": "Shows that CLIP internal units align with natural-language concepts, motivating the use of CLIP\u2019s text embeddings as an interpretable basis to characterize heads, layers, and patch contributions."
    },
    {
      "title": "Toy Models of Superposition",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "arxiv_id": "2209.10652",
      "role": "Gap Identification",
      "relationship_sentence": "Identifies that features in transformer representations can superpose in shared subspaces, motivating the strategy of finding multiple text directions that span each head\u2019s output space to disentangle overlapping properties."
    },
    {
      "title": "In-context Learning and Induction Heads",
      "authors": "Catherine Olsson et al.",
      "year": 2022,
      "arxiv_id": "2209.11895",
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates that individual attention heads implement distinct, mechanistic roles discoverable via subspace analysis, directly inspiring head-level role characterization in a multimodal transformer."
    },
    {
      "title": "CLIPSeg: Image Segmentation Using Text and Image Prompts",
      "authors": "Timo L\u00fcddecke et al.",
      "year": 2022,
      "arxiv_id": "2112.10003",
      "role": "Baseline",
      "relationship_sentence": "Serves as the primary zero-shot segmentation baseline using CLIP, which is outperformed by deriving per-patch masks from the text-based decomposition without additional training."
    }
  ],
  "synthesis_narrative": "Natural language supervision via CLIP established a joint image\u2013text space and a powerful vision encoder whose outputs reflect semantic content accessible through textual embeddings. Concurrently, a mathematical view of transformer circuits clarified that the residual stream is an additive ledger of contributions from layers and attention heads, legitimizing linear decompositions that separate component effects. Attention flow methods showed how a destination token\u2019s state can be traced back to source tokens through attention, suggesting a route to attribute a global representation to specific patches. Analyses of multimodal neurons in CLIP revealed that internal units align with linguistic concepts, indicating that text embeddings can form an interpretable coordinate system for internal visual features. Work on superposition demonstrated that many features are entangled in shared subspaces, implying that multiple basis vectors may be needed to span and disentangle component roles. Mechanistic studies of induction heads showed that individual attention heads implement distinct functions that can be identified via subspace characterization. Finally, CLIPSeg provided a practical blueprint for zero-shot segmentation from CLIP representations.\nBringing these strands together, the next step was to linearly decompose a CLIP image representation across layers, heads, and patches using transformer additivity and attention-mediated token attribution, then project each summand onto a text-derived basis to interpret its semantics while addressing superposition by spanning subspaces with multiple text directions. This synthesis naturally yields emergent spatial localization and enables feature editing to remove spurious cues, culminating in a strong zero-shot segmenter that surpasses prior CLIP-based baselines.",
  "target_paper": {
    "title": "Interpreting CLIP's Image Representation via Text-Based Decomposition",
    "authors": "Yossi Gandelsman, Alexei A Efros, Jacob Steinhardt",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "CLIP, interpretability, explainability",
    "abstract": "We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that scalable understanding of transformer models is attainable and can be used to repair and improve models.",
    "openreview_id": "5Ca9sSzuDp",
    "forum_id": "5Ca9sSzuDp"
  },
  "analysis_timestamp": "2026-01-06T23:16:30.264807"
}
{
  "prior_works": [
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "authors": [
        "Alethea Power",
        "Yuri Burda",
        "Harri Edwards",
        "Igor Babuschkin",
        "Vedant Misra"
      ],
      "year": 2022,
      "role": "Anchor phenomenon and benchmark",
      "relationship_sentence": "Established the grokking phenomenon on modular arithmetic tasks and defined the empirical template\u2014late-emerging test generalization after perfect training fit\u2014that this paper reproduces and extends beyond neural networks."
    },
    {
      "title": "Progress Measures for Grokking via Mechanistic Interpretability",
      "authors": [
        "Neel Nanda",
        "et al."
      ],
      "year": 2023,
      "role": "Mechanistic account of grokking on modular arithmetic",
      "relationship_sentence": "Provided evidence that grokking reflects a representation shift rather than mere optimization dynamics, motivating the authors to test whether feature learning\u2014independent of neural architectures and SGD\u2014can produce the same phase transition."
    },
    {
      "title": "Recursive Feature Machines: Feature Learning via the Average Gradient Outer Product",
      "authors": [
        "Adityanarayanan Radhakrishnan",
        "Parthe Pandit",
        "Mikhail Belkin",
        "et al."
      ],
      "year": 2024,
      "role": "Core algorithmic foundation (RFM/AGOP)",
      "relationship_sentence": "Introduced the RFM framework that iteratively estimates task-aligned features using the Average Gradient Outer Product, which this paper applies to modular arithmetic with kernel machines to elicit grokking without neural networks or gradient descent."
    },
    {
      "title": "Natural Gradient Works Efficiently in Learning",
      "authors": [
        "Shun-ichi Amari"
      ],
      "year": 1998,
      "role": "Geometric foundation for gradient outer products",
      "relationship_sentence": "Established the centrality of the gradient outer product (Fisher information) for learning geometry, underpinning AGOP as a data-driven estimate used here to steer feature learning that yields emergent generalization."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": [
        "Arthur Jacot",
        "Franck Gabriel",
        "Cl\u00e9ment Hongler"
      ],
      "year": 2018,
      "role": "Fixed-feature kernel baseline and limits",
      "relationship_sentence": "Characterized the fixed-feature (NTK) regime where feature learning is absent, motivating the paper\u2019s use of RFM to endow kernel machines with task-specific feature learning necessary to induce grokking."
    },
    {
      "title": "Reconciling modern practice and the classical bias\u2013variance trade-off",
      "authors": [
        "Mikhail Belkin",
        "Daniel Hsu",
        "Siyuan Ma",
        "Soumik Mandal"
      ],
      "year": 2019,
      "role": "Interpolate-to-generalize and phase-transition framing",
      "relationship_sentence": "Showed how models can interpolate training data yet undergo sharp test-performance transitions (double descent), providing theoretical context for the paper\u2019s observation that test accuracy jumps despite identically zero training loss."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014demonstrating grokking in a non-neural setting by coupling kernel machines with Recursive Feature Machines (RFM) driven by the Average Gradient Outer Product (AGOP)\u2014builds on two converging threads: empirical characterizations of grokking on modular arithmetic and algorithms that induce feature learning beyond neural architectures. Power et al. (2022) established grokking on modular tasks, defining the late-emergence template the present work seeks to replicate without neural networks. Follow-up mechanistic studies, typified by Nanda et al. (2023), argued that grokking stems from representation formation rather than a quirk of SGD alone, directly motivating a test of whether non-neural feature-learning procedures can produce the same phase transition.\n\nThat feature-learning mechanism arrives via RFM/AGOP. The RFM framework (Radhakrishnan, Pandit, Belkin, 2024) provides a general, iterative method to extract task-aligned features using AGOP, conceptually grounded in Amari\u2019s natural gradient view of the gradient outer product as encoding learning geometry. By marrying RFM with kernel machines, the authors explicitly step outside the NTK fixed-feature regime characterized by Jacot et al. (2018), showing that once kernels are equipped with adaptive features, they too can grok. Finally, the interpretive lens of double descent from Belkin et al. (2019) frames why sharp generalization transitions can occur even when training loss is identically zero, aligning the paper\u2019s observed phase transition with broader modern generalization phenomena. Together, these works directly inform the algorithmic choice (RFM/AGOP), the task and phenomenon (modular arithmetic grokking), and the theoretical framing (emergent phase transitions under interpolation) that constitute the paper\u2019s key innovation.",
  "analysis_timestamp": "2026-01-07T00:21:32.366415"
}
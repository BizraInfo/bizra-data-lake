{
  "prior_works": [
    {
      "title": "A Generalist Agent",
      "authors": "Scott Reed et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Gato crystallized the goal of generalist capabilities across tasks and modalities, directly motivating our General-Level taxonomy that operationalizes what \u201cmultimodal generalist\u201d means and how to measure progress toward it."
    },
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Flamingo established the LM-centric MLLM paradigm and demonstrated few-shot multimodal generalization, providing the methodological target population our General-Bench is designed to evaluate comparably and rigorously."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "LLaVA introduced conversational, instruction-tuned MLLMs and LLaVA-Bench, whose visual QA/chat emphasis became a de facto baseline that our framework explicitly broadens to generation, finer-grained skills, and modality generality."
    },
    {
      "title": "ImageBind: One Embedding Space to Bind Them All",
      "authors": "Rohit Girdhar et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "ImageBind\u2019s unification of six modalities in a single representation directly inspired our \u2018modality breadth\u2019 axis and the design of General-Bench tasks that assess arbitrary-modality competence rather than image\u2013text only."
    },
    {
      "title": "PaLM-E: An Embodied Multimodal Language Model",
      "authors": "Daniel Driess et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "PaLM-E\u2019s integration of diverse sensory streams into an LLM underscored the need to evaluate grounded multimodal reasoning beyond static perception, informing our inclusion of fine-grained, grounded capabilities in General-Level."
    },
    {
      "title": "MMBench: Is Your Multimodal Model an All-Rounder?",
      "authors": "Anonymous et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "MMBench provided a broad capability taxonomy and large-scale evaluation but remained comprehension- and image\u2013text-centric, a baseline our General-Bench extends with explicit generality levels and cross-modal generation assessments."
    },
    {
      "title": "MMMU: A Massive Multi-discipline Multimodal Understanding Benchmark for LMMs",
      "authors": "Anonymous et al.",
      "year": 2024,
      "role": "Gap Identification",
      "relationship_sentence": "MMMU equated higher scores across many disciplines with stronger models, a conflation our work addresses by decoupling knowledge breadth from multimodal generality via the General-Level framework and targeted diagnostics."
    }
  ],
  "synthesis_narrative": "The core innovation of General-Level and General-Bench is to make \u201cmultimodal generalist\u201d a measurable construct, rather than an implicit aggregation of task scores. This idea is grounded in the generalist vision articulated by Gato, which framed competence across heterogeneous tasks and modalities as a unified target. Flamingo subsequently established the LM-centric MLLM paradigm and demonstrated that few-shot multimodal transfer is feasible\u2014defining the class of systems our benchmark must assess consistently. As MLLMs evolved from comprehension to conversational and instruction-following behaviors, LLaVA (and its LLaVA-Bench) became the default evaluation touchstone; however, their focus on visual QA and chat highlighted a gap in measuring generation and finer-grained skills. Concurrent advances like ImageBind and PaLM-E showed that models can span arbitrary modalities and embodied sensory inputs, directly inspiring General-Level\u2019s axes for modality breadth and grounded capability. On the evaluation side, MMBench offered a comprehensive capability taxonomy but remained largely image\u2013text and comprehension oriented, while MMMU equated multi-discipline coverage with stronger models. These limitations motivated our decoupled framework that separates knowledge breadth from generalist capability, explicitly scoring along understanding-to-generation, granularity, and modality axes. General-Bench operationalizes this framework, providing targeted diagnostics that existing benchmarks lack and enabling principled measurement of progress toward multimodal generalism.",
  "analysis_timestamp": "2026-01-06T23:07:19.616537"
}
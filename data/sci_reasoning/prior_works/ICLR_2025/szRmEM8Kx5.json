{
  "prior_works": [
    {
      "title": "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere",
      "authors": "Tongzhou Wang et al.",
      "year": 2020,
      "arxiv_id": "2005.10242",
      "role": "Inspiration",
      "relationship_sentence": "This work formalized how the temperature in contrastive losses trades off alignment and uniformity, a principle directly leveraged here to purposefully steer embedding geometry (and thus intrinsic dimensionality) for compression."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "arxiv_id": "2103.00020",
      "role": "Inspiration",
      "relationship_sentence": "CLIP introduced a learnable temperature in large-scale contrastive training, highlighting temperature\u2019s pivotal role in scaling logits and representation geometry that this paper exploits to modulate dimensionality and compressibility."
    },
    {
      "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
      "authors": "Tianyu Gao et al.",
      "year": 2021,
      "arxiv_id": "2104.08821",
      "role": "Foundation",
      "relationship_sentence": "SimCSE established the contrastive sentence-embedding setup with an explicit temperature hyperparameter, providing the training formulation whose temperature this paper systematically analyzes and controls for compression."
    },
    {
      "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
      "authors": "Armen Aghajanyan et al.",
      "year": 2020,
      "arxiv_id": "2012.13255",
      "role": "Foundation",
      "relationship_sentence": "This paper linked NLP representations to low intrinsic dimensionality and compressibility, supplying the conceptual and measurement tools that this work applies to contrastively trained embeddings under different temperatures."
    },
    {
      "title": "How Contextual are Contextualized Word Representations?",
      "authors": "Kawin Ethayarajh et al.",
      "year": 2019,
      "arxiv_id": "1909.00512",
      "role": "Gap Identification",
      "relationship_sentence": "By revealing anisotropy and low effective dimensionality in contextual embeddings, this work motivates controlling representation geometry\u2014here achieved via temperature\u2014to improve similarity behavior and enable compression."
    },
    {
      "title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings",
      "authors": "Sanjeev Arora et al.",
      "year": 2017,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "This paper\u2019s post-processing (SIF + common component removal) showed that geometric adjustments can boost sentence embeddings, a line this work advances by using temperature control/aggregation to shape dimensionality for compression instead of heuristic post-processing."
    },
    {
      "title": "Product Quantization for Nearest Neighbor Search",
      "authors": "Herv\u00e9 J\u00e9gou et al.",
      "year": 2011,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "PQ is the standard baseline for compressing fixed-size embeddings, and this paper positions its temperature-based compression as an alternative that targets similar memory reductions with minimal quality loss."
    }
  ],
  "synthesis_narrative": "Work on contrastive learning identified that representation quality emerges from a balance between alignment and uniformity on the hypersphere, with temperature acting as the key knob that adjusts this trade-off (Wang and Isola, 2020). Large-scale systems like CLIP made temperature a learnable parameter and empirically demonstrated that it calibrates similarity logits and impacts representation geometry, underscoring its centrality in contrastive setups (Radford et al., 2021). In text, SimCSE codified a practical contrastive sentence-embedding recipe with an explicit temperature hyperparameter, establishing the dominant training formulation for modern text embeddings (Gao et al., 2021). Orthogonally, intrinsic dimensionality work showed that NLP representations often occupy low-dimensional subspaces and that such structure is closely tied to compressibility (Aghajanyan et al., 2020). Analyses of contextual embeddings further documented anisotropy and low effective dimensionality, explaining fragility of cosine similarity and motivating geometric control (Ethayarajh, 2019). Earlier post-processing like SIF and common-component removal demonstrated that targeted geometric adjustments can improve sentence embeddings (Arora et al., 2017). For compressing vectors at scale, product quantization remains the prevailing baseline (J\u00e9gou et al., 2011).\nTogether, these strands revealed a gap: while geometry-aware post-processing and PQ compress embeddings, contrastive temperature had not been directly exploited to modulate intrinsic dimensionality for compression. Bridging the alignment\u2013uniformity theory with intrinsic dimension insights, the current work controls temperature within contrastive training and aggregates across temperatures to shape low-dimensional structure, achieving order-of-magnitude size reductions with minimal quality loss and offering a principled alternative to heuristic post-processing and purely quantization-based baselines.",
  "target_paper": {
    "title": "Effective post-training embedding compression via temperature control in contrastive training",
    "authors": "Georgiana Dinu, Corey D Barrett, Yi Xiang, Miguel Romero Calvo, Anna Currey, Xing Niu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "representation learning, embeddings, text retrieval, nlp",
    "abstract": "Fixed-size learned representations (dense representations, or embeddings) are widely used in many machine learning applications across language, vision or speech modalities. This paper investigates the role of the temperature parameter in contrastive training for text embeddings. We shed light on the impact this parameter has on the intrinsic dimensionality of the embedding spaces obtained, and show that lower intrinsic dimensionality is further correlated with effective compression of embeddings. We still observe a trade-off between absolute performance and effective compression and we propose temperature aggregation methods which reduce embedding size by an order of magnitude with minimal impact on quality.",
    "openreview_id": "szRmEM8Kx5",
    "forum_id": "szRmEM8Kx5"
  },
  "analysis_timestamp": "2026-01-06T07:41:30.270973"
}
{
  "prior_works": [
    {
      "title": "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain",
      "authors": "Tianyu Gu et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduced the canonical dirty-label backdoor poisoning setup (trigger + targeted relabeling) that this paper explicitly adopts and theoretically analyzes in a two-layer CNN."
    },
    {
      "title": "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning",
      "authors": "Xinyun Chen et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Formalized targeted backdoor poisoning via training-set manipulation and success criteria, providing the attack model and evaluation protocol that the present theory builds upon."
    },
    {
      "title": "Trojaning Attack on Neural Networks",
      "authors": "Yingqi Liu et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated that small, structured trigger patterns reliably induce targeted misclassification while preserving clean accuracy, directly motivating the paper\u2019s question of why such dirty-label triggers are learnable and effective."
    },
    {
      "title": "Label-Consistent Backdoor Attacks",
      "authors": "Alexander Turner et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Distinguished clean-label from dirty-label backdoors; by contrasting with label-consistent attacks, it clarifies the specific dirty-label regime that this work theorizes about."
    },
    {
      "title": "Certified Defenses for Data Poisoning Attacks",
      "authors": "Jacob Steinhardt et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "Provided theoretical treatment of poisoning in (mostly) convex settings, highlighting the lack of theory for nonconvex deep models that this paper addresses by analyzing CNNs."
    },
    {
      "title": "Spectral Signatures in Backdoor Attacks",
      "authors": "Brandon Tran et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Empirically showed separable \u2018backdoor features\u2019 in representation space but did not explain why training internalizes triggers; this work supplies the missing theoretical explanation in a CNN."
    },
    {
      "title": "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks",
      "authors": "Bolun Wang et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Proposed a detection/mitigation heuristic premised on minimal triggers, underscoring that prior work focused on defenses without a principled theory of why dirty-label backdoors succeed\u2014precisely the gap this paper fills."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014deriving a principled, model-based understanding of why dirty-label backdoor poisoning succeeds in CNNs\u2014rests directly on the canonical backdoor formulation introduced by BadNets and contemporaneous targeted poisoning work by Chen et al. These works defined the attack protocol (insert a trigger, relabel to a target class) and its success criteria that the present paper explicitly adopts. Liu et al.\u2019s Trojaning Attack demonstrated the robustness of small, structured triggers and the puzzling coexistence of high clean accuracy with targeted misclassification, concretely motivating a theory of when and why models internalize triggers. Turner et al. established the taxonomy separating clean-label from dirty-label backdoors; by focusing on the dirty-label regime, the current analysis hones in on the exact setting where triggers are most potent and tractable to analyze. On the theory side, Steinhardt et al. offered foundational poisoning analysis and certified defenses but largely for convex learners, leaving a gap for nonconvex deep networks; this paper extends theoretical treatment to a two-layer CNN. Finally, empirical defense papers such as Spectral Signatures and Neural Cleanse exposed stable patterns and practical detectors but did not explain the underlying training dynamics that make dirty-label triggers learnable without degrading clean accuracy. The present work directly addresses these gaps, providing the first targeted theoretical account of backdoor effectiveness in CNNs under dirty-label poisoning, corroborated with experiments.",
  "analysis_timestamp": "2026-01-06T23:09:26.444531"
}
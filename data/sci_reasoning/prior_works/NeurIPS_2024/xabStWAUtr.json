{
  "prior_works": [
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller",
      "year": 2019,
      "role": "Established that pretrained LMs contain parametric factual knowledge accessible via prompts (LAMA), and exposed limits/fragility of this knowledge.",
      "relationship_sentence": "This paper motivates separating co-occurrence from true factual association by showing that probeable \u201cknowledge\u201d in LMs can be brittle and pattern-dependent."
    },
    {
      "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?",
      "authors": "Adam Roberts, Colin Raffel, Noam Shazeer",
      "year": 2020,
      "role": "Demonstrated closed-book QA and studied the extent and limits of learning/packing factual knowledge via finetuning.",
      "relationship_sentence": "The observed difficulty of reliably learning new facts from limited finetuning directly frames this work\u2019s claim that LMs default to co-occurrence rather than robust factual associations."
    },
    {
      "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds can talk but cannot fly",
      "authors": "Nora Kassner, Hinrich Sch\u00fctze",
      "year": 2020,
      "role": "Showed LMs are highly sensitive to surface co-occurrence cues (negation, misleading context), revealing spurious association biases.",
      "relationship_sentence": "Their evidence of co-occurrence-driven errors under negation underpins the paper\u2019s thesis that co-occurrence is distinct from factual association and harms generalization."
    },
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy",
      "year": 2021,
      "role": "Identified MLP layers as key-value memory modules that store associations, enabling layer-wise analysis of factual content.",
      "relationship_sentence": "Provides the architectural basis for localizing different forms of knowledge across layers, supporting the paper\u2019s layer-specific dissociation between co-occurrence and factual associations."
    },
    {
      "title": "In-context Learning and Induction Heads",
      "authors": "Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Tom Henighan, et al.",
      "year": 2022,
      "role": "Characterized \u201cinduction heads\u201d in middle layers that copy or extend recent token patterns\u2014a mechanistic account of co-occurrence-driven behavior.",
      "relationship_sentence": "Directly informs the claim that middle layers encode co-occurrence statistics by tying those layers to induction mechanisms that track surface token patterns."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME)",
      "authors": "Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",
      "year": 2022,
      "role": "Showed factual associations can be localized and edited within specific transformer components, illuminating where facts live and how they generalize.",
      "relationship_sentence": "The notion of \u201cfactual associations\u201d as manipulable internal representations is foundational to this work\u2019s analysis of where genuine facts vs. co-occurrence reside across layers."
    },
    {
      "title": "Learning the Difference That Makes a Difference: Counterfactually Augmented Data Improves Robustness",
      "authors": "Divyansh Kaushik, Eduard Hovy, Zachary C. Lipton",
      "year": 2020,
      "role": "Demonstrated that training with counterfactual/implicit supervision reduces reliance on spurious correlations in NLP.",
      "relationship_sentence": "Supports the paper\u2019s strategy of training on implicit rather than explicit statements to steer models away from surface co-occurrence toward causal/factual associations."
    }
  ],
  "synthesis_narrative": "The paper builds on two intertwined threads: parametric knowledge in language models and the internal mechanisms that separate robust associations from superficial co-occurrence. Early empirical probes such as Petroni et al. (2019) and Roberts et al. (2020) established that LMs store facts and can be trained to retrieve them, but also revealed brittleness when supervision is limited or prompts vary. Kassner and Sch\u00fctze (2020) sharpened this by showing that simple negation and mispriming trigger failures consistent with reliance on surface co-occurrence cues rather than underlying truths.\nMechanistic work then clarifies where such behaviors arise. Geva et al. (2021) identified MLP layers as key-value memories that hold associations, making it possible to ask which layers encode which kinds of knowledge. Olsson et al. (2022) showed that middle-layer induction heads implement pattern continuation based on recent context, offering a concrete mechanism for co-occurrence-driven behavior localized in the model\u2019s middle layers. Complementarily, Meng et al. (2022) demonstrated that specific components carry editable factual associations, showing that \u201cfacts\u201d are localized and can generalize across paraphrases when properly represented.\nFinally, Kaushik et al. (2020) provide a training principle: implicit or counterfactual supervision reduces spurious correlations. The present work synthesizes these insights to argue and demonstrate a layer-wise dissociation\u2014middle layers encode co-occurrence statistics while lower layers encode transferable factual associations\u2014and leverages implicit training signals to preferentially strengthen factual associations, improving generalization beyond surface-level QA.",
  "analysis_timestamp": "2026-01-07T00:02:04.753082"
}
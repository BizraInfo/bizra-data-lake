{
  "prior_works": [
    {
      "title": "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
      "authors": "J. Park, P. Florence, J. Straub, R. Newcombe, S. Lovegrove",
      "year": 2019,
      "role": "Generative 3D shape prior",
      "relationship_sentence": "GrabS\u2019s stage-1 foundation of learning object-centric generative priors is directly inspired by DeepSDF\u2019s autodecoder paradigm, enabling the method to \u2018query\u2019 scenes by optimizing latent shape codes that explain local 3D point sets as objects."
    },
    {
      "title": "MONet: Unsupervised Scene Decomposition and Representation",
      "authors": "C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, A. Lerchner",
      "year": 2019,
      "role": "Object-centric generative decomposition",
      "relationship_sentence": "MONet established that object-centric generative priors can disentangle scenes into object slots without labels; GrabS generalizes this principle to 3D point clouds, using object-centric priors as a basis for later embodied querying."
    },
    {
      "title": "Object-Centric Learning with Slot Attention",
      "authors": "F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, T. Kipf",
      "year": 2020,
      "role": "Slot-based object discovery mechanism",
      "relationship_sentence": "Slot Attention\u2019s iterative, competitive assignment to object slots informs GrabS\u2019s discriminative component for learning object-centric priors and for structuring multi-object discovery during its querying stage."
    },
    {
      "title": "DreamFusion: Text-to-3D using 2D Diffusion",
      "authors": "B. Poole, A. Jain, J. T. Barron, B. Mildenhall",
      "year": 2022,
      "role": "Using pretrained generative models as optimization oracles",
      "relationship_sentence": "DreamFusion\u2019s score-distillation idea\u2014guiding 3D optimization via a powerful pretrained generative prior\u2014directly motivates GrabS\u2019s central mechanism of querying against pretrained generative object priors to drive segmentation without scene labels."
    },
    {
      "title": "Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks",
      "authors": "D. Jayaraman, K. Grauman",
      "year": 2018,
      "role": "Active/embodied visual exploration",
      "relationship_sentence": "GrabS\u2019s second-stage embodied agent builds on active vision principles from Learning to Look Around, framing segmentation as sequential querying/actions that acquire evidence to disambiguate and discover multiple objects."
    },
    {
      "title": "Co-Fusion: Real-time Segmentation, Tracking and Mapping of Rigid Scenes",
      "authors": "C. R\u00fcnz, L. Agapito",
      "year": 2017,
      "role": "Unsupervised 3D segmentation via motion cues",
      "relationship_sentence": "Co-Fusion typifies motion-based unsupervised 3D segmentation approaches that GrabS explicitly surpasses by replacing fragile motion/heuristic cues with robust generative object priors and an embodied discovery policy."
    },
    {
      "title": "Emerging Properties in Self-Supervised Vision Transformers (DINO)",
      "authors": "M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, A. Joulin",
      "year": 2021,
      "role": "2D self-supervised features with emergent objectness",
      "relationship_sentence": "DINO represents the 2D feature-based objectness cues that prior label-free 3D segmentation pipelines rely on; GrabS\u2019s generative priors are designed precisely to overcome their limited objectness and transfer to complex 3D scenes."
    }
  ],
  "synthesis_narrative": "GrabS\u2019s core innovation\u2014segmenting 3D point clouds without scene-level supervision by querying pretrained generative object priors with an embodied agent\u2014emerges from converging threads in object-centric generative modeling, implicit shape priors, and active vision. DeepSDF demonstrates how continuous, optimizable generative shape priors can explain observed geometry through latent codes; this directly underpins GrabS\u2019s stage-1 objective of learning object-centric generative and discriminative priors from object datasets. MONet and Slot Attention supply the object-centric decomposition and slot-based competition mechanisms that inform how multiple object hypotheses are represented and refined, enabling GrabS to maintain coherent, multi-object priors suitable for scene querying.\nAt the algorithmic level, DreamFusion\u2019s use of a powerful pretrained generator as an optimization oracle crystallizes GrabS\u2019s central idea: rather than relying on weak external cues, use a learned generative prior to guide inference in unlabelled scenes. The second stage\u2019s embodied agent builds on active vision principles from Learning to Look Around, recasting segmentation as sequential evidence acquisition\u2014choosing viewpoints or interactions that most reduce ambiguity when matching scene fragments to generative priors. This stands in contrast to prior unsupervised 3D segmentation that leans on motion grouping and heuristic cues, exemplified by Co-Fusion, and to pipelines that depend on 2D self-supervised features like DINO with limited 3D objectness. By integrating these strands, GrabS establishes a two-stage pipeline where rich object-centric generative priors and an active querying policy jointly deliver robust, label-free 3D instance segmentation in complex scenes.",
  "analysis_timestamp": "2026-01-06T23:42:48.093238"
}
#!/usr/bin/env python3
"""
PAT Team Evaluator â€” Local Performance Assessment with CLEAR Framework
======================================================================

Activates the PAT (Personal Agentic Team) and evaluates performance using
the True Spearpoint CLEAR framework (Cost, Latency, Efficacy, Assurance, Reliability).

Standing on Giants:
- BIZRA PAT: Personal Agentic Team (7 agents)
- BIZRA PEK: Proactive Execution Kernel
- True Spearpoint: Benchmark Dominance Loop
- CLEAR Framework: 5D evaluation metrics

Usage:
    python scripts/pat_evaluator.py --quick           # Quick evaluation
    python scripts/pat_evaluator.py --full            # Full benchmark suite
    python scripts/pat_evaluator.py --cycles 5        # Run 5 PEK cycles
"""

from __future__ import annotations

import argparse
import asyncio
import json
import logging
import sys
import time
from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    handlers=[
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger("PAT.Evaluator")


# ============================================================================
# PAT Team Structure
# ============================================================================

@dataclass
class PATAgentConfig:
    """Configuration for a PAT agent."""
    name: str
    role: str
    specialty: str
    model_tier: str = "LOCAL"  # NANO, EDGE, LOCAL, POOL, FRONTIER


PAT_TEAM_CONFIG = [
    PATAgentConfig("Strategist", "strategist", "Goal decomposition & planning", "LOCAL"),
    PATAgentConfig("Researcher", "researcher", "Information synthesis", "LOCAL"),
    PATAgentConfig("Analyst", "analyst", "Data analysis & pattern recognition", "LOCAL"),
    PATAgentConfig("Creator", "creator", "Content generation & ideation", "LOCAL"),
    PATAgentConfig("Executor", "executor", "Task execution & automation", "EDGE"),
    PATAgentConfig("Guardian", "guardian", "Security & ethics validation", "LOCAL"),
    PATAgentConfig("Coordinator", "coordinator", "Multi-agent orchestration", "LOCAL"),
]


@dataclass
class PATEvalResult:
    """Result of PAT team evaluation."""
    timestamp: str
    duration_ms: float
    agents_active: int
    tasks_completed: int
    tasks_failed: int
    clear_scores: Dict[str, float]
    overall_score: float
    snr_score: float
    ihsan_compliant: bool
    cycle_results: List[Dict[str, Any]] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)


# ============================================================================
# CLEAR Metrics Integration
# ============================================================================

@dataclass
class CLEARMetrics:
    """5-dimension CLEAR metrics for PAT evaluation."""
    cost: float = 0.0          # Cost efficiency (0-1, higher = better)
    latency: float = 0.0       # Response speed (0-1, higher = faster)
    efficacy: float = 0.0      # Task success rate (0-1)
    assurance: float = 0.0     # Security/ethics score (0-1)
    reliability: float = 0.0   # Consistency/determinism (0-1)

    def overall(self, weights: Optional[Dict[str, float]] = None) -> float:
        """Calculate weighted overall score."""
        w = weights or {
            "cost": 0.20,
            "latency": 0.15,
            "efficacy": 0.35,
            "assurance": 0.15,
            "reliability": 0.15,
        }
        return (
            self.cost * w["cost"]
            + self.latency * w["latency"]
            + self.efficacy * w["efficacy"]
            + self.assurance * w["assurance"]
            + self.reliability * w["reliability"]
        )

    def to_dict(self) -> Dict[str, float]:
        return {
            "cost": self.cost,
            "latency": self.latency,
            "efficacy": self.efficacy,
            "assurance": self.assurance,
            "reliability": self.reliability,
        }

    def weakest_dimension(self) -> str:
        """Identify the dimension needing most improvement."""
        scores = self.to_dict()
        return min(scores, key=scores.get)


# ============================================================================
# PAT Evaluator Core
# ============================================================================

class PATEvaluator:
    """
    Evaluates PAT team performance using CLEAR metrics.

    Modes:
    - Quick: Simulated evaluation for rapid feedback
    - Full: Actual PEK kernel execution with real inference
    """

    def __init__(
        self,
        state_dir: Path = PROJECT_ROOT / "sovereign_state",
        ihsan_threshold: float = 0.95,
        snr_threshold: float = 0.85,
    ):
        self.state_dir = state_dir
        self.ihsan_threshold = ihsan_threshold
        self.snr_threshold = snr_threshold
        self.state_dir.mkdir(parents=True, exist_ok=True)

        # Lazy-loaded components
        self._pek_kernel = None
        self._proactive_team = None
        self._clear_framework = None

    async def _init_components(self) -> None:
        """Initialize PAT/PEK components lazily."""
        # Try to load CLEAR framework from True Spearpoint
        try:
            from core.benchmark.clear_framework import CLEARFramework
            self._clear_framework = CLEARFramework()
            logger.info("âœ“ CLEAR Framework loaded from True Spearpoint")
        except ImportError as e:
            logger.warning(f"CLEAR Framework not available: {e}")

        # Try to load ProactiveTeam
        try:
            from core.sovereign.proactive_team import ProactiveTeam
            self._proactive_team = ProactiveTeam(ihsan_threshold=self.ihsan_threshold)
            logger.info("âœ“ ProactiveTeam loaded")
        except ImportError as e:
            logger.warning(f"ProactiveTeam not available: {e}")

        # Try to load PEK kernel
        try:
            from core.pek.kernel import ProactiveExecutionKernel, ProactiveExecutionKernelConfig
            from core.sovereign.opportunity_pipeline import OpportunityPipeline

            pipeline = OpportunityPipeline()
            config = ProactiveExecutionKernelConfig(
                cycle_interval_seconds=1.0,  # Fast for evaluation
                min_snr=self.snr_threshold,
            )
            self._pek_kernel = ProactiveExecutionKernel(
                opportunity_pipeline=pipeline,
                state_dir=self.state_dir,
                config=config,
            )
            logger.info("âœ“ PEK Kernel loaded")
        except ImportError as e:
            logger.warning(f"PEK Kernel not available: {e}")

    async def run_quick_eval(self) -> PATEvalResult:
        """
        Quick evaluation using simulated metrics.
        Fast feedback loop without full inference.
        """
        logger.info("=" * 60)
        logger.info("PAT TEAM â€” QUICK EVALUATION")
        logger.info("=" * 60)

        start = time.perf_counter()

        # Simulate PAT team activation
        logger.info(f"\nğŸ“‹ PAT Team Configuration ({len(PAT_TEAM_CONFIG)} agents):")
        for i, agent in enumerate(PAT_TEAM_CONFIG, 1):
            logger.info(f"   {i}. {agent.name} [{agent.role}] @ {agent.model_tier}")

        # Simulate task execution
        tasks = [
            ("Strategic planning", True, 0.92),
            ("Research synthesis", True, 0.88),
            ("Data analysis", True, 0.95),
            ("Content creation", True, 0.85),
            ("Task automation", True, 0.90),
            ("Security validation", True, 0.98),
            ("Team coordination", True, 0.87),
        ]

        cycle_results = []
        completed = 0
        failed = 0

        logger.info("\nğŸ”„ Simulating PAT Execution Cycles...")
        for task_name, success, score in tasks:
            completed += 1 if success else 0
            failed += 0 if success else 1
            cycle_results.append({
                "task": task_name,
                "success": success,
                "score": score,
                "agent": PAT_TEAM_CONFIG[len(cycle_results) % len(PAT_TEAM_CONFIG)].name,
            })
            logger.info(f"   âœ“ {task_name}: {score:.2f}")

        # Calculate CLEAR metrics
        avg_score = sum(t[2] for t in tasks) / len(tasks)
        clear = CLEARMetrics(
            cost=0.85,  # Simulated cost efficiency
            latency=0.90,  # Fast response
            efficacy=avg_score,
            assurance=0.95,  # High security
            reliability=0.88,  # Good consistency
        )

        overall = clear.overall()
        snr = self._calculate_snr(clear)
        ihsan_ok = overall >= self.ihsan_threshold

        duration = (time.perf_counter() - start) * 1000

        # Generate recommendations
        recommendations = self._generate_recommendations(clear)

        result = PATEvalResult(
            timestamp=datetime.now(timezone.utc).isoformat(),
            duration_ms=duration,
            agents_active=len(PAT_TEAM_CONFIG),
            tasks_completed=completed,
            tasks_failed=failed,
            clear_scores=clear.to_dict(),
            overall_score=overall,
            snr_score=snr,
            ihsan_compliant=ihsan_ok,
            cycle_results=cycle_results,
            recommendations=recommendations,
        )

        self._print_results(result, clear)
        return result

    async def run_full_eval(self, cycles: int = 3) -> PATEvalResult:
        """
        Full evaluation with actual PEK kernel execution.
        Runs real proactive cycles and measures performance.
        """
        logger.info("=" * 60)
        logger.info("PAT TEAM â€” FULL EVALUATION")
        logger.info(f"Cycles: {cycles}")
        logger.info("=" * 60)

        start = time.perf_counter()

        # Initialize components
        await self._init_components()

        cycle_results = []
        completed = 0
        failed = 0

        # Try to use PAT engine with LLM backend
        pat_engine_available = await self._check_pat_engine()

        if pat_engine_available:
            logger.info("\nğŸ”„ Running PAT Engine with LLM Backend...")
            for i in range(cycles):
                try:
                    result = await self._run_pat_engine_cycle(i + 1)
                    cycle_results.append(result)
                    completed += result.get("tasks_completed", 0)
                    failed += result.get("tasks_failed", 0)
                except Exception as e:
                    logger.error(f"Cycle {i + 1} failed: {e}")
                    failed += 1
                    cycle_results.append({"cycle": i + 1, "error": str(e), "tasks_failed": 1})
        elif self._proactive_team:
            logger.info("\nğŸ”„ Running ProactiveTeam with Injected Tasks...")
            # Inject test opportunities to trigger actual work
            await self._inject_test_opportunities()
            for i in range(cycles):
                try:
                    result = await self._run_proactive_cycle(i + 1)
                    cycle_results.append(result)
                    completed += result.get("tasks_completed", 0)
                    failed += result.get("tasks_failed", 0)
                except Exception as e:
                    logger.error(f"Cycle {i + 1} failed: {e}")
                    failed += 1
                    cycle_results.append({"cycle": i + 1, "error": str(e)})
        else:
            # Fallback to simulated cycles with realistic task simulation
            logger.info("\nğŸ”„ Running Simulated PAT Cycles (LLM not available)...")
            for i in range(cycles):
                result = await self._run_simulated_pat_cycle(i + 1)
                cycle_results.append(result)
                completed += result.get("tasks_completed", 0)
                failed += result.get("tasks_failed", 0)

        # Calculate CLEAR metrics from cycle results
        clear = self._calculate_clear_from_cycles(cycle_results)

        overall = clear.overall()
        snr = self._calculate_snr(clear)
        ihsan_ok = overall >= self.ihsan_threshold

        duration = (time.perf_counter() - start) * 1000

        recommendations = self._generate_recommendations(clear)

        result = PATEvalResult(
            timestamp=datetime.now(timezone.utc).isoformat(),
            duration_ms=duration,
            agents_active=len(PAT_TEAM_CONFIG),
            tasks_completed=completed,
            tasks_failed=failed,
            clear_scores=clear.to_dict(),
            overall_score=overall,
            snr_score=snr,
            ihsan_compliant=ihsan_ok,
            cycle_results=cycle_results,
            recommendations=recommendations,
        )

        self._print_results(result, clear)
        return result

    async def _run_proactive_cycle(self, cycle_num: int) -> Dict[str, Any]:
        """Run a single proactive team cycle."""
        logger.info(f"\n   Cycle {cycle_num}:")

        if self._proactive_team:
            # Real cycle execution
            cycle_result = await self._proactive_team.run_cycle()
            logger.info(f"      Opportunities: {cycle_result.opportunities_detected}")
            logger.info(f"      Tasks created: {cycle_result.tasks_created}")
            logger.info(f"      Tasks executed: {cycle_result.tasks_executed}")
            logger.info(f"      Synergy: {cycle_result.synergy_score:.2f}")
            return {
                "cycle": cycle_num,
                "opportunities_detected": cycle_result.opportunities_detected,
                "tasks_created": cycle_result.tasks_created,
                "tasks_completed": cycle_result.tasks_executed,
                "synergy_score": cycle_result.synergy_score,
                "duration_ms": cycle_result.duration_ms,
            }
        else:
            # Simulated
            return {
                "cycle": cycle_num,
                "opportunities_detected": 2,
                "tasks_created": 1,
                "tasks_completed": 1,
                "synergy_score": 0.85,
                "duration_ms": 100.0,
            }

    async def _check_pat_engine(self) -> bool:
        """Check if PAT engine with LLM backend is available."""
        try:
            import httpx
            # Check for LM Studio at default endpoint
            async with httpx.AsyncClient(timeout=3.0) as client:
                try:
                    resp = await client.get("http://192.168.56.1:1234/v1/models")
                    if resp.status_code == 200:
                        logger.info("âœ“ LM Studio backend available")
                        return True
                except:
                    pass
                # Check for Ollama
                try:
                    resp = await client.get("http://localhost:11434/api/tags")
                    if resp.status_code == 200:
                        logger.info("âœ“ Ollama backend available")
                        return True
                except:
                    pass
            return False
        except ImportError:
            return False

    async def _run_pat_engine_cycle(self, cycle_num: int) -> Dict[str, Any]:
        """Run a PAT engine cycle with actual LLM inference."""
        logger.info(f"\n   Cycle {cycle_num}:")
        start = time.perf_counter()

        # Define test tasks for evaluation
        test_tasks = [
            ("Analyze system health", "Strategist"),
            ("Research optimization opportunities", "Researcher"),
            ("Generate status report", "Creator"),
        ]

        tasks_completed = 0
        task_scores = []

        try:
            import httpx
            async with httpx.AsyncClient(timeout=60.0) as client:
                for task_name, agent in test_tasks:
                    try:
                        # Try LM Studio first
                        prompt = f"You are a {agent} agent. Task: {task_name}. Respond briefly."
                        payload = {
                            "model": "local-model",
                            "messages": [{"role": "user", "content": prompt}],
                            "max_tokens": 100,
                            "temperature": 0.7,
                        }
                        resp = await client.post(
                            "http://192.168.56.1:1234/v1/chat/completions",
                            json=payload,
                        )
                        if resp.status_code == 200:
                            tasks_completed += 1
                            task_scores.append(0.90)
                            logger.info(f"      âœ“ {task_name} [{agent}]: SUCCESS")
                        else:
                            task_scores.append(0.0)
                            logger.info(f"      âœ— {task_name} [{agent}]: FAILED ({resp.status_code})")
                    except Exception as e:
                        task_scores.append(0.0)
                        logger.warning(f"      âœ— {task_name} [{agent}]: {e}")
        except Exception as e:
            logger.error(f"PAT engine cycle failed: {e}")

        duration = (time.perf_counter() - start) * 1000
        avg_score = sum(task_scores) / len(task_scores) if task_scores else 0.0

        return {
            "cycle": cycle_num,
            "opportunities_detected": len(test_tasks),
            "tasks_created": len(test_tasks),
            "tasks_completed": tasks_completed,
            "tasks_failed": len(test_tasks) - tasks_completed,
            "synergy_score": avg_score,
            "duration_ms": duration,
        }

    async def _inject_test_opportunities(self) -> None:
        """Inject test opportunities into the proactive team."""
        if not self._proactive_team:
            return

        logger.info("\n   Injecting test opportunities...")
        test_opportunities = [
            {
                "description": "Optimize memory usage",
                "criteria": ["Reduce RAM by 10%"],
                "priority": 0.8,
            },
            {
                "description": "Update security policies",
                "criteria": ["Review access controls"],
                "priority": 0.9,
            },
            {
                "description": "Generate daily summary",
                "criteria": ["Compile metrics"],
                "priority": 0.6,
            },
        ]

        for opp in test_opportunities:
            try:
                await self._proactive_team._process_opportunity(opp)
                logger.info(f"      âœ“ Injected: {opp['description']}")
            except Exception as e:
                logger.warning(f"      âœ— Failed to inject: {e}")

    async def _run_simulated_pat_cycle(self, cycle_num: int) -> Dict[str, Any]:
        """Run a simulated PAT cycle with realistic metrics."""
        logger.info(f"\n   Cycle {cycle_num} (simulated):")
        start = time.perf_counter()

        # Simulate realistic agent work
        agents_work = [
            ("Strategist", 0.92, 50),
            ("Researcher", 0.88, 80),
            ("Analyst", 0.95, 40),
            ("Creator", 0.85, 120),
            ("Executor", 0.90, 30),
            ("Guardian", 0.98, 20),
            ("Coordinator", 0.87, 35),
        ]

        tasks_completed = 0
        total_score = 0.0

        for agent, score, latency_ms in agents_work:
            await asyncio.sleep(latency_ms / 1000)  # Simulate work
            tasks_completed += 1
            total_score += score
            logger.info(f"      âœ“ {agent}: {score:.2f} ({latency_ms}ms)")

        duration = (time.perf_counter() - start) * 1000
        avg_score = total_score / len(agents_work)

        return {
            "cycle": cycle_num,
            "opportunities_detected": len(agents_work),
            "tasks_created": len(agents_work),
            "tasks_completed": tasks_completed,
            "tasks_failed": 0,
            "synergy_score": avg_score,
            "duration_ms": duration,
        }

    def _calculate_clear_from_cycles(self, cycles: List[Dict]) -> CLEARMetrics:
        """Calculate CLEAR metrics from cycle results."""
        if not cycles:
            return CLEARMetrics()

        # Aggregate metrics
        total_completed = sum(c.get("tasks_completed", 0) for c in cycles)
        total_failed = sum(c.get("tasks_failed", 0) for c in cycles)
        total_tasks = total_completed + total_failed
        avg_synergy = sum(c.get("synergy_score", 0.8) for c in cycles) / len(cycles)
        avg_duration = sum(c.get("duration_ms", 100) for c in cycles) / len(cycles)

        # Calculate individual dimensions
        efficacy = total_completed / max(total_tasks, 1)
        latency = min(1.0, 500 / max(avg_duration, 1))  # 500ms = 1.0
        cost = 0.85  # Assume efficient local inference
        assurance = 0.95  # High assurance with constitutional gates
        reliability = avg_synergy  # Use synergy as reliability proxy

        return CLEARMetrics(
            cost=cost,
            latency=latency,
            efficacy=efficacy,
            assurance=assurance,
            reliability=reliability,
        )

    def _calculate_snr(self, clear: CLEARMetrics) -> float:
        """Calculate Signal-to-Noise Ratio from CLEAR metrics."""
        signal = (clear.efficacy * 0.5) + (clear.assurance * 0.3) + (clear.reliability * 0.2)
        noise = max(0.01, 1 - clear.cost)  # Cost inefficiency = noise
        return signal / noise

    def _generate_recommendations(self, clear: CLEARMetrics) -> List[str]:
        """Generate improvement recommendations based on CLEAR scores."""
        recs = []
        weakest = clear.weakest_dimension()

        if clear.cost < 0.80:
            recs.append("âš¡ Optimize cost by using EDGE/NANO models for simple tasks")
        if clear.latency < 0.80:
            recs.append("ğŸš€ Reduce latency with speculative decoding or model caching")
        if clear.efficacy < 0.85:
            recs.append("ğŸ¯ Improve efficacy with better prompts and agent specialization")
        if clear.assurance < 0.90:
            recs.append("ğŸ›¡ï¸ Strengthen assurance with additional constitutional gates")
        if clear.reliability < 0.85:
            recs.append("ğŸ”„ Increase reliability with seed sweep and deterministic settings")

        if not recs:
            recs.append("âœ… All dimensions performing well. Consider pushing to FRONTIER tier.")

        recs.insert(0, f"ğŸ” Focus on improving: {weakest.upper()} (lowest score)")
        return recs

    def _print_results(self, result: PATEvalResult, clear: CLEARMetrics) -> None:
        """Print formatted evaluation results."""
        print("\n")
        print("=" * 60)
        print("               PAT EVALUATION RESULTS")
        print("=" * 60)
        print(f"\n  Timestamp:     {result.timestamp}")
        print(f"  Duration:      {result.duration_ms:.1f}ms")
        print(f"  Agents Active: {result.agents_active}")
        print(f"  Tasks:         {result.tasks_completed} completed, {result.tasks_failed} failed")

        print("\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print("  â”‚                  CLEAR METRICS                         â”‚")
        print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        print(f"  â”‚  Cost (C):       {self._bar(clear.cost)} {clear.cost:.2f}        â”‚")
        print(f"  â”‚  Latency (L):    {self._bar(clear.latency)} {clear.latency:.2f}        â”‚")
        print(f"  â”‚  Efficacy (E):   {self._bar(clear.efficacy)} {clear.efficacy:.2f}        â”‚")
        print(f"  â”‚  Assurance (A):  {self._bar(clear.assurance)} {clear.assurance:.2f}        â”‚")
        print(f"  â”‚  Reliability (R):{self._bar(clear.reliability)} {clear.reliability:.2f}        â”‚")
        print("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        print(f"  â”‚  OVERALL SCORE:  {self._bar(result.overall_score)} {result.overall_score:.2f}        â”‚")
        print(f"  â”‚  SNR SCORE:      {result.snr_score:.2f}                              â”‚")
        print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

        status = "âœ… PASS" if result.ihsan_compliant else "âŒ FAIL"
        print(f"\n  IHSÄ€N COMPLIANCE: {status} (threshold: {self.ihsan_threshold})")

        print(f"\n  Weakest Dimension: {clear.weakest_dimension().upper()}")

        print("\n  ğŸ“‹ RECOMMENDATIONS:")
        for rec in result.recommendations:
            print(f"     {rec}")

        print("\n" + "=" * 60)

    def _bar(self, value: float, width: int = 20) -> str:
        """Create ASCII progress bar."""
        filled = int(value * width)
        return "â–ˆ" * filled + "â–‘" * (width - filled)

    def save_report(self, result: PATEvalResult, path: Optional[Path] = None) -> Path:
        """Save evaluation report to JSON."""
        path = path or (self.state_dir / "pat_evaluation_report.json")
        path.write_text(json.dumps(asdict(result), indent=2))
        logger.info(f"Report saved to: {path}")
        return path


# ============================================================================
# CLI Entry Point
# ============================================================================

async def main():
    parser = argparse.ArgumentParser(
        description="PAT Team Evaluator â€” Local Performance Assessment",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/pat_evaluator.py --quick           # Quick simulated evaluation
  python scripts/pat_evaluator.py --full            # Full PEK cycle evaluation
  python scripts/pat_evaluator.py --full --cycles 5 # 5 full cycles
        """,
    )
    parser.add_argument(
        "--quick",
        action="store_true",
        help="Run quick simulated evaluation",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="Run full evaluation with PEK cycles",
    )
    parser.add_argument(
        "--cycles",
        type=int,
        default=3,
        help="Number of PEK cycles for full evaluation (default: 3)",
    )
    parser.add_argument(
        "--output",
        type=Path,
        help="Output path for JSON report",
    )
    parser.add_argument(
        "--ihsan",
        type=float,
        default=0.95,
        help="IhsÄn threshold (default: 0.95)",
    )
    parser.add_argument(
        "--snr",
        type=float,
        default=0.85,
        help="SNR threshold (default: 0.85)",
    )

    args = parser.parse_args()

    # Default to quick if neither specified
    if not args.quick and not args.full:
        args.quick = True

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—          â•‘
â•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•    â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘          â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘          â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘          â•‘
â•‘   â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘          â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘          â•‘
â•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘          â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘          â•‘
â•‘   â•šâ•â•     â•šâ•â•  â•šâ•â•   â•šâ•â•          â•šâ•â•   â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•     â•šâ•â•          â•‘
â•‘                                                                              â•‘
â•‘   Personal Agentic Team â€” CLEAR Framework Evaluation                         â•‘
â•‘   True Spearpoint Integration                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    evaluator = PATEvaluator(
        ihsan_threshold=args.ihsan,
        snr_threshold=args.snr,
    )

    if args.quick:
        result = await evaluator.run_quick_eval()
    else:
        result = await evaluator.run_full_eval(cycles=args.cycles)

    # Save report
    report_path = evaluator.save_report(result, args.output)

    # Print final status
    if result.ihsan_compliant:
        print("\nâœ… PAT TEAM: OPERATIONAL â€” Meeting IhsÄn standards")
    else:
        print("\nâš ï¸  PAT TEAM: NEEDS IMPROVEMENT â€” Below IhsÄn threshold")

    return 0 if result.ihsan_compliant else 1


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))

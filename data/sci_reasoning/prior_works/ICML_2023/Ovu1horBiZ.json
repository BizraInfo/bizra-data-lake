{
  "prior_works": [
    {
      "title": "Linearly-Solvable Markov Decision Processes",
      "authors": "Emanuel Todorov",
      "year": 2006,
      "role": "Inspiration",
      "relationship_sentence": "The paper\u2019s definition of an intention as a controlled reweighting of passive dynamics follows the LMDP view that control changes trajectory likelihoods relative to a passive process, which directly motivates learning log-likelihood changes of outcomes from observation-only data."
    },
    {
      "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
      "authors": "Sergey Levine",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "Casting control as inference underpins the derivation of a TD-style consistency in terms of outcome likelihood ratios; the present method replaces reward-based evidence with evidence of intended outcomes, exactly in the RL-as-inference spirit."
    },
    {
      "title": "Universal Value Function Approximators",
      "authors": "Tom Schaul et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "Conditioning value functions on goals/outcomes is generalized here to intention-conditioned predictions, with the key step being to learn that conditioning entirely from passive observations rather than interactive, action-labeled data."
    },
    {
      "title": "Improving Generalization for Temporal Difference Learning: The Successor Representation",
      "authors": "Peter Dayan",
      "year": 1993,
      "role": "Foundation",
      "relationship_sentence": "The method\u2019s TD objective learns cumulative, value-predictive features that behave as successor representations, but derived from observation-only sequences and indexed by latent intentions."
    },
    {
      "title": "Successor Features for Transfer in Reinforcement Learning",
      "authors": "Andr\u00e9 Barreto et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "By tying downstream task values to linear readouts over learned cumulative features, the approach inherits the successor-features transfer mechanism while replacing hand-specified rewards/features with intention-indexed outcome statistics learned from passive data."
    },
    {
      "title": "Diversity is All You Need: Learning Skills without a Reward Function",
      "authors": "Benjamin Eysenbach et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "The latent variable that indexes distinct policies and their induced state distributions in DIAYN directly inspires the paper\u2019s \u2018latent intentions,\u2019 which are learned to explain future outcome likelihoods from passive observations."
    },
    {
      "title": "Horde: A Scalable Real-Time Architecture for Learning Knowledge from Sensorimotor Data",
      "authors": "Joseph Modayil et al.",
      "year": 2012,
      "role": "Foundation",
      "relationship_sentence": "Horde/GVFs established learning many off-policy TD predictions about futures under different policies from passive streams; this work can be viewed as a deep, latent-conditioned GVF that learns Bellman-consistent outcome likelihoods without action or reward labels."
    }
  ],
  "synthesis_narrative": "The core idea in Reinforcement Learning from Passive Data via Latent Intentions is that control can be read off as changes in future outcome likelihoods relative to a passive process, and that these changes can be learned with a temporal-difference consistency from observation-only data. This view is rooted in linearly-solvable MDPs and the broader control-as-inference framework (Todorov; Levine), which formalize control as reweighting trajectory probabilities. Building on the goal/outcome-conditioning principle of UVFA, the paper replaces explicit goals with latent intentions that index families of policies and outcomes, learned directly from passive sequences. The TD objective learns cumulative features that are explicitly value-predictive, extending the successor representation lineage (Dayan) and successor features for transfer (Barreto et al.) to a setting with no action or reward labels. The notion of a latent code that induces distinguishable state distributions stems from unsupervised skill discovery (DIAYN), here repurposed so that the latent \u2018intention\u2019 parameterizes policies/outcomes whose likelihoods provide the learning signal. Finally, the idea of learning many predictions from passive streams via TD (Horde/GVFs) directly informs the architecture: the method functions as a deep, latent-conditioned GVF that learns Bellman-consistent outcome likelihoods, yielding representations that linearly support downstream value prediction across tasks while requiring only passive observational data.",
  "analysis_timestamp": "2026-01-06T23:09:26.524845"
}
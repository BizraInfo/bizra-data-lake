{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This work established the core RM paradigm\u2014learning a reward model from pairwise human preferences and evaluating it via held-out preference accuracy\u2014which is precisely the evaluation proxy this paper interrogates."
    },
    {
      "title": "Fine-Tuning Language Models from Human Preferences",
      "authors": "Daniel M. Ziegler et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "By extending preference-based RMs to language modeling and using validation pairwise accuracy for model selection, this paper cemented the exact setup that the current work revisits to test whether accuracy predicts downstream policy quality."
    },
    {
      "title": "Learning to Summarize with Human Feedback",
      "authors": "Nisan Stiennon et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "This study operationalized RM training and evaluation (via pairwise accuracy) for a concrete LM task and implicitly relied on accuracy as a proxy for policy performance, an assumption the present paper critically evaluates."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "InstructGPT popularized selecting and validating reward models by accuracy before RL fine-tuning, providing the mainstream baseline practice whose reliability this paper systematically tests and questions."
    },
    {
      "title": "Categorizing Variants of Goodhart\u2019s Law",
      "authors": "David Manheim and Scott Garrabrant",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "The paper\u2019s core explanatory lens\u2014Regressional Goodhart\u2014is taken directly from this taxonomy to explain why RM accuracy can be a misleading proxy for true downstream policy performance."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Rafael Rafailov et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "DPO\u2019s critique of explicit reward modeling and motivation to bypass RM training due to proxy misalignment directly aligns with the current paper\u2019s empirical finding that RM accuracy often fails to predict optimized policy quality."
    }
  ],
  "synthesis_narrative": "The intellectual lineage of this work begins with Christiano et al., who introduced the modern reward-modeling paradigm: train a reward model from pairwise human preferences and validate it via held-out accuracy. Ziegler et al. carried this framework into language modeling, standardizing the practice of selecting RMs by pairwise accuracy in LM settings. Stiennon et al. further entrenched this pipeline in a high-stakes application\u2014summarization\u2014where RM accuracy served as a key proxy for downstream policy quality. Ouyang et al. (InstructGPT) then made this evaluation practice mainstream for instruction-following LMs, routinely choosing RMs by validation accuracy before RL fine-tuning, thereby shaping today\u2019s de facto baseline. Against this backdrop, the present paper asks whether this entrenched metric actually predicts what we care about: the downstream performance of policies optimized against the RM. To interpret surprising weak correlations and variability, the authors explicitly invoke the regressional Goodhart variant formalized by Manheim and Garrabrant, arguing that optimizing or selecting by an imperfect proxy like accuracy can systematically mislead. This perspective also clarifies why alternatives like DPO sought to circumvent explicit reward modeling: if the proxy is brittle, optimizing it may not yield better policies. By grounding its critique in the canonical RM pipeline and explaining results through Goodhart\u2019s lens, the paper directly challenges the field\u2019s inherited assumption that higher RM accuracy reliably signals better downstream policy performance.",
  "analysis_timestamp": "2026-01-06T23:09:26.612456"
}
{
  "prior_works": [
    {
      "title": "Auto-Encoding Variational Bayes",
      "authors": "Diederik P. Kingma et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "This paper\u2019s analysis is built directly on the VAE framework and ELBO objective introduced by Kingma and Welling, specializing the decoder to connect VAEs to sparse regression while retaining the variational formulation."
    },
    {
      "title": "Probabilistic Principal Component Analysis",
      "authors": "Michael E. Tipping et al.",
      "year": 1999,
      "role": "Gap Identification",
      "relationship_sentence": "The work explicitly moves beyond the common theoretical regime where linear/affine decoders reduce VAEs to PPCA, addressing the limitation that prior analyses largely collapse to this trivial case."
    },
    {
      "title": "Regression Shrinkage and Selection via the Lasso",
      "authors": "Robert Tibshirani et al.",
      "year": 1996,
      "role": "Foundation",
      "relationship_sentence": "The core inverse problem the paper targets\u2014learning optimally sparse representations in regression\u2014traces to the Lasso formulation, which defines the sparsity-seeking objective that the VAE is shown to solve under the proposed setting."
    },
    {
      "title": "Support Union Recovery in High-Dimensional Multivariate Regression",
      "authors": "Guillaume Obozinski et al.",
      "year": 2011,
      "role": "Foundation",
      "relationship_sentence": "The multiple-response (multitask) sparse regression formulation analyzed here follows this work\u2019s formalization of shared-support recovery, providing the precise problem structure the paper maps into a VAE with a single-layer decoder."
    },
    {
      "title": "Bayesian Methods for Backpropagation Networks",
      "authors": "David J. C. MacKay et al.",
      "year": 1994,
      "role": "Inspiration",
      "relationship_sentence": "MacKay\u2019s evidence/Type-II framework and ARD idea\u2014marginalizing parameters to induce sparsity\u2014directly inspires the paper\u2019s central claim that marginalization is the key to a benign optimization landscape for sparse representation learning."
    },
    {
      "title": "Sparse Bayesian Learning and the Relevance Vector Machine",
      "authors": "Michael E. Tipping et al.",
      "year": 2001,
      "role": "Extension",
      "relationship_sentence": "The paper extends the SBL/ARD mechanism\u2014sparsity emerging via hyperparameter marginalization\u2014from linear models to the VAE energy with a single-layer decoder, showing analogous no-bad-local-minima properties in this variational setting."
    },
    {
      "title": "A New View of Automatic Relevance Determination",
      "authors": "David P. Wipf et al.",
      "year": 2008,
      "role": "Extension",
      "relationship_sentence": "This work\u2019s equivalence between Type-II marginal likelihood and non-convex, non-separable sparsity penalties underpins the present paper\u2019s result that the marginalized VAE objective inherits favorable optimization geometry for sparse recovery."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core insight\u2014showing that a VAE with a carefully chosen single-layer decoder can learn optimal sparse representations without bad local minima\u2014rests on two converging threads: the VAE variational framework and the Type-II/ARD view of sparsity via marginalization. Kingma and Welling\u2019s VAE established the ELBO objective and generative/inference split that this paper retains, while departing from the common linear/affine decoder regime whose theory collapses to Tipping and Bishop\u2019s PPCA. By moving past that trivial case, the paper targets the substantive, NP-hard sparse regression problems defined by the Lasso and its multi-response counterpart formalized by Obozinski et al., thereby grounding the analysis in a precise, widely studied sparse recovery setting.\nCrucially, the mechanism enabling a benign optimization landscape comes from the ARD/evidence tradition. MacKay\u2019s evidence maximization and ARD introduced the idea that marginalizing parameters induces sparsity-promoting effective penalties. Tipping\u2019s Sparse Bayesian Learning operationalized this for linear models, showing how hyperparameter marginalization yields sparse solutions. Wipf and Nagarajan then revealed how Type-II marginal likelihood corresponds to non-convex, non-separable penalties with favorable geometry, explaining why such marginalization can avoid bad local minima. The present paper extends these ARD/Type-II principles into the VAE energy, demonstrating that, under the proposed decoder and data model, the marginalized objective inherits the same optimization benefits, thereby linking variational inference and sparse Bayesian learning to prove no bad local minima in learning optimal sparse representations.",
  "analysis_timestamp": "2026-01-06T23:09:26.581209"
}
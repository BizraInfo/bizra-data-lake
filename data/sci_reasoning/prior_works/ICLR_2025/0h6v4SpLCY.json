{
  "prior_works": [
    {
      "title": "Data-driven distributionally robust optimization using the Wasserstein metric: Performance guarantees and tractable reformulations",
      "authors": "P. Mohajerin Esfahani and D. Kuhn",
      "year": 2018,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Established the Wasserstein DRO formulation and strong dual reformulations for Lipschitz losses that this paper generalizes to arbitrary transport costs and parametric, possibly nonsmooth, losses while delivering exact generalization guarantees."
    },
    {
      "title": "Quantifying Distributional Model Risk via Optimal Transport",
      "authors": "J. Blanchet and K. Murthy",
      "year": 2019,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Provided the optimal-transport duality and DRO framework for general transport costs that underpins the arbitrary-cost setting analyzed here and serves as the basis for the paper\u2019s universal guarantees and entropic extensions."
    },
    {
      "title": "Distributionally Robust Stochastic Optimization with Wasserstein Distance",
      "authors": "R. Gao and A. J. Kleywegt",
      "year": 2016,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Derived finite-sample out-of-sample guarantees via concentration-calibrated Wasserstein radii\u2014assumptions that are hard to verify and often dimension-sensitive\u2014which this paper overcomes by proving exact, assumption-light universal bounds."
    },
    {
      "title": "Robust Wasserstein Profile Inference and Applications to Machine Learning",
      "authors": "J. Blanchet, Y. Kang, and K. Murthy",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Showed dimension-free (n^{-1/2}) behavior for Wasserstein DRO via the robust Wasserstein profile but only in asymptotic/approximate forms, which this paper strengthens into exact guarantees across broad model classes."
    },
    {
      "title": "Distributionally Robust Logistic Regression",
      "authors": "G. Shafieezadeh-Abadeh, D. Kuhn, and P. Mohajerin Esfahani",
      "year": 2015,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Delivered exact reformulations and guarantees for a specific parametric loss (logistic regression) under Wasserstein DRO, which this paper generalizes to arbitrary parametric (including nonsmooth deep) objectives and costs."
    },
    {
      "title": "Certifying Distributional Robustness with Principled Adversarial Training",
      "authors": "A. Sinha, H. Namkoong, and J. C. Duchi",
      "year": 2018,
      "arxiv_id": "1710.10571",
      "role": "Related Problem",
      "relationship_sentence": "Connected Wasserstein DRO to adversarial training for deep networks via smoothness-based surrogates, motivating the need for exact generalization guarantees for nonsmooth deep objectives addressed here."
    },
    {
      "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport",
      "authors": "M. Cuturi",
      "year": 2013,
      "arxiv_id": "1306.0895",
      "role": "Foundation",
      "relationship_sentence": "Introduced entropy-regularized optimal transport (Sinkhorn), forming the basis for the entropic OT robust models whose generalization guarantees are extended by this paper."
    }
  ],
  "synthesis_narrative": "Wasserstein distributionally robust optimization (DRO) was formalized with performance guarantees and tractable dual reformulations by Mohajerin Esfahani and Kuhn, who showed how worst-case risks over Wasserstein balls reduce to penalized objectives for Lipschitz losses. Blanchet and Murthy broadened this framework by grounding DRO in optimal transport duality with general transport costs, enabling robust modeling beyond standard Lp metrics. Gao and Kleywegt provided finite-sample out-of-sample guarantees by calibrating Wasserstein radii through concentration, but their approach hinges on assumptions that are difficult to verify and can be dimension-sensitive. In contrast, Blanchet, Kang, and Murthy\u2019s robust Wasserstein profile (RWP) theory revealed that the robust objective can enjoy n^{-1/2} behavior, suggesting dimension-free generalization, albeit via asymptotic or approximate analyses. Shafieezadeh-Abadeh, Kuhn, and Mohajerin Esfahani demonstrated exactness and tractability for a particular parametric loss (logistic regression) under Wasserstein DRO, indicating that exact guarantees are attainable in specific convex cases. Meanwhile, Sinha, Namkoong, and Duchi linked Wasserstein DRO to adversarial training for deep networks using smoothness proxies, highlighting the importance of handling nonsmooth, highly parametric objectives. Cuturi\u2019s entropy-regularized optimal transport established the Sinkhorn framework, which defines a practical class of entropic OT losses and robust models. Together, these works expose a gap: existing dimension-free insights are approximate or case-specific, and prior guarantees often rely on restrictive, hard-to-verify assumptions. The natural next step is a universal theory that operates with arbitrary transport costs and general parametric (including nonsmooth, deep) loss classes, yields exact generalization and excess-risk bounds, and seamlessly extends to entropic-regularized Wasserstein models\u2014precisely synthesizing the duality foundations, RWP insight, and deep-learning motivation from these prior works.",
  "target_paper": {
    "title": "Universal generalization guarantees for Wasserstein distributionally robust models",
    "authors": "Tam Le, Jerome Malick",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "generalization guarantees, optimal transport, distributionally robust optimization, nonsmooth analysis",
    "abstract": "Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that generalization guarantees of robust models based on the Wasserstein distance have generalization guarantees that do not suffer from the curse of dimensionality. However, these results are either approximate, obtained in specific cases, or based on assumptions difficult to verify in practice. In contrast, we establish exact generalization guarantees that cover a wide range of cases, with arbitrary transport costs and parametric loss functions, including deep learning objectives with nonsmooth activations. We complete our analysis with an excess bound on the robust objective and an extension to Wasserstein robust models with entropic regularizations.",
    "openreview_id": "0h6v4SpLCY",
    "forum_id": "0h6v4SpLCY"
  },
  "analysis_timestamp": "2026-01-06T10:14:40.966418"
}
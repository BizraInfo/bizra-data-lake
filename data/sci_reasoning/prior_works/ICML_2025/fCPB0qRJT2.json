{
  "prior_works": [
    {
      "title": "Design Space for Graph Neural Networks",
      "authors": "You et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "This work systematically showed that the best-performing GNN design choices vary widely across datasets and tasks, directly motivating AutoGFM\u2019s focus on resolving architecture inconsistency across domains."
    },
    {
      "title": "GraphNAS: Graph Neural Architecture Search with Reinforcement Learning",
      "authors": "Gao et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "GraphNAS established the problem and mechanisms of neural architecture search tailored to GNNs; AutoGFM extends this line by bringing GNAS into the foundation-model setting and across multiple domains/tasks."
    },
    {
      "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks",
      "authors": "Hu et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "GPT-GNN introduced large-scale pretraining for graphs (a precursor to GFMs) but relied on fixed backbones; AutoGFM inherits the pretraining paradigm while replacing hand-crafted, fixed architectures with automated, adaptive ones."
    },
    {
      "title": "GraphMAE: Masked Autoencoders for Graphs",
      "authors": "Hou et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "GraphMAE is a strong pretraining baseline built on a fixed GNN backbone; AutoGFM targets the core limitation by searching and customizing the backbone architecture per domain/task to surpass fixed-design GFMs."
    },
    {
      "title": "Graph Prompt Learning for Node Classification",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "Prompt-based graph adaptation keeps the backbone fixed; AutoGFM complements and goes beyond this by adapting the architecture itself, addressing cases where prompts alone cannot resolve backbone mismatch across domains."
    },
    {
      "title": "Invariant Risk Minimization",
      "authors": "Arjovsky et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "IRM\u2019s principle of learning invariant mechanisms across environments inspires AutoGFM\u2019s discovery of an invariant graph\u2013architecture relationship shared across diverse domains and tasks."
    },
    {
      "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search",
      "authors": "Chu et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "FairNAS identified and mitigated training bias in weight-sharing NAS; AutoGFM adapts fairness-aware sampling/optimization ideas to counter the data domination phenomenon during multi-domain architecture search."
    }
  ],
  "synthesis_narrative": "AutoGFM\u2019s central idea\u2014automatically customizing graph backbones for a graph foundation model across diverse domains\u2014sits at the intersection of GNN design, pretraining, and architecture search. The evidence for the need to adapt architectures comes from You et al., who demonstrated that optimal GNN design choices vary markedly across datasets, revealing the architecture inconsistency that AutoGFM targets. On the mechanism side, GraphNAS established GNN-specific NAS, which AutoGFM advances by scaling NAS into a foundation-model setting that must serve many domains and tasks. The GFM/pretraining lineage is anchored by GPT-GNN and strong pretraining baselines like GraphMAE: both validate the efficacy of large-scale graph pretraining yet rely on fixed, hand-crafted backbones\u2014exactly the constraint that AutoGFM removes via automated, task/domain-aware customization. While prompt-based adaptation methods (e.g., Graph Prompt Learning) show that non-architectural adaptation can transfer knowledge across tasks, their fixed backbones leave performance on mismatched domains capped; AutoGFM complements this by adapting the architecture itself. To make adaptation stable across domains, AutoGFM draws inspiration from IRM, seeking invariant graph\u2013architecture relations that generalize across environments. Finally, because weight-sharing NAS can be biased, FairNAS\u2019s fairness-aware training informs AutoGFM\u2019s strategy to mitigate data domination during multi-domain supernet training, ensuring balanced optimization of candidate architectures.",
  "analysis_timestamp": "2026-01-06T23:07:19.638386"
}
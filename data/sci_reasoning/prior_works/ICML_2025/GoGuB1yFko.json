{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "AMCN relies on CLIP\u2019s image\u2013text alignment and promptable text embeddings so that learnable ID/OOD textual prompts can serve as decision anchors in lieu of abundant OOD or ID images."
    },
    {
      "title": "Learning to Prompt for Vision-Language Models",
      "authors": "Kaiyang Zhou et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "CoOp introduced learnable textual context vectors for CLIP from few labeled samples; AMCN extends this idea by jointly learning multiple adaptive prompts and, critically, learning both ID and OOD prompts to shape the ID\u2013OOD boundary."
    },
    {
      "title": "Conditional Prompt Learning for Vision-Language Models",
      "authors": "Kaiyang Zhou et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "CoCoOp\u2019s instance-conditioned prompting motivates AMCN\u2019s adaptive, class-aware multi-prompt mechanism to capture inter- and intra-class diversity when separating ID from OOD."
    },
    {
      "title": "MaPLe: Multi-modal Prompt Learning",
      "authors": "Muhammad Uzair Khattak et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "MaPLe shows that using multiple prompts across modalities improves CLIP adaptation; AMCN generalizes the multi-prompt design to model class distributions and to learn complementary OOD prompts that explicitly push away non-ID regions."
    },
    {
      "title": "Supervised Contrastive Learning",
      "authors": "Prannay Khosla et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "AMCN\u2019s training objective mirrors supervised contrastive principles\u2014pulling images toward class-specific (ID) prompts and repelling them from OOD prompts\u2014to learn inter-class separation and intra-class compactness under few-shot constraints."
    },
    {
      "title": "Energy-based Out-of-distribution Detection",
      "authors": "Weitang Liu et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Energy-based OOD detection is strong but typically requires substantial ID data and model retraining; AMCN explicitly addresses this limitation by operating in CLIP space with few-shot prompt learning and no real OOD samples."
    },
    {
      "title": "Prototypical Networks for Few-shot Learning",
      "authors": "Jake Snell et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "AMCN\u2019s modeling of per-class distributions from few labeled examples echoes prototype-based few-shot learning, replacing vector prototypes with multiple learned textual prompts that act as class-conditioned centers and margins for OOD separation."
    }
  ],
  "synthesis_narrative": "AMCN\u2019s core idea\u2014adapting an ID\u2013OOD decision boundary in a few-shot regime by learning multiple class-aware textual prompts and training them contrastively\u2014emerges at the intersection of vision\u2013language modeling, prompt learning, and few-shot representation learning. CLIP established the foundation by aligning images and text in a shared space, making textual prompts viable as class descriptors and, crucially, as surrogates for missing OOD samples. Building on CLIP, CoOp showed that learnable textual contexts can be optimized from few labeled examples, a capability AMCN extends by learning not only multiple ID prompts per class but also complementary OOD prompts to explicitly carve out non-ID regions. CoCoOp\u2019s conditional prompting inspired AMCN\u2019s adaptive treatment of inter- and intra-class diversity, while MaPLe\u2019s demonstration that multi-prompt strategies enhance CLIP adaptation directly informed AMCN\u2019s multi-prompt architecture for distribution modeling. The training dynamics of AMCN are rooted in supervised contrastive learning, which supplies the principle of pulling images toward their class prompts while pushing away OOD prompts to enforce compactness and separation in feature space. Finally, prevailing OOD approaches such as energy-based methods highlight a key gap\u2014dependence on abundant ID data and retraining\u2014which AMCN addresses by operating data-efficiently in CLIP\u2019s space. Prototype-based few-shot learning further influenced AMCN\u2019s use of prompts as class-conditioned centers and margins, enabling few-shot, class-aware boundary adaptation.",
  "analysis_timestamp": "2026-01-06T23:07:19.640315"
}
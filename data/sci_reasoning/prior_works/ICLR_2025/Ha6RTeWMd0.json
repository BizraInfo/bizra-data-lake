{
  "prior_works": [
    {
      "title": "Segment Anything",
      "authors": "Kirillov et al.",
      "year": 2023,
      "role": "Predecessor: promptable segmentation and data-engine paradigm",
      "relationship_sentence": "SAM 2 directly extends SAM\u2019s promptable mask prediction and interactive data-engine approach, redesigning the architecture for higher speed and adding temporal capability to bring prompting from images to videos."
    },
    {
      "title": "Space-Time Memory Networks for Video Object Segmentation (STM)",
      "authors": "Oh, Lee, Xu, Kim",
      "year": 2019,
      "role": "Core mechanism: key\u2013value memory for mask propagation",
      "relationship_sentence": "SAM 2\u2019s streaming memory echoes STM\u2019s key\u2013value memory mechanism for propagating segmentation signals across frames, but integrates it natively into a transformer and supports interactive prompt updates."
    },
    {
      "title": "Associating Objects with Transformers for Video Object Segmentation (AOT)",
      "authors": "Yang, Wei, Yang",
      "year": 2021,
      "role": "Architectural inspiration: transformer-based association with memory",
      "relationship_sentence": "SAM 2 inherits the idea of using transformer attention to associate current-frame tokens with a memory of past frames/masks, scaling it to real-time, multi-object, promptable segmentation."
    },
    {
      "title": "MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition",
      "authors": "Fan et al.",
      "year": 2022,
      "role": "Streaming efficiency: persistent memory/state in video transformers",
      "relationship_sentence": "SAM 2 adopts the principle of persistent memory/state from MeMViT to enable low-latency streaming inference over long videos while preserving temporal context."
    },
    {
      "title": "Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation",
      "authors": "Cheng et al.",
      "year": 2022,
      "role": "Decoder design: mask transformer for dense prediction",
      "relationship_sentence": "SAM 2\u2019s simple mask-transformer head and masked-attention-style decoding are influenced by Mask2Former\u2019s unified transformer formulation for high-quality, general-purpose segmentation."
    },
    {
      "title": "Deep Extreme Cut (DEXTR): From Extreme Points to Object Segmentation",
      "authors": "Maninis, Pont-Tuset, Caelles, Van Gool",
      "year": 2018,
      "role": "Prompting interface: point/box-based interactive segmentation",
      "relationship_sentence": "SAM 2\u2019s point/box prompts and interactive protocol trace back to DEXTR\u2019s extreme-point prompting, now extended from images to videos with dramatically fewer user interactions."
    }
  ],
  "synthesis_narrative": "SAM 2\u2019s core contribution\u2014promptable segmentation that seamlessly spans images and videos with real-time streaming memory\u2014emerges from three converging lines of prior work. First, Segment Anything (SAM) established promptable mask prediction and a scalable data-engine pipeline; SAM 2 generalizes this paradigm to time by retaining flexible prompts while collecting a massive video segmentation corpus via interactive annotation. Second, the video object segmentation community contributed the essential idea of memory-based temporal propagation: STM introduced key\u2013value memories to carry segmentation cues across frames, while AOT demonstrated how transformers can associate current-frame tokens with stored memories for multi-object tracking and segmentation. SAM 2 internalizes these ideas into a streamlined, end-to-end transformer equipped with a streaming memory that updates with user prompts and past frames, enabling accuracy with far fewer interactions. Third, advances in transformer design for dense prediction and efficient video processing shaped SAM 2\u2019s architecture: Mask2Former\u2019s masked-attention mask decoding influenced a simple, general segmentation head, and MeMViT\u2019s persistent memory/state provided a blueprint for low-latency, long-horizon video processing. Finally, the interactive prompting tradition from DEXTR furnished the human-in-the-loop interface\u2014points and boxes\u2014that SAM 2 leverages across both modalities. Together, these works directly underwrite SAM 2\u2019s scalable data engine, promptable transformer with streaming memory, and strong accuracy-speed trade-off in images and videos.",
  "analysis_timestamp": "2026-01-06T23:42:48.101158"
}
{
  "prior_works": [
    {
      "title": "Measuring Massive Multitask Language Understanding",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "MMAU adopts MMLU\u2019s exam-style, multi-discipline evaluation paradigm for testing expert knowledge and complex reasoning, but grounds it in the audio modality with QA over clips."
    },
    {
      "title": "MMMU: A Massive Multi-discipline Multimodal Understanding Benchmark for AGI",
      "authors": "Xinyu Yue et al.",
      "year": 2024,
      "role": "Inspiration",
      "relationship_sentence": "MMAU mirrors MMMU\u2019s emphasis on expertise-oriented, cross-domain reasoning and extends that blueprint to audio, creating an audio counterpart focused on perception plus reasoning."
    },
    {
      "title": "SUPERB: Speech processing Universal PERformance Benchmark",
      "authors": "Shu-wen Yang et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "SUPERB established unified speech benchmarks but centers on task-specific speech metrics; MMAU addresses this gap by unifying speech with non-speech sounds and music in a QA framework that explicitly tests reasoning skills."
    },
    {
      "title": "HEAR 2021: Holistic Evaluation of Audio Representations",
      "authors": "Chris Donahue Turian et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "HEAR provided a multi-task audio evaluation for representations, but emphasized recognition and embeddings; MMAU directly responds by evaluating natural-language audio understanding with information extraction and expert reasoning across 27 skills."
    },
    {
      "title": "AudioSet: An ontology and human-labeled dataset for audio events",
      "authors": "Jort F. Gemmeke et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "AudioSet\u2019s broad ontology and coverage of environmental sounds underpins MMAU\u2019s multi-domain scope; MMAU builds on this foundation by moving from event tagging to knowledge-intensive QA and reasoning."
    },
    {
      "title": "Clotho: An Audio Captioning Dataset",
      "authors": "Konstantinos Drossos et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Clotho established audio-to-language grounding via captions; MMAU builds on this audio\u2013text formulation but advances to question answering that requires compositional understanding and domain-specific reasoning."
    }
  ],
  "synthesis_narrative": "MMAU\u2019s core innovation\u2014an expert-level, multi-task audio understanding and reasoning benchmark\u2014draws directly on the exam-style paradigm of MMLU and its multimodal extension MMMU. These works crystallized the idea that evaluating advanced AI requires diverse, high-difficulty questions spanning multiple disciplines; MMAU transposes this template to audio, where perception must be integrated with domain knowledge and multi-step reasoning. On the audio side, SUPERB and HEAR 2021 established comprehensive, unified evaluations, but primarily for speech processing and representation quality on recognition-style tasks. MMAU explicitly addresses their limitations by adopting a natural-language QA format that stresses information extraction, compositional reasoning, and expert knowledge across speech, environmental sounds, and music, rather than siloed metrics per task. Foundationally, AudioSet\u2019s ontology and large-scale coverage of non-speech environmental sounds\u2014and Clotho\u2019s audio-to-text grounding\u2014enabled the very notion of standardized, language-based evaluation for audio. MMAU builds on this base and reframes evaluation from labeling or captioning to rigorous question answering that probes 27 distinct skills under realistic, expert-level challenges. In essence, MMAU fuses the multi-discipline reasoning ethos of MMLU/MMMU with the audio task ecosystems of SUPERB/HEAR and AudioSet/Clotho, creating a benchmark that directly tests whether modern audio-language models can truly listen, understand, and reason.",
  "analysis_timestamp": "2026-01-06T23:09:26.605360"
}
{
  "prior_works": [
    {
      "title": "The Generalized Context Model: An Exemplar-Based Account of Categorization",
      "authors": "Robert M. Nosofsky",
      "year": 1986,
      "role": "Foundational theory introducing exemplar-based categorization, where categories are represented by stored examples and similarity in a learned space drives decisions.",
      "relationship_sentence": "GnnXemplar explicitly instantiates exemplar theory by selecting representative nodes in the GNN embedding space and explaining class decisions via their neighborhoods."
    },
    {
      "title": "\u201cWhy Should I Trust You?\u201d Explaining the Predictions of Any Classifier (and Submodular Pick for Global Insight)",
      "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin",
      "year": 2016,
      "role": "Introduced instance-level explanations (LIME) and a submodular pick strategy to select a small set of representative examples that maximally cover important features for global understanding.",
      "relationship_sentence": "GnnXemplar\u2019s exemplar selection mirrors SP-LIME\u2019s coverage-based selection, but operationalizes coverage in the GNN latent space via reverse k-NN sets to yield a faithful global summary."
    },
    {
      "title": "Prototypical Networks for Few-shot Learning",
      "authors": "Jake Snell, Kevin Swersky, Richard S. Zemel",
      "year": 2017,
      "role": "Popularized metric-learning in embedding space for classification via proximity to class representatives.",
      "relationship_sentence": "GnnXemplar leverages the same metric-space intuition\u2014decisions become geometry in the learned embedding\u2014but uses actual exemplars (rather than centroids) to preserve interpretability and trace back to real nodes."
    },
    {
      "title": "Anchors: High-Precision Model-Agnostic Explanations",
      "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin",
      "year": 2018,
      "role": "Proposed rule-based, high-precision if\u2013then explanations that are easy to verbalize as natural language.",
      "relationship_sentence": "GnnXemplar derives concise natural language rules around exemplars, aligning with Anchors\u2019 principle of high-precision rule statements to characterize model behavior."
    },
    {
      "title": "XGNN: Towards Model-Level Explanations of Graph Neural Networks",
      "authors": "Hao Yuan, Haiyang Yu, Shurui Gui, Shuiwang Ji",
      "year": 2020,
      "role": "Pioneered global explanations for GNNs by synthesizing graph motifs that maximize class scores.",
      "relationship_sentence": "GnnXemplar addresses XGNN\u2019s limitation of motif reliance by replacing motif discovery with exemplar coverage in embedding space, enabling scalability to large, attribute-rich graphs."
    },
    {
      "title": "Reverse Nearest Neighbor Search in Metric Spaces",
      "authors": "Flip Korn, S. Muthukrishnan",
      "year": 2000,
      "role": "Introduced the concept and algorithms for reverse k-nearest neighbors (RkNN), identifying points for which a query is among their nearest neighbors.",
      "relationship_sentence": "GnnXemplar frames exemplar selection as maximizing coverage over RkNN sets in the GNN embedding, directly building on the RkNN concept to quantify representativeness."
    },
    {
      "title": "An analysis of approximations for maximizing submodular set functions",
      "authors": "George L. Nemhauser, Laurence A. Wolsey, Marshall L. Fisher",
      "year": 1978,
      "role": "Established the (1 \u2212 1/e)-approximation guarantee of the greedy algorithm for monotone submodular maximization under cardinality constraints.",
      "relationship_sentence": "GnnXemplar\u2019s greedy selection for exemplar coverage inherits the classical (1 \u2212 1/e) guarantee by casting the objective as a monotone submodular function over RkNN coverage."
    }
  ],
  "synthesis_narrative": "GnnXemplar fuses three intellectual threads to deliver scalable, global interpretability for GNNs. First, it operationalizes exemplar theory (Nosofsky, 1986) in a modern representation-learning setting, selecting real nodes in the GNN embedding space as the basis for class-level explanation. This choice is grounded in the metric-learning paradigm of Prototypical Networks (Snell et al., 2017), leveraging geometric structure in latent space to tie class behavior to neighborhoods of representative instances rather than synthetic patterns. Second, it adopts a principled, coverage-driven selection strategy inspired by submodular pick in LIME (Ribeiro et al., 2016), but adapts it to graphs by defining coverage via reverse k-nearest neighbor (RkNN) sets (Korn & Muthukrishnan, 2000). This allows the method to explicitly quantify how many nodes each exemplar \u201cexplains\u201d in embedding space. By casting the objective as a monotone submodular function, GnnXemplar applies a greedy algorithm with the classical (1\u22121/e) guarantee (Nemhauser et al., 1978), yielding an efficient and theoretically sound selection routine. Third, to communicate model behavior, the approach converts exemplar neighborhoods into high-precision, human-readable rules, aligning with the philosophy of Anchors (Ribeiro et al., 2018) for concise natural language explanations. Together, these design choices address the shortcomings of motif-centric global explainers such as XGNN (Yuan et al., 2020), enabling robust global summaries in large, attribute-rich graphs where repeated motifs are rare and predictions hinge on nuanced structure\u2013attribute interactions.",
  "analysis_timestamp": "2026-01-07T00:02:04.917020"
}
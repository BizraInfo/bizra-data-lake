{
  "prior_works": [
    {
      "title": "Curriculum Learning",
      "authors": "Yoshua Bengio et al.",
      "year": 2009,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Introduces the principle of organizing training from simple to complex, which this work instantiates as a progressive curriculum over compositional difficulty for text-to-image models."
    },
    {
      "title": "Winoground: Probing Multimodal Compositionality in Vision-Language Models",
      "authors": "Andrew Thrush et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Establishes minimal contrastive pairs to test fine-grained compositional understanding, directly motivating the construction of minimally different image pairs used for contrastive training."
    },
    {
      "title": "TIFA: Accurate Text-to-Image Faithfulness Evaluation with Question Answering",
      "authors": "Yushi Hu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Shows that VQA pipelines can automatically verify whether generated images satisfy textual claims, enabling this paper\u2019s use of VQA checkers to curate and label contrastive image pairs."
    },
    {
      "title": "GLIGEN: Open-Set Grounded Text-to-Image Generation",
      "authors": "Jianfeng Li et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Relies on externally provided layouts/boxes to enforce composition, highlighting the limitation of fixed, prespecified structure that this work avoids via a structure-agnostic, curriculum approach."
    },
    {
      "title": "Attend-and-Excite: Attention-Based Methods for Object-Attribute Binding in Text-to-Image Generation",
      "authors": "Hila Chefer et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Improves attribute/object binding through attention manipulation but presupposes token-level architectural control, motivating a data-driven training curriculum that generalizes beyond such fixed mechanisms."
    },
    {
      "title": "Compositional Visual Generation with Composable Diffusion Models",
      "authors": "Xiang Lisa Li et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Combines textual components by composing diffusion conditionals to control objects and attributes, providing a key baseline whose limitations on complex, natural compositions this work targets with progressive contrastive training."
    }
  ],
  "synthesis_narrative": "Curriculum learning established that models benefit from sequencing training examples from easy to hard, enabling the acquisition of complex concepts through staged exposure to increasing difficulty. Winoground introduced a stringent test of visio-linguistic compositionality using minimal contrastive pairs, where tiny textual changes imply distinct visual arrangements, crystallizing the need for fine-grained, contrastive supervision. TIFA showed that visual question answering can automatically assess whether generated images satisfy compositional textual claims, demonstrating a practical pipeline for scalable, programmatic verification without costly human annotation. GLIGEN enforced compositionality by conditioning on layouts and boxes, delivering control but requiring prespecified structure that can hinder open-set generalization. Attend-and-Excite improved attribute and object binding by manipulating attention maps, yet relies on architectural control at the token level, which does not directly teach broad, data-driven compositional understanding. Composable diffusion models proposed composing textual conditionals to render multi-object, attribute-rich scenes, but their fixed textual factorization struggles with complex, naturalistic compositions and relations.\nTogether, these strands reveal a gap: scalable, structure-agnostic training signals that explicitly teach compositional distinctions without hand-crafted architectures or fixed factorization. Leveraging VQA\u2019s automatic verification enables mining reliable, minimally different positive\u2013negative image pairs, while the minimal-pair insight from Winoground clarifies how to focus learning on critical compositional contrasts. Sequencing these contrasts from simple attribute\u2013object bindings to multi-object relations naturally operationalizes curriculum learning. Building on, and addressing limitations of, composable and attention-guided baselines, this synthesis yields a progressive, contrastive training regimen that equips diffusion models with robust, generalizable compositional understanding in complex, real-world prompts.",
  "target_paper": {
    "title": "Progressive Compositionality in Text-to-Image Generative Models",
    "authors": "Xu Han, Linghao Jin, Xiaofeng Liu, Paul Pu Liang",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "compositional text-to-image generation, contrastive learning, compositional understanding, T2I generation",
    "abstract": "Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing approaches through building compositional architectures or generating difficult negative captions often assume a fixed prespecified compositional structure, which limits generalization to new distributions. In this paper, we argue that curriculum training is crucial to equipping generative models with a fundamental understanding of compositionality. To achieve this, we leverage large-language models (LLMs) to automatically compose complex scenarios and harness Visual-Question Answering (VQA) checkers to automatically curate a contrastive dataset, ConPair, consisting of 15k pairs of high-quality contrastive images. These pairs feature minimal visual discrepancies and cover a wide range of attribute categories, especially complex and natural scenarios. To learn effectiv",
    "openreview_id": "S85PP4xjFD",
    "forum_id": "S85PP4xjFD"
  },
  "analysis_timestamp": "2026-01-06T11:24:20.579965"
}
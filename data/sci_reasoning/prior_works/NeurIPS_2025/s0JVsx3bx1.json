{
  "prior_works": [
    {
      "title": "Universal Value Function Approximators",
      "authors": "Tom Schaul, Daniel Horgan, Karol Gregor, David Silver",
      "year": 2015,
      "role": "Algorithmic foundation for goal-conditioned RL",
      "relationship_sentence": "Established conditioning value/policy functions on goal vectors; the present work retains this UVFA formulation but demonstrates that scaling network depth (via very deep residual MLPs) unlocks substantially better goal-reaching under self-supervision."
    },
    {
      "title": "Hindsight Experience Replay",
      "authors": "Marcin Andrychowicz et al.",
      "year": 2017,
      "role": "Learning signal for sparse goal-reaching",
      "relationship_sentence": "Pioneered relabeling techniques that make goal-conditioned learning feasible without dense rewards; the new paper operates in a similar no-reward setting and contrasts its deep, self-supervised approach against HER-style baselines."
    },
    {
      "title": "Reinforcement Learning with Imagined Goals (RIG)",
      "authors": "Ashvin Nair et al.",
      "year": 2018,
      "role": "Unsupervised goal-conditioned RL method",
      "relationship_sentence": "Introduced learning to reach visually specified goals via latent goal spaces and no external rewards; the current work targets the same unsupervised goal-reaching regime but replaces VAE-based goal modeling with a contrastive objective and shows depth scaling dramatically improves performance."
    },
    {
      "title": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
      "authors": "Aravind Srinivas, Michael Laskin, Pieter Abbeel",
      "year": 2020,
      "role": "RL-specific contrastive learning backbone",
      "relationship_sentence": "Demonstrated InfoNCE-style contrastive objectives can yield strong representations for RL; the present paper scales a contrastive goal-conditioned RL algorithm to extreme depth, showing that deeper networks yield 2\u00d7\u201350\u00d7 gains."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",
      "year": 2020,
      "role": "Self-supervised contrastive objective design",
      "relationship_sentence": "Provided the practical recipe (augmentations, InfoNCE, temperature) that underlies many contrastive RL losses; the new work leverages this contrastive machinery and shows that scaling depth, not just width or data, is a key driver in the RL setting."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "year": 2016,
      "role": "Architectural enabler for 1000+ layer networks",
      "relationship_sentence": "Introduced residual connections that make optimization of very deep networks feasible; the paper adapts residual block design and initialization to stabilize 1,024-layer networks within off-policy goal-conditioned RL."
    },
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, et al.",
      "year": 2020,
      "role": "Motivation and methodology for systematic scaling",
      "relationship_sentence": "Showed predictable performance improvements from scaling model size/data in self-supervised learning; the new work transposes this scaling perspective to RL and isolates depth as the critical axis, conducting ablations analogous to scaling-law studies."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014showing that scaling depth to 1,024 layers can unlock new goal-reaching capabilities in self-supervised RL\u2014stands at the intersection of goal-conditioned RL, contrastive self-supervision, and deep network optimization. UVFA provided the fundamental formulation for conditioning value functions and policies on goals, while HER and RIG established that meaningful goal-reaching can be learned without dense external rewards via relabeling or latent goal spaces. These works define the learning problem and baseline practices in the no-reward, goal-conditioned regime the paper targets.\nOn the representation-learning side, SimCLR and CURL operationalized InfoNCE-based contrastive learning for perception and RL, respectively, giving a mature, scalable self-supervised objective that this paper adopts as the backbone for goal-conditioned credit assignment. The novelty here is not inventing a new contrastive loss, but demonstrating that the capacity and depth of the function approximator are pivotal for performance in this regime.\nCrucially, the feasibility of training 1000-layer networks derives from residual architectures introduced by ResNets, whose skip connections and initialization strategies stabilize optimization at extreme depths. Finally, inspired by scaling-law methodology from language modeling, the paper systematically probes depth as a scaling axis in RL, revealing predictable gains and emergent capabilities when depth is expanded well beyond the 2\u20135 layer norms. Together, these threads directly inform the paper\u2019s design and validate its central claim: depth scaling is a first-class lever for self-supervised, goal-conditioned RL.",
  "analysis_timestamp": "2026-01-07T00:21:32.253697"
}
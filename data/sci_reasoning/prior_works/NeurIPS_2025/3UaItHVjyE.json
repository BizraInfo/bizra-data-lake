{
  "prior_works": [
    {
      "title": "The Implicit Bias of Structured State Space Models",
      "authors": "Yotam Alexander, Yonatan Slutzky, Noam Razin, Nadav Cohen",
      "year": 2024,
      "role": "Baseline theory this paper overturns/refines",
      "relationship_sentence": "Established that gradient descent on SSMs trained on data from a low-dimensional teacher tends to recover low-order dynamics and generalize, providing the positive guarantee that the present work shows can be broken by carefully chosen, clean-labeled examples."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces (S4)",
      "authors": "Albert Gu, Karan Goel, Christopher R\u00e9",
      "year": 2022,
      "role": "Architectural foundation for SSMs",
      "relationship_sentence": "Introduced the modern SSM formulation used widely in practice, giving the model class whose optimization landscape and implicit bias are analyzed and stress-tested by the current paper."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "authors": "Albert Gu, Tri Dao",
      "year": 2024,
      "role": "Contemporary SSM variant and empirical testbed",
      "relationship_sentence": "Provided a high-performance SSM instantiation, motivating the practical importance of understanding SSM implicit bias and serving as a platform where the paper\u2019s clean-label poisoning phenomenon is empirically demonstrated."
    },
    {
      "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks",
      "authors": "Suriya Gunasekar, Jason D. Lee, Daniel Soudry, Nati Srebro",
      "year": 2018,
      "role": "Methodological blueprint linking parameterization to norms",
      "relationship_sentence": "Showed how architecture induces specific implicit norms under gradient descent; the paper leverages analogous reasoning for SSMs (which realize structured convolutions) to identify how special samples can redirect the induced bias."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry, Elad Hoffer, Mor Shpigel, Nathan Srebro",
      "year": 2018,
      "role": "Foundational implicit bias result for GD dynamics",
      "relationship_sentence": "Provided core analytical tools for characterizing gradient descent attractors, conceptually underpinning the present analysis of how training data content steers SSM solutions."
    },
    {
      "title": "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks",
      "authors": "Amir Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, Tom Goldstein",
      "year": 2018,
      "role": "Precedent for clean-label poisoning as a phenomenon",
      "relationship_sentence": "Demonstrated that adding a small number of clean-labeled points can derail learning; the paper provides a rigorous, mechanism-level counterpart in SSMs by proving clean labels can corrupt implicit bias itself."
    },
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh, Percy Liang",
      "year": 2017,
      "role": "Tooling to reason about the effect of individual samples",
      "relationship_sentence": "Showed how specific training examples can steer model solutions, informing the paper\u2019s construction and interpretation of \u2018special\u2019 clean examples that deflect SSM implicit bias away from generalizing solutions."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a proof that the implicit bias of gradient descent in structured state space models (SSMs) can be derailed by the inclusion of a few strategically chosen, teacher-labeled examples\u2014rests on three converging lines of prior work. First, architectural advances in SSMs, inaugurated by S4 and extended by Mamba, defined the modern, convolutional-like state-space layers whose learning dynamics and inductive priors warrant careful study. Second, the SSM-specific positive guarantee, articulated in prior work on the implicit bias of SSMs in the low-dimensional teacher setting, argued that gradient descent preferentially discovers low-order dynamics that generalize. This paper revisits that exact setting and shows the guarantee is fragile: particular clean examples can redirect the optimization trajectory to a non-generalizing solution.\nThird, general implicit bias theory\u2014Soudry et al. on maximum-margin dynamics and Gunasekar et al. linking architectural parameterizations to implicit norms\u2014provides the analytical lens for understanding how model structure plus gradient descent select among interpolating solutions. The clean-label poisoning literature (e.g., Poison Frogs) and influence-function analyses supply the conceptual precedent that a handful of clean points can systematically steer learned models. Synthesizing these strands, the authors pinpoint and formalize a previously unobserved mechanism in SSMs: special, correctly labeled samples can corrupt the architecture-induced bias that otherwise promotes generalization, yielding rigorous theorems and empirical evidence across standalone SSMs and hybrid non-linear networks.",
  "analysis_timestamp": "2026-01-07T00:21:32.282913"
}
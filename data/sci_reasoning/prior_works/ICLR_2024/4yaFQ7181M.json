{
  "prior_works": [
    {
      "title": "Neural Ordinary Differential Equations",
      "authors": "Ricky T. Q. Chen et al.",
      "year": 2018,
      "arxiv_id": "1806.07366",
      "role": "Foundation",
      "relationship_sentence": "Introduced continuous-time neural dynamics that can be integrated at arbitrary time points, directly enabling the paper\u2019s time-continuous forecasting component of its coupled dynamical systems."
    },
    {
      "title": "Neural Controlled Differential Equations for Irregular Time Series",
      "authors": "Patrick Kidger et al.",
      "year": 2020,
      "arxiv_id": "2005.08926",
      "role": "Inspiration",
      "relationship_sentence": "Provided a principled way to drive latent dynamics with irregular, sparse observation streams, which motivates the paper\u2019s dynamical system defined on sparse sensor positions."
    },
    {
      "title": "Conditional Neural Processes",
      "authors": "Marta Garnelo et al.",
      "year": 2018,
      "arxiv_id": "1807.01613",
      "role": "Inspiration",
      "relationship_sentence": "Pioneered conditioning on sparse context points to predict function values at arbitrary query inputs, a paradigm the paper adapts to map sparse observations to continuous space-time field predictions."
    },
    {
      "title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
      "authors": "Lu Lu et al.",
      "year": 2021,
      "arxiv_id": "1910.03193",
      "role": "Foundation",
      "relationship_sentence": "Established a coordinate-query (trunk) and conditioning-function (branch) decomposition for operator learning, informing the paper\u2019s coordinate-conditioned decoder for continuous spatial predictions from learned dynamics."
    },
    {
      "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
      "authors": "Zongyi Li et al.",
      "year": 2021,
      "arxiv_id": "2010.08895",
      "role": "Baseline",
      "relationship_sentence": "Serves as a main operator-learning baseline tied to grid-based representations, whose fixed computational support the paper explicitly overcomes with continuous space-time predictions from sparse data."
    },
    {
      "title": "Learning Mesh-Based Simulation with Graph Networks",
      "authors": "Tobias Pfaff et al.",
      "year": 2021,
      "arxiv_id": "2010.03409",
      "role": "Gap Identification",
      "relationship_sentence": "Showed strong learned simulators on fixed meshes, highlighting the limitation of mesh-tied supports that the paper addresses by decoupling predictions from any discretization."
    },
    {
      "title": "Learning to Simulate Complex Physics with Graph Networks",
      "authors": "Alvaro Sanchez-Gonzalez et al.",
      "year": 2020,
      "arxiv_id": "2002.09405",
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated particle/graph-based simulators with fixed supports, whose inability to query arbitrary points in space-time motivates the paper\u2019s continuous-domain formulation."
    }
  ],
  "synthesis_narrative": "Neural Ordinary Differential Equations introduced the idea of parameterizing dynamics as continuous flows that can be integrated at arbitrary times, establishing a foundation for time-continuous forecasting. Neural Controlled Differential Equations extended this to settings with sparse, irregular observations by treating the data stream as a control path driving latent dynamics, offering a principled mechanism for assimilation from partial observations. Conditional Neural Processes showed how to condition on a set of context points and produce function values at arbitrary targets, formalizing learning from sparse samples to continuous function predictions. DeepONet framed operator learning via a branch network for conditioning inputs and a trunk network for coordinate queries, enabling evaluation anywhere in the domain and separating conditioning from spatial coordinates. In contrast, Fourier Neural Operators advanced data-driven PDE surrogates but remained tied to grid-based computations despite multi-resolution generalization. Likewise, MeshGraphNets and graph-network simulators achieved strong performance yet operate on fixed meshes or particle supports, inherently constraining predictions to discretization points. Together these works reveal the opportunity to combine continuous-time latent dynamics with coordinate-conditioned decoders, while removing reliance on fixed supports and enabling learning from sparse measurements. Synthesizing these insights, the paper interlinks a dynamics on sparse observation sites (for assimilation) with a dynamics/decoder defined over continuous coordinates, adopting the context-to-target conditioning of neural processes and the coordinate-querying of DeepONet, but driven by continuous-time flows \u00e0 la Neural ODE/CDE, thereby enabling space-time continuous fluid simulation from partial observations and addressing the fixed-support limitations of FNO and graph-based simulators.",
  "target_paper": {
    "title": "Space and time continuous physics simulation from partial observations",
    "authors": "Steeven JANNY, Madiha Nadri, Julie Digne, Christian Wolf",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Physics, simulation, interpolation",
    "abstract": "Modern techniques for physical simulations rely on numerical schemes and mesh-refinement methods to address trade-offs between precision and complexity, but these handcrafted solutions are tedious and require high computational power. Data-driven methods based on large-scale machine learning promise high adaptivity by integrating long-range dependencies more directly and efficiently. In this work, we focus on computational fluid dynamics and address the shortcomings of a large part of the literature, which are based on fixed support for computations and predictions in the form of regular or irregular grids. We propose a novel setup to perform predictions in a continuous spatial and temporal domain while being trained on sparse observations. We formulate the task as a double observation problem and propose a solution with two interlinked dynamical systems defined on, respectively, the sparse positions and the continuous domain, which allows to forecast and interpolate a solution from th",
    "openreview_id": "4yaFQ7181M",
    "forum_id": "4yaFQ7181M"
  },
  "analysis_timestamp": "2026-01-07T00:24:45.792503"
}
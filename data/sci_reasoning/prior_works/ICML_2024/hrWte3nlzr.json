{
  "prior_works": [
    {
      "title": "Constrained Markov Decision Processes",
      "authors": "Eitan Altman",
      "year": 1999,
      "role": "Foundation",
      "relationship_sentence": "Established the CMDP formalism (occupancy-measure LP and Lagrangian duality) that this paper\u2019s primal\u2013dual treatment and constraint handling build upon."
    },
    {
      "title": "Near-Optimal Regret Bounds for Reinforcement Learning",
      "authors": "Thomas Jaksch et al.",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "Introduced the UCRL2 optimistic model-based framework and confidence-set construction that the proposed model-based primal\u2013dual algorithm adopts to explore unknown CMDPs."
    },
    {
      "title": "Online Convex Optimization with Long-Term Constraints",
      "authors": "Atousa Mahdavi et al.",
      "year": 2012,
      "role": "Gap Identification",
      "relationship_sentence": "Pioneered the standard long-term constraint metric allowing cumulative error cancellations; the present work explicitly removes this allowance by proving sublinear regret without cancellations."
    },
    {
      "title": "Online Convex Optimization in MDPs",
      "authors": "Aviv Rosenberg et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Provided the convex-analytic/occupancy-measure OCO view for MDPs that underlies the paper\u2019s regularized primal\u2013dual updates and analysis for CMDPs."
    },
    {
      "title": "Exploration-Exploitation in Constrained MDPs",
      "authors": "Yonathan Efroni et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Identified that existing primal\u2013dual CMDP methods achieve guarantees only via error cancellations and posed the open question of obtaining sublinear regret without them\u2014precisely resolved here."
    },
    {
      "title": "Upper Confidence Reinforcement Learning in Constrained MDPs",
      "authors": "Shoubo Qiu et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "A state-of-the-art optimistic model-based primal\u2013dual CMDP algorithm achieving sublinear regret with cancellation; the new method improves by guaranteeing truly no-regret without cancellations."
    },
    {
      "title": "Training GANs with Optimism",
      "authors": "Constantinos Daskalakis et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "Showed last-iterate convergence of regularized/optimistic primal\u2013dual dynamics in saddle-point problems; this paper generalizes such last-iterate convergence ideas to CMDPs with multiple constraints."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014achieving sublinear regret in constrained MDPs without allowing error cancellations\u2014rests on three intertwined lines of work. First, Altman\u2019s monograph established the CMDP formalism and Lagrangian/occupancy-measure LP tools that make primal\u2013dual handling of constraints natural. Building on this convex-analytic perspective, Rosenberg and Mansour framed MDP learning as online convex optimization over occupancy measures, providing the vehicle by which regularized primal\u2013dual dynamics can be analyzed in control settings. For exploration in unknown dynamics, the model-based optimism and confidence sets of UCRL2 (Jaksch et al.) directly inform the proposed optimistic primal\u2013dual learner. \nSecond, the paper directly responds to limitations in long-term constraint literature: Mahdavi et al. introduced the canonical formulation where cumulative constraint violations can cancel across rounds, a feature inherited by CMDP algorithms. Efroni et al. crystallized this as a safety gap in CMDPs and posed the open question of whether primal\u2013dual methods can achieve sublinear regret without cancellations\u2014a challenge this work resolves. \nThird, on the algorithmic analysis side, last-iterate convergence results for regularized/optimistic primal\u2013dual dynamics in saddle-point problems (e.g., Daskalakis et al.) inspire the paper\u2019s extension of last-iterate guarantees to CMDPs with multiple constraints. Against prior optimistic primal\u2013dual CMDP learners (e.g., Qiu et al.), which retain cancellation-based guarantees, the present algorithm and analysis deliver truly no-regret learning while ensuring non-canceling constraint control throughout.",
  "analysis_timestamp": "2026-01-06T23:09:26.410785"
}
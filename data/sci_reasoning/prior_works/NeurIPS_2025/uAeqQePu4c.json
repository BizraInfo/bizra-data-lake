{
  "prior_works": [
    {
      "title": "KOR-Bench: Knowledge-Orthogonal Reasoning Benchmark for LLMs",
      "authors": "Zhang et al.",
      "year": 2024,
      "role": "Established the principle and protocols for evaluating reasoning independent of factual knowledge.",
      "relationship_sentence": "KORGym directly extends KOR-Bench\u2019s knowledge-orthogonal philosophy to a richer, dynamic, game-based setting spanning text and vision with interactive, multi-turn tasks."
    },
    {
      "title": "OpenAI Gym",
      "authors": "Brockman et al.",
      "year": 2016,
      "role": "Provided the standard RL environment API and tooling conventions for reproducible agent evaluation.",
      "relationship_sentence": "KORGym adopts Gym-style interfaces and environment abstractions to support RL scenarios, standardized evaluation loops, and easy integration with agent training."
    },
    {
      "title": "TextWorld: A Learning Environment for Text-Based Games",
      "authors": "C\u00f4t\u00e9 et al.",
      "year": 2018,
      "role": "Pioneered interactive, procedurally generated text-game environments for reasoning and planning.",
      "relationship_sentence": "KORGym generalizes TextWorld\u2019s interactive text-game paradigm to a broader suite of reasoning games and adds multi-modal (visual) variants and knowledge-orthogonal design."
    },
    {
      "title": "On the Measure of Intelligence (ARC)",
      "authors": "Fran\u00e7ois Chollet",
      "year": 2019,
      "role": "Introduced knowledge-lean abstraction and reasoning assessment via schema induction.",
      "relationship_sentence": "KORGym\u2019s emphasis on knowledge-orthogonal reasoning and pattern abstraction is conceptually grounded in ARC\u2019s methodology for testing general reasoning beyond memorized knowledge."
    },
    {
      "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning (Procgen Benchmark)",
      "authors": "Cobbe et al.",
      "year": 2020,
      "role": "Demonstrated the value of procedurally generated games for measuring generalization and robustness in RL.",
      "relationship_sentence": "KORGym leverages Procgen-style procedural diversity to create many games and dynamic variants, enabling robust measurement of reasoning generalization across tasks and seeds."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Yao et al.",
      "year": 2023,
      "role": "Showed how interleaving chain-of-thought reasoning with actions improves performance in interactive environments.",
      "relationship_sentence": "KORGym explicitly evaluates reasoning strategies (e.g., CoT/ReAct) in multi-turn settings, operationalizing ReAct\u2019s insights into a standardized, scalable evaluation platform."
    },
    {
      "title": "AgentBench: Evaluating LLMs as Agents",
      "authors": "Zhou et al.",
      "year": 2023,
      "role": "Established protocols for assessing LLMs\u2019 interactive decision-making across diverse tasks.",
      "relationship_sentence": "KORGym builds on AgentBench\u2019s agent-evaluation framing, extending it to a unified suite of reasoning-focused games with RL support and cross-modal (text/vision) assessment."
    }
  ],
  "synthesis_narrative": "KORGym\u2019s core innovation\u2014an interactive, dynamic, and modality-agnostic game platform for knowledge-orthogonal reasoning\u2014emerges from the convergence of three lines of prior work. First, the knowledge-orthogonal evaluation philosophy, crystallized by ARC and operationalized for language models in KOR-Bench, motivates KORGym\u2019s focus on abstraction and reasoning divorced from stored factual knowledge. This foundation shapes the task designs and performance analyses that probe reasoning ability rather than recall. Second, the RL environment lineage of OpenAI Gym and the procedural generalization ethos of Procgen provide the infrastructural blueprint and methodological rationale for dynamic, multi-game evaluation. KORGym adopts Gym-style APIs and embraces procedural diversity to test robustness and generalization across many game instances and seeds. Third, interactive agent evaluation from TextWorld, ReAct, and AgentBench informs KORGym\u2019s multi-turn, action-conditioned assessment of reasoning strategies. TextWorld demonstrates the viability of text-game interaction, ReAct highlights the performance gains from interleaving thought and action, and AgentBench codifies protocols for benchmarking LLM agents. KORGym synthesizes these strands into a single, extensible platform spanning textual and visual formats, enabling controlled comparisons across modalities, strategies (e.g., CoT/ReAct), and RL training regimes. This integration directly supports the paper\u2019s key contributions: revealing consistent reasoning patterns within model families, quantifying modality and strategy effects, and providing a standardized arena for studying reinforcement learning with LLMs and VLMs.",
  "analysis_timestamp": "2026-01-07T00:21:32.245805"
}
{
  "prior_works": [
    {
      "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries",
      "authors": "Sam Ainsworth, Jonathan Hayase, Siddhartha Srinivasa",
      "year": 2023,
      "role": "Permutation-based alignment for model fusion",
      "relationship_sentence": "Introduced weight-matching via neuron permutations to reparameterize and merge models, establishing the permutation-symmetry lens that this paper generalizes to a continuous (rotation) symmetry tailored to transformers."
    },
    {
      "title": "Model Soup: Averaging weights of multiple fine-tuned models improves accuracy",
      "authors": "Mitchell Wortsman, Gabriel Ilharco, et al.",
      "year": 2022,
      "role": "Weight-space averaging baseline and motivation",
      "relationship_sentence": "Showed that naive weight averaging can work but is sensitive to misalignment, motivating symmetry-aware alignment; the present work expands the alignment toolkit by exploiting rotation symmetry in attention layers to enable more principled fusion."
    },
    {
      "title": "Merging Models with Fisher-Weighted Averaging",
      "authors": "Tatiana Matena, Colin Raffel",
      "year": 2022,
      "role": "Curvature-aware model merging baseline",
      "relationship_sentence": "Proposed Fisher-weighted interpolation to respect parameter importance, yet still suffers from parameter non-identifiability; the new rotation symmetry provides an alignment step that can precede or augment such merging."
    },
    {
      "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of Deep Neural Networks",
      "authors": "Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson",
      "year": 2018,
      "role": "Evidence for connectivity among solutions in weight space",
      "relationship_sentence": "Showed many minima are connected in weight space, motivating reparameterizations to traverse/merge solutions; rotation symmetry enlarges the equivalence set for transformers, enabling smoother connectivity and fusion."
    },
    {
      "title": "Similarity of Neural Network Representations Revisited",
      "authors": "Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton",
      "year": 2019,
      "role": "Orthogonal-invariant representation comparison (CKA)",
      "relationship_sentence": "Established that representation similarity is often preserved up to orthogonal transforms, directly suggesting the rotational degrees of freedom that this paper formalizes as a parameter-space symmetry for self-attention."
    },
    {
      "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability",
      "authors": "Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",
      "year": 2017,
      "role": "Linear subspace alignment across networks",
      "relationship_sentence": "Demonstrated that different networks\u2019 features are related by low-dimensional linear transforms, anticipating the use of orthogonal (rotation) alignments that preserve dot products in attention and enable principled parameter matching."
    },
    {
      "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis",
      "authors": "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael Carbin",
      "year": 2020,
      "role": "Connectivity and identifiability across solutions",
      "relationship_sentence": "Argued that seemingly disparate solutions can be connected linearly after suitable reparameterizations, reinforcing the idea that exploiting symmetries (here, rotations) can reconcile and fuse transformer solutions."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014formalizing a continuous rotation symmetry in transformer parameter space and leveraging it for optimal parameter matching in model fusion\u2014builds on two intertwined threads: symmetry-aware model alignment and representational equivalences up to orthogonal transforms. Early evidence that distinct solutions are connectable in weight space (Garipov et al.; Frankle et al.) motivated reparameterizations that make models compatible for interpolation or fusion. Concrete mechanisms appeared with Git Re-Basin, which aligned neurons via permutations to merge models, but its discrete symmetry is often brittle for transformers with structured self-attention. In parallel, representation-similarity work (SVCCA, CKA) showed that learned features across networks are frequently equivalent up to orthogonal transformations\u2014crucially, transformations that preserve inner products. Since dot-product self-attention depends only on inner products of queries and keys, these results imply an inherent rotational degree of freedom in attention projections. Model Soup and Fisher-weighted merging highlighted practical gains from weight-space averaging yet exposed failures when parameters are misaligned, underscoring the need for principled alignment before fusion. This paper synthesizes these insights by elevating symmetry from permutations to rotations in attention layers, thereby enlarging the equivalence class from discrete to continuous and better matching transformers\u2019 inductive structure. It then operationalizes this symmetry with a theoretically optimal parameter-matching algorithm, providing a plug-and-play module that complements and improves existing fusion strategies.",
  "analysis_timestamp": "2026-01-07T00:21:33.198199"
}
{
  "prior_works": [
    {
      "title": "StreamingLLM: Efficient Streaming Language Models with Attention Sinks",
      "authors": "Xiao et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work coined and operationalized the \u201cattention sink\u201d phenomenon by showing that LMs consistently assign high attention to the first tokens and leveraging them to stabilize streaming generation, directly motivating a deeper investigation into why and how such sinks emerge."
    },
    {
      "title": "LM-Infinite: Zero-Shot Transfer of LLMs to Infinite-Length Context",
      "authors": "Han et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "By relying on preserving a few early \u2018sink\u2019 tokens while evicting most KV states to extend context essentially without retraining, this paper highlighted the functional reliance on sink positions and exposed open questions about their origin and stability."
    },
    {
      "title": "H2O: Heavy-Hitter Oracle for Efficient KV Cache",
      "authors": "Zhang et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "H2O\u2019s observation that early tokens frequently appear as \u2018heavy hitters\u2019 for attention-based KV retention underscored a systematic bias toward first tokens that lacked a principled explanation, motivating a study into the training dynamics behind sinks."
    },
    {
      "title": "Scissorhands: Efficient KV Cache Management for Large Language Model Inference",
      "authors": "Liu et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "Its token-pruning/eviction strategy implicitly protects initial tokens as anchors due to their persistent high attention, pointing to an underlying mechanism that this paper interrogates during pre-training."
    },
    {
      "title": "SnapKV: Fast and Accurate KV Cache Compression for Large Language Models",
      "authors": "Wang et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "SnapKV\u2019s attention-magnitude\u2013based selection often prioritizes BOS/early tokens, revealing that naive importance metrics can be dominated by sink effects and motivating a causal account of when and why sinks arise."
    },
    {
      "title": "Vision Transformers Need Registers",
      "authors": "Wortsman et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By showing that adding trainable \u2018register\u2019 tokens (attention attractors) improves stability and performance in Transformers, this work suggested a general optimization-driven need for sink-like tokens and inspired probing their emergence in language model pre-training."
    }
  ],
  "synthesis_narrative": "StreamingLLM introduced and operationalized the attention sink: the consistent tendency of autoregressive Transformers to allocate substantial attention to the initial tokens, and exploited this to stabilize streaming generation. LM-Infinite extended this idea to long and effectively unbounded contexts by retaining a small set of early tokens while evicting most key\u2013value states, demonstrating that preserving these sink positions preserves model stability without retraining. H2O formalized heavy-hitter retention for KV caches and repeatedly found early tokens selected as \u2018heavy hitters,\u2019 implying a structural bias that was not yet explained mechanistically. Scissorhands advanced token-level KV eviction and, in practice, shielded initial tokens due to their persistent high attention, highlighting an empirical regularity rather than a principled rationale. SnapKV further showed that attention-magnitude\u2013based importance often locks onto BOS/first tokens, suggesting that sink effects can dominate ostensibly generic selection heuristics. In parallel, Vision Transformers Need Registers showed that adding tokens acting as global attention attractors improves training stability, hinting that sink-like roles may emerge from optimization pressures broadly in Transformers.\nTogether these works established that sink tokens are real, useful, and ubiquitous across streaming, long-context, and KV compression\u2014but left open why they arise, where they localize, and how training choices affect them. The present study synthesizes these clues by tracing sink emergence through pre-training, isolating the roles of optimization efficacy, data scale and distribution, loss design, and architecture, and showing that sinks appear after sufficient optimization, with their positions correlating with loss and data statistics\u2014providing the missing causal understanding that prior engineering exploited but did not explain.",
  "target_paper": {
    "title": "When Attention Sink Emerges in Language Models: An Empirical View",
    "authors": "Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Attention Sink, Language Models, Empirical Study",
    "abstract": "Auto-regressive language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as **attention sink**. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others.  Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in auto-regressive LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how *optimization*, *data distribution*, *loss function*, and *model architecture* in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most",
    "openreview_id": "78Nn4QJTEN",
    "forum_id": "78Nn4QJTEN"
  },
  "analysis_timestamp": "2026-01-06T16:48:31.053709"
}
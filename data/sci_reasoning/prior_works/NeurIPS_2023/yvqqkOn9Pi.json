{
  "prior_works": [
    {
      "title": "Hindsight Credit Assignment",
      "authors": "Harutyunyan et al.",
      "year": 2019,
      "role": "Immediate predecessor; core conceptual scaffold for assigning credit via estimated influence on future outcomes",
      "relationship_sentence": "COCOA directly builds on HCA\u2019s idea of quantifying an action\u2019s influence on later outcomes, but demonstrates that HCA\u2019s state-based contributions can be spurious and replaces them with reward-centric counterfactual contributions computed with a learned model."
    },
    {
      "title": "Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search",
      "authors": "Buesing et al.",
      "year": 2019,
      "role": "Model-based counterfactual estimator using structural causal models",
      "relationship_sentence": "COCOA adopts WCS\u2019s use of model-based counterfactual queries to evaluate alternative actions, specializing the idea to quantify \u201cwould I have gotten that reward?\u201d for long-horizon credit assignment and variance reduction."
    },
    {
      "title": "COMA: Counterfactual Multi-Agent Policy Gradients",
      "authors": "Foerster et al.",
      "year": 2018,
      "role": "Counterfactual baselines for isolating an agent/action\u2019s marginal contribution",
      "relationship_sentence": "COCOA generalizes the counterfactual-baseline principle from multi-agent settings to single-agent long-term credit, marginalizing over alternative actions to isolate an action\u2019s causal contribution to subsequent rewards."
    },
    {
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning (REINFORCE)",
      "authors": "Ronald J. Williams",
      "year": 1992,
      "role": "Foundational likelihood-ratio gradient estimator and variance baseline",
      "relationship_sentence": "COCOA positions its estimator against REINFORCE, proving that HCA often collapses to REINFORCE\u2019s high-variance gradient and showing that reward-centric counterfactual contributions yield substantially lower-variance estimates."
    },
    {
      "title": "Improving Policies via Search in Cooperative Partially Observable Games",
      "authors": "Foerster et al.",
      "year": 2017,
      "role": "Early use of counterfactual reasoning to reduce variance and disentangle contributions",
      "relationship_sentence": "COCOA\u2019s contribution-isolation ethos echoes this line\u2019s counterfactual reasoning to separate an action\u2019s effect from confounding context, but applies it to single-agent, long-horizon reward reachability."
    },
    {
      "title": "The Successor Representation and Temporal Context",
      "authors": "Peter Dayan",
      "year": 1993,
      "role": "Foundational idea of future state-occupancy prediction underpinning state-based credit",
      "relationship_sentence": "COCOA\u2019s critique of state-based contributions traces to SR-like state-occupancy reasoning, motivating its shift from attributing credit to future states toward directly counterfactualizing the reachability of rewards or reward-object representations."
    },
    {
      "title": "RUDDER: Return Decomposition for Delayed Rewards",
      "authors": "Arjona-Medina et al.",
      "year": 2019,
      "role": "Alternative long-term credit assignment via contribution analysis and reward redistribution",
      "relationship_sentence": "Like RUDDER, COCOA targets delayed credit by attributing returns to causally responsible decisions, but it does so with model-based counterfactual queries focused on rewards rather than sequence-level return redistribution."
    }
  ],
  "synthesis_narrative": "COCOA\u2019s core innovation\u2014assigning long-term credit by asking the counterfactual \u201cWould I have gotten that reward if I had taken another action?\u201d\u2014emerges by synthesizing three strands: hindsight-based credit, model-based counterfactuals, and variance-reducing baselines. Hindsight Credit Assignment (HCA) provided the immediate scaffold for linking actions to future outcomes, but its reliance on rewarding states made credits susceptible to confounding, which COCOA explicitly diagnoses and repairs by targeting rewards or learned reward-object representations. This shift is enabled by model-based counterfactual reasoning as in Woulda, Coulda, Shoulda (WCS), which formalized how a learned causal world model can evaluate alternative actions to reduce gradient variance. COMA and related counterfactual-baseline methods demonstrate how marginalizing over alternatives disentangles an individual contribution from context, a principle COCOA adapts to single-agent, long-horizon settings. Against the variance baseline set by REINFORCE, COCOA clarifies when state-based hindsight degenerates to high-variance likelihood ratios and why reward-centric counterfactuals improve sample efficiency. The critique of state-centric credit connects back to the Successor Representation lineage, where predicting future state occupancy can conflate causes of reward; COCOA instead counterfactualizes reward reachability. Finally, RUDDER shows an orthogonal route\u2014return decomposition and reward redistribution\u2014toward delayed credit; COCOA complements this by using explicit model-based counterfactuals, offering a precise and lower-variance path to attributing credit to actions that truly enabled future rewards.",
  "analysis_timestamp": "2026-01-06T23:42:49.059642"
}
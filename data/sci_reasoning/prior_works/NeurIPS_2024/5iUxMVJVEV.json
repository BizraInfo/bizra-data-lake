{
  "prior_works": [
    {
      "title": "STL: A Seasonal-Trend Decomposition Procedure Based on Loess",
      "authors": [
        "Robert B. Cleveland",
        "William S. Cleveland",
        "Jean E. McRae",
        "Irma Terpenning"
      ],
      "year": 1990,
      "role": "Classical foundation for explicit decomposition of time series into trend and multiple seasonal components.",
      "relationship_sentence": "Peri-midFormer\u2019s decoupling of complex multi-periodicity into structured components builds on STL\u2019s principle that seasonal patterns can be explicitly separated and analyzed rather than implicitly modeled."
    },
    {
      "title": "Prophet: Forecasting at Scale",
      "authors": [
        "Sean J. Taylor",
        "Benjamin Letham"
      ],
      "year": 2018,
      "role": "Practical multi-seasonal modeling (yearly/weekly/daily) with additive components and changepoints.",
      "relationship_sentence": "The paper\u2019s formulation of inclusion/overlap among multiple periodic levels echoes Prophet\u2019s operational treatment of concurrent seasonalities, motivating an explicit multi-level periodic design."
    },
    {
      "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting",
      "authors": [
        "Boris N. Oreshkin",
        "Dmitri Carpov",
        "Nicolas Chapados",
        "Yoshua Bengio"
      ],
      "year": 2020,
      "role": "Deep, interpretable decomposition into trend/seasonality via hierarchical stacks and residual links.",
      "relationship_sentence": "Peri-midFormer\u2019s pyramid representation of components and hierarchical aggregation is informed by N-BEATS\u2019 learnable decomposition and stacked, interpretable architecture."
    },
    {
      "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
      "authors": [
        "Haoyi Zhou",
        "Shanghang Zhang",
        "Jieqi Peng",
        "Jianxin Li",
        "Hui Xiong",
        "Wulong Liu"
      ],
      "year": 2021,
      "role": "Scalable Transformer for long sequences via ProbSparse attention and distillation.",
      "relationship_sentence": "The scalability requirements for modeling long-range periodic dependencies in Peri-midFormer are underpinned by Informer\u2019s efficient attention design for long time series."
    },
    {
      "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting",
      "authors": [
        "Haixu Wu",
        "Jiehui Xu",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "year": 2021,
      "role": "Introduced series decomposition and an Auto-Correlation mechanism to capture periodic dependencies.",
      "relationship_sentence": "Peri-midFormer\u2019s explicit periodic modeling and reliance on cross-period dependency capture are direct continuations of Autoformer\u2019s decomposition and periodicity-aware attention ideas."
    },
    {
      "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": 2021,
      "role": "Hierarchical Transformer with multi-scale, windowed processing and cross-level interactions.",
      "relationship_sentence": "The concept of hierarchical, windowed feature processing with inclusion/overlap across scales informs Peri-midFormer\u2019s periodic pyramid and its cross-level interactions among periodic components."
    },
    {
      "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
      "authors": [
        "Wenhai Wang",
        "Enze Xie",
        "Xiang Li",
        "Deng-Ping Fan",
        "Kaitao Song",
        "Ding Liang",
        "Tong Lu",
        "Ping Luo",
        "Ling Shao"
      ],
      "year": 2021,
      "role": "Pyramid Transformer backbone enabling multi-level feature hierarchies and cross-level aggregation.",
      "relationship_sentence": "Peri-midFormer adapts the pyramid Transformer philosophy from PVT to the temporal domain, structuring periodic components in a coarse-to-fine hierarchy with explicit inclusion relations."
    }
  ],
  "synthesis_narrative": "Peri-midFormer\u2019s core innovation is to reframe multi-periodic time series as an explicit pyramid of periodic components with inclusion and overlap relationships, and to process this structure with a hierarchical Transformer. This idea stands on two converging lines of prior work. First, classical and deep decomposition approaches demonstrated that forecasting improves when trend and seasonal effects are explicitly separated. STL formalized seasonal-trend decomposition, and Prophet operationalized concurrent seasonalities (yearly/weekly/daily) with additive components; together, they motivate Peri-midFormer\u2019s assumption that multiple periodicities can and should be disentangled. N-BEATS extended this into a learnable, stacked decomposition, shaping the notion that a hierarchical organization of components can yield both accuracy and interpretability\u2014an idea mirrored in Peri-midFormer\u2019s periodic pyramid and cross-level aggregation.\nSecond, advances in Transformer-based time-series modeling and hierarchical vision Transformers provided the architectural blueprint. Informer addressed scalability for long sequences, a prerequisite for modeling long-term periodicities. Autoformer brought decomposition into the Transformer and introduced auto-correlation to explicitly capture periodic dependencies, directly informing Peri-midFormer\u2019s periodic attention across levels. From the vision side, Swin Transformer and Pyramid Vision Transformer established effective pyramid hierarchies with inclusion/overlap across scales, which Peri-midFormer adapts to temporal periodic scales (e.g., year\u2192month\u2192week\u2192day). By uniting explicit multi-period decomposition with pyramid-style hierarchical Transformers, Peri-midFormer translates well-established ideas into a principled and scalable architecture tailored for complex, multi-periodic time series.",
  "analysis_timestamp": "2026-01-06T23:39:42.966247"
}
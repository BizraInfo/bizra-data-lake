{
  "prior_works": [
    {
      "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
      "authors": "Jianlin Su et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "VideoRoPE builds directly on RoPE\u2019s complex-valued rotational encoding, and its 3D design, temporal frequency allocation, and spacing adjustments are explicit adaptations of RoFormer\u2019s core mechanism to video\u2019s spatio-temporal structure."
    },
    {
      "title": "Train Short, Test Long: Attention with Linear Biases",
      "authors": "Ofir Press et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "This work established the importance of positional schemes that generalize to longer contexts, motivating VideoRoPE\u2019s emphasis on low-frequency temporal allocation to avoid periodic failures analogous to long-context degradation."
    },
    {
      "title": "Extending Context Window of Large Language Models via Position Interpolation",
      "authors": "Xiang Lisa Li et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Position interpolation showed that re-scaling positions effectively reduces RoPE\u2019s high-frequency oscillations for long contexts; VideoRoPE adapts this insight to time by allocating lower temporal frequencies and adjustable temporal spacing."
    },
    {
      "title": "XPos: Explicit Position Encoding for Length Generalization",
      "authors": "Yutao Sun et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "XPos modifies RoPE to improve length extrapolation by controlling phase growth, which directly informs VideoRoPE\u2019s design choice to down-weight high temporal frequencies that cause periodic confusions in long videos."
    },
    {
      "title": "ViViT: A Video Vision Transformer",
      "authors": "Anurag Arnab et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "ViViT\u2019s formulation of factorized spatio-temporal modeling established the need to separately and jointly encode space and time, a structural premise that VideoRoPE operationalizes with a true 3D RoPE and diagonal spatial layout."
    },
    {
      "title": "Is Space-Time Attention All You Need for Video Understanding?",
      "authors": "Gedas Bertasius et al.",
      "year": 2021,
      "role": "Related Problem",
      "relationship_sentence": "TimeSformer highlighted the necessity of carefully balancing spatial and temporal channels in video transformers; VideoRoPE addresses the analogous balance inside RoPE by allocating dimensions and frequencies across space and time."
    },
    {
      "title": "LLaVA-OneVision: Unifying Vision Tasks via Pixel-Aligned RoPE",
      "authors": "Haotian Liu et al.",
      "year": 2024,
      "role": "Extension",
      "relationship_sentence": "Pixel-aligned RoPE (P\u2011RoPE) established a 2D RoPE mapping for spatial symmetry and localization; VideoRoPE generalizes this idea, adopting a diagonal layout for spatial tokens and extending it to a 3D spatio-temporal RoPE."
    }
  ],
  "synthesis_narrative": "VideoRoPE\u2019s core contribution\u2014an explicitly 3D rotary position embedding with low\u2011frequency temporal allocation, diagonal spatial layout, and adjustable temporal spacing\u2014emerges from two converging threads. First, RoFormer introduced rotary position embedding (RoPE), whose rotation-based, translation-equivariant formulation is the substrate VideoRoPE extends to video. Subsequent long-context works exposed where RoPE breaks: Position Interpolation demonstrated that scaling positions tames high-frequency oscillations, while XPos adjusted RoPE\u2019s phase growth to improve length extrapolation. These insights directly motivate VideoRoPE\u2019s low\u2011frequency temporal allocation and controllable temporal spacing to prevent periodic confusions in long video sequences.\nSecond, video transformers such as ViViT and TimeSformer crystallized the architectural need to jointly yet distinctly encode space and time. Meanwhile, recent multimodal advances showed how to make RoPE spatially meaningful: LLaVA\u2011OneVision\u2019s pixel\u2011aligned RoPE (P\u2011RoPE) provided a concrete 2D mapping that preserves spatial symmetry and localization. VideoRoPE generalizes this to a 3D design: it adopts a diagonal spatial layout for symmetry, then adds a principled temporal axis with lower frequencies to resist periodic distractors\u2014validated by the proposed V\u2011NIAH\u2011D stress test. Together, these predecessors define the problem (spatio\u2011temporal encoding), reveal RoPE\u2019s long\u2011context failure modes (oscillation/periodicity), and offer spatial mapping templates (2D RoPE), all of which VideoRoPE integrates and extends into a unified, video\u2011native positional encoding.",
  "analysis_timestamp": "2026-01-06T23:07:19.611421"
}
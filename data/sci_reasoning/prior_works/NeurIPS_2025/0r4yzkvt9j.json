{
  "prior_works": [
    {
      "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
      "authors": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew M. Stuart, Anima Anandkumar",
      "year": 2021,
      "role": "Neural operator baseline using convolution-like spectral kernels on regular grids",
      "relationship_sentence": "CALM-PDE preserves FNO\u2019s convolutional efficiency for operator learning but overcomes its reliance on uniform grids by introducing continuous, coordinate-conditioned kernels that operate on arbitrarily sampled latent states."
    },
    {
      "title": "Learning Operators: DeepONet",
      "authors": "Lu Lu, Pengzhan Jin, George E. Karniadakis",
      "year": 2021,
      "role": "Operator learning from irregular sensor locations via branch\u2013trunk networks",
      "relationship_sentence": "DeepONet established that operators can be learned from irregularly sampled inputs; CALM-PDE attains this irregular-sampling capability with continuous/adaptive convolutions instead of branch\u2013trunk architectures, providing better memory scalability than attention-heavy alternatives."
    },
    {
      "title": "Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders",
      "authors": "K. Lee, K. T. Carlberg",
      "year": 2020,
      "role": "Autoencoder-based reduced-order modeling for PDEs in a compressed latent manifold",
      "relationship_sentence": "CALM-PDE builds directly on the idea of solving dynamics in a compressed latent space, but introduces a new latent evolution operator based on continuous and adaptive convolutions that handles arbitrary discretizations."
    },
    {
      "title": "Learning to Simulate Complex Physics with Graph Networks",
      "authors": "Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, Peter W. Battaglia",
      "year": 2020,
      "role": "Simulation on arbitrary meshes via message passing on graphs",
      "relationship_sentence": "Targeting the same irregular-mesh regime as MeshGraphNets, CALM-PDE replaces message passing/attention with continuous convolution operators to regain CNN-like efficiency while remaining mesh-agnostic."
    },
    {
      "title": "SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels",
      "authors": "Matthias Fey, Jan E. Lenssen, Frank Weichert, Heinrich M\u00fcller",
      "year": 2018,
      "role": "Continuous kernel parameterization for convolutions on graphs using relative coordinates",
      "relationship_sentence": "CALM-PDE generalizes the notion of continuous, coordinate-based kernels from SplineCNN to define latent-space convolutions over irregular samples for PDE states."
    },
    {
      "title": "PointConv: Deep Convolutional Networks on 3D Point Clouds",
      "authors": "Wenxuan Wu, Zhongang Qi, Li Fuxin",
      "year": 2019,
      "role": "Density-compensated continuous convolutions on irregular point sets",
      "relationship_sentence": "Adopting the principle of coordinate-conditioned kernels from PointConv, CALM-PDE performs memory-efficient convolutions at arbitrary sample locations in latent space instead of resorting to attention."
    },
    {
      "title": "Deformable Convolutional Networks",
      "authors": "Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei",
      "year": 2017,
      "role": "Adaptive convolution with learnable sampling offsets",
      "relationship_sentence": "CALM-PDE\u2019s adaptive convolution mechanism is inspired by deformable convolutions, enabling receptive fields that adjust to local sampling density/geometry in irregular discretizations."
    }
  ],
  "synthesis_narrative": "CALM-PDE sits at the intersection of operator learning, reduced-order modeling, and geometric deep learning on irregular domains. Neural operator methods such as the Fourier Neural Operator (FNO) established efficient convolution-like mappings for PDE solution operators, but they hinge on regular grids. DeepONet demonstrated that operators can be learned from irregularly sampled inputs, clarifying the feasibility of grid-free operator learning. Complementing these, autoencoder-based reduced-order modeling showed that PDE dynamics can be faithfully advanced in a compact latent manifold, motivating CALM-PDE\u2019s choice to operate in a compressed latent space for efficiency.\nTo break the grid constraint without incurring attention\u2019s quadratic memory costs, CALM-PDE draws from continuous convolution designs developed for graphs and point clouds. SplineCNN introduced continuous, coordinate-based kernels on graphs, while PointConv provided density-aware, coordinate-conditioned convolutions on irregular point sets. These ideas underpin CALM-PDE\u2019s continuous latent convolutions that natively handle arbitrary sample locations. Further, the model\u2019s adaptive convolutional mechanism echoes deformable convolutions, allowing receptive fields to adjust to local sampling patterns and geometry\u2014critical for nonuniform discretizations.\nFinally, MeshGraphNets exemplified learning simulators on arbitrary meshes with message passing; CALM-PDE targets the same irregular-domain regime but replaces attention/message passing with continuous and adaptive convolutions, restoring CNN-like memory and compute efficiency. Together, these strands converge in CALM-PDE\u2019s key contribution: a continuous-and-adaptive convolutional operator that advances time-dependent PDEs efficiently in latent space while supporting arbitrarily discretized domains.",
  "analysis_timestamp": "2026-01-07T00:21:32.276318"
}
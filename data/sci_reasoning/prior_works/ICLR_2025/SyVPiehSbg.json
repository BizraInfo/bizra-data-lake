{
  "prior_works": [
    {
      "title": "On the Representation of Continuous Functions of Several Variables by Superposition of Continuous Functions of One Variable and Addition",
      "authors": "A. N. Kolmogorov",
      "year": 1957,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This theorem provides the core superposition structure\u2014sums of compositions of univariate inner and outer functions\u2014that ActNet explicitly re-parameterizes to obtain a practical, scalable KST-based neural architecture."
    },
    {
      "title": "On Functions of Three Variables",
      "authors": "V. I. Arnold",
      "year": 1957,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Arnold\u2019s refinement of the Kolmogorov superposition elucidates the 2n+1 inner-function structure and universality properties that ActNet relaxes/repurposes to reduce unknowns and improve trainability."
    },
    {
      "title": "On the Structure of Continuous Functions of Several Variables",
      "authors": "D. A. Sprecher",
      "year": 1965,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "Sprecher\u2019s constructive variants showing that fixed inner functions can suffice directly motivate ActNet\u2019s design choice to share or structurally constrain inner (activation-like) components while shifting learnability to outer univariate maps."
    },
    {
      "title": "On a Constructive Proof of Kolmogorov\u2019s Superposition Theorem",
      "authors": "J. Braun et al.",
      "year": 2009,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "Constructive attempts exposed severe practical obstacles\u2014nonsmooth inner functions, large constants, and proliferation of unknowns\u2014that ActNet targets by proposing a deep-learning parameterization that curbs inner-function degrees of freedom."
    },
    {
      "title": "KAN: Kolmogorov\u2013Arnold Networks",
      "authors": "Ziming Liu et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "KAN operationalized KST via trainable univariate spline functions on edges, and ActNet is introduced as a KST-based alternative that directly addresses KAN\u2019s training instability and parameter inefficiency stemming from Kolmogorov\u2019s original formulation."
    },
    {
      "title": "Neural Additive Models: Interpretable Machine Learning with Neural Nets",
      "authors": "Rishabh Agarwal et al.",
      "year": 2021,
      "arxiv_id": "2004.13912",
      "role": "Related Problem",
      "relationship_sentence": "NAM\u2019s strategy of representing targets as sums of learned univariate subnetworks informs ActNet\u2019s use of univariate activation blocks and linear aggregation to control complexity while retaining KST-style structure."
    },
    {
      "title": "Physics-Informed Neural Networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
      "authors": "M. Raissi et al.",
      "year": 2019,
      "arxiv_id": "1711.10561",
      "role": "Foundation",
      "relationship_sentence": "PINNs supply the physics-constrained loss formulation and low-dimensional PDE setting where KST-style architectures excel, forming the evaluation framework guiding ActNet\u2019s design choices and empirical validation."
    }
  ],
  "synthesis_narrative": "Kolmogorov established that any continuous multivariate function can be written as a finite sum of compositions of univariate inner and outer functions, and Arnold clarified the universality and 2n+1 inner-function structure that underpins practical hopes for such representations. Sprecher advanced constructive perspectives showing that fixed inner functions can suffice, implicitly suggesting architectural designs where learnability is concentrated in outer univariate maps while inner components are shared or constrained. Later constructive efforts demonstrated feasibility but revealed stark practical issues: numerically ill-behaved, nonsmooth inner functions and a proliferation of unknown parameters made raw KST instantiations unwieldy. In parallel, Neural Additive Models showed that modeling with sums of learned univariate subnetworks can be effective and interpretable, hinting at scalable realizations of superposition-style decompositions. Most recently, Kolmogorov\u2013Arnold Networks operationalized KST in deep learning using trainable univariate spline functions on edges, providing a concrete baseline but exhibiting mixed empirical gains and training challenges traceable to the original KST formulation. Physics-Informed Neural Networks defined a rigorous, low-dimensional PDE setting and loss formulation where such KST-style decompositions should be advantageous. Together, these works reveal both the representational promise and the practical pitfalls of KST-based models: while superposition structures are powerful, na\u00efvely learning many inner functions hampers scalability and stability. The natural next step is to reparameterize the KST decomposition to reduce inner-function unknowns, share or constrain activation-like components, and align the architecture with physics-constrained training; ActNet synthesizes these insights into a scalable alternative that directly addresses KAN\u2019s shortcomings while preserving KST\u2019s strengths in PINN settings.",
  "target_paper": {
    "title": "Deep Learning Alternatives Of The Kolmogorov Superposition Theorem",
    "authors": "Leonardo Ferreira Guilhoto, Paris Perdikaris",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Kolmogorov-Arnold Representation Theorem, Function Approximation, Physics Informed Neural Networks, AI4Science",
    "abstract": "This paper explores alternative formulations of the Kolmogorov Superposition Theorem (KST) as a foundation for neural network design. The original KST formulation, while mathematically elegant, presents practical challenges due to its limited insight into the structure of inner and outer functions and the large number of unknown variables it introduces. Kolmogorov-Arnold Networks (KANs) leverage KST for function approximation, but they have faced scrutiny due to mixed results compared to traditional multilayer perceptrons (MLPs) and practical limitations imposed by the original KST formulation. To address these issues, we introduce ActNet, a scalable deep learning model that builds on the KST and overcomes some of the drawbacks of Kolmogorov's original formulation. We evaluate ActNet in the context of Physics-Informed Neural Networks (PINNs), a framework well-suited for leveraging KST's strengths in low-dimensional function approximation, particularly for simulating partial differentia",
    "openreview_id": "SyVPiehSbg",
    "forum_id": "SyVPiehSbg"
  },
  "analysis_timestamp": "2026-01-06T07:47:13.839791"
}
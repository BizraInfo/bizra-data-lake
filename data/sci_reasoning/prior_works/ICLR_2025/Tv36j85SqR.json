{
  "prior_works": [
    {
      "title": "End-to-end optimized image compression",
      "authors": "Johannes Ball\u00e9 et al.",
      "year": 2017,
      "arxiv_id": "1611.01704",
      "role": "Foundation",
      "relationship_sentence": "This work established the nonlinear transform coding pipeline with elementwise integer rounding (and its training relaxations) in the latent space\u2014the exact quantization stage that the current paper replaces with nearest-lattice-point quantization."
    },
    {
      "title": "Variational image compression with a scale hyperprior",
      "authors": "Johannes Ball\u00e9 et al.",
      "year": 2018,
      "arxiv_id": "1802.01436",
      "role": "Baseline",
      "relationship_sentence": "This hyperprior-based learned codec is the de-facto baseline that uses scalar latent quantization and entropy modeling, which the current paper keeps intact while substituting the scalar quantizer with a lattice quantizer to improve rate\u2013distortion."
    },
    {
      "title": "Universally Quantized Neural Compression",
      "authors": "Eirikur Agustsson and Lucas Theis",
      "year": 2020,
      "arxiv_id": "2006.09952",
      "role": "Gap Identification",
      "relationship_sentence": "By formalizing dithered scalar uniform quantization for learned compression and proving universality, this work crystallized the reliance on axis-aligned cubical cells whose inefficiency in dimensions >1 is the explicit limitation addressed by adopting lattice quantization."
    },
    {
      "title": "Quantization",
      "authors": "Robert M. Gray and David L. Neuhoff",
      "year": 1998,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This tutorial consolidates high-rate quantization theory showing that cubic (scalar) cells incur a space-filling loss relative to optimal Voronoi cells and that good lattices minimize normalized second moment\u2014principles directly used to motivate and select lattice quantizers."
    },
    {
      "title": "Asymptotic quantization error of continuous signals and the quantization dimension",
      "authors": "Paul L. Zador",
      "year": 1982,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Zador\u2019s asymptotic constants quantify the rate\u2013distortion advantage of high-dimensional vector quantizers with spherical/Voronoi cells over scalar quantizers, motivating the shift to lattice quantization in the latent space."
    },
    {
      "title": "Fast quantizing and decoding algorithms for lattice quantizers",
      "authors": "John H. Conway and Neil J. A. Sloane",
      "year": 1982,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This paper provides practical nearest-lattice-point algorithms and identifies efficient lattices (e.g., A2, E8) that enable low-complexity implementations of near-optimal lattice quantizers used by the proposed method."
    },
    {
      "title": "Neural Discrete Representation Learning",
      "authors": "Aaron van den Oord et al.",
      "year": 2017,
      "arxiv_id": "1711.00937",
      "role": "Related Problem",
      "relationship_sentence": "VQ-VAE introduced straight-through gradient training for nearest-neighbor vector quantization, a mechanism the current work leverages to backpropagate through nearest-lattice-point assignments in learned compression."
    }
  ],
  "synthesis_narrative": "Nonlinear transform coding for learned compression was crystallized by work that maps inputs to latents and performs elementwise rounding with differentiable training relaxations, establishing the scalar-quantized latent paradigm. The hyperprior architecture further refined entropy modeling while retaining elementwise integer quantization as the core discretization step. Universally quantized neural compression formalized dithered scalar uniform quantization and its universality, cementing axis-aligned cubical quantization cells as the standard choice in learned compressors. Classical quantization theory shows that such cubic cells suffer a space-filling loss relative to optimal Voronoi partitions, and that good lattices achieve lower normalized second moments, directly implying better high-rate rate\u2013distortion. Zador\u2019s asymptotic constants quantify the advantage of high-dimensional vector quantizers with near-spherical cells over scalar quantizers, highlighting that the benefit grows with intrinsic dimension. Practicality comes from lattice literature that provides efficient nearest-point algorithms and identifies best-known lattices (e.g., A2, E8) for low-complexity, near-optimal quantization. Finally, VQ-VAE introduced straight-through training for discrete nearest-neighbor assignments in neural networks, offering a recipe to differentiate through quantization.\nTaken together, these works expose a clear opportunity: learned transform coding is mature, but its scalar latent quantization imposes suboptimal, square-like cells, especially harmful for higher-dimensional structure; lattice theory both quantifies the gap and offers implementable quantizers and decoders. The current paper naturally fuses these strands by swapping scalar rounding for nearest-lattice-point quantization within the established transform\u2013entropy framework and training it end-to-end using VQ-style surrogates, thereby approximating optimal vector quantization at practical complexity and approaching rate\u2013distortion limits.",
  "target_paper": {
    "title": "Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding",
    "authors": "Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Neural compression, vector quantization, lattice quantization, nonlinear transform coding",
    "abstract": "Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal on a few specific sources, we show that it can be highly sub-optimal on synthetic sources whose intrinsic dimensionality is greater than one. With integer rounding in the latent space, the quantization regions induced by neural transformations, remain square-like and fail to match those of optimal vector quantization. We demonstrate that this phenomenon is due to the choice of scalar quantization in the latent space, and not the transform design. By employing lattice quantization instead, we propose  Lattice Transform Coding (LTC) and show that it approximately recovers optimal vector quantization at reasonable complexity. On real-world sources, LTC improve",
    "openreview_id": "Tv36j85SqR",
    "forum_id": "Tv36j85SqR"
  },
  "analysis_timestamp": "2026-01-06T07:53:50.454771"
}
{
  "prior_works": [
    {
      "title": "The Program Dependence Graph and Its Use in Optimization",
      "authors": "Jeanne Ferrante, Karl J. Ottenstein, Joe D. Warren",
      "year": 1987,
      "role": "Foundational program analysis structure capturing semantics-preserving control/data dependencies",
      "relationship_sentence": "SymC\u2019s code symmetry group is defined over permutations of the program dependence graph (PDG), directly leveraging Ferrante et al.\u2019s PDG as the semantics-grounded object whose automorphisms preserve program meaning."
    },
    {
      "title": "Learning to Represent Programs with Graphs",
      "authors": "Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi",
      "year": 2018,
      "role": "Demonstrated the utility of data/control-flow program graphs for learning program semantics",
      "relationship_sentence": "By showing that incorporating data/control-flow edges improves semantic reasoning, this work motivates SymC\u2019s choice to operate on PDG-structured inputs and to encode structural priors at the architectural level."
    },
    {
      "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow",
      "authors": "Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Lei Li, Jian Yin, Jie Zhou, Nan Duan",
      "year": 2021,
      "role": "Integrated data-flow relations into Transformer-based code models",
      "relationship_sentence": "GraphCodeBERT established that injecting data-flow signals aids code understanding; SymC advances this by enforcing provable equivariance to PDG-induced symmetries in the attention mechanism rather than relying on pretraining alone."
    },
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco Cohen, Max Welling",
      "year": 2016,
      "role": "Introduced group-equivariant layers and the group-theoretic perspective on symmetry in deep learning",
      "relationship_sentence": "SymC adopts the group-theoretic lens of Cohen and Welling\u2014treating semantics-preserving code transformations as group actions\u2014to design layers (attention) that are equivariant to those transformations."
    },
    {
      "title": "Invariant and Equivariant Graph Networks",
      "authors": "Haggai Maron, Heli Ben-Hamu, Nadav Shamir, Yaron Lipman",
      "year": 2019,
      "role": "Provided formal constructions for permutation-invariant/equivariant architectures on graphs",
      "relationship_sentence": "Maron et al.\u2019s representation-theoretic treatment of permutation equivariance on graphs underpins SymC\u2019s guarantee of equivariance to PDG-defined permutation groups within its attention blocks."
    },
    {
      "title": "Deep Sets",
      "authors": "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab\u00e1s P\u00f3czos, Ruslan R. Salakhutdinov, Alexander J. Smola",
      "year": 2017,
      "role": "Characterized permutation-invariant/equivariant functions over sets",
      "relationship_sentence": "Deep Sets\u2019 formalization of permutation symmetry informs SymC\u2019s requirement that outputs transform consistently under node permutations corresponding to semantics-preserving renamings/reorderings."
    },
    {
      "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Networks",
      "authors": "Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, Yee Whye Teh",
      "year": 2019,
      "role": "Developed attention mechanisms that are permutation-invariant/equivariant for set-structured inputs",
      "relationship_sentence": "Set Transformer\u2019s permutation-aware attention motivates SymC\u2019s design of a self-attention variant with provable equivariance, specialized from sets to PDG-induced permutation groups."
    }
  ],
  "synthesis_narrative": "SymC\u2019s core idea\u2014endowing a code model with provable equivariance to semantics-preserving transformations\u2014arises from merging group-theoretic equivariance with program-graph semantics. Ferrante et al.\u2019s Program Dependence Graph (PDG) provides the semantic backbone: a representation whose structure encodes control and data dependencies and thus preserves program meaning. Building on the success of graph-based code learning (Allamanis et al.) and data-flow\u2013aware Transformers (GraphCodeBERT), SymC adopts the PDG not merely as an input graph but as the object defining a symmetry group: permutations of nodes/edges that leave semantics intact (e.g., \u03b1-renaming, reordering independent statements).\n\nOn the modeling side, SymC draws from the group-equivariant learning paradigm inaugurated by Cohen and Welling, which frames symmetries as group actions to be respected by neural layers. Maron et al. extend this to graphs, providing principled constructions for permutation-invariant/equivariant operators\u2014a theoretical scaffold for reasoning about equivariance under PDG-induced permutations. Complementing this, Deep Sets formalizes when functions over collections should be permutation invariant/equivariant, and Set Transformer shows how attention can be designed to respect such symmetries. SymC synthesizes these threads by instantiating a self-attention mechanism that is provably equivariant to the permutation group defined over the PDG, encoding the code-structural prior directly into the architecture. This yields better sample efficiency and generalization on program analysis tasks, surpassing large pretrained code LLMs without pretraining, thereby validating structural equivariance as a potent inductive bias for learning program semantics.",
  "analysis_timestamp": "2026-01-07T00:02:04.884522"
}
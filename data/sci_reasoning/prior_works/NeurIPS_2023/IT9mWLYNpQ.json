{
  "prior_works": [
    {
      "title": "Gradient Descent on the Edge of Stability",
      "authors": "Cohen et al.",
      "year": 2021,
      "role": "Empirical/theoretical identification of the EoS regime for large constant step sizes in GD",
      "relationship_sentence": "This paper introduced and analyzed the edge-of-stability phenomenon\u2014non-monotonic losses with sharpness hovering near 2/\u03b7\u2014which directly motivates and frames the present work\u2019s focus on constant-stepsize GD dynamics for logistic regression in the EoS regime."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry, Elad Hoffer, Mor Nacson, Suriya Gunasekar, Nati Srebro",
      "year": 2018,
      "role": "Foundational implicit-bias result for linear classification with exponential-tailed losses",
      "relationship_sentence": "Established that for separable data GD (on logistic/exponential loss) converges in direction to the hard-margin SVM, providing the max-margin baseline that this paper extends to large constant step sizes at the edge of stability."
    },
    {
      "title": "Risk and Parameter Convergence of Logistic Regression",
      "authors": "Ziwei Ji, Matus Telgarsky",
      "year": 2019,
      "role": "Asymptotic analysis of logistic regression under GD, including parameter divergence and margin growth",
      "relationship_sentence": "Supplies tools and asymptotics for logistic regression (e.g., directional convergence with diverging norms) that underpin the present paper\u2019s decomposition into max-margin and orthogonal components under EoS."
    },
    {
      "title": "Convergence of Gradient Descent on Separable Data",
      "authors": "Itay Nacson, Nati Srebro, Daniel Soudry",
      "year": 2019,
      "role": "Convergence guarantees and rates for GD with constant step sizes on separable data",
      "relationship_sentence": "Analyzes constant-stepsize GD and the structure of convergence toward max-margin, providing technical groundwork that the present work adapts to the non-monotone EoS regime and refines via an orthogonal-complement potential."
    },
    {
      "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
      "authors": "Kaifeng Lyu, Jian Li",
      "year": 2019,
      "role": "Generalization of margin-maximization implicit bias beyond linear models",
      "relationship_sentence": "Reinforces the centrality of margin maximization in gradient dynamics, supporting the present paper\u2019s identification of the max-margin direction as the escape direction even under large steps at EoS."
    },
    {
      "title": "Support-Vector Networks",
      "authors": "Corinna Cortes, Vladimir Vapnik",
      "year": 1995,
      "role": "Foundational definition of the hard-margin SVM direction",
      "relationship_sentence": "Provides the canonical max-margin target (hard-margin SVM) used to characterize the directional limit of GD iterates in the present analysis."
    }
  ],
  "synthesis_narrative": "The paper unites two lines of research: the edge-of-stability (EoS) phenomenon for gradient descent with large constant step sizes and the implicit bias of gradient methods on separable data. Cohen et al. (2021) documented that practical training often operates at EoS, with non-monotonic losses and sharpness saturating around 2/\u03b7, challenging classical stability analyses. Building upon this, the current work asks whether the well-known implicit bias results still hold under EoS. Foundational studies by Soudry et al. (2018) and Ji & Telgarsky (2019) established that for logistic (and related exponential-tailed) losses on linearly separable data, gradient descent parameters diverge while their direction converges to the hard-margin SVM solution. Nacson et al. (2019) further developed convergence guarantees and rates for constant-stepsize GD, offering tools and decompositions crucial for handling the non-monotone dynamics encountered at EoS.\nIntegrating these insights, the present paper proves that even with any constant step size in the EoS regime, logistic loss is minimized over long time scales. It sharpens the implicit bias picture by decomposing the iterate trajectory: along the max-margin (SVM) direction, parameters diverge, while in the orthogonal complement they converge to the minimizer of a strongly convex potential. Lyu & Li (2019) reinforce the broader principle of margin maximization under gradient dynamics, while Cortes & Vapnik (1995) provide the canonical max-margin target. The paper also contrasts logistic with exponential loss, highlighting catastrophic divergence at EoS for the latter, thereby delineating loss-specific stability and implicit bias behaviors under large-step training.",
  "analysis_timestamp": "2026-01-06T23:42:49.135822"
}
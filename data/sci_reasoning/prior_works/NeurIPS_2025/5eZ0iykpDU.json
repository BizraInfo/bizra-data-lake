{
  "prior_works": [
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang et al.",
      "year": 2023,
      "role": "Empirical foundation for multi-sample, diverse reasoning",
      "relationship_sentence": "Showed that sampling multiple, diverse reasoning paths markedly boosts accuracy, directly motivating the paper\u2019s focus on solution diversity and inspiring the potential@k evaluation that captures best-of-k reasoning potential."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan",
      "year": 2023,
      "role": "Search-based diversification of reasoning trajectories",
      "relationship_sentence": "By explicitly exploring and selecting among diverse intermediate reasoning branches, this work reinforced the importance of diversity across solution paths, informing the paper\u2019s premise that diversity during training can improve reasoning competence."
    },
    {
      "title": "DeepSeek-R1: Incentivizing Reasoning in LLMs via Reinforcement Learning (Technical Report)",
      "authors": "DeepSeek-AI Team",
      "year": 2024,
      "role": "RL-based training paradigm for reasoning LLMs",
      "relationship_sentence": "Established RL as an effective mechanism to enhance LLM reasoning, providing the training setting into which the paper integrates a diversity-aware policy objective."
    },
    {
      "title": "Proximal Policy Optimization Algorithms",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov",
      "year": 2017,
      "role": "Algorithmic backbone for policy optimization",
      "relationship_sentence": "Provides the standard policy-gradient framework widely used in RLHF-style training of LLMs, into which the proposed token-level diversity regularization can be cleanly incorporated."
    },
    {
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine",
      "year": 2018,
      "role": "Theoretical grounding for entropy-regularized (diversity-promoting) RL",
      "relationship_sentence": "Motivates adding entropy/diversity terms to the policy objective to encourage broad exploration, informing the paper\u2019s token-level diversity objective as a principled way to promote action-sequence variety."
    },
    {
      "title": "Diversity is All You Need: Learning Diverse Skills without a Reward Function (DIAYN)",
      "authors": "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine",
      "year": 2018,
      "role": "Explicit diversity maximization in RL",
      "relationship_sentence": "Demonstrates that directly maximizing diversity (via mutual information) can yield robust, generalizable behaviors, inspiring the paper\u2019s explicit promotion of diversity within language trajectories during RL training."
    },
    {
      "title": "Evaluating Large Language Models Trained on Code",
      "authors": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan et al.",
      "year": 2021,
      "role": "Metric inspiration (pass@k) for best-of-k potential",
      "relationship_sentence": "Popularized pass@k as a best-of-k oracle metric; this directly influenced the paper\u2019s potential@k metric to quantify an LLM\u2019s reasoning potential across multiple sampled solutions."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014diversity-aware policy optimization for LLM reasoning\u2014emerges from the confluence of three lines of prior work. First, self-consistency and search-based prompting methods (Wang et al., Tree of Thoughts) established that sampling and exploring multiple, heterogeneous reasoning paths boosts problem-solving performance. These works motivate the thesis that diversity is not merely a decoding trick but a property worth cultivating, and they inspire the paper\u2019s potential@k metric that quantifies best-of-k reasoning potential. The pass@k tradition from code evaluation (Chen et al.) provides a direct template for such an oracle-style metric.\nSecond, reinforcement learning for reasoning LLMs (DeepSeek-R1) created a practical training substrate in which policies can be shaped beyond supervised imitation, making it natural to ask how to encode diversity incentives during optimization. PPO (Schulman et al.) supplies the widely adopted policy-gradient backbone into which additional objectives can be incorporated without destabilizing training.\nThird, entropy-regularized and diversity-maximizing RL (Soft Actor-Critic; DIAYN) offer principled mechanisms to encourage broad exploration and diverse behaviors. Translating these insights to sequence generation, the paper formulates a token-level diversity objective and applies it selectively to positive samples, aligning exploration pressure with constructive reasoning paths. Together, these strands directly inform the paper\u2019s finding that solution diversity correlates with potential@k, and they underlie its practical algorithm for explicitly promoting diversity in RL-based LLM reasoning.",
  "analysis_timestamp": "2026-01-06T23:42:48.111231"
}
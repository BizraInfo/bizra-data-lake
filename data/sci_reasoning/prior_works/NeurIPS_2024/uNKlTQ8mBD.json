{
  "prior_works": [
    {
      "title": "HOList: An Environment for Machine Learning of Higher-Order Theorem Proving",
      "authors": "Stanislas Bansal, Sarah M. Loos, Markus N. Rabe, Christian Szegedy",
      "year": 2019,
      "role": "RL framework for formal theorem proving with policy/value-guided search",
      "relationship_sentence": "Established theorem proving as an RL/search problem with learned policy and value guidance in a higher-order logic setting, directly informing this paper\u2019s use of a unified model to guide proof search in a formal system."
    },
    {
      "title": "Generative Language Modeling for Automated Theorem Proving in Lean",
      "authors": "Stanislav Polu, Ilya Sutskever",
      "year": 2020,
      "role": "Language-model-guided proof search in a dependently-typed assistant",
      "relationship_sentence": "Demonstrated that large language models can propose tactics/proofs in Lean, motivating this work\u2019s LM-based policy/value for proof search and extending it to also generate conjectures."
    },
    {
      "title": "Hindsight Experience Replay",
      "authors": "Marcin Andrychowicz et al.",
      "year": 2017,
      "role": "Goal relabeling to improve sample efficiency in sparse-reward settings",
      "relationship_sentence": "Inspired the paper\u2019s novel hindsight relabeling over proof search trees, adapting HER\u2019s core idea to relabel goals/conjectures from partial proofs to greatly improve data efficiency."
    },
    {
      "title": "Automatic Goal Generation for Reinforcement Learning Agents (GoalGAN)",
      "authors": "Carlos Florensa, David Held, Michael Wulfmeier, Michael Zhang, Pieter Abbeel",
      "year": 2018,
      "role": "Intrinsic motivation and adaptive goal generation of target difficulty",
      "relationship_sentence": "Provides the blueprint for generating hard-but-solvable goals relative to the agent\u2019s current competence, which this paper instantiates as conjecture generation calibrated to the evolving prover."
    },
    {
      "title": "Program Synthesis from Polymorphic Refinement Types (Synquid)",
      "authors": "Nadia Polikarpova, Ivan Kuraj, Armando Solar-Lezama",
      "year": 2016,
      "role": "Type-directed synthesis ensuring well-typedness by construction",
      "relationship_sentence": "Informs the paper\u2019s type-directed construction of conjectures that are guaranteed well-formed (and meaningful) under a type system, enabling valid sampling even from an untrained model."
    },
    {
      "title": "A Syntactic Neural Model for General-Purpose Code Generation",
      "authors": "Xinyun Yin, Graham Neubig",
      "year": 2017,
      "role": "Grammar/AST-constrained decoding to enforce syntactic validity",
      "relationship_sentence": "Motivates the constrained decoding component that enforces structural well-formedness during LM sampling, here adapted to dependent type theory for valid conjecture generation."
    },
    {
      "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (AlphaZero)",
      "authors": "David Silver et al.",
      "year": 2017,
      "role": "Unified policy/value networks and self-play curriculum via search",
      "relationship_sentence": "Influences the paper\u2019s self-improving loop where a single model provides both policy and value to guide search while the training distribution adapts with capability, analogous to self-play."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014a self-improving agent that jointly conjectures and proves in a dependently typed setting\u2014sits at the intersection of learned proof search, adaptive curriculum via intrinsic motivation, and structure-aware generation. HOList framed formal proving as an RL-guided search over tactics with learned policy/value functions, a paradigm that this work adopts and extends by using a single language model to represent both policy and value within dependent type theory. GPT-f showed that language models can effectively guide proof construction in Lean, directly motivating the use of LMs as the backbone for proof search here while expanding the scope to also generate conjectures.\nTo ensure conjectures are valid from the outset\u2014even with an untrained model\u2014the paper marries constrained decoding with type-directed synthesis: Synquid provides the foundation for type-directed construction guaranteeing well-typedness, while syntax/AST-constrained decoding from code generation research (Yin & Neubig) offers a practical mechanism to enforce structural correctness during sampling. The agent\u2019s intrinsic-motivation loop draws from GoalGAN: it generates conjectures of appropriate difficulty relative to its current proving ability, creating a moving target that drives continual learning. Finally, the paper\u2019s hindsight relabeling for proof trees builds on HER, translating goal relabeling to the domain of structured proof search to dramatically improve sample efficiency. Together, these threads yield an agent that not only proves but also invents challenging, solvable mathematics, closing the loop between discovery and verification.",
  "analysis_timestamp": "2026-01-06T23:33:36.261958"
}
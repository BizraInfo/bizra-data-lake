{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "training paradigm",
      "relationship_sentence": "Established the contrastive image\u2013text pretraining objective and English-centric zero-shot evaluation setup that this work adopts while probing the effect of adding multilingual-origin samples."
    },
    {
      "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (ALIGN)",
      "authors": "Chao Jia et al.",
      "year": 2021,
      "role": "scaling evidence",
      "relationship_sentence": "Showed that large-scale, weakly filtered web image\u2013text pairs (often multilingual) can yield strong representations, motivating the hypothesis that non\u2011English web data contain useful signal beyond English-only curation."
    },
    {
      "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
      "authors": "Christoph Schuhmann et al.",
      "year": 2022,
      "role": "dataset/curation pipeline",
      "relationship_sentence": "Provided a multilingual web-crawled corpus and a CLIP-score\u2013based filtering pipeline with language identification; the present work builds directly on this paradigm by translating non\u2011English captions to English and re-filtering to retain useful samples."
    },
    {
      "title": "DataComp: In search of the next generation of multimodal datasets",
      "authors": "Gabriel Ilharco et al.",
      "year": 2023,
      "role": "curation benchmark",
      "relationship_sentence": "Demonstrated that curation and filtering choices dominate vision\u2013language performance, informing this paper\u2019s systematic re-filtering after translation and its evaluation protocol; several authors overlap with the present work."
    },
    {
      "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning",
      "authors": "Srinivasan et al.",
      "year": 2021,
      "role": "multilingual dataset",
      "relationship_sentence": "Introduced a large multilingual image\u2013text dataset and showed the value of culturally diverse captions, supporting the paper\u2019s premise that non\u2011English data encode complementary concepts beneficial to representation learning."
    },
    {
      "title": "No Language Left Behind (NLLB-200): Scaling Human-Centered Machine Translation",
      "authors": "NLLB Team (Meta AI)",
      "year": 2022,
      "role": "machine translation",
      "relationship_sentence": "Provided high-quality translation coverage across 200 languages, enabling the core methodological step of translating non\u2011English captions to English at scale for unified filtering and training."
    },
    {
      "title": "MURAL: Multimodal, Multitask Retrieval Across Languages",
      "authors": "Prajjwal Jain et al.",
      "year": 2021,
      "role": "multilingual V-L alignment",
      "relationship_sentence": "Showed that multilingual/cross-lingual supervision improves image\u2013text retrieval, directly motivating the idea that multilingual signals\u2014whether via native captions or translation\u2014enhance vision\u2013language representations."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central insight\u2014that multilingual diversity in web image\u2013text data improves vision\u2013language representations even for English benchmarks\u2014builds on three converging threads. First, CLIP established the contrastive pretraining objective and English-centric zero-shot evaluation that became the de facto training/evaluation protocol. ALIGN and LAION-5B then showed that scaling to billions of noisy, web-crawled image\u2013text pairs, often containing many languages, can produce strong models; LAION-5B also crystallized practical filtering (language ID, CLIP-score thresholds) for such data. Second, a line of multilingual resources and models\u2014WIT and MURAL\u2014demonstrated that captions from diverse languages encode culturally salient and complementary concepts, and that cross-lingual supervision improves image\u2013text alignment. These results directly motivate the hypothesis that non-English web data carry signal beneficial to models evaluated in English.\nThird, DataComp provided a rigorous framework proving that curation choices dominate performance, suggesting that better selection, rather than just more data, is key. The present work operationalizes these lessons with a simple but powerful twist: translate non-English captions to English using modern machine translation (e.g., NLLB-200) and re-apply strong English filtering to recover multilingual-origin samples that prior English-only pipelines would discard. This translation-plus-refiltering leverages multilingual diversity while staying within an English text-encoder training regime, yielding measurable gains on standard English vision benchmarks and offering concrete guidance for future data curation.",
  "analysis_timestamp": "2026-01-06T23:42:49.036726"
}
{
  "prior_works": [
    {
      "title": "Adaptive Mixtures of Local Experts",
      "authors": [
        "Robert A. Jacobs",
        "Michael I. Jordan",
        "Steven J. Nowlan",
        "Geoffrey E. Hinton"
      ],
      "year": 1991,
      "role": "Foundational MoE formulation and gating symmetries",
      "relationship_sentence": "This work introduced the mixture-of-experts framework with learnable gating, establishing the permutation symmetry of experts (and their gates) that the paper formalizes as the key symmetry class underlying LMC in MoE."
    },
    {
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "authors": [
        "Noam Shazeer",
        "Azalia Mirhoseini",
        "Krzysztof Maziarz",
        "Andy Davis",
        "Quoc V. Le",
        "Geoffrey E. Hinton",
        "Jeff Dean"
      ],
      "year": 2017,
      "role": "Modern scalable MoE and sparse gating regime",
      "relationship_sentence": "By defining and popularizing sparsely gated MoE at scale, this paper provides the architectural and routing regime (top-k gating) that the present work analyzes for linear mode connectivity and permutation-induced symmetries."
    },
    {
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "authors": [
        "William Fedus",
        "Barret Zoph",
        "Noam Shazeer"
      ],
      "year": 2021,
      "role": "Extreme sparse gating (top-1) and training dynamics of MoE",
      "relationship_sentence": "Switch Transformers exemplify the sparse routing regime and training dynamics that the current paper probes when testing whether independently trained MoE models admit low-loss linear connections after expert/gate permutation alignment."
    },
    {
      "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of Deep Neural Networks",
      "authors": [
        "Timur Garipov",
        "Pavel Izmailov",
        "Dmitrii Podoprikhin",
        "Dmitry Vetrov",
        "Andrew Gordon Wilson"
      ],
      "year": 2018,
      "role": "Foundational mode connectivity methodology",
      "relationship_sentence": "This work introduced mode connectivity and linear paths in weight space, inspiring the paper\u2019s central question of whether similar linear connections exist between independently trained MoE models."
    },
    {
      "title": "Essentially No Barriers in Neural Network Energy Landscape",
      "authors": [
        "Felix Draxler",
        "Kambis Veschgini",
        "Manfred Salmhofer",
        "Fred A. Hamprecht"
      ],
      "year": 2018,
      "role": "Independent demonstration of low-barrier paths between solutions",
      "relationship_sentence": "By showing low-loss paths between solutions of standard nets, this paper motivates searching for analogous paths in MoE parameter spaces and informs the interpolation protocols used."
    },
    {
      "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis",
      "authors": [
        "Jonathan Frankle",
        "Gintare Karolina Dziugaite",
        "Daniel M. Roy",
        "Michael Carbin"
      ],
      "year": 2020,
      "role": "Conditions for LMC across training runs",
      "relationship_sentence": "This study analyzed when LMC holds (e.g., shared initializations, training trajectories), shaping the present work\u2019s experimental controls and hypotheses about when MoE models can be linearly connected after addressing symmetries."
    },
    {
      "title": "Model Soups: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy",
      "authors": [
        "Mitchell Wortsman",
        "Gabriel Ilharco",
        "Suchin Gururangan",
        "Hadi Salman",
        "Ali Farhadi",
        "Ludwig Schmidt"
      ],
      "year": 2022,
      "role": "Empirical evidence for low-loss linear subspaces and ensembling in weight space",
      "relationship_sentence": "Demonstrating practical gains from weight-space interpolation, this work motivates the utility of establishing LMC in MoE for ensembling and hints that permutation-aware alignment may be necessary for sparse routed models."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014characterizing and empirically validating linear mode connectivity (LMC) in Mixture-of-Experts (MoE) architectures by explicitly accounting for expert and gate permutations\u2014sits at the intersection of two lines of work: mode connectivity in standard networks and the architectural symmetries of MoE. Foundational LMC studies by Garipov et al. and Draxler et al. established that seemingly isolated minima can be connected by low-loss paths, setting the methodological blueprint for probing connectivity via linear or simple curves in parameter space. Frankle et al. further clarified when LMC emerges across training runs, informing the paper\u2019s experimental design (e.g., independence of runs, initialization, and training protocol controls).\nOn the architectural side, Jacobs et al.\u2019s original MoE formulation defines gating and expert exchangeability, making clear that permutations of experts (and their associated gating parameters) are intrinsic symmetries\u2014precisely the invariances the paper formalizes for MoE LMC. Modern scalable MoE implementations by Shazeer et al. (sparsely gated MoE) and Fedus et al. (Switch Transformers) supply the concrete sparse and extreme-sparse routing regimes the authors analyze, highlighting practical training dynamics and permutation structure at scale. Finally, Wortsman et al.\u2019s model soups demonstrate the practical benefits of weight-space interpolation and suggest that achieving low-loss linear combinations in complex models is useful for ensembling\u2014providing motivation to resolve permutation symmetries in MoE so that interpolation (and thus LMC) is realized in practice for both dense and sparse gating cases.",
  "analysis_timestamp": "2026-01-07T00:05:12.541347"
}
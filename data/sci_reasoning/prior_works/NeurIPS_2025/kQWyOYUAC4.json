{
  "prior_works": [
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Yao et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "AI-Researcher adopts ReAct-style interleaving of reasoning traces with tool calls to drive the end-to-end research workflow (searching literature, coding, running experiments, and drafting), and extends it to long-horizon, multi-stage scientific tasks."
    },
    {
      "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
      "authors": "Wu et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "The system extends AutoGen\u2019s conversational multi-agent collaboration by instantiating domain-specific research roles (e.g., Reviewer, Implementer, Author) and structured handoffs to cover the full research pipeline."
    },
    {
      "title": "MetaGPT: Meta Programming for Multi-Agent Collaborative Framework",
      "authors": "Hong et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "AI-Researcher is inspired by MetaGPT\u2019s role-specialized process orchestration and adapts its role-decomposition idea from software projects to the scientific research lifecycle."
    },
    {
      "title": "WebGPT: Browser-assisted question-answering with human feedback",
      "authors": "Nakano et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Its methodology for grounded browsing and citation directly informs AI-Researcher\u2019s literature-review module, which retrieves, reads, and cites papers to motivate hypotheses and justify design choices."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Self-Reflection",
      "authors": "Shinn et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "AI-Researcher incorporates Reflexion-style critique\u2013revise loops to iteratively improve hypotheses, implementations, and manuscript drafts based on failures and evaluator feedback."
    },
    {
      "title": "AgentBench: Evaluating LLMs as Agents",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "AgentBench\u2019s framing of evaluating agents across multi-tool tasks motivates Scientist-Bench, which fills its gap by providing research-domain tasks (guided innovation and open-ended exploration)."
    },
    {
      "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
      "authors": "Wang et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "AI-Researcher borrows SWE-bench\u2019s repository-level, test-driven evaluation philosophy to assess autonomous implementation of SOTA research ideas, extending from bug fixing to full algorithm reproduction and modification."
    }
  ],
  "synthesis_narrative": "AI-Researcher\u2019s core contribution\u2014an autonomous system that executes the entire scientific research pipeline and a benchmark to evaluate it\u2014emerges directly from the agentic reasoning, multi-agent coordination, and grounded evaluation threads in recent LLM research. ReAct provides the operational backbone: interleaving reasoning with tool use enables the system to browse literature, write code, run experiments, and draft manuscripts in a single, coherent loop. Building on this, AutoGen\u2019s multi-agent conversational framework is extended into a research-specific organization, with specialized roles and structured handoffs that cover literature review, hypothesis generation, implementation, evaluation, and writing. MetaGPT\u2019s role-specialized orchestration in software engineering inspires AI-Researcher\u2019s analogous role decomposition for scientific work, translating a proven process pattern to a new domain. For literature review and claim grounding, WebGPT\u2019s browser-assisted, citation-centric method underpins AI-Researcher\u2019s evidence-backed hypothesis and related-work synthesis. Reflexion\u2019s self-critique mechanisms are adapted to long-horizon research, enabling iterative refinement of hypotheses, code, and manuscripts after failed tests or weak results. On the evaluation side, AgentBench motivates an agent-centric assessment perspective but lacks science-specific tasks; AI-Researcher addresses this gap with Scientist-Bench, targeting guided innovation and open-ended research challenges. Finally, SWE-bench\u2019s test-driven, repository-level evaluation philosophy informs AI-Researcher\u2019s rigorous measurement of implementation success, extending from fixing issues to reproducing and advancing state-of-the-art research results.",
  "analysis_timestamp": "2026-01-06T23:08:23.937256"
}
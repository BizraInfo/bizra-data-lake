{
  "prior_works": [
    {
      "title": "Conditional Neural Processes",
      "authors": "Marta Garnelo et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Defines the NP problem formulation (permutation-invariant context-to-target regression with uncertainty) that Gridded TNPs retain while altering the processor architecture."
    },
    {
      "title": "Attentive Neural Processes",
      "authors": "Hyunjik Kim et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "Introduces attention into NPs, boosting accuracy but incurring quadratic cost in context size; the proposed gridded pseudo-token processor directly addresses this scalability bottleneck."
    },
    {
      "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
      "authors": "Juho Lee et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Its induced set attention blocks use inducing tokens to reduce attention cost; Gridded TNPs instantiate this idea as spatially-structured pseudo-tokens on a grid to obtain efficient NP processing."
    },
    {
      "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs",
      "authors": "Andrew Jaegle et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "The latent-bottleneck design with cross-attention to and from a latent array directly inspires using pseudo-tokens; the paper makes these tokens gridded and translation-(approx)equivariant for spatio-temporal NPs with flexible I/O."
    },
    {
      "title": "Convolutional Conditional Neural Processes",
      "authors": "James Gordon et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Introduces set-to-grid and grid-to-set modules to impose translation equivariance but relies on fixed-resolution grids and convolutional processors; Gridded TNPs generalize this pipeline with transformer processors and efficient grid attention to overcome those constraints."
    },
    {
      "title": "Convolutional Gaussian Neural Processes",
      "authors": "Eric A. Foong et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Extends ConvCNPs with principled Gaussian predictive distributions and similar equivariant set\u2194grid interfaces; the new work replaces the convolutional core with a gridded transformer, preserving equivariance while enabling long-range, scalable attention."
    }
  ],
  "synthesis_narrative": "Gridded Transformer Neural Processes sit squarely in the Neural Process lineage introduced by Conditional Neural Processes, keeping the permutation-invariant context-to-target formulation and predictive uncertainty while rethinking how information is processed. Attentive Neural Processes demonstrated that attention dramatically improves NP accuracy, but their quadratic attention cost limits scalability on large spatio-temporal datasets\u2014the central bottleneck this paper tackles. Two key ideas from the attention-for-sets literature provide the enabling mechanism: Set Transformer\u2019s induced set attention showed how inducing tokens can reduce complexity, and Perceiver IO generalized this into a latent bottleneck that can flexibly interface with heterogeneous inputs and outputs via cross-attention. The present paper concretizes these ideas for spatio-temporal NPs by introducing gridded pseudo-tokens\u2014structured inducing/latent tokens laid out on a spatial-temporal grid\u2014allowing efficient grid-based attention while maintaining the NP interface to unstructured context and target sets. On the inductive-bias side, Convolutional Conditional Neural Processes, and their probabilistic refinement in Convolutional Gaussian Neural Processes, pioneered set-to-grid and grid-to-set modules to achieve translation equivariance, but they constrain processing to fixed-resolution grids and convolutional receptive fields. Gridded TNPs directly extend this pipeline: they keep the equivariant encoders/decoders and replace the convolutional core with a grid-transformer processor, enabling exact or approximate translation equivariance with scalable attention and improved long-range modeling.",
  "analysis_timestamp": "2026-01-06T23:07:19.596976"
}
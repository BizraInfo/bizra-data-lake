{
  "prior_works": [
    {
      "title": "Sinkhorn Distances: Lightspeed Computation of Optimal Transport",
      "authors": "Marco Cuturi",
      "year": 2013,
      "role": "Introduced entropic regularization for OT, yielding smooth, efficiently computable dual objectives.",
      "relationship_sentence": "The paper\u2019s expectile regularization continues Cuturi\u2019s idea of replacing hard c-transform constraints with a stable surrogate, but proposes an alternative regularizer tailored to enforcing dual binding conditions without costly inner optimization."
    },
    {
      "title": "Stochastic Optimization for Large-scale Optimal Transport",
      "authors": "Aude Genevay, Gabriel Peyr\u00e9, Marco Cuturi",
      "year": 2016,
      "role": "Established semi-dual stochastic training of Kantorovich potentials and practical c-transform approximations from samples.",
      "relationship_sentence": "This work exposed the computational bottleneck of repeatedly approximating the c-transform during neural training, directly motivating the paper\u2019s goal to eliminate that step via an expectile-based loss."
    },
    {
      "title": "Input Convex Neural Networks",
      "authors": "Brandon Amos, Lei Xu, J. Zico Kolter",
      "year": 2017,
      "role": "Provided a practical parameterization for convex functions and their conjugates.",
      "relationship_sentence": "The proposed method relies on learning dual potentials that respect convex-analytic structure; ICNNs underpin representing Kantorovich potentials and their conjugates, to which the expectile regularization is applied."
    },
    {
      "title": "Optimal Transport Mapping via Input Convex Neural Networks",
      "authors": "Yaron K. Makkuva, Amirhossein Taghvaei, Hyeongjun Oh, Sewoong Oh Lee",
      "year": 2020,
      "role": "Demonstrated learning OT maps by coupling convex potentials with conjugate computations, typically via min\u2013max training.",
      "relationship_sentence": "By highlighting the non-convex min\u2013max and conjugate-computation burdens, this work sets the stage for the paper\u2019s expectile loss as a direct, single-stage alternative to enforce dual constraints without inner maximization."
    },
    {
      "title": "Learning with Fenchel-Young Losses",
      "authors": "Mathieu Blondel, Andr\u00e9 F. T. Martins, Vlad Niculae",
      "year": 2019,
      "role": "Unified a family of convex losses built from regularizers and convex conjugates to yield tight upper bounds and stable training.",
      "relationship_sentence": "The expectile objective can be interpreted in the Fenchel-Young spirit as a convex-analytic surrogate that upper bounds target quantities, grounding the paper\u2019s theoretical claim that expectile regularization upper-bounds conjugate potentials."
    },
    {
      "title": "Asymmetric Least Squares Estimation and a Class of Quantile Models",
      "authors": "Whitney K. Newey, James L. Powell",
      "year": 1987,
      "role": "Introduced expectile regression via asymmetric squared loss and characterized its tail-weighting properties.",
      "relationship_sentence": "The core innovation\u2014using expectile regularization to enforce binding conditions and produce upper-bound estimates of conjugate potentials\u2014directly builds on the statistical properties of expectiles from this foundational work."
    },
    {
      "title": "Offline Reinforcement Learning with Implicit Q-Learning",
      "authors": "Ilya Kostrikov, Ashvin Nair, Sergey Levine",
      "year": 2021,
      "role": "Used expectile regression to estimate conservative/upper-tail value functions for stability in RL.",
      "relationship_sentence": "This modern application of expectiles as a stability-inducing, upper-tail estimator informs the paper\u2019s design choice to use expectiles to stabilize neural OT training and avoid fine-tuning of c-transforms."
    }
  ],
  "synthesis_narrative": "Neural optimal transport methods rely on dual Kantorovich potentials whose conjugate (the c-transform) enforces the inequality constraints and binding conditions that characterize optimal plans. Existing solvers often approximate the c-transform via inner maximization or per-sample fine-tuning, making training unstable and computationally heavy. Genevay\u2013Peyr\u00e9\u2013Cuturi (2016) highlighted these costs in stochastic semi-dual training, while ICNN-based approaches (Amos et al., 2017; Makkuva et al., 2020) showed how to parameterize convex potentials but still required non-convex min\u2013max or explicit conjugate computations. Cuturi\u2019s entropic regularization (2013) established that carefully chosen surrogates for hard constraints can make OT training both smooth and efficient.\nBuilding on this lineage, the paper reframes the c-transform approximation as a learning problem with a principled surrogate: expectile regularization. The statistical foundation of expectiles (Newey & Powell, 1987) provides an asymmetric squared loss that targets upper tails, effectively yielding an upper-bound estimator over the distribution of feasible conjugate potentials. This mirrors the Fenchel-Young perspective (Blondel et al., 2019), where convex-analytic losses provide tight upper bounds and stable optimization landscapes. Recent success of expectiles in IQL (Kostrikov et al., 2021) further demonstrates their stabilizing effect when estimating upper-tail functionals. Together, these works shape a method that enforces dual binding conditions through an expectile-based loss, removing the need for inner c-transform fine-tuning while retaining accuracy and improving training stability.",
  "analysis_timestamp": "2026-01-06T23:39:42.964762"
}
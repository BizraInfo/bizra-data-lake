{
  "prior_works": [
    {
      "title": "Deep Learning with Differential Privacy",
      "authors": "Mart\u00edn Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang",
      "year": 2016,
      "role": "Target algorithm and accounting framework",
      "relationship_sentence": "This work introduced DP-SGD and the moments accountant, providing the concrete training procedure and theoretical privacy benchmarks that the one-run auditor is designed to empirically lower-bound with a single model."
    },
    {
      "title": "Calibrating Noise to Sensitivity in Private Data Analysis",
      "authors": "Cynthia Dwork, Frank McSherry, Kobbi Nissim, Adam Smith",
      "year": 2006,
      "role": "Foundational DP definition and composition (including parallel composition)",
      "relationship_sentence": "The paper\u2019s ability to add/remove many examples independently and audit them in parallel without incurring linear group-privacy blowup relies directly on the DP notion and its parallel composition principle originating in this work."
    },
    {
      "title": "The Reusable Holdout: Preserving Validity in Adaptive Data Analysis",
      "authors": "Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, Aaron Roth",
      "year": 2015,
      "role": "DP implies generalization",
      "relationship_sentence": "The core analysis\u2014linking differential privacy to statistical generalization to avoid paying the cost of group privacy\u2014builds on the formal guarantee that DP mechanisms generalize, established by this line of work."
    },
    {
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "authors": "Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov",
      "year": 2017,
      "role": "Auditing via membership inference",
      "relationship_sentence": "The one-run auditing framework inherits the idea of empirically probing privacy by testing membership, and adapts it to a DP-specific, parallel, and statistically grounded audit rather than repeated retraining."
    },
    {
      "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
      "authors": "Samuel Yeom, Irene Giacomelli, Matt Fredrikson, Somesh Jha",
      "year": 2018,
      "role": "Link between membership inference and generalization/loss",
      "relationship_sentence": "By formalizing how generalization and loss relate to membership inference, this work directly informs the paper\u2019s statistical tests that aggregate many independent example-level probes without group-privacy penalties."
    },
    {
      "title": "The Secret Sharer: Measuring Unintended Memorization in Neural Networks",
      "authors": "Nicholas Carlini, Chang Liu, Jernej Kos, \u00dalfar Erlingsson, Dawn Song",
      "year": 2019,
      "role": "Single-run canary insertion paradigm",
      "relationship_sentence": "Demonstrating that many canaries can be inserted and evaluated within a single training run to quantify memorization inspired the paper\u2019s single-run, multi-probe design, adapted here to certify DP leakage via formal analysis."
    },
    {
      "title": "Evaluating Differentially Private Machine Learning in Practice",
      "authors": "Bargav Jayaraman, David Evans",
      "year": 2019,
      "role": "Empirical auditing of DP-SGD and limitations of multi-run audits",
      "relationship_sentence": "By showing practical challenges and limited power of prior audits that require many training runs of DP-SGD, this work motivated and contextualized the need for the paper\u2019s one-run auditing scheme."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014auditing differentially private training with a single run by probing many examples in parallel\u2014sits at the intersection of DP theory, generalization guarantees, and empirical privacy auditing. Its theoretical backbone draws from two foundational strands: (i) the original formulation of differential privacy and its composition properties, especially parallel composition, which enable changing many disjoint records without linear privacy loss; and (ii) results establishing that DP mechanisms generalize, allowing the authors to analyze aggregated, per-example probes statistically without invoking costly group privacy. On the empirical side, the auditing methodology evolves from membership inference attacks, reframed here as a principled test of privacy loss that can be executed in black-box or white-box settings. Yeom et al.\u2019s connection between loss, overfitting, and membership advantage provides the quantitative link that underlies the paper\u2019s test statistics, while Shokri et al. supplies the auditing paradigm. The single-run, many-probe design is further inspired by memorization measurement via canary insertion, which showed that large-scale probing of a single model can be both feasible and informative. Finally, DP-SGD and its moments accountant supply the primary target and baseline for evaluation; prior practical evaluations highlighted the inefficiency of multi-run audits, directly motivating the proposed one-run framework that yields meaningful empirical lower bounds with far less computational cost.",
  "analysis_timestamp": "2026-01-07T00:02:04.871039"
}
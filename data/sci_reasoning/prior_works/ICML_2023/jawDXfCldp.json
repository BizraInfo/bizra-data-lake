{
  "prior_works": [
    {
      "title": "Statistical Treatment Rules",
      "authors": "Charles F. Manski",
      "year": 2004,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized treatment choice as a statistical decision problem, providing the decision-theoretic welfare objective that underlies the policy-targeting tasks for which the present work designs experiments."
    },
    {
      "title": "Who Should Be Treated? Empirical Welfare Maximization Methods for Treatment Choice",
      "authors": "Toru Kitagawa et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "It introduced empirical welfare maximization as a learning objective for treatment assignment policies; the current paper explicitly designs experiments to minimize regret for such policy-learning tasks rather than purely estimating ATE."
    },
    {
      "title": "Adaptive Treatment Assignment in Experiments for Welfare Maximization",
      "authors": "Maximilian Kasy et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "By advocating covariate-dependent/adaptive assignment specifically for policy learning, this work identified the need for task-aware experimental design but focused on sequential welfare maximization; the present paper fills the gap by deriving closed-form, batch sampling rules tailored to multiple downstream tasks (e.g., targeting/ranking) beyond welfare alone."
    },
    {
      "title": "Doubly Robust Policy Evaluation and Learning",
      "authors": "Miroslav Dudik et al.",
      "year": 2011,
      "role": "Extension",
      "relationship_sentence": "The paper\u2019s IPS/DR framework provides the estimators and risk decompositions whose asymptotic variance depends on propensities; the current work extends this by optimizing assignment probabilities to minimize DR risk for specific downstream decision tasks."
    },
    {
      "title": "On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects",
      "authors": "Jinyong Hahn",
      "year": 1998,
      "role": "Extension",
      "relationship_sentence": "Hahn\u2019s efficient influence-function analysis for ATE links estimator variance to treatment propensities; the present paper generalizes this influence-function design logic from ATE (recovering e(x)=0.5 as a special case) to task-specific objectives like policy targeting and ranking."
    },
    {
      "title": "Policy Learning with Observational Data",
      "authors": "Susan Athey et al.",
      "year": 2021,
      "role": "Related Problem",
      "relationship_sentence": "This work develops orthogonalized, doubly robust objectives and regret bounds for policy learning on fixed data; the current paper targets the same policy-learning objective but moves upstream to design the data-collection propensities that minimize the resulting learning error."
    },
    {
      "title": "Real-World Uplift Modelling with Significance",
      "authors": "Nicholas J. Radcliffe et al.",
      "year": 2011,
      "role": "Foundation",
      "relationship_sentence": "By defining the uplift/individualized targeting objective and associated evaluation (e.g., Qini), this paper supplies the downstream task for which the present work derives tailored experimental sampling strategies."
    }
  ],
  "synthesis_narrative": "The core innovation of task-specific experimental design arises at the intersection of treatment choice, policy learning, and semiparametric efficiency. Manski\u2019s seminal formulation of statistical treatment rules cast treatment assignment as a welfare-maximizing decision problem, later operationalized by Kitagawa and Tetenov through empirical welfare maximization (EWM) for policy learning. These works define the downstream objectives\u2014choosing who to treat or target\u2014on which the present paper focuses.\n\nOn the estimation side, Dudik et al. introduced doubly robust policy evaluation and learning, making explicit how estimator risk depends on the logging propensities. Hahn\u2019s influence-function analysis for ATE further clarifies how propensities drive efficiency, yielding e(x)=0.5 as optimal for ATE\u2014providing the blueprint the current paper generalizes: rather than designing for ATE alone, it derives propensity functions that minimize the DR (or influence-function) risk for specific tasks such as policy targeting and uplift ranking.\n\nKasy and Sautmann directly motivate the need for task-aware experimentation by proposing adaptive, covariate-dependent assignment for welfare maximization; the present work addresses their limitations by providing nonadaptive, closed-form sampling rules that cover multiple downstream tasks and operate in batch settings. Finally, Athey and Wager\u2019s framework for policy learning with orthogonalized, DR objectives and Radcliffe and Surry\u2019s uplift formulation pin down the concrete targets\u2014policy regret and uplift performance\u2014so the experiment itself can be optimized for the end use. Together, these works enable and directly inspire the paper\u2019s central idea: choose treatment propensities to optimize the ultimate task, not just generic effect estimation.",
  "analysis_timestamp": "2026-01-06T23:09:26.519376"
}
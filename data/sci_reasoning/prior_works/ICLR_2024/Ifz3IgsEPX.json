{
  "prior_works": [
    {
      "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
      "authors": "Brian Lester et al.",
      "year": 2021,
      "arxiv_id": "2104.08691",
      "role": "Gap Identification",
      "relationship_sentence": "This work established soft prompt tuning as an effective adaptation method but requires embedding-level access to the model, highlighting the practical and privacy limitations that DP-OPT overcomes by shifting to offsite, differentially private discrete prompts."
    },
    {
      "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
      "authors": "Taylor Shin et al.",
      "year": 2020,
      "arxiv_id": "2010.15980",
      "role": "Inspiration",
      "relationship_sentence": "AutoPrompt showed that automatic search over discrete textual triggers can achieve strong performance, directly inspiring DP-OPT's automatic discrete prompt optimization paradigm while replacing gradient-based search with LLM-generated candidates and private selection."
    },
    {
      "title": "Large Language Models are Human-Level Prompt Engineers",
      "authors": "Jiaxin Zhou et al.",
      "year": 2022,
      "arxiv_id": "2211.01910",
      "role": "Extension",
      "relationship_sentence": "DP-OPT extends the APE pipeline of having an LLM generate and rank candidate instruction prompts by introducing a differentially private selection mechanism and offsite deployment to target cloud models."
    },
    {
      "title": "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
      "authors": "Jinghui Deng et al.",
      "year": 2022,
      "arxiv_id": "2205.12548",
      "role": "Baseline",
      "relationship_sentence": "RLPrompt provides a main black-box discrete prompt optimization baseline that DP-OPT improves upon by enabling privacy-preserving evaluation/selection and removing the need to interact with the target provider\u2019s model during tuning."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2023,
      "arxiv_id": "2212.10560",
      "role": "Inspiration",
      "relationship_sentence": "Self-Instruct demonstrated that LLMs can generate high-quality, transferable task instructions, a key insight DP-OPT leverages by using LLMs as local prompt engineers whose outputs are then privately selected for deployment."
    },
    {
      "title": "Mechanism Design via Differential Privacy",
      "authors": "Frank McSherry and Kunal Talwar",
      "year": 2007,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This paper introduces the exponential mechanism for privately selecting high-utility items from a discrete set, which DP-OPT instantiates to choose the best prompt based on sensitive client data under formal DP guarantees."
    }
  ],
  "synthesis_narrative": "Soft prompt tuning established that adapting only prompt parameters can effectively steer large models, but its reliance on embedding-level access makes it impractical with hosted, closed models and raises privacy risks when tuned artifacts encode sensitive data. AutoPrompt revealed that automatic search over discrete textual triggers can elicit model behavior competitively, inaugurating a line of discrete prompt optimization that avoids modifying model weights. RLPrompt further showed that discrete prompts can be optimized in a black-box fashion, scoring candidates via task performance without internal access to the model. In parallel, Automatic Prompt Engineer demonstrated that LLMs themselves can generate and iteratively refine candidate instruction prompts from task I/O, and Self-Instruct showed such LLM-generated instructions are high-quality and transferable across models and tasks. Crucially, the exponential mechanism from differential privacy provides a principled way to select a high-utility item from a discrete set while protecting the underlying sensitive data used to score candidates.\nTogether, these works revealed a path: use LLMs as prompt generators to produce strong, transferable discrete instructions; evaluate candidates on private client data without touching the provider\u2019s model; and ensure privacy by applying a DP selection mechanism. DP-OPT synthesizes these components by conducting LLM-driven, offsite discrete prompt search and instantiating the exponential mechanism to privately pick prompts, thereby addressing soft-prompt access constraints and privacy leakage while enabling deployment on unmodified cloud LLMs.",
  "target_paper": {
    "title": "DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer",
    "authors": "Junyuan Hong, Jiachen T. Wang, Chenhui Zhang, Zhangheng LI, Bo Li, Zhangyang Wang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "large language model, privacy, prompt tuing",
    "abstract": "Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local LLM and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning (DP-OPT) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising performance significantly. To ensure that the prompts do not",
    "openreview_id": "Ifz3IgsEPX",
    "forum_id": "Ifz3IgsEPX"
  },
  "analysis_timestamp": "2026-01-06T14:55:39.169163"
}
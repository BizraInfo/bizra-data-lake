{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Foundational infinite-width training theory",
      "relationship_sentence": "The paper\u2019s Graphon NTK directly extends the NTK framework by replacing finite connectivity with a graphon-defined limit, inheriting NTK\u2019s linearized training dynamics to analyze pruned sparse networks at infinite width."
    },
    {
      "title": "Wide Neural Networks of Any Depth Evolve as Linear Models under Gradient Descent",
      "authors": "Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington",
      "year": 2019,
      "role": "Formalization of NTK training dynamics",
      "relationship_sentence": "This work\u2019s rigorous characterization of gradient descent dynamics in the infinite-width limit underpins the derivation and use of the Graphon NTK to study the trainability of pruned architectures."
    },
    {
      "title": "Limits of Dense Graph Sequences",
      "authors": "L\u00e1szl\u00f3 Lov\u00e1sz, Bal\u00e1zs Szegedy",
      "year": 2006,
      "role": "Core graphon limit theory",
      "relationship_sentence": "Introduces graphons and convergence of graph sequences, providing the mathematical backbone for modeling layer-wise connectivity patterns as width grows and for positing the Graphon Limit Hypothesis."
    },
    {
      "title": "An Lp Theory of Sparse Graph Convergence",
      "authors": "Christian Borgs, Jennifer T. Chayes, Henry Cohn, Nathan Holden",
      "year": 2018,
      "role": "Graphon limits for sparse graphs",
      "relationship_sentence": "Extends graphon theory to sparse regimes, enabling the paper to rigorously treat pruned neural network connectivity as sparse graphons and to analyze their convergence properties."
    },
    {
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "authors": "Jonathan Frankle, Michael Carbin",
      "year": 2019,
      "role": "Problem motivation and phenomenon",
      "relationship_sentence": "Provides the central empirical puzzle\u2014why certain sparse subnetworks are trainable\u2014motivating the paper\u2019s graphon-based explanation of implicit structural biases induced by pruning."
    },
    {
      "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity",
      "authors": "Namhoon Lee, Thalaiyasingam Ajanthan, Philip H. S. Torr",
      "year": 2019,
      "role": "Representative pruning method defining connectivity bias",
      "relationship_sentence": "As a pruning-at-initialization method, SNIP induces specific sparse connectivity patterns whose large-width limits can be captured by graphons, illustrating method-dependent structural biases analyzed via Graphon NTK."
    },
    {
      "title": "Rigging the Lottery: Making All Tickets Winners",
      "authors": "Mustafa Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, Erich Elsen",
      "year": 2020,
      "role": "Dynamic sparse training and topology evolution",
      "relationship_sentence": "RigL shows training-time evolution of sparse topologies; the paper\u2019s framework interprets such evolving patterns through convergence to characteristic graphons, enabling comparisons of trainability via Graphon NTK."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014a Graphon Limit Hypothesis for pruned neural networks and the derivation of a Graphon NTK\u2014sits at the intersection of infinite-width learning theory and graph limit theory, applied to the longstanding pruning puzzle. On the learning-theoretic side, Jacot et al. (2018) introduced the Neural Tangent Kernel, and Lee et al. (2019) established that wide networks evolve as linear models under gradient descent. These works supply the analytical infrastructure for translating network training dynamics into kernel evolution in the infinite-width regime; the present paper extends this to sparsity by defining an NTK parameterized by a limiting graphon that encodes connectivity structure.\n\nOn the graph-theoretic side, Lov\u00e1sz and Szegedy (2006) created the graphon framework for limits of graph sequences, and Borgs et al. (2018) generalized it to sparse settings. These results justify modeling layer-wise bipartite connectivity patterns as graphons and formalize convergence of pruning-induced patterns as width increases\u2014precisely the paper\u2019s Graphon Limit Hypothesis.\n\nFinally, empirical pruning literature provides both the motivation and the concrete connectivity distributions to analyze. The Lottery Ticket Hypothesis (Frankle & Carbin, 2019) crystallized the observation that some sparse subnetworks are especially trainable, while methods such as SNIP (Lee et al., 2019) and dynamic sparse training via RigL (Evci et al., 2020) demonstrably impose different structural biases on connectivity. By mapping these method-specific sparsity patterns to limiting graphons and importing NTK dynamics, the paper offers a unifying theory of how structural biases impact trainability in the infinite-width limit.",
  "analysis_timestamp": "2026-01-07T00:21:32.227190"
}
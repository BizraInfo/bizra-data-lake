{
  "prior_works": [
    {
      "title": "Multivariate Stochastic Approximation Using a Simultaneous Perturbation Gradient Approximation",
      "authors": "James C. Spall",
      "year": 1992,
      "role": "Foundational zeroth-order gradient estimator (SPSA)",
      "relationship_sentence": "MeZO directly instantiates the two-point simultaneous perturbation idea from SPSA\u2014estimating gradients from paired forward evaluations along random directions\u2014to update all LM parameters without backprop."
    },
    {
      "title": "Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming",
      "authors": "Saeed Ghadimi, Guanghui Lan",
      "year": 2013,
      "role": "Theory and algorithmic template for ZO-SGD with Gaussian smoothing",
      "relationship_sentence": "The paper adapts ZO-SGD-style updates grounded in Ghadimi & Lan\u2019s Gaussian-smoothing analysis, but implements them in-place to match inference memory while training large LMs."
    },
    {
      "title": "Random Gradient-Free Minimization of Convex Functions",
      "authors": "Yurii Nesterov, Vladimir Spokoiny",
      "year": 2017,
      "role": "Variance/rate analysis for random-direction zeroth-order methods",
      "relationship_sentence": "MeZO is motivated by this line\u2019s insights on two-point random-direction estimators and their dimension dependence, and shows that a practical in-place realization can work at LM scale despite theory\u2019s pessimism."
    },
    {
      "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
      "authors": "Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, Ilya Sutskever",
      "year": 2017,
      "role": "Scalable parameter-space zeroth-order optimization for deep models",
      "relationship_sentence": "This work demonstrated that random-perturbation gradient estimates can scale to very high-dimensional neural networks, directly inspiring MeZO\u2019s forward-only, parameter-space training paradigm."
    },
    {
      "title": "ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks",
      "authors": "Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh",
      "year": 2017,
      "role": "Application of ZO estimators to deep nets via query-based gradients",
      "relationship_sentence": "By showing that neural network objectives admit usable zeroth-order gradients from only function queries, ZOO provided empirical grounding for MeZO\u2019s use of forward-pass-only gradient estimates."
    },
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen",
      "year": 2022,
      "role": "Parameter-efficient fine-tuning to reduce memory cost",
      "relationship_sentence": "LoRA framed the memory bottleneck of full-parameter LM fine-tuning; MeZO tackles the same constraint from an orthogonal angle\u2014zeroth-order in-place updates\u2014achieving full-model adaptation with inference-level memory."
    },
    {
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "authors": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer",
      "year": 2023,
      "role": "Memory-efficient LM fine-tuning via 4-bit quantization and paged optimizers",
      "relationship_sentence": "QLoRA exemplifies state-of-the-art memory-saving backprop pipelines; MeZO provides an alternative by eliminating backprop and optimizer states entirely through zeroth-order, forward-only training."
    }
  ],
  "synthesis_narrative": "MeZO\u2019s core idea\u2014fine-tuning large language models using only forward passes\u2014stands on the classic zeroth-order optimization paradigm developed for high-dimensional problems. Spall\u2019s SPSA introduced the two-point simultaneous perturbation estimator, a minimal-query mechanism to approximate gradients from paired function evaluations; Ghadimi and Lan formalized stochastic zeroth-order methods via Gaussian smoothing and ZO-SGD templates; and Nesterov\u2013Spokoiny clarified the variance and dimensional dependence of random-direction estimators. Together, these works provide the estimator, update rule, and theoretical caveats that MeZO adapts.\n\nA second lineage demonstrated that such estimators can be practical for deep nets. OpenAI\u2019s Evolution Strategies showed parameter-space perturbations scaling to millions of parameters, while ZOO established that query-based gradient estimates can effectively steer neural network objectives. These results directly motivate MeZO\u2019s claim that forward-only, parameter-space optimization can work even for modern LMs, provided the implementation is efficient.\n\nFinally, recent memory-efficient fine-tuning techniques for LMs\u2014LoRA and QLoRA\u2014shaped the problem focus: enabling strong downstream adaptation under tight GPU memory budgets. Whereas these approaches reduce memory by restricting trainable subspaces or quantizing weights while still relying on backprop, MeZO attacks the bottleneck at its source by eliminating backprop and optimizer state, executing ZO-SGD in-place so that training\u2019s memory footprint matches inference. The synthesis of classical ZO estimators with LM-scale engineering yields a surprisingly competitive and highly memory-frugal fine-tuning method.",
  "analysis_timestamp": "2026-01-07T00:02:04.799116"
}
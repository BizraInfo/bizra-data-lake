{
  "prior_works": [
    {
      "title": "Connecting Vision and Language with Localized Narratives",
      "authors": "Antoni Pont-Tuset, Jasper R. R. Uijlings, Soravit Changpinyo, Radu Soricut, Vittorio Ferrari",
      "year": 2020,
      "role": "dataset/method",
      "relationship_sentence": "Introduced point-and-speak localized narrative traces that tightly couple language with spatial attention, directly enabling Voila-A\u2019s idea of using localized narratives to mimic and supervise human gaze patterns."
    },
    {
      "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
      "authors": "Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, Dhruv Batra",
      "year": 2016,
      "role": "analysis/dataset",
      "relationship_sentence": "Established that human attention maps benefit VQA and revealed a gap between model and human focus, motivating Voila-A\u2019s explicit alignment of VLM attention with user gaze."
    },
    {
      "title": "Where are they looking?",
      "authors": "Adri\u00e0 Recasens, Cristina Vondrick, Aditya Khosla, Antonio Torralba",
      "year": 2015,
      "role": "method/dataset",
      "relationship_sentence": "Modeled gaze as image-conditioned spatial distributions, informing Voila-A\u2019s representation and injection of gaze heatmaps as a prior to guide vision-language reasoning."
    },
    {
      "title": "Microsoft COCO: Common Objects in Context",
      "authors": "Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C. Lawrence Zitnick",
      "year": 2014,
      "role": "dataset",
      "relationship_sentence": "Provides the complex, multi-object scenes that underpin VOILA-COCO, ensuring the gaze-aligned supervision targets the real-world visual complexity Voila-A aims to handle."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li, Dongxu Li, Silvio Savarese, Steven C. H. Hoi",
      "year": 2023,
      "role": "architecture",
      "relationship_sentence": "Supplied a widely adopted VLM architecture bridging visual encoders and LLMs, which Voila-A extends by conditioning this bridge on user gaze to steer attention and responses."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",
      "year": 2023,
      "role": "method/dataset",
      "relationship_sentence": "Pioneered GPT-4\u2013assisted generation of multimodal instruction data and conversational fine-tuning, directly inspiring Voila-A\u2019s GPT-4\u2013based automatic annotation pipeline for VOILA-COCO and its instruction-style training."
    }
  ],
  "synthesis_narrative": "Voila-A\u2019s core innovation\u2014aligning vision-language models with user gaze\u2014sits at the intersection of human attention supervision, fine-grained grounding, and modern instruction-tuned VLMs. The Localized Narratives work demonstrated a scalable way to attach continuous spatial traces to natural language, showing that pointing-based narratives can act as a faithful, low-friction proxy for human visual attention. This directly motivated Voila-A\u2019s use of localized narratives to mimic and supervise gaze signals. Complementing this, Human Attention in VQA quantified the mismatch between model and human focus and showed the utility of human attention maps, providing the conceptual impetus for explicit attention alignment in multimodal reasoning. Methodologically, Where are they looking? modeled gaze as image-conditioned spatial priors, informing Voila-A\u2019s representation and integration of gaze heatmaps to guide the model toward user-intended regions.\nOn the VLM side, BLIP-2 established a practical blueprint for coupling frozen visual encoders with large language models, a scaffolding that Voila-A can condition with gaze to modulate cross-modal attention. LLaVA\u2019s visual instruction tuning and GPT-4\u2013assisted data generation directly influenced Voila-A\u2019s automatic annotation pipeline and conversational training paradigm, which Voila-A extends with gaze-aware prompts and supervision. Finally, MS COCO\u2019s complex, multi-object imagery underpins VOILA-COCO, ensuring that gaze alignment is learned in realistic, cluttered scenes where user-specific attention matters most. Together, these works form the methodological and empirical backbone enabling Voila-A to operationalize user gaze as a controllable signal for VLM alignment.",
  "analysis_timestamp": "2026-01-06T23:42:49.026172"
}
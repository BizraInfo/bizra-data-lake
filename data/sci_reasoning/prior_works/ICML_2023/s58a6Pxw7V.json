{
  "prior_works": [
    {
      "title": "Variational Learning of Inducing Variables in Sparse Gaussian Processes",
      "authors": "Michalis Titsias et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "Provides the variational inducing-variable GP framework and ELBO that underpins both inter-domain inducing features and the decoupled/orthogonally-decoupled approximations extended by this paper."
    },
    {
      "title": "A Framework for Interdomain and Multioutput Gaussian Processes",
      "authors": "Mark van der Wilk et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Formalizes inducing variables as linear inter-domain operators; the proposed spherical inter-domain features and NN-activation inducing variables are instantiated directly within this framework."
    },
    {
      "title": "Variational Fourier Features for Gaussian Processes",
      "authors": "James Hensman et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "Demonstrates a practical inter-domain inducing-feature construction (Fourier features); the present work generalizes this idea to learn data-dependent spherical features and neural activation features for both principal and orthogonal components."
    },
    {
      "title": "Variational Inference for Gaussian Process Models with Linear Complexity",
      "authors": "Cheng and Boots",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "Introduces decoupled inducing sets for mean and covariance but suffers from instability/conditioning issues; the current paper addresses these limitations by adopting an orthogonal decomposition and designing spherical inter-domain features to stabilize and scale representation learning."
    },
    {
      "title": "Orthogonally Decoupled Variational Gaussian Processes",
      "authors": "Salimbeni et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Provides the orthogonal decomposition of GP variational approximations that this paper builds on; the proposed spherical inter-domain features extend ODGP by learning flexible, data-dependent bases for both the principal and orthogonal subspaces."
    },
    {
      "title": "Rates of Convergence for Sparse Variational Gaussian Process Regression",
      "authors": "James Burt et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Shows that approximation quality hinges on the inducing-feature span and motivates data-dependent bases; the paper directly targets this by learning spherical inter-domain features to reduce projection error in both components of the decomposition."
    },
    {
      "title": "Deep Neural Networks as Gaussian Processes",
      "authors": "Jaehoon Lee et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "Establishes the NN\u2013GP correspondence that motivates using NN hidden-unit activations as inducing variables; the present method operationalizes this idea within the inter-domain/orthogonal framework to enable scalable representation learning."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014learning data-dependent, spherical inter-domain inducing features within an orthogonally-decoupled variational GP\u2014sits at the intersection of variational sparse GPs, inter-domain inducing constructions, and the NN\u2013GP connection. The variational inducing-variable foundation of Titsias (2009) provides the ELBO and conditioning structure on which all subsequent extensions rely. Building on this, van der Wilk et al. (2020) formalized inter-domain inducing variables as linear operators, enabling inducing features beyond input-space points; Hensman et al. (2017) exemplified this with variational Fourier features, showing how to engineer inducing features in transformed domains. In parallel, efforts to scale variational GPs via decoupling (Cheng & Boots, 2017) exposed instability and conditioning issues, which orthogonally-decoupled variational GPs (Salimbeni et al., 2018) resolved by decomposing the function into principal and orthogonal components. The present work directly extends ODGP by designing spherical, data-dependent inter-domain features for both components, thereby stabilizing and improving flexibility. This choice is also theoretically motivated by Burt et al. (2019), who showed that approximation error is governed by the span of inducing features, motivating learned bases that minimize projection error. Finally, the use of NN hidden-unit activations as inducing variables is inspired by the NN\u2013GP correspondence (Lee et al., 2018), enabling representation learning within a GP framework. Together, these works directly enable, motivate, and define the gaps addressed by the proposed spherical inter-domain features for orthogonally-decoupled GPs.",
  "analysis_timestamp": "2026-01-06T23:09:26.539334"
}
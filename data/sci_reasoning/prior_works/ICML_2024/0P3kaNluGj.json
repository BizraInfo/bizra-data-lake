{
  "prior_works": [
    {
      "title": "Programmatically Interpretable Reinforcement Learning",
      "authors": "Abhinav Verma et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Established the problem formulation of learning symbolic/programmatic policies for interpretability; the present work adopts symbolic policies as the core policy representation and extends this line by learning them jointly with perception from pixels."
    },
    {
      "title": "Deep Symbolic Reinforcement Learning",
      "authors": "Marta Garnelo et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "Proposed a neuro-symbolic pipeline that constructs structured (symbolic) state from vision and learns an interpretable policy, but with a largely fixed perception stage; the new paper directly improves on this by enabling reward-guided refinement of the structured state via distillation and end-to-end training."
    },
    {
      "title": "VIPER: Verifiable Reinforcement Learning via Policy Extraction",
      "authors": "Osbert Bastani et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Showed how to extract decision-tree policies post hoc for interpretability, highlighting the limitation of non-end-to-end interpretability; the current work addresses this gap by learning symbolic policies directly and jointly with perception, rather than extracting them after training a black-box policy."
    },
    {
      "title": "Slot Attention: Object-Centric Learning with Slot Attention",
      "authors": "Mario Locatello et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Introduced a general mechanism for obtaining structured, object-centric state representations from images; the present work builds on this idea of structured states but makes it practical for RL by distilling a vision foundation model and refining the perception module with reward signals."
    },
    {
      "title": "DINOv2: Learning Robust Visual Features without Supervision",
      "authors": "Maxime Oquab et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Provides strong, general-purpose vision foundation model features that the paper explicitly leverages by distilling into a lightweight perception module that produces structured states suitable for symbolic policies."
    },
    {
      "title": "Distilling the Knowledge in a Neural Network",
      "authors": "Geoffrey Hinton et al.",
      "year": 2015,
      "role": "Extension",
      "relationship_sentence": "Introduced knowledge distillation, the core technique the paper adapts to transfer a vision foundation model into an efficient student perception module that can be refined during policy learning."
    },
    {
      "title": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations",
      "authors": "Upol Ehsan et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "Pioneered generating textual explanations for RL agents\u2019 decisions; the new paper extends this idea by prompting GPT-4 to produce policy- and decision-level explanations grounded in learned symbolic policies, reducing cognitive load for users."
    }
  ],
  "synthesis_narrative": "This work sits at the intersection of interpretable policy learning, structured visual representations, and modern vision and language foundations. Programmatically Interpretable Reinforcement Learning (Verma et al.) crystallized the goal of learning symbolic policies, establishing the interpretability target that the present paper retains. Deep Symbolic Reinforcement Learning (Garnelo et al.) demonstrated the neuro-symbolic pipeline from pixels to symbolic decision-making, but relied on a largely fixed perception stage; that limitation, echoed by post-hoc extraction approaches like VIPER (Bastani et al.), directly motivates the paper\u2019s central contribution: end-to-end neuro-symbolic RL that jointly learns structured states and symbolic policies. The structured-state premise is rooted in object-centric perception methods such as Slot Attention (Locatello et al.), which showed how to form discrete, compositional representations from images. To make such perception both effective and RL-efficient, the authors draw on recent vision foundation models\u2014specifically DINOv2 (Oquab et al.)\u2014and transfer their capacity into a compact student via knowledge distillation (Hinton et al.), enabling reward-driven refinement of perception during policy learning. Finally, the work\u2019s textual explanations connect to rationalization for RL agents (Ehsan et al.), but exploit the alignment between symbolic policies and language, using GPT-4 prompting to generate faithful, low-cognitive-load explanations of policies and decisions. Together, these threads produce a practical, end-to-end neuro-symbolic RL framework with both learnable perception and accessible explanations.",
  "analysis_timestamp": "2026-01-06T23:09:26.460254"
}
{
  "prior_works": [
    {
      "title": "Circuits",
      "authors": "Chris Olah, Nick Cammarata, Ludwig Schubert, et al.",
      "year": 2020,
      "role": "Conceptual framework",
      "relationship_sentence": "Introduced the core notion of circuits as sparse, compositional subnetworks mediating behaviors, establishing the interpretability lens that ACDC seeks to automate."
    },
    {
      "title": "A Mathematical Framework for Transformer Circuits",
      "authors": "Nelson Elhage, Neel Nanda, Catherine Olsson, et al.",
      "year": 2021,
      "role": "Theory of transformer mechanisms",
      "relationship_sentence": "Formalized how information flows through residual streams, attention, and MLPs in transformers, providing the structural assumptions that underlie edge-level circuit discovery in ACDC."
    },
    {
      "title": "In-Context Learning and Induction Heads",
      "authors": "Nelson Elhage, Neel Nanda, Catherine Olsson, et al.",
      "year": 2022,
      "role": "Empirical circuit case study",
      "relationship_sentence": "Demonstrated a manual pipeline\u2014task design, metrics, and activation interventions\u2014to identify a concrete circuit (induction heads), the very workflow ACDC systematizes and automates at the connection level."
    },
    {
      "title": "Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2",
      "authors": "Kevin Wang, Neel Nanda, Matthew Chan, et al.",
      "year": 2022,
      "role": "Empirical circuit case study and methodology",
      "relationship_sentence": "Established activation patching and dataset-driven probing as practical tools to map components and their interactions in GPT-2, directly motivating ACDC\u2019s automated edge selection via intervention-based scoring."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME)",
      "authors": "Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",
      "year": 2022,
      "role": "Intervention methodology",
      "relationship_sentence": "Popularized causal tracing/activation patching to localize knowledge to specific layers and neurons, influencing ACDC\u2019s use of targeted activation replacement as a causal signal for connection importance."
    },
    {
      "title": "Causal Scrubbing",
      "authors": "Matthew Chan, Neel Nanda, et al.",
      "year": 2023,
      "role": "Validation framework",
      "relationship_sentence": "Provided a principled protocol to test circuit hypotheses via interventions, shaping ACDC\u2019s emphasis on causal verification of discovered connections rather than purely correlational selection."
    },
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva, Roei Schuster, Yoav Tsur, Omer Levy, Jonathan Berant",
      "year": 2021,
      "role": "Component-level mechanism",
      "relationship_sentence": "Characterized MLPs as key\u2013value memory lookups, informing ACDC\u2019s modeling of edge roles between attention outputs and MLPs when assembling sparse functional circuits."
    }
  ],
  "synthesis_narrative": "The key contribution of Conmy et al. is to automate the labor-intensive step of mapping connections between abstract units that constitute a mechanistic circuit. This builds directly on the \"Circuits\" program (Olah et al.) and the Anthropic Transformer Circuits line, which framed model behaviors as sparse, compositional subnetworks and articulated how residual streams, attention, and MLPs interact. Empirical case studies\u2014Induction Heads and the IOI circuit\u2014crystallized a practical workflow: design a behavior-eliciting dataset and metric, then use activation patching to localize components and reason about their interactions. ACDC essentially codifies this workflow at the edge level, replacing manual, intuition-driven link enumeration with intervention-guided edge selection.\n\nMethodologically, ROME helped normalize causal tracing/activation patching as a robust way to localize mechanisms by measuring metric changes under targeted activation replacement, a signal ACDC leverages to score connections. Causal Scrubbing then provided a rigorous standard for validating hypothesized mechanisms via interventions, shaping ACDC\u2019s emphasis on causal, not correlational, evidence when assembling circuits. Finally, component-level insights such as MLPs-as-key\u2013value memories (Geva et al.) informed the types of edges and interactions worth searching over (e.g., attention-to-MLP and MLP-to-residual paths) and guided interpretation of recovered subgraphs. Together, these works supplied the conceptual schema of circuits, the practical activation-patching toolkit, and the validation norms that ACDC integrates into an automated, reproducible algorithm for discovering sparse functional wiring in transformers.",
  "analysis_timestamp": "2026-01-07T00:02:04.775036"
}
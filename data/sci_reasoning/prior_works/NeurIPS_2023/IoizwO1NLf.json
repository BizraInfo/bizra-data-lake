{
  "prior_works": [
    {
      "title": "Curriculum Learning",
      "authors": "Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, Jason Weston",
      "year": 2009,
      "role": "Foundational concept of ordering training to facilitate learning",
      "relationship_sentence": "Skill-it operationalizes the core curriculum-learning idea by defining data-driven \"skills\" and an explicit prerequisite order, showing that ordering training data by skills improves LM sample efficiency."
    },
    {
      "title": "Self-Paced Learning for Latent Variable Models",
      "authors": "M. Pawan Kumar, Benjamin Packer, Daphne Koller",
      "year": 2010,
      "role": "Methodology for model-driven, data-dependent ordering of examples",
      "relationship_sentence": "Skill-it extends the self-paced principle from per-example easiness to structured, data-defined skills, using model feedback to decide which skill-specific data to sample next."
    },
    {
      "title": "Automated Curriculum Learning for Neural Networks",
      "authors": "Alex Graves, Marc G. Bellemare, Jacob Menick, R\u00e9mi Munos, Koray Kavukcuoglu",
      "year": 2017,
      "role": "Online progress-based task/data selection",
      "relationship_sentence": "Skill-it\u2019s online data sampling algorithm echoes progress-based curricula by prioritizing data that unlocks downstream skills, but grounds the schedule in an empirically inferred skill-order graph."
    },
    {
      "title": "Competence-based Curriculum Learning for Neural Machine Translation",
      "authors": "Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnab\u00e1s P\u00f3czos, Tom M. Mitchell",
      "year": 2019,
      "role": "Curriculum design in NLP with competence schedules",
      "relationship_sentence": "Skill-it generalizes competence-based curricula from single-task NMT to multi-skill LM pretraining by formalizing skills as data subsets and learning their prerequisite order to guide sampling."
    },
    {
      "title": "Intelligent Selection of Language Model Training Data",
      "authors": "Robert C. Moore, William Lewis",
      "year": 2010,
      "role": "Precedent for data selection to improve language models",
      "relationship_sentence": "Skill-it reframes data selection from domain similarity to skill prerequisites, selecting data that most efficiently builds capabilities needed across downstream tasks under a token budget."
    },
    {
      "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics",
      "authors": "Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, Yejin Choi",
      "year": 2020,
      "role": "Using training dynamics to characterize example difficulty and learnability",
      "relationship_sentence": "Skill-it builds on the idea that training dynamics reveal structure in data, using model behavior to infer skill groupings and prerequisite relations that drive its sampling policy."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, et al.",
      "year": 2022,
      "role": "Motivation for data-efficient training under fixed token/compute budgets",
      "relationship_sentence": "Skill-it directly targets the Chinchilla-motivated regime by asking which tokens to use; its skill-ordered selection achieves better downstream performance with fewer tokens."
    }
  ],
  "synthesis_narrative": "Skill-it\u2019s core contribution\u2014a data-driven framework that defines skills as subsets of data and discovers an ordered, prerequisite structure to guide online sampling\u2014sits at the intersection of curriculum design, data selection, and training dynamics. The foundational notion that ordering learning can improve generalization comes from curriculum learning, while self-paced learning makes this ordering model- and data-dependent. Graves et al. extend this to automated, online curricula that prioritize material yielding maximal learning progress; Skill-it echoes this by sampling data according to inferred prerequisite skills to unlock more advanced capabilities efficiently.\nIn NLP, competence-based curricula for NMT demonstrate practical schedules for staged learning, which Skill-it generalizes beyond a single task to multi-skill pretraining by grounding the schedule in a learned skill DAG. Classic data selection for LMs (Moore\u2013Lewis) shows that choosing the right tokens matters; Skill-it departs from domain similarity and instead selects data that advances prerequisite skills shared across downstream tasks. Dataset Cartography provides the insight that training dynamics reveal example difficulty and learnability; Skill-it scales this idea to cluster datapoints into skills and infer their dependencies using model feedback.\nFinally, the Chinchilla results sharpen the constraint of fixed token budgets, making data efficiency central. Skill-it addresses this regime directly: by discovering and exploiting skill order, it demonstrates that pretraining on prerequisites reduces the data needed to acquire more advanced skills, yielding better performance-per-token across tasks.",
  "analysis_timestamp": "2026-01-06T23:42:49.076656"
}
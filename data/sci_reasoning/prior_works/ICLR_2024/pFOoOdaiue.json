{
  "prior_works": [
    {
      "title": "Robust Adversarial Reinforcement Learning",
      "authors": "Lerrel Pinto et al.",
      "year": 2017,
      "arxiv_id": "1703.02702",
      "role": "Baseline",
      "relationship_sentence": "RARL establishes the protagonist\u2013adversary zero-sum training paradigm that this work directly modifies by adding entropy regularization and replacing the Nash target with a bounded-rationality QRE, forming the primary baseline being improved."
    },
    {
      "title": "Quantal Response Equilibria for Normal Form Games",
      "authors": "Richard D. McKelvey et al.",
      "year": 1995,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper provides the bounded-rationality equilibrium concept (logit/QRE) and rationality parameter that underpins the paper\u2019s entropy-regularized min\u2013max objective and the idea of scheduling agents\u2019 rationality."
    },
    {
      "title": "Smoothing Techniques for Computing Equilibria in Extensive-Form Games",
      "authors": "Sanae Hoda et al.",
      "year": 2010,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Hoda et al. show that entropy (dilated-entropy) regularization smooths saddle-point problems and yields QRE solutions, an insight this paper extends to zero-sum Markov games to ease adversarial RL optimization."
    },
    {
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
      "authors": "Tuomas Haarnoja et al.",
      "year": 2018,
      "arxiv_id": "1801.01290",
      "role": "Inspiration",
      "relationship_sentence": "SAC introduces the maximum-entropy RL objective and temperature-controlled stochastic policies that this work adopts for both agents to implement bounded rationality and derive soft best responses in the two-player game."
    },
    {
      "title": "Markov Games as a Framework for Multi-Agent Reinforcement Learning",
      "authors": "Michael L. Littman",
      "year": 1994,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Littman\u2019s formulation of zero-sum Markov games and their Nash equilibria provides the formal problem setting that this work relaxes via entropy regularization to target QRE instead of strict Nash."
    },
    {
      "title": "Action Robust Reinforcement Learning",
      "authors": "Chen Tessler et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By modeling adversarial action perturbations, this work highlights brittleness and optimization difficulties of worst-case min\u2013max training, motivating the need for smoother, regularized objectives pursued here."
    },
    {
      "title": "Adversarial Policies: Attacking Deep Reinforcement Learning",
      "authors": "Adam Gleave et al.",
      "year": 2019,
      "arxiv_id": "1905.10615",
      "role": "Gap Identification",
      "relationship_sentence": "This paper shows learned adversaries can reliably exploit RL policies in zero-sum settings, directly motivating adversarial training schemes whose instability the present work mitigates via bounded-rationality curricula."
    }
  ],
  "synthesis_narrative": "RARL introduced training a protagonist against a learned adversary in a zero-sum setting, targeting Nash equilibria but exposing the practical difficulty of nonconvex, nonconcave saddle-point optimization. Quantal Response Equilibrium (QRE), defined by McKelvey and Palfrey, formalized bounded rationality via logit (entropy-regularized) best responses governed by a rationality parameter, offering a smoother alternative to strict Nash. In extensive-form games, Hoda and colleagues demonstrated that adding entropy (dilated-entropy) regularization smooths the optimization landscape and that the resulting equilibria correspond to QRE, making computation more tractable. In single-agent RL, Soft Actor-Critic established the maximum-entropy objective and temperature-controlled stochastic policies, providing a practical mechanism to implement bounded rationality and soft best responses. Littman\u2019s Markov games framework supplied the formal zero-sum, multi-agent substrate within which these ideas can be combined. Complementary robust RL work on action perturbations revealed brittleness and conservatism in worst-case formulations, while adversarial policy attacks underscored the real vulnerability of standard policies to learned opponents. Taken together, these works exposed both the need for adversarial robustness and the optimization challenges of aiming directly for Nash. The natural synthesis is to entropy-regularize the two-player Markov game so that the learning dynamics target QRE, not Nash, and to exploit the rationality parameter as a controllable knob. By scheduling (curricularizing) agents\u2019 bounded rationality\u2014enabled by maximum-entropy control\u2014the method smooths training early and progressively sharpens toward harder opponents, yielding a tractable path to robust policies.",
  "target_paper": {
    "title": "Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula",
    "authors": "Aryaman Reddi, Maximilian T\u00f6lle, Jan Peters, Georgia Chalvatzaki, Carlo D'Eramo",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "reinforcement learning, adversarial, bounded rationality, curriculum",
    "abstract": "Robustness against adversarial attacks and distribution shifts is a long-standing goal of Reinforcement Learning (RL). To this end, Robust Adversarial Reinforcement Learning (RARL) trains a protagonist against destabilizing forces exercised by an adversary in a competitive zero-sum Markov game, whose optimal solution, i.e., rational strategy, corresponds to a Nash equilibrium. However, finding Nash equilibria requires facing complex saddle point optimization problems, which can be prohibitive to solve, especially for high-dimensional control. In this paper, we propose a novel approach for adversarial RL based on entropy regularization to ease the complexity of the saddle point optimization problem. We show that the solution of this entropy-regularized problem corresponds to a Quantal Response Equilibrium (QRE), a generalization of Nash equilibria that accounts for bounded rationality, i.e., agents sometimes play random actions instead of optimal ones. Crucially, the connection between ",
    "openreview_id": "pFOoOdaiue",
    "forum_id": "pFOoOdaiue"
  },
  "analysis_timestamp": "2026-01-07T00:19:05.222946"
}
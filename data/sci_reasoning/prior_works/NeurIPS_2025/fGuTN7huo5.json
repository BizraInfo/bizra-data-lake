{
  "prior_works": [
    {
      "title": "FlowNet: Learning Optical Flow with Convolutional Networks",
      "authors": "Alexey Dosovitskiy et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "Established the modern deep-learning formulation of dense optical flow that Opt-CWM ultimately targets, providing the canonical task definition and evaluation setting that Opt-CWM aims to solve without supervision."
    },
    {
      "title": "UnFlow: Unsupervised Learning of Optical Flow with a Bidirectional Census Loss",
      "authors": "Simon Meister et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Pioneered self-supervised flow via photometric consistency and occlusion heuristics, whose reliance on hand-tuned losses and fragile occlusion handling is a core limitation Opt-CWM explicitly avoids by extracting motion directly from pretrained video models."
    },
    {
      "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
      "authors": "Zachary Teed et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Serves as the dominant strong baseline for optical flow but depends on supervised training (often synthetic), motivating Opt-CWM\u2019s zero-shot alternative that recovers flow/occlusion by probing a pretrained video predictor."
    },
    {
      "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation",
      "authors": "Ruth Fong et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Introduced optimization-based perturbations to reveal causal evidence in a model\u2019s prediction; Opt-CWM adapts this perturbation-by-optimization idea into counterfactual probes over a video world model to isolate motion primitives."
    },
    {
      "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR",
      "authors": "Sandra Wachter et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Formalized counterfactual explanations as minimal changes that elicit desired model outputs; Opt-CWM operationalizes this principle in video by optimizing counterfactuals of a pretrained predictor to expose flow and occlusion signals."
    },
    {
      "title": "Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space",
      "authors": "Anh Nguyen et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "Demonstrated gradient-based steering of a pretrained generator via auxiliary objectives; Opt-CWM extends this latent-space optimization paradigm to temporal world models, steering them counterfactually to read out motion fields."
    },
    {
      "title": "Video Diffusion Models",
      "authors": "Jonathan Ho et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Established powerful pretrained video prediction/generation models that encode rich motion dynamics; Opt-CWM assumes such a pretrained video model and shows how to zero-shot extract flow and occlusion from it via counterfactual optimization."
    }
  ],
  "synthesis_narrative": "Opt-CWM\u2019s core innovation\u2014recovering optical flow and occlusion in a zero-shot manner by optimizing counterfactual probes of a pretrained video world model\u2014stands at the intersection of three direct lines of work. First, deep optical flow methods such as FlowNet defined the modern dense motion estimation task, while UnFlow and related self-supervised approaches highlighted the limitations of photometric losses and hand-crafted occlusion heuristics. RAFT set the performance bar but remained dependent on supervised, often synthetic, labels. These gaps motivate an approach that avoids direct supervision and brittle heuristics altogether. Second, counterfactual and perturbation-based explanation methods (Wachter et al.; Fong & Vedaldi) directly inspire Opt-CWM\u2019s mechanism: optimize model inputs/latents to induce targeted output changes, thereby revealing the internal causal factors\u2014here, motion primitives. Plug & Play Generative Networks further provides a concrete template for gradient-based steering of pretrained generators via auxiliary objectives, which Opt-CWM adapts to temporal world models to extract flow and occlusions as the causal directions that move predicted frames in desired ways. Third, the emergence of strong pretrained video predictors (e.g., Video Diffusion Models) furnishes the world-model substrate required for such probing to work at scale. Opt-CWM fuses these threads by treating the video model as a causal simulator and using counterfactual optimization to expose the latent motion variables\u2014sidestepping supervision and hand-tuned heuristics while improving generalization in real-world video.",
  "analysis_timestamp": "2026-01-06T23:08:23.942505"
}
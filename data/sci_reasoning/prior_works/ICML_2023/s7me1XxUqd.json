{
  "prior_works": [
    {
      "title": "Ridge Regression: Biased Estimation for Nonorthogonal Problems",
      "authors": "Hoerl et al.",
      "year": 1970,
      "role": "Baseline",
      "relationship_sentence": "The paper\u2019s central comparison point\u2014and the risk target it matches\u2014is classical ridge regression; the authors show a full ridgeless subsample ensemble can attain the same optimal risk as the best-tuned ridge estimator introduced by Hoerl and Kennard."
    },
    {
      "title": "Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter",
      "authors": "Golub et al.",
      "year": 1979,
      "role": "Extension",
      "relationship_sentence": "The work extends Golub\u2013Heath\u2013Wahba\u2019s GCV from single-model ridge to subsample ridge ensembles in the proportional regime, proving strong uniform consistency over subsample sizes to enable tuning without sample splitting."
    },
    {
      "title": "Bagging Predictors",
      "authors": "Breiman",
      "year": 1996,
      "role": "Inspiration",
      "relationship_sentence": "The core idea of variance reduction via averaging predictors directly inspires analyzing ensembles formed by fitting linear models on many subsamples, culminating in the \u2018full ridgeless ensemble\u2019 studied here."
    },
    {
      "title": "Bagging, Subagging and Bragging",
      "authors": "B\u00fchlmann",
      "year": 2002,
      "role": "Foundation",
      "relationship_sentence": "This work formalized subagging\u2014aggregation over subsamples\u2014which is precisely the ensemble mechanism the paper analyzes to establish equivalence between subsample-averaged ridgeless fits and optimally tuned ridge."
    },
    {
      "title": "High-Dimensional Asymptotics of Prediction: Ridge Regression and Classification",
      "authors": "Dobriban et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "The proportional asymptotics framework and risk characterizations for ridge provided the technical foundation the authors use to derive risk contours in the (\u03bb, \u03c6_s)-plane and to compare ensemble risk to optimal ridge risk."
    },
    {
      "title": "Surprises in High-Dimensional Ridgeless Least Squares Interpolation",
      "authors": "Hastie et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "By revealing the distinct risk behavior of ridgeless least squares in high dimensions, this paper exposes limitations that the present work addresses by showing that ensembling ridgeless fits over subsamples can recover optimal ridge-level risk."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014proving that a full ridgeless subsample ensemble can match the optimal risk of ridge regression and enabling its GCV-based tuning\u2014rests on two intertwined lines of prior work. First, Hoerl and Kennard\u2019s ridge regression set the baseline objective: shrinkage that minimizes prediction risk. Golub\u2013Heath\u2013Wahba then provided the GCV criterion for selecting ridge penalties, which the present paper extends to ensembles, establishing strong uniform consistency over subsample sizes to enable tuning without sample splitting.\n\nSecond, the ensemble mechanism itself descends from Breiman\u2019s bagging idea and B\u00fchlmann\u2019s subagging formalization, which motivate aggregating linear predictors fitted on subsamples. The paper makes this connection precise in high dimensions by adopting the proportional asymptotics framework of Dobriban and Wager, whose random-matrix-based risk formulas for ridge supply the machinery to derive risk contours in the (\u03bb, \u03c6_s)-plane and to prove the equivalence between optimal ridge and the full ridgeless ensemble.\n\nFinally, the ridgeless perspective from Hastie, Montanari, Rosset, and Tibshirani highlights peculiar risk behavior of the min-norm interpolator in overparameterized settings, motivating the question of whether one can obtain ridgeless-like simplicity yet recover ridge-level risk. The paper answers this by showing that averaging ridgeless fits over all subsamples yields exactly that, and by extending GCV to consistently tune such ensembles in the proportional regime.",
  "analysis_timestamp": "2026-01-06T23:09:26.556034"
}
{
  "prior_works": [
    {
      "title": "Growing Neural Cellular Automata",
      "authors": "Alexander Mordvintsev et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Its differentiable, convolutional per-cell update rule and training setup for tasks like pattern growth and regeneration define the NCA workloads (stochastic firing, multi-channel states, learned neighborhood filters) that CAX implements as first-class primitives and accelerates at scale."
    },
    {
      "title": "Self-Organizing Textures",
      "authors": "Eyvind Niklasson et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing that texture synthesis with NCAs benefits from flexible neighborhoods, channel counts, noise injection, and custom scheduling, this work directly motivated CAX\u2019s modular CA step and configurable API (arbitrary neighborhoods, stochastic updates, multi-field states)."
    },
    {
      "title": "Lenia: Biology-Inspired Artificial Life",
      "authors": "Bert Chan",
      "year": 2019,
      "arxiv_id": "1812.05433",
      "role": "Foundation",
      "relationship_sentence": "Lenia\u2019s continuous-state CA with convolution-based interactions in 2D/3D provides the precise continuous CA formalism that CAX natively supports (real-valued grids, smooth kernels, arbitrary dimensionality)."
    },
    {
      "title": "Generalization of Conway\u2019s Game of Life to a continuous domain (SmoothLife)",
      "authors": "Stephan Rafler",
      "year": 2011,
      "arxiv_id": "1111.1567",
      "role": "Foundation",
      "relationship_sentence": "SmoothLife\u2019s stencil/integral-based continuous update rule is a direct precursor to the convolutional, differentiable CA computations that CAX generalizes and accelerates across hardware backends."
    },
    {
      "title": "Cellular Automata as Convolutional Neural Networks",
      "authors": "William Gilpin",
      "year": 2019,
      "arxiv_id": "1809.02942",
      "role": "Extension",
      "relationship_sentence": "By formalizing CA updates as convolutional operators amenable to GPU parallelism and autodiff, this work provided the key computational template that CAX standardizes into a general-purpose, multi-dimensional CA kernel framework."
    },
    {
      "title": "JAX MD: A Framework for Differentiable Physics",
      "authors": "Samuel S. Schoenholz et al.",
      "year": 2020,
      "arxiv_id": "1912.04232",
      "role": "Related Problem",
      "relationship_sentence": "Demonstrating how XLA-jitted, vectorized simulation kernels and a functional JAX API yield portable high performance directly informed CAX\u2019s architecture (jit/vmap/pmap compilation strategy and composable simulator design)."
    }
  ],
  "synthesis_narrative": "Neural cellular automata were crystallized by Growing Neural Cellular Automata, which introduced a differentiable per-cell update rule implemented as small learned convolutions with stochastic firing, enabling tasks such as pattern growth and damage-robust regeneration. Self-Organizing Textures expanded this paradigm, revealing that practical NCA applications require configurable neighborhoods, multi-channel internal states, noise injection, and flexible update scheduling to control emergent behavior. In parallel, Lenia established a continuous-state, convolution-based CA formalism, including 2D/3D variants, proving that smooth, real-valued fields and kernels can yield rich self-organizing phenomena beyond discrete rules. SmoothLife earlier showed how Life-like dynamics can be generalized to continuous domains via integral/stencil computations, anchoring the use of convolutional neighborhoods for continuous CA. Complementing these modeling advances, Cellular Automata as Convolutional Neural Networks demonstrated that CA updates can be framed as convolutional operators compatible with GPU parallelism and automatic differentiation. Finally, JAX MD showed that a functional JAX design with jit-compiled, vectorized kernels enables high-performance, portable scientific simulation libraries. Together these works exposed a clear opportunity: while NCA and continuous CA require flexible, convolutional stencil updates and benefit from differentiable, hardware-accelerated execution, implementations remained ad hoc and task-specific. Synthesizing these insights, the natural next step was to standardize convolutional CA kernels in a modular, functional JAX library that natively supports discrete and continuous CA across dimensions, jit-compiles to modern accelerators, and reproduces canonical NCA and Lenia-style workloads with state-of-the-art performance and reproducibility.",
  "target_paper": {
    "title": "CAX: Cellular Automata Accelerated in JAX",
    "authors": "Maxence Faldor, Antoine Cully",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "cellular automata, emergence, self-organization, neural cellular automata",
    "abstract": "Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX delivers cutting-edge performance through hardware acceleration while maintaining flexibility through its modular architecture, intuitive API, and support for both discrete and continuous cellular automata in arbitrary dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits,",
    "openreview_id": "o2Igqm95SJ",
    "forum_id": "o2Igqm95SJ"
  },
  "analysis_timestamp": "2026-01-06T11:18:32.280743"
}
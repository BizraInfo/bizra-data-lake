{
  "prior_works": [
    {
      "title": "One-Shot Learning for Semantic Segmentation",
      "authors": "Shaban et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "MM-FSS adopts the support\u2013query episodic formulation for few-shot segmentation introduced by OSLSM, transferring this setup from 2D to the 3D point-cloud domain."
    },
    {
      "title": "Prototypical Networks for Few-shot Learning",
      "authors": "Snell et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "MM-FSS\u2019s correlation-based matching between support and query features builds on the prototypical metric-learning paradigm, replacing pure visual prototypes with multimodal (image\u2013point\u2013text) semantics."
    },
    {
      "title": "Hypercorrelation Squeeze for Few-Shot Segmentation (HSNet)",
      "authors": "Min et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "The proposed Multimodal Correlation Fusion (MCF) and Multimodal Semantic Fusion (MSF) generalize HSNet\u2019s dense support\u2013query correlation matching and refinement to 3D and explicitly incorporate image and text modalities."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Radford et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "MM-FSS relies on a pretrained text encoder aligned with visual semantics (as in CLIP) to turn class names into language embeddings that guide few-shot 3D segmentation."
    },
    {
      "title": "Language-driven Semantic Segmentation (LSeg)",
      "authors": "Li et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "LSeg demonstrated that CLIP-derived text embeddings can condition segmentation; MM-FSS transports this idea to 3D few-shot settings and fuses language cues with 3D and 2D features via MCF/MSF."
    },
    {
      "title": "ULIP: Learning Unified Representations for Language, Image, and Point Clouds with Contrastive Learning",
      "authors": "Xue et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "ULIP\u2019s tri-modal alignment motivates MM-FSS\u2019s shared backbone and intermodal head, with MM-FSS extending this alignment to task-level few-shot 3D segmentation through explicit correlation fusion."
    },
    {
      "title": "PointCLIP: Point Cloud Understanding by CLIP",
      "authors": "Zhu et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "By showing how CLIP\u2019s language\u2013vision space can be exploited for 3D recognition, PointCLIP provides direct evidence that language/image cues can supervise 3D data, which MM-FSS leverages for few-shot 3D segmentation with text and optional images."
    }
  ],
  "synthesis_narrative": "The core advance of MM-FSS is to move few-shot 3D point-cloud semantic segmentation from a unimodal paradigm to a multimodal one that explicitly fuses text and, when available, images with 3D geometry. This traces back to the few-shot segmentation formulation of OSLSM, which established the episodic support\u2013query setup, and to Prototypical Networks, which defined metric-based class matching\u2014both of which anchor MM-FSS\u2019s support\u2013query design and prototype-style correlations. HSNet then provided a powerful mechanism for dense support\u2013query correlation construction and refinement; MM-FSS\u2019s Multimodal Correlation Fusion (MCF) and Multimodal Semantic Fusion (MSF) can be viewed as extending HSNet\u2019s hypercorrelation idea to operate across 3D points, image features, and language embeddings.\nConcurrently, CLIP revealed that language embeddings align with visual semantics, enabling class names to act as supervision; LSeg demonstrated how such text cues can directly condition segmentation. Building on these, MM-FSS brings language priors into the few-shot 3D setting and fuses them with 3D and 2D visual evidence in its correlation modules. Finally, ULIP and PointCLIP showed practical paths to couple point clouds with image\u2013language spaces; MM-FSS draws on this tri-modal alignment insight but pushes it from representation learning or classification into a task-driven, few-shot 3D segmentation model. Collectively, these works motivate the paper\u2019s new multimodal FS-PCS setup and directly inform the design of MCF/MSF for exploiting cross-modal correlations.",
  "analysis_timestamp": "2026-01-06T23:09:26.604702"
}
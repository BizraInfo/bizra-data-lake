{
  "prior_works": [
    {
      "title": "Nonnegative Decomposition of Multivariate Information",
      "authors": "Paul L. Williams; Randall D. Beer",
      "year": 2010,
      "role": "Information-theoretic foundation (Partial Information Decomposition, PID)",
      "relationship_sentence": "Provides the unique/redundant/synergistic mutual information decomposition that MCR operationalizes to explicitly allocate modality-specific and shared contributions during multimodal training."
    },
    {
      "title": "Quantifying Unique Information",
      "authors": "Thomas Bertschinger; Johannes Rauh; Eckehard Olbrich; J\u00fcrgen Jost; Nihat Ay",
      "year": 2014,
      "role": "Formalization of unique information within PID",
      "relationship_sentence": "Informs MCR\u2019s treatment of modality-unique information by motivating separate estimation and tight bounding of unique versus redundant terms to mitigate modality overshadowing."
    },
    {
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": "Aaron van den Oord; Yazhe Li; Oriol Vinyals",
      "year": 2018,
      "role": "Practical MI lower-bound (InfoNCE) for representation learning",
      "relationship_sentence": "Guides the use of contrastive lower bounds when estimating MI components in MCR, enabling tractable optimization of task-relevant information per modality."
    },
    {
      "title": "On Variational Bounds of Mutual Information",
      "authors": "Ben Poole; Sherjil Ozair; Aaron van den Oord; Alex A. Alemi",
      "year": 2019,
      "role": "Unifying analysis of MI bounds and their properties",
      "relationship_sentence": "Provides the theoretical toolkit to select and refine MI bounds in MCR, balancing bias\u2013variance trade-offs across lower bounds used for different MI terms."
    },
    {
      "title": "CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information",
      "authors": "Y. Cheng et al.",
      "year": 2020,
      "role": "Tractable upper bound on MI",
      "relationship_sentence": "Supplies an optimizable MI upper bound that MCR leverages to constrain redundant information terms, complementing lower-bound estimators for tighter control of information allocation."
    },
    {
      "title": "Multi-Task Learning as Multi-Objective Optimization",
      "authors": "Ozan Sener; Vladlen Koltun",
      "year": 2018,
      "role": "Gradient-based Pareto optimization (MGDA) for balancing competing objectives",
      "relationship_sentence": "Inspires MCR\u2019s framing of modalities as competing objectives whose gradients must be balanced, transferring multi-objective insights to the multimodal setting."
    },
    {
      "title": "Multi-Task Learning as a Bargaining Game",
      "authors": "Aviv Navon; Aviv Shamsian; Ethan Fetaya; Gal Chechik",
      "year": 2022,
      "role": "Game-theoretic balancing via Nash bargaining",
      "relationship_sentence": "Directly motivates MCR\u2019s game-theoretic regularization by showing how Nash-style solutions can adaptively allocate learning resources among players\u2014here, modalities\u2014 to reduce competition."
    }
  ],
  "synthesis_narrative": "The core innovation of Multimodal Competition Regularizer (MCR) marries information-theoretic decomposition with game-theoretic balancing to mitigate modality competition. Foundationally, Williams and Beer\u2019s Partial Information Decomposition and Bertschinger et al.\u2019s formalization of unique information establish the conceptual target: disentangling unique, redundant, and synergistic information so each modality\u2019s task-relevant contribution can be separately encouraged or constrained. Turning this into a trainable objective requires principled mutual information estimation. Contrastive Predictive Coding introduces the InfoNCE lower bound that enables scalable estimation of MI-driven objectives, while Poole et al. unify variational MI bounds and illuminate their bias\u2013variance trade-offs, guiding MCR\u2019s choice and refinement of lower bounds for different PID terms. Complementing lower bounds, CLUB contributes a tractable MI upper bound that lets MCR explicitly cap redundancy, jointly tightening bounds across unique and shared components to better reflect the intended decomposition during learning.\n\nTo address the dynamics of modality competition, MCR draws on multi-objective and game-theoretic insights from MGDA and Nash-MTL. Sener and Koltun\u2019s view of learning as multi-objective optimization motivates balancing conflicting gradients across objectives, and Navon et al.\u2019s bargaining-game formulation offers a principled way to allocate learning resources among players. MCR extends these ideas by defining modality-specific payoffs via bounded MI terms and regularizing toward equilibria that reward each modality\u2019s informative role. Together, these works directly enable MCR\u2019s adaptive, information-aware, game-theoretic strategy for consistent multimodal gains.",
  "analysis_timestamp": "2026-01-07T00:21:32.277785"
}
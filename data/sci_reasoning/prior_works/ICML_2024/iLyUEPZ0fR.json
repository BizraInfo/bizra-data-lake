{
  "prior_works": [
    {
      "title": "A Coordinate Gradient Descent Method for Nonsmooth Separable Minimization",
      "authors": "P. Tseng et al.",
      "year": 2009,
      "role": "Baseline",
      "relationship_sentence": "This paper formalized coordinate/block gradient descent with per-block Lipschitz stepsizes (1/L_i); the present work directly revisits and optimizes this stepsize rule for least-squares, yielding closed-form block stepsizes that outperform the vanilla 1/L_i choice."
    },
    {
      "title": "On the Convergence of Block Coordinate Descent Type Methods",
      "authors": "A. Beck et al.",
      "year": 2013,
      "role": "Gap Identification",
      "relationship_sentence": "Beck and Tetruashvili\u2019s deterministic rates for cyclic BCD with Lipschitz-based stepsizes have worse constants than full GD, a limitation explicitly motivating this paper\u2019s search for optimal block stepsizes that provably close (and surpass) that gap in least-squares."
    },
    {
      "title": "Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems",
      "authors": "Y. Nesterov et al.",
      "year": 2012,
      "role": "Foundation",
      "relationship_sentence": "Nesterov established the modern CD/BCD framework and popularized block-wise Lipschitz stepsizes with associated complexity bounds; the current paper challenges this default by deriving the truly optimal block steps for two-block least-squares under orthogonality."
    },
    {
      "title": "Iteration Complexity of Randomized Block-Coordinate Descent Methods for Minimizing a Composite Function",
      "authors": "P. Richt\u00e1rik et al.",
      "year": 2014,
      "role": "Related Problem",
      "relationship_sentence": "This work analyzes randomized BCD using block Lipschitz constants and highlights rate constants tied to block smoothness; the present paper provides a deterministic counterpart showing that tailored, optimal block steps can yield superior asymptotic factors in least-squares."
    },
    {
      "title": "Two-Point Step Size Gradient Methods",
      "authors": "J. Barzilai et al.",
      "year": 1988,
      "role": "Inspiration",
      "relationship_sentence": "Barzilai\u2013Borwein showed that carefully chosen stepsizes can accelerate gradient methods on quadratic/least-squares problems without momentum; this paper adopts that philosophy at the block level, designing optimal block steps that deliver acceleration."
    },
    {
      "title": "Performance of First-Order Methods for Smooth Convex Minimization: A Novel Approach",
      "authors": "Y. Drori et al.",
      "year": 2014,
      "role": "Inspiration",
      "relationship_sentence": "Drori\u2013Teboulle\u2019s worst-case analysis framework emphasizes optimizing algorithm parameters (e.g., stepsizes) to minimize convergence factors; analogously, this paper derives block stepsizes that minimize the asymptotic rate (spectral radius) for two-block least-squares."
    }
  ],
  "synthesis_narrative": "The core of this paper is to overturn the long-standing default stepsize rule for block gradient descent\u20141 over the block-wise Lipschitz constant\u2014by exhibiting the truly optimal block stepsizes for least-squares in a clean two-block, orthogonal design. The coordinate/block gradient paradigm and its canonical stepsize trace directly to Tseng\u2013Yun (CGD) and Nesterov\u2019s coordinate-descent framework, which established per-block Lipschitz scaling and its complexity implications. However, deterministic analyses such as Beck\u2013Tetruashvili\u2014and related randomized analyses like Richt\u00e1rik\u2013Tak\u00e1\u010d\u2014revealed that with these default stepsizes, BCD\u2019s convergence constants are often worse than full gradient descent, leaving the empirical superiority of block methods theoretically unexplained. This gap motivates a re-examination of stepsize choice as the lever for acceleration. The inspiration comes from two lines: Barzilai\u2013Borwein showed decades ago that stepsize design alone can substantially accelerate gradient methods on quadratic (least-squares) problems without resorting to momentum; and Drori\u2013Teboulle\u2019s performance-estimation view formalized optimizing algorithm parameters to minimize worst-case rates. Building on these, the present work targets the specific least-squares/two-block/orthogonal setting to derive closed-form optimal block stepsizes that minimize the asymptotic convergence factor. The result is a momentum-free, principled acceleration of block gradient descent that directly addresses the prior constant-gap, finally providing a theoretical explanation for the observed advantage of block updates over standard GD in this regime.",
  "analysis_timestamp": "2026-01-06T23:09:26.398758"
}
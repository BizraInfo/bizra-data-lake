{
  "prior_works": [
    {
      "title": "Unsupervised Cross-lingual Representation Learning at Scale",
      "authors": "Alexis Conneau et al.",
      "year": 2020,
      "arxiv_id": "1911.02116",
      "role": "Baseline",
      "relationship_sentence": "XLM-R establishes the shared-vocabulary, fully shared-parameter multilingual pre-training paradigm that DEPT replaces by keeping a shared transformer body while decoupling per-source token embeddings."
    },
    {
      "title": "The Curse of Multilinguality in Multilingual Neural Machine Translation",
      "authors": "Naveen Arivazhagan et al.",
      "year": 2019,
      "arxiv_id": "1907.05019",
      "role": "Gap Identification",
      "relationship_sentence": "This work documents performance degradation when scaling to many languages with a single shared vocabulary/model, directly motivating DEPT\u2019s design to avoid a shared vocabulary and mitigate cross-source interference."
    },
    {
      "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "authors": "Zhenzhong Lan et al.",
      "year": 2019,
      "arxiv_id": "1909.11942",
      "role": "Inspiration",
      "relationship_sentence": "ALBERT\u2019s factorized embedding parameterization showed that input embeddings can be decoupled from the transformer hidden size, inspiring DEPT\u2019s further decoupling of token embeddings as modular, source-specific components."
    },
    {
      "title": "Exploiting Shared Representations for Personalized Federated Learning (FedRep)",
      "authors": "Collins et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "FedRep\u2019s split of a globally shared representation with small personalized modules is directly generalized in DEPT by treating token embeddings as per-source modules while synchronizing only the shared transformer body."
    },
    {
      "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data (FedAvg)",
      "authors": "Brendan McMahan et al.",
      "year": 2017,
      "arxiv_id": "1602.05629",
      "role": "Foundation",
      "relationship_sentence": "FedAvg formalizes the round-based communication model that DEPT exploits by keeping large per-source embedding parameters local and only communicating the shared body, reducing communication in proportion to embedding size."
    },
    {
      "title": "ByT5: Towards a token-free future for text",
      "authors": "Linting Xue et al.",
      "year": 2022,
      "arxiv_id": "2105.13626",
      "role": "Gap Identification",
      "relationship_sentence": "ByT5 demonstrates that dropping a shared subword vocabulary can help multilingual robustness but incurs significant computational cost, highlighting the need for a more efficient alternative that DEPT achieves via decoupled subword embeddings."
    },
    {
      "title": "MAD-X: An Adapter-based Framework for Multilingual Transfer",
      "authors": "Jonas Pfeiffer et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "MAD-X shows that small language-specific modules attached to a shared transformer can mitigate interference across languages, informing DEPT\u2019s choice to modularize token embeddings while sharing the transformer body."
    }
  ],
  "synthesis_narrative": "XLM-R established the dominant approach to multilingual pre-training by sharing both a subword vocabulary and all model parameters across languages, achieving strong cross-lingual transfer but locking languages into one lexical space. Arivazhagan et al. showed that such sharing can degrade performance as the number of languages grows, highlighting a \"curse of multilinguality\" rooted in interference. ALBERT demonstrated that input embeddings need not be tightly coupled to the transformer hidden size by factorizing and projecting embeddings, revealing embeddings as a separable, parameter-dominant component. In federated settings, FedAvg defined round-based synchronization where communication scales with model size, while FedRep showed that sharing a global representation and keeping small personalized modules local improves robustness under heterogeneity and reduces communication. ByT5 removed shared vocabularies entirely via byte-level modeling, proving robustness without a shared lexicon but at notable computational cost. MAD-X further evidenced that small, language-specific modules added to a shared transformer can curb cross-language interference without full parameter duplication.\nTaken together, these works imply that the primary locus of heterogeneity lies in the lexical interface and that embeddings are both separable and communication-heavy. The natural next step is to share only the transformer body for generalization while keeping per-source lexical modules local, avoiding a shared vocabulary and reducing communication by not syncing large embedding tables. Building on ALBERT\u2019s decoupling insight, FedRep\u2019s split-sharing strategy, and multilingual findings on interference and vocabulary design, the current work operationalizes a decoupled-embedding framework that robustly trains across heterogeneous domains and languages with communication savings aligned to embedding size.",
  "target_paper": {
    "title": "DEPT: Decoupled Embeddings for Pre-training Language Models",
    "authors": "Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, William F. Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas Donald Lane",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Decentralized Training, Federated Learning, Multi-domain Training, Multilingual Training",
    "abstract": "Language Model pre-training uses broad data mixtures to enhance performance across domains and languages. However, training on such heterogeneous text corpora requires extensive and expensive efforts. Since these data sources vary significantly in lexical, syntactic, and semantic aspects, they cause negative interference or the ``curse of multilinguality''. To address these challenges we propose a communication-efficient pre-training framework, DEPT. Our method decouples embeddings from the transformer body while simultaneously training the latter on multiple data sources without requiring a shared vocabulary. DEPT can: (1) train robustly and effectively under significant data heterogeneity, (2) minimize token embedding parameters to only what the data source vocabulary requires, while cutting communication costs in direct proportion to both the communication frequency and the reduction in parameters, (3) enhance transformer body plasticity and generalization, improving both average pe",
    "openreview_id": "vf5aUZT0Fz",
    "forum_id": "vf5aUZT0Fz"
  },
  "analysis_timestamp": "2026-01-06T09:37:03.362726"
}
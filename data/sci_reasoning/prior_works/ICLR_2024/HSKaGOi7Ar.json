{
  "prior_works": [
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu et al.",
      "year": 2019,
      "arxiv_id": "1810.00826",
      "role": "Foundation",
      "relationship_sentence": "By establishing the Weisfeiler\u2013Lehman (WL) test as the de facto benchmark for GNN expressiveness and tying MPNNs to 1-WL, this work set the qualitative, WL-centric yardstick that the present paper replaces with a quantitative homomorphism-based measure."
    },
    {
      "title": "Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks",
      "authors": "Christopher Morris et al.",
      "year": 2019,
      "arxiv_id": "1810.02244",
      "role": "Foundation",
      "relationship_sentence": "By aligning higher-order GNN architectures with the k-WL hierarchy, this paper entrenched WL as the organizing principle for expressiveness, motivating the need for a unified quantitative framework that can compare such models beyond coarse WL levels."
    },
    {
      "title": "The Logical Expressive Power of Graph Neural Networks",
      "authors": "Pablo Barcel\u00f3 et al.",
      "year": 2020,
      "arxiv_id": "1909.13021",
      "role": "Inspiration",
      "relationship_sentence": "Their characterization of GNNs via first-order logic with counting highlighted counting as the core mechanism of GNN expressivity, directly inspiring the shift to a homomorphism-count\u2013based expressivity measure."
    },
    {
      "title": "Large Networks and Graph Limits",
      "authors": "L\u00e1szl\u00f3 Lov\u00e1sz",
      "year": 2012,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Lov\u00e1sz\u2019s results that homomorphism count vectors form complete graph invariants provide the mathematical backbone for defining homomorphism expressivity and proving its completeness for comparing GNN models."
    },
    {
      "title": "Expressive Power of Invariant and Equivariant Graph Neural Networks",
      "authors": "Alireza Azizian and Marc Lelarge",
      "year": 2021,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By connecting invariant/equivariant GNNs to polynomial graph invariants tightly linked to subgraph and homomorphism counts, this work motivates treating homomorphism counting as a principled, quantitative yardstick for GNN expressiveness."
    },
    {
      "title": "Provably Powerful Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "arxiv_id": "1905.11136",
      "role": "Baseline",
      "relationship_sentence": "These tensor-power architectures exemplify beyond-1-WL expressiveness via higher-order invariants, serving as a primary model class that the new homomorphism expressivity framework evaluates and compares quantitatively."
    },
    {
      "title": "Improving Graph Neural Networks with Learnable Structural and Positional Representations",
      "authors": "Georgios Bouritsas et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "By explicitly targeting motif/subgraph counting as a missing practical capability in standard GNNs, this work exposes the concrete need\u2014quantifying substructure-counting power\u2014that the homomorphism expressivity metric directly addresses."
    }
  ],
  "synthesis_narrative": "Xu et al. formalized GNN expressiveness through the lens of the 1-WL test and popularized WL as the field\u2019s benchmark, while Morris et al. extended this alignment to higher-order GNNs via k-WL, cementing a qualitative, WL-centric hierarchy of expressiveness. Barcel\u00f3 et al. then provided a logic-with-counting characterization of GNNs, revealing that counting is the operative mechanism underlying the distinctions WL captures. Lov\u00e1sz\u2019s theory established that homomorphism count vectors constitute complete graph invariants, offering a principled quantitative basis to compare graphs through counts. Azizian and Lelarge connected invariant/equivariant GNNs to polynomial invariants intimately tied to subgraph and homomorphism counts, suggesting a natural bridge from neural architectures to count-based measures. Maron et al. introduced tensor-power architectures achieving beyond-1-WL power via higher-order invariants, providing concrete models whose strengths should be compared quantitatively. In parallel, Bouritsas et al. highlighted practical needs around motif and subgraph counting, underscoring the limitations of purely WL-based assessments for real tasks. Together, these works expose that WL offers only a coarse, qualitative scale, while theory and practice increasingly revolve around counting. The current paper synthesizes these insights by proposing homomorphism expressivity\u2014a complete, quantitative measure grounded in homomorphism counts\u2014that spans model families like MPNNs, higher-order/tensor GNNs, and subgraph-aware designs. This framework naturally follows from Lov\u00e1sz\u2019s completeness, the logic-of-counting view, and the community\u2019s emphasis on subgraph counting, yielding a unified tool that compares models\u2019 expressivity and concretely interprets abilities such as subgraph counting.",
  "target_paper": {
    "title": "Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness",
    "authors": "Bohang Zhang, Jingchu Gai, Yiheng Du, Qiwei Ye, Di He, Liwei Wang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Graph Neural Networks, Expressive Power, Homomorphism, Subgraph Counting, Weisfeiler-Lehman",
    "abstract": "Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in the graph learning community. So far, GNN expressiveness has been primarily assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse, qualitative, and may not well reflect practical requirements (e.g., the ability to encode substructures). In this paper, we introduce a novel framework for quantitatively studying the expressiveness of GNN architectures, addressing all the above limitations. Specifically, we identify a fundamental expressivity measure termed homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. Homomorphism expressivity offers a complete and practical assessment tool: the completeness enables direct expressivity comparisons between GNN models, while the practicality allows for understanding concrete GNN abilities such as subgraph counting. By examining four classes ",
    "openreview_id": "HSKaGOi7Ar",
    "forum_id": "HSKaGOi7Ar"
  },
  "analysis_timestamp": "2026-01-06T09:20:54.824936"
}
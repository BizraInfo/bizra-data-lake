{
  "prior_works": [
    {
      "title": "Neural Empirical Bayes",
      "authors": "Saeed Saremi et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Introduced the original walk-jump scheme\u2014Langevin \u201cwalk\u201d on a Gaussian-smoothed density followed by a one-step denoising \u201cjump\u201d to the data manifold\u2014which this paper adapts and operationalizes for discrete sequence spaces with a learned smoothed energy and projection."
    },
    {
      "title": "A Connection Between Score Matching and Denoising Autoencoders",
      "authors": "Pascal Vincent et al.",
      "year": 2011,
      "arxiv_id": "1105.4537",
      "role": "Foundation",
      "relationship_sentence": "Provides the theoretical link that one-step denoising estimates the score of a Gaussian-smoothed log-density, directly enabling the paper\u2019s single-noise training and the denoising projection from the smoothed manifold back to the discrete data space."
    },
    {
      "title": "Training Products of Experts by Minimizing Contrastive Divergence",
      "authors": "Geoffrey Hinton",
      "year": 2002,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Establishes contrastive divergence learning for energy-based models, which the paper uses to train its smoothed energy function that underpins the Langevin walk."
    },
    {
      "title": "Generative Modeling by Estimating Gradients of the Data Distribution (Noise Conditional Score Networks)",
      "authors": "Yang Song et al.",
      "year": 2019,
      "arxiv_id": "1907.05600",
      "role": "Baseline",
      "relationship_sentence": "Demonstrates high-fidelity synthesis via multi-noise-level score estimation and annealed Langevin dynamics, serving as the key generative baseline whose sample quality this work matches while removing the multi-sigma schedule by using a single-noise smoothed energy."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho et al.",
      "year": 2020,
      "arxiv_id": "2006.11239",
      "role": "Related Problem",
      "relationship_sentence": "Popularizes iterative denoising as a generative mechanism, which this paper replaces with a single denoising jump by leveraging a smoothed energy landscape to avoid long reverse chains."
    },
    {
      "title": "Structured Denoising Diffusion Models in Discrete State-Spaces (D3PM)",
      "authors": "Jacob Austin et al.",
      "year": 2021,
      "arxiv_id": "2107.03006",
      "role": "Gap Identification",
      "relationship_sentence": "Shows how discrete diffusion requires intricate transition kernels and slow sampling, motivating the paper\u2019s continuous smoothed-manifold Langevin walk plus one-step discrete projection to address these discrete-modeling limitations."
    }
  ],
  "synthesis_narrative": "Neural Empirical Bayes introduced a two-phase sampling strategy that first performs Langevin dynamics on a Gaussian-smoothed data density and then makes a one-step denoising jump back to the data manifold, highlighting how smoothing can regularize sampling while denoising provides a principled projection. The theoretical basis for this denoising-as-score estimate comes from the connection between denoising autoencoders and score matching, which shows that a single-step denoiser recovers the gradient of the smoothed log-density, enabling both learning and projection at one noise level. Contrastive divergence supplied the practical learning rule for energy-based models, providing a tractable objective to estimate an energy (or score) even when exact likelihood gradients are intractable. Noise Conditional Score Networks demonstrated that multi-noise score estimation plus annealed Langevin yields strong sample quality, establishing a benchmark for generative fidelity. Denoising Diffusion Probabilistic Models further popularized iterative denoising as a generative paradigm, albeit requiring long reverse-time chains. For discrete data, D3PM formalized diffusion with categorical transitions but at the cost of complex kernels and slow sampling.\nTogether these works suggested a path: keep the robustness and sample quality of score-based approaches while avoiding multi-noise schedules and discrete diffusion\u2019s complexity by moving stochastic exploration to a continuous, smoothed energy surface and using a theoretically grounded, single-step denoising projection. The present method synthesizes these ideas by training a smoothed energy with contrastive divergence, walking with Langevin on the smoothed manifold, and jumping once to the discrete data space\u2014delivering diffusion-level quality with simplified training and efficient sampling tailored to discrete protein sequences.",
  "target_paper": {
    "title": "Protein Discovery with Discrete Walk-Jump Sampling",
    "authors": "Nathan C. Frey, Dan Berenberg, Karina Zadorozhny, Joseph Kleinhenz, Julien Lafrance-Vanasse, Isidro Hotzel, Yan Wu, Stephen Ra, Richard Bonneau, Kyunghyun Cho, Andreas Loukas, Vladimir Gligorijevic, Saeed Saremi",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "generative modeling, langevin mcmc, energy-based models, score-based models, protein design, protein discovery",
    "abstract": "We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our $\\textit{Discrete Walk-Jump Sampling}$ formalism combines the contrastive divergence training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the $\\textit{distributional conformity score}$ to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100\\% of generated samples are successfully expressed and purified and 70\\% of functional designs show equal or improved binding affinity compared to known functional antibodies on",
    "openreview_id": "zMPHKOmQNb",
    "forum_id": "zMPHKOmQNb"
  },
  "analysis_timestamp": "2026-01-06T09:42:59.382133"
}
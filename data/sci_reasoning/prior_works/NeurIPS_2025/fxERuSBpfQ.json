{
  "prior_works": [
    {
      "title": "MAttNet: Modular Attention Network for Referring Expression Comprehension",
      "authors": "Licheng Yu et al.",
      "year": 2018,
      "role": "Attribute-aware grounding method (modular REC)",
      "relationship_sentence": "MAttNet\u2019s decomposition of language into subject/location/relationship modules with learned gating directly inspired EventRefer\u2019s attribute-specific experts and dynamic fusion (MoEE) across multiple grounding cues."
    },
    {
      "title": "RefCOCO, RefCOCO+ and RefCOCOg: Datasets for Referring Expression Comprehension",
      "authors": "Licheng Yu et al.",
      "year": 2016,
      "role": "Foundational datasets and task formulation for REC",
      "relationship_sentence": "These large-scale referring expression benchmarks established the annotation protocols and evaluation for language-driven grounding, which Talk2Event extends to event-based, dynamic driving scenes with an expanded attribute taxonomy (appearance, status, viewer/other relations)."
    },
    {
      "title": "TransVG: End-to-End Visual Grounding With Transformers",
      "authors": "Shizhe Chen et al.",
      "year": 2021,
      "role": "Transformer-based cross-modal grounding architecture",
      "relationship_sentence": "TransVG\u2019s end-to-end transformer alignment of text and visual features informed EventRefer\u2019s cross-modal design while MoEE augments this with attribute-wise expert fusion tailored to event streams."
    },
    {
      "title": "DSEC: A Stereo Event Camera Dataset for Driving Scenarios",
      "authors": "Daniel Gehrig et al.",
      "year": 2021,
      "role": "Real-world driving dataset for event cameras",
      "relationship_sentence": "DSEC established large-scale, high-quality event data collection in driving, providing the sensing setup and benchmarks that motivated Talk2Event\u2019s language annotation of real-world event streams."
    },
    {
      "title": "Events-to-Video: Bringing Modern Computer Vision to Event Cameras (E2VID)",
      "authors": "Henri Rebecq et al.",
      "year": 2019,
      "role": "Event-to-intensity reconstruction enabling event\u2013frame fusion",
      "relationship_sentence": "E2VID demonstrated effective mappings from events to frame-like representations, motivating Talk2Event\u2019s event\u2013frame fusion setting and informing EventRefer\u2019s modality-adaptive feature processing."
    },
    {
      "title": "Sparsely-Gated Mixture-of-Experts Layer",
      "authors": "Noam Shazeer et al.",
      "year": 2017,
      "role": "Core mixture-of-experts gating mechanism",
      "relationship_sentence": "The MoE principle of dynamically routing inputs to specialized experts underpins EventRefer\u2019s Mixture of Event-Attribute Experts, enabling adaptive weighting of appearance/temporal/relational branches across scene dynamics."
    },
    {
      "title": "Refer-Youtube-VOS: A Temporal Referring Video Object Segmentation Dataset",
      "authors": "Daizong Ding et al.",
      "year": 2020,
      "role": "Language grounding in dynamic video scenes",
      "relationship_sentence": "By framing language-guided localization over time, Refer-Youtube-VOS influenced Talk2Event\u2019s emphasis on temporal status and relational reasoning and its evaluation of grounding in dynamic scenarios."
    }
  ],
  "synthesis_narrative": "Talk2Event\u2019s core contribution\u2014bringing language-driven object grounding to event-camera streams with an attribute-aware, modality-adaptive model\u2014emerges from two converging lines of prior work. On the language grounding side, RefCOCO/RefCOCO+/RefCOCOg defined large-scale referring expression protocols, while MAttNet showed that decomposing expressions into attribute-specific modules with learned gates yields strong grounding. This modular, attribute-centric view is extended in Talk2Event with a richer taxonomy (appearance, status, viewer/other relations) and operationalized by EventRefer\u2019s Mixture of Event-Attribute Experts (MoEE), which explicitly learns to weight specialized branches. Transformer-based end-to-end grounding, as exemplified by TransVG, informed the cross-modal alignment backbone that MoEE augments with dynamic attribute fusion.\nOn the sensing side, DSEC established robust, real-world driving collections for event cameras, validating the feasibility and value of large-scale event datasets that Talk2Event annotates with language. E2VID demonstrated how to bridge events and conventional vision by reconstructing intensity, motivating EventRefer\u2019s support for event-only, frame-only, and event\u2013frame fusion regimes. Finally, the temporal aspect of grounding draws from video referring literature such as Refer-Youtube-VOS, which emphasized language-conditioned localization in dynamic scenes; this shaped Talk2Event\u2019s inclusion of temporal \u201cstatus\u201d and relational attributes and its evaluation design. Underlying the model, Shazeer et al.\u2019s sparsely gated Mixture-of-Experts provides the principled mechanism for MoEE\u2019s adaptive routing across attribute experts, enabling robust grounding across rapidly changing event-driven dynamics.",
  "analysis_timestamp": "2026-01-07T00:21:32.350811"
}
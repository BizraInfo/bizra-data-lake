{
  "prior_works": [
    {
      "title": "Without-Replacement Sampling for Stochastic Gradient Methods",
      "authors": "Ohad Shamir",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "Formalized SGD without-replacement (random reshuffling) and provided core convergence analyses that this paper builds on and explicitly seeks to strengthen from average-iterate (and strong-convexity/distance metrics) to last-iterate function-value guarantees."
    },
    {
      "title": "Why Random Reshuffling Beats Stochastic Gradient Descent",
      "authors": "A. G\u00fcrb\u00fczbalaban et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "Established that random reshuffling can outperform with-replacement SGD and developed epoch-wise analytical tools for finite-sum shuffling that directly motivate the present last-iterate analysis for RR/SO/IG."
    },
    {
      "title": "Incremental Subgradient Methods for Nondifferentiable Optimization",
      "authors": "Angelia Nedi\u0107 et al.",
      "year": 2001,
      "role": "Foundation",
      "relationship_sentence": "Introduced and analyzed incremental (including projected) gradient/subgradient methods\u2014the IG component of shuffling methods\u2014whose constrained optimization setting the current paper targets when proving last-iterate convergence in function value."
    },
    {
      "title": "Tight Complexity Bounds for Optimizing Finite Sums",
      "authors": "Blake Woodworth et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "Provided sharp lower bounds for finite-sum optimization methods; the present work positions its last-iterate function-value rates relative to these bounds and claims to match or nearly match the known last-iterate lower limits."
    },
    {
      "title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning",
      "authors": "Eric Moulines et al.",
      "year": 2011,
      "role": "Gap Identification",
      "relationship_sentence": "Delivered last-iterate convergence in mean-squared error for strongly convex problems (with-replacement sampling), highlighting the limitation to distance metrics and strong convexity that this paper overcomes for shuffling methods and function-value criteria."
    },
    {
      "title": "Introductory Lectures on Convex Optimization: A Basic Course",
      "authors": "Yurii Nesterov",
      "year": 2004,
      "role": "Foundation",
      "relationship_sentence": "Established the canonical last-iterate function-value convergence framework for projected gradient methods in convex optimization, which this paper extends to the shuffling/incremental regime without assuming strong convexity."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014proving last-iterate convergence rates in objective value for shuffling gradient methods (RR, SO, IG) without strong convexity\u2014rests on the finite-sum and without-replacement lineage. Shamir (2016) formalized SGD without-replacement and delivered the baseline analyses for random reshuffling, largely focused on average-iterate guarantees and settings where strong convexity or distance-based metrics were central. G\u00fcrb\u00fczbalaban et al. (2015) showed why random reshuffling can outperform with-replacement sampling and introduced epoch-wise tools for analyzing permutation-induced bias\u2014techniques that shape the analytical backbone for last-iterate reasoning in shuffled passes. The incremental gradient lineage originates with Nedi\u0107 and Bertsekas (2001), who defined and studied IG and its projected variants in constrained convex optimization\u2014precisely the regime where function value (not squared distance) is the appropriate performance metric and where the present paper closes a long-standing gap. On the limits side, Woodworth and Srebro (2016) provided tight lower bounds for finite-sum optimization, offering the yardstick that this paper uses when claiming its last-iterate rates match or nearly match the best possible. Classical non-asymptotic results for strongly convex problems (Moulines and Bach, 2011) emphasized last-iterate performance in squared distance under with-replacement sampling; the present work moves beyond both the metric and the strong-convexity assumption. Finally, Nesterov\u2019s convex optimization framework (2004) sets the function-value, last-iterate benchmark for projected gradient methods that this paper extends to shuffled, incremental passes.",
  "analysis_timestamp": "2026-01-06T23:09:26.457294"
}
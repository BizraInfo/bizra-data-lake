{
  "prior_works": [
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "authors": "Gu and Dao et al.",
      "year": 2023,
      "arxiv_id": "2312.00752",
      "role": "Foundation",
      "relationship_sentence": "Mamba formalized input-dependent (selective) state-space operators, providing the concrete linear input-varying systems paradigm that STAR generalizes into a broader architectural search space."
    },
    {
      "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
      "authors": "Poli et al.",
      "year": 2023,
      "arxiv_id": "2302.10866",
      "role": "Related Problem",
      "relationship_sentence": "Hyena introduced long-range, FFT-based convolutional operators as efficient alternatives to attention, supplying a key class of deep signal-processing units that STAR includes as primitives in its search space."
    },
    {
      "title": "StripedHyena: Efficient Long-Context Language Models with Hybrid Alternating Convolution and Attention",
      "authors": "Poli et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "StripedHyena demonstrated that manually designed hybrids of attention and long-convolution can push the quality\u2013efficiency frontier, forming the primary hybrid baseline whose hand-crafted patterns STAR aims to automatically synthesize and surpass."
    },
    {
      "title": "Evolving Deep Neural Networks (CoDeepNEAT)",
      "authors": "Miikkulainen et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "CoDeepNEAT introduced hierarchical genotype\u2013phenotype encodings and recombination for neural architectures, which STAR extends with a numeric, hierarchical genome tailored to linear input-varying operators and their interconnections."
    },
    {
      "title": "Regularized Evolution for Image Classifier Architecture Search",
      "authors": "Real et al.",
      "year": 2019,
      "arxiv_id": "1802.01548",
      "role": "Foundation",
      "relationship_sentence": "Regularized (aging) evolution established a robust gradient-free algorithm for neural architecture search that STAR adopts and adapts to explore its LIV-based genome space at population scale."
    },
    {
      "title": "A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II",
      "authors": "Deb et al.",
      "year": 2002,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "NSGA-II provides the Pareto-based multi-objective selection mechanism that STAR leverages to simultaneously optimize model quality and efficiency across large architecture populations."
    },
    {
      "title": "DARTS: Differentiable Architecture Search",
      "authors": "Liu et al.",
      "year": 2019,
      "arxiv_id": "1806.09055",
      "role": "Gap Identification",
      "relationship_sentence": "DARTS\u2019s cell-based search spaces tend to yield simple, repetitive motifs, a limitation explicitly targeted by STAR\u2019s richer LIV-theoretic search space and hierarchical genome design."
    }
  ],
  "synthesis_narrative": "Selective state-space models showed that sequence processing can be cast as input-dependent linear dynamical operators with hardware-efficient scans, grounding a rigorous class of linear input-varying systems. In parallel, long-range convolutional operators demonstrated that learned filters and FFT-based mechanisms can rival attention for dependency modeling while improving efficiency. Building on these insights, manually engineered hybrids that interleave long convolutions with local attention revealed that mixing complementary units improves the quality\u2013efficiency trade-off, albeit via bespoke design rules. Independently, neuroevolution established hierarchical genotype\u2013phenotype encodings and recombination as practical tools for exploring large architectural spaces, while regularized evolution provided a simple, stable algorithm for population-based search at scale. Multi-objective evolutionary methods further introduced Pareto selection to balance accuracy with deployment costs. Yet differentiable NAS with cell-level search spaces often converged to shallow, repetitive motifs, indicating that the expressiveness of the search space\u2014not just the optimizer\u2014was a key bottleneck.\nTogether, these strands suggested an opportunity: unify input-varying linear operators and deep signal-processing units as first-class primitives, then automatically discover hybrid interconnections under multi-objective constraints using evolutionary search. By adopting a hierarchical numerical genome tailored to linear input-varying compositions, and by coupling aging evolution with Pareto selection, the current work operationalizes this synthesis, automating the discovery of diverse, efficient hybrids that generalize beyond hand-crafted patterns and overcome the simplicity limits of prior NAS spaces.",
  "target_paper": {
    "title": "STAR: Synthesis of Tailored Architectures",
    "authors": "Armin W Thomas, Rom Parnichkun, Alexander Amini, Stefano Massaroli, Michael Poli",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "alternative architectures, deep signal processing, language models",
    "abstract": "Iterative improvement of model architectures is fundamental to deep learning: Transformers first enabled scaling, and recent advances in model hybridization have pushed the quality-efficiency frontier. However, optimizing architectures remains challenging and expensive, with a variety of automated or manual approaches that fall short, due to limited progress in the design of search spaces and due to the simplicity of resulting patterns and heuristics. In this work, we propose a new approach for the synthesis of tailored architectures (STAR). Our approach combines a novel search space based on the theory of linear input-varying systems, supporting a hierarchical numerical encoding into architecture genomes. STAR genomes are automatically refined and recombined with gradient-free, evolutionary algorithms to optimize for multiple model quality and efficiency metrics. Using STAR, we optimize large populations of new architectures, leveraging diverse computational units and interconnection ",
    "openreview_id": "HsHxSN23rM",
    "forum_id": "HsHxSN23rM"
  },
  "analysis_timestamp": "2026-01-06T13:40:55.763675"
}
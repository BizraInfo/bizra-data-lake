{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Foundational contrastive baseline and dominant paradigm for web-scale image\u2013text pretraining",
      "relationship_sentence": "This paper directly challenges the CLIP contrastive objective by performing an apples-to-apples comparison and showing that captioning-only pretraining can produce encoders that match or surpass CLIP-style models under matched data, compute, and capacity."
    },
    {
      "title": "ALIGN: Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
      "authors": "Chao Jia et al.",
      "year": 2021,
      "role": "Demonstrated the effectiveness and scaling of contrastive learning on large, noisy web image\u2013text data",
      "relationship_sentence": "ALIGN\u2019s results and data regime established contrastive learning as the scalable default on web data, providing the foil and experimental template that this work replicates to fairly test captioning at equal scale."
    },
    {
      "title": "VirTex: Learning Visual Representations from Textual Annotations",
      "authors": "Karan Desai, Justin Johnson",
      "year": 2021,
      "role": "Early evidence that autoregressive captioning can learn transferable vision features",
      "relationship_sentence": "VirTex motivated the core hypothesis of this paper by showing captioning-as-pretraining yields useful visual encoders, which this work scales to noisy web data and rigorously compares against contrastive pretraining."
    },
    {
      "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
      "authors": "Zihang Wang et al.",
      "year": 2022,
      "role": "Large-scale generative (prefix-LM) pretraining on noisy web image\u2013text data",
      "relationship_sentence": "SimVLM provided strong evidence that pure generative objectives scale competitively on vision\u2013language tasks, informing this paper\u2019s focus on captioning-only training and its scaling analysis."
    },
    {
      "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
      "authors": "Jiahui Yu et al.",
      "year": 2022,
      "role": "Hybrid contrastive+captioning framework with an encoder\u2013decoder architecture",
      "relationship_sentence": "CoCa showed the synergy of contrastive and captioning signals and popularized the encoder\u2013decoder setup; this work isolates the captioning objective within a similar architecture to test whether captioning alone is sufficient."
    },
    {
      "title": "GIT: A Generalist Image-to-Text Transformer for Vision\u2013Language Tasks",
      "authors": "Peng Wang et al.",
      "year": 2022,
      "role": "Generative captioning-style pretraining that scales across diverse V&L tasks",
      "relationship_sentence": "GIT\u2019s success with a purely generative training regime supported the feasibility of captioners as scalable learners, which this paper probes by measuring encoder quality and zero-shot transfer under controlled conditions."
    },
    {
      "title": "PaLI: A Jointly-Scaled Multilingual Multimodal Model",
      "authors": "Xi Chen et al.",
      "year": 2023,
      "role": "Massive encoder\u2013decoder captioning-based VLM demonstrating favorable scaling",
      "relationship_sentence": "PaLI\u2019s scaling results for captioning-style models informed this paper\u2019s systematic scaling study, reinforcing that captioners can improve with data, model, and compute in ways comparable to or better than contrastive models."
    }
  ],
  "synthesis_narrative": "The dominant narrative that contrastive learning on web-scale image\u2013text pairs is the most effective way to build vision backbones was established by CLIP and ALIGN, which demonstrated strong zero-shot classification and robust scaling on noisy web data. At the same time, VirTex provided early evidence that captioning-style autoregressive objectives can yield transferable visual features, but it operated at smaller scales and on cleaner captions. Subsequent large-scale generative efforts\u2014SimVLM, GIT, and PaLI\u2014showed that decoder-based or prefix-LM captioning on noisy web data can scale impressively across vision\u2013language tasks, suggesting captioning is not inherently inferior. CoCa bridged the two camps by unifying contrastive and captioning signals in an encoder\u2013decoder framework, implying that the generative signal adds value even when contrastive training is present.\n\nBuilding on these threads, \u201cImage Captioners Are Scalable Vision Learners Too\u201d isolates the captioning objective within a standard encoder\u2013decoder Transformer and, crucially, conducts a fair comparison against contrastive pretraining by matching data, compute, and model capacity. Inspired by CLIP/ALIGN\u2019s protocols and CoCa\u2019s architecture, and grounded by VirTex/SimVLM/GIT/PaLI\u2019s evidence that generative training scales, the paper demonstrates that captioning-only pretraining produces vision encoders competitive on classification and superior on vision\u2013language tasks. It further shows captioning exhibits equal or better scaling with model size and data, overturning the prevailing assumption of captioning\u2019s inferiority and clarifying objective-choice trade-offs for building general-purpose vision backbones.",
  "analysis_timestamp": "2026-01-06T23:42:49.092884"
}
{
  "prior_works": [
    {
      "title": "A Distributional Perspective on Reinforcement Learning",
      "authors": "Marc G. Bellemare et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized the distributional Bellman operator and the notion of learning the full return distribution, which FDE directly adopts to define distributional policy evaluation targets and contraction metrics."
    },
    {
      "title": "Distributional Reinforcement Learning with Quantile Regression",
      "authors": "Will Dabney et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "QR-DQN introduced quantile representations and pinball losses for approximating return distributions; FDE leverages these representations and losses as core building blocks for fitted distributional updates and their analysis."
    },
    {
      "title": "An Analysis of Categorical Distributional Reinforcement Learning",
      "authors": "Mark Rowland et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "The analysis of projection operators and contraction under Cram\u00e9r distance in categorical distributional RL informs FDE\u2019s principled use of projection steps and choice of discrepancy metrics for convergence guarantees."
    },
    {
      "title": "Tree-Based Batch Mode Reinforcement Learning (Fitted Q Iteration)",
      "authors": "Damien Ernst et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "Fitted Q Iteration established the fitted-iteration template\u2014Bellman regression with function approximation\u2014that FQE instantiates for mean values and FDE generalizes to the distributional setting."
    },
    {
      "title": "Finite-Time Bounds for Fitted Value Iteration",
      "authors": "R\u00e9mi Munos et al.",
      "year": 2008,
      "role": "Foundation",
      "relationship_sentence": "This work provided error-propagation analyses for fitted iterations under function approximation; FDE extends this theoretical machinery to distributional operators to obtain non-tabular convergence results."
    },
    {
      "title": "Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning",
      "authors": "C. Voloshin et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "This study popularized and established Fitted Q Evaluation (FQE) as a strong, practical OPE baseline for expected returns, which the present work explicitly generalizes to the distributional OPE setting."
    },
    {
      "title": "Doubly Robust Off-policy Evaluation for Reinforcement Learning",
      "authors": "Nan Jiang et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "By formalizing OPE and focusing on estimators for expected value (e.g., IS/DR) with notable bias\u2013variance trade-offs, this work underscores the limitation of expectation-only evaluation that FDE addresses by modeling full return distributions."
    }
  ],
  "synthesis_narrative": "The core innovation of FDE\u2014extending fitted Q-style evaluation to full return distributions\u2014sits at the intersection of two lines of work: fitted value methods for off-policy evaluation and distributional reinforcement learning. The fitted-iteration paradigm originated with Ernst et al. (2005), and its finite-sample behavior under function approximation was analyzed by Munos and Szepesv\u00e1ri (2008). Building directly on this scaffold, the OPE community adopted Fitted Q Evaluation as a practical baseline for mean-value estimation, as evidenced by Voloshin et al. (2019), but it remained confined to expectations. In parallel, Bellemare et al. (2017) reframed RL around the distributional Bellman operator, showing that modeling the entire return distribution can confer theoretical and empirical benefits, thereby exposing a gap in OPE methods that targeted only the mean (as highlighted more broadly by Jiang and Li, 2016). Subsequent distributional RL advances\u2014Dabney et al. (2018) with quantile regression and Rowland et al. (2018) with categorical projections and contraction analyses\u2014provided practical parameterizations and the projection-theoretic tools needed to make distributional updates stable and analyzable. FDE synthesizes these threads: it retains the fitted-iteration backbone of FQE, swaps in the distributional Bellman targets and appropriate divergence metrics from distributional RL, and ports the fitted-iteration error-propagation arguments to the distributional setting. This yields a principled design framework, new algorithms with convergence guarantees in non-tabular settings, and theoretical justification for existing distributional evaluation procedures.",
  "analysis_timestamp": "2026-01-06T23:08:23.954108"
}
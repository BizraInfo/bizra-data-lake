{
  "prior_works": [
    {
      "title": "Learning Classifiers from Only Positive and Unlabeled Data",
      "authors": "Charles Elkan et al.",
      "year": 2008,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The paper adopts Elkan & Noto\u2019s PU formulation\u2014using a positive prior to correct risk under missing labels\u2014to reframe AI-text detection so that ambiguous short machine texts can be treated as unlabeled rather than confidently labeled negatives."
    },
    {
      "title": "Positive-Unlabeled Learning with Non-Negative Risk Estimator",
      "authors": "Masashi Kiryo et al.",
      "year": 2017,
      "arxiv_id": "1703.00593",
      "role": "Extension",
      "relationship_sentence": "The proposed Multiscale PU Loss directly extends the nnPU risk estimator by incorporating length-conditioned positive priors, preventing negative risk overfitting while making the PU objective sensitive to text length."
    },
    {
      "title": "Estimating the Class Prior in Positive and Unlabeled Data through Decision Tree Induction",
      "authors": "Jesse Davis et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Building on the necessity of class-prior estimation emphasized by TIcE, the paper replaces a single global prior with a learned, multiscale (length-dependent) prior estimated by a recurrent model to match scale-variant corpora."
    },
    {
      "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
      "authors": "Eric Mitchell et al.",
      "year": 2023,
      "arxiv_id": "2301.11305",
      "role": "Baseline",
      "relationship_sentence": "DetectGPT serves as a primary zero-shot baseline whose performance degrades on short passages, directly motivating a supervised, length-aware PU training scheme to recover power on short texts without sacrificing long-text accuracy."
    },
    {
      "title": "Defending Against Neural Fake News",
      "authors": "Rowan Zellers et al.",
      "year": 2019,
      "arxiv_id": "1905.12616",
      "role": "Foundation",
      "relationship_sentence": "This work established supervised discriminators for human vs. machine text detection, which the paper retains but retools with a PU objective to cope with label ambiguity that is especially acute for short, human-like generations."
    },
    {
      "title": "A Watermark for Large Language Models",
      "authors": "Jacob Kirchenbauer et al.",
      "year": 2023,
      "arxiv_id": "2301.10226",
      "role": "Gap Identification",
      "relationship_sentence": "By showing detection power grows with sample length and is weak on short outputs, this work crystallizes the length-driven detectability gap that the paper addresses via a multiscale, length-sensitive PU prior and loss."
    }
  ],
  "synthesis_narrative": "Supervised detection of machine-generated text was popularized by Grover, which demonstrated that discriminators trained on human vs. generator outputs can be effective but implicitly assume clean labels across examples. Zero-shot detection advanced with DetectGPT, which exploits probability curvature but exhibits a marked drop on short inputs. Watermarking further quantified the statistical reality that detection power scales with sample length, making very short texts intrinsically hard to flag. In parallel, Elkan and Noto formalized learning from positive and unlabeled data by correcting risk with a positive prior under missing labels, while Kiryo et al. stabilized PU training via the non-negative risk estimator to avoid negative risk overfitting. Davis and colleagues (TIcE) emphasized that estimating the class prior is central in PU settings, motivating methods that infer priors from structure in the data rather than assuming a fixed constant. Together, these lines showed that short texts are both label-ambiguous and statistically underpowered for detection, and that PU learning with careful prior estimation can handle missing or unreliable labels. The current paper synthesizes these insights by recasting short machine outputs as unlabeled within a PU framework and generalizing nnPU with a length-sensitive, multiscale loss. By learning length-conditioned positive priors\u2014rather than assuming a global prior\u2014it aligns the PU objective with detectability that varies by text length, closing the short-text gap while preserving long-text performance.",
  "target_paper": {
    "title": "Multiscale Positive-Unlabeled Detection of AI-Generated Texts",
    "authors": "Yuchuan Tian, Hanting Chen, Xutao Wang, Zheyuan Bai, QINGHUA ZHANG, Ruifeng Li, Chao Xu, Yunhe Wang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Large Language Models, AI-Generated Texts, Positive-Unlabeled Learning",
    "abstract": "Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot methods, and finetuned language classification models. However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially \"unlabeled\". Then in this PU context, we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora",
    "openreview_id": "5Lp6qU9hzV",
    "forum_id": "5Lp6qU9hzV"
  },
  "analysis_timestamp": "2026-01-06T13:16:22.365134"
}
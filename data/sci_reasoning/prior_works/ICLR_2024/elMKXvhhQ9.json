{
  "prior_works": [
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "authors": "Kihyuk Sohn et al.",
      "year": 2020,
      "arxiv_id": "2001.07685",
      "role": "Inspiration",
      "relationship_sentence": "ConsisGAD adopts FixMatch\u2019s core idea of enforcing prediction consistency between weakly and strongly augmented views to exploit unlabeled data, but instantiates it on graphs with a trainable augmentation module rather than hand-crafted policies."
    },
    {
      "title": "Unsupervised Data Augmentation for Consistency Training",
      "authors": "Qizhe Xie et al.",
      "year": 2019,
      "arxiv_id": "1904.12848",
      "role": "Foundation",
      "relationship_sentence": "UDA provides the foundational consistency-regularization principle that ConsisGAD brings to GAD\u2014leveraging abundant unlabeled samples by enforcing invariant predictions under augmentation."
    },
    {
      "title": "GraphCL: Contrastive Self-Supervised Learning on Graphs",
      "authors": "Yuning You et al.",
      "year": 2020,
      "arxiv_id": "2007.08025",
      "role": "Foundation",
      "relationship_sentence": "GraphCL established the importance of graph data augmentations to create semantically consistent views; ConsisGAD repurposes this multi-view idea for consistency training and shifts from fixed to learnable graph augmentations."
    },
    {
      "title": "Graph Contrastive Learning with Adaptive Augmentation",
      "authors": "Yanqiao Zhu et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By showing that augmentation quality should be data-dependent, GCA motivates ConsisGAD\u2019s learnable augmentation mechanism that adaptively injects controlled noise for more informative consistency signals."
    },
    {
      "title": "FLAG: Adversarial Data Augmentation for Graph Neural Networks",
      "authors": "Chenxin Xie et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "FLAG\u2019s gradient-based feature perturbations inspire ConsisGAD\u2019s idea of trainable, model-informed perturbations; ConsisGAD extends this by integrating a learnable augmenter into a consistency objective tailored to anomaly detection."
    },
    {
      "title": "DOMINANT: Deep Anomaly Detection on Attributed Networks",
      "authors": "Ling Huang Ding et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "DOMINANT formalized node-level anomaly detection on attributed graphs and provided standard benchmarks and evaluation protocols that ConsisGAD builds upon while moving from unsupervised reconstruction to limited-supervision consistency."
    },
    {
      "title": "Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs",
      "authors": "Chuan-Sheng Zhu et al.",
      "year": 2020,
      "arxiv_id": "2006.11468",
      "role": "Related Problem",
      "relationship_sentence": "This work\u2019s insight that message passing behaves differently under varying homophily informs ConsisGAD\u2019s simplified GNN backbone that leverages homophily distribution differences between normal and anomalous nodes."
    }
  ],
  "synthesis_narrative": "FixMatch demonstrated that semi-supervised learners can harness unlabeled data by enforcing prediction agreement between weakly and strongly augmented views, crystallizing consistency regularization as a practical recipe. UDA further grounded this principle by directly tying augmentation-induced invariance to improved utilization of unlabeled samples. In the graph domain, GraphCL showed that generating multiple stochastic views via graph-specific augmentations creates meaningful invariances for representation learning, highlighting augmentations as a first-class design choice. GCA then made augmentation adaptive, revealing that which edges or features to perturb should depend on graph structure and node importance rather than fixed rules. Complementing these ideas, FLAG introduced model-aware, gradient-driven perturbations as a form of learnable, controllable augmentation for node-level tasks, suggesting that augmenters can be optimized jointly with GNNs. Meanwhile, DOMINANT set the problem context and benchmarks for node anomaly detection on attributed graphs, against which advances are measured. Finally, Beyond Homophily clarified that message passing interacts with homophily/heterophily regimes, motivating architectures that explicitly account for differences in neighborhood label alignment.\nTogether these works exposed a gap: graph anomaly detection with scarce labels lacks a principled way to exploit unlabeled nodes while using the right, data-dependent perturbations, and backbone designs should reflect homophily differences between normal and anomalous nodes. The natural next step is to fuse consistency regularization with a learnable, model-informed graph augmenter that injects controlled noise, and to pair it with a backbone simplified to capitalize on homophily variance\u2014precisely the synthesis that enables effective GAD under limited supervision and class imbalance.",
  "target_paper": {
    "title": "Consistency Training with Learnable Data Augmentation for Graph Anomaly Detection with Limited Supervision",
    "authors": "Nan Chen, Zemin Liu, Bryan Hooi, Bingsheng He, Rizal Fathony, Jun Hu, Jia Chen",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Graph anomaly detection, consistency training, learnable data augmentation",
    "abstract": "Graph Anomaly Detection (GAD) has surfaced as a significant field of research, predominantly due to its substantial influence in production environments. Although existing approaches for node anomaly detection have shown effectiveness, they have yet to fully address two major challenges: operating in settings with limited supervision and managing class imbalance effectively. In response to these challenges, we propose a novel model, ConsisGAD, which is tailored for GAD in scenarios characterized by limited supervision and is anchored in the principles of consistency training. Under limited supervision, ConsisGAD effectively leverages the abundance of unlabeled data for consistency training by incorporating a novel learnable data augmentation mechanism, thereby introducing controlled noise into the dataset. Moreover, ConsisGAD takes advantage of the variance in homophily distribution between normal and anomalous nodes to craft a simplified GNN backbone, enhancing its capability to disti",
    "openreview_id": "elMKXvhhQ9",
    "forum_id": "elMKXvhhQ9"
  },
  "analysis_timestamp": "2026-01-06T05:56:29.058187"
}
{
  "prior_works": [
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Search-based reasoning for LLMs",
      "relationship_sentence": "ThinkLite-VL repurposes the ToT idea of exploring multi-step reasoning trajectories, using search effort (iterations to a correct solution) as an operational difficulty signal to drive sample selection."
    },
    {
      "title": "Mastering the game of Go without human knowledge (AlphaGo Zero) / AlphaZero",
      "authors": "David Silver et al.",
      "year": 2017,
      "role": "MCTS as a principled mechanism to quantify problem difficulty via search depth/rollouts",
      "relationship_sentence": "The paper leverages the AlphaZero paradigm that MCTS search statistics reflect task complexity; it adapts MCTS rollouts in VLM reasoning to measure instance difficulty and filter hard-but-solvable samples."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2022,
      "role": "Multiple reasoning attempts as a proxy for solution robustness",
      "relationship_sentence": "By counting the number of reasoning attempts needed to reach a correct answer, ThinkLite-VL echoes self-consistency\u2019s insight that sampling over reasoning paths reveals instance difficulty and solution confidence."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "role": "Reinforcement learning from AI feedback (RLAIF) enabling self-improvement without human labels",
      "relationship_sentence": "ThinkLite-VL\u2019s pure reinforcement fine-tuning without distillation builds on the RLAIF paradigm of using automated signals to drive learning, replacing human feedback with verifiable correctness and MCTS-derived difficulty cues."
    },
    {
      "title": "Curriculum Learning",
      "authors": "Yoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, Jason Weston",
      "year": 2009,
      "role": "Principle that ordering/selecting examples by difficulty improves learning efficiency",
      "relationship_sentence": "The core contribution operationalizes curriculum learning for visual reasoning by explicitly selecting appropriately challenging (yet solvable) examples, estimated via MCTS-measured reasoning depth."
    },
    {
      "title": "Self-Paced Learning for Latent Variable Models",
      "authors": "M. P. Kumar, Benjamin Packer, Daphne Koller",
      "year": 2010,
      "role": "Algorithmic framework for model-driven difficulty-aware sample selection",
      "relationship_sentence": "ThinkLite-VL extends self-paced ideas by using the model\u2019s own search effort as a data-dependent hardness measure, curating a training subset that maximizes reinforcement-driven reasoning gains."
    },
    {
      "title": "Training Region-based Object Detectors with Online Hard Example Mining",
      "authors": "Abhinav Shrivastava, Abhinav Gupta, Ross Girshick",
      "year": 2016,
      "role": "Hard-example mining to focus learning on informative, challenging samples",
      "relationship_sentence": "Analogous to OHEM, the method targets difficult-but-solvable instances; it innovates by defining \u201chardness\u201d via MCTS reasoning iterations rather than loss magnitudes, tailored to multi-step visual reasoning."
    }
  ],
  "synthesis_narrative": "ThinkLite-VL\u2019s key innovation\u2014using MCTS-guided sample selection to enable data-efficient reinforcement fine-tuning for visual reasoning\u2014stands at the intersection of curriculum design, search-based reasoning, and self-improving RL. Curriculum Learning and Self-Paced Learning established that training benefits from examples of appropriate difficulty, motivating principled selection rather than indiscriminate scaling. Online Hard Example Mining sharpened this insight by focusing optimization on challenging, high-yield samples. \nTree of Thoughts then brought explicit search over reasoning trajectories into the LLM era, showing that problem solving improves when models explore multiple intermediate states. AlphaZero demonstrated that Monte Carlo Tree Search provides meaningful, quantitative signals about problem complexity through search depth and rollout statistics. ThinkLite-VL fuses these threads by using MCTS not to solve tasks per se, but to measure how many reasoning iterations a vision-language model requires to reach a correct solution\u2014thereby transforming search effort into a scalable, model-centric difficulty metric to curate a hard-but-solvable training subset. \nFinally, the reinforcement aspect is grounded in the self-improvement paradigm of RL from AI Feedback (e.g., Constitutional AI), replacing human supervision and distillation with verifiable correctness signals and MCTS-derived difficulty to drive RFT. The result is a practical curriculum: select instances that induce deeper reasoning while remaining solvable, and reinforce the model on precisely those samples. This synthesis yields state-of-the-art visual reasoning with an order of magnitude less data.",
  "analysis_timestamp": "2026-01-07T00:29:41.029745"
}
{
  "prior_works": [
    {
      "title": "Deep Sets",
      "authors": "Manzil Zaheer et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Deep Sets introduced the canonical shallow permutation-invariant architecture and sum-decomposition theorem, providing the problem formulation and baseline model class whose universality properties this paper systematizes and stratifies into distinct universality classes."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "By equating Message Passing GNN expressiveness with 1\u2011WL separation, this work cemented the field\u2019s focus on separation power, whose limitations as a proxy for approximation this paper explicitly exposes by showing identical-separation models can differ in universality."
    },
    {
      "title": "Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks",
      "authors": "Christopher Morris et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "This paper connected higher\u2011order GNNs to the WL hierarchy, establishing the dominant expressivity lens (separation via k\u2011WL) that our work moves beyond by characterizing approximation (universality) independently of WL separation."
    },
    {
      "title": "Universal Invariant and Equivariant Graph Neural Networks",
      "authors": "Nicolas Keriven et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Providing universality results for invariant/equivariant graph networks via polynomial and tensor constructions, this work is directly extended here by refining which shallow invariant architectures are universal and by delineating distinct universality classes despite matched separation power."
    },
    {
      "title": "Invariant and Equivariant Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "By introducing tensor-based invariant/equivariant layers that can achieve universality at sufficient tensor orders, this paper inspired our analysis showing how architectural choices within shallow invariant networks lead to different approximation capabilities even when separation power is the same."
    },
    {
      "title": "On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups",
      "authors": "Taco Cohen et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "This representation-theoretic framework for group-equivariant networks underpins our formal treatment of invariant/equivariant function spaces, enabling the principled characterization of universality classes studied in this work."
    }
  ],
  "synthesis_narrative": "The core innovation of this paper is to decouple separation power from approximation and to classify the universality of shallow invariant architectures. This builds squarely on two intertwined lines of prior work. First, Deep Sets established the canonical shallow permutation-invariant form and a precise functional decomposition, seeding the architectural and analytical template this paper systematizes. The broader representation-theoretic foundation for equivariant models, provided by the general group-convolution framework, supplies the mathematical setting to reason about invariant/equivariant function spaces and their approximation. Second, the dominant view of GNN expressiveness has been framed through the Weisfeiler\u2013Leman hierarchy: Xu et al. tied message-passing power to 1-WL separation, while Morris et al. extended this to higher orders. These works identified separation as the central metric, but they left open whether separation faithfully reflects approximation power\u2014a gap this paper directly addresses. On the universality side, Keriven and Peyr\u00e9 and Maron et al. proved that certain invariant/equivariant architectures can be universal, often via high-order tensor constructions. This paper extends and refines those insights to the shallow regime, demonstrating that models with identical WL-level separation can fall into distinct universality classes. Collectively, these works provide the architectural baseline, formal lens, and known limitations that this paper integrates and advances to a finer, approximation-centric theory of equivariant networks.",
  "analysis_timestamp": "2026-01-06T23:08:23.942961"
}
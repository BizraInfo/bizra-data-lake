{
  "prior_works": [
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan; Sam McCandlish; Tom Henighan; Tom B. Brown; Benjamin Chess; Rewon Child; Scott Gray; Alec Radford; Jeffrey Wu; Dario Amodei",
      "year": 2020,
      "role": "Foundational scaling laws for LMs",
      "relationship_sentence": "Provided the core power-law framework linking loss to parameters, data, and compute that this paper explicitly extends to the data-constrained regime by modeling repeated tokens and excess parameters."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann; Sebastian Borgeaud; Arthur Mensch; et al.",
      "year": 2022,
      "role": "Compute-optimal scaling (Chinchilla)",
      "relationship_sentence": "Established the modern compute-optimal frontier balancing parameters and tokens; the present work generalizes this notion when unique data is limited, deriving a compute-optimal law that discounts repeated tokens."
    },
    {
      "title": "Deep Learning Scaling is Predictable, Empirically",
      "authors": "Joel Hestness; Sharan Narang; Newsha Ardalani; Gregory Diamos; et al.",
      "year": 2017,
      "role": "Early empirical scaling law evidence",
      "relationship_sentence": "Demonstrated robust power-law behavior of loss vs. data/model size across tasks, laying the empirical groundwork that the paper leverages when quantifying how repetition alters the data exponent."
    },
    {
      "title": "Scaling Laws for Transfer",
      "authors": "Daniel Hernandez; Jared Kaplan; Tom Henighan; Sam McCandlish",
      "year": 2021,
      "role": "Scaling under task/compute constraints",
      "relationship_sentence": "Analyzed compute budgeting and the tradeoff between pretraining scale and downstream performance; this paper adopts a similar compute-centric view but adapts it to the practical constraint of limited unique tokens."
    },
    {
      "title": "Scaling Laws for Autoregressive Generative Modeling",
      "authors": "Tom Henighan; Jared Kaplan; et al.",
      "year": 2020,
      "role": "Cross-domain scaling laws and loss decomposition",
      "relationship_sentence": "Provided a general scaling framework and loss decompositions that inform the present paper\u2019s formulation of a modified scaling law where the marginal value of tokens decays with repetition."
    },
    {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J. Liu",
      "year": 2020,
      "role": "Data curation/dedup practices (C4) for LM pretraining",
      "relationship_sentence": "Showed the importance of large, cleaned, and deduplicated corpora for LM pretraining (C4), motivating this paper\u2019s focus on how limited unique data and repetition affect training dynamics and optimal compute allocation."
    },
    {
      "title": "Deduplicating Training Data Makes Language Models Better",
      "authors": "Katherine Lee; Daphne Ippolito; et al.",
      "year": 2021,
      "role": "Empirical evidence on harmful effects of duplication",
      "relationship_sentence": "Provided direct evidence that duplicated text skews training and evaluation and can increase memorization, motivating the present work\u2019s systematic study of multi-epoch repetition and its incorporation into a revised scaling law."
    }
  ],
  "synthesis_narrative": "Muennighoff et al. build squarely on the modern scaling-law lineage while addressing a gap: how to scale when unique data is scarce. Kaplan et al. established that cross-entropy scales as power laws in model size, data, and compute, providing the mathematical template the authors extend. Henighan et al. generalized these laws across modalities and offered loss decompositions that inform modeling choices. Hoffmann et al. (Chinchilla) reframed compute-optimal training, arguing for more data and fewer parameters than earlier prescriptions; this paper asks what happens when the data knob cannot turn further and formalizes compute-optimality when tokens must be reused. Hestness et al.\u2019s early empirical findings that loss predictably follows power laws underlie the new paper\u2019s assumption that modified exponents can capture regimes with repetition and overparameterization. Hernandez et al. analyze scaling under practical constraints and downstream transfer, a perspective echoed here via explicit compute budgeting in data-limited settings.\nBeyond the laws, the work is motivated by data quality and duplication practices from large-scale pretraining. Raffel et al. (T5/C4) highlighted the importance of deduped corpora, and Lee & Ippolito\u2019s study showed duplication can distort training and memorization. Together, these works directly inform the paper\u2019s experimental design on multi-epoch repetition and its key contribution: a revised compute-optimal scaling relation that discounts repeated tokens and excess parameters, empirically validated up to 900B tokens and 9B-parameter models.",
  "analysis_timestamp": "2026-01-06T23:42:49.096109"
}
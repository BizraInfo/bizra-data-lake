{
  "prior_works": [
    {
      "title": "A Mechanistic Interpretability Case Study on Indirect Object Identification in GPT-2",
      "authors": "Wang et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This IOI case study identified the specific multi-head circuit (e.g., name-mover and S-inhibition heads) that the current paper reproduces in a larger GPT-2 and tests for reuse on a second task."
    },
    {
      "title": "Colored Objects",
      "authors": "Ippolito and Callison-Burch",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work defines the Colored Objects task used as the second, ostensibly different problem on which the paper evaluates whether the IOI circuit components are reused."
    },
    {
      "title": "A Mathematical Framework for Transformer Circuits",
      "authors": "Elhage et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This framework formalizes the notion of transformer circuits and head-level roles that the paper adopts to compare and quantify component overlap across tasks."
    },
    {
      "title": "In-Context Learning and Induction Heads",
      "authors": "Olsson et al.",
      "year": 2022,
      "arxiv_id": "2201.11903",
      "role": "Inspiration",
      "relationship_sentence": "By showing a concrete head-level algorithm that recurs across tasks and scales, this work motivated testing whether the IOI circuit\u2019s components similarly generalize and are reused."
    },
    {
      "title": "Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias",
      "authors": "Vig et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper introduced causal intervention/patching methodology that underlies the paper\u2019s proof-of-concept interventions to verify that specific heads mediate behavior across tasks."
    },
    {
      "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
      "authors": "Voita et al.",
      "year": 2019,
      "arxiv_id": "1905.09418",
      "role": "Related Problem",
      "relationship_sentence": "This study established head specialization and validated head-level ablation as an analysis tool, which the paper leverages when quantifying overlap and reuse of specific attention heads."
    }
  ],
  "synthesis_narrative": "A mechanistic account of how GPT-2 solves Indirect Object Identification mapped a concrete multi-head circuit, including name-mover and inhibitory heads, and validated it with targeted interventions and ablations; this IOI case study provided a canonical, fine-grained circuit whose roles and interactions were precisely documented. The Colored Objects work introduced a controlled prompting task that requires tracking entities and their attributes under distractors, offering a crisp probe of compositional reference resolution that parallels IOI\u2019s symmetry-breaking demands. A formal framework for transformer circuits established the abstraction of head- and MLP-level subgraphs implementing algorithms, along with methodological norms for attributing behavior to circuit components. Evidence that induction heads instantiate a reusable, algorithmic mechanism across models and contexts showed that specific head types can generalize, suggesting a path toward cross-task reuse of higher-level circuits. Causal mediation analysis in NLP provided the intervention toolkit\u2014activation patching and path-specific tests\u2014to causally verify that particular heads and pathways transmit the information driving a prediction. Earlier evidence that attention heads specialize and can be individually ablated grounded the practice of measuring head-level contributions and overlap. Together, these works revealed a detailed IOI circuit, a second, structurally analogous task, a shared circuit abstraction, and causal tools, collectively opening the opportunity to test whether the same component-level mechanisms recur across tasks and scales; synthesizing these insights, the paper evaluates and causally verifies substantial head-level overlap between IOI and Colored Objects, demonstrating circuit component reuse.",
  "target_paper": {
    "title": "Circuit Component Reuse Across Tasks in Transformer Language Models",
    "authors": "Jack Merullo, Carsten Eickhoff, Ellie Pavlick",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "interpretability, llms, mechanistic interpretability, circuit",
    "abstract": "Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in (Wang, 2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in w",
    "openreview_id": "fpoAYV6Wsk",
    "forum_id": "fpoAYV6Wsk"
  },
  "analysis_timestamp": "2026-01-06T07:27:41.691632"
}
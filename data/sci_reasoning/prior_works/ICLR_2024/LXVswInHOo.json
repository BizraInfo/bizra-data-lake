{
  "prior_works": [
    {
      "title": "Language Models are Few-Shot Learners",
      "authors": "Tom B. Brown et al.",
      "year": 2020,
      "arxiv_id": "2005.14165",
      "role": "Foundation",
      "relationship_sentence": "This work established that next-token pretraining on randomly concatenated documents yields in-context learning, directly motivating the idea that modifying the pretraining context itself could strengthen cross-document reasoning."
    },
    {
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "authors": "Hugo Touvron et al.",
      "year": 2023,
      "arxiv_id": "2302.13971",
      "role": "Baseline",
      "relationship_sentence": "LLaMA codified the standard large-scale pretraining pipeline that concatenates heterogeneous documents with EOS tokens for efficiency, providing the exact baseline ordering that the new approach replaces with similarity-based sequences."
    },
    {
      "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
      "authors": "Kelvin Guu et al.",
      "year": 2020,
      "arxiv_id": "2002.08909",
      "role": "Inspiration",
      "relationship_sentence": "REALM showed that conditioning predictions on retrieved, semantically related documents during pretraining improves knowledge-intensive performance, inspiring the idea to expose models to relevant cross-document context as part of the training signal."
    },
    {
      "title": "Improving language models by retrieving from trillions of tokens",
      "authors": "Maja Borgeaud et al.",
      "year": 2022,
      "arxiv_id": "2112.04426",
      "role": "Gap Identification",
      "relationship_sentence": "RETRO demonstrated large gains from attending to retrieved neighbors but at the cost of a dedicated retriever and inference-time cross-attention, highlighting the gap that motivates achieving similar benefits purely via training-time document ordering."
    },
    {
      "title": "Generalization through Memorization: Nearest Neighbor Language Models",
      "authors": "Urvashi Khandelwal et al.",
      "year": 2020,
      "arxiv_id": "1911.00172",
      "role": "Related Problem",
      "relationship_sentence": "kNN-LM evidenced that augmenting with nearest-neighbor contexts improves predictions, underscoring the value of semantically proximate text while also revealing the drawback of relying on an external datastore at inference."
    },
    {
      "title": "ATLAS: Few-shot Learning with Retrieval Augmented Language Models",
      "authors": "Gautier Izacard et al.",
      "year": 2022,
      "arxiv_id": "2208.03299",
      "role": "Related Problem",
      "relationship_sentence": "ATLAS showed that pretraining and finetuning with retrieval markedly boost few-shot performance yet require a retrieval component at inference, motivating a training-only route to similar gains."
    },
    {
      "title": "MetaICL: Learning to Learn In Context",
      "authors": "Sewon Min et al.",
      "year": 2022,
      "arxiv_id": "2110.15943",
      "role": "Inspiration",
      "relationship_sentence": "MetaICL demonstrated that training on sequences of related examples teaches models to utilize in-context information, directly inspiring the notion of organizing pretraining data into semantically related sequences to induce cross-document reasoning."
    }
  ],
  "synthesis_narrative": "Few-shot prompting was crystallized by work showing that next-token language modeling over randomly concatenated documents can yield in-context learning abilities, implying that the distribution and structure of training context matter for downstream reasoning. Large-scale systems like LLaMA operationalized this practice by concatenating heterogeneous documents separated by EOS tokens for efficient utilization of long contexts, establishing the de facto pretraining data ordering. In parallel, retrieval-augmented approaches such as REALM revealed that conditioning predictions on semantically related documents during pretraining substantially improves knowledge-intensive performance. RETRO further amplified this insight, showing large gains by attending to retrieved neighbors at scale, though at the expense of maintaining a retriever and incurring inference-time cross-attention costs. kNN-LM similarly proved the utility of local nearest-neighbor contexts while exposing reliance on external datastores. ATLAS extended retrieval-augmented pretraining to few-shot settings, confirming that retrieval-aware training boosts in-context generalization but still ties models to a retriever at inference. MetaICL provided a complementary insight: arranging training inputs as sequences of related examples teaches models to read and leverage in-context information explicitly. These strands collectively surfaced a clear opportunity: the benefits of related context could be realized without inference-time retrieval by rethinking the training data order. By replacing random document concatenation with sequences of semantically related documents, one can expose models to multi-document context during pretraining, preserving standard pipelines while internalizing cross-document reasoning capabilities that prior retrieval methods supplied externally.",
  "target_paper": {
    "title": "In-Context Pretraining: Language Modeling Beyond Document Boundaries",
    "authors": "Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Wen-tau Yih, Mike Lewis",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Large Language Models",
    "abstract": "Language models are currently trained to predict tokens given document prefixes, enabling them to zero shot long form generation and prompting-style tasks which can be reduced to document completion. We instead present IN-CONTEXT PRETRAINING, a new approach where language models are trained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. Our approach builds on the fact that current pipelines train by concatenating random sets of shorter documents to create longer context windows; this improves efficiency even though the prior documents provide no signal for predicting the next document. Given this fact, we can do IN-CONTEXT PRETRAINING by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual simi",
    "openreview_id": "LXVswInHOo",
    "forum_id": "LXVswInHOo"
  },
  "analysis_timestamp": "2026-01-06T07:35:21.019897"
}
{
  "prior_works": [
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
      "year": 2020,
      "role": "Foundational diffusion framework",
      "relationship_sentence": "Established the stepwise reverse-diffusion process whose intermediate activations this paper probes, enabling analysis of how semantic structure emerges across timesteps."
    },
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
      "year": 2022,
      "role": "Base text-to-image architecture",
      "relationship_sentence": "Provided the widely used Stable Diffusion/latent U-Net backbone with cross-attention that the authors instrument with sparse autoencoders to discover interpretable concept features."
    },
    {
      "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
      "authors": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba",
      "year": 2017,
      "role": "Concept-level interpretability methodology",
      "relationship_sentence": "Introduced the paradigm of mapping internal units to human concepts, directly inspiring the paper\u2019s focus on discovering and validating human-interpretable concepts within diffusion activations."
    },
    {
      "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks",
      "authors": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba",
      "year": 2019,
      "role": "Interpretability for generative models",
      "relationship_sentence": "Demonstrated concept discovery and causal intervention in generative networks, motivating analogous concept discovery and steering in diffusion models via SAE features."
    },
    {
      "title": "Toy Models of Superposition",
      "authors": "Nelson Elhage, Neel Nanda, Catherine Olsson, Nicholas Joseph, Nova DasSarma, Tom Henighan, Chris Olah",
      "year": 2022,
      "role": "Theoretical basis for feature superposition",
      "relationship_sentence": "Explained why neural features superpose and why sparse dictionary learning can recover monosemantic features, underpinning the paper\u2019s choice of sparse autoencoders for disentangling diffusion activations."
    },
    {
      "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 using Sparse Autoencoders",
      "authors": "Anthropic Interpretability Team",
      "year": 2024,
      "role": "Scalable SAE methodology and practice",
      "relationship_sentence": "Provided training recipes and evaluation protocols showing SAEs can yield robust, steerable features at scale, which this paper adapts to the diffusion U-Net setting."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole",
      "year": 2021,
      "role": "Continuous-time diffusion theory",
      "relationship_sentence": "Offered a time-continuous view of diffusion dynamics that supports analyzing when semantic information arises, bolstering the paper\u2019s finding that scene composition is predictable very early in the reverse process."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014using sparse autoencoders (SAEs) to uncover human-interpretable, causally meaningful concepts inside text-to-image diffusion models and showing that scene composition is predictable before the first reverse step completes\u2014stands at the intersection of diffusion modeling and mechanistic interpretability. Foundationally, Ho et al.\u2019s DDPM and Song et al.\u2019s SDE formulation define the multi-step generative process and its temporal semantics, enabling a principled analysis of how information emerges over time. Rombach et al.\u2019s Latent Diffusion Models supply the practical, cross-attention\u2013conditioned U-Net backbone (e.g., Stable Diffusion) whose internal activations the authors probe.\n\nOn the interpretability side, Network Dissection and GAN Dissection established that generative networks contain units aligned with human concepts and that intervening on them can steer outputs\u2014key precedents for seeking concept-level structure and control within diffusion activations. The theoretical justification for recovering such structure via SAEs comes from Elhage et al.\u2019s Toy Models of Superposition, which explains why features are entangled in dense representations and how sparse dictionary learning can tease apart monosemantic features. Building on that theory, Anthropic\u2019s Scaling Monosemanticity provides scalable SAE training and evaluation practices, demonstrating robust, steerable features in large language models; the present paper adapts these techniques to the diffusion U-Net, revealing spatially localized, interpretable features and showing they forecast global scene composition at the very start of reverse diffusion. Together, these works directly enable the paper\u2019s methodological choice (SAEs), target domain (latent diffusion U-Nets), and headline finding (early emergence of interpretable, predictive concepts).",
  "analysis_timestamp": "2026-01-06T23:42:48.121444"
}
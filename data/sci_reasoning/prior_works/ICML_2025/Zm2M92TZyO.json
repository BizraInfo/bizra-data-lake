{
  "prior_works": [
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": "Robin Rombach et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "AGDiff directly adapts the latent diffusion paradigm\u2014performing diffusion in an encoder\u2019s latent space\u2014to perturb graph representations and synthesize pseudo-anomalous graphs that remain close to normal ones."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "The diffusion training and sampling mechanics (forward noising and reverse denoising) from DDPM underpin AGDiff\u2019s controlled perturbation process for generating pseudo anomalies."
    },
    {
      "title": "Deep One-Class Classification",
      "authors": "Lukas Ruff et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Deep SVDD exemplifies the prevailing normality-modeling approach that AGDiff surpasses by learning with explicit pseudo-anomalous graphs to obtain a more discriminative decision boundary."
    },
    {
      "title": "Deep Anomaly Detection with Outlier Exposure",
      "authors": "Dan Hendrycks et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Outlier Exposure shows the benefit of training with anomalies but relies on external OOD data; AGDiff removes this dependency by generating in-domain pseudo anomalies via latent diffusion."
    },
    {
      "title": "Adversarially Learned One-Class Classifier for Novelty Detection",
      "authors": "Mohammad Sabokrou et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "ALOCC demonstrated that synthesizing pseudo-negative samples to jointly train a discriminator improves novelty detection; AGDiff replaces GAN-based negatives with diffusion-generated pseudo-anomalous graphs in the graph domain."
    },
    {
      "title": "DRAEM: A Discriminatively Trained Reconstruction Embedding for Surface Anomaly Detection",
      "authors": "Ziga Zavrtanik et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "DRAEM showed that training on synthetic anomalies that closely resemble normal data yields strong detectors; AGDiff generalizes this idea by using latent diffusion to produce subtle, near-normal pseudo anomalies for GLAD."
    },
    {
      "title": "Virtual Adversarial Training: A Regularization Method for Supervised and Semi-supervised Learning",
      "authors": "Takeru Miyato et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "VAT\u2019s principle of small, targeted perturbations to create challenging near-boundary examples informs AGDiff\u2019s use of controlled latent perturbations to elicit discriminative learning."
    }
  ],
  "synthesis_narrative": "AGDiff\u2019s core idea\u2014explicitly generating pseudo-anomalous graphs that stay close to normal data and training a classifier on them\u2014emerges from three converging lines of work. First, diffusion modeling provides the generative backbone. DDPM established the forward\u2013reverse noising framework, and Latent Diffusion Models (LDM) showed how performing diffusion in a compact latent space yields efficient, controllable synthesis. AGDiff directly extends LDM by operating diffusion on graph representations produced by a GNN, enabling subtle, structured perturbations that preserve graph semantics while inducing anomalous cues.\nSecond, the paper draws on the insight that discriminative boundaries improve when models are exposed to anomalous examples during training. Outlier Exposure formalized this but depends on external OOD data; AGDiff addresses that gap by internally generating pseudo anomalies. Earlier one-class methods like Deep SVDD typify normality-only training; AGDiff surpasses this by learning from explicit negatives. Complementary inspiration comes from ALOCC, which used synthetic negatives via GANs, and from DRAEM, which crafted near-normal synthetic anomalies to train stronger detectors\u2014AGDiff replaces hand-crafted or GAN-based synthesis with principled latent diffusion tailored to graphs.\nThird, the choice to apply small, controlled perturbations aligns with Virtual Adversarial Training\u2019s philosophy of creating near-boundary examples to sharpen decision surfaces. Together, these works directly shape AGDiff\u2019s innovation: latent diffusion\u2013based pseudo-anomaly generation in graph space, coupled with joint discriminative training for graph-level anomaly detection.",
  "analysis_timestamp": "2026-01-06T23:07:19.631725"
}
{
  "prior_works": [
    {
      "title": "Scaling Vision Transformers to 22 Billion Parameters",
      "authors": "Mostafa Dehghani et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work identified the large-scale instability of attention logit growth and proposed practical mitigations (e.g., QK normalization/soft-capping), which the current paper explicitly targets to reproduce at small scale and validate via high\u2013learning-rate proxies."
    },
    {
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "authors": "Aakanksha Chowdhery et al.",
      "year": 2022,
      "arxiv_id": "2204.02311",
      "role": "Foundation",
      "relationship_sentence": "PaLM documented divergence between output logits and log probabilities at scale and popularized using z-loss to stabilize training, a failure mode and mitigation the current paper reproduces and tests in small models with high learning rates."
    },
    {
      "title": "The Z-Loss: A Shift and Scale Invariant Classification Loss",
      "authors": "Alexandre de Br\u00e9bisson et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper introduced the z-loss regularizer that directly mitigates the logit\u2013log-prob divergence phenomenon, enabling the stabilization technique the current work evaluates within its small-scale instability proxies."
    },
    {
      "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
      "authors": "Greg Yang et al.",
      "year": 2022,
      "arxiv_id": "2203.03466",
      "role": "Inspiration",
      "relationship_sentence": "\u03bcP provided the principle that certain hyperparameter behaviors (notably learning rate) transfer predictably across scale, motivating the paper\u2019s focus on learning-rate\u2013loss curves as a small-scale proxy for large-scale instabilities."
    },
    {
      "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization",
      "authors": "Benno Kroth et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "NormFormer showed that additional normalization on attention (including normalizing Q/K projections) reduces attention logit magnitudes, a mitigation class the paper evaluates and finds effective in the small-scale, high\u2013learning-rate regime."
    },
    {
      "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "authors": "William Fedus et al.",
      "year": 2021,
      "arxiv_id": "2101.03961",
      "role": "Gap Identification",
      "relationship_sentence": "Switch Transformers reported large-scale training instabilities and reliance on z-loss for stability, underscoring the need for resource-efficient ways to study such failures that the current paper addresses via small-scale proxies."
    }
  ],
  "synthesis_narrative": "Large-scale Vision Transformer training revealed that attention logits can grow during optimization, saturating softmax and destabilizing learning; Dehghani et al. documented this failure mode and demonstrated that interventions like query\u2013key normalization or soft-capping the logits prevent entropy collapse. In massively scaled language models, Chowdhery et al. observed another pathology: a widening gap between pre-softmax logits and their log probabilities, leading to unstable training unless an auxiliary penalty is applied. The z-loss, first introduced by de Br\u00e9bisson and Vincent as a shift- and scale-invariant regularizer on the log partition function, became a practical fix for this divergence and was adopted in large-model training. Complementing these observations, NormFormer showed that injecting extra normalization around attention projections curbs logit magnitudes and improves stability. More broadly, Switch Transformers highlighted the fragility of trillion-parameter training and the pragmatic need for stabilizers like z-loss. Finally, \u03bcP established that certain hyperparameter relationships\u2014especially learning-rate behavior\u2014transfer across model sizes, suggesting a path to study scale phenomena with smaller models.\nTogether, these works suggested a clear opportunity: if instability mechanisms are tied to logit scaling and overconfidence, and if learning-rate behavior transfers across scale, then small models trained at aggressive learning rates could act as faithful stand-ins for large-scale failures. Building on the exact failure modes and fixes (attention-logit growth and z-lossable logit\u2013prob divergence), the paper systematizes learning-rate\u2013loss sweeps across sizes and shows that the same mitigations succeed in small-scale proxies, providing a practical, low-cost testbed for diagnosing and preventing large-model instabilities.",
  "target_paper": {
    "title": "Small-scale proxies for large-scale Transformer training instabilities",
    "authors": "Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie E Everett, Alexander A Alemi, Ben Adlam, John D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, Simon Kornblith",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Small Transformers, Training, Stability",
    "abstract": "Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, we seek ways to reproduce and study training instability at smaller scales. First, we focus on two sources of training instability described in previous work: the growth of logits in attention layers (Dehghani et al., 2023) and divergence of the output logits from the log probabilities (Chowdhery et al., 2022). By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. This prompts us to investigate the extent to which other k",
    "openreview_id": "d8w0pmvXbZ",
    "forum_id": "d8w0pmvXbZ"
  },
  "analysis_timestamp": "2026-01-06T15:00:44.641978"
}
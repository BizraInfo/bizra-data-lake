{
  "prior_works": [
    {
      "title": "Understanding image representations by measuring their equivariance and equivalence",
      "authors": "Karel Lenc, Andrea Vedaldi",
      "year": 2015,
      "role": "Methodological precursor (functional alignment via learned adapters)",
      "relationship_sentence": "Introduced learning simple mappings between feature spaces to test representational equivalence, effectively seeding the model-stitching paradigm that this paper scrutinizes."
    },
    {
      "title": "How transferable are features in deep neural networks?",
      "authors": "Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson",
      "year": 2014,
      "role": "Empirical foundation for cross-network grafting",
      "relationship_sentence": "Demonstrated that early and late network components can be swapped and still function with limited adaptation, motivating the notion that subnetworks can be grafted\u2014i.e., stitched\u2014across models."
    },
    {
      "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability",
      "authors": "Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",
      "year": 2017,
      "role": "Foundational representational similarity metric",
      "relationship_sentence": "Established a widely used framework for comparing internal representations, helping cement the community belief that representational alignment implies informational similarity\u2014a belief this paper challenges in the functional setting."
    },
    {
      "title": "Similarity of Neural Network Representations Revisited",
      "authors": "Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton",
      "year": 2019,
      "role": "Stronger baseline for representation comparison (CKA)",
      "relationship_sentence": "Popularized CKA as a robust measure of representational similarity, shaping the prevailing inference that similar (or alignable) representations reflect similar information, which the present work shows can be misleading for functional alignment."
    },
    {
      "title": "Convergent Learning: Do different neural networks learn the same representations?",
      "authors": "Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, John E. Hopcroft",
      "year": 2016,
      "role": "Empirical claim of alignment across independently trained nets",
      "relationship_sentence": "Showed that independently trained networks can be aligned via neuron matching/linear maps, directly informing the idea that simple transformations can functionally reconcile distinct representations\u2014precisely what this paper interrogates."
    },
    {
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias in CNNs improves accuracy and robustness",
      "authors": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, Wieland Brendel",
      "year": 2019,
      "role": "Empirical basis on divergent inductive biases",
      "relationship_sentence": "Established that high-performing vision models can rely on very different biases (texture vs. shape), a key setup the current paper uses to show such differently biased models can still be stitched."
    },
    {
      "title": "Understanding deep learning requires rethinking generalization",
      "authors": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals",
      "year": 2017,
      "role": "Capacity and memorization underpinning stitched alignment to noise",
      "relationship_sentence": "Showed that deep networks can fit random labels/noise, supporting this paper\u2019s finding that even representations of clustered random noise can be stitched into trained models while preserving downstream performance."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central contribution\u2014demonstrating that functional alignment via model stitching can be highly misleading about informational similarity\u2014emerges from two converging lines of prior work. First, methodological and empirical precedents for stitching: Lenc and Vedaldi\u2019s linear mappings to test representation equivalence and Yosinski et al.\u2019s layer-swapping studies established that subnetworks can be grafted across models with minimal adaptation. Li et al. extended this with evidence that independently trained networks can be aligned via simple transformations, reinforcing the intuition that functional compatibility implies similar internal content. Second, the representational similarity literature (SVCCA; CKA) normalized the practice of comparing internal spaces and fostered a community prior that alignment signals shared information. Against this backdrop, Geirhos et al. showed that equally accurate models can embody starkly different inductive biases (texture vs. shape), and Zhang et al. revealed that deep nets can memorize arbitrary noise. The present paper synthesizes these threads: it adopts the stitching mechanism to align systems that, by design, should encode different information (different biases, tasks, modalities, and even clustered noise) and shows that functional alignment can still succeed. This directly undermines the prevalent inference\u2014from both geometric and functional alignment\u2014that similar performance under simple adapters implies informational equivalence, and it motivates evaluation tools that track information content rather than mere alignability.",
  "analysis_timestamp": "2026-01-07T00:21:32.382880"
}
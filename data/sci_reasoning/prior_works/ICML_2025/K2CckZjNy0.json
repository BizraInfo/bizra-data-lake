{
  "prior_works": [
    {
      "title": "Toy Models of Superposition",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "role": "Theoretical foundation for SAEs and feature sparsity",
      "relationship_sentence": "AxBench directly evaluates SAE-based interpretability/steering claims that are motivated by the superposition picture introduced in this work."
    },
    {
      "title": "Scaling Monosemanticity: Extracting Interpretable Features from LMs with Sparse Autoencoders",
      "authors": "Catherine Olsson et al.",
      "year": 2024,
      "role": "Primary SAE methodology and claims",
      "relationship_sentence": "AxBench includes SAEs as a core comparator and tests, at scale, whether SAE-derived features support steering and concept detection as suggested by this line of work."
    },
    {
      "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
      "authors": "Siddharth Dathathri et al.",
      "year": 2020,
      "role": "Representation-based steering baseline",
      "relationship_sentence": "PPLM established activation-space control of generation without finetuning, a paradigm AxBench systematizes and benchmarks against prompting, finetuning, and other rep-based methods."
    },
    {
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "authors": "Been Kim et al.",
      "year": 2018,
      "role": "Concept-direction methodology",
      "relationship_sentence": "AxBench\u2019s concept detection and difference-in-means steering vectors build on the CAV idea of linear concept directions in internal representations."
    },
    {
      "title": "Understanding Intermediate Layers Using Linear Classifier Probes",
      "authors": "Guillaume Alain and Yoshua Bengio",
      "year": 2016,
      "role": "Linear probing baseline for representations",
      "relationship_sentence": "AxBench evaluates linear probes for concept detection as a strong, simple baseline derived from this probing framework."
    },
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu et al.",
      "year": 2022,
      "role": "Low-rank parameterization for efficient adaptation",
      "relationship_sentence": "The paper\u2019s ReFT-r1 method is conceptually aligned with LoRA\u2019s insight that low-rank updates can be highly effective, here instantiated as rank-1 updates in representation-level finetuning."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME)",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "role": "Rank-one model editing",
      "relationship_sentence": "ROME demonstrates that targeted rank-one updates can localize behavioral edits, directly inspiring AxBench\u2019s rank-1 representation finetuning (ReFT-r1) design."
    }
  ],
  "synthesis_narrative": "AxBench\u2019s central contribution is to establish a rigorous, model- and task-spanning benchmark for steering and concept detection while introducing a simple but effective rank-1 representation finetuning method (ReFT-r1). Two lines of prior work directly shaped this agenda. First, the mechanistic interpretability push toward sparsity and monosemantic features\u2014initiated by Toy Models of Superposition and advanced by Anthropic\u2019s SAE program\u2014claims that decomposed features can be used both to identify and to steer high-level concepts. AxBench explicitly stress-tests these claims across models and tasks, finding SAEs comparatively weak for steering while simple statistical baselines excel at concept detection. Second, the activation-space control literature (PPLM; TCAV; linear probes) provided concrete, general-purpose procedures for extracting linear concept directions and manipulating activations, which AxBench unifies and evaluates head-to-head against prompting and finetuning. On the optimization side, the effectiveness of low-rank interventions (LoRA) and precise rank-one edits (ROME) suggested that extremely low-rank updates can be sufficient for meaningful behavior change. AxBench operationalizes this insight at the representation level with ReFT-r1, a weakly supervised, rank-1 update that competes with more complex methods. By situating SAE-based claims alongside robust prompting/finetuning and linear baselines, and by leveraging low-rank insights for representation finetuning, AxBench clarifies which techniques actually deliver reliable steering and which are best suited for concept detection.",
  "analysis_timestamp": "2026-01-07T00:27:38.145247"
}
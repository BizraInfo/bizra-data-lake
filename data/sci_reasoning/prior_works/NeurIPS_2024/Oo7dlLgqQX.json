{
  "prior_works": [
    {
      "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
      "authors": [
        "Lisa Argyle",
        "Ethan C. Busby",
        "Joshua Gubler",
        "Christopher Rytting",
        "David Wingate"
      ],
      "year": 2023,
      "role": "Methodological precedent in using LLMs as survey respondents",
      "relationship_sentence": "This paper popularized treating LLMs as survey participants to infer human-like demographics and opinions, providing the exact practice and prompting conventions that the NeurIPS 2024 paper scrutinizes and empirically challenges."
    },
    {
      "title": "The Political Biases of ChatGPT",
      "authors": [
        "David Rozado"
      ],
      "year": 2023,
      "role": "Empirical precedent claiming ideological leanings via survey-style instruments",
      "relationship_sentence": "By assigning political ideologies to LLMs using standardized questionnaires, this work exemplifies the inference the NeurIPS paper argues can be spurious due to option-label and ordering artifacts."
    },
    {
      "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
      "authors": [
        "Tony Zhao",
        "Tianyi Wang",
        "Mark Yatskar",
        "Vicente Ordonez",
        "Kai-Wei Chang"
      ],
      "year": 2021,
      "role": "Identified and corrected label/option bias in prompted classification",
      "relationship_sentence": "This study demonstrated inherent label and position biases in LMs and proposed calibration, directly foreshadowing the NeurIPS paper\u2019s finding that survey responses are driven by labeling/ordering rather than latent preferences."
    },
    {
      "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
      "authors": [
        "Sewon Min",
        "Mike Lewis",
        "Hannaneh Hajishirzi",
        "Luke Zettlemoyer"
      ],
      "year": 2022,
      "role": "Showed sensitivity to surface cues, label mappings, and order in ICL",
      "relationship_sentence": "By revealing that surface forms and ordering in prompts heavily influence LM outputs, this work motivates the NeurIPS paper\u2019s randomized-answer design and its interpretation of near-uniform response distributions."
    },
    {
      "title": "True Few-Shot Learning with Language Models",
      "authors": [
        "Ethan Perez",
        "Douwe Kiela",
        "Kyunghyun Cho"
      ],
      "year": 2021,
      "role": "Established evaluation variance from prompt and option order",
      "relationship_sentence": "This paper documented large performance swings from prompt and choice ordering, a core mechanism the NeurIPS paper systematically exposes in the survey-response setting."
    },
    {
      "title": "Questions and Answers in Attitude Surveys: Experiments on Question Form, Wording, and Context",
      "authors": [
        "Howard Schuman",
        "Stanley Presser"
      ],
      "year": 1981,
      "role": "Foundational survey methodology on order/wording effects",
      "relationship_sentence": "Classic evidence that human survey responses are sensitive to wording and order informs the NeurIPS paper\u2019s application of randomized answer ordering as a methodological control for LLM surveys."
    },
    {
      "title": "Measuring Massive Multitask Language Understanding",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Saurabh Basart",
        "Jerry Li",
        "Dawn Song",
        "Jacob Steinhardt"
      ],
      "year": 2021,
      "role": "Established letter-labeled multiple-choice evaluation conventions",
      "relationship_sentence": "The widespread use of letter-labeled multiple-choice formats typified by MMLU contextualizes the NeurIPS paper\u2019s discovery of systematic letter biases (e.g., toward option \u201cA\u201d) in LLM responses."
    }
  ],
  "synthesis_narrative": "The paper challenges the growing practice of inferring LLM demographics, values, and ideology from survey-style prompts. That practice was catalyzed by Argyle et al.\u2019s use of LLMs as simulated survey populations and reinforced by studies like Rozado\u2019s that mapped models onto political scales. The present work interrogates the validity of those inferences by drawing on two strands of prior evidence about response sensitivity. First, classic survey methodology (Schuman & Presser) documents strong order and wording effects in human respondents, suggesting that careful randomization and controls are necessary. Second, the LLM prompting literature has repeatedly uncovered option and order sensitivity: Perez et al. showed large variance from prompt/choice ordering; Zhao et al. identified label/position biases and proposed calibration; and Min et al. demonstrated that superficial label forms and ordering can drive in-context learning behavior. These insights motivate the paper\u2019s core methodological move\u2014randomizing answer ordering and examining label effects\u2014and its central finding that, once such artifacts are controlled, many LLMs\u2019 survey responses collapse toward uniform randomness rather than stable preferences. Finally, the pervasiveness of letter-labeled multiple-choice formats in LM evaluation (e.g., MMLU) contextualizes the observed systematic biases toward particular option labels (such as \u201cA\u201d). Together, these works directly inform the paper\u2019s critique: much of the apparent alignment between LLM survey responses and specific human subgroups arises from prompt-induced artifacts, not from genuine latent attitudes.",
  "analysis_timestamp": "2026-01-06T23:39:42.951252"
}
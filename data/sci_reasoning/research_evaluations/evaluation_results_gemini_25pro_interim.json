{
  "summary": {
    "model": "gemini-2.5-pro",
    "k": 10,
    "crawl_method": "exa_ai",
    "total_papers": 70,
    "hits": 36,
    "hit_rate_percent": 51.43,
    "average_crawl_rate": 100.0,
    "runtime_minutes": 111.81,
    "cost": {
      "input_tokens": 738045,
      "output_tokens": 574903,
      "input_cost_usd": 0.92,
      "output_cost_usd": 5.75,
      "total_cost_usd": 6.67
    },
    "timestamp": "2026-01-05T04:50:41.312113"
  },
  "results": [
    {
      "paper_idx": 0,
      "paper_title": "Generalized Linear Mode Connectivity for Transformers",
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "crawl_rate": 1.0,
      "input_tokens": 12304,
      "output_tokens": 10985,
      "ideas_generated": 10,
      "contribution": "They develop a unified, symmetry\u2011aware reparameterization framework (permutations, semi\u2011permutations, orthogonal transforms, and general invertible maps) that uncovers low\u2011 and zero\u2011barrier linear interpolation paths between independently trained Transformers (including Vision Transformers and GPT\u20112) and across architectures of differing widths.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": true
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": true
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 1,
      "paper_title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
      "hit_at_k": true,
      "matching_idea_idx": 4,
      "crawl_rate": 1.0,
      "input_tokens": 13468,
      "output_tokens": 12824,
      "ideas_generated": 10,
      "contribution": "Introduce a compositional phase-domain diffusion framework (with ACT-PAE, SPDM and TPDM) that denoises semantic and transition-aware phase latents so long multi-segment motion sequences are both semantically aligned and smoothly transitioned.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": true
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": true
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 2,
      "paper_title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "crawl_rate": 1.0,
      "input_tokens": 12232,
      "output_tokens": 11998,
      "ideas_generated": 10,
      "contribution": "Introduce an exemplar-based global GNN explainer that selects representative nodes in embedding space via a coverage-maximization over reverse k-nearest neighbors and converts their neighborhoods into concise natural-language rules using an LLM self-refinement prompting strategy, yielding scalable, high-fidelity, and human-interpretable class-level explanations.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": true
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 3,
      "paper_title": "RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation",
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "crawl_rate": 1.0,
      "input_tokens": 10519,
      "output_tokens": 11771,
      "ideas_generated": 10,
      "contribution": "Introduce RAG4GFM, an end-to-end retrieval-augmented generation framework that adapts the RAG paradigm to graph corpora via hierarchical multi-level graph indexing, task-aware retrieval, and graph-fusion enhancement to enable fast knowledge updating and more faithful reasoning for Graph Foundation Models.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": true
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": true
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 4,
      "paper_title": "Agnostic Active Learning Is Always Better Than Passive Learning",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 7648,
      "output_tokens": 5215,
      "ideas_generated": 1,
      "contribution": "A new agnostic active learning algorithm and analysis that give a sharp, instance-independent first-order query complexity for all concept classes whose leading term is always strictly smaller than passive sample complexity, eliminating disagreement-coefficient-type factors from the leading term.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 5,
      "paper_title": "Learning Linear Attention in Polynomial Time",
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "crawl_rate": 1.0,
      "input_tokens": 12259,
      "output_tokens": 10489,
      "ideas_generated": 10,
      "contribution": "Shows that multi\u2011head linear attention can be learned in polynomial time by recasting the model as learning a rank\u2011H kernel predictor in an RKHS, and provides an algorithm that both finds near\u2011optimal MHLA parameters and certifies when all empirical best\u2011fits implement the same computation.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": true
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 6,
      "paper_title": "Optimal Mistake Bounds for Transductive Online Learning",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 10260,
      "output_tokens": 11700,
      "ideas_generated": 10,
      "contribution": "Shows that the transductive online mistake bound is \u0398(\u221ad) (giving an \u2126(\u221ad) lower bound and a matching O(\u221ad) upper bound), thereby establishing a quadratic separation from the standard online bound of \u0398(d).",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 7,
      "paper_title": "State Entropy Regularization for Robust Reinforcement Learning",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 9949,
      "output_tokens": 6126,
      "ideas_generated": 2,
      "contribution": "Shows that regularizing the entropy of the state-visitation distribution yields provable robustness to structured and spatially correlated perturbations (under reward and transition uncertainty), contrasts these guarantees with policy-entropy regularization, and analyzes practical sensitivities such as number of rollouts.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 8,
      "paper_title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 8105,
      "output_tokens": 4260,
      "ideas_generated": 1,
      "contribution": "Shows that the stochasticity of conditional targets is not the primary driver of generalization in flow matching: closed-form velocity targets match (and sometimes improve) performance, and generalization instead arises from the neural network's failure to perfectly approximate the optimal closed-form velocity field in particular time intervals.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 9,
      "paper_title": "Why Diffusion Models Don\u2019t Memorize:  The Role of Implicit Dynamical Regularization in Training",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 12887,
      "output_tokens": 11797,
      "ideas_generated": 10,
      "contribution": "The paper shows that training dynamics impose an implicit dynamical regularization in diffusion models: there are two distinct timescales (\u03c4gen and \u03c4mem) so that models generalize for a wide, growing window of training times (\u03c4 \u2208 [\u03c4gen, \u03c4mem]) because \u03c4mem scales linearly with dataset size n while \u03c4gen remains constant, explaining why memorization is avoided in practice and giving a tractable random-features theory that matches experiments.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 10,
      "paper_title": "Adjoint Schr\u00f6dinger Bridge Sampler",
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "crawl_rate": 1.0,
      "input_tokens": 12385,
      "output_tokens": 11953,
      "ideas_generated": 10,
      "contribution": "Combines Schr\u00f6dinger-bridge stochastic optimal control with adjoint matching to learn scalable, importance-weight-free diffusion samplers that transport arbitrary source distributions to unnormalized energy-defined targets.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": true
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": true
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 11,
      "paper_title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies",
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "crawl_rate": 1.0,
      "input_tokens": 10018,
      "output_tokens": 11469,
      "ideas_generated": 10,
      "contribution": "Demonstrates that adding an explicit, compute-aware inference phase (using search/optimization strategies such as tree search, sampling and adaptation) on top of trained RL policies substantially breaks zero-shot performance ceilings in complex multi-agent and combinatorial tasks, yielding large empirical gains with modest extra wall-clock time.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": true
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": true
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 12,
      "paper_title": "High-Dimensional Calibration from Swap Regret",
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "crawl_rate": 1.0,
      "input_tokens": 12768,
      "output_tokens": 12743,
      "ideas_generated": 10,
      "contribution": "Shows that multi-dimensional online calibration over any convex P and norm ||\u00b7|| reduces to a swap-regret control implied by optimal regularizers for online linear optimization, and uses TreeSwap+FTL to obtain efficient high-dimensional calibration rates (T = exp(O(\u03c1/\u03b5^2))) recovering and generalizing prior polynomial-in-d bounds without requiring OLO subroutines or knowledge of \u03c1.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": true
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": true
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 13,
      "paper_title": "In Search of Adam\u2019s Secret Sauce",
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "crawl_rate": 1.0,
      "input_tokens": 13456,
      "output_tokens": 10980,
      "ideas_generated": 10,
      "contribution": "Through a large empirical study and a focused theoretical simplification (\u03b21 = \u03b22), the paper shows that Adam\u2019s empirical advantage over signed/momentum methods largely stems from its coupled mean/variance estimation \u2014 giving a near\u2011optimal, interpretable optimizer that can be seen as an online mean/variance estimator arising from a mean\u2011field Gaussian variational inference view.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": true
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 14,
      "paper_title": "An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 11820,
      "output_tokens": 5492,
      "ideas_generated": 3,
      "contribution": "They refine the Franz\u2013Parisi (FP) geometric criterion to better capture overlap structure and prove that this optimized FP is equivalent to Statistical Query (SQ) lower bounds under a mild, verifiable assumption, thereby unifying physics-inspired geometry with SQ complexity for a broad class of statistical models.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 15,
      "paper_title": "MaxSup: Overcoming Representation Collapse in Label Smoothing",
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "crawl_rate": 1.0,
      "input_tokens": 10470,
      "output_tokens": 6216,
      "ideas_generated": 3,
      "contribution": "A theoretical decomposition of label smoothing that exposes an error-amplification term, and a simple logit-level regularizer (Max Suppression) that penalizes the top-1 logit to retain LS\u2019s benefits while avoiding overconfident misclassifications and representation collapse.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 16,
      "paper_title": "Memory Mosaics at scale",
      "hit_at_k": true,
      "matching_idea_idx": 4,
      "crawl_rate": 1.0,
      "input_tokens": 12495,
      "output_tokens": 11054,
      "ideas_generated": 10,
      "contribution": "Scaled and redesigned networks of associative key\u2013value memories (Memory Mosaics v2) that match transformers on training\u2011knowledge storage while substantially improving new\u2011task and in\u2011context learning at large scale.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": true
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": true
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 17,
      "paper_title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "crawl_rate": 1.0,
      "input_tokens": 14501,
      "output_tokens": 11533,
      "ideas_generated": 10,
      "contribution": "Shows that sparse-attention circuits emerge as predictable phase-transitions in training dynamics driven by task structure, optimizer/architecture choices, and data distribution\u2014and that repeating examples can dramatically accelerate this emergence.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": true
        },
        {
          "idea_idx": 2,
          "is_match": true
        },
        {
          "idea_idx": 3,
          "is_match": true
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": true
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 18,
      "paper_title": "ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 13915,
      "output_tokens": 12572,
      "ideas_generated": 10,
      "contribution": "Introduce a prompt-modulated restoration-and-fusion network trained on physically simulated composite degradations that uses language-vision degradation prompts plus a spatial-frequency visual adapter to produce controllable, degradation-robust infrared-visible image fusion.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 19,
      "paper_title": "Identifiability of Deep Polynomial Neural Networks",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 12150,
      "output_tokens": 4151,
      "ideas_generated": 1,
      "contribution": "Provides a comprehensive, constructive characterization of when deep polynomial neural networks are (finitely and/or globally) identifiable by reducing identifiability to low-rank polynomial/tensor decomposition uniqueness and settling open dimension and degree-threshold conjectures for neurovarieties.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 20,
      "paper_title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 10802,
      "output_tokens": 4835,
      "ideas_generated": 1,
      "contribution": "A systematic diagnosis showing that GPU/kernel-level floating-point non\u2011associativity and reduction ordering produce large, reproducibility\u2011breaking output differences in LLM inference, and a lightweight inference pipeline to mitigate these numerical sources of nondeterminism.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 21,
      "paper_title": "PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models",
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "crawl_rate": 1.0,
      "input_tokens": 11060,
      "output_tokens": 12931,
      "ideas_generated": 10,
      "contribution": "PRIMT reduces human labeling and improves reward learning in preference-based RL by using a hierarchical fusion of multimodal foundation models for synthetic feedback together with foresight and hindsight trajectory synthesis (including SCM-based counterfactuals) to reduce query ambiguity and improve credit assignment.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": true
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 22,
      "paper_title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 11124,
      "output_tokens": 11078,
      "ideas_generated": 10,
      "contribution": "Identifies and formalizes 'feature absorption'\u2014a systematic failure mode of Sparse Autoencoders (SAEs) where seemingly monosemantic latents are suppressed by their hierarchical children under sparsity pressure\u2014introduces a metric to detect it, and empirically shows it is pervasive and not remedied by simple SAE size or sparsity tuning.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 23,
      "paper_title": "EvoLM: In Search of Lost Language Model Training Dynamics",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 8878,
      "output_tokens": 4321,
      "ideas_generated": 1,
      "contribution": "EvoLM builds a transparent, end-to-end model suite and experimental pipeline (100+ 1B/4B decoder-only LMs trained from scratch on open data) to systematically trace training dynamics across pre-training, continued pre-training, supervised fine-tuning, and RL, revealing practical trade-offs (diminishing returns, forgetting, bridging roles of continued pre-training, and SFT/RL trade-offs) and releasing all models, data, and code for reproducible study.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 24,
      "paper_title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 9341,
      "output_tokens": 4465,
      "ideas_generated": 1,
      "contribution": "A theoretical and algorithmic treatment of gradient-based training on AIMC devices with general, asymmetric and nonlinear pulse-response functions, proving that residual-learning updates (a bilevel formulation) remove the implicit bias caused by asymmetric responses and recover convergence to true critical points while also handling limited response granularity.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 25,
      "paper_title": "Discovering Opinion Intervals from Conflicts in Signed Graphs",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 8761,
      "output_tokens": 5778,
      "ideas_generated": 2,
      "contribution": "Introduce and study the problem of recovering a small set of interpretable opinion intervals on a line that explain the positive/negative edges of a signed graph, prove hardness results, derive a polynomial-time approximation scheme by connecting the model to interval/indifference graphs and correlation clustering, and provide scalable heuristics with empirical validation.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        },
        {
          "idea_idx": 1,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 26,
      "paper_title": "A Clean Slate for Offline Reinforcement Learning",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 9929,
      "output_tokens": 4962,
      "ideas_generated": 2,
      "contribution": "They introduce a rigorous, budget-aware evaluation and a set of minimal single-file implementations, unify prior algorithmic choices into a single hyperparameterized family (Unifloral), and\u2014using that clean infrastructure\u2014develop two new algorithms (TD3-AWR and MoBRAC) that outperform prior baselines under transparent, quantified offline evaluation budgets.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 27,
      "paper_title": "Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy",
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "crawl_rate": 1.0,
      "input_tokens": 12332,
      "output_tokens": 11600,
      "ideas_generated": 10,
      "contribution": "They develop novel high-probability spectral-norm perturbation bounds for the top-p low-rank approximation of a symmetric matrix under arbitrary symmetric noise, using a new 'contour bootstrapping' complex-analytic technique, and apply these bounds to give strictly sharper utility guarantees for differentially private PCA (improvements up to a factor \u221an).",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": true
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 28,
      "paper_title": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 10165,
      "output_tokens": 4812,
      "ideas_generated": 1,
      "contribution": "By analyzing algorithm-dependent sample concentration and GP sample-path properties to refine information-gain estimates, the paper proves improved high-probability regret bounds for GP-UCB\u2014eO(\u221aT) under certain Mat\u00e9rn kernels and O(\u221a(T ln^2 T)) for the squared-exponential kernel.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 29,
      "paper_title": "Auto-Compressing Networks",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 11383,
      "output_tokens": 4972,
      "ideas_generated": 1,
      "contribution": "Introduce a novel architecture (ACN) that replaces short residual connections with long additive feedforward connections to the output, inducing an architectural auto-compression dynamic that concentrates useful information into earlier layers during training and yields compact, more robust representations without sacrificing accuracy.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 30,
      "paper_title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 9823,
      "output_tokens": 4861,
      "ideas_generated": 1,
      "contribution": "Introduce a multimodal-aware low-rank adaptation method (MokA) that decomposes adaptation into modality-specific unimodal compression and explicit cross-modal interaction, yielding efficient and effective fine-tuning for MLLMs.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 31,
      "paper_title": "Advancing Expert Specialization for Better MoE",
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "crawl_rate": 1.0,
      "input_tokens": 11101,
      "output_tokens": 10965,
      "ideas_generated": 10,
      "contribution": "Introduces complementary orthogonality and variance regularizers that, when added to standard MoE auxiliary balancing losses, reduce expert overlap and produce more discriminative routing and specialist experts\u2014improving downstream performance without architectural changes.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": true
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 32,
      "paper_title": "From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 11738,
      "output_tokens": 6298,
      "ideas_generated": 2,
      "contribution": "Provides a two-stage gradient-flow analysis of linearized Transformer attention training under small initialization, proving an initial escape-and-rowwise-condensation phase for value/output parameters followed by an active key/query-driven phase that produces asymptotic normalized rank collapse.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 33,
      "paper_title": "Large Language Diffusion Models",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 12951,
      "output_tokens": 11015,
      "ideas_generated": 10,
      "contribution": "Introduces LLaDA, a large-scale discrete diffusion language model trained under the same pretraining and SFT paradigms as modern LLMs, showing that diffusion-based approaches can match autoregressive LLMs on in-context learning, instruction following, and other tasks.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 34,
      "paper_title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 10822,
      "output_tokens": 4815,
      "ideas_generated": 1,
      "contribution": "A training-free, plug-and-play method (ALFAR) that maximizes the utility of retrieved contextual knowledge for MLLMs by (1) adaptively reallocating attention from visual to relevant context tokens (guided by query-context relevance) and (2) decoupling and adaptively weighting parametric and contextual signals at the output logits to resolve knowledge conflicts.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 35,
      "paper_title": "Interactive Cross-modal Learning for Text-3D Scene Retrieval",
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "crawl_rate": 1.0,
      "input_tokens": 9888,
      "output_tokens": 12156,
      "ideas_generated": 10,
      "contribution": "Introduce IDeal, an interactive Text-3D Scene Retrieval method that iteratively refines text\u20133D alignment with a questioner/answerer loop (IRR) and an Interaction Adaptation Tuning (IAT) strategy to fuse feature- and semantic-level signals and bridge domain gaps for improved re-ranking.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": true
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": true
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 36,
      "paper_title": "Rethinking Joint Maximum Mean Discrepancy for Visual Domain Adaptation",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 8936,
      "output_tokens": 5084,
      "ideas_generated": 1,
      "contribution": "They derive a concise, representer-theorem based form of JMMD that (1) unifies marginal/conditional/weighted distances as special cases via label kernels, (2) explains why JMMD can hurt feature discrimination through a graph-embedding view, and (3) repairs this by jointly optimizing JMMD with HSIC (JMMD-HSIC) to produce a tractable, discrimination-preserving adaptation loss.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 37,
      "paper_title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 6865,
      "output_tokens": 4705,
      "ideas_generated": 1,
      "contribution": "Introduce a lightweight, learnable look-up-table (LUT) framework (PGLUT, SDLUT, AOLUT) that replaces heavy CNN components to perform high-quality, extremely fast pan-sharpening capable of processing very large remote-sensing images on commodity GPUs/CPUs.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 38,
      "paper_title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "crawl_rate": 1.0,
      "input_tokens": 14942,
      "output_tokens": 12547,
      "ideas_generated": 10,
      "contribution": "Using dynamical mean field theory the authors show that, in the joint large-width and large-sample regime, training dynamics exhibits a separation of timescales that (i) produces slow growth of function complexity, (ii) yields an inductive bias toward low-complexity solutions determined by initialization, and (iii) dynamically decouples feature learning from overfitting \u2014 predicting nonmonotone test error and a late-time 'feature unlearning' regime.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": true
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": true
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 39,
      "paper_title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 14123,
      "output_tokens": 11172,
      "ideas_generated": 10,
      "contribution": "Demonstrates that dramatically increasing network depth (up to 1024 layers) in a self-supervised, goal-conditioned contrastive RL setup yields large quantitative gains and qualitatively new goal-reaching behaviors that shallower agents cannot discover.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 40,
      "paper_title": "Depth-Bounds for Neural Networks via the Braid Arrangement",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 9275,
      "output_tokens": 6893,
      "ideas_generated": 4,
      "contribution": "For ReLU (and related maxout) networks compatible with the braid fan, the paper proves a non-constant lower bound \u2126(log log d) on the number of hidden layers needed to compute the maximum of d numbers, gives a combinatorial proof that max of 5 numbers needs three hidden layers under the same compatibility assumption, and supplies a tighter constructive upper bound in the maxout setting (rank-3 followed by rank-2 suffices for max of 7).",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 41,
      "paper_title": "Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization",
      "hit_at_k": true,
      "matching_idea_idx": 4,
      "crawl_rate": 1.0,
      "input_tokens": 12282,
      "output_tokens": 10719,
      "ideas_generated": 10,
      "contribution": "Introduce a new CMI-style generalization bound that injects stochastic projection and lossy compression (quantization) into the CMI super-sample framework to obtain strictly tighter, non\u2011vacuous O(1/\u221an) guarantees on instances where prior MI/CMI bounds fail, and to argue that memorization is not necessary for good generalization.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": true
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": true
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 42,
      "paper_title": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning",
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "crawl_rate": 1.0,
      "input_tokens": 12781,
      "output_tokens": 11675,
      "ideas_generated": 10,
      "contribution": "Introduce a local data-attribution framework for online RL (PPO) using gradient-similarity-based influence from recent buffers, and leverage it to diagnose learning and to iteratively filter experiences (IIF) to speed and stabilize training.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": true
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 43,
      "paper_title": "High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 12946,
      "output_tokens": 12638,
      "ideas_generated": 10,
      "contribution": "Shows analytically and empirically that low-dimensional recurrent latent dynamics can produce high-dimensional observed neural activity (after neuronal nonlinearities), and introduces a provably interpretable latent\u2011variable method (NCE) to recover the latent dimensionality from recordings.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 44,
      "paper_title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 11072,
      "output_tokens": 5910,
      "ideas_generated": 2,
      "contribution": "The paper presents a novel training approach for Spiking Neural Networks that utilizes adaptive surrogate gradients and a guiding policy to enhance performance in sequential reinforcement learning tasks.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        },
        {
          "idea_idx": 1,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 45,
      "paper_title": "Class-wise Balancing Data Replay for Federated Class-Incremental Learning",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 8238,
      "output_tokens": 5160,
      "ideas_generated": 1,
      "contribution": "Introduce FedCBDR, a privacy-preserving, global-perspective replay pipeline that (1) reconstructs class-level pseudo features for coordinated, class-balanced exemplar sampling across heterogeneous clients and (2) applies task-aware temperature scaling to mitigate class imbalance and overconfidence between replayed and new classes.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 46,
      "paper_title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 10257,
      "output_tokens": 5381,
      "ideas_generated": 1,
      "contribution": "The authors show that convolutional recurrent encoders trained on realistic, temporally-structured whisker simulator data \u2014 using both supervised and tactile-specific contrastive self-supervision \u2014 produce internal representations that closely match neural activity in rodent somatosensory cortex, and that recurrence and task performance predict neural alignment.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 47,
      "paper_title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "crawl_rate": 1.0,
      "input_tokens": 10066,
      "output_tokens": 12090,
      "ideas_generated": 10,
      "contribution": "Introduce Elastic Multimodal Parallelism (EMP) and ElasticMM, a serving system that decouples multimodal inference stages, performs modality-aware load balancing, and elastically adjusts per-stage parallelism and caching to greatly reduce TTFT and improve throughput for MLLMs.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": true
        },
        {
          "idea_idx": 2,
          "is_match": true
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": true
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 48,
      "paper_title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 10094,
      "output_tokens": 4817,
      "ideas_generated": 2,
      "contribution": "The paper introduces a dynamical low-rank training scheme with a novel spectral regularizer that enhances adversarial robustness while achieving significant compression in neural networks.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        },
        {
          "idea_idx": 1,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 49,
      "paper_title": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training",
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "crawl_rate": 1.0,
      "input_tokens": 10878,
      "output_tokens": 7690,
      "ideas_generated": 5,
      "contribution": "QoQ-Med is the first open generalist clinical foundation model that effectively reasons across heterogeneous clinical data types.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": true
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 50,
      "paper_title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 9971,
      "output_tokens": 11025,
      "ideas_generated": 10,
      "contribution": "This paper presents a systematic investigation of gating mechanisms in softmax attention variants, demonstrating that a simple head-specific sigmoid gate can enhance performance in large language models significantly.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 51,
      "paper_title": "Learning long range dependencies through time reversal symmetry breaking",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 7990,
      "output_tokens": 5754,
      "ideas_generated": 1,
      "contribution": "Introducing Recurrent Hamiltonian Echo Learning (RHEL) for training state space models using Hamiltonian dynamics that efficiently compute gradients without backward pass or Jacobian calculations.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 52,
      "paper_title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 11856,
      "output_tokens": 4353,
      "ideas_generated": 1,
      "contribution": "FuXi-Ocean is the first data-driven global ocean forecasting model achieving six-hourly predictions at eddy-resolving 1/12\u00b0 spatial resolution.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 53,
      "paper_title": "Superposition Yields Robust Neural Scaling",
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "crawl_rate": 1.0,
      "input_tokens": 6742,
      "output_tokens": 11594,
      "ideas_generated": 10,
      "contribution": "The paper identifies representation superposition as a central driver of neural scaling laws, providing insights into the conditions under which these scaling laws can be enhanced or may break down.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": true
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 54,
      "paper_title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 5964,
      "output_tokens": 3945,
      "ideas_generated": 1,
      "contribution": "The paper establishes that CNNs predominantly rely on local shape features rather than being inherently biased towards texture, offering a new evaluation of feature reliance through controlled suppression.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 55,
      "paper_title": "On Linear Mode Connectivity of Mixture-of-Experts Architectures",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 9624,
      "output_tokens": 6054,
      "ideas_generated": 2,
      "contribution": "This paper investigates Linear Mode Connectivity (LMC) within Mixture-of-Experts (MoE) architectures, proposing a matching algorithm for aligning independently trained MoEs to discover low-loss paths in parameter space.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        },
        {
          "idea_idx": 1,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 56,
      "paper_title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model",
      "hit_at_k": true,
      "matching_idea_idx": 6,
      "crawl_rate": 1.0,
      "input_tokens": 14344,
      "output_tokens": 11454,
      "ideas_generated": 10,
      "contribution": "OpenHOI introduces the first framework for synthesizing open-world hand-object interactions using multimodal large language models.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": true
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 57,
      "paper_title": "Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 6186,
      "output_tokens": 4188,
      "ideas_generated": 1,
      "contribution": "Representation Entanglement for Generation (REG) enhances the training efficiency and quality of image generation in diffusion models by entangling class tokens from pretrained models with low-level image latents.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 58,
      "paper_title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 11645,
      "output_tokens": 11174,
      "ideas_generated": 10,
      "contribution": "Dynam3D presents a dynamic layered 3D representation model that enhances vision-and-language navigation by improving spatial understanding and flexibility in changing environments.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 59,
      "paper_title": "Learning (Approximately) Equivariant Networks via Constrained Optimization",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 11355,
      "output_tokens": 5351,
      "ideas_generated": 2,
      "contribution": "The paper introduces Adaptive Constrained Equivariance (ACE), a framework that systematically relaxes equivariance constraints during training to improve performance in equivariant neural networks.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 60,
      "paper_title": "SAGE: A Unified Framework for Generalizable Object State Recognition with State-Action Graph Embedding",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 9373,
      "output_tokens": 6353,
      "ideas_generated": 3,
      "contribution": "SAGE introduces a unified framework for recognizing object physical states and their temporal evolutions using State-Action Graph Embedding, enhancing generalization to unseen objects and actions.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 61,
      "paper_title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 4891,
      "output_tokens": 5088,
      "ideas_generated": 2,
      "contribution": "The paper critically evaluates the effectiveness of Reinforcement Learning with Verifiable Rewards (RLVR) in enhancing the reasoning capabilities of large language models (LLMs) and reveals that the improvements are primarily superficial, as they do not generate fundamentally new reasoning patterns beyond those already established by base models.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 62,
      "paper_title": "Learning to Learn with Contrastive Meta-Objective",
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "crawl_rate": 1.0,
      "input_tokens": 10348,
      "output_tokens": 11758,
      "ideas_generated": 10,
      "contribution": "The paper introduces ConML, a meta-learning framework that utilizes task identity as additional supervision through contrastive learning to enhance generalizability.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": true
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 63,
      "paper_title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 5798,
      "output_tokens": 4124,
      "ideas_generated": 1,
      "contribution": "KVzip introduces a query-agnostic KV cache eviction method that enables the reuse of compressed KV caches across diverse queries, significantly reducing memory overhead and attention latency.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 64,
      "paper_title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "crawl_rate": 1.0,
      "input_tokens": 9996,
      "output_tokens": 5531,
      "ideas_generated": 2,
      "contribution": "HyperET introduces a paradigm for effectively training multi-modal large language models in hyperbolic space to align visual and textual representations across varying levels of granularity with improved efficiency.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": true
        },
        {
          "idea_idx": 1,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 65,
      "paper_title": "SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 5868,
      "output_tokens": 5018,
      "ideas_generated": 1,
      "contribution": "Introduction of SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic audio-visual scenes, and a novel training-free reasoning pipeline that enhances AV-LLMs' performance in understanding such environments.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 66,
      "paper_title": "A multiscale analysis of mean-field transformers in the moderate interaction regime",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 10256,
      "output_tokens": 10723,
      "ideas_generated": 10,
      "contribution": "The paper provides a multiscale framework to analyze the dynamics of tokens in transformer models by treating them as mean-field interacting particles, especially in the moderate interaction regime.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 67,
      "paper_title": "Exploring Diffusion Transformer Designs via Grafting",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 10839,
      "output_tokens": 9551,
      "ideas_generated": 10,
      "contribution": "The paper introduces grafting as a method for efficiently modifying pretrained diffusion transformer architectures to explore new designs without extensive retraining.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        },
        {
          "idea_idx": 2,
          "is_match": false
        },
        {
          "idea_idx": 3,
          "is_match": false
        },
        {
          "idea_idx": 4,
          "is_match": false
        },
        {
          "idea_idx": 5,
          "is_match": false
        },
        {
          "idea_idx": 6,
          "is_match": false
        },
        {
          "idea_idx": 7,
          "is_match": false
        },
        {
          "idea_idx": 8,
          "is_match": false
        },
        {
          "idea_idx": 9,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 68,
      "paper_title": "Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 11088,
      "output_tokens": 5433,
      "ideas_generated": 2,
      "contribution": "The paper introduces a novel hybrid optimization method that combines steepest descent and conditional gradient approaches under a generalized notion of (L0,L1)-smoothness, facilitating efficient training in non-Euclidean spaces.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        },
        {
          "idea_idx": 1,
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 69,
      "paper_title": "Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion",
      "hit_at_k": false,
      "matching_idea_idx": null,
      "crawl_rate": 1.0,
      "input_tokens": 7449,
      "output_tokens": 4759,
      "ideas_generated": 1,
      "contribution": "The paper introduces a novel multimodal learning approach that dynamically balances the classification abilities of strong and weak modalities using a sustained boosting algorithm and adaptive classifier assignment.",
      "judgments": [
        {
          "idea_idx": 0,
          "is_match": false
        }
      ]
    }
  ]
}
{
  "prior_works": [
    {
      "title": "Offline Reinforcement Learning with Implicit Q-Learning",
      "authors": "Ilya Kostrikov et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Introduced an offline actor-critic formulation (expectile value regression with advantage-weighted policy extraction) enabling learning from mixed-quality data; this paper scales that principle to large Perceiver-based models and multi-task datasets."
    },
    {
      "title": "A Minimalist Approach to Offline Reinforcement Learning",
      "authors": "Scott Fujimoto et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Showed that a simple actor-critic with behavior-cloning regularization (TD3+BC) is a strong offline baseline; the present paper demonstrates that such simple offline actor-critic methods continue to improve with model scale and outperform pure BC when implemented with large attention backbones."
    },
    {
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "authors": "Aviral Kumar et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Identified distributional shift and overestimation as central failure modes in offline actor-critic and proposed conservative Q regularization; this insight motivates the stability considerations when training large attention-based critics in the present work."
    },
    {
      "title": "AWAC: Advantage-Weighted Actor-Critic for Offline Reinforcement Learning",
      "authors": "Ashvin Nair et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Proposed advantage-weighted actor updates for offline RL; the current work extends this idea by implementing actor and critic with a Perceiver architecture and showing it scales across 132 tasks with mixed-quality data."
    },
    {
      "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
      "authors": "Lili Chen et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "Framed offline RL as supervised sequence modeling and set the large-model behavioral cloning baseline for multi-task control; this paper directly compares against it and shows offline actor-critic with transformers surpasses it under scaling."
    },
    {
      "title": "A Generalist Agent (Gato)",
      "authors": "Scott Reed et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated a generalist agent trained purely by behavioral cloning on diverse tasks, establishing the prevailing BC paradigm; the present work targets this paradigm\u2019s limitations and argues offline actor-critic is a more scalable alternative."
    },
    {
      "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs",
      "authors": "Andrew Jaegle et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Introduced the self- and cross-attention latent architecture adopted here; this paper adapts Perceiver-style modules to actor-critic training and identifies the key features needed for offline RL to work with them at scale."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core insight\u2014that simple offline actor-critic methods can scale with large attention-based models to outperform behavioral cloning\u2014rests on three converging lines of prior work. First, offline actor-critic algorithms provided the methodological backbone. Implicit Q-Learning established a robust offline AC formulation that handles mixed-quality data via expectile value regression and advantage-weighted policy extraction, while TD3+BC demonstrated that very simple actor-critic objectives with a BC prior can be competitive in the offline regime. AWAC further distilled the advantage-weighted update that this paper scales up, directly informing the actor training design used with large attention backbones. Complementing these, CQL crystallized the distribution shift challenge and emphasized critic conservatism, shaping the stability considerations for training large critics.\nSecond, the dominance of large-model behavioral cloning baselines set the target this work aims to surpass. Decision Transformer popularized sequence-modeling as the de facto large-model approach for offline multi-task control, and Gato showed the power\u2014and limitations\u2014of the BC paradigm in building generalist agents. The present paper explicitly positions offline actor-critic as a superior path when scaled, addressing those limitations.\nThird, Perceiver IO inspired the architectural choice: a self-/cross-attention latent bottleneck that can flexibly fuse multi-modal context. By adapting Perceiver-style modules to both actor and critic, the authors identify the features needed to make offline RL stable and effective with large transformers, enabling multi-task policies across 132 domains, including real robotics.",
  "analysis_timestamp": "2026-01-06T23:09:26.452069"
}
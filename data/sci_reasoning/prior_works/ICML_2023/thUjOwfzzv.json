{
  "prior_works": [
    {
      "title": "RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning",
      "authors": "Yan Duan et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "AdA instantiates the RL^2 meta-RL formulation\u2014learning a within-episode adaptation algorithm via a recurrent policy\u2014scaling it to a vast 3D task distribution to yield human-timescale in-context adaptation."
    },
    {
      "title": "Learning to Reinforcement Learn",
      "authors": "Jane X. Wang et al.",
      "year": 2016,
      "role": "Extension",
      "relationship_sentence": "This work showed that a memory-based policy can learn exploration and exploitation strategies across task distributions; AdA extends that idea to open-ended, embodied 3D tasks with far larger models and richer task diversity."
    },
    {
      "title": "Stabilizing Transformers for Reinforcement Learning",
      "authors": "Emilio Parisotto et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "AdA\u2019s large attention-based memory architecture builds directly on the insight that transformer-style sequence models (GTrXL) can serve as stable, long-horizon memory policies in RL, enabling the emergence of in-context learning at scale."
    },
    {
      "title": "Teacher\u2013Student Curriculum Learning",
      "authors": "Tarmo Matiisen et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "AdA\u2019s automated curriculum that prioritizes tasks near the agent\u2019s competence frontier is a direct large-scale realization of teacher\u2013student curriculum principles that sample tasks of appropriate difficulty to maximize learning progress."
    },
    {
      "title": "ALP-GMM: Active Learning for Automatic Curriculum Generation in Deep Reinforcement Learning",
      "authors": "Adrien Portelas et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "AdA\u2019s frontier-focused task selection echoes ALP-GMM\u2019s learning-progress-based sampling, explicitly targeting tasks where the agent is improving most to drive rapid adaptation."
    },
    {
      "title": "POET: Paired Open-Ended Trailblazer: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions",
      "authors": "Rui Wang et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "POET established that co-evolving tasks and agents via frontier challenges produces open-ended skill growth; AdA adopts this open-endedness principle, but replaces evolutionary search with meta-RL plus an automated frontier curriculum."
    },
    {
      "title": "A Generalist Agent",
      "authors": "Scott Reed et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Gato demonstrated that a single large attention-based policy can act across diverse embodied tasks; AdA leverages a similar large-scale, sequence-model policy but trains it via meta-RL so it can adapt in-context to novel 3D dynamics."
    }
  ],
  "synthesis_narrative": "AdA\u2019s core innovation\u2014human-timescale in-context adaptation to novel, open-ended 3D tasks\u2014stands on three intertwined lines of prior work. First, meta-reinforcement learning established the problem formulation that a recurrent policy can itself learn an inner learning algorithm from experience within an episode. RL^2 (Duan et al.) and Learning to Reinforcement Learn (Wang et al.) showed that memory-based agents can internalize fast exploration\u2013exploitation strategies across task distributions. AdA squarely builds on this foundation, but scales it to a much broader, smoother task space where meta-learning can express general-purpose in-context learning.\nSecond, realizing such rapid adaptation required a memory architecture with long effective context. GTrXL (Parisotto et al.) demonstrated that transformers can be stabilized for RL and serve as powerful sequence memories. AdA extends this idea by employing a large attention-based memory policy to enable hypothesis-driven exploration and exploitation over long horizons, including the ability to condition on first-person demonstration prompts.\nThird, the automated curriculum is rooted in open-ended and curriculum-learning principles that focus training on the frontier of competence. Teacher\u2013Student Curriculum Learning and ALP-GMM formalized sampling by learning progress and task difficulty, while POET showed the power of frontier challenges in open-ended settings. AdA operationalizes these ideas at scale: an autocurriculum continually surfaces tasks at the agent\u2019s capability boundary, driving sustained meta-learning. Finally, Gato\u2019s success with a single large attention policy acting across modalities motivated AdA\u2019s architecture choice, while AdA\u2019s contribution is to endow such a policy with meta-RL-driven, fast in-context adaptation.",
  "analysis_timestamp": "2026-01-06T23:09:26.561593"
}
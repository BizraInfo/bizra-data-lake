{
  "prior_works": [
    {
      "title": "Prevalence of neural collapse during the terminal phase of deep learning",
      "authors": "Vardan Papyan et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "The Canonical Representation Hypothesis (CRH) explicitly generalizes the last-layer alignment and simplex-ETF geometry identified by Neural Collapse to most hidden layers and further includes gradient alignment, making Papyan et al.\u2019s discovery the foundational empirical/theoretical template this work extends."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "CRH\u2019s claim that representations and weights become mutually aligned during training builds on the classic dynamical analysis showing alignment of weights with input\u2013output singular vectors in deep linear nets, which this paper extends conceptually to nonlinear networks and to gradient alignment."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Results on the directional convergence of weights under cross-entropy (toward max-margin) underpin the paper\u2019s argument that classifier weights align with learned features and gradients; CRH weaves this implicit-bias mechanism into a multi-layer, R\u2013W\u2013G alignment principle."
    },
    {
      "title": "Heavy-Tailed Self-Regularization in Deep Neural Networks",
      "authors": "Charles H. Martin et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Empirical findings of power-law behavior in neural network spectra directly motivate the Polynomial Alignment Hypothesis (PAH), which posits reciprocal power-law relations among representations, weights, and gradients when CRH breaks."
    },
    {
      "title": "Opening the Black Box of Deep Neural Networks via Information",
      "authors": "Ravid Shwartz-Ziv et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "While the Information Bottleneck view hypothesizes representation compression, it lacks a concrete mechanism; CRH addresses this gap by providing an explicit alignment-based mechanism for compact, task-invariant representations during training."
    },
    {
      "title": "Unsupervised learning of invariant representations in hierarchical architectures",
      "authors": "Fabio Anselmi et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "This i-theory work formalized how hierarchical architectures can yield invariances; CRH leverages and updates that invariance objective, proposing that alignment among R\u2013W\u2013G naturally enforces invariance to task-irrelevant transformations in modern trained networks."
    },
    {
      "title": "Spectral bias and task-model alignment explain generalization in kernel regression",
      "authors": "Murat Canatar et al.",
      "year": 2021,
      "role": "Related Problem",
      "relationship_sentence": "The notion that model\u2013task alignment governs learning directly informs CRH\u2019s broader alignment relations, with this paper extending alignment from kernel predictors to joint alignment of features, weights, and gradients in deep networks."
    }
  ],
  "synthesis_narrative": "The paper\u2019s Canonical Representation Hypothesis (CRH) emerges by unifying and extending several precise strands of prior work on alignment, invariance, and collapse. Neural Collapse (Papyan et al., 2020) established that, late in training, last-layer features align with classifier weights and form simplex-ETF geometry; CRH generalizes this principle beyond the terminal layer and introduces gradient alignment as a co-equal element, thereby explaining representation formation throughout the network. The feasibility of such alignment dynamics is grounded in classic results on deep linear nets (Saxe et al., 2014), where weights align with input\u2013output structures over training, and in implicit-bias theory (Soudry et al., 2018), which shows gradient descent drives classifier directions toward max-margin\u2014providing a mechanism for weight\u2013feature alignment. Building on i-theory (Anselmi et al., 2016), CRH links this alignment to invariance: when R, W, and G are canonically aligned, the learned representation becomes compact and insensitive to task-irrelevant transformations, supplying a concrete mechanism that the Information Bottleneck perspective (Shwartz-Ziv & Tishby, 2017) hypothesized but did not mechanistically specify. Finally, the paper\u2019s Polynomial Alignment Hypothesis (PAH) connects deviations from CRH to reciprocal power laws among R, W, and G, motivated by heavy-tailed spectral regularization observed in trained networks (Martin & Mahoney, 2019). Complementing these, alignment-centric generalization insights from kernel regression (Canatar et al., 2021) inform the broader framing that task\u2013model alignment governs learning, now elevated to joint alignment of features, weights, and gradients.",
  "analysis_timestamp": "2026-01-06T23:09:26.633888"
}
{
  "prior_works": [
    {
      "title": "Neural Message Passing for Quantum Chemistry",
      "authors": "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl",
      "year": 2017,
      "role": "Foundational message-passing formalism for GNNs",
      "relationship_sentence": "MPC builds directly on the MPNN paradigm introduced here, quantifying task difficulty specifically in terms of what can be achieved through message passing."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka",
      "year": 2019,
      "role": "Cornerstone of WL-based expressivity theory (1-WL and GIN)",
      "relationship_sentence": "The paper\u2019s core critique and its preservation of impossibility results are anchored in the 1-WL expressivity framework formalized and popularized by this work."
    },
    {
      "title": "Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks",
      "authors": "Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, Martin Grohe",
      "year": 2019,
      "role": "Expressivity beyond 1-WL via higher-order GNNs",
      "relationship_sentence": "By showing how higher-order models surpass 1-WL, this work defines the expressivity ladder that MPC explicitly argues is often unnecessary and complements with a continuous complexity lens."
    },
    {
      "title": "The Logic of Graph Neural Networks",
      "authors": "Martin Grohe",
      "year": 2021,
      "role": "Logical characterization and limits of GNN expressivity",
      "relationship_sentence": "MPC\u2019s claim to preserve expressivity-theoretic impossibility results draws on the logical limits articulated here, while moving beyond their binary nature."
    },
    {
      "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
      "authors": "Uri Alon, Eran Yahav",
      "year": 2021,
      "role": "Identification of over-squashing as a practical limitation",
      "relationship_sentence": "MPC explicitly models the difficulty of long-range dependency aggregation central to over-squashing, translating this practical bottleneck into a quantitative complexity measure."
    },
    {
      "title": "Understanding Over-Squashing and Bottlenecks in Graph Neural Networks",
      "authors": "Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, Michael M. Bronstein",
      "year": 2022,
      "role": "Geometric/curvature view and diagnostics of over-squashing",
      "relationship_sentence": "The curvature-based diagnosis of information contraction informs MPC\u2019s emphasis on graph-induced message aggregation constraints as a core source of task complexity."
    },
    {
      "title": "Graph Neural Networks Exponentially Lose Expressive Power as Depth Increases",
      "authors": "Kenta Oono, Taiji Suzuki",
      "year": 2020,
      "role": "Depth-related contraction and over-smoothing theory",
      "relationship_sentence": "MPC internalizes depth-induced contraction effects highlighted here, treating them as quantitative factors that increase the message-passing difficulty of tasks."
    }
  ],
  "synthesis_narrative": "The proposed Message Passing Complexity (MPC) emerges from reconciling two established lines of GNN theory: the WL-based expressivity viewpoint and the practical limitations of message passing. Gilmer et al. provided the formal MPNN framework that MPC explicitly targets, i.e., quantifying how hard a task is when solved via local message exchanges. On the expressivity side, Xu et al. rigorously tied standard GNNs to the 1-WL test and showed that simple architectures can already reach the WL ceiling; Morris et al. extended this to higher-order models, charting a hierarchy of expressivity. MPC positions itself as orthogonal: while preserving the impossibility results implied by these WL-based analyses, it critiques their binary nature and idealized assumptions by supplying a continuous, task-specific difficulty measure.\nCrucially, MPC is motivated by practical bottlenecks in information propagation. Alon and Yahav identified over-squashing as a dominant failure mode for long-range dependencies; Topping et al. further connected this to graph geometry and curvature, providing diagnostics of where and why messages are compressed. Oono and Suzuki theoretically established depth-induced contraction, explaining why simply adding layers often degrades information fidelity. MPC operationalizes these phenomena into a quantitative lens that predicts when and why message passing will struggle, while remaining consistent with the logical expressivity limits captured by Grohe. Together, these works directly inform MPC\u2019s central contribution: a principled, continuous measure that bridges expressivity theory with the real, architecture- and graph-dependent difficulty of GNN tasks.",
  "analysis_timestamp": "2026-01-06T23:42:48.131287"
}
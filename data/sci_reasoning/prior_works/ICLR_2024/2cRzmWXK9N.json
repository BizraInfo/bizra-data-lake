{
  "prior_works": [
    {
      "title": "Direct Preference Optimization: Your Language Model Is Secretly a Reward Model",
      "authors": "Rafailov et al.",
      "year": 2023,
      "arxiv_id": "2305.18290",
      "role": "Extension",
      "relationship_sentence": "f-DPO directly generalizes DPO by replacing its reverse-KL regularization with a broad class of f-divergences and deriving the corresponding reward\u2013policy linkage via KKT, addressing DPO\u2019s restriction to reverse KL."
    },
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Christiano et al.",
      "year": 2017,
      "arxiv_id": "1706.03741",
      "role": "Gap Identification",
      "relationship_sentence": "By formalizing RLHF with learned reward models and KL-regularized policy optimization, this work exposed the complexity and reward-model dependence that DPO (and thus f-DPO) explicitly aim to avoid while remaining equivalent under suitable divergence constraints."
    },
    {
      "title": "Fine-Tuning Language Models from Human Preferences",
      "authors": "Ziegler et al.",
      "year": 2019,
      "arxiv_id": "1909.08593",
      "role": "Foundation",
      "relationship_sentence": "This paper established the KL-regularized RLHF template against a reference policy, providing the divergence-regularized policy objective that f-DPO generalizes from reverse KL to Jensen\u2013Shannon, forward KL, and alpha-divergences."
    },
    {
      "title": "Relative Entropy Policy Search",
      "authors": "Peters et al.",
      "year": 2010,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "REPS showed how to solve divergence-constrained policy optimization via Lagrangian/KKT analysis, a technique f-DPO adopts to derive closed-form reward\u2013policy relationships under general f-divergence constraints."
    },
    {
      "title": "Information-type measures of difference of probability distributions (f-divergences)",
      "authors": "Csisz\u00e1r",
      "year": 1967,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work defined the f-divergence family and its convexity properties, enabling f-DPO to systematically substitute reverse KL with other divergences (e.g., JS and alpha) while retaining tractable optimality conditions."
    },
    {
      "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization",
      "authors": "Nowozin et al.",
      "year": 2016,
      "arxiv_id": "1606.00709",
      "role": "Inspiration",
      "relationship_sentence": "f-GAN demonstrated that swapping among f-divergences yields markedly different learning behaviors and provided practical parameterizations, motivating f-DPO\u2019s exploration of JS, forward KL, and alpha-divergences for preference optimization."
    },
    {
      "title": "Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons",
      "authors": "Bradley and Terry",
      "year": 1952,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The Bradley\u2013Terry model provides the pairwise preference likelihood linking reward differences to comparison probabilities, the assumption under which DPO and f-DPO derive the reward\u2013policy relation."
    }
  ],
  "synthesis_narrative": "Direct Preference Optimization (DPO) showed that, under a reverse-KL regularization to a reference policy, the reward\u2013policy relationship for pairwise preferences admits a closed form, enabling direct optimization of preference likelihood without training an explicit reward model. Deep RL from Human Preferences (Christiano et al.) formulated the modern RLHF pipeline\u2014learning a reward model from pairwise comparisons and optimizing a KL-regularized policy\u2014thereby highlighting both the effectiveness of preference data and the practical burden of reward-model training. Fine-Tuning Language Models from Human Preferences (Ziegler et al.) established the explicit KL penalty to a reference model as the central regularizer in preference-optimized language models. Relative Entropy Policy Search (REPS) provided the Lagrangian/KKT route to solving divergence-constrained policy optimization, yielding closed-form optimality conditions under information-theoretic constraints. Csisz\u00e1r\u2019s f-divergences unified a broad family of divergences (including KL, Jensen\u2013Shannon, and alpha-divergences) with convexity properties amenable to optimality analysis. f-GAN further demonstrated that different f-divergences lead to distinct optimization behaviors, motivating principled divergence choices. The Bradley\u2013Terry model grounded pairwise preference likelihoods in reward differences.\n\nTogether, these works reveal both a template\u2014preference-driven, divergence-regularized policy optimization\u2014and a limitation: existing direct methods hinge on reverse KL. The convergence of KKT-based solutions for divergence constraints (REPS), the generality of f-divergences (Csisz\u00e1r), and evidence that divergence choice matters (f-GAN) makes it natural to generalize DPO beyond reverse KL. By carrying the Bradley\u2013Terry pairwise likelihood through a KKT analysis for multiple f-divergences, one can recover closed-form reward\u2013policy relations and retain DPO\u2019s reward-model-free advantages while tailoring behavior via JS, forward KL, or alpha-divergences.",
  "target_paper": {
    "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints",
    "authors": "Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, Yuxin Chen",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Large language models, Preference optimization, AI Alignment",
    "abstract": "The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative; and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush\u2013Kuhn\u2013Tucker conditions. This eliminates the need for estimat",
    "openreview_id": "2cRzmWXK9N",
    "forum_id": "2cRzmWXK9N"
  },
  "analysis_timestamp": "2026-01-06T16:24:31.592985"
}
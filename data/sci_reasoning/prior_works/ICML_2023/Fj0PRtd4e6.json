{
  "prior_works": [
    {
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": "Wei-Ning Hsu et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "BEATs adopts HuBERT\u2019s core idea of predicting offline discovered discrete units under a masked modeling objective and extends HuBERT\u2019s iterative target refinement into an explicit iterative co-optimization of an acoustic tokenizer and the SSL model for general audio."
    },
    {
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": "Alexei Baevski et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "By showing that learning to predict quantized discrete targets yields strong speech representations without reconstruction, wav2vec 2.0 directly motivates BEATs\u2019 choice of discrete label prediction rather than spectrogram reconstruction."
    },
    {
      "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations",
      "authors": "Alexei Baevski et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "vq-wav2vec introduced learning discrete acoustic units via vector quantization for use as prediction targets, a mechanism BEATs generalizes by learning a semantic-rich acoustic tokenizer whose codes become the labels for masked prediction."
    },
    {
      "title": "BEiT: BERT Pre-Training of Image Transformers",
      "authors": "Hangbo Bao et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "BEiT pioneered masked modeling using discrete labels produced by an external tokenizer; BEATs transfers this paradigm to audio by predicting tokens from an acoustic tokenizer instead of reconstructing spectrograms."
    },
    {
      "title": "BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers",
      "authors": "Hangbo Bao et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "BEiT v2\u2019s emphasis on improving and aligning the tokenizer to yield semantically meaningful targets directly informs BEATs\u2019 iterative pipeline that jointly optimizes a semantic-rich acoustic tokenizer and the audio SSL model."
    },
    {
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": "Sanyuan Chen et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "WavLM extends HuBERT-style masked unit prediction and provides strong semantic speech representations; BEATs leverages this lineage to bootstrap semantic acoustic tokens and extends the approach from speech to general audio events via iterative tokenizer-model refinement."
    },
    {
      "title": "SSAST: Self-Supervised Audio Spectrogram Transformer",
      "authors": "Yuan Gong et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "As a leading reconstruction-based audio SSL method (masked spectrogram modeling), SSAST serves as a primary baseline whose limitation\u2014overemphasis on low-level reconstruction\u2014BEATs addresses by predicting semantic discrete labels from an acoustic tokenizer."
    }
  ],
  "synthesis_narrative": "BEATs emerges at the intersection of two lines of work: masked prediction of discrete acoustic units from speech SSL and tokenizer-driven masked modeling from vision. On the speech side, vq-wav2vec and wav2vec 2.0 established that discrete, quantized targets can be powerful objectives for representation learning, while HuBERT crystallized the masked prediction of offline-discovered units and popularized an iterative refinement of targets. WavLM further demonstrated how HuBERT-style unit prediction can be scaled and enriched to capture higher-level semantics across diverse speech tasks. On the vision side, BEiT introduced the idea of using an external tokenizer to generate discrete labels for masked modeling, and BEiT v2 emphasized improving the tokenizer itself so that targets carry richer semantics. BEATs synthesizes these insights into the audio domain: it replaces reconstruction losses common in audio SSL with discrete label prediction and, crucially, iteratively co-optimizes an acoustic tokenizer and the SSL model so that the tokenizer\u2019s codes reflect high-level audio semantics rather than low-level details. This directly addresses the limitations of reconstruction-based audio SSL exemplified by SSAST, enabling BEATs to learn abstractions that transfer broadly across audio classification tasks. In short, BEATs\u2019 core innovation\u2014semantic acoustic tokenization coupled with iterative masked prediction\u2014stands on the explicit foundations of HuBERT-style unit prediction and BEiT-style tokenizer-driven pretraining, extended and unified for general audio.",
  "analysis_timestamp": "2026-01-06T23:09:26.573625"
}
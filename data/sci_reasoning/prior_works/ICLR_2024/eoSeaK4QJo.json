{
  "prior_works": [
    {
      "title": "Learning both Weights and Connections for Efficient Neural Networks",
      "authors": "Song Han et al.",
      "year": 2015,
      "arxiv_id": "1506.02626",
      "role": "Baseline",
      "relationship_sentence": "This work\u2019s magnitude-based unstructured weight pruning is adopted as the core weight-sparsification mechanism that the paper integrates into a spike- and energy-aware pruning framework."
    },
    {
      "title": "Learning Efficient Convolutional Networks through Network Slimming",
      "authors": "Zhuang Liu et al.",
      "year": 2017,
      "arxiv_id": "1708.06519",
      "role": "Inspiration",
      "relationship_sentence": "The idea of removing neurons/channels based on learned importance provided the key conceptual spark for introducing neuron-level importance and pruning\u2014here adapted to unstructured neuron pruning for SNNs rather than structured channel pruning in ANNs."
    },
    {
      "title": "Deep Residual Learning in Spiking Neural Networks",
      "authors": "Wei Fang et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By pushing deep SNN accuracy with residual architectures while incurring higher spike activity and energy, this work highlighted the unmet need for sparsity-centric approaches to recover energy efficiency, which the new framework directly targets."
    },
    {
      "title": "Going Deeper with Directly-Trained Larger Spiking Neural Networks (with Temporal-Dependent Batch Normalization)",
      "authors": "Zheng et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This paper demonstrated that deep, directly trained SNNs can match ANN accuracy but did not introduce energy-targeted sparsification, motivating the proposed pruning approach to explicitly exploit sparsity for energy savings."
    },
    {
      "title": "Temporal Efficient Training of Spiking Neural Networks",
      "authors": "Deng et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By reducing timesteps to cut compute/energy without exploiting synaptic or neuron sparsity, this method exposed a complementary gap that the new framework fills via unstructured weight and neuron pruning."
    },
    {
      "title": "Loihi: A Neuromorphic Manycore Processor with On-Chip Learning",
      "authors": "Mike Davies et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Loihi\u2019s energy model and demonstrations that energy scales with spike and synapse activity established that unstructured connectivity sparsity yields real hardware energy savings, grounding the paper\u2019s objective to maximize sparsity utilization."
    }
  ],
  "synthesis_narrative": "Magnitude-based unstructured weight pruning showed that many connections can be removed with minimal accuracy loss, establishing a practical route to fine-grained sparsity that maps well to computational savings. Network Slimming then demonstrated that neuron/channel importance can be learned and used to remove units, indicating that pruning at the neuron level\u2014beyond weights\u2014can further reduce compute. In spiking models, residual spiking architectures achieved strong accuracy, but their depth and activity increased spike events and compute, signaling that accuracy gains were eroding the energy advantage. Deeper, directly trained SNNs with temporal-dependent normalization similarly proved high performance was attainable without introducing mechanisms to curb spiking energy via structural sparsity. Temporal Efficient Training reduced timesteps, lowering temporal cost, yet left spatial sparsity largely untapped. Meanwhile, neuromorphic hardware results on Loihi made explicit that energy is tightly coupled to spike and synapse activity and that unstructured sparse connectivity directly translates to energy savings.\nBringing these threads together revealed a clear opportunity: combine fine-grained weight sparsity with neuron-level sparsity tailored to SNN dynamics to exploit neuromorphic sparsity end-to-end. The present framework synthesizes magnitude-based unstructured weight pruning with unstructured neuron pruning informed by spiking activity, directly targeting the energy model of neuromorphic processors. This is a natural next step after performance-centric deep SNNs and timestep reduction methods, aligning pruning granularity with hardware-relevant sparsity to restore and amplify SNN energy efficiency.",
  "target_paper": {
    "title": "Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework",
    "authors": "Xinyu Shi, Jianhao Ding, Zecheng Hao, Zhaofei Yu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Spiking Neural Networks, Network Pruning",
    "abstract": "Spiking Neural Networks (SNNs)  have emerged as energy-efficient alternatives to  Artificial Neural Networks (ANNs) when deployed on neuromorphic chips.  While recent studies have demonstrated the impressive performance of deep SNNs on challenging tasks, their energy efficiency advantage has been diminished. Existing methods targeting energy consumption reduction do not fully exploit sparsity, whereas powerful pruning methods can achieve high sparsity but are not directly targeted at energy efficiency, limiting their effectiveness in energy saving. Furthermore, none of these works fully exploit the sparsity of neurons or the potential for unstructured neuron pruning in SNNs. In this paper, we propose a novel pruning framework that combines unstructured weight pruning with unstructured neuron pruning to maximize the utilization of the sparsity of neuromorphic computing, thereby enhancing energy efficiency. To the best of our knowledge, this is the first application of unstructured neuro",
    "openreview_id": "eoSeaK4QJo",
    "forum_id": "eoSeaK4QJo"
  },
  "analysis_timestamp": "2026-01-07T00:24:57.428187"
}
{
  "prior_works": [
    {
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
      "authors": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, Wieland Brendel",
      "year": 2019,
      "role": "Conceptual and methodological precedent for cue-disentangling via controlled image manipulations",
      "relationship_sentence": "Cue3D generalizes Geirhos et al.\u2019s texture-vs-shape probe to 3D generation by systematically perturbing multiple monocular cues (shading, texture, silhouette, perspective) and measuring downstream 3D quality, leading to the finding that shape-meaningfulness, not texture, governs generalization."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations (ImageNet-C)",
      "authors": "Dan Hendrycks, Thomas Dietterich",
      "year": 2019,
      "role": "Methodological template for perturbation-based, model-agnostic evaluation benchmarks",
      "relationship_sentence": "Cue3D adopts the ImageNet-C philosophy of standardized, systematic input corruptions\u2014here, cue-specific perturbations\u2014to quantify performance drops across diverse single-image 3D generators in a unified, model-agnostic benchmark."
    },
    {
      "title": "Shape from Shading",
      "authors": "Berthold K. P. Horn, Michael J. Brooks",
      "year": 1989,
      "role": "Classical foundation establishing shading as a primary geometric cue for 3D inference",
      "relationship_sentence": "Horn and Brooks\u2019s theory motivates treating shading as a distinct, testable cue in Cue3D; perturbing shading isolates its contribution and supports the paper\u2019s central result that geometric cues\u2014especially shading\u2014are critical for single-image 3D generation."
    },
    {
      "title": "The Visual Hull Concept for Silhouette-Based Image Understanding",
      "authors": "Andrea Laurentini",
      "year": 1994,
      "role": "Classical formalization of silhouette constraints on 3D shape",
      "relationship_sentence": "Cue3D\u2019s analysis of over-reliance on provided silhouettes is grounded in Laurentini\u2019s visual hull theory, clarifying how silhouette cues can dominate reconstructions and motivating targeted silhouette perturbations in the benchmark."
    },
    {
      "title": "Neural 3D Mesh Renderer",
      "authors": "Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada",
      "year": 2018,
      "role": "Technical enabler for training from 2D cues via differentiable rendering (silhouettes/edges/photometric loss)",
      "relationship_sentence": "Differentiable rendering popularized by Kato et al. made silhouette and edge cues central supervision signals in single-image 3D; Cue3D directly audits how such cues drive modern methods\u2019 behavior and generalization."
    },
    {
      "title": "Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision",
      "authors": "Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger",
      "year": 2020,
      "role": "Representative implicit rendering paradigm trained from image cues",
      "relationship_sentence": "Cue3D evaluates implicit 3D methods like DVR under cue perturbations, revealing the sensitivity of photometric supervision to shading and perspective alterations and quantifying the geometric cue dependencies of this paradigm."
    },
    {
      "title": "Shap-E: Generating Conditional 3D Implicit Functions",
      "authors": "Heewoo Jun, Alex Nichol",
      "year": 2023,
      "role": "Native 3D generative modeling for image-conditioned 3D synthesis",
      "relationship_sentence": "As a native 3D generative approach conditioned on images, Shap-E motivates Cue3D\u2019s cross-paradigm benchmark; Cue3D shows how cue perturbations differentially affect native 3D generators versus regression and multi-view pipelines."
    }
  ],
  "synthesis_narrative": "Cue3D\u2019s core innovation\u2014a model-agnostic framework that quantifies how individual image cues drive single-image 3D generation\u2014stands on three pillars of prior work. First, classical vision established the cue taxonomy and its geometric import: Horn and Brooks formalized shading as a primary route from image intensities to shape, while Laurentini\u2019s visual hull theory articulated how silhouettes constrain 3D geometry. These works motivate treating shading and silhouette as distinct, testable factors and underpin Cue3D\u2019s finding that geometric cues\u2014especially shading\u2014are decisive, whereas texture contributes less to generalization. Second, the methodology of perturbation-based auditing comes from robustness and cue-conflict analyses in 2D vision. Geirhos et al. demonstrated that controlled manipulations (stylization) can expose texture versus shape reliance; Hendrycks and Dietterich showed that standardized corruptions enable model-agnostic, quantitative comparisons of performance drops. Cue3D generalizes these ideas to the 3D setting with cue-specific perturbations (shading, texture, silhouette, perspective, edges, continuity) and evaluates their impact on 3D quality. Third, differentiable rendering and modern 3D generative paradigms provide the technical context and targets for analysis. Kato et al. and Niemeyer et al. made 3D learning from 2D cues practical, which explains silhouette and photometric dependencies that Cue3D measures and critiques. Finally, native 3D generative models like Shap-E motivated a cross-paradigm benchmark, enabling Cue3D to show how cue reliance varies across regression, multi-view, and native 3D generators, culminating in the central insight that shape meaningfulness, not texture, dictates generalization.",
  "analysis_timestamp": "2026-01-07T00:05:12.555515"
}
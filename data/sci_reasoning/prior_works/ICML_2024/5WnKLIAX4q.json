{
  "prior_works": [
    {
      "title": "Gaussian Processes for Machine Learning",
      "authors": "Carl E. Rasmussen, Christopher K. I. Williams",
      "year": 2006,
      "role": "Foundational GP theory and conjugate closed-form inference under Gaussian noise",
      "relationship_sentence": "RCGP explicitly preserves the classical GP conjugacy and closed-form conditioning developed in Rasmussen & Williams, while extending it to be robust under model misspecification."
    },
    {
      "title": "Robust Gaussian Process Regression with a Student-t Likelihood",
      "authors": "Mika Jyl\u00e4nki, Jarno Vanhatalo, Aki Vehtari",
      "year": 2011,
      "role": "Robust GP baseline using heavy-tailed likelihoods that breaks conjugacy",
      "relationship_sentence": "This work established robust GP regression via non-Gaussian likelihoods but required approximate inference; RCGP targets the same robustness goal while retaining exact conjugate updates and comparable computational cost."
    },
    {
      "title": "A general framework for updating belief distributions",
      "authors": "Pier Giovanni Bissiri, Chris Holmes, Stephen G. Walker",
      "year": 2016,
      "role": "Generalized Bayesian inference (Gibbs posteriors) via loss-based updating",
      "relationship_sentence": "RCGP is built on the Bissiri\u2013Holmes\u2013Walker framework, replacing the strict likelihood with a loss-derived objective to obtain robust posteriors that can be engineered to preserve GP conjugacy."
    },
    {
      "title": "Robust and Efficient Estimation by Minimising a Density Power Divergence",
      "authors": "Ayanendranath Basu, Ian R. Harris, Nils L. Hjort, M. C. Jones",
      "year": 1998,
      "role": "Density power divergence underpinning \u03b2-likelihood/tempered objectives for robustness",
      "relationship_sentence": "RCGP leverages ideas from density power divergence (e.g., power/\u03b2-likelihoods) to downweight outliers while, in Gaussian observation models, maintaining the algebra that yields closed-form GP updates."
    },
    {
      "title": "Inconsistency of Bayesian Inference for Misspecified Models and How to Fix It",
      "authors": "Peter Gr\u00fcnwald, Thijs van Ommen",
      "year": 2017,
      "role": "SafeBayes and \u03b1-posteriors for robustness under misspecification",
      "relationship_sentence": "RCGP\u2019s use of tempered/generalized posteriors for reliability under misspecification is directly motivated by SafeBayes, which advocates power posteriors to achieve robust learning rates."
    },
    {
      "title": "General Bayesian updating and the loss-likelihood bootstrap",
      "authors": "Paul Lyddon, Chris Holmes, Stephen G. Walker",
      "year": 2019,
      "role": "Calibration and practical implementation of generalized Bayes",
      "relationship_sentence": "RCGP benefits from methods to set and calibrate the learning-rate/temperature in generalized Bayes, as developed by Lyddon et al., enabling principled robustness without sacrificing tractability."
    },
    {
      "title": "Variational Learning of Inducing Variables in Sparse Gaussian Processes",
      "authors": "Michalis K. Titsias",
      "year": 2009,
      "role": "Inducing-point variational GP machinery leveraging conjugate linear\u2013Gaussian structure",
      "relationship_sentence": "By preserving conjugacy, RCGP slots into inducing-point sparse variational GP frameworks \u00e0 la Titsias with virtually no additional computational cost, enabling scalable robust inference."
    }
  ],
  "synthesis_narrative": "The core innovation of Robust and Conjugate Gaussian Process Regression (RCGP) is to achieve outlier- and misspecification-robust GP inference while retaining the conjugate, closed-form updates that make standard GPs practical. This synthesis builds on three intertwined strands. First, the classical GP framework (Rasmussen & Williams) identifies conjugacy under Gaussian noise as the engine behind analytic conditioning and efficient linear algebra. Second, robust GP methods using heavy-tailed likelihoods (Jyl\u00e4nki, Vanhatalo & Vehtari) demonstrated the need for robustness but at the cost of breaking conjugacy and requiring approximate inference. Third, generalized Bayesian inference provides the mechanism to reconcile robustness with tractability: the Bissiri\u2013Holmes\u2013Walker framework formalizes loss-based posteriors; density power divergence (Basu et al.) motivates \u03b2/power-likelihoods that downweight outliers; and SafeBayes (Gr\u00fcnwald & van Ommen) shows that tempered/\u03b1-posteriors address misspecification while often preserving conjugate algebra in Gaussian models. Practical deployment hinges on calibrating the generalized Bayes temperature, for which Lyddon\u2013Holmes\u2013Walker provide principled tools. Crucially, when Gaussian observation models are raised to a power or replaced by appropriate robust scoring rules, the resulting posterior remains Gaussian, so all closed-form GP conditioning carries over. This preserved linear\u2013Gaussian structure lets RCGP plug seamlessly into scalable inducing-point variational schemes (Titsias), delivering robustness at virtually no extra cost. Together, these works directly enable RCGP\u2019s central result: provably robust GP updates that remain exact and conjugate wherever standard GP conjugacy applies.",
  "analysis_timestamp": "2026-01-06T23:42:48.078047"
}
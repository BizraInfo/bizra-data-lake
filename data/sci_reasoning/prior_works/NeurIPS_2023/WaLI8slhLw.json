{
  "prior_works": [
    {
      "title": "Neural Discrete Representation Learning (VQ-VAE)",
      "authors": "A\u00e4ron van den Oord, Oriol Vinyals, Koray Kavukcuoglu",
      "year": 2017,
      "role": "Core method: discrete latent quantization",
      "relationship_sentence": "DeWave\u2019s \u201cquantized variational encoder\u201d directly builds on VQ-VAE\u2019s idea of learning a discrete codebook to convert continuous signals into symbolic token sequences."
    },
    {
      "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations",
      "authors": "Alexei Baevski, Steffen Schneider, Michael Auli",
      "year": 2020,
      "role": "Architectural inspiration for discretizing a biosignal",
      "relationship_sentence": "By showing how to learn meaningful discrete units from continuous audio, vq-wav2vec provided a blueprint DeWave adapts to EEG for deriving codebook-based token sequences."
    },
    {
      "title": "On Generative Spoken Language Modeling from Raw Audio (Textless NLP)",
      "authors": "Lakshya Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, et al.",
      "year": 2021,
      "role": "Language modeling over discrete acoustic units",
      "relationship_sentence": "Textless NLP demonstrated that powerful language models can operate over learned discrete units, motivating DeWave\u2019s alignment of EEG-derived discrete codes with pretrained LMs for open-vocabulary generation."
    },
    {
      "title": "wav2vec-U: Unsupervised Speech Recognition",
      "authors": "Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, Michael Auli",
      "year": 2021,
      "role": "Segmentation-free mapping from discrete units to text",
      "relationship_sentence": "wav2vec-U\u2019s use of learned discrete units plus a language model to map continuous signals to text without frame-level labels directly informed DeWave\u2019s segmentation-free EEG-to-text alignment strategy."
    },
    {
      "title": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks",
      "authors": "Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, J\u00fcrgen Schmidhuber",
      "year": 2006,
      "role": "Training paradigm for unsegmented sequence-to-sequence mapping",
      "relationship_sentence": "CTC introduced the core principle for handling unsegmented inputs, which underlies DeWave\u2019s objective of eliminating reliance on event markers or word-level alignments."
    },
    {
      "title": "Semantic reconstruction of continuous language from non-invasive brain recordings",
      "authors": "Jerry Tang, Shailee Jain, Alexander G. Huth, et al.",
      "year": 2023,
      "role": "Brain-to-text using pretrained language models",
      "relationship_sentence": "This work showed that pretrained language models provide a powerful prior for decoding continuous, noninvasive brain signals to text, a paradigm DeWave extends to EEG with discrete codes."
    },
    {
      "title": "ZuCo: A Cross-Subject Dataset for EEG and Eye Tracking during Natural Reading",
      "authors": "Nora Hollenstein, Ce Zhang, Lena J\u00e4ger, et al.",
      "year": 2018,
      "role": "Problem framing and prior reliance on eye-fixation segmentation",
      "relationship_sentence": "ZuCo and related EEG\u2013eye-tracking reading studies established word-level decoding pipelines that depend on fixation/event markers, a dependency DeWave explicitly removes with its discrete, alignment-free approach."
    }
  ],
  "synthesis_narrative": "DeWave\u2019s key contribution\u2014translating EEG into open-vocabulary text without eye-fixation/event markers\u2014emerges from uniting discrete latent modeling with language-model-based decoding paradigms. The discrete side is grounded in VQ-VAE, which introduced learning codebooks to convert continuous signals into symbolic units, and vq-wav2vec, which showed how to obtain such units from audio. Textless NLP extended this by demonstrating that strong language models can operate directly over learned discrete units, enabling natural language generation without supervised transcripts. wav2vec-U further crystallized the recipe for segmentation-free mapping from discrete units to text by pairing unit discovery with a language model, offering a template DeWave adapts from speech to EEG. Conceptually, CTC provided the foundational principle for learning from unsegmented sequences, reinforcing DeWave\u2019s removal of word-level alignment requirements. In parallel, recent brain-to-text advances from fMRI decoding used pretrained language models as powerful priors to reconstruct continuous language, validating the strategy of leveraging PLMs for brain signal interpretation. Finally, EEG-and-eye-tracking reading work such as ZuCo established a prevailing reliance on fixation/event markers for word-level alignment; DeWave\u2019s discrete \u201ccodex\u201d and LM alignment specifically target eliminating that constraint. Together, these lines of work directly shape DeWave\u2019s design: learn discrete EEG units, align them with a pretrained LM, and perform open-vocabulary EEG-to-text translation without external segmentation.",
  "analysis_timestamp": "2026-01-07T00:02:04.869192"
}
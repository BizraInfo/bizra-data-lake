{
  "prior_works": [
    {
      "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
      "authors": "Bruno A. Olshausen, David J. Field",
      "year": 1996,
      "role": "Efficient coding baseline; shows localized, oriented RFs emerge from explicit sparsity",
      "relationship_sentence": "Established that localized receptive fields can be learned from natural images via explicit sparsity constraints, providing the canonical benchmark that the present work seeks to match without top-down efficiency objectives and motivating a focus on what image statistics alone can induce."
    },
    {
      "title": "The 'independent components' of natural scenes are edge filters",
      "authors": "Anthony J. Bell, Terrence J. Sejnowski",
      "year": 1997,
      "role": "ICA and non-Gaussianity; links heavy-tailed natural image statistics to edge-like RFs",
      "relationship_sentence": "Demonstrated that maximizing statistical independence (leveraging non-Gaussianity) yields edge-filter-like bases, directly inspiring the present paper\u2019s emphasis on non-Gaussian statistics as the driver of localization absent explicit sparsity/independence objectives."
    },
    {
      "title": "Natural image statistics and neural representation",
      "authors": "Eero P. Simoncelli, Bruno A. Olshausen",
      "year": 2001,
      "role": "Characterization of natural image statistics (heavy tails, higher-order structure)",
      "relationship_sentence": "Characterized the higher-order, non-Gaussian regularities of natural images that underlie localized filters; these statistics inform the data model used and the hypothesis that kurtosis drives the learning dynamics toward localization."
    },
    {
      "title": "From basic network principles to neural architecture (series)",
      "authors": "Ralph Linsker",
      "year": 1986,
      "role": "Hebbian learning yields localized RFs from input statistics and architectural constraints",
      "relationship_sentence": "Showed that localized receptive fields can emerge via unsupervised Hebbian learning driven by input statistics, foreshadowing the current work\u2019s goal of explaining localization through bottom-up learning dynamics without explicit efficient-coding objectives."
    },
    {
      "title": "On-line learning in soft committee machines",
      "authors": "David Saad, Sara A. Solla",
      "year": 1995,
      "role": "Order-parameter theory for gradient-descent learning dynamics in multilayer nets",
      "relationship_sentence": "Provided the order-parameter framework to derive effective learning dynamics in multilayer networks, a methodological foundation the present paper adapts to analyze how non-Gaussian inputs drive weight alignment and localization."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli",
      "year": 2013,
      "role": "Analytic learning dynamics; timescales and mode-wise evolution under gradient descent",
      "relationship_sentence": "Introduced exact analyses of gradient-descent dynamics via mode decomposition, directly informing the present work\u2019s derivation of effective dynamics for receptive-field formation and extending the approach beyond Gaussian/linear settings."
    },
    {
      "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning",
      "authors": "Adam Coates, Honglak Lee, Andrew Y. Ng",
      "year": 2011,
      "role": "Empirical emergence of localized filters in shallow nets trained on natural images",
      "relationship_sentence": "Documented that simple feedforward learning on natural images produces Gabor-like localized features without handcrafted sparsity penalties, motivating a mechanistic theory\u2014developed here\u2014that explains the underlying dynamics."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central advance\u2014deriving the effective learning dynamics that explain how localized receptive fields arise in feedforward networks from natural image-like data without explicit efficient-coding constraints\u2014rests on two converging lines of prior work. First, efficient-coding studies (Olshausen & Field; Bell & Sejnowski) and the statistical characterization of natural images (Simoncelli & Olshausen) established that localized, oriented filters align with the heavy-tailed, non-Gaussian structure of natural scenes. These works linked localization to higher-order statistics\u2014particularly kurtosis\u2014while typically invoking explicit sparsity or independence objectives. In parallel, classic theoretical neuroscience (Linsker) showed that receptive fields can emerge from bottom-up learning driven by input statistics and network architecture, suggesting that explicit top-down constraints may not be necessary. The missing piece was a dynamical account connecting non-Gaussian statistics to the emergence of localized receptive fields under gradient-based learning.\nMethodologically, analytic treatments of learning dynamics in multilayer networks (Saad & Solla) and exact mode-wise solutions for gradient descent (Saxe, McClelland & Ganguli) provided the mathematical toolkit to derive low-dimensional order-parameter dynamics governing feature formation. Empirical demonstrations that shallow networks trained on natural images yield Gabor-like features (Coates, Lee & Ng) further motivated a theory explaining this phenomenon without enforcing sparsity. Synthesizing these strands, the present work extends mode-wise learning dynamics to non-Gaussian input models characteristic of natural images, revealing how higher-order statistics bias gradient descent toward localized receptive fields and thereby offering a principled, dynamical mechanism for localization absent explicit efficient-coding objectives.",
  "analysis_timestamp": "2026-01-07T00:02:04.734935"
}
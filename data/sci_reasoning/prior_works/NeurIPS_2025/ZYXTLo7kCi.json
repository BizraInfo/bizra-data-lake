{
  "prior_works": [
    {
      "title": "Abstracting Causal Models",
      "authors": "Sander Beckers, Joseph Y. Halpern",
      "year": 2019,
      "role": "Foundational theory of causal abstraction",
      "relationship_sentence": "Provides the formal definition of when a high-level causal model is an abstraction of a lower-level one, the very notion this paper scrutinizes by showing that unconstrained alignment maps can make such abstractions trivial."
    },
    {
      "title": "Inducing Causal Structure for Interpretable Neural Networks (Interchange Intervention Training)",
      "authors": "Atticus Geiger, Zhengxuan Wu, Thomas Icard, Christopher Potts, Noah D. Goodman, et al.",
      "year": 2021,
      "role": "Introduces causal abstraction applied to neural networks",
      "relationship_sentence": "Operationalizes causal abstraction for NNs via alignment maps and interchange interventions, establishing the practical framework that this paper critiques when alignment maps are allowed to be arbitrarily non-linear."
    },
    {
      "title": "Understanding intermediate layers using linear classifier probes",
      "authors": "Guillaume Alain, Yoshua Bengio",
      "year": 2017,
      "role": "Origin of linear probing for representations",
      "relationship_sentence": "Popularizes linear maps for extracting information from hidden representations, a methodological ancestor that motivates the paper\u2019s focus on how relaxing linearity in alignment maps undermines interpretive value."
    },
    {
      "title": "A Structural Probe for Finding Syntax in Word Representations",
      "authors": "John Hewitt, Christopher D. Manning",
      "year": 2019,
      "role": "Evidence for linear structure in representations",
      "relationship_sentence": "Demonstrates that complex linguistic structure can be recovered with linear projections, exemplifying the linear representation assumption whose removal the present work shows renders causal abstraction vacuous."
    },
    {
      "title": "Toy Models of Superposition in Neural Networks",
      "authors": "Nelson Elhage, Neel Nanda, Catherine Olsson, et al.",
      "year": 2022,
      "role": "Formulates the linear representation hypothesis",
      "relationship_sentence": "Articulates and analyzes the idea that features live as linear directions under superposition, a core hypothesis that justifies linear alignment maps and is explicitly questioned by allowing non-linear maps."
    },
    {
      "title": "Locating and Editing Factual Knowledge in GPT (ROME)",
      "authors": "Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",
      "year": 2022,
      "role": "Causal interventions assuming localized linear structure",
      "relationship_sentence": "Shows effective causal edits via rank-one linear updates to internal weights, reinforcing the practical success of linear mappings that this paper contrasts with the degeneracy of unconstrained non-linear alignments."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014a proof and demonstration that allowing arbitrarily powerful non-linear alignment maps trivializes causal abstraction for mechanistic interpretability\u2014builds directly on two intertwined threads. First, the formal basis of causal abstraction comes from work in causal modeling, notably Beckers and Halpern\u2019s definition of when a high-level causal system abstracts a lower-level one. Geiger and colleagues operationalized this idea for neural networks through alignment maps and interchange interventions, making causal abstraction a practical interpretability tool.\n\nSecond, the empirical tradition in representation analysis has strongly favored linear mappings. Alain and Bengio inaugurated linear probes to read out information from hidden layers, while Hewitt and Manning showed that even complex structures like syntax can be captured via linear projections. Elhage et al.\u2019s toy models further crystallized the linear representation hypothesis, arguing that features correspond to approximately linear directions\u2014even under superposition. Practical causal-editing methods such as ROME leveraged this linearity, achieving targeted interventions with rank-one updates.\n\nBy juxtaposing these lines, the present work argues that the informativeness of causal abstraction hinges critically on constraining the alignment map. Once the linearity (or similar structural constraints) is removed, the abstraction relation becomes too permissive: in theory\u2014and as their experiments illustrate\u2014one can align any neural network to any algorithm, defeating the goal of mechanistic understanding. Thus, the paper reframes causal abstraction as a constraint-sensitive tool, urging future work to specify and justify the permissible class of alignment maps.",
  "analysis_timestamp": "2026-01-07T00:29:41.026638"
}
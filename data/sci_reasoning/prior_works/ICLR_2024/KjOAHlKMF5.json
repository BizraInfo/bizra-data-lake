{
  "prior_works": [
    {
      "title": "An Experimental Comparison of Click Models for Web Search",
      "authors": "Craswell et al.",
      "year": 2008,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper introduced the cascade user behavior model (sequential examination with first-click), which directly underpins the reward and feedback structure assumed in cascading decision-making."
    },
    {
      "title": "Cascading Bandits: Learning to Rank in the Cascade Model",
      "authors": "Kveton et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "It formalized the learning problem for ordered lists under cascade feedback and developed UCB-style learning for unknown attraction probabilities, whose cascade objective and feedback are adopted and then generalized to the stateful RL setting."
    },
    {
      "title": "Combinatorial Cascading Bandits",
      "authors": "Kveton et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By exploiting cascade structure for tractable optimization over large combinatorial action spaces but remaining stateless, this work highlighted the missing piece of modeling and optimizing long-term state transitions that the new framework targets."
    },
    {
      "title": "Cascading Bandits with Linear Generalization",
      "authors": "Zong et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Although it introduced feature-based generalization for item attractions under cascade feedback, it explicitly lacked user-state dynamics, motivating an RL formulation where attraction depends on state and actions influence successor states."
    },
    {
      "title": "SlateQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets",
      "authors": "Ie et al.",
      "year": 2019,
      "arxiv_id": "1905.12767",
      "role": "Baseline",
      "relationship_sentence": "SlateQ\u2019s Q-function decomposition for slate actions under a user choice model provided the structural template for valuing ordered lists in an MDP, against which a cascade-specific decomposition with online exploration is developed."
    },
    {
      "title": "Minimax Regret Bounds for Reinforcement Learning",
      "authors": "Azar et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "The optimism-in-the-face-of-uncertainty value-iteration framework (UCBVI) and bonus design are adapted to the cascade slate setting to achieve computationally and sample-efficient exploration with regret guarantees."
    }
  ],
  "synthesis_narrative": "Craswell et al. established the cascade click model, where a user examines an ordered list and clicks the first attractive item, creating a distinctive reward and partial-feedback structure for ranking problems. Building on this, Kveton et al. formulated cascading bandits, showing how to learn unknown item attraction probabilities under cascade feedback and developing UCB-style algorithms tailored to ordered lists. They further extended tractability to large action spaces in combinatorial cascading bandits by exploiting the cascade structure, while still treating the problem as stateless. Zong et al. advanced the modeling by introducing linear generalization, enabling state-dependent (contextual) attraction estimation for cascade feedback but still without user-state transitions. In parallel, SlateQ framed slate recommendation as an MDP and proposed a slate Q-function decomposition under a user choice model, making slate RL computationally tractable. Azar et al. provided the UCBVI optimism template and regret analysis tools for finite-horizon RL, offering a principled way to combine planning with exploration.\nTogether these works reveal a gap: cascade methods capture ordered-list feedback but ignore evolving user states, while slate RL methods offer tractable decomposition but lack a cascade-specific formulation with online exploration guarantees. The natural next step is to fuse cascade feedback with MDP dynamics, decomposing the value of an ordered list in a way compatible with sequential examination and first-click behavior, and to pair this structure with UCBVI-style optimism to learn both attraction and transition effects efficiently\u2014achieving computational tractability and provable sample efficiency in cascading reinforcement learning.",
  "target_paper": {
    "title": "Cascading Reinforcement Learning",
    "authors": "Yihan Du, R. Srikant, Wei Chen",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "reinforcement learning, cascading bandits, combinatorial action space, computational and sample efficiency",
    "abstract": "Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with  large attraction probabilities but also leading to good successor states. This i",
    "openreview_id": "KjOAHlKMF5",
    "forum_id": "KjOAHlKMF5"
  },
  "analysis_timestamp": "2026-01-06T16:51:13.423334"
}
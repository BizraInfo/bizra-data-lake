{
  "prior_works": [
    {
      "title": "Attention Is All You Need",
      "authors": "Vaswani et al.",
      "year": 2017,
      "role": "foundational architecture",
      "relationship_sentence": "Provides the transformer architecture and autoregressive decoding paradigm that the paper adopts to model event histories and to support multiple prediction heads conditioned on shared contextual representations."
    },
    {
      "title": "Recurrent Marked Temporal Point Processes: Embedding Event History to Vector",
      "authors": "Du et al.",
      "year": 2016,
      "role": "algorithmic precursor in neural MTPP",
      "relationship_sentence": "Introduces neural modeling of marked temporal point processes with joint prediction of inter-event times and event marks, a blueprint the paper generalizes from RNNs to transformers and from discrete-only marks to mixed-type outputs."
    },
    {
      "title": "A Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process",
      "authors": "Mei and Eisner",
      "year": 2017,
      "role": "modeling paradigm for neural TPP",
      "relationship_sentence": "Demonstrates flexible, history-dependent neural TPPs for jointly modeling times and marks; the proposed method follows this paradigm but replaces recurrent/ODE dynamics with transformer attention and a more expressive continuous output head."
    },
    {
      "title": "Self-Attentive Hawkes Process (SAHP)",
      "authors": "Zhang et al.",
      "year": 2020,
      "role": "attention-based TPP precursor",
      "relationship_sentence": "Shows that self-attention can replace recurrence in TPPs; the present work builds on attention-based sequence modeling but removes Hawkes-specific constraints and adds mixed discrete\u2013continuous heads for heterogeneous events."
    },
    {
      "title": "Transformer Hawkes Process",
      "authors": "Zuo et al.",
      "year": 2020,
      "role": "direct architectural inspiration for TPP with transformers",
      "relationship_sentence": "Establishes transformer decoders as strong models for event sequences and marked TPPs; the new paper extends this by unifying discrete and continuous event attributes in one model and by using a flow-based continuous head to avoid intensity integrals."
    },
    {
      "title": "Real NVP: Density Estimation using Real-valued Non-Volume Preserving Transformations",
      "authors": "Dinh, Sohl-Dickstein, and Bengio",
      "year": 2016,
      "role": "foundational method for normalizing flows",
      "relationship_sentence": "Provides tractable change-of-variables with exact likelihoods for continuous variables; the paper leverages this principle to model continuous marks/inter-event times with exact densities rather than numerically integrating intensities."
    },
    {
      "title": "Neural Spline Flows",
      "authors": "Durkan, Bekasov, Murray, and Papamakarios",
      "year": 2019,
      "role": "expressive flow for conditional continuous heads",
      "relationship_sentence": "Introduces highly flexible monotonic transforms enabling sharp conditional density modeling; this directly motivates the paper\u2019s expressive normalizing-flow head for continuous event attributes conditioned on transformer states."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a single autoregressive transformer that models mixed-type (discrete and continuous) event sequences within an MTPP framework\u2014sits at the intersection of three lines of work. First, the adoption of self-attention for sequence modeling descends from Vaswani et al. (2017), enabling a unified contextual representation from which separate heads can predict heterogeneous event attributes. Within temporal point processes, Du et al. (2016) and Mei & Eisner (2017) established neural approaches to jointly model inter-event times and marks, demonstrating the value of rich history-dependent representations; the present work preserves this joint viewpoint but replaces recurrent/ODE dynamics with attention. Attention-based TPPs\u2014specifically SAHP (Zhang et al., 2020) and the Transformer Hawkes Process (Zuo et al., 2020)\u2014directly inspire the architectural choice to use transformers for event histories, yet those models either retain Hawkes structure or rely on intensity-based likelihoods that entail numerical integration. The second pillar is normalizing flows: Real NVP (Dinh et al., 2016) and Neural Spline Flows (Durkan et al., 2019) provide tractable, expressive density models for continuous variables. Conditioning such flows on transformer states yields an exact, integration-free likelihood for continuous attributes (including inter-event times), addressing a key computational bottleneck in intensity-based training. By combining these strands, the paper advances beyond tokenization-only EHR transformers and Hawkes-style attention models, delivering a unified, flexible model that natively handles mixed discrete\u2013continuous event attributes with exact likelihoods.",
  "analysis_timestamp": "2026-01-07T00:21:32.274198"
}
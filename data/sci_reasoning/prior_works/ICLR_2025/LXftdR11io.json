{
  "prior_works": [
    {
      "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback",
      "authors": "Adith Swaminathan et al.",
      "year": 2015,
      "role": "Baseline",
      "relationship_sentence": "POTEC directly targets the IPS-based policy optimization paradigm introduced by CRM/POEM, replacing its high-variance single-stage importance-weighted gradients with a decomposed, low-variance cluster-level gradient plus within-cluster learning."
    },
    {
      "title": "The Self-Normalized Estimator for Counterfactual Learning",
      "authors": "Adith Swaminathan et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "POTEC\u2019s novel low-variance gradient estimator is explicitly motivated by variance-reduction principles from self-normalized importance weighting, but achieves further variance reduction by Rao-Blackwellizing over action clusters rather than globally normalizing weights."
    },
    {
      "title": "Doubly Robust Policy Evaluation and Learning",
      "authors": "Miroslav Dud\u00edk et al.",
      "year": 2011,
      "role": "Foundation",
      "relationship_sentence": "POTEC\u2019s second-stage, regression-based action selection builds on the DM/DR insight that reward modeling can reduce variance, while addressing DR/DM\u2019s susceptibility to model misspecification by imposing a local (within-cluster) correctness condition instead of global calibration."
    },
    {
      "title": "The Offset Tree for Learning with Partial Labels",
      "authors": "Alina Beygelzimer et al.",
      "year": 2008,
      "role": "Inspiration",
      "relationship_sentence": "POTEC\u2019s two-stage policy decomposition echoes Offset Tree\u2019s reduction of multiclass bandit learning into simpler decisions to curb variance; POTEC adapts this decomposition idea to off-policy learning by first selecting an action cluster and then deciding within it."
    },
    {
      "title": "Deep Reinforcement Learning in Large Discrete Action Spaces",
      "authors": "Gabriel Dulac-Arnold et al.",
      "year": 2015,
      "role": "Related Problem",
      "relationship_sentence": "This work demonstrated that hierarchical/decomposed action selection and action clustering can make learning tractable in large discrete action spaces, directly motivating POTEC\u2019s cluster-first, then in-cluster selection strategy."
    },
    {
      "title": "Off-Policy Actor-Critic",
      "authors": "Thomas Degris et al.",
      "year": 2012,
      "role": "Gap Identification",
      "relationship_sentence": "By formalizing off-policy policy gradients with importance weights and highlighting their high variance, this paper underscored the core limitation POTEC overcomes with its low-variance off-policy gradient for cluster selection."
    }
  ],
  "synthesis_narrative": "POTEC\u2019s core innovation\u2014decomposing off-policy contextual bandit learning for large action spaces into (i) low-variance, policy-based cluster selection and (ii) regression-based action selection within clusters\u2014emerges from two intertwined lines of work. On the policy-based side, Counterfactual Risk Minimization (CRM/POEM) established the standard IPS-driven off-policy learning objective, but its importance-weighted gradients suffer severe variance in large action spaces. This problem was sharpened by off-policy policy-gradient analyses such as Off-Policy Actor-Critic, and prompted variance-reduction techniques like the Self-Normalized Estimator. POTEC advances this trajectory by deriving a novel, lower-variance gradient that effectively Rao-Blackwellizes importance weighting over action clusters, stabilizing learning at the first stage. On the structural side, Offset Tree illustrated that decomposing multiclass bandit decisions into simpler subproblems can curb variance, while work on deep RL in large discrete action spaces showed that hierarchical/clustered action selection scales learning. POTEC fuses these insights\u2014adopting a decomposition that first selects clusters and then acts within them. For the second stage, POTEC leverages the DM/DR tradition: regression-based estimates can reduce variance, but are biased under misspecification; POTEC\u2019s local correctness condition restricts the needed fidelity to within-cluster ranking, mitigating global bias. Together, these threads directly shape POTEC\u2019s two-stage design and its low-variance off-policy gradient estimator.",
  "analysis_timestamp": "2026-01-06T23:08:23.927244"
}
{
  "prior_works": [
    {
      "title": "A general reinforcement learning algorithm that masters chess, shogi and Go through self-play",
      "authors": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, et al.",
      "year": 2018,
      "role": "Self-play from scratch",
      "relationship_sentence": "AlphaZero established that an agent can improve tabula rasa via self-play without human data; Absolute Zero extends this zero-data self-improvement paradigm from games to open-ended reasoning by coupling self-play with verifiable rewards and task generation."
    },
    {
      "title": "PowerPlay: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem",
      "authors": "J\u00fcrgen Schmidhuber",
      "year": 2013,
      "role": "Autonomous task invention via learning progress",
      "relationship_sentence": "PowerPlay\u2019s principle of inventing tasks that maximize learning progress directly motivates Absolute Zero\u2019s task-proposer, which seeks problems on the frontier of the model\u2019s competence to drive continual reasoning improvement."
    },
    {
      "title": "POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions",
      "authors": "Rui Wang, Joel Lehman, Jeff Clune, Kenneth O. Stanley, et al.",
      "year": 2019,
      "role": "Open-ended coevolution of tasks and solvers",
      "relationship_sentence": "POET\u2019s co-evolutionary loop of environment generation and agent learning underpins Absolute Zero\u2019s open-ended, self-generated curriculum where task difficulty adapts to the solver\u2019s capabilities."
    },
    {
      "title": "ALP-GMM: Automatic Curriculum Learning for Deep RL",
      "authors": "Adrien Portelas, Emmanuel Benazera, Damien Ernst, Olivier Sigaud, Pierre-Yves Oudeyer",
      "year": 2019,
      "role": "Learning-progress-based curriculum",
      "relationship_sentence": "ALP-GMM operationalizes learning progress to sample goals of appropriate difficulty; Absolute Zero adopts this idea to select tasks that maximize expected learning gains under verifiable outcome rewards."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, et al.",
      "year": 2022,
      "role": "LLM-driven task synthesis",
      "relationship_sentence": "Self-Instruct showed LMs can bootstrap capabilities by generating their own training tasks; Absolute Zero generalizes this to a closed-loop RL setting, removing reliance on human-curated Q&A and adding verifiable rewards to guide learning."
    },
    {
      "title": "STaR: Bootstrapping Reasoning With Reasoning",
      "authors": "Adam Zelikman, Yuhuai (Tony) Wu, Jesse Mu, Noah D. Goodman",
      "year": 2022,
      "role": "Self-generated rationales with answer verification",
      "relationship_sentence": "STaR leverages final-answer verifiability to filter model-generated rationales; Absolute Zero advances this idea by replacing supervised bootstrapping with RL from verifiable rewards and by generating the tasks themselves."
    },
    {
      "title": "CodeRL: Mastering Code Generation with Deep Reinforcement Learning and Execution-based Feedback",
      "authors": "Hung Le, Yue Wang, Akhilesh Deepak Gotmare, et al.",
      "year": 2022,
      "role": "RL with verifiable (execution) rewards",
      "relationship_sentence": "CodeRL demonstrated that execution-based, rule-verifiable rewards can effectively train sequence models; Absolute Zero adopts this RLVR principle for reasoning tasks, coupling it with self-proposed problems to eliminate external datasets."
    }
  ],
  "synthesis_narrative": "Absolute Zero fuses three lines of prior work into a single, closed-loop framework: (1) tabula rasa self-play, (2) autonomous curriculum via learning progress, and (3) reinforcement learning with verifiable rewards (RLVR). AlphaZero provided the core blueprint that strong policies can emerge without human data through self-play. PowerPlay, ALP-GMM, and POET supplied the missing ingredient for open-ended domains: a mechanism to autonomously propose tasks at the competence frontier, using learning progress as the driving signal and co-evolving the problem distribution with the solver. In parallel, Self-Instruct and STaR showed that language models can bootstrap from their own outputs\u2014generating tasks or rationales and using verifiable end signals (correct final answers) to filter supervision\u2014reducing reliance on human-labeled data.\n\nAbsolute Zero integrates these insights and replaces supervised filtering with RLVR: rule-based outcome checks act as rewards, akin to execution feedback in CodeRL, allowing the system to optimize directly for correctness. The result is a self-contained loop where a single model proposes tasks to maximize its learning progress, solves them under verifiable rewards, and improves without any externally curated Q&A. This synthesis bridges game-centric self-play with LLM reasoning, grounding task generation in measurable learning progress and using programmatic verification to stably train beyond the limitations of human-provided datasets.",
  "analysis_timestamp": "2026-01-07T00:21:32.315609"
}
{
  "prior_works": [
    {
      "title": "Sliced inverse regression for dimension reduction",
      "authors": "Ker-Chau Li",
      "year": 1991,
      "role": "Foundation",
      "relationship_sentence": "This work introduced the single-index model formulation that the paper adopts to study one-hidden-layer predictors and to formalize the goal of recovering/aligning the latent index direction."
    },
    {
      "title": "One-bit compressed sensing by linear programming",
      "authors": "Yaniv Plan et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "Plan and Vershynin established learning under single-index style non-linear observations and highlighted that alignment with the latent direction governs recoverability; the present paper leverages this lens to quantify how pre-training\u2013induced alignment translates into labeled sample-complexity gains for SGD."
    },
    {
      "title": "A Theoretical Analysis of Contrastive Unsupervised Representation Learning",
      "authors": "Nikhil Saunshi et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "This paper provided a rigorous framework showing that unsupervised pre-training can yield transferable representations that reduce downstream sample complexity; the current work extends this program to single-index models and online SGD, turning representation quality (alignment) into provable polynomial/exponential gains over random initialization."
    },
    {
      "title": "Provable Meta-Learning of Linear Representations",
      "authors": "Nilesh Tripuraneni et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Tripuraneni et al. showed that transfer across tasks with a shared representation reduces labeled sample complexity; the present paper extends this idea from linear regression to non-linear single-index models and concept shift, proving that transfer-based initialization accelerates online SGD."
    },
    {
      "title": "An Analytical Formula for the Population Gradient of Two-Layer Neural Networks",
      "authors": "Yuandong Tian",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "Tian analyzed gradient dynamics from random initialization for shallow networks; the current paper takes random-init SGD on a single-layer network as the baseline and proves regimes where pre-training yields polynomial\u2014and even exponential\u2014sample-complexity improvements over this baseline."
    },
    {
      "title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods",
      "authors": "Majid Janzamin et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "This work demonstrated that appropriate spectral/tensor initializations can be crucial for efficiently learning shallow networks; the new paper formalizes unsupervised pre-training/transfer as such a warm-start and quantifies the resulting sample-complexity benefits for SGD on single-index models."
    },
    {
      "title": "Distribution-specific hardness of learning neural networks",
      "authors": "Ohad Shamir",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Shamir exhibited settings where SGD from random initialization is provably inefficient; the present work identifies analogous regimes in single-index learning and shows that pre-training circumvents these difficulties, yielding exponential gains in sample complexity."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014provably quantifying how unsupervised pre-training and transfer learning reduce the labeled sample complexity of online SGD for single-index models\u2014rests on three pillars: the single-index formulation, representation-learning benefits, and the role of initialization. The single-index perspective originates with Li (1991) and the subsequent non-linear observation viewpoint of Plan and Vershynin (2013), which together justify measuring progress via alignment with the latent index direction. Building on the representation-learning literature, Saunshi et al. (2019) rigorously tied unsupervised pre-training to downstream gains, while Tripuraneni et al. (2020) showed that transferring a shared representation across tasks provably reduces labeled samples; the current paper extends these insights to non-linear single-index settings and to concept shift, linking representation quality to faster online SGD convergence. The analysis explicitly contrasts against the baseline of training from random initialization, as in population-dynamics studies like Tian (2017), and pinpoints regimes\u2014echoing distribution-specific hardness from Shamir (2018)\u2014where random-init SGD is sample-inefficient. Finally, inspired by tensor/spectral initialization guarantees (Janzamin et al., 2015), the work formalizes pre-training/transfer as a principled warm start, proving polynomial and, in surprising cases, exponential improvements over random initialization. Together these threads directly shape the paper\u2019s main theorems on when and why pre-training and transfer provably help in high-dimensional single-index learning.",
  "analysis_timestamp": "2026-01-06T23:07:19.596033"
}